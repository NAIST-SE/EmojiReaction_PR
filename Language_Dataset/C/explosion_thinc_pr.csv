explosion/thinc,https://github.com/explosion/thinc,12,2016-02-28T20:45:12Z,2016-02-29T13:06:43Z,2016-02-29T13:06:43Z,MERGED,True,17,15,1,https://github.com/scoder,save another call to fread(),1,[],https://github.com/explosion/thinc/pull/12,https://github.com/scoder,1,https://github.com/explosion/thinc/pull/12,"The gain is smaller than for #11, about 15% in total, but still visible.","The gain is smaller than for #11, about 15% in total, but still visible.",True,{}
explosion/thinc,https://github.com/explosion/thinc,13,2016-03-25T08:18:17Z,2016-03-25T10:29:51Z,2016-03-25T10:29:51Z,MERGED,True,8,15,1,https://github.com/scoder,Fix serialiser,2,[],https://github.com/explosion/thinc/pull/13,https://github.com/scoder,1,https://github.com/explosion/thinc/pull/13,"Sorry, my fault. fread() doesn't set errno, only fopen() does. This should work.","Sorry, my fault. fread() doesn't set errno, only fopen() does. This should work.",True,{}
explosion/thinc,https://github.com/explosion/thinc,14,2016-04-16T13:05:16Z,2016-12-23T14:21:19Z,2016-12-23T14:21:19Z,CLOSED,False,16,38,2,https://github.com/henningpeters,new import-include proposal,1,[],https://github.com/explosion/thinc/pull/14,https://github.com/henningpeters,1,https://github.com/explosion/thinc/pull/14,Please check new idea for getting rid of shipping C headers of dependencies. As far as I can tell with this technique the only drawback is that on first install from a source distribution pip's default wheel building/caching process causes a non-fatal error. This seems better than potentially shipping incompatible headers.,Please check new idea for getting rid of shipping C headers of dependencies. As far as I can tell with this technique the only drawback is that on first install from a source distribution pip's default wheel building/caching process causes a non-fatal error. This seems better than potentially shipping incompatible headers.,True,{}
explosion/thinc,https://github.com/explosion/thinc,14,2016-04-16T13:05:16Z,2016-12-23T14:21:19Z,2016-12-23T14:21:19Z,CLOSED,False,16,38,2,https://github.com/henningpeters,new import-include proposal,1,[],https://github.com/explosion/thinc/pull/14,https://github.com/syllog1sm,2,https://github.com/explosion/thinc/pull/14#issuecomment-210813551,Please check new idea for getting rid of shipping C headers of dependencies. As far as I can tell with this technique the only drawback is that on first install from a source distribution pip's default wheel building/caching process causes a non-fatal error. This seems better than potentially shipping incompatible headers.,"Okay so, the idea is that the setup.py gets executed in one of four ways:

Compiling cython code to build the egg. Here we can assume that all dependencies are present at compile time.
Installing from egg, dependencies are present (i.e. murmurhash, numpy are there). Here we import the  module, and ask it to give us a path to its copy of the headers.
Installing from egg, dependencies aren't present. Here we fall back to vendorised headers.
Installing from binary wheel. Here the headers aren't relevant, right?

Not sure this is all correct, but thought going through these cases seemed like a good way through.",True,{}
explosion/thinc,https://github.com/explosion/thinc,14,2016-04-16T13:05:16Z,2016-12-23T14:21:19Z,2016-12-23T14:21:19Z,CLOSED,False,16,38,2,https://github.com/henningpeters,new import-include proposal,1,[],https://github.com/explosion/thinc/pull/14,https://github.com/henningpeters,3,https://github.com/explosion/thinc/pull/14#issuecomment-210814867,Please check new idea for getting rid of shipping C headers of dependencies. As far as I can tell with this technique the only drawback is that on first install from a source distribution pip's default wheel building/caching process causes a non-fatal error. This seems better than potentially shipping incompatible headers.,"Good idea to list all possible cases. I have following scenarios is mind:

Developer build: pip install -r requirements.txt && pip install -e ..
Packaging build: pip install -r requirements.txt && python setup.py sdist bdist_wheel
Install from wheel, either manually prepared (see 2) or using the official distribution or via pip's auto-cache mechanism that pip creates after installing from source: pip install thinc (OSX, Windows or auto-cache on Linux)
Install from official source distribution (dependencies present): pip install thinc (Linux or non-standard Python dist)
Install from official source distribution (dependencies not present): pip install thinc (Linux or non-standard Python dist, fresh install)

Eggs should behave the same as wheels (can be built via pip install -r requirements.txt && python setup.py bdist_egg). We don't provide an official distribution though and I didn't test my proposal properly with it, yet.
All those cases should be covered. The tricky case is the 5th, but except a potentially confusing error message it also works. Still, it might be a path thats often used by first-timers. Old pip versions that don't suppress install log output will display it.",True,{}
explosion/thinc,https://github.com/explosion/thinc,14,2016-04-16T13:05:16Z,2016-12-23T14:21:19Z,2016-12-23T14:21:19Z,CLOSED,False,16,38,2,https://github.com/henningpeters,new import-include proposal,1,[],https://github.com/explosion/thinc/pull/14,https://github.com/syllog1sm,4,https://github.com/explosion/thinc/pull/14#issuecomment-210820872,Please check new idea for getting rid of shipping C headers of dependencies. As far as I can tell with this technique the only drawback is that on first install from a source distribution pip's default wheel building/caching process causes a non-fatal error. This seems better than potentially shipping incompatible headers.,What's the error message?,True,{}
explosion/thinc,https://github.com/explosion/thinc,14,2016-04-16T13:05:16Z,2016-12-23T14:21:19Z,2016-12-23T14:21:19Z,CLOSED,False,16,38,2,https://github.com/henningpeters,new import-include proposal,1,[],https://github.com/explosion/thinc/pull/14,https://github.com/henningpeters,5,https://github.com/explosion/thinc/pull/14#issuecomment-210821242,Please check new idea for getting rid of shipping C headers of dependencies. As far as I can tell with this technique the only drawback is that on first install from a source distribution pip's default wheel building/caching process causes a non-fatal error. This seems better than potentially shipping incompatible headers.,"thinc/linear/features.cpp:249:10: fatal error: 'murmurhash/MurmurHash3.h' file not found
  #include ""murmurhash/MurmurHash3.h""
           ^
  1 error generated.
  error: command 'cc' failed with exit status 1

  ----------------------------------------
  Failed building wheel for thinc
Failed to build thinc
Installing collected packages: murmurhash
Successfully installed murmurhash-0.26.3

You might think everything failed, but only the wheel cache subtask failed. I didn't find a way yet to work around this (suppress error, skip wheel cache or just make it work). The installation succeeded. The error message is only visible when installing from source packages locally, when using older pip versions or when enabling verbose logging mode in newer pip versions.",True,{}
explosion/thinc,https://github.com/explosion/thinc,14,2016-04-16T13:05:16Z,2016-12-23T14:21:19Z,2016-12-23T14:21:19Z,CLOSED,False,16,38,2,https://github.com/henningpeters,new import-include proposal,1,[],https://github.com/explosion/thinc/pull/14,https://github.com/syllog1sm,6,https://github.com/explosion/thinc/pull/14#issuecomment-210825600,Please check new idea for getting rid of shipping C headers of dependencies. As far as I can tell with this technique the only drawback is that on first install from a source distribution pip's default wheel building/caching process causes a non-fatal error. This seems better than potentially shipping incompatible headers.,This seems bad. What's the benefit of the change again? Just the reduced risk of vendorising the wrong headers?,True,{}
explosion/thinc,https://github.com/explosion/thinc,14,2016-04-16T13:05:16Z,2016-12-23T14:21:19Z,2016-12-23T14:21:19Z,CLOSED,False,16,38,2,https://github.com/henningpeters,new import-include proposal,1,[],https://github.com/explosion/thinc/pull/14,https://github.com/henningpeters,7,https://github.com/explosion/thinc/pull/14#issuecomment-210830096,Please check new idea for getting rid of shipping C headers of dependencies. As far as I can tell with this technique the only drawback is that on first install from a source distribution pip's default wheel building/caching process causes a non-fatal error. This seems better than potentially shipping incompatible headers.,"It's conceptionally cleaner (less moving parts) and can prevent problems, e.g., when numpy makes an incompatible change or when there are differences in headers across platforms (there are). Hence, I expect less trouble with it in the future. By vendorizing the headers you blindly trust that the headers we ship are compatible with every numpy version out there.
If we want to make the error message go away/support wheel cache we could retain a fallback with vendorized headers.",True,{}
explosion/thinc,https://github.com/explosion/thinc,14,2016-04-16T13:05:16Z,2016-12-23T14:21:19Z,2016-12-23T14:21:19Z,CLOSED,False,16,38,2,https://github.com/henningpeters,new import-include proposal,1,[],https://github.com/explosion/thinc/pull/14,https://github.com/syllog1sm,8,https://github.com/explosion/thinc/pull/14#issuecomment-210853840,Please check new idea for getting rid of shipping C headers of dependencies. As far as I can tell with this technique the only drawback is that on first install from a source distribution pip's default wheel building/caching process causes a non-fatal error. This seems better than potentially shipping incompatible headers.,"I see the appeal, but 5) is pretty much the basic case, and if I got that error I'd stop what I was doing and assume I had to fix it. This will happen on many spaCy installations.",True,{}
explosion/thinc,https://github.com/explosion/thinc,14,2016-04-16T13:05:16Z,2016-12-23T14:21:19Z,2016-12-23T14:21:19Z,CLOSED,False,16,38,2,https://github.com/henningpeters,new import-include proposal,1,[],https://github.com/explosion/thinc/pull/14,https://github.com/henningpeters,9,https://github.com/explosion/thinc/pull/14#issuecomment-210883660,Please check new idea for getting rid of shipping C headers of dependencies. As far as I can tell with this technique the only drawback is that on first install from a source distribution pip's default wheel building/caching process causes a non-fatal error. This seems better than potentially shipping incompatible headers.,"Agree, I expect that we'll see the default case becoming 3 in the near future though: once we have wheels on Linux. Just keep the PR around until then...",True,{}
explosion/thinc,https://github.com/explosion/thinc,18,2017-01-17T07:28:31Z,2017-01-17T10:36:34Z,2017-01-17T12:28:18Z,MERGED,True,10,4,1,https://github.com/kootenpv,Update examples/spacy_tagger.py,1,[],https://github.com/explosion/thinc/pull/18,https://github.com/kootenpv,1,https://github.com/explosion/thinc/pull/18,"But still does not work, it crashes with:
  File ""/Library/Frameworks/Python.framework/Versions/3.5/lib/python3.5/site-packages/thinc/extra/_vendorized/keras_generic_utils.py"", line 114, in update
    numdigits = int(np.floor(np.log10(self.target))) + 1
ValueError: cannot convert float NaN to integer","But still does not work, it crashes with:
  File ""/Library/Frameworks/Python.framework/Versions/3.5/lib/python3.5/site-packages/thinc/extra/_vendorized/keras_generic_utils.py"", line 114, in update
    numdigits = int(np.floor(np.log10(self.target))) + 1
ValueError: cannot convert float NaN to integer",True,{}
explosion/thinc,https://github.com/explosion/thinc,18,2017-01-17T07:28:31Z,2017-01-17T10:36:34Z,2017-01-17T12:28:18Z,MERGED,True,10,4,1,https://github.com/kootenpv,Update examples/spacy_tagger.py,1,[],https://github.com/explosion/thinc/pull/18,https://github.com/coveralls,2,https://github.com/explosion/thinc/pull/18#issuecomment-273041513,"But still does not work, it crashes with:
  File ""/Library/Frameworks/Python.framework/Versions/3.5/lib/python3.5/site-packages/thinc/extra/_vendorized/keras_generic_utils.py"", line 114, in update
    numdigits = int(np.floor(np.log10(self.target))) + 1
ValueError: cannot convert float NaN to integer",Coverage decreased (-0.2%) to 99.275% when pulling 308fea5 on kootenpv:patch-2 into 6de95ef on explosion:master.,True,{}
explosion/thinc,https://github.com/explosion/thinc,18,2017-01-17T07:28:31Z,2017-01-17T10:36:34Z,2017-01-17T12:28:18Z,MERGED,True,10,4,1,https://github.com/kootenpv,Update examples/spacy_tagger.py,1,[],https://github.com/explosion/thinc/pull/18,https://github.com/honnibal,3,https://github.com/explosion/thinc/pull/18#issuecomment-273098877,"But still does not work, it crashes with:
  File ""/Library/Frameworks/Python.framework/Versions/3.5/lib/python3.5/site-packages/thinc/extra/_vendorized/keras_generic_utils.py"", line 114, in update
    numdigits = int(np.floor(np.log10(self.target))) + 1
ValueError: cannot convert float NaN to integer","Hey,
This is all under very active development at the moment, so there will continue to be these rough spots. Thanks for the fixes. I hope to have the code updated for this soon, to get the example working again.",True,{}
explosion/thinc,https://github.com/explosion/thinc,18,2017-01-17T07:28:31Z,2017-01-17T10:36:34Z,2017-01-17T12:28:18Z,MERGED,True,10,4,1,https://github.com/kootenpv,Update examples/spacy_tagger.py,1,[],https://github.com/explosion/thinc/pull/18,https://github.com/kootenpv,4,https://github.com/explosion/thinc/pull/18#issuecomment-273125703,"But still does not work, it crashes with:
  File ""/Library/Frameworks/Python.framework/Versions/3.5/lib/python3.5/site-packages/thinc/extra/_vendorized/keras_generic_utils.py"", line 114, in update
    numdigits = int(np.floor(np.log10(self.target))) + 1
ValueError: cannot convert float NaN to integer",I'm very curious to the examples :) Naturally I'd like to get a taste of the extra value of using thinc over spacy...,True,{}
explosion/thinc,https://github.com/explosion/thinc,19,2017-02-08T02:07:03Z,2017-02-08T18:19:43Z,2017-02-08T18:19:43Z,CLOSED,False,3,3,1,https://github.com/almostimplemented,Fix off-by-one error in sparse array clone method,1,[],https://github.com/explosion/thinc/pull/19,https://github.com/almostimplemented,1,https://github.com/explosion/thinc/pull/19,Currently the result from clone won't have the -2 terminal key.,Currently the result from clone won't have the -2 terminal key.,True,{}
explosion/thinc,https://github.com/explosion/thinc,19,2017-02-08T02:07:03Z,2017-02-08T18:19:43Z,2017-02-08T18:19:43Z,CLOSED,False,3,3,1,https://github.com/almostimplemented,Fix off-by-one error in sparse array clone method,1,[],https://github.com/explosion/thinc/pull/19,https://github.com/coveralls,2,https://github.com/explosion/thinc/pull/19#issuecomment-278209251,Currently the result from clone won't have the -2 terminal key.,Coverage decreased (-0.1%) to 87.632% when pulling 600546a on almostimplemented:fix/sparse-clone into 95c89b6 on explosion:master.,True,{}
explosion/thinc,https://github.com/explosion/thinc,19,2017-02-08T02:07:03Z,2017-02-08T18:19:43Z,2017-02-08T18:19:43Z,CLOSED,False,3,3,1,https://github.com/almostimplemented,Fix off-by-one error in sparse array clone method,1,[],https://github.com/explosion/thinc/pull/19,https://github.com/almostimplemented,3,https://github.com/explosion/thinc/pull/19#issuecomment-278415709,Currently the result from clone won't have the -2 terminal key.,Fixed in e4ea40b .,True,{}
explosion/thinc,https://github.com/explosion/thinc,20,2017-03-19T03:07:20Z,2017-03-19T10:41:20Z,2017-03-19T10:41:20Z,MERGED,True,1,0,1,https://github.com/rmax,Fix termcolor dependency.,1,[],https://github.com/explosion/thinc/pull/20,https://github.com/rmax,1,https://github.com/explosion/thinc/pull/20,"This bug is hidden because termcolor is listed in requirements.txt, used by travis, but not listed in setup.py. See below the ImportError on a fresh pip install thinc:
$ python

In [4]: import thinc.check
---------------------------------------------------------------------------
ImportError                               Traceback (most recent call last)
<ipython-input-4-e97ab0d72623> in <module>()
----> 1 import thinc.check

/Users/rolando/miniconda3/envs/tmp-test/lib/python3.5/site-packages/thinc/check.py in <module>()
      5 from numpy import ndarray
      6
----> 7 from .exceptions import UndefinedOperatorError, DifferentLengthError
      8 from .exceptions import ExpectedTypeError, ShapeMismatchError
      9 from .exceptions import OutsideRangeError

/Users/rolando/miniconda3/envs/tmp-test/lib/python3.5/site-packages/thinc/exceptions.py in <module>()
      4
      5 import traceback
----> 6 from termcolor import colored as color
      7
      8

ImportError: No module named 'termcolor'","This bug is hidden because termcolor is listed in requirements.txt, used by travis, but not listed in setup.py. See below the ImportError on a fresh pip install thinc:
$ python

In [4]: import thinc.check
---------------------------------------------------------------------------
ImportError                               Traceback (most recent call last)
<ipython-input-4-e97ab0d72623> in <module>()
----> 1 import thinc.check

/Users/rolando/miniconda3/envs/tmp-test/lib/python3.5/site-packages/thinc/check.py in <module>()
      5 from numpy import ndarray
      6
----> 7 from .exceptions import UndefinedOperatorError, DifferentLengthError
      8 from .exceptions import ExpectedTypeError, ShapeMismatchError
      9 from .exceptions import OutsideRangeError

/Users/rolando/miniconda3/envs/tmp-test/lib/python3.5/site-packages/thinc/exceptions.py in <module>()
      4
      5 import traceback
----> 6 from termcolor import colored as color
      7
      8

ImportError: No module named 'termcolor'",True,{}
explosion/thinc,https://github.com/explosion/thinc,20,2017-03-19T03:07:20Z,2017-03-19T10:41:20Z,2017-03-19T10:41:20Z,MERGED,True,1,0,1,https://github.com/rmax,Fix termcolor dependency.,1,[],https://github.com/explosion/thinc/pull/20,https://github.com/coveralls,2,https://github.com/explosion/thinc/pull/20#issuecomment-287591078,"This bug is hidden because termcolor is listed in requirements.txt, used by travis, but not listed in setup.py. See below the ImportError on a fresh pip install thinc:
$ python

In [4]: import thinc.check
---------------------------------------------------------------------------
ImportError                               Traceback (most recent call last)
<ipython-input-4-e97ab0d72623> in <module>()
----> 1 import thinc.check

/Users/rolando/miniconda3/envs/tmp-test/lib/python3.5/site-packages/thinc/check.py in <module>()
      5 from numpy import ndarray
      6
----> 7 from .exceptions import UndefinedOperatorError, DifferentLengthError
      8 from .exceptions import ExpectedTypeError, ShapeMismatchError
      9 from .exceptions import OutsideRangeError

/Users/rolando/miniconda3/envs/tmp-test/lib/python3.5/site-packages/thinc/exceptions.py in <module>()
      4
      5 import traceback
----> 6 from termcolor import colored as color
      7
      8

ImportError: No module named 'termcolor'",Coverage remained the same at 59.285% when pulling 7e0724b on rolando-contrib:termcolor-dep into 11d575e on explosion:master.,True,{}
explosion/thinc,https://github.com/explosion/thinc,23,2017-03-20T10:41:40Z,2017-03-20T10:43:49Z,2017-03-20T15:37:05Z,MERGED,True,1,1,1,https://github.com/ogrisel,typo in README.rst: huidden_width,1,[],https://github.com/explosion/thinc/pull/23,https://github.com/ogrisel,1,https://github.com/explosion/thinc/pull/23,,,True,{}
explosion/thinc,https://github.com/explosion/thinc,23,2017-03-20T10:41:40Z,2017-03-20T10:43:49Z,2017-03-20T15:37:05Z,MERGED,True,1,1,1,https://github.com/ogrisel,typo in README.rst: huidden_width,1,[],https://github.com/explosion/thinc/pull/23,https://github.com/honnibal,2,https://github.com/explosion/thinc/pull/23#issuecomment-287725466,,"Thanks Olivier!
Btw, obviously the library doesn't really have docs atm. If you're curious about it I'd be very happy to answer questions :)",True,{}
explosion/thinc,https://github.com/explosion/thinc,23,2017-03-20T10:41:40Z,2017-03-20T10:43:49Z,2017-03-20T15:37:05Z,MERGED,True,1,1,1,https://github.com/ogrisel,typo in README.rst: huidden_width,1,[],https://github.com/explosion/thinc/pull/23,https://github.com/coveralls,3,https://github.com/explosion/thinc/pull/23#issuecomment-287726079,,Coverage remained the same at 59.285% when pulling eabd753 on ogrisel:patch-1 into dc8c91e on explosion:master.,True,{}
explosion/thinc,https://github.com/explosion/thinc,23,2017-03-20T10:41:40Z,2017-03-20T10:43:49Z,2017-03-20T15:37:05Z,MERGED,True,1,1,1,https://github.com/ogrisel,typo in README.rst: huidden_width,1,[],https://github.com/explosion/thinc/pull/23,https://github.com/coveralls,4,https://github.com/explosion/thinc/pull/23#issuecomment-287726080,,Coverage remained the same at 59.285% when pulling eabd753 on ogrisel:patch-1 into dc8c91e on explosion:master.,True,{}
explosion/thinc,https://github.com/explosion/thinc,23,2017-03-20T10:41:40Z,2017-03-20T10:43:49Z,2017-03-20T15:37:05Z,MERGED,True,1,1,1,https://github.com/ogrisel,typo in README.rst: huidden_width,1,[],https://github.com/explosion/thinc/pull/23,https://github.com/ogrisel,5,https://github.com/explosion/thinc/pull/23#issuecomment-287798147,,I was just having a quick look in passing. I have no particular NLP project at the moment. I like the explicit and local operator overloading pattern ;),True,{}
explosion/thinc,https://github.com/explosion/thinc,24,2017-03-20T23:20:14Z,2017-03-20T23:51:29Z,2017-03-21T02:06:31Z,MERGED,True,10,5,3,https://github.com/rmax,Fix integer check on array shapes.,1,[],https://github.com/explosion/thinc/pull/24,https://github.com/rmax,1,https://github.com/explosion/thinc/pull/24,"This mainly address Python 2.7's Windows interpreter where numpy's
shape have long values and hence isinstance(shape[i], int)
returns False.
--
Related: numpy/numpy#5809","This mainly address Python 2.7's Windows interpreter where numpy's
shape have long values and hence isinstance(shape[i], int)
returns False.
--
Related: numpy/numpy#5809",True,{}
explosion/thinc,https://github.com/explosion/thinc,24,2017-03-20T23:20:14Z,2017-03-20T23:51:29Z,2017-03-21T02:06:31Z,MERGED,True,10,5,3,https://github.com/rmax,Fix integer check on array shapes.,1,[],https://github.com/explosion/thinc/pull/24,https://github.com/coveralls,2,https://github.com/explosion/thinc/pull/24#issuecomment-287929872,"This mainly address Python 2.7's Windows interpreter where numpy's
shape have long values and hence isinstance(shape[i], int)
returns False.
--
Related: numpy/numpy#5809",Coverage increased (+0.03%) to 59.313% when pulling 14e31b7 on rolando-contrib:windows-tests into bae2b3c on explosion:master.,True,{}
explosion/thinc,https://github.com/explosion/thinc,24,2017-03-20T23:20:14Z,2017-03-20T23:51:29Z,2017-03-21T02:06:31Z,MERGED,True,10,5,3,https://github.com/rmax,Fix integer check on array shapes.,1,[],https://github.com/explosion/thinc/pull/24,https://github.com/coveralls,3,https://github.com/explosion/thinc/pull/24#issuecomment-287929873,"This mainly address Python 2.7's Windows interpreter where numpy's
shape have long values and hence isinstance(shape[i], int)
returns False.
--
Related: numpy/numpy#5809",Coverage increased (+0.03%) to 59.313% when pulling 14e31b7 on rolando-contrib:windows-tests into bae2b3c on explosion:master.,True,{}
explosion/thinc,https://github.com/explosion/thinc,24,2017-03-20T23:20:14Z,2017-03-20T23:51:29Z,2017-03-21T02:06:31Z,MERGED,True,10,5,3,https://github.com/rmax,Fix integer check on array shapes.,1,[],https://github.com/explosion/thinc/pull/24,https://github.com/coveralls,4,https://github.com/explosion/thinc/pull/24#issuecomment-287929874,"This mainly address Python 2.7's Windows interpreter where numpy's
shape have long values and hence isinstance(shape[i], int)
returns False.
--
Related: numpy/numpy#5809",Coverage increased (+0.03%) to 59.313% when pulling 14e31b7 on rolando-contrib:windows-tests into bae2b3c on explosion:master.,True,{}
explosion/thinc,https://github.com/explosion/thinc,24,2017-03-20T23:20:14Z,2017-03-20T23:51:29Z,2017-03-21T02:06:31Z,MERGED,True,10,5,3,https://github.com/rmax,Fix integer check on array shapes.,1,[],https://github.com/explosion/thinc/pull/24,https://github.com/coveralls,5,https://github.com/explosion/thinc/pull/24#issuecomment-287930018,"This mainly address Python 2.7's Windows interpreter where numpy's
shape have long values and hence isinstance(shape[i], int)
returns False.
--
Related: numpy/numpy#5809",Coverage increased (+0.03%) to 59.313% when pulling 14e31b7 on rolando-contrib:windows-tests into bae2b3c on explosion:master.,True,{}
explosion/thinc,https://github.com/explosion/thinc,24,2017-03-20T23:20:14Z,2017-03-20T23:51:29Z,2017-03-21T02:06:31Z,MERGED,True,10,5,3,https://github.com/rmax,Fix integer check on array shapes.,1,[],https://github.com/explosion/thinc/pull/24,https://github.com/honnibal,6,https://github.com/explosion/thinc/pull/24#issuecomment-287934064,"This mainly address Python 2.7's Windows interpreter where numpy's
shape have long values and hence isinstance(shape[i], int)
returns False.
--
Related: numpy/numpy#5809","Interesting -- I didn't know that, thanks.",True,{}
explosion/thinc,https://github.com/explosion/thinc,24,2017-03-20T23:20:14Z,2017-03-20T23:51:29Z,2017-03-21T02:06:31Z,MERGED,True,10,5,3,https://github.com/rmax,Fix integer check on array shapes.,1,[],https://github.com/explosion/thinc/pull/24,https://github.com/rmax,7,https://github.com/explosion/thinc/pull/24#issuecomment-287954831,"This mainly address Python 2.7's Windows interpreter where numpy's
shape have long values and hence isinstance(shape[i], int)
returns False.
--
Related: numpy/numpy#5809","@honnibal yeah, kinda backwards. Good thing that long is gone in python 3.x.",True,{}
explosion/thinc,https://github.com/explosion/thinc,25,2017-05-03T13:19:43Z,2017-05-22T09:57:59Z,2017-10-08T17:26:12Z,CLOSED,False,2,2,1,https://github.com/bakarov,Fix bug: gradient array in feed-forward model is not C-Contiguous,1,[],https://github.com/explosion/thinc/pull/25,https://github.com/bakarov,1,https://github.com/explosion/thinc/pull/25,"I tried to execute the script thinc/examples/text-pair/glove_mwe_multipool_predict.py, and I have encountered with an error:
/usr/lib/python3.6/site-packages/thinc/neural/_classes/maxout.py in finish_update(dX__bo, sgd)
     54 
     55         def finish_update(dX__bo, sgd=None):
---> 56             dX__bop = self.ops.backprop_maxout(dX__bo, which__bo, self.nP)
     57             self.d_b += dX__bop.sum(axis=0)

/usr/lib/python3.6/site-packages/thinc/neural/ops.pyx in thinc.neural.ops.NumpyOps.backprop_maxout (thinc/neural/ops.cpp:12005)()

/usr/lib/python3.6/site-packages/thinc/neural/ops.cpython-36m-x86_64-linux-gnu.so in View.MemoryView.memoryview_cwrapper (thinc/neural/ops.cpp:30499)()

/usr/lib/python3.6/site-packages/thinc/neural/ops.cpython-36m-x86_64-linux-gnu.so in View.MemoryView.memoryview.__cinit__ (thinc/neural/ops.cpp:26751)()

ValueError: ndarray is not C-contiguous

I have fixed this error by making the array given to finish_update C-contiguous in the callback function, and it worked for me. Perhaps some folks may also face this problem.","I tried to execute the script thinc/examples/text-pair/glove_mwe_multipool_predict.py, and I have encountered with an error:
/usr/lib/python3.6/site-packages/thinc/neural/_classes/maxout.py in finish_update(dX__bo, sgd)
     54 
     55         def finish_update(dX__bo, sgd=None):
---> 56             dX__bop = self.ops.backprop_maxout(dX__bo, which__bo, self.nP)
     57             self.d_b += dX__bop.sum(axis=0)

/usr/lib/python3.6/site-packages/thinc/neural/ops.pyx in thinc.neural.ops.NumpyOps.backprop_maxout (thinc/neural/ops.cpp:12005)()

/usr/lib/python3.6/site-packages/thinc/neural/ops.cpython-36m-x86_64-linux-gnu.so in View.MemoryView.memoryview_cwrapper (thinc/neural/ops.cpp:30499)()

/usr/lib/python3.6/site-packages/thinc/neural/ops.cpython-36m-x86_64-linux-gnu.so in View.MemoryView.memoryview.__cinit__ (thinc/neural/ops.cpp:26751)()

ValueError: ndarray is not C-contiguous

I have fixed this error by making the array given to finish_update C-contiguous in the callback function, and it worked for me. Perhaps some folks may also face this problem.",True,{}
explosion/thinc,https://github.com/explosion/thinc,25,2017-05-03T13:19:43Z,2017-05-22T09:57:59Z,2017-10-08T17:26:12Z,CLOSED,False,2,2,1,https://github.com/bakarov,Fix bug: gradient array in feed-forward model is not C-Contiguous,1,[],https://github.com/explosion/thinc/pull/25,https://github.com/coveralls,2,https://github.com/explosion/thinc/pull/25#issuecomment-298909839,"I tried to execute the script thinc/examples/text-pair/glove_mwe_multipool_predict.py, and I have encountered with an error:
/usr/lib/python3.6/site-packages/thinc/neural/_classes/maxout.py in finish_update(dX__bo, sgd)
     54 
     55         def finish_update(dX__bo, sgd=None):
---> 56             dX__bop = self.ops.backprop_maxout(dX__bo, which__bo, self.nP)
     57             self.d_b += dX__bop.sum(axis=0)

/usr/lib/python3.6/site-packages/thinc/neural/ops.pyx in thinc.neural.ops.NumpyOps.backprop_maxout (thinc/neural/ops.cpp:12005)()

/usr/lib/python3.6/site-packages/thinc/neural/ops.cpython-36m-x86_64-linux-gnu.so in View.MemoryView.memoryview_cwrapper (thinc/neural/ops.cpp:30499)()

/usr/lib/python3.6/site-packages/thinc/neural/ops.cpython-36m-x86_64-linux-gnu.so in View.MemoryView.memoryview.__cinit__ (thinc/neural/ops.cpp:26751)()

ValueError: ndarray is not C-contiguous

I have fixed this error by making the array given to finish_update C-contiguous in the callback function, and it worked for me. Perhaps some folks may also face this problem.",Coverage increased (+0.03%) to 58.693% when pulling 8a886b9 on bakarov:patch-1 into 8d7a142 on explosion:master.,True,{}
explosion/thinc,https://github.com/explosion/thinc,25,2017-05-03T13:19:43Z,2017-05-22T09:57:59Z,2017-10-08T17:26:12Z,CLOSED,False,2,2,1,https://github.com/bakarov,Fix bug: gradient array in feed-forward model is not C-Contiguous,1,[],https://github.com/explosion/thinc/pull/25,https://github.com/honnibal,3,https://github.com/explosion/thinc/pull/25#issuecomment-298916367,"I tried to execute the script thinc/examples/text-pair/glove_mwe_multipool_predict.py, and I have encountered with an error:
/usr/lib/python3.6/site-packages/thinc/neural/_classes/maxout.py in finish_update(dX__bo, sgd)
     54 
     55         def finish_update(dX__bo, sgd=None):
---> 56             dX__bop = self.ops.backprop_maxout(dX__bo, which__bo, self.nP)
     57             self.d_b += dX__bop.sum(axis=0)

/usr/lib/python3.6/site-packages/thinc/neural/ops.pyx in thinc.neural.ops.NumpyOps.backprop_maxout (thinc/neural/ops.cpp:12005)()

/usr/lib/python3.6/site-packages/thinc/neural/ops.cpython-36m-x86_64-linux-gnu.so in View.MemoryView.memoryview_cwrapper (thinc/neural/ops.cpp:30499)()

/usr/lib/python3.6/site-packages/thinc/neural/ops.cpython-36m-x86_64-linux-gnu.so in View.MemoryView.memoryview.__cinit__ (thinc/neural/ops.cpp:26751)()

ValueError: ndarray is not C-contiguous

I have fixed this error by making the array given to finish_update C-contiguous in the callback function, and it worked for me. Perhaps some folks may also face this problem.","Hey,
This stuff is all pretty messy at the moment -- there's some updated state in develop, that will allow the similar script glove_mwe_multipool_siamese on the GPU. Things may break unexpectedly.
In general to fix stuff like the error you experienced, we want to be fixing the data when it's output from the layer --- so we wouldn't want to change the hook like this. Also, we want to be using the self.ops.xp to access numpy or cupy, so that the code works the same on both CPU and GPU.
Thanks!
Matt",True,{}
explosion/thinc,https://github.com/explosion/thinc,25,2017-05-03T13:19:43Z,2017-05-22T09:57:59Z,2017-10-08T17:26:12Z,CLOSED,False,2,2,1,https://github.com/bakarov,Fix bug: gradient array in feed-forward model is not C-Contiguous,1,[],https://github.com/explosion/thinc/pull/25,https://github.com/honnibal,4,https://github.com/explosion/thinc/pull/25#issuecomment-303055260,"I tried to execute the script thinc/examples/text-pair/glove_mwe_multipool_predict.py, and I have encountered with an error:
/usr/lib/python3.6/site-packages/thinc/neural/_classes/maxout.py in finish_update(dX__bo, sgd)
     54 
     55         def finish_update(dX__bo, sgd=None):
---> 56             dX__bop = self.ops.backprop_maxout(dX__bo, which__bo, self.nP)
     57             self.d_b += dX__bop.sum(axis=0)

/usr/lib/python3.6/site-packages/thinc/neural/ops.pyx in thinc.neural.ops.NumpyOps.backprop_maxout (thinc/neural/ops.cpp:12005)()

/usr/lib/python3.6/site-packages/thinc/neural/ops.cpython-36m-x86_64-linux-gnu.so in View.MemoryView.memoryview_cwrapper (thinc/neural/ops.cpp:30499)()

/usr/lib/python3.6/site-packages/thinc/neural/ops.cpython-36m-x86_64-linux-gnu.so in View.MemoryView.memoryview.__cinit__ (thinc/neural/ops.cpp:26751)()

ValueError: ndarray is not C-contiguous

I have fixed this error by making the array given to finish_update C-contiguous in the callback function, and it worked for me. Perhaps some folks may also face this problem.",Closing this -- it should be fixed now. Thanks again for taking the time.,True,{}
explosion/thinc,https://github.com/explosion/thinc,25,2017-05-03T13:19:43Z,2017-05-22T09:57:59Z,2017-10-08T17:26:12Z,CLOSED,False,2,2,1,https://github.com/bakarov,Fix bug: gradient array in feed-forward model is not C-Contiguous,1,[],https://github.com/explosion/thinc/pull/25,https://github.com/msamogh,5,https://github.com/explosion/thinc/pull/25#issuecomment-335022729,"I tried to execute the script thinc/examples/text-pair/glove_mwe_multipool_predict.py, and I have encountered with an error:
/usr/lib/python3.6/site-packages/thinc/neural/_classes/maxout.py in finish_update(dX__bo, sgd)
     54 
     55         def finish_update(dX__bo, sgd=None):
---> 56             dX__bop = self.ops.backprop_maxout(dX__bo, which__bo, self.nP)
     57             self.d_b += dX__bop.sum(axis=0)

/usr/lib/python3.6/site-packages/thinc/neural/ops.pyx in thinc.neural.ops.NumpyOps.backprop_maxout (thinc/neural/ops.cpp:12005)()

/usr/lib/python3.6/site-packages/thinc/neural/ops.cpython-36m-x86_64-linux-gnu.so in View.MemoryView.memoryview_cwrapper (thinc/neural/ops.cpp:30499)()

/usr/lib/python3.6/site-packages/thinc/neural/ops.cpython-36m-x86_64-linux-gnu.so in View.MemoryView.memoryview.__cinit__ (thinc/neural/ops.cpp:26751)()

ValueError: ndarray is not C-contiguous

I have fixed this error by making the array given to finish_update C-contiguous in the callback function, and it worked for me. Perhaps some folks may also face this problem.","@bakarov How did you manage to recompile thinc and not break anything? I got a ValueError: thinc.extra.search.MaxViolation has the wrong size, try recompiling while trying to do that.
Also, why doesn't adding ascontiguousarray in my application code fix it?",True,{}
explosion/thinc,https://github.com/explosion/thinc,28,2017-06-02T22:37:48Z,2017-06-02T23:13:34Z,2017-06-02T23:13:34Z,MERGED,True,1,1,1,https://github.com/nirum,Fixes typo in README?,1,[],https://github.com/explosion/thinc/pull/28,https://github.com/nirum,1,https://github.com/explosion/thinc/pull/28,at least I think it's a typo,at least I think it's a typo,True,{}
explosion/thinc,https://github.com/explosion/thinc,28,2017-06-02T22:37:48Z,2017-06-02T23:13:34Z,2017-06-02T23:13:34Z,MERGED,True,1,1,1,https://github.com/nirum,Fixes typo in README?,1,[],https://github.com/explosion/thinc/pull/28,https://github.com/coveralls,2,https://github.com/explosion/thinc/pull/28#issuecomment-305925483,at least I think it's a typo,Coverage remained the same at 58.14% when pulling 803bb78 on nirum:patch-1 into dfaac12 on explosion:master.,True,{}
explosion/thinc,https://github.com/explosion/thinc,28,2017-06-02T22:37:48Z,2017-06-02T23:13:34Z,2017-06-02T23:13:34Z,MERGED,True,1,1,1,https://github.com/nirum,Fixes typo in README?,1,[],https://github.com/explosion/thinc/pull/28,https://github.com/honnibal,3,https://github.com/explosion/thinc/pull/28#issuecomment-305929362,at least I think it's a typo,Thanks!,True,{}
explosion/thinc,https://github.com/explosion/thinc,29,2017-06-06T13:08:50Z,2017-06-06T13:15:06Z,2017-06-06T13:16:06Z,MERGED,True,9,9,1,https://github.com/tammoippen,Make the README example executable,1,[],https://github.com/explosion/thinc/pull/29,https://github.com/tammoippen,1,https://github.com/explosion/thinc/pull/29,"The function backprop_relu needs the optimizer parameter, otherwise the chain function does not work.
Parameter b needs to be a 'matrix', so that you can do Y = W @ X + b
The outer function (considering it is the numpy version) only works, if X and dY consist of one training example. With more, i.e. batch learning, einsum does column-wise outer product and the sum over these matrices.
The error for the next step is dX in the backward function from create_linear_layer.
Finally, some dimensions are wrong and optimizer parameters are fixed

Now you can copy-paste the code examples into a notebook, add a optimizer function, e.g. the sgd from explicit_sgd_update and run the example.","The function backprop_relu needs the optimizer parameter, otherwise the chain function does not work.
Parameter b needs to be a 'matrix', so that you can do Y = W @ X + b
The outer function (considering it is the numpy version) only works, if X and dY consist of one training example. With more, i.e. batch learning, einsum does column-wise outer product and the sum over these matrices.
The error for the next step is dX in the backward function from create_linear_layer.
Finally, some dimensions are wrong and optimizer parameters are fixed

Now you can copy-paste the code examples into a notebook, add a optimizer function, e.g. the sgd from explicit_sgd_update and run the example.",True,{}
explosion/thinc,https://github.com/explosion/thinc,29,2017-06-06T13:08:50Z,2017-06-06T13:15:06Z,2017-06-06T13:16:06Z,MERGED,True,9,9,1,https://github.com/tammoippen,Make the README example executable,1,[],https://github.com/explosion/thinc/pull/29,https://github.com/honnibal,2,https://github.com/explosion/thinc/pull/29#issuecomment-306482366,"The function backprop_relu needs the optimizer parameter, otherwise the chain function does not work.
Parameter b needs to be a 'matrix', so that you can do Y = W @ X + b
The outer function (considering it is the numpy version) only works, if X and dY consist of one training example. With more, i.e. batch learning, einsum does column-wise outer product and the sum over these matrices.
The error for the next step is dX in the backward function from create_linear_layer.
Finally, some dimensions are wrong and optimizer parameters are fixed

Now you can copy-paste the code examples into a notebook, add a optimizer function, e.g. the sgd from explicit_sgd_update and run the example.",Thanks!,True,{}
explosion/thinc,https://github.com/explosion/thinc,29,2017-06-06T13:08:50Z,2017-06-06T13:15:06Z,2017-06-06T13:16:06Z,MERGED,True,9,9,1,https://github.com/tammoippen,Make the README example executable,1,[],https://github.com/explosion/thinc/pull/29,https://github.com/coveralls,3,https://github.com/explosion/thinc/pull/29#issuecomment-306482547,"The function backprop_relu needs the optimizer parameter, otherwise the chain function does not work.
Parameter b needs to be a 'matrix', so that you can do Y = W @ X + b
The outer function (considering it is the numpy version) only works, if X and dY consist of one training example. With more, i.e. batch learning, einsum does column-wise outer product and the sum over these matrices.
The error for the next step is dX in the backward function from create_linear_layer.
Finally, some dimensions are wrong and optimizer parameters are fixed

Now you can copy-paste the code examples into a notebook, add a optimizer function, e.g. the sgd from explicit_sgd_update and run the example.",Coverage decreased (-0.2%) to 58.03% when pulling b7a0228 on tammoippen:readme-fixes into 4ffe358 on explosion:master.,True,{}
explosion/thinc,https://github.com/explosion/thinc,40,2017-10-28T15:10:22Z,2017-10-28T15:55:06Z,2017-10-28T15:55:06Z,CLOSED,False,1,1,1,https://github.com/RaananHadar,Update setup.py,1,[],https://github.com/explosion/thinc/pull/40,https://github.com/RaananHadar,1,https://github.com/explosion/thinc/pull/40,'-arch=sm_20' is no longer a supported architecture in cuda 9.0,'-arch=sm_20' is no longer a supported architecture in cuda 9.0,True,{}
explosion/thinc,https://github.com/explosion/thinc,40,2017-10-28T15:10:22Z,2017-10-28T15:55:06Z,2017-10-28T15:55:06Z,CLOSED,False,1,1,1,https://github.com/RaananHadar,Update setup.py,1,[],https://github.com/explosion/thinc/pull/40,https://github.com/honnibal,2,https://github.com/explosion/thinc/pull/40#issuecomment-340197965,'-arch=sm_20' is no longer a supported architecture in cuda 9.0,Okay but does this break CUDA 8.0?,True,{}
explosion/thinc,https://github.com/explosion/thinc,40,2017-10-28T15:10:22Z,2017-10-28T15:55:06Z,2017-10-28T15:55:06Z,CLOSED,False,1,1,1,https://github.com/RaananHadar,Update setup.py,1,[],https://github.com/explosion/thinc/pull/40,https://github.com/coveralls,3,https://github.com/explosion/thinc/pull/40#issuecomment-340198182,'-arch=sm_20' is no longer a supported architecture in cuda 9.0,Coverage remained the same at 47.731% when pulling 48634fd on RaananHadar:patch-1 into 878aa7b on explosion:master.,True,{}
explosion/thinc,https://github.com/explosion/thinc,40,2017-10-28T15:10:22Z,2017-10-28T15:55:06Z,2017-10-28T15:55:06Z,CLOSED,False,1,1,1,https://github.com/RaananHadar,Update setup.py,1,[],https://github.com/explosion/thinc/pull/40,https://github.com/RaananHadar,4,https://github.com/explosion/thinc/pull/40#issuecomment-340200050,'-arch=sm_20' is no longer a supported architecture in cuda 9.0,"You are correct. Now that i've seen your code, i've created a proper pull request that is less of a hack :) Also corrected a small bug on the way.",True,{}
explosion/thinc,https://github.com/explosion/thinc,41,2017-10-28T15:41:07Z,2017-10-28T16:18:34Z,2017-10-28T16:18:34Z,MERGED,True,1,1,1,https://github.com/RaananHadar,Update setup.py to work with CUDA 9.0,1,[],https://github.com/explosion/thinc/pull/41,https://github.com/RaananHadar,1,https://github.com/explosion/thinc/pull/41,"Setup.py didn't run on cuda 9, fixed the proper compile options. This fix should also work on CUDA 8","Setup.py didn't run on cuda 9, fixed the proper compile options. This fix should also work on CUDA 8",True,{}
explosion/thinc,https://github.com/explosion/thinc,41,2017-10-28T15:41:07Z,2017-10-28T16:18:34Z,2017-10-28T16:18:34Z,MERGED,True,1,1,1,https://github.com/RaananHadar,Update setup.py to work with CUDA 9.0,1,[],https://github.com/explosion/thinc/pull/41,https://github.com/coveralls,2,https://github.com/explosion/thinc/pull/41#issuecomment-340200241,"Setup.py didn't run on cuda 9, fixed the proper compile options. This fix should also work on CUDA 8",Coverage remained the same at 47.731% when pulling 603b26c on RaananHadar:patch-2 into 878aa7b on explosion:master.,True,{}
explosion/thinc,https://github.com/explosion/thinc,41,2017-10-28T15:41:07Z,2017-10-28T16:18:34Z,2017-10-28T16:18:34Z,MERGED,True,1,1,1,https://github.com/RaananHadar,Update setup.py to work with CUDA 9.0,1,[],https://github.com/explosion/thinc/pull/41,https://github.com/honnibal,3,https://github.com/explosion/thinc/pull/41#issuecomment-340202357,"Setup.py didn't run on cuda 9, fixed the proper compile options. This fix should also work on CUDA 8","Great, thanks!",True,{}
explosion/thinc,https://github.com/explosion/thinc,43,2017-11-10T13:53:41Z,2017-11-23T12:27:20Z,2017-11-23T12:27:20Z,CLOSED,False,1,1,1,https://github.com/christophfink,changed nvcc -arch parameter,1,[],https://github.com/explosion/thinc/pull/43,https://github.com/christophfink,1,https://github.com/explosion/thinc/pull/43,Addressing issue #42,Addressing issue #42,True,{}
explosion/thinc,https://github.com/explosion/thinc,43,2017-11-10T13:53:41Z,2017-11-23T12:27:20Z,2017-11-23T12:27:20Z,CLOSED,False,1,1,1,https://github.com/christophfink,changed nvcc -arch parameter,1,[],https://github.com/explosion/thinc/pull/43,https://github.com/coveralls,2,https://github.com/explosion/thinc/pull/43#issuecomment-343480752,Addressing issue #42,Coverage remained the same at 46.903% when pulling c597e5b on christophfink:master into 67d9e6f on explosion:master.,True,{}
explosion/thinc,https://github.com/explosion/thinc,43,2017-11-10T13:53:41Z,2017-11-23T12:27:20Z,2017-11-23T12:27:20Z,CLOSED,False,1,1,1,https://github.com/christophfink,changed nvcc -arch parameter,1,[],https://github.com/explosion/thinc/pull/43,https://github.com/honnibal,3,https://github.com/explosion/thinc/pull/43#issuecomment-346605595,Addressing issue #42,"Fix now integrated, thanks.",True,{}
explosion/thinc,https://github.com/explosion/thinc,44,2017-11-16T20:49:07Z,2018-03-27T19:32:24Z,2018-03-27T19:32:24Z,CLOSED,False,1763,183,41,https://github.com/honnibal,"Add ""optimized"" maxout window encoding class",94,[],https://github.com/explosion/thinc/pull/44,https://github.com/honnibal,1,https://github.com/explosion/thinc/pull/44,"Integrated class for integrated Residual(CNN >> LayerNorm(Maxout))) block. May or may not be faster, depending on how our linalg.pxd performs.","Integrated class for integrated Residual(CNN >> LayerNorm(Maxout))) block. May or may not be faster, depending on how our linalg.pxd performs.",True,{}
explosion/thinc,https://github.com/explosion/thinc,44,2017-11-16T20:49:07Z,2018-03-27T19:32:24Z,2018-03-27T19:32:24Z,CLOSED,False,1763,183,41,https://github.com/honnibal,"Add ""optimized"" maxout window encoding class",94,[],https://github.com/explosion/thinc/pull/44,https://github.com/coveralls,2,https://github.com/explosion/thinc/pull/44#issuecomment-345338286,"Integrated class for integrated Residual(CNN >> LayerNorm(Maxout))) block. May or may not be faster, depending on how our linalg.pxd performs.",Coverage decreased (-0.08%) to 46.821% when pulling 5b1a1a9 on feature/fast-mwe into 291cd10 on master.,True,{}
explosion/thinc,https://github.com/explosion/thinc,54,2018-03-13T03:22:32Z,2018-03-13T09:08:31Z,2020-11-19T16:20:17Z,MERGED,True,1128,248,22,https://github.com/honnibal,Add option to statically link openblas at compile time,21,[],https://github.com/explosion/thinc/pull/54,https://github.com/honnibal,1,https://github.com/explosion/thinc/pull/54,"See explosion/spaCy#1508 for motivation.
Example usage:
THINC_CBLAS=/home/matt/repos/OpenBLAS/libopenblas.a python setup.py build_ext --inplace

This should allow us to have a statically linked single threaded BLAS. This allows us to happily compute on one core, and allow the caller to worry about parallisation.","See explosion/spaCy#1508 for motivation.
Example usage:
THINC_CBLAS=/home/matt/repos/OpenBLAS/libopenblas.a python setup.py build_ext --inplace

This should allow us to have a statically linked single threaded BLAS. This allows us to happily compute on one core, and allow the caller to worry about parallisation.",True,{}
explosion/thinc,https://github.com/explosion/thinc,54,2018-03-13T03:22:32Z,2018-03-13T09:08:31Z,2020-11-19T16:20:17Z,MERGED,True,1128,248,22,https://github.com/honnibal,Add option to statically link openblas at compile time,21,[],https://github.com/explosion/thinc/pull/54,https://github.com/coveralls,2,https://github.com/explosion/thinc/pull/54#issuecomment-372539529,"See explosion/spaCy#1508 for motivation.
Example usage:
THINC_CBLAS=/home/matt/repos/OpenBLAS/libopenblas.a python setup.py build_ext --inplace

This should allow us to have a statically linked single threaded BLAS. This allows us to happily compute on one core, and allow the caller to worry about parallisation.",Coverage increased (+0.04%) to 46.711% when pulling 3581e2e on feature/openblas into 90bcb21 on master.,True,{}
explosion/thinc,https://github.com/explosion/thinc,54,2018-03-13T03:22:32Z,2018-03-13T09:08:31Z,2020-11-19T16:20:17Z,MERGED,True,1128,248,22,https://github.com/honnibal,Add option to statically link openblas at compile time,21,[],https://github.com/explosion/thinc/pull/54,https://github.com/coveralls,3,https://github.com/explosion/thinc/pull/54#issuecomment-372539530,"See explosion/spaCy#1508 for motivation.
Example usage:
THINC_CBLAS=/home/matt/repos/OpenBLAS/libopenblas.a python setup.py build_ext --inplace

This should allow us to have a statically linked single threaded BLAS. This allows us to happily compute on one core, and allow the caller to worry about parallisation.",Coverage increased (+0.04%) to 46.711% when pulling 63f9a74 on feature/openblas into 90bcb21 on master.,True,{}
explosion/thinc,https://github.com/explosion/thinc,54,2018-03-13T03:22:32Z,2018-03-13T09:08:31Z,2020-11-19T16:20:17Z,MERGED,True,1128,248,22,https://github.com/honnibal,Add option to statically link openblas at compile time,21,[],https://github.com/explosion/thinc/pull/54,https://github.com/coveralls,4,https://github.com/explosion/thinc/pull/54#issuecomment-372539531,"See explosion/spaCy#1508 for motivation.
Example usage:
THINC_CBLAS=/home/matt/repos/OpenBLAS/libopenblas.a python setup.py build_ext --inplace

This should allow us to have a statically linked single threaded BLAS. This allows us to happily compute on one core, and allow the caller to worry about parallisation.",Coverage increased (+0.04%) to 46.711% when pulling 63f9a74 on feature/openblas into 90bcb21 on master.,True,{}
explosion/thinc,https://github.com/explosion/thinc,54,2018-03-13T03:22:32Z,2018-03-13T09:08:31Z,2020-11-19T16:20:17Z,MERGED,True,1128,248,22,https://github.com/honnibal,Add option to statically link openblas at compile time,21,[],https://github.com/explosion/thinc/pull/54,https://github.com/coveralls,5,https://github.com/explosion/thinc/pull/54#issuecomment-372539532,"See explosion/spaCy#1508 for motivation.
Example usage:
THINC_CBLAS=/home/matt/repos/OpenBLAS/libopenblas.a python setup.py build_ext --inplace

This should allow us to have a statically linked single threaded BLAS. This allows us to happily compute on one core, and allow the caller to worry about parallisation.",Coverage increased (+0.04%) to 46.711% when pulling 63f9a74 on feature/openblas into 90bcb21 on master.,True,{}
explosion/thinc,https://github.com/explosion/thinc,55,2018-03-16T13:19:27Z,2018-03-20T10:26:59Z,2018-03-20T10:26:59Z,MERGED,True,3,8,1,https://github.com/darkdreamingdan,Fixed basic_tagger example,1,[],https://github.com/explosion/thinc/pull/55,https://github.com/darkdreamingdan,1,https://github.com/explosion/thinc/pull/55,"The basic_tagger example doesn't seem to run out of the box.  It looks like a typo in the Import.  I've resolved this here and made a few other tidy ups.
basic_tagger is now running on my system.","The basic_tagger example doesn't seem to run out of the box.  It looks like a typo in the Import.  I've resolved this here and made a few other tidy ups.
basic_tagger is now running on my system.",True,{}
explosion/thinc,https://github.com/explosion/thinc,55,2018-03-16T13:19:27Z,2018-03-20T10:26:59Z,2018-03-20T10:26:59Z,MERGED,True,3,8,1,https://github.com/darkdreamingdan,Fixed basic_tagger example,1,[],https://github.com/explosion/thinc/pull/55,https://github.com/coveralls,2,https://github.com/explosion/thinc/pull/55#issuecomment-373711508,"The basic_tagger example doesn't seem to run out of the box.  It looks like a typo in the Import.  I've resolved this here and made a few other tidy ups.
basic_tagger is now running on my system.",Coverage remained the same at 46.889% when pulling 9dbec73 on eigentechnologies:bug/basic-tagger-import into 5891c40 on explosion:master.,True,{}
explosion/thinc,https://github.com/explosion/thinc,55,2018-03-16T13:19:27Z,2018-03-20T10:26:59Z,2018-03-20T10:26:59Z,MERGED,True,3,8,1,https://github.com/darkdreamingdan,Fixed basic_tagger example,1,[],https://github.com/explosion/thinc/pull/55,https://github.com/honnibal,3,https://github.com/explosion/thinc/pull/55#issuecomment-374547280,"The basic_tagger example doesn't seem to run out of the box.  It looks like a typo in the Import.  I've resolved this here and made a few other tidy ups.
basic_tagger is now running on my system.",Thanks! I'm sorry the examples are such a mess. Appreciate the help.,True,{}
explosion/thinc,https://github.com/explosion/thinc,56,2018-03-19T17:45:52Z,2018-03-20T10:27:33Z,2018-03-20T11:32:19Z,MERGED,True,1,1,1,https://github.com/alephmelo,Add missing space in statement.,1,[],https://github.com/explosion/thinc/pull/56,https://github.com/alephmelo,1,https://github.com/explosion/thinc/pull/56,"The lack of space was triggering my OCD (and many others I suppose). 
Before:

After:

I know it's a small thing, but anyway, here it is. :)","The lack of space was triggering my OCD (and many others I suppose). 
Before:

After:

I know it's a small thing, but anyway, here it is. :)",True,{}
explosion/thinc,https://github.com/explosion/thinc,56,2018-03-19T17:45:52Z,2018-03-20T10:27:33Z,2018-03-20T11:32:19Z,MERGED,True,1,1,1,https://github.com/alephmelo,Add missing space in statement.,1,[],https://github.com/explosion/thinc/pull/56,https://github.com/coveralls,2,https://github.com/explosion/thinc/pull/56#issuecomment-374304615,"The lack of space was triggering my OCD (and many others I suppose). 
Before:

After:

I know it's a small thing, but anyway, here it is. :)",Coverage remained the same at 46.87% when pulling b88ccc0 on alephmelo:alephmelo-patch-1 into 5c4340e on explosion:master.,True,{}
explosion/thinc,https://github.com/explosion/thinc,56,2018-03-19T17:45:52Z,2018-03-20T10:27:33Z,2018-03-20T11:32:19Z,MERGED,True,1,1,1,https://github.com/alephmelo,Add missing space in statement.,1,[],https://github.com/explosion/thinc/pull/56,https://github.com/honnibal,3,https://github.com/explosion/thinc/pull/56#issuecomment-374547415,"The lack of space was triggering my OCD (and many others I suppose). 
Before:

After:

I know it's a small thing, but anyway, here it is. :)","Thanks!
Btw while you're looking at things: The data downloads occasionally fail and I don't have an error on that. I've gotten used to the missing stair, but it'd be good to have it fixed.",True,{'THUMBS_UP': ['https://github.com/alephmelo']}
explosion/thinc,https://github.com/explosion/thinc,56,2018-03-19T17:45:52Z,2018-03-20T10:27:33Z,2018-03-20T11:32:19Z,MERGED,True,1,1,1,https://github.com/alephmelo,Add missing space in statement.,1,[],https://github.com/explosion/thinc/pull/56,https://github.com/alephmelo,4,https://github.com/explosion/thinc/pull/56#issuecomment-374565108,"The lack of space was triggering my OCD (and many others I suppose). 
Before:

After:

I know it's a small thing, but anyway, here it is. :)",Great! I'm gonna take a look on this later on today. o/,True,{}
explosion/thinc,https://github.com/explosion/thinc,57,2018-03-20T02:02:37Z,2018-03-20T11:03:29Z,2020-11-19T16:20:24Z,MERGED,True,1637414,150,1801,https://github.com/honnibal,"Ship our own copy of select openblas functions, to remove system dependency",8,[],https://github.com/explosion/thinc/pull/57,https://github.com/honnibal,1,https://github.com/explosion/thinc/pull/57,"The previous solution of having an install-time switch to control the BLAS linkge didn't work, as the switch needs to be flipped at cythonize time. Instead we switch to a higher effort but better solution: build the bits of OpenBLAS we need with distutils, and incorporate it into the library.
This should allow us to remove dependencies on the system state, and avoid conflicting with the user's other openblas settings that control threads etc. We also get nogil, no overhead calls.","The previous solution of having an install-time switch to control the BLAS linkge didn't work, as the switch needs to be flipped at cythonize time. Instead we switch to a higher effort but better solution: build the bits of OpenBLAS we need with distutils, and incorporate it into the library.
This should allow us to remove dependencies on the system state, and avoid conflicting with the user's other openblas settings that control threads etc. We also get nogil, no overhead calls.",True,{}
explosion/thinc,https://github.com/explosion/thinc,57,2018-03-20T02:02:37Z,2018-03-20T11:03:29Z,2020-11-19T16:20:24Z,MERGED,True,1637414,150,1801,https://github.com/honnibal,"Ship our own copy of select openblas functions, to remove system dependency",8,[],https://github.com/explosion/thinc/pull/57,https://github.com/coveralls,2,https://github.com/explosion/thinc/pull/57#issuecomment-374447711,"The previous solution of having an install-time switch to control the BLAS linkge didn't work, as the switch needs to be flipped at cythonize time. Instead we switch to a higher effort but better solution: build the bits of OpenBLAS we need with distutils, and incorporate it into the library.
This should allow us to remove dependencies on the system state, and avoid conflicting with the user's other openblas settings that control threads etc. We also get nogil, no overhead calls.",Coverage remained the same at 46.87% when pulling 50c5232 on feature/vendorize-openblas into 5c4340e on master.,True,{}
explosion/thinc,https://github.com/explosion/thinc,57,2018-03-20T02:02:37Z,2018-03-20T11:03:29Z,2020-11-19T16:20:24Z,MERGED,True,1637414,150,1801,https://github.com/honnibal,"Ship our own copy of select openblas functions, to remove system dependency",8,[],https://github.com/explosion/thinc/pull/57,https://github.com/honnibal,3,https://github.com/explosion/thinc/pull/57#issuecomment-374556910,"The previous solution of having an install-time switch to control the BLAS linkge didn't work, as the switch needs to be flipped at cythonize time. Instead we switch to a higher effort but better solution: build the bits of OpenBLAS we need with distutils, and incorporate it into the library.
This should allow us to remove dependencies on the system state, and avoid conflicting with the user's other openblas settings that control threads etc. We also get nogil, no overhead calls.","This is still pretty flakey, but current state of master is basically broken and this is definitely the future, so merging.",True,{}
explosion/thinc,https://github.com/explosion/thinc,60,2018-03-27T12:42:23Z,2018-03-27T12:52:52Z,2018-03-27T12:52:53Z,MERGED,True,2,2,1,https://github.com/dvsrepo,fix: Setup problem with misppeling open blas arguments,1,['bug'],https://github.com/explosion/thinc/pull/60,https://github.com/dvsrepo,1,https://github.com/explosion/thinc/pull/60,Fixes a small issue when building from source,Fixes a small issue when building from source,True,{}
explosion/thinc,https://github.com/explosion/thinc,60,2018-03-27T12:42:23Z,2018-03-27T12:52:52Z,2018-03-27T12:52:53Z,MERGED,True,2,2,1,https://github.com/dvsrepo,fix: Setup problem with misppeling open blas arguments,1,['bug'],https://github.com/explosion/thinc/pull/60,https://github.com/coveralls,2,https://github.com/explosion/thinc/pull/60#issuecomment-376512126,Fixes a small issue when building from source,Coverage remained the same at 51.626% when pulling ee36229 on recognai:fix/setup into 18664ed on explosion:master.,True,{}
explosion/thinc,https://github.com/explosion/thinc,60,2018-03-27T12:42:23Z,2018-03-27T12:52:52Z,2018-03-27T12:52:53Z,MERGED,True,2,2,1,https://github.com/dvsrepo,fix: Setup problem with misppeling open blas arguments,1,['bug'],https://github.com/explosion/thinc/pull/60,https://github.com/ines,3,https://github.com/explosion/thinc/pull/60#issuecomment-376513451,Fixes a small issue when building from source,Thanks! ,True,{}
explosion/thinc,https://github.com/explosion/thinc,61,2018-03-27T15:59:57Z,2018-05-19T06:13:36Z,2018-05-19T06:13:36Z,MERGED,True,140,3,3,https://github.com/dvsrepo,Wip/wrap pytorch rnns,4,[],https://github.com/explosion/thinc/pull/61,https://github.com/dvsrepo,1,https://github.com/explosion/thinc/pull/61,"This a very rough attempt and WIP for adding support for RNNs.
@honnibal, if you can take a look and comment I can evolve this into a proper PR :-)
It would be nice to add some tests as well, output shapes of RNNs vary greatly with directions and num_layers.
PS: It also adds some minor fixes","This a very rough attempt and WIP for adding support for RNNs.
@honnibal, if you can take a look and comment I can evolve this into a proper PR :-)
It would be nice to add some tests as well, output shapes of RNNs vary greatly with directions and num_layers.
PS: It also adds some minor fixes",True,{}
explosion/thinc,https://github.com/explosion/thinc,61,2018-03-27T15:59:57Z,2018-05-19T06:13:36Z,2018-05-19T06:13:36Z,MERGED,True,140,3,3,https://github.com/dvsrepo,Wip/wrap pytorch rnns,4,[],https://github.com/explosion/thinc/pull/61,https://github.com/coveralls,2,https://github.com/explosion/thinc/pull/61#issuecomment-376581593,"This a very rough attempt and WIP for adding support for RNNs.
@honnibal, if you can take a look and comment I can evolve this into a proper PR :-)
It would be nice to add some tests as well, output shapes of RNNs vary greatly with directions and num_layers.
PS: It also adds some minor fixes",Coverage decreased (-0.4%) to 51.23% when pulling cbfac37 on recognai:wip/wrap_pytorch_rnns into f58d59f on explosion:master.,True,{}
explosion/thinc,https://github.com/explosion/thinc,61,2018-03-27T15:59:57Z,2018-05-19T06:13:36Z,2018-05-19T06:13:36Z,MERGED,True,140,3,3,https://github.com/dvsrepo,Wip/wrap pytorch rnns,4,[],https://github.com/explosion/thinc/pull/61,https://github.com/dvsrepo,3,https://github.com/explosion/thinc/pull/61#issuecomment-376875246,"This a very rough attempt and WIP for adding support for RNNs.
@honnibal, if you can take a look and comment I can evolve this into a proper PR :-)
It would be nice to add some tests as well, output shapes of RNNs vary greatly with directions and num_layers.
PS: It also adds some minor fixes","Hi @honnibal, I have added some changes trying to reflect your comments. Now fwd returns a tuple and backward has a tuple as argument. Currently, we just ignore the second argument (h_n), I am sure you will have some ideas for refining this.
I have also included some silly tests in the examples/wrap_pytorch_rnn.py just to verify the current state of affairs, based on your existing test for pytorch_wrapper (learning a zeroed output).
I hope I didn't screw up too much :-)",True,{}
explosion/thinc,https://github.com/explosion/thinc,61,2018-03-27T15:59:57Z,2018-05-19T06:13:36Z,2018-05-19T06:13:36Z,MERGED,True,140,3,3,https://github.com/dvsrepo,Wip/wrap pytorch rnns,4,[],https://github.com/explosion/thinc/pull/61,https://github.com/honnibal,4,https://github.com/explosion/thinc/pull/61#issuecomment-390382875,"This a very rough attempt and WIP for adding support for RNNs.
@honnibal, if you can take a look and comment I can evolve this into a proper PR :-)
It would be nice to add some tests as well, output shapes of RNNs vary greatly with directions and num_layers.
PS: It also adds some minor fixes",@dvsrepo Sorry about the delay getting this reviewed. Looks good! Going to merge so I can more easily test it :),True,{}
explosion/thinc,https://github.com/explosion/thinc,62,2018-04-16T15:25:35Z,2018-05-19T06:33:51Z,2018-05-19T06:33:51Z,MERGED,True,16,2,2,https://github.com/justindujardin,Model.to_disk fails with Python3,4,[],https://github.com/explosion/thinc/pull/62,https://github.com/justindujardin,1,https://github.com/explosion/thinc/pull/62,Demonstrate failure with tests. Fix by try/except use of basestring,Demonstrate failure with tests. Fix by try/except use of basestring,True,{}
explosion/thinc,https://github.com/explosion/thinc,62,2018-04-16T15:25:35Z,2018-05-19T06:33:51Z,2018-05-19T06:33:51Z,MERGED,True,16,2,2,https://github.com/justindujardin,Model.to_disk fails with Python3,4,[],https://github.com/explosion/thinc/pull/62,https://github.com/coveralls,2,https://github.com/explosion/thinc/pull/62#issuecomment-381647119,Demonstrate failure with tests. Fix by try/except use of basestring,Coverage increased (+0.3%) to 51.907% when pulling 24130b5 on justindujardin:python3-model-to-disk into b7b7ae7 on explosion:master.,True,{}
explosion/thinc,https://github.com/explosion/thinc,62,2018-04-16T15:25:35Z,2018-05-19T06:33:51Z,2018-05-19T06:33:51Z,MERGED,True,16,2,2,https://github.com/justindujardin,Model.to_disk fails with Python3,4,[],https://github.com/explosion/thinc/pull/62,https://github.com/justindujardin,3,https://github.com/explosion/thinc/pull/62#issuecomment-385678821,Demonstrate failure with tests. Fix by try/except use of basestring,"I forgot to comment that the build failure appears unrelated to this change:
Collecting wheel
  HTTP error 404 while getting https://files.pythonhosted.org/packages/1b/d2/22cde5ea9af055f81814f9f2545f5ed8a053eb749c08d186b369959189a8/wheel-0.31.0-py2.py3-none-any.whl#sha256=9cdc8ab2cc9c3c2e2727a4b67c22881dbb0e1c503d592992594c5e131c867107 (from https://pypi.org/simple/wheel/) (requires-python:>=2.7, !=3.0.*, !=3.1.*, !=3.2.*, !=3.3.*)
  Could not install requirement wheel from https://files.pythonhosted.org/packages/1b/d2/22cde5ea9af055f81814f9f2545f5ed8a053eb749c08d186b369959189a8/wheel-0.31.0-py2.py3-none-any.whl#sha256=9cdc8ab2cc9c3c2e2727a4b67c22881dbb0e1c503d592992594c5e131c867107 because of error 404 Client Error: Not Found for url: https://files.pythonhosted.org/packages/1b/d2/22cde5ea9af055f81814f9f2545f5ed8a053eb749c08d186b369959189a8/wheel-0.31.0-py2.py3-none-any.whl
Could not install requirement wheel from https://files.pythonhosted.org/packages/1b/d2/22cde5ea9af055f81814f9f2545f5ed8a053eb749c08d186b369959189a8/wheel-0.31.0-py2.py3-none-any.whl#sha256=9cdc8ab2cc9c3c2e2727a4b67c22881dbb0e1c503d592992594c5e131c867107 because of HTTP error 404 Client Error: Not Found for url: https://files.pythonhosted.org/packages/1b/d2/22cde5ea9af055f81814f9f2545f5ed8a053eb749c08d186b369959189a8/wheel-0.31.0-py2.py3-none-any.whl for URL https://files.pythonhosted.org/packages/1b/d2/22cde5ea9af055f81814f9f2545f5ed8a053eb749c08d186b369959189a8/wheel-0.31.0-py2.py3-none-any.whl#sha256=9cdc8ab2cc9c3c2e2727a4b67c22881dbb0e1c503d592992594c5e131c867107 (from https://pypi.org/simple/wheel/) (requires-python:>=2.7, !=3.0.*, !=3.1.*, !=3.2.*, !=3.3.*)
You are using pip version 9.0.1, however version 10.0.0 is available.
You should consider upgrading via the 'python -m pip install --upgrade pip' command.
Command exited with code 1",True,{}
explosion/thinc,https://github.com/explosion/thinc,62,2018-04-16T15:25:35Z,2018-05-19T06:33:51Z,2018-05-19T06:33:51Z,MERGED,True,16,2,2,https://github.com/justindujardin,Model.to_disk fails with Python3,4,[],https://github.com/explosion/thinc/pull/62,https://github.com/honnibal,4,https://github.com/explosion/thinc/pull/62#issuecomment-390383736,Demonstrate failure with tests. Fix by try/except use of basestring,"Missed this before, and fixed the error independently. More tests are definitely great though, thanks!",True,{}
explosion/thinc,https://github.com/explosion/thinc,66,2018-06-23T19:55:21Z,2018-07-20T18:18:03Z,2018-07-20T18:18:03Z,MERGED,True,3,2,1,https://github.com/nehaljwani,Improve link command generation for THINC_BLAS,1,[],https://github.com/explosion/thinc/pull/66,https://github.com/nehaljwani,1,https://github.com/explosion/thinc/pull/66,"This patch allows one to specify BLAS library on macOS and Windows too
export THINC_BLAS=/path/to/libmkl_rt.dylib

set THINC_BLAS=\path\to\mkl_rt.lib","This patch allows one to specify BLAS library on macOS and Windows too
export THINC_BLAS=/path/to/libmkl_rt.dylib

set THINC_BLAS=\path\to\mkl_rt.lib",True,{}
explosion/thinc,https://github.com/explosion/thinc,66,2018-06-23T19:55:21Z,2018-07-20T18:18:03Z,2018-07-20T18:18:03Z,MERGED,True,3,2,1,https://github.com/nehaljwani,Improve link command generation for THINC_BLAS,1,[],https://github.com/explosion/thinc/pull/66,https://github.com/coveralls,2,https://github.com/explosion/thinc/pull/66#issuecomment-399704917,"This patch allows one to specify BLAS library on macOS and Windows too
export THINC_BLAS=/path/to/libmkl_rt.dylib

set THINC_BLAS=\path\to\mkl_rt.lib",Coverage remained the same at 55.727% when pulling 704aa39 on nehaljwani:thinc_blas_link into 7371c14 on explosion:master.,True,{}
explosion/thinc,https://github.com/explosion/thinc,66,2018-06-23T19:55:21Z,2018-07-20T18:18:03Z,2018-07-20T18:18:03Z,MERGED,True,3,2,1,https://github.com/nehaljwani,Improve link command generation for THINC_BLAS,1,[],https://github.com/explosion/thinc/pull/66,https://github.com/honnibal,3,https://github.com/explosion/thinc/pull/66#issuecomment-406685475,"This patch allows one to specify BLAS library on macOS and Windows too
export THINC_BLAS=/path/to/libmkl_rt.dylib

set THINC_BLAS=\path\to\mkl_rt.lib","Thanks, this looks good!",True,{}
explosion/thinc,https://github.com/explosion/thinc,77,2018-10-23T13:00:57Z,2018-10-23T20:33:58Z,2020-11-19T16:20:52Z,MERGED,True,39,1640851,1812,https://github.com/honnibal,"Depend on cython-blis, instead of shipping OpenBLAS kernel",20,[],https://github.com/explosion/thinc/pull/77,https://github.com/honnibal,1,https://github.com/explosion/thinc/pull/77,"This patch replaces the vendorized OpenBLAS kernel previously shipped in v6.11 with a dependency on our Blis package: https://github.com/explosion/cython-blis
Multiplying matrices efficiently continues to be a massive struggle :(. The initial strategy of doing this via numpy causes thread safety problems on OSX, and can cause poor performance. I therefore tried to replace the numpy dependency with our own OpenBLAS kernel. However, this made building on Windows too difficult, and prevented us from shipping wheels, as I was relying on detecting the architecture during the setup.py.
This patch instead delegates the matrix multiplications to the Blis linear algebra library, provided by our new blis package. We now have wheels up for OSX and Windows for 64-bit versions of Python 3. I hope to have wheels for 32-bit Python 3 up soon. Windows Python 2.7 would be nice, but seems difficult. Linux wheels are unfortunately quite tough: I don't think it can be done with manylinux1, as we need gcc 6.","This patch replaces the vendorized OpenBLAS kernel previously shipped in v6.11 with a dependency on our Blis package: https://github.com/explosion/cython-blis
Multiplying matrices efficiently continues to be a massive struggle :(. The initial strategy of doing this via numpy causes thread safety problems on OSX, and can cause poor performance. I therefore tried to replace the numpy dependency with our own OpenBLAS kernel. However, this made building on Windows too difficult, and prevented us from shipping wheels, as I was relying on detecting the architecture during the setup.py.
This patch instead delegates the matrix multiplications to the Blis linear algebra library, provided by our new blis package. We now have wheels up for OSX and Windows for 64-bit versions of Python 3. I hope to have wheels for 32-bit Python 3 up soon. Windows Python 2.7 would be nice, but seems difficult. Linux wheels are unfortunately quite tough: I don't think it can be done with manylinux1, as we need gcc 6.",True,{}
explosion/thinc,https://github.com/explosion/thinc,77,2018-10-23T13:00:57Z,2018-10-23T20:33:58Z,2020-11-19T16:20:52Z,MERGED,True,39,1640851,1812,https://github.com/honnibal,"Depend on cython-blis, instead of shipping OpenBLAS kernel",20,[],https://github.com/explosion/thinc/pull/77,https://github.com/bradjonesca,2,https://github.com/explosion/thinc/pull/77#issuecomment-432328347,"This patch replaces the vendorized OpenBLAS kernel previously shipped in v6.11 with a dependency on our Blis package: https://github.com/explosion/cython-blis
Multiplying matrices efficiently continues to be a massive struggle :(. The initial strategy of doing this via numpy causes thread safety problems on OSX, and can cause poor performance. I therefore tried to replace the numpy dependency with our own OpenBLAS kernel. However, this made building on Windows too difficult, and prevented us from shipping wheels, as I was relying on detecting the architecture during the setup.py.
This patch instead delegates the matrix multiplications to the Blis linear algebra library, provided by our new blis package. We now have wheels up for OSX and Windows for 64-bit versions of Python 3. I hope to have wheels for 32-bit Python 3 up soon. Windows Python 2.7 would be nice, but seems difficult. Linux wheels are unfortunately quite tough: I don't think it can be done with manylinux1, as we need gcc 6.",Just an FYI for an alternative to Travis https://github.com/fastify/fastify/pull/1226#issuecomment-432325195,True,{}
explosion/thinc,https://github.com/explosion/thinc,77,2018-10-23T13:00:57Z,2018-10-23T20:33:58Z,2020-11-19T16:20:52Z,MERGED,True,39,1640851,1812,https://github.com/honnibal,"Depend on cython-blis, instead of shipping OpenBLAS kernel",20,[],https://github.com/explosion/thinc/pull/77,https://github.com/coveralls,3,https://github.com/explosion/thinc/pull/77#issuecomment-432332610,"This patch replaces the vendorized OpenBLAS kernel previously shipped in v6.11 with a dependency on our Blis package: https://github.com/explosion/cython-blis
Multiplying matrices efficiently continues to be a massive struggle :(. The initial strategy of doing this via numpy causes thread safety problems on OSX, and can cause poor performance. I therefore tried to replace the numpy dependency with our own OpenBLAS kernel. However, this made building on Windows too difficult, and prevented us from shipping wheels, as I was relying on detecting the architecture during the setup.py.
This patch instead delegates the matrix multiplications to the Blis linear algebra library, provided by our new blis package. We now have wheels up for OSX and Windows for 64-bit versions of Python 3. I hope to have wheels for 32-bit Python 3 up soon. Windows Python 2.7 would be nice, but seems difficult. Linux wheels are unfortunately quite tough: I don't think it can be done with manylinux1, as we need gcc 6.",Coverage decreased (-4.7%) to 50.756% when pulling d09eea7 on feature/blis into 1c5ce3a on master.,True,{}
explosion/thinc,https://github.com/explosion/thinc,78,2018-10-23T20:49:39Z,2018-10-23T21:26:33Z,2020-11-19T16:20:56Z,MERGED,True,19,232,4,https://github.com/honnibal,Remove GPU ops from master,8,[],https://github.com/explosion/thinc/pull/78,https://github.com/honnibal,1,https://github.com/explosion/thinc/pull/78,Cherry pick commits from v6.10.x branch that extract the GPU ops to a separate library.,Cherry pick commits from v6.10.x branch that extract the GPU ops to a separate library.,True,{}
explosion/thinc,https://github.com/explosion/thinc,78,2018-10-23T20:49:39Z,2018-10-23T21:26:33Z,2020-11-19T16:20:56Z,MERGED,True,19,232,4,https://github.com/honnibal,Remove GPU ops from master,8,[],https://github.com/explosion/thinc/pull/78,https://github.com/coveralls,2,https://github.com/explosion/thinc/pull/78#issuecomment-432420747,Cherry pick commits from v6.10.x branch that extract the GPU ops to a separate library.,Coverage remained the same at 50.756% when pulling 613d156 on feature/remove-gpu-ops into 93c01ec on master.,True,{}
explosion/thinc,https://github.com/explosion/thinc,81,2018-11-09T13:18:00Z,2018-11-10T17:39:54Z,2020-11-19T16:21:01Z,MERGED,True,109,0,2,https://github.com/honnibal,"Add module with rates and schedules, for learning rates etc",6,[],https://github.com/explosion/thinc/pull/81,https://github.com/honnibal,1,https://github.com/explosion/thinc/pull/81,,,True,{}
explosion/thinc,https://github.com/explosion/thinc,81,2018-11-09T13:18:00Z,2018-11-10T17:39:54Z,2020-11-19T16:21:01Z,MERGED,True,109,0,2,https://github.com/honnibal,"Add module with rates and schedules, for learning rates etc",6,[],https://github.com/explosion/thinc/pull/81,https://github.com/coveralls,2,https://github.com/explosion/thinc/pull/81#issuecomment-437362987,,Coverage increased (+0.5%) to 51.253% when pulling 9dc9196 on feature/rates into 8d891b6 on master.,True,{}
explosion/thinc,https://github.com/explosion/thinc,82,2018-11-13T14:19:29Z,2018-11-13T14:28:26Z,2018-11-13T14:30:51Z,MERGED,True,2,2,1,https://github.com/ccoulombe,Fixed readme gpu_ops import,1,[],https://github.com/explosion/thinc/pull/82,https://github.com/ccoulombe,1,https://github.com/explosion/thinc/pull/82,"The README shows to import thinc.neural.gpu_ops but this has forked in its own repository thinc_gpu_ops and no gpu_ops module exists anymore.
(.thinc) ~ $ pip install thinc_gpu_ops-0.0.3-cp36-cp36m-linux_x86_64.whl cupy-4.5.0-cp36-cp36m-linux_x86_64.whl thinc-6.12.0-cp36-cp36m-linux_x86_64.whl
(.thinc) ~ $ python -c ""import cupy; assert cupy"" # Check it installed
(.thinc) ~ $ python -c ""import thinc_gpu_ops as gpu_ops;"" # Check the GPU ops were built
(.thinc) ~ $ 
(.thinc) ~ $ python -c ""import thinc.neural.gpu_ops"" # Check the GPU ops were built
Traceback (most recent call last):
  File ""<string>"", line 1, in <module>
ModuleNotFoundError: No module named 'thinc.neural.gpu_ops'","The README shows to import thinc.neural.gpu_ops but this has forked in its own repository thinc_gpu_ops and no gpu_ops module exists anymore.
(.thinc) ~ $ pip install thinc_gpu_ops-0.0.3-cp36-cp36m-linux_x86_64.whl cupy-4.5.0-cp36-cp36m-linux_x86_64.whl thinc-6.12.0-cp36-cp36m-linux_x86_64.whl
(.thinc) ~ $ python -c ""import cupy; assert cupy"" # Check it installed
(.thinc) ~ $ python -c ""import thinc_gpu_ops as gpu_ops;"" # Check the GPU ops were built
(.thinc) ~ $ 
(.thinc) ~ $ python -c ""import thinc.neural.gpu_ops"" # Check the GPU ops were built
Traceback (most recent call last):
  File ""<string>"", line 1, in <module>
ModuleNotFoundError: No module named 'thinc.neural.gpu_ops'",True,{'HEART': ['https://github.com/honnibal']}
explosion/thinc,https://github.com/explosion/thinc,82,2018-11-13T14:19:29Z,2018-11-13T14:28:26Z,2018-11-13T14:30:51Z,MERGED,True,2,2,1,https://github.com/ccoulombe,Fixed readme gpu_ops import,1,[],https://github.com/explosion/thinc/pull/82,https://github.com/honnibal,2,https://github.com/explosion/thinc/pull/82#issuecomment-438285072,"The README shows to import thinc.neural.gpu_ops but this has forked in its own repository thinc_gpu_ops and no gpu_ops module exists anymore.
(.thinc) ~ $ pip install thinc_gpu_ops-0.0.3-cp36-cp36m-linux_x86_64.whl cupy-4.5.0-cp36-cp36m-linux_x86_64.whl thinc-6.12.0-cp36-cp36m-linux_x86_64.whl
(.thinc) ~ $ python -c ""import cupy; assert cupy"" # Check it installed
(.thinc) ~ $ python -c ""import thinc_gpu_ops as gpu_ops;"" # Check the GPU ops were built
(.thinc) ~ $ 
(.thinc) ~ $ python -c ""import thinc.neural.gpu_ops"" # Check the GPU ops were built
Traceback (most recent call last):
  File ""<string>"", line 1, in <module>
ModuleNotFoundError: No module named 'thinc.neural.gpu_ops'",Thanks!,True,{}
explosion/thinc,https://github.com/explosion/thinc,82,2018-11-13T14:19:29Z,2018-11-13T14:28:26Z,2018-11-13T14:30:51Z,MERGED,True,2,2,1,https://github.com/ccoulombe,Fixed readme gpu_ops import,1,[],https://github.com/explosion/thinc/pull/82,https://github.com/coveralls,3,https://github.com/explosion/thinc/pull/82#issuecomment-438285589,"The README shows to import thinc.neural.gpu_ops but this has forked in its own repository thinc_gpu_ops and no gpu_ops module exists anymore.
(.thinc) ~ $ pip install thinc_gpu_ops-0.0.3-cp36-cp36m-linux_x86_64.whl cupy-4.5.0-cp36-cp36m-linux_x86_64.whl thinc-6.12.0-cp36-cp36m-linux_x86_64.whl
(.thinc) ~ $ python -c ""import cupy; assert cupy"" # Check it installed
(.thinc) ~ $ python -c ""import thinc_gpu_ops as gpu_ops;"" # Check the GPU ops were built
(.thinc) ~ $ 
(.thinc) ~ $ python -c ""import thinc.neural.gpu_ops"" # Check the GPU ops were built
Traceback (most recent call last):
  File ""<string>"", line 1, in <module>
ModuleNotFoundError: No module named 'thinc.neural.gpu_ops'",Coverage remained the same at 51.253% when pulling ccffeb2 on ccoulombe:fix/readme_gpu_ops into a7949fd on explosion:master.,True,{}
explosion/thinc,https://github.com/explosion/thinc,82,2018-11-13T14:19:29Z,2018-11-13T14:28:26Z,2018-11-13T14:30:51Z,MERGED,True,2,2,1,https://github.com/ccoulombe,Fixed readme gpu_ops import,1,[],https://github.com/explosion/thinc/pull/82,https://github.com/coveralls,4,https://github.com/explosion/thinc/pull/82#issuecomment-438285592,"The README shows to import thinc.neural.gpu_ops but this has forked in its own repository thinc_gpu_ops and no gpu_ops module exists anymore.
(.thinc) ~ $ pip install thinc_gpu_ops-0.0.3-cp36-cp36m-linux_x86_64.whl cupy-4.5.0-cp36-cp36m-linux_x86_64.whl thinc-6.12.0-cp36-cp36m-linux_x86_64.whl
(.thinc) ~ $ python -c ""import cupy; assert cupy"" # Check it installed
(.thinc) ~ $ python -c ""import thinc_gpu_ops as gpu_ops;"" # Check the GPU ops were built
(.thinc) ~ $ 
(.thinc) ~ $ python -c ""import thinc.neural.gpu_ops"" # Check the GPU ops were built
Traceback (most recent call last):
  File ""<string>"", line 1, in <module>
ModuleNotFoundError: No module named 'thinc.neural.gpu_ops'",Coverage remained the same at 51.253% when pulling ccffeb2 on ccoulombe:fix/readme_gpu_ops into a7949fd on explosion:master.,True,{}
explosion/thinc,https://github.com/explosion/thinc,84,2018-12-02T01:31:23Z,2018-12-02T04:52:12Z,2018-12-02T04:52:12Z,MERGED,True,15,6,1,https://github.com/justindujardin,fix issue compiling the latest thinc on MacOS 10.3.6,1,[],https://github.com/explosion/thinc/pull/84,https://github.com/justindujardin,1,https://github.com/explosion/thinc/pull/84,"Related to explosion/spaCy#2998
I changed the variable names to be consistent with spaCy.","Related to explosion/spaCy#2998
I changed the variable names to be consistent with spaCy.",True,{'THUMBS_UP': ['https://github.com/honnibal']}
explosion/thinc,https://github.com/explosion/thinc,84,2018-12-02T01:31:23Z,2018-12-02T04:52:12Z,2018-12-02T04:52:12Z,MERGED,True,15,6,1,https://github.com/justindujardin,fix issue compiling the latest thinc on MacOS 10.3.6,1,[],https://github.com/explosion/thinc/pull/84,https://github.com/coveralls,2,https://github.com/explosion/thinc/pull/84#issuecomment-443473175,"Related to explosion/spaCy#2998
I changed the variable names to be consistent with spaCy.",Coverage remained the same at 52.005% when pulling 204fa52 on justindujardin:bugfix/mac_compile into e4a9472 on explosion:master.,True,{}
explosion/thinc,https://github.com/explosion/thinc,84,2018-12-02T01:31:23Z,2018-12-02T04:52:12Z,2018-12-02T04:52:12Z,MERGED,True,15,6,1,https://github.com/justindujardin,fix issue compiling the latest thinc on MacOS 10.3.6,1,[],https://github.com/explosion/thinc/pull/84,https://github.com/coveralls,3,https://github.com/explosion/thinc/pull/84#issuecomment-443473176,"Related to explosion/spaCy#2998
I changed the variable names to be consistent with spaCy.",Coverage remained the same at 52.005% when pulling 204fa52 on justindujardin:bugfix/mac_compile into e4a9472 on explosion:master.,True,{}
explosion/thinc,https://github.com/explosion/thinc,85,2018-12-02T18:52:05Z,2018-12-02T19:28:55Z,2020-01-22T00:54:39Z,MERGED,True,31,42,20,https://github.com/ines,"Replace msgpack, dill, pickle and cloudpickle with srsly",3,[],https://github.com/explosion/thinc/pull/85,https://github.com/ines,1,https://github.com/explosion/thinc/pull/85,https://github.com/explosion/srsly,https://github.com/explosion/srsly,True,{'THUMBS_UP': ['https://github.com/honnibal']}
explosion/thinc,https://github.com/explosion/thinc,86,2018-12-02T20:07:54Z,2018-12-02T20:19:50Z,2018-12-02T20:20:03Z,MERGED,True,13,68,3,https://github.com/ines,Add traceback printer via wasabi,1,[],https://github.com/explosion/thinc/pull/86,https://github.com/ines,1,https://github.com/explosion/thinc/pull/86,"Keeps Thinc tidy and makes the code reusable and easier to maintain. See here:
https://github.com/ines/wasabi/blob/master/wasabi/traceback.py","Keeps Thinc tidy and makes the code reusable and easier to maintain. See here:
https://github.com/ines/wasabi/blob/master/wasabi/traceback.py",True,{}
explosion/thinc,https://github.com/explosion/thinc,86,2018-12-02T20:07:54Z,2018-12-02T20:19:50Z,2018-12-02T20:20:03Z,MERGED,True,13,68,3,https://github.com/ines,Add traceback printer via wasabi,1,[],https://github.com/explosion/thinc/pull/86,https://github.com/coveralls,2,https://github.com/explosion/thinc/pull/86#issuecomment-443538453,"Keeps Thinc tidy and makes the code reusable and easier to maintain. See here:
https://github.com/ines/wasabi/blob/master/wasabi/traceback.py",Coverage decreased (-0.8%) to 51.184% when pulling 8ea91b7 on feature/add-wasabi into 4057866 on master.,True,{}
explosion/thinc,https://github.com/explosion/thinc,86,2018-12-02T20:07:54Z,2018-12-02T20:19:50Z,2018-12-02T20:20:03Z,MERGED,True,13,68,3,https://github.com/ines,Add traceback printer via wasabi,1,[],https://github.com/explosion/thinc/pull/86,https://github.com/coveralls,3,https://github.com/explosion/thinc/pull/86#issuecomment-443538455,"Keeps Thinc tidy and makes the code reusable and easier to maintain. See here:
https://github.com/ines/wasabi/blob/master/wasabi/traceback.py",Coverage decreased (-0.8%) to 51.184% when pulling 8ea91b7 on feature/add-wasabi into 4057866 on master.,True,{}
explosion/thinc,https://github.com/explosion/thinc,86,2018-12-02T20:07:54Z,2018-12-02T20:19:50Z,2018-12-02T20:20:03Z,MERGED,True,13,68,3,https://github.com/ines,Add traceback printer via wasabi,1,[],https://github.com/explosion/thinc/pull/86,https://github.com/coveralls,4,https://github.com/explosion/thinc/pull/86#issuecomment-443538456,"Keeps Thinc tidy and makes the code reusable and easier to maintain. See here:
https://github.com/ines/wasabi/blob/master/wasabi/traceback.py",Coverage decreased (-0.8%) to 51.184% when pulling 8ea91b7 on feature/add-wasabi into 4057866 on master.,True,{}
explosion/thinc,https://github.com/explosion/thinc,87,2018-12-02T21:42:57Z,2018-12-03T01:43:36Z,2018-12-03T01:43:39Z,MERGED,True,2538,1947,98,https://github.com/ines,Tidy up and auto-format,3,[],https://github.com/explosion/thinc/pull/87,https://github.com/ines,1,https://github.com/explosion/thinc/pull/87,"Use black to auto-format all .py files.
 Update code to be compatible with flake8 rules
 Fix various small bugs, inconsistencies and messy stuff","Use black to auto-format all .py files.
 Update code to be compatible with flake8 rules
 Fix various small bugs, inconsistencies and messy stuff",True,{}
explosion/thinc,https://github.com/explosion/thinc,87,2018-12-02T21:42:57Z,2018-12-03T01:43:36Z,2018-12-03T01:43:39Z,MERGED,True,2538,1947,98,https://github.com/ines,Tidy up and auto-format,3,[],https://github.com/explosion/thinc/pull/87,https://github.com/coveralls,2,https://github.com/explosion/thinc/pull/87#issuecomment-443545451,"Use black to auto-format all .py files.
 Update code to be compatible with flake8 rules
 Fix various small bugs, inconsistencies and messy stuff",Coverage decreased (-0.1%) to 51.066% when pulling 760024b on chore/tidy-up-auto-format into 3070532 on master.,True,{}
explosion/thinc,https://github.com/explosion/thinc,96,2019-03-25T20:16:58Z,2019-07-10T08:36:47Z,2019-07-10T08:36:47Z,CLOSED,False,25,6,2,https://github.com/mwakaba2,Comment out assert statement for empty length X in unflatten(),2,[],https://github.com/explosion/thinc/pull/96,https://github.com/mwakaba2,1,https://github.com/explosion/thinc/pull/96,"This fix is related to the spacy issue 3456
The unflatten function fails with a two-dimensional list that contains empty lists.
I added a test function test_nested_unflatten_length_zero to test this specific example.","This fix is related to the spacy issue 3456
The unflatten function fails with a two-dimensional list that contains empty lists.
I added a test function test_nested_unflatten_length_zero to test this specific example.",True,{}
explosion/thinc,https://github.com/explosion/thinc,96,2019-03-25T20:16:58Z,2019-07-10T08:36:47Z,2019-07-10T08:36:47Z,CLOSED,False,25,6,2,https://github.com/mwakaba2,Comment out assert statement for empty length X in unflatten(),2,[],https://github.com/explosion/thinc/pull/96,https://github.com/honnibal,2,https://github.com/explosion/thinc/pull/96#issuecomment-476604219,"This fix is related to the spacy issue 3456
The unflatten function fails with a two-dimensional list that contains empty lists.
I added a test function test_nested_unflatten_length_zero to test this specific example.","Thanks!
I'm still a little confused about where my assumptions in that function went wrong. If X has trailing data here, isn't that bad? How come we don't need those rows?",True,{}
explosion/thinc,https://github.com/explosion/thinc,96,2019-03-25T20:16:58Z,2019-07-10T08:36:47Z,2019-07-10T08:36:47Z,CLOSED,False,25,6,2,https://github.com/mwakaba2,Comment out assert statement for empty length X in unflatten(),2,[],https://github.com/explosion/thinc/pull/96,https://github.com/mwakaba2,3,https://github.com/explosion/thinc/pull/96#issuecomment-476637650,"This fix is related to the spacy issue 3456
The unflatten function fails with a two-dimensional list that contains empty lists.
I added a test function test_nested_unflatten_length_zero to test this specific example.","Yes Im also confused to the trailing data for empty inputs. Im not familiar with how the thinc tagger outputs for empty strings, so Im hoping someone whose more familiar with the codebase can propose a better solution.
Commenting out an assertion isnt an ideal fix.

FYI, I only tested if the nlp.pipe doesnt break when I feed in an empty phrase.  I probably want to double check what the tagger outputs look like for those.

 On Mar 26, 2019, at 7:31 AM, Matthew Honnibal ***@***.***> wrote:

 Thanks!

 I'm still a little confused about where my assumptions in that function went wrong. If X has trailing data here, isn't that bad? How come we don't need those rows?

 
 You are receiving this because you authored the thread.
 Reply to this email directly, view it on GitHub, or mute the thread.",True,{}
explosion/thinc,https://github.com/explosion/thinc,96,2019-03-25T20:16:58Z,2019-07-10T08:36:47Z,2019-07-10T08:36:47Z,CLOSED,False,25,6,2,https://github.com/mwakaba2,Comment out assert statement for empty length X in unflatten(),2,[],https://github.com/explosion/thinc/pull/96,https://github.com/honnibal,4,https://github.com/explosion/thinc/pull/96#issuecomment-509967442,"This fix is related to the spacy issue 3456
The unflatten function fails with a two-dimensional list that contains empty lists.
I added a test function test_nested_unflatten_length_zero to test this specific example.",Now fixed by #104  . Thanks for your help on this.,True,{'HOORAY': ['https://github.com/mwakaba2']}
explosion/thinc,https://github.com/explosion/thinc,98,2019-04-12T15:14:44Z,2019-06-08T09:28:24Z,2019-06-19T16:31:41Z,MERGED,True,1,1,1,https://github.com/simonhkswan,Fix SyntaxError in try-except import cPickle Statement,1,[],https://github.com/explosion/thinc/pull/98,https://github.com/simonhkswan,1,https://github.com/explosion/thinc/pull/98,"This doesnt follow the correct grammar of python, an 'except' is always
needed before an else in a try-except-else-finally statement.","This doesnt follow the correct grammar of python, an 'except' is always
needed before an else in a try-except-else-finally statement.",True,"{'THUMBS_UP': ['https://github.com/kiendang', 'https://github.com/m-x-k', 'https://github.com/pzbitskiy']}"
explosion/thinc,https://github.com/explosion/thinc,98,2019-04-12T15:14:44Z,2019-06-08T09:28:24Z,2019-06-19T16:31:41Z,MERGED,True,1,1,1,https://github.com/simonhkswan,Fix SyntaxError in try-except import cPickle Statement,1,[],https://github.com/explosion/thinc/pull/98,https://github.com/m-x-k,2,https://github.com/explosion/thinc/pull/98#issuecomment-489078933,"This doesnt follow the correct grammar of python, an 'except' is always
needed before an else in a try-except-else-finally statement.",Any progress on merging this pull request. Its currently preventing installation of spaCy,True,{}
explosion/thinc,https://github.com/explosion/thinc,98,2019-04-12T15:14:44Z,2019-06-08T09:28:24Z,2019-06-19T16:31:41Z,MERGED,True,1,1,1,https://github.com/simonhkswan,Fix SyntaxError in try-except import cPickle Statement,1,[],https://github.com/explosion/thinc/pull/98,https://github.com/pzbitskiy,3,https://github.com/explosion/thinc/pull/98#issuecomment-497975949,"This doesnt follow the correct grammar of python, an 'except' is always
needed before an else in a try-except-else-finally statement.",@syllog1sm @honnibal please merge.,True,{}
explosion/thinc,https://github.com/explosion/thinc,98,2019-04-12T15:14:44Z,2019-06-08T09:28:24Z,2019-06-19T16:31:41Z,MERGED,True,1,1,1,https://github.com/simonhkswan,Fix SyntaxError in try-except import cPickle Statement,1,[],https://github.com/explosion/thinc/pull/98,https://github.com/ifokeev,4,https://github.com/explosion/thinc/pull/98#issuecomment-503635018,"This doesnt follow the correct grammar of python, an 'except' is always
needed before an else in a try-except-else-finally statement.","will it be released? annoying error, can't install spacy =\",True,{}
explosion/thinc,https://github.com/explosion/thinc,100,2019-06-03T17:59:39Z,2019-06-08T09:52:08Z,2019-06-08T09:52:08Z,MERGED,True,682,15,5,https://github.com/giannisdaras,"Add MultiHeadedAttention, SparseAttention and Transformer",474,[],https://github.com/explosion/thinc/pull/100,https://github.com/giannisdaras,1,https://github.com/explosion/thinc/pull/100,"Transformer model
This pull request adds Transformer architecture, as described in Attention Is All You Need, implemented in thinc/neural/classes/encoder_decoder.py.
MultiHeadedAttention, SparseAttention
This pull requests also adds MultiHeadedAttention layer as described in Attention Is All You Need and a modification of the fixed pattern for factorizing the attention matrix proposed in Generative Modeling with Sparse Transformers.
This work aims to do the groundwork for the integration of attention layers to spaCy, by introducing these two powerful components.
The attention layers support the self attention and the outer attention case. The functionality of the layers changes depending on the input arguments. Also, they support sentX, sentY which are meant for visualizing attention patterns after training for intrinsic evaluation.
SparseAttention layer for now supports only the left fixed pattern from the Generative Sparse Attention paper, as this pattern has been shown to perform better than others for the NLP domain. More patterns can be integrated in following PRs.
The factorization of the attention matrix is performed in steps which basically means that we are doing it once for the first mask and then we are using this result as the query vector for the next mask. There are other ways of doing that as well, such as keeping a seperate head to attend to both other heads, or attending directly to the positions of the two masks. However, this implementation performs the same with the others in the experiments conducted while keeping the computational cost much lower.
Changes to PytorchWrappers
In my code, I am using PytorchWrappers for some layers needed for encoder_decoder.py, multiheaded_attention.py. Those layers can be replaced by pure Thinc layers in the future, but to avoid wrong computation of the backpropagations, I used some layers from Pytorch. To do so, I had to change the PytorchWrappers in order to support the case of multiple inputs and multiple outputs.
I think that this change can be useful for other cases as well and enable smoother combination of spaCy and Pytorch. To be more specific, till now pytorch wrappers could be used only for the case of a layer with one input and one output. This is not always the case, however. For example, a model that does translation needs the output till now to predict the next token. To make this happen, I extended the PytorchWrappers with one optional arguments, config that if set, make this possible.
Config is a list of the following variables: i_grad, o_xp, b_map, ret_x. i_grad is a boolean iterable which controls which of the inputs should be backpropagated. o_xp determines the number of outputs of this layer. b_map is used only when we are combining this Pytorch layer with a thinc layer which returns multiple gradients and it is basically a mapping between the gradients and the output variables. For pytorch users, this can be always None. ret_x is also used only when a Thinc layer follows this PytorchLayer and states which of the inputs should be returned in backpropagation.
I am not sure if the PytorchWrappers changes should be in separate pull request
I am happy to provide more info or change the code of this PR to get this merged, so looking forward to your feedback.","Transformer model
This pull request adds Transformer architecture, as described in Attention Is All You Need, implemented in thinc/neural/classes/encoder_decoder.py.
MultiHeadedAttention, SparseAttention
This pull requests also adds MultiHeadedAttention layer as described in Attention Is All You Need and a modification of the fixed pattern for factorizing the attention matrix proposed in Generative Modeling with Sparse Transformers.
This work aims to do the groundwork for the integration of attention layers to spaCy, by introducing these two powerful components.
The attention layers support the self attention and the outer attention case. The functionality of the layers changes depending on the input arguments. Also, they support sentX, sentY which are meant for visualizing attention patterns after training for intrinsic evaluation.
SparseAttention layer for now supports only the left fixed pattern from the Generative Sparse Attention paper, as this pattern has been shown to perform better than others for the NLP domain. More patterns can be integrated in following PRs.
The factorization of the attention matrix is performed in steps which basically means that we are doing it once for the first mask and then we are using this result as the query vector for the next mask. There are other ways of doing that as well, such as keeping a seperate head to attend to both other heads, or attending directly to the positions of the two masks. However, this implementation performs the same with the others in the experiments conducted while keeping the computational cost much lower.
Changes to PytorchWrappers
In my code, I am using PytorchWrappers for some layers needed for encoder_decoder.py, multiheaded_attention.py. Those layers can be replaced by pure Thinc layers in the future, but to avoid wrong computation of the backpropagations, I used some layers from Pytorch. To do so, I had to change the PytorchWrappers in order to support the case of multiple inputs and multiple outputs.
I think that this change can be useful for other cases as well and enable smoother combination of spaCy and Pytorch. To be more specific, till now pytorch wrappers could be used only for the case of a layer with one input and one output. This is not always the case, however. For example, a model that does translation needs the output till now to predict the next token. To make this happen, I extended the PytorchWrappers with one optional arguments, config that if set, make this possible.
Config is a list of the following variables: i_grad, o_xp, b_map, ret_x. i_grad is a boolean iterable which controls which of the inputs should be backpropagated. o_xp determines the number of outputs of this layer. b_map is used only when we are combining this Pytorch layer with a thinc layer which returns multiple gradients and it is basically a mapping between the gradients and the output variables. For pytorch users, this can be always None. ret_x is also used only when a Thinc layer follows this PytorchLayer and states which of the inputs should be returned in backpropagation.
I am not sure if the PytorchWrappers changes should be in separate pull request
I am happy to provide more info or change the code of this PR to get this merged, so looking forward to your feedback.",True,{}
explosion/thinc,https://github.com/explosion/thinc,101,2019-06-09T10:44:57Z,2019-07-10T09:40:14Z,2020-11-19T16:21:08Z,MERGED,True,508,454,10,https://github.com/honnibal,Remove PyTorch and numpy calls from multi-headed attention,35,[],https://github.com/explosion/thinc/pull/101,https://github.com/honnibal,1,https://github.com/explosion/thinc/pull/101,"Implement backprop of softmax
Replace PyTorch mask with our mask function
Add matmul op, to remove calls to numpy.matmul

The layer would mostly be usable in spaCy now, although the padding is problematic if the sequences are long. The implementation is about 2-3x faster than the previous, on CPU.","Implement backprop of softmax
Replace PyTorch mask with our mask function
Add matmul op, to remove calls to numpy.matmul

The layer would mostly be usable in spaCy now, although the padding is problematic if the sequences are long. The implementation is about 2-3x faster than the previous, on CPU.",True,{}
explosion/thinc,https://github.com/explosion/thinc,102,2019-06-22T20:11:36Z,2019-07-10T08:35:55Z,2019-07-10T08:35:55Z,MERGED,True,5,1,1,https://github.com/chssch,Pass seed to init,1,[],https://github.com/explosion/thinc/pull/102,https://github.com/chssch,1,https://github.com/explosion/thinc/pull/102,"Bugfix: Recognize passed seed parameter
Basis to fix explosion/spaCy#3870","Bugfix: Recognize passed seed parameter
Basis to fix explosion/spaCy#3870",True,{}
explosion/thinc,https://github.com/explosion/thinc,102,2019-06-22T20:11:36Z,2019-07-10T08:35:55Z,2019-07-10T08:35:55Z,MERGED,True,5,1,1,https://github.com/chssch,Pass seed to init,1,[],https://github.com/explosion/thinc/pull/102,https://github.com/chssch,2,https://github.com/explosion/thinc/pull/102#issuecomment-504695169,"Bugfix: Recognize passed seed parameter
Basis to fix explosion/spaCy#3870",Should the default actually be the id? Is there any reason that it's not a constant? This would also fix explosion/spaCy#3870,True,{}
explosion/thinc,https://github.com/explosion/thinc,102,2019-06-22T20:11:36Z,2019-07-10T08:35:55Z,2019-07-10T08:35:55Z,MERGED,True,5,1,1,https://github.com/chssch,Pass seed to init,1,[],https://github.com/explosion/thinc/pull/102,https://github.com/honnibal,3,https://github.com/explosion/thinc/pull/102#issuecomment-505852607,"Bugfix: Recognize passed seed parameter
Basis to fix explosion/spaCy#3870","@chssch If we made it constant, then we might get confusing results if you create two hash embeddings and add them, as you'd expect them to distribute differently.
Does your patch alter the trained models? I think not, right -- the trained models load the seed from the cfg file?",True,{}
explosion/thinc,https://github.com/explosion/thinc,102,2019-06-22T20:11:36Z,2019-07-10T08:35:55Z,2019-07-10T08:35:55Z,MERGED,True,5,1,1,https://github.com/chssch,Pass seed to init,1,[],https://github.com/explosion/thinc/pull/102,https://github.com/chssch,4,https://github.com/explosion/thinc/pull/102#issuecomment-505900409,"Bugfix: Recognize passed seed parameter
Basis to fix explosion/spaCy#3870","@chssch If we made it constant, then we might get confusing results if you create two hash embeddings and add them, as you'd expect them to distribute differently.

Okay, thanks, got it.

Does your patch alter the trained models? I think not, right -- the trained models load the seed from the cfg file?

No, it shouldn't alter the trained modes. It's just cosmetic preparation to fix explosion/spaCy#3870, but no blocker, since we can also set the seed directly on the object.
I guess the main problem is, how can we generally inject a constant seed from top level before a training run to get reproducible results. (but thats maybe for  the discussion in explosion/spaCy#3870. )",True,{'THUMBS_UP': ['https://github.com/honnibal']}
explosion/thinc,https://github.com/explosion/thinc,104,2019-07-04T11:05:19Z,2019-07-10T08:35:33Z,2019-10-01T12:10:17Z,MERGED,True,1,1,1,https://github.com/svlandeg,fix unflatten padding when last element is empty,1,[],https://github.com/explosion/thinc/pull/104,https://github.com/svlandeg,1,https://github.com/explosion/thinc/pull/104,"Attempt to fix spaCy issue explosion/spaCy#3880. The error in that issue is raised by ops.pyx on the line  assert len(X) == 0 (https://github.com/explosion/thinc/blob/master/thinc/neural/ops.pyx#L138), and it happens when the tagger model is run with an empty doc as last element in a call to nlp.pipe().
I think this happens because the flatten function adds one additional bit of padding at the end (https://github.com/explosion/thinc/blob/master/thinc/neural/ops.pyx#L119) which is not stripped away when unflatten removes the padding only when length != 0 (https://github.com/explosion/thinc/blob/master/thinc/neural/ops.pyx#L136) for the last variable length in lengths (accessed outside of its for loop).
I don't have a proper way of testing this yet, so I'm definitely not sure about this.","Attempt to fix spaCy issue explosion/spaCy#3880. The error in that issue is raised by ops.pyx on the line  assert len(X) == 0 (https://github.com/explosion/thinc/blob/master/thinc/neural/ops.pyx#L138), and it happens when the tagger model is run with an empty doc as last element in a call to nlp.pipe().
I think this happens because the flatten function adds one additional bit of padding at the end (https://github.com/explosion/thinc/blob/master/thinc/neural/ops.pyx#L119) which is not stripped away when unflatten removes the padding only when length != 0 (https://github.com/explosion/thinc/blob/master/thinc/neural/ops.pyx#L136) for the last variable length in lengths (accessed outside of its for loop).
I don't have a proper way of testing this yet, so I'm definitely not sure about this.",True,{}
explosion/thinc,https://github.com/explosion/thinc,104,2019-07-04T11:05:19Z,2019-07-10T08:35:33Z,2019-10-01T12:10:17Z,MERGED,True,1,1,1,https://github.com/svlandeg,fix unflatten padding when last element is empty,1,[],https://github.com/explosion/thinc/pull/104,https://github.com/honnibal,2,https://github.com/explosion/thinc/pull/104#issuecomment-508629771,"Attempt to fix spaCy issue explosion/spaCy#3880. The error in that issue is raised by ops.pyx on the line  assert len(X) == 0 (https://github.com/explosion/thinc/blob/master/thinc/neural/ops.pyx#L138), and it happens when the tagger model is run with an empty doc as last element in a call to nlp.pipe().
I think this happens because the flatten function adds one additional bit of padding at the end (https://github.com/explosion/thinc/blob/master/thinc/neural/ops.pyx#L119) which is not stripped away when unflatten removes the padding only when length != 0 (https://github.com/explosion/thinc/blob/master/thinc/neural/ops.pyx#L136) for the last variable length in lengths (accessed outside of its for loop).
I don't have a proper way of testing this yet, so I'm definitely not sure about this.","This makes a lot of sense! I've hit this error as well recently with empty docs. It makes perfect sense that the error arises when the doc is last, thanks. It should be easy to add a test for this in the unit tests.",True,{}
explosion/thinc,https://github.com/explosion/thinc,105,2019-07-11T14:31:03Z,2019-07-11T14:42:49Z,2019-07-11T14:43:23Z,MERGED,True,2,1,1,https://github.com/svlandeg,avoid allocating a negative shape,1,[],https://github.com/explosion/thinc/pull/105,https://github.com/svlandeg,1,https://github.com/explosion/thinc/pull/105,"Addressing an issue in spaCy around ngrams (Issue explosion/spaCy#3611), I found a line in ops.pyx that could allocate negative lengths, e.g. when allocating 2-grams for a 1-token doc it would try to allocate (1-2,) shape.
Setting to min threshold 0 fixes the bug.","Addressing an issue in spaCy around ngrams (Issue explosion/spaCy#3611), I found a line in ops.pyx that could allocate negative lengths, e.g. when allocating 2-grams for a 1-token doc it would try to allocate (1-2,) shape.
Setting to min threshold 0 fixes the bug.",True,{'THUMBS_UP': ['https://github.com/honnibal']}
explosion/thinc,https://github.com/explosion/thinc,106,2019-07-12T07:47:47Z,2019-07-12T08:13:10Z,2019-07-12T08:20:36Z,MERGED,True,1,1,1,https://github.com/svlandeg,upgrading pytest to 4.0,1,[],https://github.com/explosion/thinc/pull/106,https://github.com/svlandeg,1,https://github.com/explosion/thinc/pull/106,upgrade pytest to 4.0,upgrade pytest to 4.0,True,{'THUMBS_UP': ['https://github.com/ines']}
explosion/thinc,https://github.com/explosion/thinc,109,2019-08-09T20:59:35Z,2019-08-22T17:53:54Z,2019-08-23T16:28:07Z,MERGED,True,18,3,3,https://github.com/hervenicol,Fix Collections Abstract Base Classes imports for Python 3.8,1,[],https://github.com/explosion/thinc/pull/109,https://github.com/hervenicol,1,https://github.com/explosion/thinc/pull/109,"Since Python version 3.3, Collections Abstract Base Classes have been moved to the collections.abc module. For backwards compatibility, they continue to be visible in this module through Python 3.7. Subsequently, they will be removed entirely.
(source: https://docs.python.org/3/library/collections.html)
Related to issue #108","Since Python version 3.3, Collections Abstract Base Classes have been moved to the collections.abc module. For backwards compatibility, they continue to be visible in this module through Python 3.7. Subsequently, they will be removed entirely.
(source: https://docs.python.org/3/library/collections.html)
Related to issue #108",True,{}
explosion/thinc,https://github.com/explosion/thinc,109,2019-08-09T20:59:35Z,2019-08-22T17:53:54Z,2019-08-23T16:28:07Z,MERGED,True,18,3,3,https://github.com/hervenicol,Fix Collections Abstract Base Classes imports for Python 3.8,1,[],https://github.com/explosion/thinc/pull/109,https://github.com/honnibal,2,https://github.com/explosion/thinc/pull/109#issuecomment-524010647,"Since Python version 3.3, Collections Abstract Base Classes have been moved to the collections.abc module. For backwards compatibility, they continue to be visible in this module through Python 3.7. Subsequently, they will be removed entirely.
(source: https://docs.python.org/3/library/collections.html)
Related to issue #108",Thanks!,True,{}
explosion/thinc,https://github.com/explosion/thinc,109,2019-08-09T20:59:35Z,2019-08-22T17:53:54Z,2019-08-23T16:28:07Z,MERGED,True,18,3,3,https://github.com/hervenicol,Fix Collections Abstract Base Classes imports for Python 3.8,1,[],https://github.com/explosion/thinc/pull/109,https://github.com/hervenicol,3,https://github.com/explosion/thinc/pull/109#issuecomment-524238036,"Since Python version 3.3, Collections Abstract Base Classes have been moved to the collections.abc module. For backwards compatibility, they continue to be visible in this module through Python 3.7. Subsequently, they will be removed entirely.
(source: https://docs.python.org/3/library/collections.html)
Related to issue #108","@honnibal , it seems that master branch is more up-to-date than develop.
Maybe I should rather base my work on master ?",True,{}
explosion/thinc,https://github.com/explosion/thinc,109,2019-08-09T20:59:35Z,2019-08-22T17:53:54Z,2019-08-23T16:28:07Z,MERGED,True,18,3,3,https://github.com/hervenicol,Fix Collections Abstract Base Classes imports for Python 3.8,1,[],https://github.com/explosion/thinc/pull/109,https://github.com/honnibal,4,https://github.com/explosion/thinc/pull/109#issuecomment-524378858,"Since Python version 3.3, Collections Abstract Base Classes have been moved to the collections.abc module. For backwards compatibility, they continue to be visible in this module through Python 3.7. Subsequently, they will be removed entirely.
(source: https://docs.python.org/3/library/collections.html)
Related to issue #108","@hervenicol Sorry I missed that about this as well: yes, work off master.",True,{}
explosion/thinc,https://github.com/explosion/thinc,111,2019-08-23T16:33:59Z,2019-08-23T17:17:10Z,2020-11-19T16:21:12Z,MERGED,True,24,24,5,https://github.com/honnibal,Update to blis v0.4.0,9,[],https://github.com/explosion/thinc/pull/111,https://github.com/honnibal,1,https://github.com/explosion/thinc/pull/111,"Allow read-only numpy arrays
Update to blis v0.4.0

Note that read-only arrays means the new version won't work with the prior version of cython-blis, because only blis v0.4.0 supports read-only arrays. Even if the user isn't passing read-only arrays, they'll be marked as possibly read-only by the type-system, so we'll get an error if we try to pass them to a function which says they might be modified. Basically it's like const: if we want to be const-correct, we need the library we're calling into to be const-correct too.","Allow read-only numpy arrays
Update to blis v0.4.0

Note that read-only arrays means the new version won't work with the prior version of cython-blis, because only blis v0.4.0 supports read-only arrays. Even if the user isn't passing read-only arrays, they'll be marked as possibly read-only by the type-system, so we'll get an error if we try to pass them to a function which says they might be modified. Basically it's like const: if we want to be const-correct, we need the library we're calling into to be const-correct too.",True,{}
explosion/thinc,https://github.com/explosion/thinc,112,2019-08-23T20:16:51Z,2019-08-23T22:47:40Z,2019-08-23T22:47:40Z,MERGED,True,12,2,2,https://github.com/hervenicol,use collections.abc when possible,1,[],https://github.com/explosion/thinc/pull/112,https://github.com/hervenicol,1,https://github.com/explosion/thinc/pull/112,"Just like #109 , but (hopefully) without bug, and applied to master branch.
Since Python version 3.3, Collections Abstract Base Classes have been moved to the collections.abc module. For backwards compatibility, they continue to be visible in this module through Python 3.7. Subsequently, they will be removed entirely.
(source: https://docs.python.org/3/library/collections.html)
Related to issue #108","Just like #109 , but (hopefully) without bug, and applied to master branch.
Since Python version 3.3, Collections Abstract Base Classes have been moved to the collections.abc module. For backwards compatibility, they continue to be visible in this module through Python 3.7. Subsequently, they will be removed entirely.
(source: https://docs.python.org/3/library/collections.html)
Related to issue #108",True,{'THUMBS_UP': ['https://github.com/honnibal']}
explosion/thinc,https://github.com/explosion/thinc,114,2019-10-04T14:36:59Z,2019-10-18T22:37:44Z,2019-10-18T22:37:44Z,MERGED,True,1,1,1,https://github.com/rupsaijna,Update datasets.py,1,[],https://github.com/explosion/thinc/pull/114,https://github.com/rupsaijna,1,https://github.com/explosion/thinc/pull/114,"Fixed QUORA_QUESTIONS_URL
Issue #88","Fixed QUORA_QUESTIONS_URL
Issue #88",True,{}
explosion/thinc,https://github.com/explosion/thinc,114,2019-10-04T14:36:59Z,2019-10-18T22:37:44Z,2019-10-18T22:37:44Z,MERGED,True,1,1,1,https://github.com/rupsaijna,Update datasets.py,1,[],https://github.com/explosion/thinc/pull/114,https://github.com/honnibal,2,https://github.com/explosion/thinc/pull/114#issuecomment-543986168,"Fixed QUORA_QUESTIONS_URL
Issue #88",Thanks!,True,{}
explosion/thinc,https://github.com/explosion/thinc,115,2019-10-15T13:16:55Z,2019-10-18T23:52:35Z,2019-10-19T01:41:12Z,MERGED,True,1,3,1,https://github.com/KoichiYasuoka,Update ufuncobject.h,1,[],https://github.com/explosion/thinc/pull/115,https://github.com/KoichiYasuoka,1,https://github.com/explosion/thinc/pull/115,Recent cygwin-devel-3.0.6 (64bit) has fenv.h instead of fenv/fenv.c. (cf. explosion/spaCy#2411 ),Recent cygwin-devel-3.0.6 (64bit) has fenv.h instead of fenv/fenv.c. (cf. explosion/spaCy#2411 ),True,{}
explosion/thinc,https://github.com/explosion/thinc,115,2019-10-15T13:16:55Z,2019-10-18T23:52:35Z,2019-10-19T01:41:12Z,MERGED,True,1,3,1,https://github.com/KoichiYasuoka,Update ufuncobject.h,1,[],https://github.com/explosion/thinc/pull/115,https://github.com/honnibal,2,https://github.com/explosion/thinc/pull/115#issuecomment-543809705,Recent cygwin-devel-3.0.6 (64bit) has fenv.h instead of fenv/fenv.c. (cf. explosion/spaCy#2411 ),"Thanks, I'm pretty sure this just needs to be updated from master --- the test failures look unrelated.",True,{'THUMBS_UP': ['https://github.com/KoichiYasuoka']}
explosion/thinc,https://github.com/explosion/thinc,115,2019-10-15T13:16:55Z,2019-10-18T23:52:35Z,2019-10-19T01:41:12Z,MERGED,True,1,3,1,https://github.com/KoichiYasuoka,Update ufuncobject.h,1,[],https://github.com/explosion/thinc/pull/115,https://github.com/KoichiYasuoka,3,https://github.com/explosion/thinc/pull/115#issuecomment-544043916,Recent cygwin-devel-3.0.6 (64bit) has fenv.h instead of fenv/fenv.c. (cf. explosion/spaCy#2411 ),"Thank you, @honnibal. I'm looking forward to pypi update.",True,{}
explosion/thinc,https://github.com/explosion/thinc,116,2019-10-18T14:00:06Z,2019-10-18T22:36:34Z,2019-10-19T10:14:58Z,MERGED,True,132,190,4,https://github.com/honnibal,Improve GPU support and PyTorch wrapper,12,[],https://github.com/explosion/thinc/pull/116,https://github.com/honnibal,1,https://github.com/explosion/thinc/pull/116,"spacy-transformers revealed a few bugs in the PyTorch wrapper, especially around saving and loading across GPU and CPU. This PR pulls in those changes, and also simplifies the wrapper classes quite a lot, by introducing new methods that subclasses can customize to control the input and output to and from PyTorch.","spacy-transformers revealed a few bugs in the PyTorch wrapper, especially around saving and loading across GPU and CPU. This PR pulls in those changes, and also simplifies the wrapper classes quite a lot, by introducing new methods that subclasses can customize to control the input and output to and from PyTorch.",True,{}
explosion/thinc,https://github.com/explosion/thinc,117,2019-10-18T23:35:28Z,2019-10-19T00:06:11Z,2021-09-03T09:50:46Z,MERGED,True,506,16,6,https://github.com/honnibal,Ditch thinc_gpu_ops for simpler GPU install,14,[],https://github.com/explosion/thinc/pull/117,https://github.com/honnibal,1,https://github.com/explosion/thinc/pull/117,"spaCy and Thinc have had one last pesky non-wheeled module: thinc_gpu_ops, which needed to be compiled with a GPU card to get the GPU support working. This was bad news on Windows, as it forced Windows users to set up a compiler toolchain.
This patch replaces the thinc_gpu_ops, instead bringing the kernels into Thinc and using cupy's RawKernel to compile them at runtime.
Many Windows users had previously found it difficult to use the GPU support. This should fix the problem.
I've also improved the kernels somewhat, although they weren't a bottleneck, so I doubt it'll make much performance difference. If we do hit a new bug in the GPU code though, that will be why.
I've preferred to keep separate .cu source files, instead of embedding the strings into the Python source. Having code as strings feels pretty nasty. Instead I just read in the source file, and segment it with some regexes.
Closes #51, explosion/spaCy#4088","spaCy and Thinc have had one last pesky non-wheeled module: thinc_gpu_ops, which needed to be compiled with a GPU card to get the GPU support working. This was bad news on Windows, as it forced Windows users to set up a compiler toolchain.
This patch replaces the thinc_gpu_ops, instead bringing the kernels into Thinc and using cupy's RawKernel to compile them at runtime.
Many Windows users had previously found it difficult to use the GPU support. This should fix the problem.
I've also improved the kernels somewhat, although they weren't a bottleneck, so I doubt it'll make much performance difference. If we do hit a new bug in the GPU code though, that will be why.
I've preferred to keep separate .cu source files, instead of embedding the strings into the Python source. Having code as strings feels pretty nasty. Instead I just read in the source file, and segment it with some regexes.
Closes #51, explosion/spaCy#4088",True,{'HEART': ['https://github.com/svlandeg']}
explosion/thinc,https://github.com/explosion/thinc,117,2019-10-18T23:35:28Z,2019-10-19T00:06:11Z,2021-09-03T09:50:46Z,MERGED,True,506,16,6,https://github.com/honnibal,Ditch thinc_gpu_ops for simpler GPU install,14,[],https://github.com/explosion/thinc/pull/117,https://github.com/svlandeg,2,https://github.com/explosion/thinc/pull/117#issuecomment-544170029,"spaCy and Thinc have had one last pesky non-wheeled module: thinc_gpu_ops, which needed to be compiled with a GPU card to get the GPU support working. This was bad news on Windows, as it forced Windows users to set up a compiler toolchain.
This patch replaces the thinc_gpu_ops, instead bringing the kernels into Thinc and using cupy's RawKernel to compile them at runtime.
Many Windows users had previously found it difficult to use the GPU support. This should fix the problem.
I've also improved the kernels somewhat, although they weren't a bottleneck, so I doubt it'll make much performance difference. If we do hit a new bug in the GPU code though, that will be why.
I've preferred to keep separate .cu source files, instead of embedding the strings into the Python source. Having code as strings feels pretty nasty. Instead I just read in the source file, and segment it with some regexes.
Closes #51, explosion/spaCy#4088",Windows users thank you ,True,{}
explosion/thinc,https://github.com/explosion/thinc,118,2019-10-19T11:51:34Z,2019-10-19T16:09:34Z,2019-10-19T16:09:36Z,MERGED,True,369,182,4,https://github.com/honnibal,Fix ExtractWindow nW>=2 (closes #47),13,[],https://github.com/explosion/thinc/pull/118,https://github.com/honnibal,1,https://github.com/explosion/thinc/pull/118,Past time this embarrassing bug got squashed. This might improve the performance of seq2col on GPU as well.,Past time this embarrassing bug got squashed. This might improve the performance of seq2col on GPU as well.,True,{}
explosion/thinc,https://github.com/explosion/thinc,119,2019-10-22T00:07:38Z,2019-10-27T13:15:48Z,2020-01-22T00:55:47Z,MERGED,True,124,5,3,https://github.com/honnibal,New optimizer features,17,[],https://github.com/explosion/thinc/pull/119,https://github.com/honnibal,1,https://github.com/explosion/thinc/pull/119,"Add a grab-bag of new optimizer stuff. Lookahead seems promising, RAdam isn't helping so far (maybe my implementation is bad)?
I got this stuff from folks on the FastAI forum, e.g.: https://github.com/lessw2020/Ranger-Mish-ImageWoof-5
Everything should be disabled by default, so merging should be safe.","Add a grab-bag of new optimizer stuff. Lookahead seems promising, RAdam isn't helping so far (maybe my implementation is bad)?
I got this stuff from folks on the FastAI forum, e.g.: https://github.com/lessw2020/Ranger-Mish-ImageWoof-5
Everything should be disabled by default, so merging should be safe.",True,{}
explosion/thinc,https://github.com/explosion/thinc,120,2019-10-22T00:12:09Z,2019-10-24T11:00:19Z,2020-01-22T00:55:48Z,MERGED,True,282,6,6,https://github.com/honnibal,Add mish activation,13,[],https://github.com/explosion/thinc/pull/120,https://github.com/honnibal,1,https://github.com/explosion/thinc/pull/120,"Mish is a new activation function, a bit like swish. The experiments are pretty convincing and the shape of the activation makes sense.
It's a bit more expensive than ReLU, so I've added CUDA kernels, and implemented an elementwise loop for CPU. I haven't benchmarked those yet, but the speed seems okay.","Mish is a new activation function, a bit like swish. The experiments are pretty convincing and the shape of the activation makes sense.
It's a bit more expensive than ReLU, so I've added CUDA kernels, and implemented an elementwise loop for CPU. I haven't benchmarked those yet, but the speed seems okay.",True,"{'THUMBS_UP': ['https://github.com/svlandeg', 'https://github.com/justindujardin'], 'HOORAY': ['https://github.com/justindujardin']}"
explosion/thinc,https://github.com/explosion/thinc,121,2019-11-12T12:07:14Z,2019-11-14T22:50:41Z,2019-11-14T22:50:43Z,MERGED,True,10,1,1,https://github.com/ines,Add CI for Python 3.8,2,[],https://github.com/explosion/thinc/pull/121,https://github.com/ines,1,https://github.com/explosion/thinc/pull/121,,,True,{}
explosion/thinc,https://github.com/explosion/thinc,122,2019-11-19T20:43:33Z,2019-11-28T13:33:11Z,2019-11-28T13:33:28Z,MERGED,True,1,1,1,https://github.com/mmaybeno,Fix cupy to cuda 10.1 version,1,[],https://github.com/explosion/thinc/pull/122,https://github.com/mmaybeno,1,https://github.com/explosion/thinc/pull/122,I think this is a typo but wanted to make sure. cupy has a different version of CUDA 10.1 and therefore unable to run thinc[cuda101].,I think this is a typo but wanted to make sure. cupy has a different version of CUDA 10.1 and therefore unable to run thinc[cuda101].,True,{'THUMBS_UP': ['https://github.com/honnibal']}
explosion/thinc,https://github.com/explosion/thinc,122,2019-11-19T20:43:33Z,2019-11-28T13:33:11Z,2019-11-28T13:33:28Z,MERGED,True,1,1,1,https://github.com/mmaybeno,Fix cupy to cuda 10.1 version,1,[],https://github.com/explosion/thinc/pull/122,https://github.com/honnibal,2,https://github.com/explosion/thinc/pull/122#issuecomment-559496364,I think this is a typo but wanted to make sure. cupy has a different version of CUDA 10.1 and therefore unable to run thinc[cuda101].,Thanks! Could've sworn I merged this fix before...,True,{}
explosion/thinc,https://github.com/explosion/thinc,123,2019-11-20T19:13:04Z,2019-11-23T13:54:24Z,2019-11-23T13:54:24Z,MERGED,True,19,5,3,https://github.com/adrianeboyd,Add destructor handling for Beam,2,[],https://github.com/explosion/thinc/pull/123,https://github.com/adrianeboyd,1,https://github.com/explosion/thinc/pull/123,"Add destructor handling for Beam, which is required for explosion/spaCy#4686.","Add destructor handling for Beam, which is required for explosion/spaCy#4686.",True,{'THUMBS_UP': ['https://github.com/honnibal']}
explosion/thinc,https://github.com/explosion/thinc,124,2019-11-20T19:32:02Z,2019-11-28T17:41:50Z,2019-11-28T17:41:50Z,MERGED,True,79,48,3,https://github.com/adrianeboyd,Make Model operators and device ops thread-local,7,[],https://github.com/explosion/thinc/pull/124,https://github.com/adrianeboyd,1,https://github.com/explosion/thinc/pull/124,"Make old/saved Model operators thread-local, too. Doesn't seem like a particularly great solution, but seems to least temporarily fix explosion/spaCy#4349. I think better solutions would require a fairly major restructuring of Model.","Make old/saved Model operators thread-local, too. Doesn't seem like a particularly great solution, but seems to least temporarily fix explosion/spaCy#4349. I think better solutions would require a fairly major restructuring of Model.",True,{}
explosion/thinc,https://github.com/explosion/thinc,124,2019-11-20T19:32:02Z,2019-11-28T17:41:50Z,2019-11-28T17:41:50Z,MERGED,True,79,48,3,https://github.com/adrianeboyd,Make Model operators and device ops thread-local,7,[],https://github.com/explosion/thinc/pull/124,https://github.com/adrianeboyd,2,https://github.com/explosion/thinc/pull/124#issuecomment-556287791,"Make old/saved Model operators thread-local, too. Doesn't seem like a particularly great solution, but seems to least temporarily fix explosion/spaCy#4349. I think better solutions would require a fairly major restructuring of Model.",I suspect use_device() has similar problems.,True,{}
explosion/thinc,https://github.com/explosion/thinc,124,2019-11-20T19:32:02Z,2019-11-28T17:41:50Z,2019-11-28T17:41:50Z,MERGED,True,79,48,3,https://github.com/adrianeboyd,Make Model operators and device ops thread-local,7,[],https://github.com/explosion/thinc/pull/124,https://github.com/adrianeboyd,3,https://github.com/explosion/thinc/pull/124#issuecomment-556337714,"Make old/saved Model operators thread-local, too. Doesn't seem like a particularly great solution, but seems to least temporarily fix explosion/spaCy#4349. I think better solutions would require a fairly major restructuring of Model.","If I try the same test outside pytest it does throw an UndefinedOperatorError:
from thinc.neural._classes.model import Model

op = ""+""
m1 = Model(name=""a"")
m2 = Model(name=""b"")
with Model.define_operators({op: lambda a, b: a.name + b.name}):
    if op == ""+"":
        value = m1 + m2
    else:
        value = m1 + m2
    if op == ""-"":
        value = m1 - m2
    else:
        value = m1 - m2 # raises UndefinedOperatorError

Hmm, it looks like the operator * is hanging around from test_overload_operators_in_subthread(), which seems weird, although potentially related, of course. I'm not really sure what's going on?",True,{}
explosion/thinc,https://github.com/explosion/thinc,124,2019-11-20T19:32:02Z,2019-11-28T17:41:50Z,2019-11-28T17:41:50Z,MERGED,True,79,48,3,https://github.com/adrianeboyd,Make Model operators and device ops thread-local,7,[],https://github.com/explosion/thinc/pull/124,https://github.com/adrianeboyd,4,https://github.com/explosion/thinc/pull/124#issuecomment-557046266,"Make old/saved Model operators thread-local, too. Doesn't seem like a particularly great solution, but seems to least temporarily fix explosion/spaCy#4349. I think better solutions would require a fairly major restructuring of Model.","I initially thought this setup was doomed to fail with _operators as a class variable, but I think I can get it to work by reworking the thread-local definitions a bit. It's still kind of strange overall.",True,{}
explosion/thinc,https://github.com/explosion/thinc,124,2019-11-20T19:32:02Z,2019-11-28T17:41:50Z,2019-11-28T17:41:50Z,MERGED,True,79,48,3,https://github.com/adrianeboyd,Make Model operators and device ops thread-local,7,[],https://github.com/explosion/thinc/pull/124,https://github.com/adrianeboyd,5,https://github.com/explosion/thinc/pull/124#issuecomment-557077513,"Make old/saved Model operators thread-local, too. Doesn't seem like a particularly great solution, but seems to least temporarily fix explosion/spaCy#4349. I think better solutions would require a fairly major restructuring of Model.","With CupyOps this test fails: test_backprop_sum_pool(), but that seems unrelated to these changes.",True,{}
explosion/thinc,https://github.com/explosion/thinc,124,2019-11-20T19:32:02Z,2019-11-28T17:41:50Z,2019-11-28T17:41:50Z,MERGED,True,79,48,3,https://github.com/adrianeboyd,Make Model operators and device ops thread-local,7,[],https://github.com/explosion/thinc/pull/124,https://github.com/adrianeboyd,6,https://github.com/explosion/thinc/pull/124#issuecomment-557085840,"Make old/saved Model operators thread-local, too. Doesn't seem like a particularly great solution, but seems to least temporarily fix explosion/spaCy#4349. I think better solutions would require a fairly major restructuring of Model.",Is with Model.use_device() ever nested like with Model.define_operators()?,True,{}
explosion/thinc,https://github.com/explosion/thinc,124,2019-11-20T19:32:02Z,2019-11-28T17:41:50Z,2019-11-28T17:41:50Z,MERGED,True,79,48,3,https://github.com/adrianeboyd,Make Model operators and device ops thread-local,7,[],https://github.com/explosion/thinc/pull/124,https://github.com/honnibal,7,https://github.com/explosion/thinc/pull/124#issuecomment-559570854,"Make old/saved Model operators thread-local, too. Doesn't seem like a particularly great solution, but seems to least temporarily fix explosion/spaCy#4349. I think better solutions would require a fairly major restructuring of Model.","@adrianeboyd No we don't nest with Mode.use_device(). I think that design should probably be revised anyway.
Agree it's a bit ugly, but I think it's the best we can do.",True,{}
explosion/thinc,https://github.com/explosion/thinc,125,2019-11-27T05:35:04Z,2019-11-27T14:15:17Z,2019-11-28T11:52:35Z,MERGED,True,1,1,1,https://github.com/kmaehashi,CuPy package for CUDA 10.1 is `cupy-cuda101`,1,[],https://github.com/explosion/thinc/pull/125,https://github.com/kmaehashi,1,https://github.com/explosion/thinc/pull/125,"Currently cupy-cuda110 does not exist.
https://docs-cupy.chainer.org/en/latest/install.html#install-cupy
https://pypi.org/project/cupy-cuda101/","Currently cupy-cuda110 does not exist.
https://docs-cupy.chainer.org/en/latest/install.html#install-cupy
https://pypi.org/project/cupy-cuda101/",True,{}
explosion/thinc,https://github.com/explosion/thinc,125,2019-11-27T05:35:04Z,2019-11-27T14:15:17Z,2019-11-28T11:52:35Z,MERGED,True,1,1,1,https://github.com/kmaehashi,CuPy package for CUDA 10.1 is `cupy-cuda101`,1,[],https://github.com/explosion/thinc/pull/125,https://github.com/honnibal,2,https://github.com/explosion/thinc/pull/125#issuecomment-559104809,"Currently cupy-cuda110 does not exist.
https://docs-cupy.chainer.org/en/latest/install.html#install-cupy
https://pypi.org/project/cupy-cuda101/","Oops, thanks!",True,{'THUMBS_UP': ['https://github.com/kmaehashi']}
explosion/thinc,https://github.com/explosion/thinc,126,2019-12-07T20:42:27Z,2019-12-09T20:36:21Z,2020-01-22T00:55:44Z,MERGED,True,600,32,15,https://github.com/honnibal,Add Config class,17,[],https://github.com/explosion/thinc/pull/126,https://github.com/honnibal,1,https://github.com/explosion/thinc/pull/126,"With the move to use catalogue for extended configuration in spaCy, we need a solution for configuration management. It makes sense for the solution to live in Thinc, because Thinc's optimizers, layers etc should all work with the config management.
I looked closely at all of the options for config management. The config files will get pretty big, so we shouldn't just use JSON. YAML is terrible. XML would work but it'll involve a fair bit of code. TOML and HCL don't have first-part support in Python, and they're a bit less common.
I'm proposing a solution based on Python's configparser module, with a couple of small conventions.
Configparser's format is an ini-like format that maps into a two-level dict:
[outer key 1]
inner key 1a = 1
inner key 1b = 2

[outer key 2]
inner key 2a = 3
inner key 2b = 4

Produces:
{
  ""outer key 1"": {
    ""inner key 1a"": ""1"",
    ""inner key 1b"": ""2""
  },
  ""outer key 2"": {
    ""outer key 2a"": ""3"",
    ""outer key 2b"": ""4""
  }
}

Our configs need nested values. I propose we do these via convention, transforming sections like parser.model so that they become config[""parser""][""model""]. This actually works out pretty nicely: the file is just a long list of leaf-data, with the structure handled in the titles. It's also pretty easy to read and write the data this way.
I've also added some simple value interpretation logic, so that floats, ints and booleans come back as the correct values. The configparser supports unquoted strings in the values, which I think is pretty sloppy...I think we probably want to raise on those. That would also allow us to add some extra syntax for paths.
The other bit of convention I've introduced is using @ to denote that a section refers to a catalogue entry. The configparser format supports both : and = as delimiters. I'm proposing using : only for the @ key.
Finally, configparser also supports variable interpolation. You can refer to a different value in the config by writing ${other_section:other_key}. If you're referring to a value within the same section, you don't need to write the section name.","With the move to use catalogue for extended configuration in spaCy, we need a solution for configuration management. It makes sense for the solution to live in Thinc, because Thinc's optimizers, layers etc should all work with the config management.
I looked closely at all of the options for config management. The config files will get pretty big, so we shouldn't just use JSON. YAML is terrible. XML would work but it'll involve a fair bit of code. TOML and HCL don't have first-part support in Python, and they're a bit less common.
I'm proposing a solution based on Python's configparser module, with a couple of small conventions.
Configparser's format is an ini-like format that maps into a two-level dict:
[outer key 1]
inner key 1a = 1
inner key 1b = 2

[outer key 2]
inner key 2a = 3
inner key 2b = 4

Produces:
{
  ""outer key 1"": {
    ""inner key 1a"": ""1"",
    ""inner key 1b"": ""2""
  },
  ""outer key 2"": {
    ""outer key 2a"": ""3"",
    ""outer key 2b"": ""4""
  }
}

Our configs need nested values. I propose we do these via convention, transforming sections like parser.model so that they become config[""parser""][""model""]. This actually works out pretty nicely: the file is just a long list of leaf-data, with the structure handled in the titles. It's also pretty easy to read and write the data this way.
I've also added some simple value interpretation logic, so that floats, ints and booleans come back as the correct values. The configparser supports unquoted strings in the values, which I think is pretty sloppy...I think we probably want to raise on those. That would also allow us to add some extra syntax for paths.
The other bit of convention I've introduced is using @ to denote that a section refers to a catalogue entry. The configparser format supports both : and = as delimiters. I'm proposing using : only for the @ key.
Finally, configparser also supports variable interpolation. You can refer to a different value in the config by writing ${other_section:other_key}. If you're referring to a value within the same section, you don't need to write the section name.",True,{}
explosion/thinc,https://github.com/explosion/thinc,126,2019-12-07T20:42:27Z,2019-12-09T20:36:21Z,2020-01-22T00:55:44Z,MERGED,True,600,32,15,https://github.com/honnibal,Add Config class,17,[],https://github.com/explosion/thinc/pull/126,https://github.com/svlandeg,2,https://github.com/explosion/thinc/pull/126#issuecomment-562891170,"With the move to use catalogue for extended configuration in spaCy, we need a solution for configuration management. It makes sense for the solution to live in Thinc, because Thinc's optimizers, layers etc should all work with the config management.
I looked closely at all of the options for config management. The config files will get pretty big, so we shouldn't just use JSON. YAML is terrible. XML would work but it'll involve a fair bit of code. TOML and HCL don't have first-part support in Python, and they're a bit less common.
I'm proposing a solution based on Python's configparser module, with a couple of small conventions.
Configparser's format is an ini-like format that maps into a two-level dict:
[outer key 1]
inner key 1a = 1
inner key 1b = 2

[outer key 2]
inner key 2a = 3
inner key 2b = 4

Produces:
{
  ""outer key 1"": {
    ""inner key 1a"": ""1"",
    ""inner key 1b"": ""2""
  },
  ""outer key 2"": {
    ""outer key 2a"": ""3"",
    ""outer key 2b"": ""4""
  }
}

Our configs need nested values. I propose we do these via convention, transforming sections like parser.model so that they become config[""parser""][""model""]. This actually works out pretty nicely: the file is just a long list of leaf-data, with the structure handled in the titles. It's also pretty easy to read and write the data this way.
I've also added some simple value interpretation logic, so that floats, ints and booleans come back as the correct values. The configparser supports unquoted strings in the values, which I think is pretty sloppy...I think we probably want to raise on those. That would also allow us to add some extra syntax for paths.
The other bit of convention I've introduced is using @ to denote that a section refers to a catalogue entry. The configparser format supports both : and = as delimiters. I'm proposing using : only for the @ key.
Finally, configparser also supports variable interpolation. You can refer to a different value in the config by writing ${other_section:other_key}. If you're referring to a value within the same section, you don't need to write the section name.","This looks pretty neat!

You can refer to a different value in the config by writing ${other_section:other_key}.

I don't suppose you could also do basic numeric operations like
width = ${other_section1:width1} + ${other_section2:width2} ?
If not - wouldn't there be cases where we need that kind of functionality?",True,{}
explosion/thinc,https://github.com/explosion/thinc,126,2019-12-07T20:42:27Z,2019-12-09T20:36:21Z,2020-01-22T00:55:44Z,MERGED,True,600,32,15,https://github.com/honnibal,Add Config class,17,[],https://github.com/explosion/thinc/pull/126,https://github.com/honnibal,3,https://github.com/explosion/thinc/pull/126#issuecomment-562968941,"With the move to use catalogue for extended configuration in spaCy, we need a solution for configuration management. It makes sense for the solution to live in Thinc, because Thinc's optimizers, layers etc should all work with the config management.
I looked closely at all of the options for config management. The config files will get pretty big, so we shouldn't just use JSON. YAML is terrible. XML would work but it'll involve a fair bit of code. TOML and HCL don't have first-part support in Python, and they're a bit less common.
I'm proposing a solution based on Python's configparser module, with a couple of small conventions.
Configparser's format is an ini-like format that maps into a two-level dict:
[outer key 1]
inner key 1a = 1
inner key 1b = 2

[outer key 2]
inner key 2a = 3
inner key 2b = 4

Produces:
{
  ""outer key 1"": {
    ""inner key 1a"": ""1"",
    ""inner key 1b"": ""2""
  },
  ""outer key 2"": {
    ""outer key 2a"": ""3"",
    ""outer key 2b"": ""4""
  }
}

Our configs need nested values. I propose we do these via convention, transforming sections like parser.model so that they become config[""parser""][""model""]. This actually works out pretty nicely: the file is just a long list of leaf-data, with the structure handled in the titles. It's also pretty easy to read and write the data this way.
I've also added some simple value interpretation logic, so that floats, ints and booleans come back as the correct values. The configparser supports unquoted strings in the values, which I think is pretty sloppy...I think we probably want to raise on those. That would also allow us to add some extra syntax for paths.
The other bit of convention I've introduced is using @ to denote that a section refers to a catalogue entry. The configparser format supports both : and = as delimiters. I'm proposing using : only for the @ key.
Finally, configparser also supports variable interpolation. You can refer to a different value in the config by writing ${other_section:other_key}. If you're referring to a value within the same section, you don't need to write the section name.","I don't suppose you could also do basic numeric operations like
width = ${other_section1:width1} + ${other_section2:width2} ?
If not - wouldn't there be cases where we need that kind of functionality?

Yeah I've been thinking about that. I do think we'll want something like that. Not sure what the best way to go about it is. One solution would be to encourage use of a templating language? For instance you could use jinja2 to generate the template: https://jinja.palletsprojects.com/en/master/templates/#math",True,{}
explosion/thinc,https://github.com/explosion/thinc,127,2019-12-16T19:13:07Z,2019-12-18T09:52:30Z,2019-12-18T09:52:30Z,MERGED,True,5,5,1,https://github.com/adrianeboyd,Revert thread-local ops tests,1,[],https://github.com/explosion/thinc/pull/127,https://github.com/adrianeboyd,1,https://github.com/explosion/thinc/pull/127,Revert thread-local ops tests,Revert thread-local ops tests,True,{}
explosion/thinc,https://github.com/explosion/thinc,129,2019-12-24T13:46:39Z,2019-12-25T11:51:59Z,2019-12-25T11:55:22Z,MERGED,True,476,109,13,https://github.com/honnibal,Use pydantic to fill config defaults. Drop old Pythons,15,[],https://github.com/explosion/thinc/pull/129,https://github.com/honnibal,1,https://github.com/explosion/thinc/pull/129,"Add pydantic as a dependency
Implement validation and default filling for the config system
Add a few type declarations to some registry functions
Remove Python2.7 and 3.5 from the classifiers and build system","Add pydantic as a dependency
Implement validation and default filling for the config system
Add a few type declarations to some registry functions
Remove Python2.7 and 3.5 from the classifiers and build system",True,{}
explosion/thinc,https://github.com/explosion/thinc,130,2019-12-25T12:17:08Z,2019-12-25T13:22:49Z,2020-01-22T00:55:43Z,MERGED,True,16,391,16,https://github.com/honnibal,Remove unused modules and code,11,[],https://github.com/explosion/thinc/pull/130,https://github.com/honnibal,1,https://github.com/explosion/thinc/pull/130,"Remove thinc.neural._lsuv module
Remove thinc.neural._aligned_alloc module
Remove thinc/cpu.pxd and thinc/neural/cpu.pxd
Remove thinc/neural/vec2vec.py, thinc/neural/vecs2vec.py, thinc/neural/vecs2vecs.py","Remove thinc.neural._lsuv module
Remove thinc.neural._aligned_alloc module
Remove thinc/cpu.pxd and thinc/neural/cpu.pxd
Remove thinc/neural/vec2vec.py, thinc/neural/vecs2vec.py, thinc/neural/vecs2vecs.py",True,{}
explosion/thinc,https://github.com/explosion/thinc,131,2019-12-25T14:31:17Z,2019-12-25T14:41:13Z,2020-01-22T00:55:41Z,MERGED,True,94,30,2,https://github.com/honnibal," Refactor dims, grads and params system",2,[],https://github.com/explosion/thinc/pull/131,https://github.com/honnibal,1,https://github.com/explosion/thinc/pull/131,"Classes usually define their dimension, gradients and parameter
attributes using the @describe.attributes class decorator. This
decorator attaches descriptor objects to the class, allowing some
indirection features such as late definintion of dimensions.
The previous implementation was confusing, because important logic was
implemented within the descriptors, instead of on the Model class. This
PR refactors that logic into the Model class, and has the descriptors
delegate into those methods.
The new methods on Model are:
model.{has, get, set}_{dim, param, grad}()

model.has_dim(name: str) -> bool: Whether the model has that dimension
model.get_param(name: str) -> array: Get the parameter.
model.set_grad(name: str, value: array) -> None: Set the gradient.","Classes usually define their dimension, gradients and parameter
attributes using the @describe.attributes class decorator. This
decorator attaches descriptor objects to the class, allowing some
indirection features such as late definintion of dimensions.
The previous implementation was confusing, because important logic was
implemented within the descriptors, instead of on the Model class. This
PR refactors that logic into the Model class, and has the descriptors
delegate into those methods.
The new methods on Model are:
model.{has, get, set}_{dim, param, grad}()

model.has_dim(name: str) -> bool: Whether the model has that dimension
model.get_param(name: str) -> array: Get the parameter.
model.set_grad(name: str, value: array) -> None: Set the gradient.",True,{}
explosion/thinc,https://github.com/explosion/thinc,132,2019-12-25T14:47:12Z,2019-12-25T15:01:18Z,2019-12-25T15:01:24Z,MERGED,True,433,1262,120,https://github.com/ines,Finish update to Python 3.6+,14,['enhancement'],https://github.com/explosion/thinc/pull/132,https://github.com/ines,1,https://github.com/explosion/thinc/pull/132,"remove unicode declarations
 update CI
 remove pathlib backport dependency
 remove compat helpers
 replace ordered dict
 use f-strings if possible","remove unicode declarations
 update CI
 remove pathlib backport dependency
 remove compat helpers
 replace ordered dict
 use f-strings if possible",True,{'HOORAY': ['https://github.com/svlandeg']}
explosion/thinc,https://github.com/explosion/thinc,133,2019-12-25T15:17:20Z,2019-12-25T15:23:49Z,2019-12-25T15:23:53Z,MERGED,True,28,5188,15,https://github.com/honnibal,Remove 'checks' system,6,['enhancement'],https://github.com/explosion/thinc/pull/133,https://github.com/honnibal,1,https://github.com/explosion/thinc/pull/133,"Drop the 'checks' decorator system and associated errors. These annotations will be better handled with Python 3 type checking.
Also lets us drop the wrapt vendored dependency.","Drop the 'checks' decorator system and associated errors. These annotations will be better handled with Python 3 type checking.
Also lets us drop the wrapt vendored dependency.",True,{}
explosion/thinc,https://github.com/explosion/thinc,134,2019-12-25T15:22:41Z,2019-12-25T15:37:11Z,2019-12-25T15:37:15Z,MERGED,True,77,143,31,https://github.com/ines,Tidy up tests,4,[],https://github.com/explosion/thinc/pull/134,https://github.com/ines,1,https://github.com/explosion/thinc/pull/134,,,True,{}
explosion/thinc,https://github.com/explosion/thinc,135,2019-12-25T15:51:57Z,2019-12-25T16:16:07Z,2019-12-25T16:18:12Z,MERGED,True,127,125,8,https://github.com/ines,Clean setup and add setup.cfg,3,[],https://github.com/explosion/thinc/pull/135,https://github.com/ines,1,https://github.com/explosion/thinc/pull/135,"add setup.cfg
 clean up setup.py","add setup.cfg
 clean up setup.py",True,{}
explosion/thinc,https://github.com/explosion/thinc,136,2019-12-25T16:01:39Z,2019-12-25T16:15:53Z,2019-12-25T16:17:55Z,MERGED,True,0,1344,7,https://github.com/honnibal,Remove some include files,1,[],https://github.com/explosion/thinc/pull/136,https://github.com/honnibal,1,https://github.com/explosion/thinc/pull/136,,,True,{}
explosion/thinc,https://github.com/explosion/thinc,137,2019-12-25T16:28:16Z,2019-12-25T17:38:22Z,2020-01-22T00:55:39Z,MERGED,True,2,10527,30,https://github.com/honnibal,Try to decruft setup.py,5,[],https://github.com/explosion/thinc/pull/137,https://github.com/honnibal,1,https://github.com/explosion/thinc/pull/137,,,True,{}
explosion/thinc,https://github.com/explosion/thinc,138,2019-12-26T13:17:12Z,2019-12-26T13:33:44Z,2020-01-22T00:55:37Z,MERGED,True,588,519,10,https://github.com/honnibal,Refactor layer composition functions,15,[],https://github.com/explosion/thinc/pull/138,https://github.com/honnibal,1,https://github.com/explosion/thinc/pull/138,"Move code out of the thinc.api module, and refactor some of the main combinators.
This PR also fixes a long-standing inefficiency in the concatenate operator. When we define concatenations like (a | b | c), that gets mapped into concatenate(concatenate(a, b), c). It's more efficient to flatten that to concatenate(a, b, c). This PR takes care of that, by adding a create_variadic helper function to flatten this type of nested call to functions which can take variable numbers of arguments.","Move code out of the thinc.api module, and refactor some of the main combinators.
This PR also fixes a long-standing inefficiency in the concatenate operator. When we define concatenations like (a | b | c), that gets mapped into concatenate(concatenate(a, b), c). It's more efficient to flatten that to concatenate(a, b, c). This PR takes care of that, by adding a create_variadic helper function to flatten this type of nested call to functions which can take variable numbers of arguments.",True,{}
explosion/thinc,https://github.com/explosion/thinc,139,2019-12-26T13:38:14Z,2019-12-26T13:57:08Z,2020-01-22T00:55:29Z,MERGED,True,104,481,27,https://github.com/honnibal,Remove unused layers,9,[],https://github.com/explosion/thinc/pull/139,https://github.com/honnibal,1,https://github.com/explosion/thinc/pull/139,"Remove several unused layers:

BatchNorm
ELU
SELU

The BatchNorm layer is the only one that people might miss. However, we don't use it in any of our models, and I don't know that it's correct. I don't recommend BatchNorm for NLP in general, because it's really hard to reason about. Best not to carry this implementation if we don't use it and can't vouch for it.
This PR also moves all current examples under a subdirectory, examples/old/. Examples should be moved back as they're updated.","Remove several unused layers:

BatchNorm
ELU
SELU

The BatchNorm layer is the only one that people might miss. However, we don't use it in any of our models, and I don't know that it's correct. I don't recommend BatchNorm for NLP in general, because it's really hard to reason about. Best not to carry this implementation if we don't use it and can't vouch for it.
This PR also moves all current examples under a subdirectory, examples/old/. Examples should be moved back as they're updated.",True,{}
explosion/thinc,https://github.com/explosion/thinc,140,2019-12-26T14:09:31Z,2019-12-26T14:17:22Z,2020-01-22T00:55:27Z,MERGED,True,1,56,3,https://github.com/honnibal,Remove Trainer class,2,[],https://github.com/explosion/thinc/pull/140,https://github.com/honnibal,1,https://github.com/explosion/thinc/pull/140,"Remove the outdated thinc.neural.train.Trainer class. This was used in a lot of examples, and it means the model.begin_training() method is no longer a context manager, so some code will need to be updated.","Remove the outdated thinc.neural.train.Trainer class. This was used in a lot of examples, and it means the model.begin_training() method is no longer a context manager, so some code will need to be updated.",True,{}
explosion/thinc,https://github.com/explosion/thinc,141,2019-12-26T15:27:33Z,2019-12-26T15:34:08Z,2020-01-22T00:55:25Z,MERGED,True,89,188,14,https://github.com/honnibal,"Remove unnecessary ""hooks"" stuff from Model",12,[],https://github.com/explosion/thinc/pull/141,https://github.com/honnibal,1,https://github.com/explosion/thinc/pull/141,"The Model class supported some weird features with class attributes, using the description system. This PR cleans the unnecessary stuff up.
We no longer have on_init_hooks, and on_data_hooks is removed as a descriptor -- instead it should just be set in the model __init__ function.
We also add a model.infer_dimensions method, to allow clearer calls if that's what the code is trying to do.","The Model class supported some weird features with class attributes, using the description system. This PR cleans the unnecessary stuff up.
We no longer have on_init_hooks, and on_data_hooks is removed as a descriptor -- instead it should just be set in the model __init__ function.
We also add a model.infer_dimensions method, to allow clearer calls if that's what the code is trying to do.",True,{}
explosion/thinc,https://github.com/explosion/thinc,142,2019-12-26T15:56:29Z,2019-12-26T16:22:11Z,2020-01-22T00:55:23Z,MERGED,True,64,91,14,https://github.com/honnibal,Refactor descriptors,3,[],https://github.com/explosion/thinc/pull/142,https://github.com/honnibal,1,https://github.com/explosion/thinc/pull/142,,,True,{}
explosion/thinc,https://github.com/explosion/thinc,143,2019-12-26T18:13:21Z,2020-01-28T20:34:17Z,2020-01-28T20:34:17Z,MERGED,True,20519,32422,339,https://github.com/honnibal,"Big refactor, document, and drop old Pythons",1117,['enhancement'],https://github.com/explosion/thinc/pull/143,https://github.com/honnibal,1,https://github.com/explosion/thinc/pull/143,,,True,"{'HEART': ['https://github.com/DeNeutoy', 'https://github.com/svlandeg', 'https://github.com/bdewilde', 'https://github.com/ines', 'https://github.com/Abhijit-2592', 'https://github.com/justindujardin']}"
explosion/thinc,https://github.com/explosion/thinc,143,2019-12-26T18:13:21Z,2020-01-28T20:34:17Z,2020-01-28T20:34:17Z,MERGED,True,20519,32422,339,https://github.com/honnibal,"Big refactor, document, and drop old Pythons",1117,['enhancement'],https://github.com/explosion/thinc/pull/143,https://github.com/apps/codecov,2,https://github.com/explosion/thinc/pull/143#issuecomment-571843147,,"Codecov Report

 No coverage uploaded for pull request base (master@5357ca5). Click here to learn what that means.
The diff coverage is n/a.


@@            Coverage Diff            @@
##             master     #143   +/-   ##
=========================================
  Coverage          ?   87.96%           
=========================================
  Files             ?       67           
  Lines             ?     4329           
  Branches          ?        0           
=========================================
  Hits              ?     3808           
  Misses            ?      521           
  Partials          ?        0



Impacted Files
Coverage 





thinc/layers/with_flatten.py
37.5% <> ()



thinc/layers/maxout.py
100% <> ()



thinc/__init__.py
100% <> ()



thinc/layers/with_reshape.py
23.07% <> ()



thinc/layers/hashembed.py
95.91% <> ()



thinc/layers/add.py
100% <> ()



thinc/about.py
100% <> ()



thinc/shims/shim.py
100% <> ()



thinc/layers/padded2list.py
100% <> ()



thinc/layers/with_padded.py
80.51% <> ()



... and 56 more






Continue to review full report at Codecov.

Legend - Click here to learn more
 = absolute <relative> (impact),  = not affected, ? = missing data
Powered by Codecov. Last update 5357ca5...3303b0e. Read the comment docs.",True,{}
explosion/thinc,https://github.com/explosion/thinc,144,2019-12-26T19:53:21Z,2019-12-26T20:05:53Z,2020-01-22T00:55:22Z,MERGED,True,108,120,23,https://github.com/honnibal,Remove sgd argument from backward callback,3,[],https://github.com/explosion/thinc/pull/144,https://github.com/honnibal,1,https://github.com/explosion/thinc/pull/144,"This is a pretty pervasive breaking change. Previously we had the following in our models:
Y, backward = model.begin_update(X)
dX = backward(dY, sgd=optimizer)

After this PR, we'll have this instead:
Y, backward = model.begin_update(X)
dX = backward(dY)
model.finish_update(sgd)

There are a number of reasons why the old way was bad. One is that we were changing the weights in the middle of the backward pass, which could introduce some nasty problems, especially when you had architectures with the same model instance twice. It also made it harder to divide up sub-batches, and just generally complicated the design (it made the signatures really ugly, for instance).","This is a pretty pervasive breaking change. Previously we had the following in our models:
Y, backward = model.begin_update(X)
dX = backward(dY, sgd=optimizer)

After this PR, we'll have this instead:
Y, backward = model.begin_update(X)
dX = backward(dY)
model.finish_update(sgd)

There are a number of reasons why the old way was bad. One is that we were changing the weights in the middle of the backward pass, which could introduce some nasty problems, especially when you had architectures with the same model instance twice. It also made it harder to divide up sub-batches, and just generally complicated the design (it made the signatures really ugly, for instance).",True,{}
explosion/thinc,https://github.com/explosion/thinc,145,2019-12-26T21:19:16Z,2019-12-27T00:12:40Z,2020-01-22T00:55:20Z,MERGED,True,121,143,20,https://github.com/honnibal,Move dropout to its own layer,1,[],https://github.com/explosion/thinc/pull/145,https://github.com/honnibal,1,https://github.com/explosion/thinc/pull/145,"Remove the drop argument to layers, and instead add a Dropout layer.","Remove the drop argument to layers, and instead add a Dropout layer.",True,{}
explosion/thinc,https://github.com/explosion/thinc,146,2019-12-28T14:59:32Z,2019-12-28T15:13:19Z,2020-01-22T00:55:18Z,MERGED,True,612,614,34,https://github.com/honnibal,Move thinc.neural.ops -> thinc.backends,19,[],https://github.com/explosion/thinc/pull/146,https://github.com/honnibal,1,https://github.com/explosion/thinc/pull/146,,,True,{}
explosion/thinc,https://github.com/explosion/thinc,147,2019-12-28T15:21:14Z,2019-12-28T16:08:31Z,2020-01-22T00:55:16Z,MERGED,True,7,1441,25,https://github.com/honnibal,Remove averaged perceptron and associated code,12,[],https://github.com/explosion/thinc/pull/147,https://github.com/honnibal,1,https://github.com/explosion/thinc/pull/147,,,True,{}
explosion/thinc,https://github.com/explosion/thinc,148,2019-12-28T21:26:22Z,2019-12-28T21:36:19Z,2019-12-28T21:36:26Z,MERGED,True,8,720,11,https://github.com/ines,Replace built-in datasets with ml_datasets package,1,[],https://github.com/explosion/thinc/pull/148,https://github.com/ines,1,https://github.com/explosion/thinc/pull/148,"use ml_datasets instead of thinc.extra.datasets
remove _vendorized and other old code","use ml_datasets instead of thinc.extra.datasets
remove _vendorized and other old code",True,"{'THUMBS_UP': ['https://github.com/honnibal', 'https://github.com/svlandeg']}"
explosion/thinc,https://github.com/explosion/thinc,149,2019-12-30T20:13:02Z,2019-12-31T12:37:36Z,2020-02-13T21:35:02Z,MERGED,True,23,6,1,https://github.com/svlandeg,Fix murmur hash for GPU/Windows,1,['bug'],https://github.com/explosion/thinc/pull/149,https://github.com/svlandeg,1,https://github.com/explosion/thinc/pull/149,"Fixes #128 : test_hash_gives_distinct_keys[CupyOps] stops failing
Fixes explosion/spaCy#4758: no more weird POS & dep tags
Probably fixes explosion/spaCy#4816 (should be tested after merge)
Description
The murmur hash calculation for Windows (when using GPU/CUDA) was not working properly.
I finally figured out why: int64_t was being defined as long int. This is correct on LINUX, but not on Windows.
So I added a bit of code to do some byte-size checking upfront and only then define the correct types. It's a little ugly, but I couldn't get this implemented in a more clean fashion because

I couldn't find a way to check for platform (#if defined(_MSC_VER) e.g. didn't work)
I couldn't find a way to include the platform-specific header files (which is what you see in most implementations of the murmur hash because of this type incompatibility)
You're not allowed to redefine a typedef

Nonetheless, I think this works :-)
Example output
This has been the source of many quirks reported with GPU & Windows. I ran some tests myself and this was the output:
BEFORE PR
en_core_web_sm
tok.text ['The', 'decrease', 'in', '2008', 'primarily', 'relates', 'to', 'the', 'decrease', 'in', 'cash', 'and', 'cash', 'equivalents', '1', '.', '\n']
tok.pos_ ['PUNCT', 'PUNCT', 'PUNCT', 'PUNCT', 'PUNCT', 'PUNCT', 'PUNCT', 'PUNCT', 'PUNCT', 'PUNCT', 'PUNCT', 'PUNCT', 'PUNCT', 'PUNCT', 'PUNCT', 'PUNCT', 'SPACE']
tok.dep_ ['ROOT', 'pobj', 'neg', 'neg', 'neg', 'neg', 'neg', 'neg', 'neg', 'neg', 'neg', 'neg', 'neg', 'neg', 'neg', 'neg', '']

en_core_web_md
tok.text ['The', 'decrease', 'in', '2008', 'primarily', 'relates', 'to', 'the', 'decrease', 'in', 'cash', 'and', 'cash', 'equivalents', '1', '.', '\n']
tok.pos_ ['ADP', 'NOUN', 'ADP', 'NOUN', 'ADV', 'NOUN', 'VERB', 'ADP', 'NOUN', 'ADP', 'NOUN', 'ADP', 'NOUN', 'ADJ', 'VERB', 'NOUN', 'SPACE']
tok.dep_ ['ROOT', 'pobj', 'prep', 'pobj', 'punct', 'punct', 'conj', 'pcomp', 'pobj', 'prep', 'pobj', 'punct', 'punct', 'punct', 'det', 'pobj', '']

AFTER PR
en_core_web_sm
tok.text ['The', 'decrease', 'in', '2008', 'primarily', 'relates', 'to', 'the', 'decrease', 'in', 'cash', 'and', 'cash', 'equivalents', '1', '.', '\n']
tok.pos_ ['DET', 'NOUN', 'ADP', 'NUM', 'ADV', 'VERB', 'ADP', 'DET', 'NOUN', 'ADP', 'NOUN', 'CCONJ', 'NOUN', 'NOUN', 'NUM', 'PUNCT', 'SPACE']
tok.dep_ ['det', 'nsubj', 'prep', 'pobj', 'advmod', 'ROOT', 'prep', 'det', 'pobj', 'prep', 'pobj', 'cc', 'compound', 'conj', 'nummod', 'punct', '']

en_core_web_md
tok.text ['The', 'decrease', 'in', '2008', 'primarily', 'relates', 'to', 'the', 'decrease', 'in', 'cash', 'and', 'cash', 'equivalents', '1', '.', '\n']
tok.pos_ ['DET', 'NOUN', 'ADP', 'NUM', 'ADV', 'VERB', 'ADP', 'DET', 'NOUN', 'ADP', 'NOUN', 'CCONJ', 'NOUN', 'NOUN', 'NUM', 'PUNCT', 'SPACE']
tok.dep_ ['det', 'nsubj', 'prep', 'pobj', 'advmod', 'ROOT', 'prep', 'det', 'pobj', 'prep', 'pobj', 'cc', 'compound', 'conj', 'nummod', 'punct', '']

I think it's pretty obvious that the after-PR results are preferable :-)","Fixes #128 : test_hash_gives_distinct_keys[CupyOps] stops failing
Fixes explosion/spaCy#4758: no more weird POS & dep tags
Probably fixes explosion/spaCy#4816 (should be tested after merge)
Description
The murmur hash calculation for Windows (when using GPU/CUDA) was not working properly.
I finally figured out why: int64_t was being defined as long int. This is correct on LINUX, but not on Windows.
So I added a bit of code to do some byte-size checking upfront and only then define the correct types. It's a little ugly, but I couldn't get this implemented in a more clean fashion because

I couldn't find a way to check for platform (#if defined(_MSC_VER) e.g. didn't work)
I couldn't find a way to include the platform-specific header files (which is what you see in most implementations of the murmur hash because of this type incompatibility)
You're not allowed to redefine a typedef

Nonetheless, I think this works :-)
Example output
This has been the source of many quirks reported with GPU & Windows. I ran some tests myself and this was the output:
BEFORE PR
en_core_web_sm
tok.text ['The', 'decrease', 'in', '2008', 'primarily', 'relates', 'to', 'the', 'decrease', 'in', 'cash', 'and', 'cash', 'equivalents', '1', '.', '\n']
tok.pos_ ['PUNCT', 'PUNCT', 'PUNCT', 'PUNCT', 'PUNCT', 'PUNCT', 'PUNCT', 'PUNCT', 'PUNCT', 'PUNCT', 'PUNCT', 'PUNCT', 'PUNCT', 'PUNCT', 'PUNCT', 'PUNCT', 'SPACE']
tok.dep_ ['ROOT', 'pobj', 'neg', 'neg', 'neg', 'neg', 'neg', 'neg', 'neg', 'neg', 'neg', 'neg', 'neg', 'neg', 'neg', 'neg', '']

en_core_web_md
tok.text ['The', 'decrease', 'in', '2008', 'primarily', 'relates', 'to', 'the', 'decrease', 'in', 'cash', 'and', 'cash', 'equivalents', '1', '.', '\n']
tok.pos_ ['ADP', 'NOUN', 'ADP', 'NOUN', 'ADV', 'NOUN', 'VERB', 'ADP', 'NOUN', 'ADP', 'NOUN', 'ADP', 'NOUN', 'ADJ', 'VERB', 'NOUN', 'SPACE']
tok.dep_ ['ROOT', 'pobj', 'prep', 'pobj', 'punct', 'punct', 'conj', 'pcomp', 'pobj', 'prep', 'pobj', 'punct', 'punct', 'punct', 'det', 'pobj', '']

AFTER PR
en_core_web_sm
tok.text ['The', 'decrease', 'in', '2008', 'primarily', 'relates', 'to', 'the', 'decrease', 'in', 'cash', 'and', 'cash', 'equivalents', '1', '.', '\n']
tok.pos_ ['DET', 'NOUN', 'ADP', 'NUM', 'ADV', 'VERB', 'ADP', 'DET', 'NOUN', 'ADP', 'NOUN', 'CCONJ', 'NOUN', 'NOUN', 'NUM', 'PUNCT', 'SPACE']
tok.dep_ ['det', 'nsubj', 'prep', 'pobj', 'advmod', 'ROOT', 'prep', 'det', 'pobj', 'prep', 'pobj', 'cc', 'compound', 'conj', 'nummod', 'punct', '']

en_core_web_md
tok.text ['The', 'decrease', 'in', '2008', 'primarily', 'relates', 'to', 'the', 'decrease', 'in', 'cash', 'and', 'cash', 'equivalents', '1', '.', '\n']
tok.pos_ ['DET', 'NOUN', 'ADP', 'NUM', 'ADV', 'VERB', 'ADP', 'DET', 'NOUN', 'ADP', 'NOUN', 'CCONJ', 'NOUN', 'NOUN', 'NUM', 'PUNCT', 'SPACE']
tok.dep_ ['det', 'nsubj', 'prep', 'pobj', 'advmod', 'ROOT', 'prep', 'det', 'pobj', 'prep', 'pobj', 'cc', 'compound', 'conj', 'nummod', 'punct', '']

I think it's pretty obvious that the after-PR results are preferable :-)",True,{'HOORAY': ['https://github.com/BramVanroy']}
explosion/thinc,https://github.com/explosion/thinc,149,2019-12-30T20:13:02Z,2019-12-31T12:37:36Z,2020-02-13T21:35:02Z,MERGED,True,23,6,1,https://github.com/svlandeg,Fix murmur hash for GPU/Windows,1,['bug'],https://github.com/explosion/thinc/pull/149,https://github.com/honnibal,2,https://github.com/explosion/thinc/pull/149#issuecomment-569784168,"Fixes #128 : test_hash_gives_distinct_keys[CupyOps] stops failing
Fixes explosion/spaCy#4758: no more weird POS & dep tags
Probably fixes explosion/spaCy#4816 (should be tested after merge)
Description
The murmur hash calculation for Windows (when using GPU/CUDA) was not working properly.
I finally figured out why: int64_t was being defined as long int. This is correct on LINUX, but not on Windows.
So I added a bit of code to do some byte-size checking upfront and only then define the correct types. It's a little ugly, but I couldn't get this implemented in a more clean fashion because

I couldn't find a way to check for platform (#if defined(_MSC_VER) e.g. didn't work)
I couldn't find a way to include the platform-specific header files (which is what you see in most implementations of the murmur hash because of this type incompatibility)
You're not allowed to redefine a typedef

Nonetheless, I think this works :-)
Example output
This has been the source of many quirks reported with GPU & Windows. I ran some tests myself and this was the output:
BEFORE PR
en_core_web_sm
tok.text ['The', 'decrease', 'in', '2008', 'primarily', 'relates', 'to', 'the', 'decrease', 'in', 'cash', 'and', 'cash', 'equivalents', '1', '.', '\n']
tok.pos_ ['PUNCT', 'PUNCT', 'PUNCT', 'PUNCT', 'PUNCT', 'PUNCT', 'PUNCT', 'PUNCT', 'PUNCT', 'PUNCT', 'PUNCT', 'PUNCT', 'PUNCT', 'PUNCT', 'PUNCT', 'PUNCT', 'SPACE']
tok.dep_ ['ROOT', 'pobj', 'neg', 'neg', 'neg', 'neg', 'neg', 'neg', 'neg', 'neg', 'neg', 'neg', 'neg', 'neg', 'neg', 'neg', '']

en_core_web_md
tok.text ['The', 'decrease', 'in', '2008', 'primarily', 'relates', 'to', 'the', 'decrease', 'in', 'cash', 'and', 'cash', 'equivalents', '1', '.', '\n']
tok.pos_ ['ADP', 'NOUN', 'ADP', 'NOUN', 'ADV', 'NOUN', 'VERB', 'ADP', 'NOUN', 'ADP', 'NOUN', 'ADP', 'NOUN', 'ADJ', 'VERB', 'NOUN', 'SPACE']
tok.dep_ ['ROOT', 'pobj', 'prep', 'pobj', 'punct', 'punct', 'conj', 'pcomp', 'pobj', 'prep', 'pobj', 'punct', 'punct', 'punct', 'det', 'pobj', '']

AFTER PR
en_core_web_sm
tok.text ['The', 'decrease', 'in', '2008', 'primarily', 'relates', 'to', 'the', 'decrease', 'in', 'cash', 'and', 'cash', 'equivalents', '1', '.', '\n']
tok.pos_ ['DET', 'NOUN', 'ADP', 'NUM', 'ADV', 'VERB', 'ADP', 'DET', 'NOUN', 'ADP', 'NOUN', 'CCONJ', 'NOUN', 'NOUN', 'NUM', 'PUNCT', 'SPACE']
tok.dep_ ['det', 'nsubj', 'prep', 'pobj', 'advmod', 'ROOT', 'prep', 'det', 'pobj', 'prep', 'pobj', 'cc', 'compound', 'conj', 'nummod', 'punct', '']

en_core_web_md
tok.text ['The', 'decrease', 'in', '2008', 'primarily', 'relates', 'to', 'the', 'decrease', 'in', 'cash', 'and', 'cash', 'equivalents', '1', '.', '\n']
tok.pos_ ['DET', 'NOUN', 'ADP', 'NUM', 'ADV', 'VERB', 'ADP', 'DET', 'NOUN', 'ADP', 'NOUN', 'CCONJ', 'NOUN', 'NOUN', 'NUM', 'PUNCT', 'SPACE']
tok.dep_ ['det', 'nsubj', 'prep', 'pobj', 'advmod', 'ROOT', 'prep', 'det', 'pobj', 'prep', 'pobj', 'cc', 'compound', 'conj', 'nummod', 'punct', '']

I think it's pretty obvious that the after-PR results are preferable :-)",Great work! Really nice to get that resolved.,True,{'HOORAY': ['https://github.com/svlandeg']}
explosion/thinc,https://github.com/explosion/thinc,150,2019-12-31T14:04:41Z,2020-01-01T12:20:31Z,2020-01-01T14:35:44Z,MERGED,True,1,2,1,https://github.com/svlandeg,Remove thinc_gpu_ops from install instructions,1,[],https://github.com/explosion/thinc/pull/150,https://github.com/svlandeg,1,https://github.com/explosion/thinc/pull/150,cf title - readme update after PR #117,cf title - readme update after PR #117,True,{'THUMBS_UP': ['https://github.com/ines']}
explosion/thinc,https://github.com/explosion/thinc,151,2020-01-02T12:49:09Z,2020-01-02T14:46:55Z,2020-01-02T14:46:56Z,MERGED,True,3543,4596,130,https://github.com/honnibal,Rethinc,263,[],https://github.com/explosion/thinc/pull/151,https://github.com/honnibal,1,https://github.com/explosion/thinc/pull/151,I think the redesigned version is stable enough to at least be on develop.,I think the redesigned version is stable enough to at least be on develop.,True,{}
explosion/thinc,https://github.com/explosion/thinc,152,2020-01-02T17:16:14Z,2020-01-02T18:37:35Z,2020-01-22T00:55:12Z,MERGED,True,361,287,13,https://github.com/honnibal,"Introduce 'shims' package, fix PyTorchWrapper",8,[],https://github.com/explosion/thinc/pull/152,https://github.com/honnibal,1,https://github.com/explosion/thinc/pull/152,"Introduce specific support for having a model contain a non-Thinc model. This is better than storing the model in the generic _attrs, because we want to make sure we pass along calls to optimization and other parameter-relevant operations to the external models. Without a notion of these external models, we'll need each interface to be a special subclass of Model.
The ""shims"" system introduces a new base class, Shim, which interfaces to third-party packages should subclass. The Shim API is pretty small: you basically just have to make your class serializable with to/from bytes/disk methods, and you need to implement a __call__ function that returns the output and the backprop callback. There are a couple of other methods as well, e.g. to cpu/gpu. Shims are stored within model.shims. I considered making shims a dict, but most of the time you'll only have one shim member, and there's not really much point to naming them. It's also better to make it more similar to layers.
The PyTorchWrapper tests now pass again  . It would be great to get a Tensorflow shim and wrapper in place now as well.","Introduce specific support for having a model contain a non-Thinc model. This is better than storing the model in the generic _attrs, because we want to make sure we pass along calls to optimization and other parameter-relevant operations to the external models. Without a notion of these external models, we'll need each interface to be a special subclass of Model.
The ""shims"" system introduces a new base class, Shim, which interfaces to third-party packages should subclass. The Shim API is pretty small: you basically just have to make your class serializable with to/from bytes/disk methods, and you need to implement a __call__ function that returns the output and the backprop callback. There are a couple of other methods as well, e.g. to cpu/gpu. Shims are stored within model.shims. I considered making shims a dict, but most of the time you'll only have one shim member, and there's not really much point to naming them. It's also better to make it more similar to layers.
The PyTorchWrapper tests now pass again  . It would be great to get a Tensorflow shim and wrapper in place now as well.",True,"{'HOORAY': ['https://github.com/ines', 'https://github.com/svlandeg']}"
explosion/thinc,https://github.com/explosion/thinc,153,2020-01-03T17:30:47Z,2020-01-04T00:05:03Z,2020-01-06T07:39:41Z,MERGED,True,3,3,2,https://github.com/svlandeg,Small type fixes,2,[],https://github.com/explosion/thinc/pull/153,https://github.com/svlandeg,1,https://github.com/explosion/thinc/pull/153,"adding type for operators in Model.define_operators
I think Config should get as data a dict object or a ConfigParser ? Or is there also a use-case where a Config parser itself is given as argument? (and would this not be caught already because Config inherits from dict ?) Either way a change is needed because Config.from_str calls interpret_config with a ConfigParser.","adding type for operators in Model.define_operators
I think Config should get as data a dict object or a ConfigParser ? Or is there also a use-case where a Config parser itself is given as argument? (and would this not be caught already because Config inherits from dict ?) Either way a change is needed because Config.from_str calls interpret_config with a ConfigParser.",True,{'THUMBS_UP': ['https://github.com/ines']}
explosion/thinc,https://github.com/explosion/thinc,153,2020-01-03T17:30:47Z,2020-01-04T00:05:03Z,2020-01-06T07:39:41Z,MERGED,True,3,3,2,https://github.com/svlandeg,Small type fixes,2,[],https://github.com/explosion/thinc/pull/153,https://github.com/ines,2,https://github.com/explosion/thinc/pull/153#issuecomment-570734856,"adding type for operators in Model.define_operators
I think Config should get as data a dict object or a ConfigParser ? Or is there also a use-case where a Config parser itself is given as argument? (and would this not be caught already because Config inherits from dict ?) Either way a change is needed because Config.from_str calls interpret_config with a ConfigParser.","Thanks! The Config/Configparser thing was definitely a mistake. (I vaguely remember changing this because I thought it was a typo, but... nope )",True,{}
explosion/thinc,https://github.com/explosion/thinc,154,2020-01-03T23:48:59Z,2020-01-04T00:36:30Z,2020-01-04T00:36:33Z,MERGED,True,209,150,4,https://github.com/ines,Small refactor of config and registry parsing methods,4,[],https://github.com/explosion/thinc/pull/154,https://github.com/ines,1,https://github.com/explosion/thinc/pull/154,"make registry methods return Config object instead of dict (so you can directly call to_disk etc. on the result, which makes sense, since you also typically pass in a config, and the config otherwise behaves like a dict)
 only use one _fill helper for recursive validation and filling, and add two separate classmethods to expose the result: make_from_config (returns filled config with resolved functions from registry) and fill_config (only fills in default values from schema and/or function signature and returns updated config)
 allow toggling validation for both modes
 improve handling of unset required arguments and how errors are raised for it (also skip it if validation is disabled instead of raising the error)
 skip [DEFAULT] default section in interpreted configparser result for now  not sure if we want to use this at all, since it introduces another layer of defaults? If users want it, they can add any custom config section and use that for interpolation? The DEFAULT is kinda unintuitive and messes up the validation, because all top-level schemas would now require it.
 add more tests and docstrings","make registry methods return Config object instead of dict (so you can directly call to_disk etc. on the result, which makes sense, since you also typically pass in a config, and the config otherwise behaves like a dict)
 only use one _fill helper for recursive validation and filling, and add two separate classmethods to expose the result: make_from_config (returns filled config with resolved functions from registry) and fill_config (only fills in default values from schema and/or function signature and returns updated config)
 allow toggling validation for both modes
 improve handling of unset required arguments and how errors are raised for it (also skip it if validation is disabled instead of raising the error)
 skip [DEFAULT] default section in interpreted configparser result for now  not sure if we want to use this at all, since it introduces another layer of defaults? If users want it, they can add any custom config section and use that for interpolation? The DEFAULT is kinda unintuitive and messes up the validation, because all top-level schemas would now require it.
 add more tests and docstrings",True,{'THUMBS_UP': ['https://github.com/honnibal']}
explosion/thinc,https://github.com/explosion/thinc,154,2020-01-03T23:48:59Z,2020-01-04T00:36:30Z,2020-01-04T00:36:33Z,MERGED,True,209,150,4,https://github.com/ines,Small refactor of config and registry parsing methods,4,[],https://github.com/explosion/thinc/pull/154,https://github.com/honnibal,2,https://github.com/explosion/thinc/pull/154#issuecomment-570738387,"make registry methods return Config object instead of dict (so you can directly call to_disk etc. on the result, which makes sense, since you also typically pass in a config, and the config otherwise behaves like a dict)
 only use one _fill helper for recursive validation and filling, and add two separate classmethods to expose the result: make_from_config (returns filled config with resolved functions from registry) and fill_config (only fills in default values from schema and/or function signature and returns updated config)
 allow toggling validation for both modes
 improve handling of unset required arguments and how errors are raised for it (also skip it if validation is disabled instead of raising the error)
 skip [DEFAULT] default section in interpreted configparser result for now  not sure if we want to use this at all, since it introduces another layer of defaults? If users want it, they can add any custom config section and use that for interpolation? The DEFAULT is kinda unintuitive and messes up the validation, because all top-level schemas would now require it.
 add more tests and docstrings","skip [DEFAULT] default section in interpreted configparser result for now  not sure if we want to use this at all, since it introduces another layer of defaults? If users want it, they can add any custom config section and use that for interpolation? The DEFAULT is kinda unintuitive and messes up the validation, because all top-level schemas would now require it.

Yeah that [DEFAULT] section is jank. It's a weird hold-over from the default configparser format that we really have no use for: the main significance of it is you can reference into that section even if you don't have extended interpolation. But we always set extended interpolation, so it doesn't do anything. I definitely agree we should kill it.
I see what you mean about returning a Config object, I think. Looks good!",True,{}
explosion/thinc,https://github.com/explosion/thinc,155,2020-01-04T00:46:28Z,2020-01-04T01:06:51Z,2020-01-22T00:54:34Z,MERGED,True,39,37,5,https://github.com/ines,Tidy up examples and replace plac with typer,2,[],https://github.com/explosion/thinc/pull/155,https://github.com/ines,1,https://github.com/explosion/thinc/pull/155,Pretty much a drop-in replacement and uses type hints.,Pretty much a drop-in replacement and uses type hints.,True,{'THUMBS_UP': ['https://github.com/honnibal']}
explosion/thinc,https://github.com/explosion/thinc,156,2020-01-04T01:09:18Z,2020-01-04T14:28:06Z,2020-01-04T14:28:10Z,MERGED,True,312,128,7,https://github.com/honnibal,BiLSTM,25,[],https://github.com/explosion/thinc/pull/156,https://github.com/honnibal,1,https://github.com/explosion/thinc/pull/156,"Felt kind of embarrassing not to have a BiLSTM implementation. It's still kind of unfinished: needs testing and performance checking, and also doesn't handle dropout. But a lot of the other layers are in a similar situation; it will be much easier to test these things in spaCy, because we won't have to go to the effort to set up a test script.
In future it would be great to have a version that calls into cudnn, as a fast GPU version.","Felt kind of embarrassing not to have a BiLSTM implementation. It's still kind of unfinished: needs testing and performance checking, and also doesn't handle dropout. But a lot of the other layers are in a similar situation; it will be much easier to test these things in spaCy, because we won't have to go to the effort to set up a test script.
In future it would be great to have a version that calls into cudnn, as a fast GPU version.",True,{}
explosion/thinc,https://github.com/explosion/thinc,157,2020-01-04T14:24:07Z,2020-01-04T16:31:15Z,2020-01-04T16:31:17Z,MERGED,True,211,244,20,https://github.com/honnibal,"Add Ragged and Padded data types, rename transform layers",29,[],https://github.com/explosion/thinc/pull/157,https://github.com/honnibal,1,https://github.com/explosion/thinc/pull/157,"We have a couple of types-by-convention currently that would be better as proper types.

thinc.data.Ragged: This is the ragged array format where the data is concatenated, and there's an array with the lengths to represent the sequence information. We can just have this as ragged.data and ragged.lengths
thinc.data.Padded: The LSTM PR introduces another way of representing sequences, where we pad the sequences instead of concatenating them, and pass through an array indicating where the sequences end.

We can use these types to clear up the with_flatten, flatten_add_lengths, etc transforms, giving them better names and making them easier to explain. I think we should name them x2y and with_x2y, e.g. list2ragged, padded2list, padded2ragged, with_ragged2list etc. The advantage of the 2 is it gives the with_ a clearer scoping. I think with_list2ragged is easier to read than with_list_to_ragged, as the latter looks like with list, to ragged rather than with list_to_ragged.","We have a couple of types-by-convention currently that would be better as proper types.

thinc.data.Ragged: This is the ragged array format where the data is concatenated, and there's an array with the lengths to represent the sequence information. We can just have this as ragged.data and ragged.lengths
thinc.data.Padded: The LSTM PR introduces another way of representing sequences, where we pad the sequences instead of concatenating them, and pass through an array indicating where the sequences end.

We can use these types to clear up the with_flatten, flatten_add_lengths, etc transforms, giving them better names and making them easier to explain. I think we should name them x2y and with_x2y, e.g. list2ragged, padded2list, padded2ragged, with_ragged2list etc. The advantage of the 2 is it gives the with_ a clearer scoping. I think with_list2ragged is easier to read than with_list_to_ragged, as the latter looks like with list, to ragged rather than with list_to_ragged.",True,{}
explosion/thinc,https://github.com/explosion/thinc,158,2020-01-04T16:12:29Z,2020-01-04T16:50:16Z,2020-01-04T16:50:21Z,MERGED,True,175,5,2,https://github.com/ines,WIP: Add extended custom types and validators for arrays etc.,3,[],https://github.com/explosion/thinc/pull/158,https://github.com/ines,1,https://github.com/explosion/thinc/pull/158,,,True,{}
explosion/thinc,https://github.com/explosion/thinc,159,2020-01-04T18:49:58Z,2020-01-04T23:49:47Z,2020-01-04T23:49:48Z,CLOSED,False,147,12,4,https://github.com/justindujardin,Render Model Graphs,3,[],https://github.com/explosion/thinc/pull/159,https://github.com/justindujardin,1,https://github.com/explosion/thinc/pull/159,"Inspired by the Keras model rendering utilities, use pydot to graph and connect thinc model layers.
Add an example that renders a model architecture:
model = chain(
    ExtractWindow(3),
    ReLu(n_hidden, dropout=dropout, normalize=True),
    Maxout(n_hidden * 4),
    Affine(n_hidden * 2),
    ReLu(n_hidden, dropout=dropout, normalize=True),
    Affine(n_hidden),
    ReLu(n_hidden, dropout=dropout),
    Softmax(),
)","Inspired by the Keras model rendering utilities, use pydot to graph and connect thinc model layers.
Add an example that renders a model architecture:
model = chain(
    ExtractWindow(3),
    ReLu(n_hidden, dropout=dropout, normalize=True),
    Maxout(n_hidden * 4),
    Affine(n_hidden * 2),
    ReLu(n_hidden, dropout=dropout, normalize=True),
    Affine(n_hidden),
    ReLu(n_hidden, dropout=dropout),
    Softmax(),
)",True,{'HEART': ['https://github.com/ines']}
explosion/thinc,https://github.com/explosion/thinc,159,2020-01-04T18:49:58Z,2020-01-04T23:49:47Z,2020-01-04T23:49:48Z,CLOSED,False,147,12,4,https://github.com/justindujardin,Render Model Graphs,3,[],https://github.com/explosion/thinc/pull/159,https://github.com/honnibal,2,https://github.com/explosion/thinc/pull/159#issuecomment-570830101,"Inspired by the Keras model rendering utilities, use pydot to graph and connect thinc model layers.
Add an example that renders a model architecture:
model = chain(
    ExtractWindow(3),
    ReLu(n_hidden, dropout=dropout, normalize=True),
    Maxout(n_hidden * 4),
    Affine(n_hidden * 2),
    ReLu(n_hidden, dropout=dropout, normalize=True),
    Affine(n_hidden),
    ReLu(n_hidden, dropout=dropout),
    Softmax(),
)","Hm. I agree with the spirit of supporting a name argument to the Model class, but it does complicate the chain implementation a bit. Can we drop that part of the PR for now, until we come up with an implementation that looks a bit nicer and will work across the different layers?",True,{'THUMBS_UP': ['https://github.com/justindujardin']}
explosion/thinc,https://github.com/explosion/thinc,159,2020-01-04T18:49:58Z,2020-01-04T23:49:47Z,2020-01-04T23:49:48Z,CLOSED,False,147,12,4,https://github.com/justindujardin,Render Model Graphs,3,[],https://github.com/explosion/thinc/pull/159,https://github.com/ines,3,https://github.com/explosion/thinc/pull/159#issuecomment-570830125,"Inspired by the Keras model rendering utilities, use pydot to graph and connect thinc model layers.
Add an example that renders a model architecture:
model = chain(
    ExtractWindow(3),
    ReLu(n_hidden, dropout=dropout, normalize=True),
    Maxout(n_hidden * 4),
    Affine(n_hidden * 2),
    ReLu(n_hidden, dropout=dropout, normalize=True),
    Affine(n_hidden),
    ReLu(n_hidden, dropout=dropout),
    Softmax(),
)","I already reverted this part in #161, which is only adding the visualizers.",True,{}
explosion/thinc,https://github.com/explosion/thinc,160,2020-01-04T21:00:18Z,2020-01-05T00:11:43Z,2020-01-22T00:54:30Z,MERGED,True,570,172,21,https://github.com/honnibal,Fill out Array stub,20,[],https://github.com/explosion/thinc/pull/160,https://github.com/honnibal,1,https://github.com/explosion/thinc/pull/160,"We had Array typed to Any, so we were barely getting any type checking!
The Array stub here is relatively simple, but I already caught lots of genuine bugs from it.
I'm having trouble with the TypeVar in the layers though: I guess we could just use some casts, but it feels sort of weird anyway? I'm not really sure how this should work --- I think having the Model as a generic class with two type variables will be good, but I'm not sure it's ideal to have the variables in the forward function? Genuinely don't know.","We had Array typed to Any, so we were barely getting any type checking!
The Array stub here is relatively simple, but I already caught lots of genuine bugs from it.
I'm having trouble with the TypeVar in the layers though: I guess we could just use some casts, but it feels sort of weird anyway? I'm not really sure how this should work --- I think having the Model as a generic class with two type variables will be good, but I'm not sure it's ideal to have the variables in the forward function? Genuinely don't know.",True,{}
explosion/thinc,https://github.com/explosion/thinc,161,2020-01-04T22:14:54Z,2020-01-05T00:02:27Z,2020-01-05T00:02:29Z,MERGED,True,118,1,3,https://github.com/ines,Integrate visualizers into library,9,[],https://github.com/explosion/thinc/pull/161,https://github.com/ines,1,https://github.com/explosion/thinc/pull/161,Builds upon #159. Still need to find a good way to integrate it into the registry without making it too verbose.,Builds upon #159. Still need to find a good way to integrate it into the registry without making it too verbose.,True,{'HOORAY': ['https://github.com/justindujardin']}
explosion/thinc,https://github.com/explosion/thinc,162,2020-01-04T22:27:19Z,2020-01-04T23:46:28Z,2020-01-04T23:47:06Z,MERGED,True,23,6,2,https://github.com/justindujardin,Add generic input/output types to Model class,1,[],https://github.com/explosion/thinc/pull/162,https://github.com/justindujardin,1,https://github.com/explosion/thinc/pull/162,,,True,{}
explosion/thinc,https://github.com/explosion/thinc,162,2020-01-04T22:27:19Z,2020-01-04T23:46:28Z,2020-01-04T23:47:06Z,MERGED,True,23,6,2,https://github.com/justindujardin,Add generic input/output types to Model class,1,[],https://github.com/explosion/thinc/pull/162,https://github.com/honnibal,2,https://github.com/explosion/thinc/pull/162#issuecomment-570829939,,Awesome! Will have to work through the layers and add try out typing. I'll get my array-stub PR merged as well.,True,{}
explosion/thinc,https://github.com/explosion/thinc,163,2020-01-04T23:03:14Z,2020-01-05T23:43:46Z,2020-01-22T00:54:32Z,MERGED,True,5,1,3,https://github.com/ines,Only merge when ready: Update mypy and add to CI,2,[],https://github.com/explosion/thinc/pull/163,https://github.com/ines,1,https://github.com/explosion/thinc/pull/163,,,True,{}
explosion/thinc,https://github.com/explosion/thinc,164,2020-01-05T00:53:01Z,2020-01-07T18:56:36Z,2020-01-07T18:56:40Z,MERGED,True,210,0,1,https://github.com/honnibal,[WIP] Work on parallel training example,11,[],https://github.com/explosion/thinc/pull/164,https://github.com/honnibal,1,https://github.com/explosion/thinc/pull/164,"I think we should focus on Ray for parallel training: https://ray.readthedocs.io/en/latest/index.html . I like its design much better than other parallel computing frameworks I've used, its performance is apparently pretty good, and it's actively developed --- the team just spun out a startup from their lab at Berkeley, raising a decent seed round.
Parallel processing was one of the motivations for Thinc's redesign. The redesign fixes a long-standing problem in Thinc, where the weights were being updated in the middle of the backprop. We now only increment the gradients during backprop, which makes things much cleaner.
There are two remaining problems here:


It's kind of a joke to have the script running the MNIST data on a small network. It makes it impossible to see whether there's any parallelism happening. So we need to change this over, even a decent sized LSTM textcat task should be fine.


There should be a way to adjoin all the parameters for a model, so that we only have one contiguous buffer of parameters and gradients. This should help the parallelism a lot. I had some earlier drafts of this, conceptually it's very simple: we just want to have one Memory class shared over the whole model.","I think we should focus on Ray for parallel training: https://ray.readthedocs.io/en/latest/index.html . I like its design much better than other parallel computing frameworks I've used, its performance is apparently pretty good, and it's actively developed --- the team just spun out a startup from their lab at Berkeley, raising a decent seed round.
Parallel processing was one of the motivations for Thinc's redesign. The redesign fixes a long-standing problem in Thinc, where the weights were being updated in the middle of the backprop. We now only increment the gradients during backprop, which makes things much cleaner.
There are two remaining problems here:


It's kind of a joke to have the script running the MNIST data on a small network. It makes it impossible to see whether there's any parallelism happening. So we need to change this over, even a decent sized LSTM textcat task should be fine.


There should be a way to adjoin all the parameters for a model, so that we only have one contiguous buffer of parameters and gradients. This should help the parallelism a lot. I had some earlier drafts of this, conceptually it's very simple: we just want to have one Memory class shared over the whole model.",True,{}
explosion/thinc,https://github.com/explosion/thinc,165,2020-01-05T12:15:37Z,2020-01-05T12:56:55Z,2020-01-05T13:00:13Z,MERGED,True,75,76,12,https://github.com/honnibal,Specify model generics,4,[],https://github.com/explosion/thinc/pull/165,https://github.com/honnibal,1,https://github.com/explosion/thinc/pull/165,"This can still be refined a little, but this fixes the remaining type errors around the model generics.
I think I've finally understood the TypeVar as well. It really is pretty confusing! Also I think it turns out the be important that we've put all our layers in their own files? Using typevars without having module scopes seems really hard.","This can still be refined a little, but this fixes the remaining type errors around the model generics.
I think I've finally understood the TypeVar as well. It really is pretty confusing! Also I think it turns out the be important that we've put all our layers in their own files? Using typevars without having module scopes seems really hard.",True,{}
explosion/thinc,https://github.com/explosion/thinc,166,2020-01-05T12:29:53Z,2020-01-05T12:37:18Z,2020-01-05T12:37:18Z,MERGED,True,3,3,1,https://github.com/Abhijit-2592,fix imports in utils.py,2,[],https://github.com/explosion/thinc/pull/166,https://github.com/Abhijit-2592,1,https://github.com/explosion/thinc/pull/166,fix imports in prefer_gpu() and require_gpu(),fix imports in prefer_gpu() and require_gpu(),True,{'THUMBS_UP': ['https://github.com/honnibal']}
explosion/thinc,https://github.com/explosion/thinc,167,2020-01-05T14:20:51Z,2020-01-05T15:00:18Z,2020-01-05T15:33:19Z,MERGED,True,168,22,3,https://github.com/honnibal,"Extend Model API (node references, dim/param/grad names properties)",7,[],https://github.com/explosion/thinc/pull/167,https://github.com/honnibal,1,https://github.com/explosion/thinc/pull/167,"Noticed two gaps in the Model API when preparing the ray_parallel example and starting to prepare a spaCy branch for the new Thinc.
Gap 1: Node references In spaCy, we often want to fetch some named sublayer of a network later. This comes up especially often with the tok2vec layers, but it can happen for other stuff as well (for instance, you might want to note the output layer separately or something). In previous Thinc, we just blindly banged the new attribute onto the instance, like model.tok2vec = tok2vec.
This PR adds another attribute, _refs, with get_ref/set_ref/has_ref methods to interface for it. You can also pass refs into the constructor. There's also a method remove_node that walks over the tree, and removes removes a node from any layers lists that contain it and also remove references to it.
One question is, should it be considered valid for a node to have a reference if the node does not appear anywhere in the subtree? I think maybe that shouldn't be valid, as then we won't be able to serialize and deserialize correctly.
If we don't want to allow referenes to out-of-tree nodes, the remove_node() method will need to change in implementation slightly. What we should do is remove the node from the child layers, and then get a new node-set from walk(). We can then use this to filter the references.
Gap 2: Get all param/grad/dim/attr names: We have a way to fetch a particular param, dim or grad by name, but because the dicts are private we didn't have any way of getting the available names. I've added properties that return tuples for these. Tuples are good because users should not believe that model.param_names.append() should work. The lists include the names of stuff current set to None. You can use model.has_dim() etc to filter those lists.","Noticed two gaps in the Model API when preparing the ray_parallel example and starting to prepare a spaCy branch for the new Thinc.
Gap 1: Node references In spaCy, we often want to fetch some named sublayer of a network later. This comes up especially often with the tok2vec layers, but it can happen for other stuff as well (for instance, you might want to note the output layer separately or something). In previous Thinc, we just blindly banged the new attribute onto the instance, like model.tok2vec = tok2vec.
This PR adds another attribute, _refs, with get_ref/set_ref/has_ref methods to interface for it. You can also pass refs into the constructor. There's also a method remove_node that walks over the tree, and removes removes a node from any layers lists that contain it and also remove references to it.
One question is, should it be considered valid for a node to have a reference if the node does not appear anywhere in the subtree? I think maybe that shouldn't be valid, as then we won't be able to serialize and deserialize correctly.
If we don't want to allow referenes to out-of-tree nodes, the remove_node() method will need to change in implementation slightly. What we should do is remove the node from the child layers, and then get a new node-set from walk(). We can then use this to filter the references.
Gap 2: Get all param/grad/dim/attr names: We have a way to fetch a particular param, dim or grad by name, but because the dicts are private we didn't have any way of getting the available names. I've added properties that return tuples for these. Tuples are good because users should not believe that model.param_names.append() should work. The lists include the names of stuff current set to None. You can use model.has_dim() etc to filter those lists.",True,{}
explosion/thinc,https://github.com/explosion/thinc,168,2020-01-05T15:57:38Z,2020-01-08T11:31:50Z,2020-01-08T11:39:46Z,MERGED,True,446,1,5,https://github.com/Abhijit-2592, Tensorflow shim,18,[],https://github.com/explosion/thinc/pull/168,https://github.com/Abhijit-2592,1,https://github.com/explosion/thinc/pull/168,"Create a TensorFlow Shim

 Use dlpack instead of copying arrays around
  Write the main shim","Create a TensorFlow Shim

 Use dlpack instead of copying arrays around
  Write the main shim",True,{}
explosion/thinc,https://github.com/explosion/thinc,168,2020-01-05T15:57:38Z,2020-01-08T11:31:50Z,2020-01-08T11:39:46Z,MERGED,True,446,1,5,https://github.com/Abhijit-2592, Tensorflow shim,18,[],https://github.com/explosion/thinc/pull/168,https://github.com/honnibal,2,https://github.com/explosion/thinc/pull/168#issuecomment-570929206,"Create a TensorFlow Shim

 Use dlpack instead of copying arrays around
  Write the main shim","Looks like a good start! If the dlpack is sorted out, hopefully the rest of the shim will come together pretty easily.",True,{}
explosion/thinc,https://github.com/explosion/thinc,168,2020-01-05T15:57:38Z,2020-01-08T11:31:50Z,2020-01-08T11:39:46Z,MERGED,True,446,1,5,https://github.com/Abhijit-2592, Tensorflow shim,18,[],https://github.com/explosion/thinc/pull/168,https://github.com/honnibal,3,https://github.com/explosion/thinc/pull/168#issuecomment-572009529,"Create a TensorFlow Shim

 Use dlpack instead of copying arrays around
  Write the main shim","Argh, I messed up the whitespace in the github merge. Since you're travelling today I'll merge and fix on develop :).
Great work! I'll make a couple of minor changes as well.",True,{}
explosion/thinc,https://github.com/explosion/thinc,168,2020-01-05T15:57:38Z,2020-01-08T11:31:50Z,2020-01-08T11:39:46Z,MERGED,True,446,1,5,https://github.com/Abhijit-2592, Tensorflow shim,18,[],https://github.com/explosion/thinc/pull/168,https://github.com/Abhijit-2592,4,https://github.com/explosion/thinc/pull/168#issuecomment-572011998,"Create a TensorFlow Shim

 Use dlpack instead of copying arrays around
  Write the main shim",Thanks @honnibal . Let me know if you require more changes. I will be able to do it after I reach,True,{}
explosion/thinc,https://github.com/explosion/thinc,169,2020-01-05T16:40:05Z,2020-01-08T11:20:04Z,2020-01-08T11:20:04Z,CLOSED,False,95,31,4,https://github.com/honnibal,Experiment with representing dimension values in the type system,10,[],https://github.com/explosion/thinc/pull/169,https://github.com/honnibal,1,https://github.com/explosion/thinc/pull/169,"We can't expect the type checking to provide full validation over literal types, as we won't be able to express something like ""concatenate returns output with dimensionality equal to the sum of its layers' dimensionality. However, even if the checking doesn't really work properly, it's worth discussing what this could look like, and what we can and can't do with it.
I'm not 100% sure I'm doing this right, but I've tried to make the Floats1d, Floats2d etc arrays into generics, giving us space to pass type variables in to represent the dimensions. I've taken care to use distinct variables for each class. I'm not sure this is actually necessary? I didn't want mypy to think the dimensions across the classes matched the same values or something.
Arguably we could avoid having the Floats1d, Floats2d etc classes, and make these generic with respect to number of dimensions. I think that's probably worse: I think it's worth being able to express Floats2d concisely, and to me it seems to make things simpler and easier to work with. If it turns out making this a parametric type with a literal is better, sure we can do that.
The main thing that I think is neat is having a way to document the expected dimensionalities in the code.
I have no idea what conventions we should use to distinguish the dimension type variables from real variables, though. We want the type variables to have concise names, but we still want concise names available within the code. I think we need a better variable convention in general around this stuff. For instance, maybe weights parameters should be named like wW, wB by convention? Then their gradients could still be dW.","We can't expect the type checking to provide full validation over literal types, as we won't be able to express something like ""concatenate returns output with dimensionality equal to the sum of its layers' dimensionality. However, even if the checking doesn't really work properly, it's worth discussing what this could look like, and what we can and can't do with it.
I'm not 100% sure I'm doing this right, but I've tried to make the Floats1d, Floats2d etc arrays into generics, giving us space to pass type variables in to represent the dimensions. I've taken care to use distinct variables for each class. I'm not sure this is actually necessary? I didn't want mypy to think the dimensions across the classes matched the same values or something.
Arguably we could avoid having the Floats1d, Floats2d etc classes, and make these generic with respect to number of dimensions. I think that's probably worse: I think it's worth being able to express Floats2d concisely, and to me it seems to make things simpler and easier to work with. If it turns out making this a parametric type with a literal is better, sure we can do that.
The main thing that I think is neat is having a way to document the expected dimensionalities in the code.
I have no idea what conventions we should use to distinguish the dimension type variables from real variables, though. We want the type variables to have concise names, but we still want concise names available within the code. I think we need a better variable convention in general around this stuff. For instance, maybe weights parameters should be named like wW, wB by convention? Then their gradients could still be dW.",True,{}
explosion/thinc,https://github.com/explosion/thinc,169,2020-01-05T16:40:05Z,2020-01-08T11:20:04Z,2020-01-08T11:20:04Z,CLOSED,False,95,31,4,https://github.com/honnibal,Experiment with representing dimension values in the type system,10,[],https://github.com/explosion/thinc/pull/169,https://github.com/honnibal,2,https://github.com/explosion/thinc/pull/169#issuecomment-570929554,"We can't expect the type checking to provide full validation over literal types, as we won't be able to express something like ""concatenate returns output with dimensionality equal to the sum of its layers' dimensionality. However, even if the checking doesn't really work properly, it's worth discussing what this could look like, and what we can and can't do with it.
I'm not 100% sure I'm doing this right, but I've tried to make the Floats1d, Floats2d etc arrays into generics, giving us space to pass type variables in to represent the dimensions. I've taken care to use distinct variables for each class. I'm not sure this is actually necessary? I didn't want mypy to think the dimensions across the classes matched the same values or something.
Arguably we could avoid having the Floats1d, Floats2d etc classes, and make these generic with respect to number of dimensions. I think that's probably worse: I think it's worth being able to express Floats2d concisely, and to me it seems to make things simpler and easier to work with. If it turns out making this a parametric type with a literal is better, sure we can do that.
The main thing that I think is neat is having a way to document the expected dimensionalities in the code.
I have no idea what conventions we should use to distinguish the dimension type variables from real variables, though. We want the type variables to have concise names, but we still want concise names available within the code. I think we need a better variable convention in general around this stuff. For instance, maybe weights parameters should be named like wW, wB by convention? Then their gradients could still be dW.","Hm, Python 3.8 fails due to method resolution order? Seems hard to fix =/",True,{}
explosion/thinc,https://github.com/explosion/thinc,169,2020-01-05T16:40:05Z,2020-01-08T11:20:04Z,2020-01-08T11:20:04Z,CLOSED,False,95,31,4,https://github.com/honnibal,Experiment with representing dimension values in the type system,10,[],https://github.com/explosion/thinc/pull/169,https://github.com/svlandeg,3,https://github.com/explosion/thinc/pull/169#issuecomment-571047629,"We can't expect the type checking to provide full validation over literal types, as we won't be able to express something like ""concatenate returns output with dimensionality equal to the sum of its layers' dimensionality. However, even if the checking doesn't really work properly, it's worth discussing what this could look like, and what we can and can't do with it.
I'm not 100% sure I'm doing this right, but I've tried to make the Floats1d, Floats2d etc arrays into generics, giving us space to pass type variables in to represent the dimensions. I've taken care to use distinct variables for each class. I'm not sure this is actually necessary? I didn't want mypy to think the dimensions across the classes matched the same values or something.
Arguably we could avoid having the Floats1d, Floats2d etc classes, and make these generic with respect to number of dimensions. I think that's probably worse: I think it's worth being able to express Floats2d concisely, and to me it seems to make things simpler and easier to work with. If it turns out making this a parametric type with a literal is better, sure we can do that.
The main thing that I think is neat is having a way to document the expected dimensionalities in the code.
I have no idea what conventions we should use to distinguish the dimension type variables from real variables, though. We want the type variables to have concise names, but we still want concise names available within the code. I think we need a better variable convention in general around this stuff. For instance, maybe weights parameters should be named like wW, wB by convention? Then their gradients could still be dW.",It seems to work when you put Array first like you did for Floats1d and Floats2d ?,True,{}
explosion/thinc,https://github.com/explosion/thinc,169,2020-01-05T16:40:05Z,2020-01-08T11:20:04Z,2020-01-08T11:20:04Z,CLOSED,False,95,31,4,https://github.com/honnibal,Experiment with representing dimension values in the type system,10,[],https://github.com/explosion/thinc/pull/169,https://github.com/honnibal,4,https://github.com/explosion/thinc/pull/169#issuecomment-572005937,"We can't expect the type checking to provide full validation over literal types, as we won't be able to express something like ""concatenate returns output with dimensionality equal to the sum of its layers' dimensionality. However, even if the checking doesn't really work properly, it's worth discussing what this could look like, and what we can and can't do with it.
I'm not 100% sure I'm doing this right, but I've tried to make the Floats1d, Floats2d etc arrays into generics, giving us space to pass type variables in to represent the dimensions. I've taken care to use distinct variables for each class. I'm not sure this is actually necessary? I didn't want mypy to think the dimensions across the classes matched the same values or something.
Arguably we could avoid having the Floats1d, Floats2d etc classes, and make these generic with respect to number of dimensions. I think that's probably worse: I think it's worth being able to express Floats2d concisely, and to me it seems to make things simpler and easier to work with. If it turns out making this a parametric type with a literal is better, sure we can do that.
The main thing that I think is neat is having a way to document the expected dimensionalities in the code.
I have no idea what conventions we should use to distinguish the dimension type variables from real variables, though. We want the type variables to have concise names, but we still want concise names available within the code. I think we need a better variable convention in general around this stuff. For instance, maybe weights parameters should be named like wW, wB by convention? Then their gradients could still be dW.","Closing this: I think the experiment really leads to the conclusion that the dimension-based typing isn't a good approach, certainly for now, and possibly in general. I still find the documentation aspect pretty appealing, but the complexity just doesn't work, and it messes up the type variables we want to use for too many methods.",True,{}
explosion/thinc,https://github.com/explosion/thinc,170,2020-01-05T18:54:45Z,2020-01-05T22:03:29Z,2020-01-22T00:55:08Z,MERGED,True,193,173,31,https://github.com/honnibal,Tighten up layer types,20,[],https://github.com/explosion/thinc/pull/170,https://github.com/honnibal,1,https://github.com/explosion/thinc/pull/170,,,True,{'HEART': ['https://github.com/ines']}
explosion/thinc,https://github.com/explosion/thinc,171,2020-01-06T12:09:17Z,2020-01-06T12:30:19Z,2020-01-22T00:55:14Z,MERGED,True,108,2,6,https://github.com/honnibal,Add/update for spaCy,9,[],https://github.com/explosion/thinc/pull/171,https://github.com/honnibal,1,https://github.com/explosion/thinc/pull/171,Add some stuff spaCy needs,Add some stuff spaCy needs,True,{}
explosion/thinc,https://github.com/explosion/thinc,172,2020-01-06T15:30:27Z,2020-01-06T17:12:21Z,2020-01-06T17:12:26Z,MERGED,True,353,246,14,https://github.com/honnibal,Add a thinc.api module that provides a flat namespace,5,[],https://github.com/explosion/thinc/pull/172,https://github.com/honnibal,1,https://github.com/explosion/thinc/pull/172,"While porting spaCy to the new Thinc, I started having trouble remembering what was where, for a couple of functions.
I suggest we add a flat namespace file, thinc.api, that imports everything in the library. The library itself should never import this module, to avoid circular imports. This also means we can't import this module during the tests, except perhaps in the body of a test file.
Users will be free to optionally just work directly with thinc.api. If they choose to do that, they'll also gain some protection against breaking changes that result from us moving things around. I think it's okay to still have the actual locations of files as part of the interface, but if we wanted to go one step further, we could make everything but thinc.api an underscored module, so that the only interface into the library is the flat thinc.api namespace. I think that's worse though --- I'd rather have thinc.api as just an option.
I've also added __all__ to the various modules.
It's kind of nice to see the API all in one place though! It's pretty small.","While porting spaCy to the new Thinc, I started having trouble remembering what was where, for a couple of functions.
I suggest we add a flat namespace file, thinc.api, that imports everything in the library. The library itself should never import this module, to avoid circular imports. This also means we can't import this module during the tests, except perhaps in the body of a test file.
Users will be free to optionally just work directly with thinc.api. If they choose to do that, they'll also gain some protection against breaking changes that result from us moving things around. I think it's okay to still have the actual locations of files as part of the interface, but if we wanted to go one step further, we could make everything but thinc.api an underscored module, so that the only interface into the library is the flat thinc.api namespace. I think that's worse though --- I'd rather have thinc.api as just an option.
I've also added __all__ to the various modules.
It's kind of nice to see the API all in one place though! It's pretty small.",True,{}
explosion/thinc,https://github.com/explosion/thinc,172,2020-01-06T15:30:27Z,2020-01-06T17:12:21Z,2020-01-06T17:12:26Z,MERGED,True,353,246,14,https://github.com/honnibal,Add a thinc.api module that provides a flat namespace,5,[],https://github.com/explosion/thinc/pull/172,https://github.com/svlandeg,2,https://github.com/explosion/thinc/pull/172#issuecomment-571194550,"While porting spaCy to the new Thinc, I started having trouble remembering what was where, for a couple of functions.
I suggest we add a flat namespace file, thinc.api, that imports everything in the library. The library itself should never import this module, to avoid circular imports. This also means we can't import this module during the tests, except perhaps in the body of a test file.
Users will be free to optionally just work directly with thinc.api. If they choose to do that, they'll also gain some protection against breaking changes that result from us moving things around. I think it's okay to still have the actual locations of files as part of the interface, but if we wanted to go one step further, we could make everything but thinc.api an underscored module, so that the only interface into the library is the flat thinc.api namespace. I think that's worse though --- I'd rather have thinc.api as just an option.
I've also added __all__ to the various modules.
It's kind of nice to see the API all in one place though! It's pretty small.","I think this is a great idea, and actually a nice overview of everything that's available in thinc :-) I wouldn't make the use of this mandatory though. I can imagine some users wanting to use the imports of the original locations to be able to find the original source code faster, for instance. But others may enjoy the additional abstraction that makes their code more maintainable !",True,{}
explosion/thinc,https://github.com/explosion/thinc,173,2020-01-06T17:38:29Z,2020-01-06T17:44:15Z,2020-01-06T17:44:16Z,MERGED,True,2,1,2,https://github.com/svlandeg,add cauchysimilarity import,1,[],https://github.com/explosion/thinc/pull/173,https://github.com/svlandeg,1,https://github.com/explosion/thinc/pull/173,"CauchySimilarity was missing from the imports in layers.init, causing trouble in spaCy imports.
Also reformated the formula for calculating output size after adding context from a window, I thought the spacing was confusing...","CauchySimilarity was missing from the imports in layers.init, causing trouble in spaCy imports.
Also reformated the formula for calculating output size after adding context from a window, I thought the spacing was confusing...",True,{}
explosion/thinc,https://github.com/explosion/thinc,174,2020-01-06T19:25:57Z,2020-01-07T01:27:00Z,2020-01-07T02:12:33Z,MERGED,True,31,17,2,https://github.com/justindujardin,Fix CI build for Python 3.8,7,[],https://github.com/explosion/thinc/pull/174,https://github.com/justindujardin,1,https://github.com/explosion/thinc/pull/174,"Python can't resolve the order of methods when a class extends the same base twice.
In this case, we have class Array(Generic[t]) and then the Floats/Ints types which extend Array(Generic[t]) and Generic[t,...]
Looking at the Array type it appears the argument it wants is the number of dimensions, so fill Floats/Ints types with Literal[ndim] in the implementation.
I'm not sure, but I suspect that specifying the type for our Array usage disambiguates the base Generic type, allowing python to derive the correct call order. Probably because the Array generic no longer has any types that have to be filled in.","Python can't resolve the order of methods when a class extends the same base twice.
In this case, we have class Array(Generic[t]) and then the Floats/Ints types which extend Array(Generic[t]) and Generic[t,...]
Looking at the Array type it appears the argument it wants is the number of dimensions, so fill Floats/Ints types with Literal[ndim] in the implementation.
I'm not sure, but I suspect that specifying the type for our Array usage disambiguates the base Generic type, allowing python to derive the correct call order. Probably because the Array generic no longer has any types that have to be filled in.",True,{'EYES': ['https://github.com/ines']}
explosion/thinc,https://github.com/explosion/thinc,174,2020-01-06T19:25:57Z,2020-01-07T01:27:00Z,2020-01-07T02:12:33Z,MERGED,True,31,17,2,https://github.com/justindujardin,Fix CI build for Python 3.8,7,[],https://github.com/explosion/thinc/pull/174,https://github.com/honnibal,2,https://github.com/explosion/thinc/pull/174#issuecomment-571315766,"Python can't resolve the order of methods when a class extends the same base twice.
In this case, we have class Array(Generic[t]) and then the Floats/Ints types which extend Array(Generic[t]) and Generic[t,...]
Looking at the Array type it appears the argument it wants is the number of dimensions, so fill Floats/Ints types with Literal[ndim] in the implementation.
I'm not sure, but I suspect that specifying the type for our Array usage disambiguates the base Generic type, allowing python to derive the correct call order. Probably because the Array generic no longer has any types that have to be filled in.","Aaah, I see what you did there, to get around that MRO problem. Interesting!
It's still an open question whether this is indeed better: I can see advantages and disadvantages. I feel like the error messages from the generic types get pretty ugly, and it's a lot of complexity for probably limited amounts of actual error checking?",True,{}
explosion/thinc,https://github.com/explosion/thinc,174,2020-01-06T19:25:57Z,2020-01-07T01:27:00Z,2020-01-07T02:12:33Z,MERGED,True,31,17,2,https://github.com/justindujardin,Fix CI build for Python 3.8,7,[],https://github.com/explosion/thinc/pull/174,https://github.com/justindujardin,3,https://github.com/explosion/thinc/pull/174#issuecomment-571348159,"Python can't resolve the order of methods when a class extends the same base twice.
In this case, we have class Array(Generic[t]) and then the Floats/Ints types which extend Array(Generic[t]) and Generic[t,...]
Looking at the Array type it appears the argument it wants is the number of dimensions, so fill Floats/Ints types with Literal[ndim] in the implementation.
I'm not sure, but I suspect that specifying the type for our Array usage disambiguates the base Generic type, allowing python to derive the correct call order. Probably because the Array generic no longer has any types that have to be filled in.","Cool, I removed the example with dimension type checks for now.
I think the complexity could be brought down in a bunch of ways from that example if we revisit it later",True,{}
explosion/thinc,https://github.com/explosion/thinc,175,2020-01-06T23:25:05Z,2020-01-07T01:41:02Z,2020-01-07T01:41:23Z,CLOSED,False,309,207,14,https://github.com/justindujardin,Feature/array ops memory types,17,[],https://github.com/explosion/thinc/pull/175,https://github.com/justindujardin,1,https://github.com/explosion/thinc/pull/175,"Add generic types for Array, Ops, and Memory classes
This probably needs a decent review on the ops function return types. They were all plain Array, and I made them infer the ArrayT from the passed in variables. I think this is right in most cases, but that may not be true.
This builds on top of #174","Add generic types for Array, Ops, and Memory classes
This probably needs a decent review on the ops function return types. They were all plain Array, and I made them infer the ArrayT from the passed in variables. I think this is right in most cases, but that may not be true.
This builds on top of #174",True,{}
explosion/thinc,https://github.com/explosion/thinc,175,2020-01-06T23:25:05Z,2020-01-07T01:41:02Z,2020-01-07T01:41:23Z,CLOSED,False,309,207,14,https://github.com/justindujardin,Feature/array ops memory types,17,[],https://github.com/explosion/thinc/pull/175,https://github.com/justindujardin,2,https://github.com/explosion/thinc/pull/175#issuecomment-571365930,"Add generic types for Array, Ops, and Memory classes
This probably needs a decent review on the ops function return types. They were all plain Array, and I made them infer the ArrayT from the passed in variables. I think this is right in most cases, but that may not be true.
This builds on top of #174","I didn't have specific examples to verify the types line up as expected, so I had to follow the errors from mypy as a guide for whether the types were right or not.
The general idea is that the type has to be carried either by:

A Class self variable like https://github.com/explosion/thinc/pull/175/files#diff-bbf863a9d29a482c9040df4c4aca7837R113
A function/method argument like https://github.com/explosion/thinc/pull/175/files#diff-d76e2c413ca47ce6c5567ca627156b53R224",True,{}
explosion/thinc,https://github.com/explosion/thinc,176,2020-01-07T00:51:54Z,2020-01-07T01:26:23Z,2020-01-22T00:54:27Z,MERGED,True,14,59,7,https://github.com/ines,Update to new ml_datasets and adjust code,1,[],https://github.com/explosion/thinc/pull/176,https://github.com/ines,1,https://github.com/explosion/thinc/pull/176,"Moved some of the data transforms into ml-datasets, which lets us kill a bunch of code in Thinc.
 You need to upgrade ml_datasets locally, otherwise the tests won't pass.","Moved some of the data transforms into ml-datasets, which lets us kill a bunch of code in Thinc.
 You need to upgrade ml_datasets locally, otherwise the tests won't pass.",True,{'THUMBS_UP': ['https://github.com/honnibal']}
explosion/thinc,https://github.com/explosion/thinc,177,2020-01-07T01:48:20Z,2020-01-07T20:14:52Z,2020-01-07T20:15:11Z,MERGED,True,245,155,29,https://github.com/justindujardin,Add generic types to Array and Ops.allocate,14,[],https://github.com/explosion/thinc/pull/177,https://github.com/justindujardin,1,https://github.com/explosion/thinc/pull/177,"for array generics work fine to give back the type of the item passed in. We could further expand these types to all the self arguments in the Array class.
add overloads for Ops.allocate that support up to 4d data with return types of Floats(n)d
the ND case is tricky and doesn't play nice with other overloads (because it catches all of them) so separate it out into allocate_nd","for array generics work fine to give back the type of the item passed in. We could further expand these types to all the self arguments in the Array class.
add overloads for Ops.allocate that support up to 4d data with return types of Floats(n)d
the ND case is tricky and doesn't play nice with other overloads (because it catches all of them) so separate it out into allocate_nd",True,{}
explosion/thinc,https://github.com/explosion/thinc,177,2020-01-07T01:48:20Z,2020-01-07T20:14:52Z,2020-01-07T20:15:11Z,MERGED,True,245,155,29,https://github.com/justindujardin,Add generic types to Array and Ops.allocate,14,[],https://github.com/explosion/thinc/pull/177,https://github.com/justindujardin,2,https://github.com/explosion/thinc/pull/177#issuecomment-571410606,"for array generics work fine to give back the type of the item passed in. We could further expand these types to all the self arguments in the Array class.
add overloads for Ops.allocate that support up to 4d data with return types of Floats(n)d
the ND case is tricky and doesn't play nice with other overloads (because it catches all of them) so separate it out into allocate_nd",There are a few mypy errors to fix from the addition of new (more strict) types. I'll clean them up in the morning.,True,{}
explosion/thinc,https://github.com/explosion/thinc,177,2020-01-07T01:48:20Z,2020-01-07T20:14:52Z,2020-01-07T20:15:11Z,MERGED,True,245,155,29,https://github.com/justindujardin,Add generic types to Array and Ops.allocate,14,[],https://github.com/explosion/thinc/pull/177,https://github.com/honnibal,3,https://github.com/explosion/thinc/pull/177#issuecomment-571543771,"for array generics work fine to give back the type of the item passed in. We could further expand these types to all the self arguments in the Array class.
add overloads for Ops.allocate that support up to 4d data with return types of Floats(n)d
the ND case is tricky and doesn't play nice with other overloads (because it catches all of them) so separate it out into allocate_nd","If we need to differentiate the unlimited dimension vs fixed dimension case anyway, maybe we really should go the other way and do alloc_f1d, alloc_f2d etc?",True,{}
explosion/thinc,https://github.com/explosion/thinc,177,2020-01-07T01:48:20Z,2020-01-07T20:14:52Z,2020-01-07T20:15:11Z,MERGED,True,245,155,29,https://github.com/justindujardin,Add generic types to Array and Ops.allocate,14,[],https://github.com/explosion/thinc/pull/177,https://github.com/justindujardin,4,https://github.com/explosion/thinc/pull/177#issuecomment-571662040,"for array generics work fine to give back the type of the item passed in. We could further expand these types to all the self arguments in the Array class.
add overloads for Ops.allocate that support up to 4d data with return types of Floats(n)d
the ND case is tricky and doesn't play nice with other overloads (because it catches all of them) so separate it out into allocate_nd","Sure, I can change it to be like that if you prefer. I did it this way to avoid a bunch of new methods, and this looked like a decent in-between where you mostly use one function and resort to the _nd function for things with unknown/dynamic shapes.
The other thing I was thinking is that you lose some ease-of-use style cases with specific functions for each dim, e.g., if you already have the shape expressed as a list, you can pass directly as an argument rather than picking them off:
shape = [1,2]
# This is okay now
a:Floats2d = ops.allocate(shape)
# And is usable with Tensorflow as well
a_tf: tf.Tensor = tf.ones(shape=shape)
# Here you have to pick out the dimensions
b: Floats2d = ops.alloc_f2d(shape[0], shape[1])
# But here it's more natural and you save []
c: Floats2d = ops.alloc_f2d(1, 2)
I can do it either way, let me know which way you prefer and I'll update the code. If we do the _f2d we'll also want the _i2d version, yeah?",True,{}
explosion/thinc,https://github.com/explosion/thinc,177,2020-01-07T01:48:20Z,2020-01-07T20:14:52Z,2020-01-07T20:15:11Z,MERGED,True,245,155,29,https://github.com/justindujardin,Add generic types to Array and Ops.allocate,14,[],https://github.com/explosion/thinc/pull/177,https://github.com/honnibal,5,https://github.com/explosion/thinc/pull/177#issuecomment-571690798,"for array generics work fine to give back the type of the item passed in. We could further expand these types to all the self arguments in the Array class.
add overloads for Ops.allocate that support up to 4d data with return types of Floats(n)d
the ND case is tricky and doesn't play nice with other overloads (because it catches all of them) so separate it out into allocate_nd","I do think it's better to have the individual functions: to me it's nicer to have the general name cover the most general case, and then the more specific methods with more specific names. I guess another way of saying it is, if you saw allocate() and allocate_nd as method names, what would you guess the difference was between them? I guess I'd think allocate could only give me a flat vector, so I'd probably try to call it with an integer. I doubt I'd guess that actually the two methods are identical at runtime, and the allocate_nd is only necessary due to a technicality of the type-checking.
Btw I think you shouldn't have to pick out the dimensions:
b: Floats2d = ops.alloc_f2d(*shape)
You'd also get a runtime error there if you're wrong about what you think shape is, which is kind of nice.
One question is, if it will be alloc_f2d etc, should the general method be named alloc?",True,{}
explosion/thinc,https://github.com/explosion/thinc,177,2020-01-07T01:48:20Z,2020-01-07T20:14:52Z,2020-01-07T20:15:11Z,MERGED,True,245,155,29,https://github.com/justindujardin,Add generic types to Array and Ops.allocate,14,[],https://github.com/explosion/thinc/pull/177,https://github.com/justindujardin,6,https://github.com/explosion/thinc/pull/177#issuecomment-571691742,"for array generics work fine to give back the type of the item passed in. We could further expand these types to all the self arguments in the Array class.
add overloads for Ops.allocate that support up to 4d data with return types of Floats(n)d
the ND case is tricky and doesn't play nice with other overloads (because it catches all of them) so separate it out into allocate_nd","Cool, I'll start with the shortened names like you suggest, and if you don't like them we can switch back",True,{}
explosion/thinc,https://github.com/explosion/thinc,177,2020-01-07T01:48:20Z,2020-01-07T20:14:52Z,2020-01-07T20:15:11Z,MERGED,True,245,155,29,https://github.com/justindujardin,Add generic types to Array and Ops.allocate,14,[],https://github.com/explosion/thinc/pull/177,https://github.com/honnibal,7,https://github.com/explosion/thinc/pull/177#issuecomment-571727042,"for array generics work fine to give back the type of the item passed in. We could further expand these types to all the self arguments in the Array class.
add overloads for Ops.allocate that support up to 4d data with return types of Floats(n)d
the ND case is tricky and doesn't play nice with other overloads (because it catches all of them) so separate it out into allocate_nd","Looking good! I only got part of the way through switching to the more specific allocation functions. I think the result looks pretty good, and it definitely makes the types easy.",True,{}
explosion/thinc,https://github.com/explosion/thinc,177,2020-01-07T01:48:20Z,2020-01-07T20:14:52Z,2020-01-07T20:15:11Z,MERGED,True,245,155,29,https://github.com/justindujardin,Add generic types to Array and Ops.allocate,14,[],https://github.com/explosion/thinc/pull/177,https://github.com/honnibal,8,https://github.com/explosion/thinc/pull/177#issuecomment-571752438,"for array generics work fine to give back the type of the item passed in. We could further expand these types to all the self arguments in the Array class.
add overloads for Ops.allocate that support up to 4d data with return types of Floats(n)d
the ND case is tricky and doesn't play nice with other overloads (because it catches all of them) so separate it out into allocate_nd",,True,{'HOORAY': ['https://github.com/ines']}
explosion/thinc,https://github.com/explosion/thinc,178,2020-01-07T11:20:16Z,2020-01-07T13:34:45Z,2020-01-07T13:46:33Z,MERGED,True,10,5,3,https://github.com/svlandeg,Track origin of ConfigValidationError,8,[],https://github.com/explosion/thinc/pull/178,https://github.com/svlandeg,1,https://github.com/explosion/thinc/pull/178,"When running a config file through validation, the current code would show a tabular format of errors from Pydantic, but without specifying where exactly in the config they came from:
Config validation error

args                 field required
kwargs               field required
depth                extra fields not permitted
embed_size           extra fields not permitted
maxout_pieces        extra fields not permitted
pretrained_vectors   extra fields not permitted
width                extra fields not permitted
window_size          extra fields not permitted

This PR changes that into:
Config validation error

nlp.pipeline.tok2vec.model   args                 field required
nlp.pipeline.tok2vec.model   kwargs               field required
nlp.pipeline.tok2vec.model   depth                extra fields not permitted
nlp.pipeline.tok2vec.model   embed_size           extra fields not permitted
nlp.pipeline.tok2vec.model   maxout_pieces        extra fields not permitted
nlp.pipeline.tok2vec.model   pretrained_vectors   extra fields not permitted
nlp.pipeline.tok2vec.model   width                extra fields not permitted
nlp.pipeline.tok2vec.model   window_size          extra fields not permitted

By keeping track of the keys we're running through as we're parsing and validating the config.","When running a config file through validation, the current code would show a tabular format of errors from Pydantic, but without specifying where exactly in the config they came from:
Config validation error

args                 field required
kwargs               field required
depth                extra fields not permitted
embed_size           extra fields not permitted
maxout_pieces        extra fields not permitted
pretrained_vectors   extra fields not permitted
width                extra fields not permitted
window_size          extra fields not permitted

This PR changes that into:
Config validation error

nlp.pipeline.tok2vec.model   args                 field required
nlp.pipeline.tok2vec.model   kwargs               field required
nlp.pipeline.tok2vec.model   depth                extra fields not permitted
nlp.pipeline.tok2vec.model   embed_size           extra fields not permitted
nlp.pipeline.tok2vec.model   maxout_pieces        extra fields not permitted
nlp.pipeline.tok2vec.model   pretrained_vectors   extra fields not permitted
nlp.pipeline.tok2vec.model   width                extra fields not permitted
nlp.pipeline.tok2vec.model   window_size          extra fields not permitted

By keeping track of the keys we're running through as we're parsing and validating the config.",True,{'THUMBS_UP': ['https://github.com/ines']}
explosion/thinc,https://github.com/explosion/thinc,179,2020-01-07T12:06:20Z,2020-01-07T12:13:48Z,2020-01-07T12:13:53Z,MERGED,True,164,98,54,https://github.com/ines,"Tidy up and register layers, rename Affine",2,[],https://github.com/explosion/thinc/pull/179,https://github.com/ines,1,https://github.com/explosion/thinc/pull/179,"register all layers in function registry
 tidy up
 rename Affine to Linear","register all layers in function registry
 tidy up
 rename Affine to Linear",True,{'HOORAY': ['https://github.com/svlandeg']}
explosion/thinc,https://github.com/explosion/thinc,180,2020-01-07T15:38:15Z,2020-01-08T17:41:26Z,2020-01-22T00:54:22Z,MERGED,True,263,1,8,https://github.com/honnibal,Allow cupy to allocate via pytorch or tensorflow,25,[],https://github.com/explosion/thinc/pull/180,https://github.com/honnibal,1,https://github.com/explosion/thinc/pull/180,"Because getting memory from CUDA is expensive, most deep learning libraries maintain an internal memory pool. This causes a problem when we have two deep learning libraries being used at once: they have their separate pools, and they don't release memory to each other, so you get less memory available.
I don't know how we'll get PyTorch and Tensorflow to co-operate, but cupy offers a nice solution. You can change cupy's memory allocator, so what I'm suggesting here is an allocator that requests memory from PyTorch. This puts PyTorch in charge of everything, so there's only one memory pool.
I don't know how we want the user experience for this to look. I guess they should call a function to do it?
It would be nice to have another memory pool option for cupy+tensorflow. Hopefully tensorflow offers a way to get blobs of memory? If not, we can go via DLPack I think.","Because getting memory from CUDA is expensive, most deep learning libraries maintain an internal memory pool. This causes a problem when we have two deep learning libraries being used at once: they have their separate pools, and they don't release memory to each other, so you get less memory available.
I don't know how we'll get PyTorch and Tensorflow to co-operate, but cupy offers a nice solution. You can change cupy's memory allocator, so what I'm suggesting here is an allocator that requests memory from PyTorch. This puts PyTorch in charge of everything, so there's only one memory pool.
I don't know how we want the user experience for this to look. I guess they should call a function to do it?
It would be nice to have another memory pool option for cupy+tensorflow. Hopefully tensorflow offers a way to get blobs of memory? If not, we can go via DLPack I think.",True,{}
explosion/thinc,https://github.com/explosion/thinc,180,2020-01-07T15:38:15Z,2020-01-08T17:41:26Z,2020-01-22T00:54:22Z,MERGED,True,263,1,8,https://github.com/honnibal,Allow cupy to allocate via pytorch or tensorflow,25,[],https://github.com/explosion/thinc/pull/180,https://github.com/apps/codecov,2,https://github.com/explosion/thinc/pull/180#issuecomment-572048277,"Because getting memory from CUDA is expensive, most deep learning libraries maintain an internal memory pool. This causes a problem when we have two deep learning libraries being used at once: they have their separate pools, and they don't release memory to each other, so you get less memory available.
I don't know how we'll get PyTorch and Tensorflow to co-operate, but cupy offers a nice solution. You can change cupy's memory allocator, so what I'm suggesting here is an allocator that requests memory from PyTorch. This puts PyTorch in charge of everything, so there's only one memory pool.
I don't know how we want the user experience for this to look. I guess they should call a function to do it?
It would be nice to have another memory pool option for cupy+tensorflow. Hopefully tensorflow offers a way to get blobs of memory? If not, we can go via DLPack I think.","Codecov Report

 No coverage uploaded for pull request base (develop@f77080f). Click here to learn what that means.
The diff coverage is 9.52%.


@@            Coverage Diff             @@
##             develop     #180   +/-   ##
==========================================
  Coverage           ?   61.91%           
==========================================
  Files              ?       62           
  Lines              ?     3442           
  Branches           ?        0           
==========================================
  Hits               ?     2131           
  Misses             ?     1311           
  Partials           ?        0



Impacted Files
Coverage 





thinc/util.py
100% <> ()



thinc/layers/residual.py
27.27% <0%> ()



thinc/backends/_cupy_tensorflow_memory_pool.py
0% <0%> ()



thinc/layers/uniqued.py
32.35% <0%> ()



thinc/model.py
80.55% <0%> ()



thinc/layers/concatenate.py
32.35% <0%> ()



thinc/backends/__init__.py
83.78% <33.33%> ()



thinc/layers/tensorflowwrapper.py
48.14% <50%> ()





Continue to review full report at Codecov.

Legend - Click here to learn more
 = absolute <relative> (impact),  = not affected, ? = missing data
Powered by Codecov. Last update f77080f...efee998. Read the comment docs.",True,{}
explosion/thinc,https://github.com/explosion/thinc,181,2020-01-07T23:32:48Z,2020-01-08T00:57:52Z,2020-01-08T00:57:55Z,MERGED,True,3827,289,31,https://github.com/ines,"Refactor tests, add code coverage etc.",4,[],https://github.com/explosion/thinc/pull/181,https://github.com/ines,1,https://github.com/explosion/thinc/pull/181,,,True,{}
explosion/thinc,https://github.com/explosion/thinc,181,2020-01-07T23:32:48Z,2020-01-08T00:57:52Z,2020-01-08T00:57:55Z,MERGED,True,3827,289,31,https://github.com/ines,"Refactor tests, add code coverage etc.",4,[],https://github.com/explosion/thinc/pull/181,https://github.com/apps/codecov,2,https://github.com/explosion/thinc/pull/181#issuecomment-571827882,,"Codecov Report

 No coverage uploaded for pull request base (develop@67d5510). Click here to learn what that means.
The diff coverage is n/a.


@@            Coverage Diff             @@
##             develop     #181   +/-   ##
==========================================
  Coverage           ?   59.62%           
==========================================
  Files              ?       58           
  Lines              ?     3277           
  Branches           ?        0           
==========================================
  Hits               ?     1954           
  Misses             ?     1323           
  Partials           ?        0

Continue to review full report at Codecov.

Legend - Click here to learn more
 = absolute <relative> (impact),  = not affected, ? = missing data
Powered by Codecov. Last update 67d5510...344295d. Read the comment docs.",True,"{'HOORAY': ['https://github.com/justindujardin'], 'HEART': ['https://github.com/justindujardin']}"
explosion/thinc,https://github.com/explosion/thinc,182,2020-01-08T01:12:00Z,2020-01-08T11:17:41Z,2020-01-08T11:23:15Z,MERGED,True,185,78,4,https://github.com/ines,Fix up config/registry API and get coverage to 100%,4,[],https://github.com/explosion/thinc/pull/182,https://github.com/ines,1,https://github.com/explosion/thinc/pull/182,"remove unused code
 make config validation error traceback nicer (add root element to location list)
 handle all exceptions in function calls (not just TypeErrors)
 return True for is_promise, even if multiple @ values are present (will raise an error afterwards anyways)
 make sure all errors related to config are ConfigValidationErrors
 make syntax for positional (spread) arguments work via __args__:

[config]
@schedules = ""my_cool_schedule.v1""
__args__ = [10, 16, 31]
final = 20
@thinc.registry.schedules(""my_cool_schedule.v1"")
def my_cool_schedule(*steps: int, final: int = 10):
    yield from steps
    while True:
        yield final

 get test coverage for module to 100% ","remove unused code
 make config validation error traceback nicer (add root element to location list)
 handle all exceptions in function calls (not just TypeErrors)
 return True for is_promise, even if multiple @ values are present (will raise an error afterwards anyways)
 make sure all errors related to config are ConfigValidationErrors
 make syntax for positional (spread) arguments work via __args__:

[config]
@schedules = ""my_cool_schedule.v1""
__args__ = [10, 16, 31]
final = 20
@thinc.registry.schedules(""my_cool_schedule.v1"")
def my_cool_schedule(*steps: int, final: int = 10):
    yield from steps
    while True:
        yield final

 get test coverage for module to 100% ",True,"{'HOORAY': ['https://github.com/justindujardin', 'https://github.com/svlandeg']}"
explosion/thinc,https://github.com/explosion/thinc,182,2020-01-08T01:12:00Z,2020-01-08T11:17:41Z,2020-01-08T11:23:15Z,MERGED,True,185,78,4,https://github.com/ines,Fix up config/registry API and get coverage to 100%,4,[],https://github.com/explosion/thinc/pull/182,https://github.com/apps/codecov,2,https://github.com/explosion/thinc/pull/182#issuecomment-571846916,"remove unused code
 make config validation error traceback nicer (add root element to location list)
 handle all exceptions in function calls (not just TypeErrors)
 return True for is_promise, even if multiple @ values are present (will raise an error afterwards anyways)
 make sure all errors related to config are ConfigValidationErrors
 make syntax for positional (spread) arguments work via __args__:

[config]
@schedules = ""my_cool_schedule.v1""
__args__ = [10, 16, 31]
final = 20
@thinc.registry.schedules(""my_cool_schedule.v1"")
def my_cool_schedule(*steps: int, final: int = 10):
    yield from steps
    while True:
        yield final

 get test coverage for module to 100% ","Codecov Report

Merging #182 into develop will increase coverage by 0.64%.
The diff coverage is 100%.


@@             Coverage Diff             @@
##           develop     #182      +/-   ##
===========================================
+ Coverage    59.62%   60.27%   +0.64%     
===========================================
  Files           58       58              
  Lines         3277     3280       +3     
===========================================
+ Hits          1954     1977      +23     
+ Misses        1323     1303      -20



Impacted Files
Coverage 





thinc/config.py
100% <100%> (+10.41%)





Continue to review full report at Codecov.

Legend - Click here to learn more
 = absolute <relative> (impact),  = not affected, ? = missing data
Powered by Codecov. Last update 14cdc2c...c7eac38. Read the comment docs.",True,{}
explosion/thinc,https://github.com/explosion/thinc,183,2020-01-08T10:18:05Z,2020-01-08T11:17:17Z,2020-01-08T11:17:35Z,MERGED,True,4,3,2,https://github.com/svlandeg,"fix input type for concatenate to support ""a | b""",1,[],https://github.com/explosion/thinc/pull/183,https://github.com/svlandeg,1,https://github.com/explosion/thinc/pull/183,"Changed the signature for concatenate() to *layers: Model instead of using a List, to allow using the operator in cases like prefix | suffix. The current version gave the error

concatenate() takes 1 positional argument but 2 were given","Changed the signature for concatenate() to *layers: Model instead of using a List, to allow using the operator in cases like prefix | suffix. The current version gave the error

concatenate() takes 1 positional argument but 2 were given",True,{'THUMBS_UP': ['https://github.com/honnibal']}
explosion/thinc,https://github.com/explosion/thinc,183,2020-01-08T10:18:05Z,2020-01-08T11:17:17Z,2020-01-08T11:17:35Z,MERGED,True,4,3,2,https://github.com/svlandeg,"fix input type for concatenate to support ""a | b""",1,[],https://github.com/explosion/thinc/pull/183,https://github.com/apps/codecov,2,https://github.com/explosion/thinc/pull/183#issuecomment-571984423,"Changed the signature for concatenate() to *layers: Model instead of using a List, to allow using the operator in cases like prefix | suffix. The current version gave the error

concatenate() takes 1 positional argument but 2 were given","Codecov Report

Merging #183 into develop will not change coverage.
The diff coverage is 100%.


@@           Coverage Diff            @@
##           develop     #183   +/-   ##
========================================
  Coverage    59.62%   59.62%           
========================================
  Files           58       58           
  Lines         3277     3277           
========================================
  Hits          1954     1954           
  Misses        1323     1323



Impacted Files
Coverage 





thinc/layers/chain.py
92.3% <100%> ()



thinc/layers/concatenate.py
31.42% <100%> ()





Continue to review full report at Codecov.

Legend - Click here to learn more
 = absolute <relative> (impact),  = not affected, ? = missing data
Powered by Codecov. Last update cf3694d...5f430a6. Read the comment docs.",True,{'EYES': ['https://github.com/svlandeg']}
explosion/thinc,https://github.com/explosion/thinc,183,2020-01-08T10:18:05Z,2020-01-08T11:17:17Z,2020-01-08T11:17:35Z,MERGED,True,4,3,2,https://github.com/svlandeg,"fix input type for concatenate to support ""a | b""",1,[],https://github.com/explosion/thinc/pull/183,https://github.com/honnibal,3,https://github.com/explosion/thinc/pull/183#issuecomment-572004637,"Changed the signature for concatenate() to *layers: Model instead of using a List, to allow using the operator in cases like prefix | suffix. The current version gave the error

concatenate() takes 1 positional argument but 2 were given","Thanks! I typed this wrongly. I think we can improve this later actually: we should use type variables so that it's Model[InT, OutT]: the concatenate operator only works on models that are the same input/output type, e.g. you can't concatenate one model that takes a Floats3d with a model that takes a Floats2d.",True,{'THUMBS_UP': ['https://github.com/svlandeg']}
explosion/thinc,https://github.com/explosion/thinc,184,2020-01-08T12:53:03Z,2020-01-08T15:18:35Z,2020-01-22T00:54:24Z,MERGED,True,191,34,12,https://github.com/ines,Improve tests and coverage and fix related issues,14,[],https://github.com/explosion/thinc/pull/184,https://github.com/ines,1,https://github.com/explosion/thinc/pull/184,,,True,{}
explosion/thinc,https://github.com/explosion/thinc,184,2020-01-08T12:53:03Z,2020-01-08T15:18:35Z,2020-01-22T00:54:24Z,MERGED,True,191,34,12,https://github.com/ines,Improve tests and coverage and fix related issues,14,[],https://github.com/explosion/thinc/pull/184,https://github.com/apps/codecov,2,https://github.com/explosion/thinc/pull/184#issuecomment-572037147,,"Codecov Report

Merging #184 into develop will increase coverage by 1.03%.
The diff coverage is 64%.


@@             Coverage Diff             @@
##           develop     #184      +/-   ##
===========================================
+ Coverage    61.08%   62.11%   +1.03%     
===========================================
  Files           60       60              
  Lines         3438     3389      -49     
===========================================
+ Hits          2100     2105       +5     
+ Misses        1338     1284      -54



Impacted Files
Coverage 





thinc/util.py
100% <> (+39.06%)



thinc/shims/__init__.py
100% <100%> ()



thinc/layers/__init__.py
100% <100%> ()



thinc/api.py
100% <100%> ()



thinc/layers/tensorflowwrapper.py
48.14% <50%> (+7.4%)



thinc/shims/tensorflow.py
27% <75%> (+0.38%)





Continue to review full report at Codecov.

Legend - Click here to learn more
 = absolute <relative> (impact),  = not affected, ? = missing data
Powered by Codecov. Last update 4bbc300...0fcb1bd. Read the comment docs.",True,{}
explosion/thinc,https://github.com/explosion/thinc,185,2020-01-08T13:04:26Z,2020-01-08T13:04:44Z,2020-01-08T13:04:44Z,CLOSED,False,446,1,5,https://github.com/Abhijit-2592,Develop,18,[],https://github.com/explosion/thinc/pull/185,https://github.com/Abhijit-2592,1,https://github.com/explosion/thinc/pull/185,,,True,{}
explosion/thinc,https://github.com/explosion/thinc,185,2020-01-08T13:04:26Z,2020-01-08T13:04:44Z,2020-01-08T13:04:44Z,CLOSED,False,446,1,5,https://github.com/Abhijit-2592,Develop,18,[],https://github.com/explosion/thinc/pull/185,https://github.com/Abhijit-2592,2,https://github.com/explosion/thinc/pull/185#issuecomment-572038924,,Wrong request,True,{}
explosion/thinc,https://github.com/explosion/thinc,186,2020-01-08T13:16:05Z,2020-01-08T15:24:04Z,2020-01-08T15:30:31Z,MERGED,True,14,9,4,https://github.com/svlandeg,remove axis attr for hstack,4,[],https://github.com/explosion/thinc/pull/186,https://github.com/svlandeg,1,https://github.com/explosion/thinc/pull/186,"Removing axis argument from hstack - I don't think that argument is valid? Also, attr axis is never defined for the concatenate model ?","Removing axis argument from hstack - I don't think that argument is valid? Also, attr axis is never defined for the concatenate model ?",True,{}
explosion/thinc,https://github.com/explosion/thinc,186,2020-01-08T13:16:05Z,2020-01-08T15:24:04Z,2020-01-08T15:30:31Z,MERGED,True,14,9,4,https://github.com/svlandeg,remove axis attr for hstack,4,[],https://github.com/explosion/thinc/pull/186,https://github.com/apps/codecov,2,https://github.com/explosion/thinc/pull/186#issuecomment-572046853,"Removing axis argument from hstack - I don't think that argument is valid? Also, attr axis is never defined for the concatenate model ?","Codecov Report

Merging #186 into develop will decrease coverage by 0.06%.
The diff coverage is 0%.


@@             Coverage Diff             @@
##           develop     #186      +/-   ##
===========================================
- Coverage    58.96%   58.89%   -0.07%     
===========================================
  Files           60       60              
  Lines         3463     3467       +4     
===========================================
  Hits          2042     2042              
- Misses        1421     1425       +4



Impacted Files
Coverage 





thinc/layers/residual.py
27.27% <0%> (-2.73%)



thinc/layers/uniqued.py
32.35% <0%> (-2.03%)



thinc/model.py
80.55% <0%> ()



thinc/layers/concatenate.py
32.35% <0%> (+0.92%)





Continue to review full report at Codecov.

Legend - Click here to learn more
 = absolute <relative> (impact),  = not affected, ? = missing data
Powered by Codecov. Last update b8fd35d...28a315d. Read the comment docs.",True,{}
explosion/thinc,https://github.com/explosion/thinc,187,2020-01-08T18:33:05Z,2020-01-11T16:02:44Z,2020-01-13T20:13:33Z,MERGED,True,246,46,6,https://github.com/justindujardin,Fix python3.6 support for TensorflowWrapper,21,"['feat / shims', 'third-party', 'interop / tensorflow']",https://github.com/explosion/thinc/pull/187,https://github.com/justindujardin,1,https://github.com/explosion/thinc/pull/187,"tfdlpack only has a pip package for python 3.7 right now. This PR removes tfdlpack and adds more tests for the Tensorflow Wrapper
Changes:

 remove tfdlpack
 add more tests for TensorflowWrapper
 install Tensorflow during CI build so wrapper tests are executed","tfdlpack only has a pip package for python 3.7 right now. This PR removes tfdlpack and adds more tests for the Tensorflow Wrapper
Changes:

 remove tfdlpack
 add more tests for TensorflowWrapper
 install Tensorflow during CI build so wrapper tests are executed",True,{}
explosion/thinc,https://github.com/explosion/thinc,187,2020-01-08T18:33:05Z,2020-01-11T16:02:44Z,2020-01-13T20:13:33Z,MERGED,True,246,46,6,https://github.com/justindujardin,Fix python3.6 support for TensorflowWrapper,21,"['feat / shims', 'third-party', 'interop / tensorflow']",https://github.com/explosion/thinc/pull/187,https://github.com/apps/codecov,2,https://github.com/explosion/thinc/pull/187#issuecomment-572199284,"tfdlpack only has a pip package for python 3.7 right now. This PR removes tfdlpack and adds more tests for the Tensorflow Wrapper
Changes:

 remove tfdlpack
 add more tests for TensorflowWrapper
 install Tensorflow during CI build so wrapper tests are executed","Codecov Report

Merging #187 into develop will decrease coverage by 0.35%.
The diff coverage is 92.15%.


@@             Coverage Diff             @@
##           develop     #187      +/-   ##
===========================================
- Coverage    67.75%   67.39%   -0.36%     
===========================================
  Files           62       62              
  Lines         3566     3533      -33     
===========================================
- Hits          2416     2381      -35     
- Misses        1150     1152       +2



Impacted Files
Coverage 





thinc/layers/list2ragged.py
64.28% <100%> ()



thinc/util.py
100% <100%> ()



thinc/layers/mish.py
39.39% <100%> ()



thinc/layers/featureextractor.py
52.63% <100%> ()



thinc/layers/strings2arrays.py
100% <100%> ()



thinc/layers/meanpool.py
60% <100%> ()



thinc/layers/with_list2array.py
58.62% <100%> ()



thinc/layers/staticvectors.py
36.58% <100%> ()



thinc/layers/maxpool.py
60% <100%> ()



thinc/layers/concatenate.py
48.71% <100%> ()



... and 15 more






Continue to review full report at Codecov.

Legend - Click here to learn more
 = absolute <relative> (impact),  = not affected, ? = missing data
Powered by Codecov. Last update 0782406...4bd298a. Read the comment docs.",True,{}
explosion/thinc,https://github.com/explosion/thinc,188,2020-01-08T20:31:47Z,2020-01-08T22:21:45Z,2020-01-08T22:53:05Z,MERGED,True,40,9,2,https://github.com/svlandeg,WIP: Trouble with recursive objects in config,3,[],https://github.com/explosion/thinc/pull/188,https://github.com/svlandeg,1,https://github.com/explosion/thinc/pull/188,"I'm pretty certain there's a bug in parsing recursive objects in thinc's _fill function on the current develop branch. I ran into it while testing from spaCy, so I tried creating a minimal example for testing here in test_config.py, avoiding generators and iterators etc for simplicity.
It looks like an object within an object is not returned properly. I tried debugging with this example. The my_cool_repetitive_schedule.v1 creates the correct learn_rate object [0.001, 0.001, 0.001, 0.001] which is correctly stored in validation[key]. But then when the recursive loop goes on to fill in my_cool_optimizer.v1, it doesn't have access to that object anymore, and fills in the original json {'@schedules': 'my_cool_repetitive_schedule.v1', 'base_rate': 0.001, 'repeat': 4} as learn_rate parameter instead.
You'll see this when the unit test crashes on assert. While the optimizer object is correct and has a proper field b1 (from beta1), its learn_rate field is still a json string instead of the List.
I've been trying to fix this but no luck so far. @honnibal: could you have a look ?
PS: also fixed EXAMPLE_CONFIG which had some invalid arguments for warmup_linear.v1","I'm pretty certain there's a bug in parsing recursive objects in thinc's _fill function on the current develop branch. I ran into it while testing from spaCy, so I tried creating a minimal example for testing here in test_config.py, avoiding generators and iterators etc for simplicity.
It looks like an object within an object is not returned properly. I tried debugging with this example. The my_cool_repetitive_schedule.v1 creates the correct learn_rate object [0.001, 0.001, 0.001, 0.001] which is correctly stored in validation[key]. But then when the recursive loop goes on to fill in my_cool_optimizer.v1, it doesn't have access to that object anymore, and fills in the original json {'@schedules': 'my_cool_repetitive_schedule.v1', 'base_rate': 0.001, 'repeat': 4} as learn_rate parameter instead.
You'll see this when the unit test crashes on assert. While the optimizer object is correct and has a proper field b1 (from beta1), its learn_rate field is still a json string instead of the List.
I've been trying to fix this but no luck so far. @honnibal: could you have a look ?
PS: also fixed EXAMPLE_CONFIG which had some invalid arguments for warmup_linear.v1",True,{}
explosion/thinc,https://github.com/explosion/thinc,188,2020-01-08T20:31:47Z,2020-01-08T22:21:45Z,2020-01-08T22:53:05Z,MERGED,True,40,9,2,https://github.com/svlandeg,WIP: Trouble with recursive objects in config,3,[],https://github.com/explosion/thinc/pull/188,https://github.com/ines,2,https://github.com/explosion/thinc/pull/188#issuecomment-572249368,"I'm pretty certain there's a bug in parsing recursive objects in thinc's _fill function on the current develop branch. I ran into it while testing from spaCy, so I tried creating a minimal example for testing here in test_config.py, avoiding generators and iterators etc for simplicity.
It looks like an object within an object is not returned properly. I tried debugging with this example. The my_cool_repetitive_schedule.v1 creates the correct learn_rate object [0.001, 0.001, 0.001, 0.001] which is correctly stored in validation[key]. But then when the recursive loop goes on to fill in my_cool_optimizer.v1, it doesn't have access to that object anymore, and fills in the original json {'@schedules': 'my_cool_repetitive_schedule.v1', 'base_rate': 0.001, 'repeat': 4} as learn_rate parameter instead.
You'll see this when the unit test crashes on assert. While the optimizer object is correct and has a proper field b1 (from beta1), its learn_rate field is still a json string instead of the List.
I've been trying to fix this but no luck so far. @honnibal: could you have a look ?
PS: also fixed EXAMPLE_CONFIG which had some invalid arguments for warmup_linear.v1",Thanks for the test case  I'll take a look!,True,{'THUMBS_UP': ['https://github.com/svlandeg']}
explosion/thinc,https://github.com/explosion/thinc,188,2020-01-08T20:31:47Z,2020-01-08T22:21:45Z,2020-01-08T22:53:05Z,MERGED,True,40,9,2,https://github.com/svlandeg,WIP: Trouble with recursive objects in config,3,[],https://github.com/explosion/thinc/pull/188,https://github.com/ines,3,https://github.com/explosion/thinc/pull/188#issuecomment-572254924,"I'm pretty certain there's a bug in parsing recursive objects in thinc's _fill function on the current develop branch. I ran into it while testing from spaCy, so I tried creating a minimal example for testing here in test_config.py, avoiding generators and iterators etc for simplicity.
It looks like an object within an object is not returned properly. I tried debugging with this example. The my_cool_repetitive_schedule.v1 creates the correct learn_rate object [0.001, 0.001, 0.001, 0.001] which is correctly stored in validation[key]. But then when the recursive loop goes on to fill in my_cool_optimizer.v1, it doesn't have access to that object anymore, and fills in the original json {'@schedules': 'my_cool_repetitive_schedule.v1', 'base_rate': 0.001, 'repeat': 4} as learn_rate parameter instead.
You'll see this when the unit test crashes on assert. While the optimizer object is correct and has a proper field b1 (from beta1), its learn_rate field is still a json string instead of the List.
I've been trying to fix this but no luck so far. @honnibal: could you have a look ?
PS: also fixed EXAMPLE_CONFIG which had some invalid arguments for warmup_linear.v1","I think I got it! Took a bit of trial and error but I think the problem here was that we weren't generating the arguments that are passed into the function from the already generated objects. And it looks like we were also not passing the tree of generated objects down correctly.
(We probably want to add some more tests covering complex cases later on.)",True,{'HOORAY': ['https://github.com/svlandeg']}
explosion/thinc,https://github.com/explosion/thinc,188,2020-01-08T20:31:47Z,2020-01-08T22:21:45Z,2020-01-08T22:53:05Z,MERGED,True,40,9,2,https://github.com/svlandeg,WIP: Trouble with recursive objects in config,3,[],https://github.com/explosion/thinc/pull/188,https://github.com/apps/codecov,4,https://github.com/explosion/thinc/pull/188#issuecomment-572255536,"I'm pretty certain there's a bug in parsing recursive objects in thinc's _fill function on the current develop branch. I ran into it while testing from spaCy, so I tried creating a minimal example for testing here in test_config.py, avoiding generators and iterators etc for simplicity.
It looks like an object within an object is not returned properly. I tried debugging with this example. The my_cool_repetitive_schedule.v1 creates the correct learn_rate object [0.001, 0.001, 0.001, 0.001] which is correctly stored in validation[key]. But then when the recursive loop goes on to fill in my_cool_optimizer.v1, it doesn't have access to that object anymore, and fills in the original json {'@schedules': 'my_cool_repetitive_schedule.v1', 'base_rate': 0.001, 'repeat': 4} as learn_rate parameter instead.
You'll see this when the unit test crashes on assert. While the optimizer object is correct and has a proper field b1 (from beta1), its learn_rate field is still a json string instead of the List.
I've been trying to fix this but no luck so far. @honnibal: could you have a look ?
PS: also fixed EXAMPLE_CONFIG which had some invalid arguments for warmup_linear.v1","Codecov Report

Merging #188 into develop will not change coverage.
The diff coverage is 100%.


@@           Coverage Diff            @@
##           develop     #188   +/-   ##
========================================
  Coverage    61.91%   61.91%           
========================================
  Files           62       62           
  Lines         3442     3442           
========================================
  Hits          2131     2131           
  Misses        1311     1311



Impacted Files
Coverage 





thinc/config.py
100% <100%> ()





Continue to review full report at Codecov.

Legend - Click here to learn more
 = absolute <relative> (impact),  = not affected, ? = missing data
Powered by Codecov. Last update 4700c1c...c249c83. Read the comment docs.",True,{}
explosion/thinc,https://github.com/explosion/thinc,189,2020-01-08T22:12:15Z,2020-01-09T01:15:59Z,2020-01-09T01:16:03Z,MERGED,True,53,22,2,https://github.com/ines,Various config fixes,6,[],https://github.com/explosion/thinc/pull/189,https://github.com/ines,1,https://github.com/explosion/thinc/pull/189,"Variable positional arguments can now be declared with *:
[some_block]
* = [1, 2, 3]
The config now also allows nested objects, which is very useful for combinators like chain:
[model]
@layers = ""chain.v0""

[model.*.relu1]
@layers = ""ReLu.v0""
nO = ${hyper_params:n_hidden}
dropout = ${hyper_params:dropout}

[model.*.relu2]
@layers = ""ReLu.v0""
nO = ${hyper_params:n_hidden}
dropout = ${hyper_params:dropout}","Variable positional arguments can now be declared with *:
[some_block]
* = [1, 2, 3]
The config now also allows nested objects, which is very useful for combinators like chain:
[model]
@layers = ""chain.v0""

[model.*.relu1]
@layers = ""ReLu.v0""
nO = ${hyper_params:n_hidden}
dropout = ${hyper_params:dropout}

[model.*.relu2]
@layers = ""ReLu.v0""
nO = ${hyper_params:n_hidden}
dropout = ${hyper_params:dropout}",True,{}
explosion/thinc,https://github.com/explosion/thinc,189,2020-01-08T22:12:15Z,2020-01-09T01:15:59Z,2020-01-09T01:16:03Z,MERGED,True,53,22,2,https://github.com/ines,Various config fixes,6,[],https://github.com/explosion/thinc/pull/189,https://github.com/apps/codecov,2,https://github.com/explosion/thinc/pull/189#issuecomment-572328562,"Variable positional arguments can now be declared with *:
[some_block]
* = [1, 2, 3]
The config now also allows nested objects, which is very useful for combinators like chain:
[model]
@layers = ""chain.v0""

[model.*.relu1]
@layers = ""ReLu.v0""
nO = ${hyper_params:n_hidden}
dropout = ${hyper_params:dropout}

[model.*.relu2]
@layers = ""ReLu.v0""
nO = ${hyper_params:n_hidden}
dropout = ${hyper_params:dropout}","Codecov Report

 No coverage uploaded for pull request base (develop@da39522). Click here to learn what that means.
The diff coverage is 100%.


@@            Coverage Diff            @@
##             develop    #189   +/-   ##
=========================================
  Coverage           ?   62.3%           
=========================================
  Files              ?      62           
  Lines              ?    3449           
  Branches           ?       0           
=========================================
  Hits               ?    2149           
  Misses             ?    1300           
  Partials           ?       0



Impacted Files
Coverage 





thinc/config.py
100% <100%> ()





Continue to review full report at Codecov.

Legend - Click here to learn more
 = absolute <relative> (impact),  = not affected, ? = missing data
Powered by Codecov. Last update da39522...4603a24. Read the comment docs.",True,{}
explosion/thinc,https://github.com/explosion/thinc,190,2020-01-09T01:57:40Z,2020-01-09T17:14:04Z,2020-01-09T17:14:13Z,MERGED,True,461,255,21,https://github.com/ines,"Refactor examples and add more tests, add strings2arrays transform",16,[],https://github.com/explosion/thinc/pull/190,https://github.com/ines,1,https://github.com/explosion/thinc/pull/190,,,True,{}
explosion/thinc,https://github.com/explosion/thinc,190,2020-01-09T01:57:40Z,2020-01-09T17:14:04Z,2020-01-09T17:14:13Z,MERGED,True,461,255,21,https://github.com/ines,"Refactor examples and add more tests, add strings2arrays transform",16,[],https://github.com/explosion/thinc/pull/190,https://github.com/apps/codecov,2,https://github.com/explosion/thinc/pull/190#issuecomment-572345805,,"Codecov Report

Merging #190 into develop will increase coverage by 0.19%.
The diff coverage is 95.23%.


@@             Coverage Diff             @@
##           develop     #190      +/-   ##
===========================================
+ Coverage    64.24%   64.44%   +0.19%     
===========================================
  Files           61       62       +1     
  Lines         3384     3403      +19     
===========================================
+ Hits          2174     2193      +19     
  Misses        1210     1210



Impacted Files
Coverage 





thinc/layers/hashembed.py
77.5% <0%> ()



thinc/api.py
100% <100%> ()



thinc/layers/strings2arrays.py
100% <100%> ()



thinc/layers/__init__.py
100% <100%> ()



thinc/config.py
100% <100%> ()





Continue to review full report at Codecov.

Legend - Click here to learn more
 = absolute <relative> (impact),  = not affected, ? = missing data
Powered by Codecov. Last update 27e83d2...c2b0009. Read the comment docs.",True,{}
explosion/thinc,https://github.com/explosion/thinc,191,2020-01-09T11:54:19Z,2020-01-09T17:16:17Z,2020-01-09T17:19:27Z,MERGED,True,24,14,4,https://github.com/svlandeg,Friendly debugging errors,6,[],https://github.com/explosion/thinc/pull/191,https://github.com/svlandeg,1,https://github.com/explosion/thinc/pull/191,"Model/layer names in error msg helps debugging
Ensure HashEmbed has a default None nI dimension
Throw an error when a none-None dimension is set to a different value (this shouldn't really happen, right ?)","Model/layer names in error msg helps debugging
Ensure HashEmbed has a default None nI dimension
Throw an error when a none-None dimension is set to a different value (this shouldn't really happen, right ?)",True,{'THUMBS_UP': ['https://github.com/honnibal']}
explosion/thinc,https://github.com/explosion/thinc,191,2020-01-09T11:54:19Z,2020-01-09T17:16:17Z,2020-01-09T17:19:27Z,MERGED,True,24,14,4,https://github.com/svlandeg,Friendly debugging errors,6,[],https://github.com/explosion/thinc/pull/191,https://github.com/apps/codecov,2,https://github.com/explosion/thinc/pull/191#issuecomment-572529919,"Model/layer names in error msg helps debugging
Ensure HashEmbed has a default None nI dimension
Throw an error when a none-None dimension is set to a different value (this shouldn't really happen, right ?)","Codecov Report

Merging #191 into develop will decrease coverage by 0.07%.
The diff coverage is 25%.


@@             Coverage Diff             @@
##           develop     #191      +/-   ##
===========================================
- Coverage     62.3%   62.23%   -0.08%     
===========================================
  Files           62       62              
  Lines         3449     3458       +9     
===========================================
+ Hits          2149     2152       +3     
- Misses        1300     1306       +6



Impacted Files
Coverage 





thinc/layers/hashembed.py
77.5% <> ()



thinc/layers/maxout.py
61.11% <> ()



thinc/model.py
79.5% <25%> (-1.05%)





Continue to review full report at Codecov.

Legend - Click here to learn more
 = absolute <relative> (impact),  = not affected, ? = missing data
Powered by Codecov. Last update 3dd05dc...2097126. Read the comment docs.",True,{}
explosion/thinc,https://github.com/explosion/thinc,191,2020-01-09T11:54:19Z,2020-01-09T17:16:17Z,2020-01-09T17:19:27Z,MERGED,True,24,14,4,https://github.com/svlandeg,Friendly debugging errors,6,[],https://github.com/explosion/thinc/pull/191,https://github.com/svlandeg,3,https://github.com/explosion/thinc/pull/191#issuecomment-572556227,"Model/layer names in error msg helps debugging
Ensure HashEmbed has a default None nI dimension
Throw an error when a none-None dimension is set to a different value (this shouldn't really happen, right ?)","(Maybe hold off on merge, will probably be adding few more smaller edits to this one in the next few hours)",True,{}
explosion/thinc,https://github.com/explosion/thinc,191,2020-01-09T11:54:19Z,2020-01-09T17:16:17Z,2020-01-09T17:19:27Z,MERGED,True,24,14,4,https://github.com/svlandeg,Friendly debugging errors,6,[],https://github.com/explosion/thinc/pull/191,https://github.com/svlandeg,4,https://github.com/explosion/thinc/pull/191#issuecomment-572646217,"Model/layer names in error msg helps debugging
Ensure HashEmbed has a default None nI dimension
Throw an error when a none-None dimension is set to a different value (this shouldn't really happen, right ?)","Ready for review. I'm not sure about throwing the error for dimension changes, but it's been really helpful for me for debugging so far. Maybe it should be an optional error or a warning instead ?",True,{}
explosion/thinc,https://github.com/explosion/thinc,191,2020-01-09T11:54:19Z,2020-01-09T17:16:17Z,2020-01-09T17:19:27Z,MERGED,True,24,14,4,https://github.com/svlandeg,Friendly debugging errors,6,[],https://github.com/explosion/thinc/pull/191,https://github.com/honnibal,5,https://github.com/explosion/thinc/pull/191#issuecomment-572660432,"Model/layer names in error msg helps debugging
Ensure HashEmbed has a default None nI dimension
Throw an error when a none-None dimension is set to a different value (this shouldn't really happen, right ?)","Looks good! Definitely agree about the error messages.
I think we should have the set_dim error for now, because currently this will result in broken behaviour. In future we can look at supporting resizable layers. Maybe it should always be a create-and-replace though? Resize in-place will probably cause lots of problems.",True,{}
explosion/thinc,https://github.com/explosion/thinc,192,2020-01-09T17:06:20Z,2020-01-09T18:31:11Z,2020-01-09T18:31:13Z,MERGED,True,307,140,8,https://github.com/honnibal,"Improve flexibility of PyTorchWrapper. Various utils too, new Ar",13,[],https://github.com/explosion/thinc/pull/192,https://github.com/honnibal,1,https://github.com/explosion/thinc/pull/192,"Wrapping PyTorch neatly is really tricky, because the layers can take anything as their inputs and outputs, and you don't necessarily need to backprop to all of the outputs. It's important that the wrapper covers a lot of cases very easily -- if people are always needing to write their own wrappers, it won't be very appealing.
Here's the best I've come up with. I've introduced a new data type, ArgsKwargs, that represents a tuple (args, kwargs) to be passed into a function. The Shim class now expects an ArgsKwargs object as input.
The PyTorchWrapper now supports two optional functions, convert_inputs and convert_outputs. These should perform the xp-to-torch and torch-to-xp conversions, handling any other mappings to prepare the inputs for the PyTorch layer. The functions return a callback to handle the inverse transformation for the backward pass. Initially I had this flattened so that you take four functions in the input, but that was actually much worse, because it's hard to make sure you're returning the same type you received.
I've updated the tensorflow shim as well, although I haven't added options for the converters to it yet.","Wrapping PyTorch neatly is really tricky, because the layers can take anything as their inputs and outputs, and you don't necessarily need to backprop to all of the outputs. It's important that the wrapper covers a lot of cases very easily -- if people are always needing to write their own wrappers, it won't be very appealing.
Here's the best I've come up with. I've introduced a new data type, ArgsKwargs, that represents a tuple (args, kwargs) to be passed into a function. The Shim class now expects an ArgsKwargs object as input.
The PyTorchWrapper now supports two optional functions, convert_inputs and convert_outputs. These should perform the xp-to-torch and torch-to-xp conversions, handling any other mappings to prepare the inputs for the PyTorch layer. The functions return a callback to handle the inverse transformation for the backward pass. Initially I had this flattened so that you take four functions in the input, but that was actually much worse, because it's hard to make sure you're returning the same type you received.
I've updated the tensorflow shim as well, although I haven't added options for the converters to it yet.",True,{}
explosion/thinc,https://github.com/explosion/thinc,192,2020-01-09T17:06:20Z,2020-01-09T18:31:11Z,2020-01-09T18:31:13Z,MERGED,True,307,140,8,https://github.com/honnibal,"Improve flexibility of PyTorchWrapper. Various utils too, new Ar",13,[],https://github.com/explosion/thinc/pull/192,https://github.com/apps/codecov,2,https://github.com/explosion/thinc/pull/192#issuecomment-572659421,"Wrapping PyTorch neatly is really tricky, because the layers can take anything as their inputs and outputs, and you don't necessarily need to backprop to all of the outputs. It's important that the wrapper covers a lot of cases very easily -- if people are always needing to write their own wrappers, it won't be very appealing.
Here's the best I've come up with. I've introduced a new data type, ArgsKwargs, that represents a tuple (args, kwargs) to be passed into a function. The Shim class now expects an ArgsKwargs object as input.
The PyTorchWrapper now supports two optional functions, convert_inputs and convert_outputs. These should perform the xp-to-torch and torch-to-xp conversions, handling any other mappings to prepare the inputs for the PyTorch layer. The functions return a callback to handle the inverse transformation for the backward pass. Initially I had this flattened so that you take four functions in the input, but that was actually much worse, because it's hard to make sure you're returning the same type you received.
I've updated the tensorflow shim as well, although I haven't added options for the converters to it yet.","Codecov Report

Merging #192 into develop will increase coverage by 3.95%.
The diff coverage is 96%.


@@             Coverage Diff             @@
##           develop     #192      +/-   ##
===========================================
+ Coverage    61.01%   64.97%   +3.95%     
===========================================
  Files           62       62              
  Lines         3558     3640      +82     
===========================================
+ Hits          2171     2365     +194     
+ Misses        1387     1275     -112



Impacted Files
Coverage 





thinc/backends/__init__.py
100% <> (+16.21%)



thinc/model.py
91.08% <> (+10.53%)



thinc/layers/pytorchwrapper.py
28.84% <0%> ()



thinc/util.py
99.01% <100%> (+23.79%)



thinc/api.py
100% <100%> ()



thinc/config.py
100% <100%> ()



thinc/layers/__init__.py
100% <100%> ()



thinc/layers/strings2arrays.py
100% <100%> ()



thinc/types.py
69.14% <0%> (+2.47%)



... and 4 more






Continue to review full report at Codecov.

Legend - Click here to learn more
 = absolute <relative> (impact),  = not affected, ? = missing data
Powered by Codecov. Last update a5ea2a8...9121a7b. Read the comment docs.",True,{}
explosion/thinc,https://github.com/explosion/thinc,193,2020-01-09T17:48:58Z,2020-01-09T18:18:56Z,2020-01-09T19:01:41Z,MERGED,True,61,4,2,https://github.com/svlandeg,Fix concatenate,5,[],https://github.com/explosion/thinc/pull/193,https://github.com/svlandeg,1,https://github.com/explosion/thinc/pull/193,"Make concatenate init look a bit more like chain. There was also a bug because layers was not passed on to the Model construction.
Added some more unit tests to test concatenate and chain with >2 models. And variants that use the >> and | operators.","Make concatenate init look a bit more like chain. There was also a bug because layers was not passed on to the Model construction.
Added some more unit tests to test concatenate and chain with >2 models. And variants that use the >> and | operators.",True,{'HOORAY': ['https://github.com/honnibal']}
explosion/thinc,https://github.com/explosion/thinc,193,2020-01-09T17:48:58Z,2020-01-09T18:18:56Z,2020-01-09T19:01:41Z,MERGED,True,61,4,2,https://github.com/svlandeg,Fix concatenate,5,[],https://github.com/explosion/thinc/pull/193,https://github.com/apps/codecov,2,https://github.com/explosion/thinc/pull/193#issuecomment-572676356,"Make concatenate init look a bit more like chain. There was also a bug because layers was not passed on to the Model construction.
Added some more unit tests to test concatenate and chain with >2 models. And variants that use the >> and | operators.","Codecov Report

 No coverage uploaded for pull request base (develop@d8160c7). Click here to learn what that means.
The diff coverage is 87.5%.


@@            Coverage Diff             @@
##             develop     #193   +/-   ##
==========================================
  Coverage           ?   64.53%           
==========================================
  Files              ?       62           
  Lines              ?     3417           
  Branches           ?        0           
==========================================
  Hits               ?     2205           
  Misses             ?     1212           
  Partials           ?        0



Impacted Files
Coverage 





thinc/layers/hashembed.py
77.5% <> ()



thinc/backends/__init__.py
100% <> ()



thinc/layers/maxout.py
61.11% <> ()



thinc/api.py
100% <100%> ()



thinc/config.py
100% <100%> ()



thinc/layers/__init__.py
100% <100%> ()



thinc/layers/strings2arrays.py
100% <100%> ()



thinc/model.py
89.05% <75%> ()





Continue to review full report at Codecov.

Legend - Click here to learn more
 = absolute <relative> (impact),  = not affected, ? = missing data
Powered by Codecov. Last update d8160c7...7753125. Read the comment docs.",True,{}
explosion/thinc,https://github.com/explosion/thinc,194,2020-01-09T19:28:47Z,2020-01-13T17:20:57Z,2020-01-13T17:20:57Z,CLOSED,False,960,3,6,https://github.com/justindujardin,Add mypy tests to test suite WIP,11,"['tests', 'feat / types']",https://github.com/explosion/thinc/pull/194,https://github.com/justindujardin,1,https://github.com/explosion/thinc/pull/194,"It's come up that types are not always clear, and knowing that we're returning Model[Floats2d, Floats2d] instead of Any is important.
Mypy has existing test machinery that is used in other plugins, so I copied it. It's pretty gross but does the job of asserting about types.
The tests exist in thinc/tests/test-data/unit which is hardcoded in the mypy test runner, so we can't rename or simplify the hierarchy. 
They are plaintext files ending in .test extension, and have a syntax similar to ini files with bracketed sections:
[case test_mypy_control_flow_type_discrimination]
from typing import Optional

def my_foo(op: Optional[int]):
   reveal_type(op) # N: Revealed type is 'Union[builtins.int, None]'
   assert op is not None
   reveal_type(op) # N: Revealed type is 'builtins.int'
[out]

The comment after reveal_type is used by the test suite to make that an assertion. You can use wildcards in the asserted type name if you want to be less specific, e.g. builtins.*
This also cleans up the return types for model forward functions, and adds a test:
from thinc.model import Model, ModelFwd
from thinc.layers import Linear
from thinc.types import Floats2d, Array
from thinc.backends import get_current_ops, Ops

ops: Ops = get_current_ops()
example = ops.alloc_f2d(1, 12)
label = ops.alloc_f2d(1, 1)
model = Linear(12)

def forward(model: Model, X: Floats2d, is_train: bool) -> ModelFwd[Floats2d]:
    def backprop(dY: Floats2d) -> Floats2d:
        pass

    return label, backprop


reveal_type(forward(model, example, False))
# Revealed type is 'Tuple[thinc.types.Floats2d, def (thinc.types.Floats2d) -> thinc.types.Floats2d]'","It's come up that types are not always clear, and knowing that we're returning Model[Floats2d, Floats2d] instead of Any is important.
Mypy has existing test machinery that is used in other plugins, so I copied it. It's pretty gross but does the job of asserting about types.
The tests exist in thinc/tests/test-data/unit which is hardcoded in the mypy test runner, so we can't rename or simplify the hierarchy. 
They are plaintext files ending in .test extension, and have a syntax similar to ini files with bracketed sections:
[case test_mypy_control_flow_type_discrimination]
from typing import Optional

def my_foo(op: Optional[int]):
   reveal_type(op) # N: Revealed type is 'Union[builtins.int, None]'
   assert op is not None
   reveal_type(op) # N: Revealed type is 'builtins.int'
[out]

The comment after reveal_type is used by the test suite to make that an assertion. You can use wildcards in the asserted type name if you want to be less specific, e.g. builtins.*
This also cleans up the return types for model forward functions, and adds a test:
from thinc.model import Model, ModelFwd
from thinc.layers import Linear
from thinc.types import Floats2d, Array
from thinc.backends import get_current_ops, Ops

ops: Ops = get_current_ops()
example = ops.alloc_f2d(1, 12)
label = ops.alloc_f2d(1, 1)
model = Linear(12)

def forward(model: Model, X: Floats2d, is_train: bool) -> ModelFwd[Floats2d]:
    def backprop(dY: Floats2d) -> Floats2d:
        pass

    return label, backprop


reveal_type(forward(model, example, False))
# Revealed type is 'Tuple[thinc.types.Floats2d, def (thinc.types.Floats2d) -> thinc.types.Floats2d]'",True,{}
explosion/thinc,https://github.com/explosion/thinc,194,2020-01-09T19:28:47Z,2020-01-13T17:20:57Z,2020-01-13T17:20:57Z,CLOSED,False,960,3,6,https://github.com/justindujardin,Add mypy tests to test suite WIP,11,"['tests', 'feat / types']",https://github.com/explosion/thinc/pull/194,https://github.com/justindujardin,2,https://github.com/explosion/thinc/pull/194#issuecomment-573774353,"It's come up that types are not always clear, and knowing that we're returning Model[Floats2d, Floats2d] instead of Any is important.
Mypy has existing test machinery that is used in other plugins, so I copied it. It's pretty gross but does the job of asserting about types.
The tests exist in thinc/tests/test-data/unit which is hardcoded in the mypy test runner, so we can't rename or simplify the hierarchy. 
They are plaintext files ending in .test extension, and have a syntax similar to ini files with bracketed sections:
[case test_mypy_control_flow_type_discrimination]
from typing import Optional

def my_foo(op: Optional[int]):
   reveal_type(op) # N: Revealed type is 'Union[builtins.int, None]'
   assert op is not None
   reveal_type(op) # N: Revealed type is 'builtins.int'
[out]

The comment after reveal_type is used by the test suite to make that an assertion. You can use wildcards in the asserted type name if you want to be less specific, e.g. builtins.*
This also cleans up the return types for model forward functions, and adds a test:
from thinc.model import Model, ModelFwd
from thinc.layers import Linear
from thinc.types import Floats2d, Array
from thinc.backends import get_current_ops, Ops

ops: Ops = get_current_ops()
example = ops.alloc_f2d(1, 12)
label = ops.alloc_f2d(1, 1)
model = Linear(12)

def forward(model: Model, X: Floats2d, is_train: bool) -> ModelFwd[Floats2d]:
    def backprop(dY: Floats2d) -> Floats2d:
        pass

    return label, backprop


reveal_type(forward(model, example, False))
# Revealed type is 'Tuple[thinc.types.Floats2d, def (thinc.types.Floats2d) -> thinc.types.Floats2d]'",I talked with @tiangolo and the test setup from pydantic looks nicer than this because it offers a better developer experience using .py files that IDEs will highlight. Closing this in favor of that.,True,{}
explosion/thinc,https://github.com/explosion/thinc,195,2020-01-09T20:15:39Z,2020-01-09T21:27:02Z,2020-01-10T12:42:05Z,MERGED,True,9,9,1,https://github.com/tiangolo,"WIP: Fix small misconfigs in example, initial pass",2,[],https://github.com/explosion/thinc/pull/195,https://github.com/tiangolo,1,https://github.com/explosion/thinc/pull/195," Fix small misconfigurations in example script.
Just an initial pass that fixes some initial problems detected while using my local branch of Pydantic (with support for generics).
It depends on samuelcolvin/pydantic#1159 merged and released.
There's also a small change to thinc.model.Model, I haven't committed it yet, and it might be unnecessary, depending on the end result of the PR in Pydantic."," Fix small misconfigurations in example script.
Just an initial pass that fixes some initial problems detected while using my local branch of Pydantic (with support for generics).
It depends on samuelcolvin/pydantic#1159 merged and released.
There's also a small change to thinc.model.Model, I haven't committed it yet, and it might be unnecessary, depending on the end result of the PR in Pydantic.",True,{'HOORAY': ['https://github.com/ines']}
explosion/thinc,https://github.com/explosion/thinc,195,2020-01-09T20:15:39Z,2020-01-09T21:27:02Z,2020-01-10T12:42:05Z,MERGED,True,9,9,1,https://github.com/tiangolo,"WIP: Fix small misconfigs in example, initial pass",2,[],https://github.com/explosion/thinc/pull/195,https://github.com/apps/codecov,2,https://github.com/explosion/thinc/pull/195#issuecomment-572737667," Fix small misconfigurations in example script.
Just an initial pass that fixes some initial problems detected while using my local branch of Pydantic (with support for generics).
It depends on samuelcolvin/pydantic#1159 merged and released.
There's also a small change to thinc.model.Model, I haven't committed it yet, and it might be unnecessary, depending on the end result of the PR in Pydantic.","Codecov Report

 No coverage uploaded for pull request base (develop@df0f085). Click here to learn what that means.
The diff coverage is 45.4%.


@@            Coverage Diff             @@
##             develop     #195   +/-   ##
==========================================
  Coverage           ?   64.06%           
==========================================
  Files              ?       62           
  Lines              ?     3515           
  Branches           ?        0           
==========================================
  Hits               ?     2252           
  Misses             ?     1263           
  Partials           ?        0



Impacted Files
Coverage 





thinc/layers/pytorchwrapper.py
28.84% <18.6%> ()



thinc/shims/tensorflow.py
27.94% <30%> ()



thinc/shims/pytorch.py
28.12% <33.33%> ()



thinc/shims/shim.py
59.45% <66.66%> ()



thinc/types.py
69.14% <82.6%> ()



thinc/layers/concatenate.py
48.71% <87.5%> ()



thinc/layers/tensorflowwrapper.py
29.62% <9.09%> ()



thinc/util.py
99.01% <96.29%> ()





Continue to review full report at Codecov.

Legend - Click here to learn more
 = absolute <relative> (impact),  = not affected, ? = missing data
Powered by Codecov. Last update df0f085...8159bbb. Read the comment docs.",True,{}
explosion/thinc,https://github.com/explosion/thinc,195,2020-01-09T20:15:39Z,2020-01-09T21:27:02Z,2020-01-10T12:42:05Z,MERGED,True,9,9,1,https://github.com/tiangolo,"WIP: Fix small misconfigs in example, initial pass",2,[],https://github.com/explosion/thinc/pull/195,https://github.com/ines,3,https://github.com/explosion/thinc/pull/195#issuecomment-572764111," Fix small misconfigurations in example script.
Just an initial pass that fixes some initial problems detected while using my local branch of Pydantic (with support for generics).
It depends on samuelcolvin/pydantic#1159 merged and released.
There's also a small change to thinc.model.Model, I haven't committed it yet, and it might be unnecessary, depending on the end result of the PR in Pydantic.","I'll already go ahead and merge this so we don't have any actual typos and mistakes in the config example here. Once the update is released with pydantic, we can update Model if needed and remove the comments about the generics not working (both in the example and in config.py).",True,{'HOORAY': ['https://github.com/tiangolo']}
explosion/thinc,https://github.com/explosion/thinc,196,2020-01-09T20:49:41Z,2020-01-09T21:00:07Z,2020-01-09T21:00:10Z,MERGED,True,61,61,5,https://github.com/honnibal,More type improvements,3,[],https://github.com/explosion/thinc/pull/196,https://github.com/honnibal,1,https://github.com/explosion/thinc/pull/196,,,True,{'HOORAY': ['https://github.com/justindujardin']}
explosion/thinc,https://github.com/explosion/thinc,196,2020-01-09T20:49:41Z,2020-01-09T21:00:07Z,2020-01-09T21:00:10Z,MERGED,True,61,61,5,https://github.com/honnibal,More type improvements,3,[],https://github.com/explosion/thinc/pull/196,https://github.com/apps/codecov,2,https://github.com/explosion/thinc/pull/196#issuecomment-572749546,,"Codecov Report

 No coverage uploaded for pull request base (develop@7aab777). Click here to learn what that means.
The diff coverage is 82.5%.


@@            Coverage Diff            @@
##             develop    #196   +/-   ##
=========================================
  Coverage           ?   64.1%           
=========================================
  Files              ?      62           
  Lines              ?    3519           
  Branches           ?       0           
=========================================
  Hits               ?    2256           
  Misses             ?    1263           
  Partials           ?       0



Impacted Files
Coverage 





thinc/layers/with_list2padded.py
79.16% <100%> ()



thinc/layers/lstm.py
85.33% <100%> ()



thinc/types.py
69.39% <100%> ()



thinc/layers/ragged2list.py
64.28% <100%> ()



thinc/backends/ops.py
48.7% <70.83%> ()





Continue to review full report at Codecov.

Legend - Click here to learn more
 = absolute <relative> (impact),  = not affected, ? = missing data
Powered by Codecov. Last update 7aab777...bb020d0. Read the comment docs.",True,{}
explosion/thinc,https://github.com/explosion/thinc,197,2020-01-10T00:24:11Z,2020-01-10T10:03:35Z,2020-01-22T00:55:02Z,MERGED,True,20,7,2,https://github.com/honnibal,Add Decorator type to fix registry functions,6,[],https://github.com/explosion/thinc/pull/197,https://github.com/honnibal,1,https://github.com/explosion/thinc/pull/197,"Registered functions were becoming type-opaque, because of the registry decorator. Even after adding type annotations in catalogue, there were still problems getting it to work properly, because we assign the decorator to a class variable, which makes things that much more opaque to mypy.
I tried a few approaches, but it's complicated having an interaction between the type variable and the class. I don't really know what's going on but it complained of some sort of ambiguity.
My solution is to use a ""protocol"" to define a new type, Decorator. We can then declare the variable to be of that type when we assign it. This works! I even got a type error in my transformers script.
There's still some tricks to be solved with the chain operator. Not sure what's up but I don't always get type errors from incorrect combinations.","Registered functions were becoming type-opaque, because of the registry decorator. Even after adding type annotations in catalogue, there were still problems getting it to work properly, because we assign the decorator to a class variable, which makes things that much more opaque to mypy.
I tried a few approaches, but it's complicated having an interaction between the type variable and the class. I don't really know what's going on but it complained of some sort of ambiguity.
My solution is to use a ""protocol"" to define a new type, Decorator. We can then declare the variable to be of that type when we assign it. This works! I even got a type error in my transformers script.
There's still some tricks to be solved with the chain operator. Not sure what's up but I don't always get type errors from incorrect combinations.",True,{}
explosion/thinc,https://github.com/explosion/thinc,197,2020-01-10T00:24:11Z,2020-01-10T10:03:35Z,2020-01-22T00:55:02Z,MERGED,True,20,7,2,https://github.com/honnibal,Add Decorator type to fix registry functions,6,[],https://github.com/explosion/thinc/pull/197,https://github.com/apps/codecov,2,https://github.com/explosion/thinc/pull/197#issuecomment-572818973,"Registered functions were becoming type-opaque, because of the registry decorator. Even after adding type annotations in catalogue, there were still problems getting it to work properly, because we assign the decorator to a class variable, which makes things that much more opaque to mypy.
I tried a few approaches, but it's complicated having an interaction between the type variable and the class. I don't really know what's going on but it complained of some sort of ambiguity.
My solution is to use a ""protocol"" to define a new type, Decorator. We can then declare the variable to be of that type when we assign it. This works! I even got a type error in my transformers script.
There's still some tricks to be solved with the chain operator. Not sure what's up but I don't always get type errors from incorrect combinations.","Codecov Report

Merging #197 into develop will increase coverage by 0.02%.
The diff coverage is 91.66%.


@@             Coverage Diff             @@
##           develop     #197      +/-   ##
===========================================
+ Coverage    64.09%   64.11%   +0.02%     
===========================================
  Files           62       62              
  Lines         3523     3528       +5     
===========================================
+ Hits          2258     2262       +4     
- Misses        1265     1266       +1



Impacted Files
Coverage 





thinc/config.py
100% <100%> ()



thinc/types.py
69.45% <85.71%> (+0.06%)





Continue to review full report at Codecov.

Legend - Click here to learn more
 = absolute <relative> (impact),  = not affected, ? = missing data
Powered by Codecov. Last update cca5325...d5a0698. Read the comment docs.",True,{}
explosion/thinc,https://github.com/explosion/thinc,198,2020-01-10T12:59:50Z,2020-01-10T16:20:22Z,2020-01-10T16:20:27Z,MERGED,True,125,45,11,https://github.com/honnibal,Improve type-checking for chain,12,[],https://github.com/explosion/thinc/pull/198,https://github.com/honnibal,1,https://github.com/explosion/thinc/pull/198,"Getting the chain operator to validate well is pretty tough. It seems mypy doesn't really support variable args: I can't get it to branch the type-logic on len(args). Here's a hack that works in the meantime -- hopefully a new version of mypy will add support for what we need.
Instead of conditioning on the length of the variable, we can define a bunch of optionals, and condition on those. I've added support for up to 10, with the remaining backing off.
This gives a (pretty cryptic) type error on a broken model. The return type still isn't right though.
I think the right response here is ""thanks I hate it"". But at least we'll be able to make this work? Hopefully we can engage with mypy and get a better solution.","Getting the chain operator to validate well is pretty tough. It seems mypy doesn't really support variable args: I can't get it to branch the type-logic on len(args). Here's a hack that works in the meantime -- hopefully a new version of mypy will add support for what we need.
Instead of conditioning on the length of the variable, we can define a bunch of optionals, and condition on those. I've added support for up to 10, with the remaining backing off.
This gives a (pretty cryptic) type error on a broken model. The return type still isn't right though.
I think the right response here is ""thanks I hate it"". But at least we'll be able to make this work? Hopefully we can engage with mypy and get a better solution.",True,{}
explosion/thinc,https://github.com/explosion/thinc,198,2020-01-10T12:59:50Z,2020-01-10T16:20:22Z,2020-01-10T16:20:27Z,MERGED,True,125,45,11,https://github.com/honnibal,Improve type-checking for chain,12,[],https://github.com/explosion/thinc/pull/198,https://github.com/apps/codecov,2,https://github.com/explosion/thinc/pull/198#issuecomment-573071466,"Getting the chain operator to validate well is pretty tough. It seems mypy doesn't really support variable args: I can't get it to branch the type-logic on len(args). Here's a hack that works in the meantime -- hopefully a new version of mypy will add support for what we need.
Instead of conditioning on the length of the variable, we can define a bunch of optionals, and condition on those. I've added support for up to 10, with the remaining backing off.
This gives a (pretty cryptic) type error on a broken model. The return type still isn't right though.
I think the right response here is ""thanks I hate it"". But at least we'll be able to make this work? Hopefully we can engage with mypy and get a better solution.","Codecov Report

Merging #198 into develop will increase coverage by 0.09%.
The diff coverage is 82.14%.


@@            Coverage Diff             @@
##           develop    #198      +/-   ##
==========================================
+ Coverage    64.11%   64.2%   +0.09%     
==========================================
  Files           62      62              
  Lines         3528    3551      +23     
==========================================
+ Hits          2262    2280      +18     
- Misses        1266    1271       +5



Impacted Files
Coverage 





thinc/layers/softmax.py
83.33% <> ()



thinc/layers/__init__.py
100% <> ()



thinc/layers/clone.py
94.11% <100%> (+0.78%)



thinc/layers/pytorchwrapper.py
27.45% <100%> ()



thinc/layers/relu.py
96.87% <100%> ()



thinc/layers/mish.py
39.39% <33.33%> ()



thinc/layers/maxout.py
61.11% <50%> ()



thinc/layers/dropout.py
53.06% <70%> ()



thinc/layers/chain.py
89.53% <87.87%> (-2.78%)



thinc/layers/noop.py
63.63% <0%> (-9.1%)





Continue to review full report at Codecov.

Legend - Click here to learn more
 = absolute <relative> (impact),  = not affected, ? = missing data
Powered by Codecov. Last update ea16112...9edb345. Read the comment docs.",True,{}
explosion/thinc,https://github.com/explosion/thinc,199,2020-01-10T14:17:59Z,2020-01-10T15:15:59Z,2020-03-10T19:30:25Z,MERGED,True,11,2,3,https://github.com/svlandeg,Fixing chain ini and maxout backprop,5,"['bug', 'feat / layers']",https://github.com/explosion/thinc/pull/199,https://github.com/svlandeg,1,https://github.com/explosion/thinc/pull/199,"The initialization of chain would throw an error if the underlying layer didn't not properly handle predict(None), so preventing that here for robustness.
maxout had a bug in its backprop function: the value to increment b needs to be reshaped to (nO, nP) (right?)
Raising an informative error when these sort of wrong shapes occur in inc_grad","The initialization of chain would throw an error if the underlying layer didn't not properly handle predict(None), so preventing that here for robustness.
maxout had a bug in its backprop function: the value to increment b needs to be reshaped to (nO, nP) (right?)
Raising an informative error when these sort of wrong shapes occur in inc_grad",True,{}
explosion/thinc,https://github.com/explosion/thinc,199,2020-01-10T14:17:59Z,2020-01-10T15:15:59Z,2020-03-10T19:30:25Z,MERGED,True,11,2,3,https://github.com/svlandeg,Fixing chain ini and maxout backprop,5,"['bug', 'feat / layers']",https://github.com/explosion/thinc/pull/199,https://github.com/apps/codecov,2,https://github.com/explosion/thinc/pull/199#issuecomment-573053866,"The initialization of chain would throw an error if the underlying layer didn't not properly handle predict(None), so preventing that here for robustness.
maxout had a bug in its backprop function: the value to increment b needs to be reshaped to (nO, nP) (right?)
Raising an informative error when these sort of wrong shapes occur in inc_grad","Codecov Report

Merging #199 into develop will increase coverage by <.01%.
The diff coverage is 60%.


@@             Coverage Diff             @@
##           develop     #199      +/-   ##
===========================================
+ Coverage    64.11%   64.11%   +<.01%     
===========================================
  Files           62       62              
  Lines         3528     3531       +3     
===========================================
+ Hits          2262     2264       +2     
- Misses        1266     1267       +1



Impacted Files
Coverage 





thinc/layers/maxout.py
61.11% <0%> ()



thinc/layers/chain.py
92.42% <100%> (+0.11%)



thinc/model.py
88.5% <50%> (-0.2%)





Continue to review full report at Codecov.

Legend - Click here to learn more
 = absolute <relative> (impact),  = not affected, ? = missing data
Powered by Codecov. Last update ea16112...6486ebc. Read the comment docs.",True,{}
explosion/thinc,https://github.com/explosion/thinc,200,2020-01-10T17:23:26Z,2020-01-11T17:36:57Z,2020-01-11T17:39:34Z,MERGED,True,784,185,24,https://github.com/honnibal,Improve transforms and Padded type,40,"['enhancement', 'feat / layers', 'feat / types']",https://github.com/explosion/thinc/pull/200,https://github.com/honnibal,1,https://github.com/explosion/thinc/pull/200,"More work towards transformers examples (which need the Padded type to be working well).
The Padded type previously lost information about how to restore the batch to its original order. We now track two additional lists with it. In total this makes the data structure:

data: A 3d array, sorted by decreasing sequence length. The dimensions are timestep, batch item, row data.
size_at_t: An array indicating how the batch can be truncated at different sequence lengths. You can do data[:, :size_at_t[t]] to get an unpadded batch. So let's say you have a batch of 4 documents, of lengths [6, 5, 2, 1]. The size_at_t will be [4, 3, 3, 3, 2, 1].
lengths: A list of ints, indicating the sequence lengths. Applies to the reordered sequences, not the original ordering. So it'll be decreasing length.
indices: Indicates how to put the items back into original order.

I've also killed a redundant padding op, and given the remaining one a more sensible name and behaviour.
I've also added a couple of transforms. I think we have too many of these and we should considate on a more polymorphic set:

with_array
with_padded
with_ragged
with_list

They would each take a type-var of Union[List[Array], Padded, Ragged], producing the same type on output. I plan to add that to this PR. Done! Made sure to test for 0-length sequences as well -- caught a few bugs. I used a pretty tricky set of pytest fixtures to test all four modules to 100% with one file.","More work towards transformers examples (which need the Padded type to be working well).
The Padded type previously lost information about how to restore the batch to its original order. We now track two additional lists with it. In total this makes the data structure:

data: A 3d array, sorted by decreasing sequence length. The dimensions are timestep, batch item, row data.
size_at_t: An array indicating how the batch can be truncated at different sequence lengths. You can do data[:, :size_at_t[t]] to get an unpadded batch. So let's say you have a batch of 4 documents, of lengths [6, 5, 2, 1]. The size_at_t will be [4, 3, 3, 3, 2, 1].
lengths: A list of ints, indicating the sequence lengths. Applies to the reordered sequences, not the original ordering. So it'll be decreasing length.
indices: Indicates how to put the items back into original order.

I've also killed a redundant padding op, and given the remaining one a more sensible name and behaviour.
I've also added a couple of transforms. I think we have too many of these and we should considate on a more polymorphic set:

with_array
with_padded
with_ragged
with_list

They would each take a type-var of Union[List[Array], Padded, Ragged], producing the same type on output. I plan to add that to this PR. Done! Made sure to test for 0-length sequences as well -- caught a few bugs. I used a pretty tricky set of pytest fixtures to test all four modules to 100% with one file.",True,{}
explosion/thinc,https://github.com/explosion/thinc,200,2020-01-10T17:23:26Z,2020-01-11T17:36:57Z,2020-01-11T17:39:34Z,MERGED,True,784,185,24,https://github.com/honnibal,Improve transforms and Padded type,40,"['enhancement', 'feat / layers', 'feat / types']",https://github.com/explosion/thinc/pull/200,https://github.com/apps/codecov,2,https://github.com/explosion/thinc/pull/200#issuecomment-573131476,"More work towards transformers examples (which need the Padded type to be working well).
The Padded type previously lost information about how to restore the batch to its original order. We now track two additional lists with it. In total this makes the data structure:

data: A 3d array, sorted by decreasing sequence length. The dimensions are timestep, batch item, row data.
size_at_t: An array indicating how the batch can be truncated at different sequence lengths. You can do data[:, :size_at_t[t]] to get an unpadded batch. So let's say you have a batch of 4 documents, of lengths [6, 5, 2, 1]. The size_at_t will be [4, 3, 3, 3, 2, 1].
lengths: A list of ints, indicating the sequence lengths. Applies to the reordered sequences, not the original ordering. So it'll be decreasing length.
indices: Indicates how to put the items back into original order.

I've also killed a redundant padding op, and given the remaining one a more sensible name and behaviour.
I've also added a couple of transforms. I think we have too many of these and we should considate on a more polymorphic set:

with_array
with_padded
with_ragged
with_list

They would each take a type-var of Union[List[Array], Padded, Ragged], producing the same type on output. I plan to add that to this PR. Done! Made sure to test for 0-length sequences as well -- caught a few bugs. I used a pretty tricky set of pytest fixtures to test all four modules to 100% with one file.","Codecov Report

 No coverage uploaded for pull request base (develop@cc00184). Click here to learn what that means.
The diff coverage is 94.92%.


@@            Coverage Diff             @@
##             develop     #200   +/-   ##
==========================================
  Coverage           ?   69.71%           
==========================================
  Files              ?       64           
  Lines              ?     3821           
  Branches           ?        0           
==========================================
  Hits               ?     2664           
  Misses             ?     1157           
  Partials           ?        0



Impacted Files
Coverage 





thinc/layers/list2ragged.py
64.28% <100%> ()



thinc/util.py
99.35% <100%> ()



thinc/layers/with_padded.py
100% <100%> ()



thinc/api.py
100% <100%> ()



thinc/layers/featureextractor.py
52.63% <100%> ()



thinc/layers/with_ragged.py
100% <100%> ()



thinc/layers/strings2arrays.py
100% <100%> ()



thinc/layers/staticvectors.py
36.58% <100%> ()



thinc/layers/with_list.py
100% <100%> ()



thinc/layers/with_array.py
100% <100%> ()



... and 23 more






Continue to review full report at Codecov.

Legend - Click here to learn more
 = absolute <relative> (impact),  = not affected, ? = missing data
Powered by Codecov. Last update cc00184...1f3af87. Read the comment docs.",True,{}
explosion/thinc,https://github.com/explosion/thinc,201,2020-01-10T21:04:42Z,2020-01-15T10:37:59Z,2020-01-15T10:40:40Z,MERGED,True,322,67,26,https://github.com/tiangolo,Implement first version of mypy plugin for chain and add,24,"['enhancement', 'feat / types']",https://github.com/explosion/thinc/pull/201,https://github.com/tiangolo,1,https://github.com/explosion/thinc/pull/201," Implement first version of mypy plugin for chain.
To use create a file mypy.ini with:
[mypy]
plugins = thinc.mypy
(that's it, the plugin is part of thinc)

It validates compatibility between layers' output -> input:

It works for arbitrary sizes of chains (here 12 layers) with the original chain function, no need for the extra function chain and chains with Optionals to deal with types.
And it infers the return type of a chain from the input of the first layer and output of the last, also, for arbitrarily long chains:"," Implement first version of mypy plugin for chain.
To use create a file mypy.ini with:
[mypy]
plugins = thinc.mypy
(that's it, the plugin is part of thinc)

It validates compatibility between layers' output -> input:

It works for arbitrary sizes of chains (here 12 layers) with the original chain function, no need for the extra function chain and chains with Optionals to deal with types.
And it infers the return type of a chain from the input of the first layer and output of the last, also, for arbitrarily long chains:",True,"{'HEART': ['https://github.com/ines', 'https://github.com/justindujardin'], 'HOORAY': ['https://github.com/svlandeg']}"
explosion/thinc,https://github.com/explosion/thinc,201,2020-01-10T21:04:42Z,2020-01-15T10:37:59Z,2020-01-15T10:40:40Z,MERGED,True,322,67,26,https://github.com/tiangolo,Implement first version of mypy plugin for chain and add,24,"['enhancement', 'feat / types']",https://github.com/explosion/thinc/pull/201,https://github.com/apps/codecov,2,https://github.com/explosion/thinc/pull/201#issuecomment-573646959," Implement first version of mypy plugin for chain.
To use create a file mypy.ini with:
[mypy]
plugins = thinc.mypy
(that's it, the plugin is part of thinc)

It validates compatibility between layers' output -> input:

It works for arbitrary sizes of chains (here 12 layers) with the original chain function, no need for the extra function chain and chains with Optionals to deal with types.
And it infers the return type of a chain from the input of the first layer and output of the last, also, for arbitrarily long chains:","Codecov Report

Merging #201 into develop will increase coverage by 14.11%.
The diff coverage is 100%.


@@             Coverage Diff              @@
##           develop     #201       +/-   ##
============================================
+ Coverage    80.17%   94.29%   +14.11%     
============================================
  Files           66       62        -4     
  Lines         3839     3188      -651     
============================================
- Hits          3078     3006       -72     
+ Misses         761      182      -579



Impacted Files
Coverage 





thinc/layers/pytorchwrapper.py
100% <> (+14.1%)



thinc/layers/mish.py
100% <100%> ()



thinc/api.py
100% <100%> ()



thinc/layers/logistic.py
100% <100%> ()



thinc/mypy.py
100% <100%> (+100%)



thinc/layers/__init__.py
100% <100%> ()



thinc/layers/add_module.py
100% <100%> ()



thinc/layers/chain_module.py
100% <100%> ()



thinc/layers/clone.py
100% <100%> ()



thinc/layers/maxout.py
100% <100%> ()



... and 28 more






Continue to review full report at Codecov.

Legend - Click here to learn more
 = absolute <relative> (impact),  = not affected, ? = missing data
Powered by Codecov. Last update 3928e67...f20c75b. Read the comment docs.",True,{}
explosion/thinc,https://github.com/explosion/thinc,202,2020-01-11T14:33:54Z,2020-01-11T15:12:57Z,2020-01-11T15:12:59Z,MERGED,True,175,237,27,https://github.com/ines,{Ints|Floats}Xd -> ArrayXd,1,"['enhancement', 'feat / types']",https://github.com/explosion/thinc/pull/202,https://github.com/ines,1,https://github.com/explosion/thinc/pull/202,"As discussed with @honnibal, this PR simplifies the Ints/Floats array distinction and changes them all to Array.","As discussed with @honnibal, this PR simplifies the Ints/Floats array distinction and changes them all to Array.",True,"{'THUMBS_UP': ['https://github.com/honnibal', 'https://github.com/svlandeg']}"
explosion/thinc,https://github.com/explosion/thinc,202,2020-01-11T14:33:54Z,2020-01-11T15:12:57Z,2020-01-11T15:12:59Z,MERGED,True,175,237,27,https://github.com/ines,{Ints|Floats}Xd -> ArrayXd,1,"['enhancement', 'feat / types']",https://github.com/explosion/thinc/pull/202,https://github.com/apps/codecov,2,https://github.com/explosion/thinc/pull/202#issuecomment-573322131,"As discussed with @honnibal, this PR simplifies the Ints/Floats array distinction and changes them all to Array.","Codecov Report

Merging #202 into develop will decrease coverage by 0.39%.
The diff coverage is 92.15%.


@@            Coverage Diff             @@
##           develop     #202     +/-   ##
==========================================
- Coverage     64.2%   63.81%   -0.4%     
==========================================
  Files           62       62             
  Lines         3554     3521     -33     
==========================================
- Hits          2282     2247     -35     
- Misses        1272     1274      +2



Impacted Files
Coverage 





thinc/layers/list2ragged.py
64.28% <100%> ()



thinc/util.py
99.01% <100%> ()



thinc/layers/mish.py
39.39% <100%> ()



thinc/layers/featureextractor.py
52.63% <100%> ()



thinc/layers/strings2arrays.py
100% <100%> ()



thinc/layers/meanpool.py
60% <100%> ()



thinc/layers/with_list2array.py
58.62% <100%> ()



thinc/layers/staticvectors.py
36.58% <100%> ()



thinc/layers/maxpool.py
60% <100%> ()



thinc/layers/concatenate.py
48.71% <100%> ()



... and 15 more






Continue to review full report at Codecov.

Legend - Click here to learn more
 = absolute <relative> (impact),  = not affected, ? = missing data
Powered by Codecov. Last update beea1d4...e1bc69d. Read the comment docs.",True,{}
explosion/thinc,https://github.com/explosion/thinc,203,2020-01-12T13:08:31Z,2020-01-12T14:36:50Z,2020-01-12T14:36:51Z,MERGED,True,160,10,7,https://github.com/ines,Add partial registry functions for losses and initializers,3,"['enhancement', 'feat / config']",https://github.com/explosion/thinc/pull/203,https://github.com/ines,1,https://github.com/explosion/thinc/pull/203,"Specifying the default settings for those functions in the config makes sense and is consistent  but we can't actually call the functions, since they take data. In those cases, the loaded config returns a partial function.","Specifying the default settings for those functions in the config makes sense and is consistent  but we can't actually call the functions, since they take data. In those cases, the loaded config returns a partial function.",True,{}
explosion/thinc,https://github.com/explosion/thinc,203,2020-01-12T13:08:31Z,2020-01-12T14:36:50Z,2020-01-12T14:36:51Z,MERGED,True,160,10,7,https://github.com/ines,Add partial registry functions for losses and initializers,3,"['enhancement', 'feat / config']",https://github.com/explosion/thinc/pull/203,https://github.com/apps/codecov,2,https://github.com/explosion/thinc/pull/203#issuecomment-573413752,"Specifying the default settings for those functions in the config makes sense and is consistent  but we can't actually call the functions, since they take data. In those cases, the loaded config returns a partial function.","Codecov Report

Merging #203 into develop will increase coverage by 0.26%.
The diff coverage is 100%.


@@             Coverage Diff             @@
##           develop     #203      +/-   ##
===========================================
+ Coverage    69.54%   69.81%   +0.26%     
===========================================
  Files           64       64              
  Lines         3688     3720      +32     
===========================================
+ Hits          2565     2597      +32     
  Misses        1123     1123



Impacted Files
Coverage 





thinc/initializers.py
100% <100%> ()



thinc/util.py
100% <100%> ()



thinc/loss.py
100% <100%> ()



thinc/config.py
100% <100%> ()





Continue to review full report at Codecov.

Legend - Click here to learn more
 = absolute <relative> (impact),  = not affected, ? = missing data
Powered by Codecov. Last update 8940080...b1449a9. Read the comment docs.",True,{}
explosion/thinc,https://github.com/explosion/thinc,204,2020-01-12T18:00:25Z,2020-01-12T20:24:27Z,2020-01-12T20:24:30Z,MERGED,True,195,3663,21,https://github.com/ines,Various bug fixes in layers and add tests,17,"['bug', 'tests', 'feat / layers']",https://github.com/explosion/thinc/pull/204,https://github.com/ines,1,https://github.com/explosion/thinc/pull/204,,,True,{}
explosion/thinc,https://github.com/explosion/thinc,204,2020-01-12T18:00:25Z,2020-01-12T20:24:27Z,2020-01-12T20:24:30Z,MERGED,True,195,3663,21,https://github.com/ines,Various bug fixes in layers and add tests,17,"['bug', 'tests', 'feat / layers']",https://github.com/explosion/thinc/pull/204,https://github.com/apps/codecov,2,https://github.com/explosion/thinc/pull/204#issuecomment-573444406,,"Codecov Report

Merging #204 into develop will increase coverage by 0.15%.
The diff coverage is 100%.


@@             Coverage Diff             @@
##           develop     #204      +/-   ##
===========================================
+ Coverage    76.71%   76.86%   +0.15%     
===========================================
  Files           64       64              
  Lines         3740     3743       +3     
===========================================
+ Hits          2869     2877       +8     
+ Misses         871      866       -5



Impacted Files
Coverage 





thinc/layers/layernorm.py
100% <100%> ()



thinc/layers/chain.py
90.21% <100%> (+0.56%)



thinc/layers/relu.py
100% <0%> (+3.12%)



thinc/layers/maxout.py
100% <0%> (+3.7%)



thinc/layers/mish.py
100% <0%> (+6.06%)





Continue to review full report at Codecov.

Legend - Click here to learn more
 = absolute <relative> (impact),  = not affected, ? = missing data
Powered by Codecov. Last update 184913d...81e8e31. Read the comment docs.",True,{}
explosion/thinc,https://github.com/explosion/thinc,205,2020-01-12T18:50:29Z,2020-01-12T19:08:28Z,2020-03-10T19:30:04Z,MERGED,True,27,5,2,https://github.com/honnibal,Improve inference of output shape,2,"['enhancement', 'feat / layers']",https://github.com/explosion/thinc/pull/205,https://github.com/honnibal,1,https://github.com/explosion/thinc/pull/205,"In chain.initialize, we were assuming that the last last would be the one which needs the output dimension set by the variable Y. In fact arbitrary output layers might be set at the end.
The implementation is still pretty hacky here, and there will still be some problems. But at least I have an idea for how this should work:
We should tell people that if their layer doesn't register an nO dimension, chain is allowed to assume the layer input will be valid output. We use this to ignore layers like dropout and LayerNorm when inferring shapes.
We would still have some problems with the layers like list2ragged, padded2list etc. Shape inference past these layers could be possible in theory, but I don't currently see how to do it. I guess we would need to split things up a bit, and allow layers to optionally specify a function that tells how to calculate the output shape given an input batch, and the input shape given an output batch. It seems pretty bureaucratic though.","In chain.initialize, we were assuming that the last last would be the one which needs the output dimension set by the variable Y. In fact arbitrary output layers might be set at the end.
The implementation is still pretty hacky here, and there will still be some problems. But at least I have an idea for how this should work:
We should tell people that if their layer doesn't register an nO dimension, chain is allowed to assume the layer input will be valid output. We use this to ignore layers like dropout and LayerNorm when inferring shapes.
We would still have some problems with the layers like list2ragged, padded2list etc. Shape inference past these layers could be possible in theory, but I don't currently see how to do it. I guess we would need to split things up a bit, and allow layers to optionally specify a function that tells how to calculate the output shape given an input batch, and the input shape given an output batch. It seems pretty bureaucratic though.",True,{}
explosion/thinc,https://github.com/explosion/thinc,205,2020-01-12T18:50:29Z,2020-01-12T19:08:28Z,2020-03-10T19:30:04Z,MERGED,True,27,5,2,https://github.com/honnibal,Improve inference of output shape,2,"['enhancement', 'feat / layers']",https://github.com/explosion/thinc/pull/205,https://github.com/apps/codecov,2,https://github.com/explosion/thinc/pull/205#issuecomment-573446462,"In chain.initialize, we were assuming that the last last would be the one which needs the output dimension set by the variable Y. In fact arbitrary output layers might be set at the end.
The implementation is still pretty hacky here, and there will still be some problems. But at least I have an idea for how this should work:
We should tell people that if their layer doesn't register an nO dimension, chain is allowed to assume the layer input will be valid output. We use this to ignore layers like dropout and LayerNorm when inferring shapes.
We would still have some problems with the layers like list2ragged, padded2list etc. Shape inference past these layers could be possible in theory, but I don't currently see how to do it. I guess we would need to split things up a bit, and allow layers to optionally specify a function that tells how to calculate the output shape given an input batch, and the input shape given an output batch. It seems pretty bureaucratic though.","Codecov Report

Merging #205 into develop will increase coverage by 0.04%.
The diff coverage is 100%.


@@            Coverage Diff             @@
##           develop    #205      +/-   ##
==========================================
+ Coverage    69.86%   69.9%   +0.04%     
==========================================
  Files           64      64              
  Lines         3727    3732       +5     
==========================================
+ Hits          2604    2609       +5     
  Misses        1123    1123



Impacted Files
Coverage 





thinc/layers/chain.py
90.21% <100%> (+0.56%)





Continue to review full report at Codecov.

Legend - Click here to learn more
 = absolute <relative> (impact),  = not affected, ? = missing data
Powered by Codecov. Last update 8e3f851...da7f0fb. Read the comment docs.",True,{}
explosion/thinc,https://github.com/explosion/thinc,206,2020-01-12T19:15:29Z,2020-01-12T23:56:51Z,2020-01-22T00:54:20Z,MERGED,True,426,110,19,https://github.com/honnibal,Add transformers example,50,['examples'],https://github.com/explosion/thinc/pull/206,https://github.com/honnibal,1,https://github.com/explosion/thinc/pull/206,"This runs, although I haven't tuned the accuracy etc yet. It'll probably be better to use an English POS tagging dataset, so we can use Distilbert.
There's some hacks to work around the Pydantic problem, but otherwise I think the structure is okay.","This runs, although I haven't tuned the accuracy etc yet. It'll probably be better to use an English POS tagging dataset, so we can use Distilbert.
There's some hacks to work around the Pydantic problem, but otherwise I think the structure is okay.",True,{}
explosion/thinc,https://github.com/explosion/thinc,206,2020-01-12T19:15:29Z,2020-01-12T23:56:51Z,2020-01-22T00:54:20Z,MERGED,True,426,110,19,https://github.com/honnibal,Add transformers example,50,['examples'],https://github.com/explosion/thinc/pull/206,https://github.com/apps/codecov,2,https://github.com/explosion/thinc/pull/206#issuecomment-573453554,"This runs, although I haven't tuned the accuracy etc yet. It'll probably be better to use an English POS tagging dataset, so we can use Distilbert.
There's some hacks to work around the Pydantic problem, but otherwise I think the structure is okay.","Codecov Report

Merging #206 into develop will increase coverage by 6.83%.
The diff coverage is 100%.


@@             Coverage Diff             @@
##           develop     #206      +/-   ##
===========================================
+ Coverage    69.04%   75.87%   +6.83%     
===========================================
  Files           66       66              
  Lines         3799     3810      +11     
===========================================
+ Hits          2623     2891     +268     
+ Misses        1176      919     -257



Impacted Files
Coverage 





thinc/layers/residual.py
25.71% <100%> ()



thinc/layers/mish.py
100% <100%> (+60.6%)



thinc/api.py
100% <100%> ()



thinc/config.py
100% <100%> ()



thinc/layers/__init__.py
100% <100%> ()



thinc/layers/hashembed.py
100% <100%> (+22.5%)



thinc/layers/recurrent.py
100% <100%> (+1.78%)



thinc/layers/lstm.py
89.33% <100%> (+4%)



thinc/layers/bidirectional.py
100% <100%> (+60%)



thinc/layers/parametricattention.py
100% <100%> (+68.29%)



... and 20 more






Continue to review full report at Codecov.

Legend - Click here to learn more
 = absolute <relative> (impact),  = not affected, ? = missing data
Powered by Codecov. Last update 2923507...558b3f4. Read the comment docs.",True,{}
explosion/thinc,https://github.com/explosion/thinc,207,2020-01-12T20:48:12Z,2020-01-13T02:01:07Z,2020-01-13T02:01:12Z,MERGED,True,399,264,21,https://github.com/ines,More layer tests and various bug fixes,52,"['bug', 'tests', 'feat / layers']",https://github.com/explosion/thinc/pull/207,https://github.com/ines,1,https://github.com/explosion/thinc/pull/207,,,True,{}
explosion/thinc,https://github.com/explosion/thinc,207,2020-01-12T20:48:12Z,2020-01-13T02:01:07Z,2020-01-13T02:01:12Z,MERGED,True,399,264,21,https://github.com/ines,More layer tests and various bug fixes,52,"['bug', 'tests', 'feat / layers']",https://github.com/explosion/thinc/pull/207,https://github.com/apps/codecov,2,https://github.com/explosion/thinc/pull/207#issuecomment-573455831,,"Codecov Report

Merging #207 into develop will increase coverage by 5.23%.
The diff coverage is 95.29%.


@@             Coverage Diff             @@
##           develop     #207      +/-   ##
===========================================
+ Coverage    75.92%   81.16%   +5.23%     
===========================================
  Files           66       65       -1     
  Lines         3818     3797      -21     
===========================================
+ Hits          2899     3082     +183     
+ Misses         919      715     -204



Impacted Files
Coverage 





thinc/layers/layernorm.py
100% <> ()



thinc/layers/with_reshape.py
23.68% <0%> (-1.32%)



thinc/layers/lstm.py
100% <100%> (+10.66%)



thinc/layers/with_getitem.py
100% <100%> (+55%)



thinc/util.py
100% <100%> ()



thinc/layers/add.py
100% <100%> (+72.22%)



thinc/api.py
100% <100%> ()



thinc/layers/pytorchwrapper.py
85.89% <100%> (+61.89%)



thinc/layers/__init__.py
100% <100%> ()



thinc/model.py
94.68% <100%> (+2.18%)



... and 14 more






Continue to review full report at Codecov.

Legend - Click here to learn more
 = absolute <relative> (impact),  = not affected, ? = missing data
Powered by Codecov. Last update 2bce43b...21ae4c9. Read the comment docs.",True,{}
explosion/thinc,https://github.com/explosion/thinc,209,2020-01-13T12:01:24Z,2020-01-13T15:41:10Z,2020-01-22T00:55:00Z,MERGED,True,49,16,10,https://github.com/honnibal,Don't try to flatten nested chain,10,[],https://github.com/explosion/thinc/pull/209,https://github.com/honnibal,1,https://github.com/explosion/thinc/pull/209,Closes #208,Closes #208,True,{}
explosion/thinc,https://github.com/explosion/thinc,209,2020-01-13T12:01:24Z,2020-01-13T15:41:10Z,2020-01-22T00:55:00Z,MERGED,True,49,16,10,https://github.com/honnibal,Don't try to flatten nested chain,10,[],https://github.com/explosion/thinc/pull/209,https://github.com/apps/codecov,2,https://github.com/explosion/thinc/pull/209#issuecomment-573718187,Closes #208,"Codecov Report

Merging #209 into develop will decrease coverage by 0.02%.
The diff coverage is 75%.


@@            Coverage Diff             @@
##           develop    #209      +/-   ##
==========================================
- Coverage    81.83%   81.8%   -0.03%     
==========================================
  Files           65      65              
  Lines         3799    3793       -6     
==========================================
- Hits          3109    3103       -6     
  Misses         690     690



Impacted Files
Coverage 





thinc/layers/pytorchwrapper.py
85.89% <> ()



thinc/layers/chain.py
100% <> ()



thinc/model.py
94.68% <100%> ()



thinc/loss.py
75% <100%> (-0.93%)



thinc/backends/_cupy_allocators.py
58.62% <50%> (-1.38%)





Continue to review full report at Codecov.

Legend - Click here to learn more
 = absolute <relative> (impact),  = not affected, ? = missing data
Powered by Codecov. Last update 8861973...acb330d. Read the comment docs.",True,{}
explosion/thinc,https://github.com/explosion/thinc,210,2020-01-13T12:48:29Z,2020-01-13T12:56:49Z,2020-01-13T12:57:48Z,MERGED,True,144,77,4,https://github.com/ines,Add tests for x2y transforms,3,"['tests', 'feat / layers']",https://github.com/explosion/thinc/pull/210,https://github.com/ines,1,https://github.com/explosion/thinc/pull/210,,,True,{}
explosion/thinc,https://github.com/explosion/thinc,210,2020-01-13T12:48:29Z,2020-01-13T12:56:49Z,2020-01-13T12:57:48Z,MERGED,True,144,77,4,https://github.com/ines,Add tests for x2y transforms,3,"['tests', 'feat / layers']",https://github.com/explosion/thinc/pull/210,https://github.com/apps/codecov,2,https://github.com/explosion/thinc/pull/210#issuecomment-573647893,,"Codecov Report

Merging #210 into develop will increase coverage by 0.65%.
The diff coverage is 100%.


@@             Coverage Diff             @@
##           develop     #210      +/-   ##
===========================================
+ Coverage    81.17%   81.83%   +0.65%     
===========================================
  Files           65       65              
  Lines         3799     3799              
===========================================
+ Hits          3084     3109      +25     
+ Misses         715      690      -25



Impacted Files
Coverage 





thinc/api.py
100% <100%> ()



thinc/layers/list2ragged.py
100% <0%> (+35.71%)



thinc/layers/ragged2list.py
100% <0%> (+35.71%)



thinc/layers/list2array.py
100% <0%> (+35.71%)



thinc/layers/padded2list.py
100% <0%> (+41.66%)



thinc/layers/list2padded.py
100% <0%> (+41.66%)





Continue to review full report at Codecov.

Legend - Click here to learn more
 = absolute <relative> (impact),  = not affected, ? = missing data
Powered by Codecov. Last update 6ead7f9...fbe35b7. Read the comment docs.",True,{}
explosion/thinc,https://github.com/explosion/thinc,211,2020-01-13T13:14:23Z,2020-01-13T13:27:49Z,2020-01-13T13:30:39Z,MERGED,True,2,2,1,https://github.com/svlandeg,fix model serialization,1,"['bug', 'serialization']",https://github.com/explosion/thinc/pull/211,https://github.com/svlandeg,1,https://github.com/explosion/thinc/pull/211,"Fix model (de)serialization:

int has a to_bytes(length, ...) method so we need to prevent calling that one
data[""refs""] was not parsed correctly in from_bytes","Fix model (de)serialization:

int has a to_bytes(length, ...) method so we need to prevent calling that one
data[""refs""] was not parsed correctly in from_bytes",True,{}
explosion/thinc,https://github.com/explosion/thinc,211,2020-01-13T13:14:23Z,2020-01-13T13:27:49Z,2020-01-13T13:30:39Z,MERGED,True,2,2,1,https://github.com/svlandeg,fix model serialization,1,"['bug', 'serialization']",https://github.com/explosion/thinc/pull/211,https://github.com/apps/codecov,2,https://github.com/explosion/thinc/pull/211#issuecomment-573658277,"Fix model (de)serialization:

int has a to_bytes(length, ...) method so we need to prevent calling that one
data[""refs""] was not parsed correctly in from_bytes","Codecov Report

Merging #211 into develop will not change coverage.
The diff coverage is 100%.


@@           Coverage Diff            @@
##           develop     #211   +/-   ##
========================================
  Coverage    81.83%   81.83%           
========================================
  Files           65       65           
  Lines         3799     3799           
========================================
  Hits          3109     3109           
  Misses         690      690



Impacted Files
Coverage 





thinc/model.py
94.68% <100%> ()





Continue to review full report at Codecov.

Legend - Click here to learn more
 = absolute <relative> (impact),  = not affected, ? = missing data
Powered by Codecov. Last update 2b3483c...50f1deb. Read the comment docs.",True,{}
explosion/thinc,https://github.com/explosion/thinc,211,2020-01-13T13:14:23Z,2020-01-13T13:27:49Z,2020-01-13T13:30:39Z,MERGED,True,2,2,1,https://github.com/svlandeg,fix model serialization,1,"['bug', 'serialization']",https://github.com/explosion/thinc/pull/211,https://github.com/honnibal,3,https://github.com/explosion/thinc/pull/211#issuecomment-573662842,"Fix model (de)serialization:

int has a to_bytes(length, ...) method so we need to prevent calling that one
data[""refs""] was not parsed correctly in from_bytes",Argh. Bloody duck-typing...@ines is thinking about a new API for this,True,{}
explosion/thinc,https://github.com/explosion/thinc,212,2020-01-13T13:14:37Z,2020-01-13T16:13:51Z,2020-01-13T18:18:44Z,MERGED,True,110,29,3,https://github.com/ines,Serializable attrs,5,"['enhancement', 'serialization']",https://github.com/explosion/thinc/pull/212,https://github.com/ines,1,https://github.com/explosion/thinc/pull/212,"Fixes serialization problems around the attrs.
All attrs are now (de)serialized with msgpack by default. For custom types, users can register their own functions (implemented via Python's single dispatch generic functions).
from thinc.api import serialize_attr, deserialize_attr

@serialize_attr.register(MyCustomAttr)
def serialize_my_custom_attr(_, value, name, model):
    return value.to_bytes()

@deserialize_attr.register(MyCustomAttr)
def deserialize_my_custom_attr(_, value, name, model):
    return MyCustomAttr().from_bytes(value)
The first argument of the function is always the instance of the attr (passed down from Model.to_bytes/Model.from_bytes). This is what the singledispatch uses to decide which function to call. In the serialization function, this is technically redundant and the same as value  but I still kept it in there for consistency. The functions also receive the attribute name and the Model instance, just in case.
Using the value of the attr to decide which serialization function to call means that custom attrs that require special (de)serialization need a default value of its type. For instance, setting attrs={""custom"": None} won't work, since the None doesn't tell us that it should be MyCustomType. We can't easily solve this with typed dicts, and the alternative of registering loaders for attribute names is messy and not really viable.
Requiring an instance of the expected value as the default value seems like an okay constraint.","Fixes serialization problems around the attrs.
All attrs are now (de)serialized with msgpack by default. For custom types, users can register their own functions (implemented via Python's single dispatch generic functions).
from thinc.api import serialize_attr, deserialize_attr

@serialize_attr.register(MyCustomAttr)
def serialize_my_custom_attr(_, value, name, model):
    return value.to_bytes()

@deserialize_attr.register(MyCustomAttr)
def deserialize_my_custom_attr(_, value, name, model):
    return MyCustomAttr().from_bytes(value)
The first argument of the function is always the instance of the attr (passed down from Model.to_bytes/Model.from_bytes). This is what the singledispatch uses to decide which function to call. In the serialization function, this is technically redundant and the same as value  but I still kept it in there for consistency. The functions also receive the attribute name and the Model instance, just in case.
Using the value of the attr to decide which serialization function to call means that custom attrs that require special (de)serialization need a default value of its type. For instance, setting attrs={""custom"": None} won't work, since the None doesn't tell us that it should be MyCustomType. We can't easily solve this with typed dicts, and the alternative of registering loaders for attribute names is messy and not really viable.
Requiring an instance of the expected value as the default value seems like an okay constraint.",True,{}
explosion/thinc,https://github.com/explosion/thinc,212,2020-01-13T13:14:37Z,2020-01-13T16:13:51Z,2020-01-13T18:18:44Z,MERGED,True,110,29,3,https://github.com/ines,Serializable attrs,5,"['enhancement', 'serialization']",https://github.com/explosion/thinc/pull/212,https://github.com/apps/codecov,2,https://github.com/explosion/thinc/pull/212#issuecomment-573657766,"Fixes serialization problems around the attrs.
All attrs are now (de)serialized with msgpack by default. For custom types, users can register their own functions (implemented via Python's single dispatch generic functions).
from thinc.api import serialize_attr, deserialize_attr

@serialize_attr.register(MyCustomAttr)
def serialize_my_custom_attr(_, value, name, model):
    return value.to_bytes()

@deserialize_attr.register(MyCustomAttr)
def deserialize_my_custom_attr(_, value, name, model):
    return MyCustomAttr().from_bytes(value)
The first argument of the function is always the instance of the attr (passed down from Model.to_bytes/Model.from_bytes). This is what the singledispatch uses to decide which function to call. In the serialization function, this is technically redundant and the same as value  but I still kept it in there for consistency. The functions also receive the attribute name and the Model instance, just in case.
Using the value of the attr to decide which serialization function to call means that custom attrs that require special (de)serialization need a default value of its type. For instance, setting attrs={""custom"": None} won't work, since the None doesn't tell us that it should be MyCustomType. We can't easily solve this with typed dicts, and the alternative of registering loaders for attribute names is messy and not really viable.
Requiring an instance of the expected value as the default value seems like an okay constraint.","Codecov Report

Merging #212 into develop will increase coverage by 0.68%.
The diff coverage is 80%.


@@             Coverage Diff             @@
##           develop     #212      +/-   ##
===========================================
+ Coverage    81.28%   81.97%   +0.68%     
===========================================
  Files           65       65              
  Lines         3804     3794      -10     
===========================================
+ Hits          3092     3110      +18     
+ Misses         712      684      -28



Impacted Files
Coverage 





thinc/layers/chain.py
100% <> ()



thinc/layers/pytorchwrapper.py
85.89% <> ()



thinc/api.py
100% <100%> ()



thinc/loss.py
75% <100%> (-0.93%)



thinc/model.py
96.2% <100%> (+0.71%)



thinc/backends/_cupy_allocators.py
58.62% <50%> (-1.38%)



thinc/layers/list2ragged.py
100% <0%> (+35.71%)



thinc/layers/ragged2list.py
100% <0%> (+35.71%)



thinc/layers/list2array.py
100% <0%> (+35.71%)



... and 2 more






Continue to review full report at Codecov.

Legend - Click here to learn more
 = absolute <relative> (impact),  = not affected, ? = missing data
Powered by Codecov. Last update d796230...4067ddc. Read the comment docs.",True,{}
explosion/thinc,https://github.com/explosion/thinc,212,2020-01-13T13:14:37Z,2020-01-13T16:13:51Z,2020-01-13T18:18:44Z,MERGED,True,110,29,3,https://github.com/ines,Serializable attrs,5,"['enhancement', 'serialization']",https://github.com/explosion/thinc/pull/212,https://github.com/honnibal,3,https://github.com/explosion/thinc/pull/212#issuecomment-573742075,"Fixes serialization problems around the attrs.
All attrs are now (de)serialized with msgpack by default. For custom types, users can register their own functions (implemented via Python's single dispatch generic functions).
from thinc.api import serialize_attr, deserialize_attr

@serialize_attr.register(MyCustomAttr)
def serialize_my_custom_attr(_, value, name, model):
    return value.to_bytes()

@deserialize_attr.register(MyCustomAttr)
def deserialize_my_custom_attr(_, value, name, model):
    return MyCustomAttr().from_bytes(value)
The first argument of the function is always the instance of the attr (passed down from Model.to_bytes/Model.from_bytes). This is what the singledispatch uses to decide which function to call. In the serialization function, this is technically redundant and the same as value  but I still kept it in there for consistency. The functions also receive the attribute name and the Model instance, just in case.
Using the value of the attr to decide which serialization function to call means that custom attrs that require special (de)serialization need a default value of its type. For instance, setting attrs={""custom"": None} won't work, since the None doesn't tell us that it should be MyCustomType. We can't easily solve this with typed dicts, and the alternative of registering loaders for attribute names is messy and not really viable.
Requiring an instance of the expected value as the default value seems like an okay constraint.","Nice!
Btw I don't know whether this works, but is it possible to use a Protocol to have a dispatch that's like HasToBytes, and then it just calls that?",True,{}
explosion/thinc,https://github.com/explosion/thinc,213,2020-01-13T15:51:02Z,2020-01-13T16:06:05Z,2020-01-13T16:13:25Z,MERGED,True,25,1,3,https://github.com/svlandeg,Add Logistic layer,2,[],https://github.com/explosion/thinc/pull/213,https://github.com/svlandeg,1,https://github.com/explosion/thinc/pull/213,,,True,{'THUMBS_UP': ['https://github.com/honnibal']}
explosion/thinc,https://github.com/explosion/thinc,213,2020-01-13T15:51:02Z,2020-01-13T16:06:05Z,2020-01-13T16:13:25Z,MERGED,True,25,1,3,https://github.com/svlandeg,Add Logistic layer,2,[],https://github.com/explosion/thinc/pull/213,https://github.com/apps/codecov,2,https://github.com/explosion/thinc/pull/213#issuecomment-573733328,,"Codecov Report

Merging #213 into develop will decrease coverage by 0.02%.
The diff coverage is 68.75%.


@@            Coverage Diff             @@
##           develop    #213      +/-   ##
==========================================
- Coverage    81.83%   81.8%   -0.03%     
==========================================
  Files           65      66       +1     
  Lines         3799    3804       +5     
==========================================
+ Hits          3109    3112       +3     
- Misses         690     692       +2



Impacted Files
Coverage 





thinc/api.py
100% <100%> ()



thinc/layers/__init__.py
100% <100%> ()



thinc/layers/logistic.py
64.28% <64.28%> ()



thinc/backends/_cupy_allocators.py
58.62% <0%> (-1.38%)



thinc/loss.py
75% <0%> (-0.93%)



thinc/layers/chain.py
100% <0%> ()



thinc/model.py
95.39% <0%> (+0.71%)





Continue to review full report at Codecov.

Legend - Click here to learn more
 = absolute <relative> (impact),  = not affected, ? = missing data
Powered by Codecov. Last update 8861973...d3d023a. Read the comment docs.",True,{}
explosion/thinc,https://github.com/explosion/thinc,214,2020-01-13T23:35:12Z,2020-01-13T23:41:57Z,2020-01-13T23:41:59Z,MERGED,True,65,5,2,https://github.com/honnibal,Test and fix uniqued layer,5,"['bug', 'tests', 'feat / layers']",https://github.com/explosion/thinc/pull/214,https://github.com/honnibal,1,https://github.com/explosion/thinc/pull/214,"There was a small long-standing bug in the backprop, that was masked by the fact that previously we returned None from most embedding layers' backprop, so we never actually called the un-unique.
I've also used a newer style from Hypothesis in this test. The old Hypothesis API was pretty tough, but this new one is very promising. I hope we can use it more in future for correctness tests, where we need a lot of examples.","There was a small long-standing bug in the backprop, that was masked by the fact that previously we returned None from most embedding layers' backprop, so we never actually called the un-unique.
I've also used a newer style from Hypothesis in this test. The old Hypothesis API was pretty tough, but this new one is very promising. I hope we can use it more in future for correctness tests, where we need a lot of examples.",True,{'HOORAY': ['https://github.com/justindujardin']}
explosion/thinc,https://github.com/explosion/thinc,215,2020-01-13T23:49:42Z,2020-01-14T00:23:40Z,2020-01-14T00:28:04Z,MERGED,True,23,4,2,https://github.com/justindujardin,TensorflowWrapper changes to support mathy model,2,"['enhancement', 'interop / tensorflow']",https://github.com/explosion/thinc/pull/215,https://github.com/justindujardin,1,https://github.com/explosion/thinc/pull/215,"This adds misc fixes/features to the thinc tensorflow shim/wrapper to support Mathy's keras models.
Changes:

 Allow specifying a custom thinc.model.Model subclass for the wrapper","This adds misc fixes/features to the thinc tensorflow shim/wrapper to support Mathy's keras models.
Changes:

 Allow specifying a custom thinc.model.Model subclass for the wrapper",True,{}
explosion/thinc,https://github.com/explosion/thinc,215,2020-01-13T23:49:42Z,2020-01-14T00:23:40Z,2020-01-14T00:28:04Z,MERGED,True,23,4,2,https://github.com/justindujardin,TensorflowWrapper changes to support mathy model,2,"['enhancement', 'interop / tensorflow']",https://github.com/explosion/thinc/pull/215,https://github.com/honnibal,2,https://github.com/explosion/thinc/pull/215#issuecomment-573938641,"This adds misc fixes/features to the thinc tensorflow shim/wrapper to support Mathy's keras models.
Changes:

 Allow specifying a custom thinc.model.Model subclass for the wrapper",LGTM. Is this ready?,True,{}
explosion/thinc,https://github.com/explosion/thinc,215,2020-01-13T23:49:42Z,2020-01-14T00:23:40Z,2020-01-14T00:28:04Z,MERGED,True,23,4,2,https://github.com/justindujardin,TensorflowWrapper changes to support mathy model,2,"['enhancement', 'interop / tensorflow']",https://github.com/explosion/thinc/pull/215,https://github.com/justindujardin,3,https://github.com/explosion/thinc/pull/215#issuecomment-573938775,"This adds misc fixes/features to the thinc tensorflow shim/wrapper to support Mathy's keras models.
Changes:

 Allow specifying a custom thinc.model.Model subclass for the wrapper","Sure, I can open another if there are more changes",True,{'THUMBS_UP': ['https://github.com/ines']}
explosion/thinc,https://github.com/explosion/thinc,215,2020-01-13T23:49:42Z,2020-01-14T00:23:40Z,2020-01-14T00:28:04Z,MERGED,True,23,4,2,https://github.com/justindujardin,TensorflowWrapper changes to support mathy model,2,"['enhancement', 'interop / tensorflow']",https://github.com/explosion/thinc/pull/215,https://github.com/apps/codecov,4,https://github.com/explosion/thinc/pull/215#issuecomment-573940112,"This adds misc fixes/features to the thinc tensorflow shim/wrapper to support Mathy's keras models.
Changes:

 Allow specifying a custom thinc.model.Model subclass for the wrapper","Codecov Report

Merging #215 into develop will increase coverage by 2.92%.
The diff coverage is 100%.


@@             Coverage Diff             @@
##           develop     #215      +/-   ##
===========================================
+ Coverage    90.95%   93.88%   +2.92%     
===========================================
  Files           60       60              
  Lines         3108     3107       -1     
===========================================
+ Hits          2827     2917      +90     
+ Misses         281      190      -91



Impacted Files
Coverage 





thinc/layers/tensorflowwrapper.py
100% <100%> ()



thinc/backends/ops.py
71.83% <0%> (+19.15%)



thinc/layers/uniqued.py
100% <0%> (+67.64%)





Continue to review full report at Codecov.

Legend - Click here to learn more
 = absolute <relative> (impact),  = not affected, ? = missing data
Powered by Codecov. Last update 6797bff...ecae646. Read the comment docs.",True,{}
explosion/thinc,https://github.com/explosion/thinc,216,2020-01-14T00:13:25Z,2020-01-14T00:29:25Z,2020-01-22T00:54:16Z,MERGED,True,26,30,3,https://github.com/ines,Fix TensorFlow shim and tests,1,"['tests', 'feat / shims', 'interop / tensorflow']",https://github.com/explosion/thinc/pull/216,https://github.com/ines,1,https://github.com/explosion/thinc/pull/216,"remove unused to_disk/from_disk from shim (we just delegate to the bytes methods here)
 make sure to_cpu/to_gpu also calls shim methods if available
 fix problem where state dict would have the wrong weight names when copying the model (because it was created before and passed in). Not sure if my fix is great but the tests pass.
 tidy up and improve test coverage","remove unused to_disk/from_disk from shim (we just delegate to the bytes methods here)
 make sure to_cpu/to_gpu also calls shim methods if available
 fix problem where state dict would have the wrong weight names when copying the model (because it was created before and passed in). Not sure if my fix is great but the tests pass.
 tidy up and improve test coverage",True,{}
explosion/thinc,https://github.com/explosion/thinc,216,2020-01-14T00:13:25Z,2020-01-14T00:29:25Z,2020-01-22T00:54:16Z,MERGED,True,26,30,3,https://github.com/ines,Fix TensorFlow shim and tests,1,"['tests', 'feat / shims', 'interop / tensorflow']",https://github.com/explosion/thinc/pull/216,https://github.com/apps/codecov,2,https://github.com/explosion/thinc/pull/216#issuecomment-573942735,"remove unused to_disk/from_disk from shim (we just delegate to the bytes methods here)
 make sure to_cpu/to_gpu also calls shim methods if available
 fix problem where state dict would have the wrong weight names when copying the model (because it was created before and passed in). Not sure if my fix is great but the tests pass.
 tidy up and improve test coverage","Codecov Report

Merging #216 into develop will increase coverage by 0.46%.
The diff coverage is 100%.


@@             Coverage Diff             @@
##           develop     #216      +/-   ##
===========================================
+ Coverage    93.88%   94.34%   +0.46%     
===========================================
  Files           60       60              
  Lines         3107     3096      -11     
===========================================
+ Hits          2917     2921       +4     
+ Misses         190      175      -15



Impacted Files
Coverage 





thinc/model.py
99.23% <> ()



thinc/shims/tensorflow.py
96.82% <100%> (+10.69%)





Continue to review full report at Codecov.

Legend - Click here to learn more
 = absolute <relative> (impact),  = not affected, ? = missing data
Powered by Codecov. Last update 6d8a2b8...d55904b. Read the comment docs.",True,{}
explosion/thinc,https://github.com/explosion/thinc,217,2020-01-14T13:00:28Z,2020-01-14T13:29:20Z,2020-01-14T13:30:12Z,MERGED,True,20,0,1,https://github.com/svlandeg,add cosine calculations,1,"['enhancement', 'feat / ops']",https://github.com/explosion/thinc/pull/217,https://github.com/svlandeg,1,https://github.com/explosion/thinc/pull/217,Add a few cosine helper functions. Not sure about the name for cosine_abs_loss as it's kind of confusing with the cosine_distance function in loss.py,Add a few cosine helper functions. Not sure about the name for cosine_abs_loss as it's kind of confusing with the cosine_distance function in loss.py,True,{'THUMBS_UP': ['https://github.com/honnibal']}
explosion/thinc,https://github.com/explosion/thinc,217,2020-01-14T13:00:28Z,2020-01-14T13:29:20Z,2020-01-14T13:30:12Z,MERGED,True,20,0,1,https://github.com/svlandeg,add cosine calculations,1,"['enhancement', 'feat / ops']",https://github.com/explosion/thinc/pull/217,https://github.com/apps/codecov,2,https://github.com/explosion/thinc/pull/217#issuecomment-574163645,Add a few cosine helper functions. Not sure about the name for cosine_abs_loss as it's kind of confusing with the cosine_distance function in loss.py,"Codecov Report

 No coverage uploaded for pull request base (develop@86076a7). Click here to learn what that means.
The diff coverage is 18.75%.


@@            Coverage Diff             @@
##             develop     #217   +/-   ##
==========================================
  Coverage           ?   94.05%           
==========================================
  Files              ?       60           
  Lines              ?     3114           
  Branches           ?        0           
==========================================
  Hits               ?     2929           
  Misses             ?      185           
  Partials           ?        0



Impacted Files
Coverage 





thinc/backends/ops.py
69.54% <18.75%> ()





Continue to review full report at Codecov.

Legend - Click here to learn more
 = absolute <relative> (impact),  = not affected, ? = missing data
Powered by Codecov. Last update 86076a7...18d7230. Read the comment docs.",True,{}
explosion/thinc,https://github.com/explosion/thinc,218,2020-01-14T13:32:45Z,2020-01-14T16:17:31Z,2020-01-22T00:54:13Z,MERGED,True,279,243,12,https://github.com/honnibal,"Replace Memory class, improve serialization format",20,[],https://github.com/explosion/thinc/pull/218,https://github.com/honnibal,1,https://github.com/explosion/thinc/pull/218,"The Memory class was pretty dusty: it was pretty much unchaged since previous Thinc, and a lot of the details didn't really make sense. It was also not a JIT-friendly design, hindering future Jax adoption.
I've introduced a much simpler ParamServer object, and also improved the serialization and copy-to-device methods (since these were affected by the previous Memory class details). I've also phrased more things in terms of the Model public API, avoiding the internals pretty much everywhere.
There aren't really many changes to outward behaviour here, but there are a couple.

Model.__init__ drops the grads argument. It's not necessary, if something wants to set a grad they can do it after the model is constructed.
The Shim class's to_cpu and to_gpu methods are replaced by a single to_device method.
The Optimizer now returns a tuple (weights, gradients), instead of None. The optimizer is allowed to modify the weights in-place, but the Model class should not rely on that -- it should call set_params and set_grad.","The Memory class was pretty dusty: it was pretty much unchaged since previous Thinc, and a lot of the details didn't really make sense. It was also not a JIT-friendly design, hindering future Jax adoption.
I've introduced a much simpler ParamServer object, and also improved the serialization and copy-to-device methods (since these were affected by the previous Memory class details). I've also phrased more things in terms of the Model public API, avoiding the internals pretty much everywhere.
There aren't really many changes to outward behaviour here, but there are a couple.

Model.__init__ drops the grads argument. It's not necessary, if something wants to set a grad they can do it after the model is constructed.
The Shim class's to_cpu and to_gpu methods are replaced by a single to_device method.
The Optimizer now returns a tuple (weights, gradients), instead of None. The optimizer is allowed to modify the weights in-place, but the Model class should not rely on that -- it should call set_params and set_grad.",True,{}
explosion/thinc,https://github.com/explosion/thinc,218,2020-01-14T13:32:45Z,2020-01-14T16:17:31Z,2020-01-22T00:54:13Z,MERGED,True,279,243,12,https://github.com/honnibal,"Replace Memory class, improve serialization format",20,[],https://github.com/explosion/thinc/pull/218,https://github.com/apps/codecov,2,https://github.com/explosion/thinc/pull/218#issuecomment-574184163,"The Memory class was pretty dusty: it was pretty much unchaged since previous Thinc, and a lot of the details didn't really make sense. It was also not a JIT-friendly design, hindering future Jax adoption.
I've introduced a much simpler ParamServer object, and also improved the serialization and copy-to-device methods (since these were affected by the previous Memory class details). I've also phrased more things in terms of the Model public API, avoiding the internals pretty much everywhere.
There aren't really many changes to outward behaviour here, but there are a couple.

Model.__init__ drops the grads argument. It's not necessary, if something wants to set a grad they can do it after the model is constructed.
The Shim class's to_cpu and to_gpu methods are replaced by a single to_device method.
The Optimizer now returns a tuple (weights, gradients), instead of None. The optimizer is allowed to modify the weights in-place, but the Model class should not rely on that -- it should call set_params and set_grad.","Codecov Report

Merging #218 into develop will increase coverage by 8.76%.
The diff coverage is 64.86%.


@@             Coverage Diff             @@
##           develop     #218      +/-   ##
===========================================
+ Coverage    85.43%   94.19%   +8.76%     
===========================================
  Files           61       61              
  Lines         3123     3133      +10     
===========================================
+ Hits          2668     2951     +283     
+ Misses         455      182     -273



Impacted Files
Coverage 





thinc/backends/_param_server.py
100% <> (+6.45%)



thinc/model.py
100% <> (+3.67%)



thinc/shims/shim.py
100% <100%> ()



thinc/shims/tensorflow.py
96.96% <100%> (+70.49%)



thinc/layers/tensorflowwrapper.py
100% <100%> (+77.41%)



thinc/backends/ops.py
69.54% <18.75%> (-2.29%)



thinc/schedules.py
100% <0%> ()



thinc/util.py
100% <0%> (+0.95%)



thinc/backends/mem.py
100% <0%> (+5.26%)



... and 6 more






Continue to review full report at Codecov.

Legend - Click here to learn more
 = absolute <relative> (impact),  = not affected, ? = missing data
Powered by Codecov. Last update f743a7e...b92ec6c. Read the comment docs.",True,{}
explosion/thinc,https://github.com/explosion/thinc,220,2020-01-14T16:08:40Z,2020-01-15T12:39:36Z,2020-01-15T12:44:48Z,MERGED,True,18,3,2,https://github.com/svlandeg,Allow to set dropout rate,4,[],https://github.com/explosion/thinc/pull/220,https://github.com/svlandeg,1,https://github.com/explosion/thinc/pull/220,"Fix cosine from PR #217
Add function to change dropout rates across all layers in a model","Fix cosine from PR #217
Add function to change dropout rates across all layers in a model",True,{}
explosion/thinc,https://github.com/explosion/thinc,220,2020-01-14T16:08:40Z,2020-01-15T12:39:36Z,2020-01-15T12:44:48Z,MERGED,True,18,3,2,https://github.com/svlandeg,Allow to set dropout rate,4,[],https://github.com/explosion/thinc/pull/220,https://github.com/honnibal,2,https://github.com/explosion/thinc/pull/220#issuecomment-574252396,"Fix cosine from PR #217
Add function to change dropout rates across all layers in a model","Ah, this should be in thinc.util, we don't want a method for it.
There's not really any reason it needs to be a method, because we don't expect subclasses to want to change the behaviour. (Actually if we wanted that we'd have to call the method on every node, which is pretty awkward.)
We also want to keep a clear line between what sort of stuff gets into the Model class, and what doesn't. The Model should stay really generic and minimal.",True,{}
explosion/thinc,https://github.com/explosion/thinc,220,2020-01-14T16:08:40Z,2020-01-15T12:39:36Z,2020-01-15T12:44:48Z,MERGED,True,18,3,2,https://github.com/svlandeg,Allow to set dropout rate,4,[],https://github.com/explosion/thinc/pull/220,https://github.com/svlandeg,3,https://github.com/explosion/thinc/pull/220#issuecomment-574253644,"Fix cosine from PR #217
Add function to change dropout rates across all layers in a model","Ok, will fix !",True,{}
explosion/thinc,https://github.com/explosion/thinc,220,2020-01-14T16:08:40Z,2020-01-15T12:39:36Z,2020-01-15T12:44:48Z,MERGED,True,18,3,2,https://github.com/svlandeg,Allow to set dropout rate,4,[],https://github.com/explosion/thinc/pull/220,https://github.com/svlandeg,4,https://github.com/explosion/thinc/pull/220#issuecomment-574384919,"Fix cosine from PR #217
Add function to change dropout rates across all layers in a model","So I was trying to figure out why my code wouldn't run anymore, and I finally realised it's because there's a circular dependency if I move the function to util and set the parameter as type Model, because then I have to from .model import Model ... :|",True,{}
explosion/thinc,https://github.com/explosion/thinc,220,2020-01-14T16:08:40Z,2020-01-15T12:39:36Z,2020-01-15T12:44:48Z,MERGED,True,18,3,2,https://github.com/svlandeg,Allow to set dropout rate,4,[],https://github.com/explosion/thinc/pull/220,https://github.com/apps/codecov,5,https://github.com/explosion/thinc/pull/220#issuecomment-574620579,"Fix cosine from PR #217
Add function to change dropout rates across all layers in a model","Codecov Report

 No coverage uploaded for pull request base (develop@1f99d6e). Click here to learn what that means.
The diff coverage is 96.1%.


@@            Coverage Diff             @@
##             develop     #220   +/-   ##
==========================================
  Coverage           ?   91.93%           
==========================================
  Files              ?       63           
  Lines              ?     3385           
  Branches           ?        0           
==========================================
  Hits               ?     3112           
  Misses             ?      273           
  Partials           ?        0



Impacted Files
Coverage 





thinc/shims/pytorch.py
83.9% <> ()



thinc/shims/tensorflow.py
96.96% <100%> ()



thinc/backends/__init__.py
100% <100%> ()



thinc/shims/shim.py
100% <100%> ()



thinc/backends/_param_server.py
100% <100%> ()



thinc/model.py
100% <100%> ()



thinc/util.py
94.69% <14.28%> ()





Continue to review full report at Codecov.

Legend - Click here to learn more
 = absolute <relative> (impact),  = not affected, ? = missing data
Powered by Codecov. Last update 1f99d6e...bf6514d. Read the comment docs.",True,{}
explosion/thinc,https://github.com/explosion/thinc,220,2020-01-14T16:08:40Z,2020-01-15T12:39:36Z,2020-01-15T12:44:48Z,MERGED,True,18,3,2,https://github.com/svlandeg,Allow to set dropout rate,4,[],https://github.com/explosion/thinc/pull/220,https://github.com/honnibal,6,https://github.com/explosion/thinc/pull/220#issuecomment-574642054,"Fix cosine from PR #217
Add function to change dropout rates across all layers in a model","Hm, yeah the circular imports can be a real pain. I think we need to break out the utils that Model needs into a separate module or something to ease things.
Small style point: I normally use the variable name node within model.walk(). And I know you've been testing this in your spaCy code, but a test would be good too :).",True,{'THUMBS_UP': ['https://github.com/svlandeg']}
explosion/thinc,https://github.com/explosion/thinc,221,2020-01-14T17:15:36Z,2020-01-15T01:48:26Z,2020-01-22T00:54:58Z,MERGED,True,136,133,4,https://github.com/honnibal,"Move optimizers module to Python, improve API",8,[],https://github.com/explosion/thinc/pull/221,https://github.com/honnibal,1,https://github.com/explosion/thinc/pull/221,"Move optimizers module to Python, instead of Cython
Remove unused options
Type declare
Make schedules passed in via the named argument, rather than as separate dict","Move optimizers module to Python, instead of Cython
Remove unused options
Type declare
Make schedules passed in via the named argument, rather than as separate dict",True,{}
explosion/thinc,https://github.com/explosion/thinc,221,2020-01-14T17:15:36Z,2020-01-15T01:48:26Z,2020-01-22T00:54:58Z,MERGED,True,136,133,4,https://github.com/honnibal,"Move optimizers module to Python, improve API",8,[],https://github.com/explosion/thinc/pull/221,https://github.com/apps/codecov,2,https://github.com/explosion/thinc/pull/221#issuecomment-574281754,"Move optimizers module to Python, instead of Cython
Remove unused options
Type declare
Make schedules passed in via the named argument, rather than as separate dict","Codecov Report

Merging #221 into develop will decrease coverage by 2.11%.
The diff coverage is 66.66%.


@@             Coverage Diff             @@
##           develop     #221      +/-   ##
===========================================
- Coverage    94.19%   92.07%   -2.12%     
===========================================
  Files           61       62       +1     
  Lines         3133     3319     +186     
===========================================
+ Hits          2951     3056     +105     
- Misses         182      263      +81



Impacted Files
Coverage 





thinc/backends/ops.py
69.54% <100%> ()



thinc/optimizers.py
56.45% <66.19%> ()





Continue to review full report at Codecov.

Legend - Click here to learn more
 = absolute <relative> (impact),  = not affected, ? = missing data
Powered by Codecov. Last update f2754e0...0b2e3ff. Read the comment docs.",True,{}
explosion/thinc,https://github.com/explosion/thinc,222,2020-01-15T09:34:49Z,2020-01-15T11:47:04Z,2020-01-15T12:45:45Z,MERGED,True,3,1,1,https://github.com/svlandeg,reshape weights and gradient before calling ops.adam,1,[],https://github.com/explosion/thinc/pull/222,https://github.com/svlandeg,1,https://github.com/explosion/thinc/pull/222,"PR #218 introduced a bug: when calling ops.adam() with a parameter of dimension >1, it would crash with the error

self.ops.adam(weights, gradient, mom1, mom2, b1, b2, eps, lr * lr_scale)
File ""thinc\backends\numpy_ops.pyx"", line 338, in thinc.backends.numpy_ops.NumpyOps.adam
ValueError: Buffer has wrong number of dimensions (expected 1, got 2)

I'm assuming this is because before that PR, Memory  stored the parameters as long arrays that could be passed as such to  ops.adam(), but now they require reshaping first.","PR #218 introduced a bug: when calling ops.adam() with a parameter of dimension >1, it would crash with the error

self.ops.adam(weights, gradient, mom1, mom2, b1, b2, eps, lr * lr_scale)
File ""thinc\backends\numpy_ops.pyx"", line 338, in thinc.backends.numpy_ops.NumpyOps.adam
ValueError: Buffer has wrong number of dimensions (expected 1, got 2)

I'm assuming this is because before that PR, Memory  stored the parameters as long arrays that could be passed as such to  ops.adam(), but now they require reshaping first.",True,{}
explosion/thinc,https://github.com/explosion/thinc,222,2020-01-15T09:34:49Z,2020-01-15T11:47:04Z,2020-01-15T12:45:45Z,MERGED,True,3,1,1,https://github.com/svlandeg,reshape weights and gradient before calling ops.adam,1,[],https://github.com/explosion/thinc/pull/222,https://github.com/apps/codecov,2,https://github.com/explosion/thinc/pull/222#issuecomment-574576036,"PR #218 introduced a bug: when calling ops.adam() with a parameter of dimension >1, it would crash with the error

self.ops.adam(weights, gradient, mom1, mom2, b1, b2, eps, lr * lr_scale)
File ""thinc\backends\numpy_ops.pyx"", line 338, in thinc.backends.numpy_ops.NumpyOps.adam
ValueError: Buffer has wrong number of dimensions (expected 1, got 2)

I'm assuming this is because before that PR, Memory  stored the parameters as long arrays that could be passed as such to  ops.adam(), but now they require reshaping first.","Codecov Report

Merging #222 into develop will decrease coverage by 0.05%.
The diff coverage is 0%.


@@             Coverage Diff             @@
##           develop     #222      +/-   ##
===========================================
- Coverage    92.07%   92.02%   -0.06%     
===========================================
  Files           62       62              
  Lines         3319     3321       +2     
===========================================
  Hits          3056     3056              
- Misses         263      265       +2



Impacted Files
Coverage 





thinc/optimizers.py
55.85% <0%> (-0.61%)





Continue to review full report at Codecov.

Legend - Click here to learn more
 = absolute <relative> (impact),  = not affected, ? = missing data
Powered by Codecov. Last update 100ac83...a3b6320. Read the comment docs.",True,{}
explosion/thinc,https://github.com/explosion/thinc,222,2020-01-15T09:34:49Z,2020-01-15T11:47:04Z,2020-01-15T12:45:45Z,MERGED,True,3,1,1,https://github.com/svlandeg,reshape weights and gradient before calling ops.adam,1,[],https://github.com/explosion/thinc/pull/222,https://github.com/svlandeg,3,https://github.com/explosion/thinc/pull/222#issuecomment-574577717,"PR #218 introduced a bug: when calling ops.adam() with a parameter of dimension >1, it would crash with the error

self.ops.adam(weights, gradient, mom1, mom2, b1, b2, eps, lr * lr_scale)
File ""thinc\backends\numpy_ops.pyx"", line 338, in thinc.backends.numpy_ops.NumpyOps.adam
ValueError: Buffer has wrong number of dimensions (expected 1, got 2)

I'm assuming this is because before that PR, Memory  stored the parameters as long arrays that could be passed as such to  ops.adam(), but now they require reshaping first.","I'm a little worried about accuracy though. Before PR #218 I got these results (omitting rows for clarity):
(myexplosion) C:\Users\Sofie\Documents\git\spacy>python -m spacy train-from-config C:\Users\Sofie\Documents\data\ud-treebanks-v2.4\UD_Dutch-Alpino\nl-alpino-json\nl_alpino-ud-train.json C:\Users\Sofie\Documents\data\ud-treebanks-v2.4\UD_Dutch-Alpino\nl-alpino-json\nl_alpino-ud-dev.json examples\experiments\ptb-joint-pos-dep\defaults.cfg
 Loading config from: examples\experiments\ptb-joint-pos-dep\defaults.cfg
 Creating nlp from config
 Loading training corpus
 Initializing the nlp pipeline
 Start training
 Training. Initial learn rate: 0.001
#        LOSS TOK2VEC   LOSS TAGGER   LOSS PARSER   TAGS_ACC   UAS      LAS      SCORE
------   ------------   -----------   -----------   --------   ------   ------   ------
     0           0.00        105.44        252.38       2.61    28.52     4.09     3.80
   200        2089.48      21742.60      40003.46      36.42    30.52    16.74    20.67
   400        7823.12      16427.26      37211.70      64.35    58.49    44.42    48.40
   600       10603.79      13205.14      37552.83      75.67    69.64    58.52    61.95
   800       11145.90      11397.03      39073.29      81.95    73.73    64.27    67.80
  1000       11971.36      10791.25      44850.52      85.46    76.55    68.09    71.56
  2000       11406.40       6535.41      33794.56      90.19    83.03    75.61    78.53
  3000       11016.46       5036.98      27103.06      91.30    84.12    77.82    80.52
  4000       11344.60       4114.21      22655.47      91.74    84.63    78.64    81.26
  5000       11705.79       3555.29      19422.58      91.72    85.40    79.73    82.12
 10000       10070.12       1718.38       7981.65      91.79    85.35    79.89    82.27

With the changes in PR #218 and this one, I get:
(myexplosion) C:\Users\Sofie\Documents\git\spacy>python -m spacy train-from-config C:\Users\Sofie\Documents\data\ud-treebanks-v2.4\UD_Dutch-Alpino\nl-alpino-json\nl_alpino-ud-train.json C:\Users\Sofie\Documents\data\ud-treebanks-v2.4\UD_Dutch-Alpino\nl-alpino-json\nl_alpino-ud-dev.json examples\experiments\ptb-joint-pos-dep\defaults.cfg
 Loading config from: examples\experiments\ptb-joint-pos-dep\defaults.cfg
 Creating nlp from config
 Loading training corpus
 Initializing the nlp pipeline
 Start training
 Training. Initial learn rate: 0.001
#        LOSS TOK2VEC   LOSS TAGGER   LOSS PARSER   TAGS_ACC   UAS      LAS      SCORE
------   ------------   -----------   -----------   --------   ------   ------   ------
     0           0.00        119.37        322.74       2.61    28.52     8.68     7.47
   200        1697.68      22447.11      38518.85      31.76    25.65     9.66    14.08
   400        6139.11      18522.96      39554.10      57.45    52.96    39.33    42.95                                                                                                                                                                       
   600        9557.41      16390.76      41057.05      67.98    61.95    50.30    53.83
   800       10079.88      15455.30      44709.68      75.32    68.57    57.47    61.04
  1000       10519.39      14678.84      51359.07      80.30    71.92    62.03    65.69
  2000       10367.26      17798.07      95033.40      88.62    79.07    71.82    75.18
  3000        9688.32      17396.35      97090.86      90.85    82.52    75.83    78.84
  4000        9541.68      13735.75      74092.21      90.93    83.78    77.48    80.17
  5000        9177.53      11785.70      59225.29      91.25    83.59    77.68    80.39
 10000        8191.42       6837.32      28947.43      91.36    83.33    77.36    80.16",True,{}
explosion/thinc,https://github.com/explosion/thinc,223,2020-01-15T11:32:03Z,2020-01-16T20:22:27Z,2020-01-16T20:22:27Z,MERGED,True,161,0,1,https://github.com/adrianeboyd,Add textcat from config example,3,['examples'],https://github.com/explosion/thinc/pull/223,https://github.com/adrianeboyd,1,https://github.com/explosion/thinc/pull/223,"A textcat from config example that supports two datasets with mutually exclusive classes:

imdb: 2 classes, from ml-datasets
dbpedia_ontology: 14 classes, loader implemented locally for fast.ai dataset on AWS

Requires syntok for tokenization.","A textcat from config example that supports two datasets with mutually exclusive classes:

imdb: 2 classes, from ml-datasets
dbpedia_ontology: 14 classes, loader implemented locally for fast.ai dataset on AWS

Requires syntok for tokenization.",True,{}
explosion/thinc,https://github.com/explosion/thinc,223,2020-01-15T11:32:03Z,2020-01-16T20:22:27Z,2020-01-16T20:22:27Z,MERGED,True,161,0,1,https://github.com/adrianeboyd,Add textcat from config example,3,['examples'],https://github.com/explosion/thinc/pull/223,https://github.com/adrianeboyd,2,https://github.com/explosion/thinc/pull/223#issuecomment-574620594,"A textcat from config example that supports two datasets with mutually exclusive classes:

imdb: 2 classes, from ml-datasets
dbpedia_ontology: 14 classes, loader implemented locally for fast.ai dataset on AWS

Requires syntok for tokenization.",It still needs a better solution for the partial config issue.,True,{}
explosion/thinc,https://github.com/explosion/thinc,223,2020-01-15T11:32:03Z,2020-01-16T20:22:27Z,2020-01-16T20:22:27Z,MERGED,True,161,0,1,https://github.com/adrianeboyd,Add textcat from config example,3,['examples'],https://github.com/explosion/thinc/pull/223,https://github.com/apps/codecov,3,https://github.com/explosion/thinc/pull/223#issuecomment-574621257,"A textcat from config example that supports two datasets with mutually exclusive classes:

imdb: 2 classes, from ml-datasets
dbpedia_ontology: 14 classes, loader implemented locally for fast.ai dataset on AWS

Requires syntok for tokenization.","Codecov Report

Merging #223 into develop will increase coverage by 1.93%.
The diff coverage is 97.28%.


@@             Coverage Diff             @@
##           develop     #223      +/-   ##
===========================================
+ Coverage     92.2%   94.13%   +1.93%     
===========================================
  Files           63       62       -1     
  Lines         3374     3308      -66     
===========================================
+ Hits          3111     3114       +3     
+ Misses         263      194      -69



Impacted Files
Coverage 





thinc/shims/pytorch.py
83.9% <> ()



thinc/util.py
100% <100%> ()



thinc/layers/mish.py
100% <100%> ()



thinc/config.py
100% <100%> ()



thinc/layers/add_module.py
100% <100%> ()



thinc/shims/tensorflow.py
96.96% <100%> ()



thinc/layers/maxout.py
100% <100%> ()



thinc/shims/shim.py
100% <100%> ()



thinc/model.py
100% <100%> ()



thinc/api.py
100% <100%> ()



... and 15 more






Continue to review full report at Codecov.

Legend - Click here to learn more
 = absolute <relative> (impact),  = not affected, ? = missing data
Powered by Codecov. Last update 360d4cd...93c59eb. Read the comment docs.",True,{}
explosion/thinc,https://github.com/explosion/thinc,223,2020-01-15T11:32:03Z,2020-01-16T20:22:27Z,2020-01-16T20:22:27Z,MERGED,True,161,0,1,https://github.com/adrianeboyd,Add textcat from config example,3,['examples'],https://github.com/explosion/thinc/pull/223,https://github.com/honnibal,4,https://github.com/explosion/thinc/pull/223#issuecomment-574640057,"A textcat from config example that supports two datasets with mutually exclusive classes:

imdb: 2 classes, from ml-datasets
dbpedia_ontology: 14 classes, loader implemented locally for fast.ai dataset on AWS

Requires syntok for tokenization.","Thanks!
While chaining the model together in the config like that works, it's usually not the best option so it's probably not what we want to show off in the example. What we probably want to do is register a function within the script, and then use it. For instance:
@registry.layers(""EmbedPoolTextcat.v0"") -> Model[List[Array2d], Array2d]:
def EmbedPoolTextcat(embed: Model[Array2d, Array2d]):
    model = chain(
        list2ragged(),
        with_array(embed),
        MeanPool(),
        Softmax()
    )
    model.set_ref(""embed"", embed)
    return model
You can then reference this model within the config, so you only need to configure two models.
I guess the rule of thumb would be that we only want to be specifying meaningful units in the config, that you would want to swap out. It's sometimes handy to be able to compose in the config, but we'd rather recommend writing larger blocks in Python.",True,{}
explosion/thinc,https://github.com/explosion/thinc,223,2020-01-15T11:32:03Z,2020-01-16T20:22:27Z,2020-01-16T20:22:27Z,MERGED,True,161,0,1,https://github.com/adrianeboyd,Add textcat from config example,3,['examples'],https://github.com/explosion/thinc/pull/223,https://github.com/adrianeboyd,5,https://github.com/explosion/thinc/pull/223#issuecomment-574670025,"A textcat from config example that supports two datasets with mutually exclusive classes:

imdb: 2 classes, from ml-datasets
dbpedia_ontology: 14 classes, loader implemented locally for fast.ai dataset on AWS

Requires syntok for tokenization.","Okay, so here's what I started with, which doesn't seem so bad:
    # Define model 
    vector_width = len(vocab) 
    model = chain(list2ragged(), 
            with_array(Embed(width, vector_width)), 
            MeanPool(), 
            Softmax(nr_class, width) 
        )

width could come from a config, but vector_width and nr_class come from the dataset.
Without a config I could change it to this, which doesn't seem better, really:
    embed = Embed(width, len(vocab))
    model = EmbedPoolTextcat(embed)
    for layer in model.walk():
        if layer.name == ""softmax"":
            layer.set_dim(""nI"", width)
            layer.set_dim(""nO"", nr_class)
            layer.initialize()

I could modify EmbedPoolTextcat to set width from the config, but I still have nr_class to set for softmax, and I haven't figured out a better way to do it...
And I don't think I can use this version of EmbedPoolTextcat in a config at all because I don't see a way to set embed. If it has a default of None the config won't load, and I don't see how you could provide a value for embed in the config at all?",True,{}
explosion/thinc,https://github.com/explosion/thinc,223,2020-01-15T11:32:03Z,2020-01-16T20:22:27Z,2020-01-16T20:22:27Z,MERGED,True,161,0,1,https://github.com/adrianeboyd,Add textcat from config example,3,['examples'],https://github.com/explosion/thinc/pull/223,https://github.com/honnibal,6,https://github.com/explosion/thinc/pull/223#issuecomment-574745281,"A textcat from config example that supports two datasets with mutually exclusive classes:

imdb: 2 classes, from ml-datasets
dbpedia_ontology: 14 classes, loader implemented locally for fast.ai dataset on AWS

Requires syntok for tokenization.","The following should work, via the shape inference:
model = chain(list2ragged(), 
    with_array(Embed(width, vector_width)), 
     MeanPool(), 
     Softmax() 
)
model.initialize(X=batch_of_inputs, Y=batch_of_outputs)

In most cases the chain function will be able to use the Y argument to figure out the output shape if it's missing. It can also use the X argument to figure out missing input shapes in many cases, although that's not relevant here.

And I don't think I can use this version of EmbedPoolTextcat in a config at all because I don't see a way to set embed. If it has a default of None the config won't load, and I don't see how you could provide a value for embed in the config at all?

Just to check, you're referring to the vocabulary size problem, right? Because if we had the vocab size in the config this would work just fine:
[model]
@layers = ""EmbedPoolTextcat.v0""

[model.embed]
@layers = ""Embed.v0""
nO = 128
nV = 30000

That's the main use-case we expect for the block arguments: passing in a submodel into a larger model construction function you define. What I really like about this is it gets around the problem of models having to create their children. If you build the tree top-down, then you have a function which needs all of the config, and it can't really know what settings to pass where. spaCy definitely has had that problem...The config lets us build the objects bottom-up instead.",True,{}
explosion/thinc,https://github.com/explosion/thinc,223,2020-01-15T11:32:03Z,2020-01-16T20:22:27Z,2020-01-16T20:22:27Z,MERGED,True,161,0,1,https://github.com/adrianeboyd,Add textcat from config example,3,['examples'],https://github.com/explosion/thinc/pull/223,https://github.com/adrianeboyd,7,https://github.com/explosion/thinc/pull/223#issuecomment-574818550,"A textcat from config example that supports two datasets with mutually exclusive classes:

imdb: 2 classes, from ml-datasets
dbpedia_ontology: 14 classes, loader implemented locally for fast.ai dataset on AWS

Requires syntok for tokenization.","That example makes sense! If I make nV in Embed optional, I can get the following to work:
[model]
@layers = ""EmbedPoolTextcat.v0""

[model.embed]
@layers = ""Embed.v0""
nO = ${hyper_params:width}

    model = loaded_config[""model""]
    model.get_ref(""embed"").set_dim(""nV"", len(vocab))
    model.initialize(X=train_X, Y=train_y)

That finally seems not-terrible.",True,{}
explosion/thinc,https://github.com/explosion/thinc,223,2020-01-15T11:32:03Z,2020-01-16T20:22:27Z,2020-01-16T20:22:27Z,MERGED,True,161,0,1,https://github.com/adrianeboyd,Add textcat from config example,3,['examples'],https://github.com/explosion/thinc/pull/223,https://github.com/honnibal,8,https://github.com/explosion/thinc/pull/223#issuecomment-574938729,"A textcat from config example that supports two datasets with mutually exclusive classes:

imdb: 2 classes, from ml-datasets
dbpedia_ontology: 14 classes, loader implemented locally for fast.ai dataset on AWS

Requires syntok for tokenization.","@adrianeboyd Great, yes let's make nV optional.",True,{}
explosion/thinc,https://github.com/explosion/thinc,223,2020-01-15T11:32:03Z,2020-01-16T20:22:27Z,2020-01-16T20:22:27Z,MERGED,True,161,0,1,https://github.com/adrianeboyd,Add textcat from config example,3,['examples'],https://github.com/explosion/thinc/pull/223,https://github.com/honnibal,9,https://github.com/explosion/thinc/pull/223#issuecomment-575208111,"A textcat from config example that supports two datasets with mutually exclusive classes:

imdb: 2 classes, from ml-datasets
dbpedia_ontology: 14 classes, loader implemented locally for fast.ai dataset on AWS

Requires syntok for tokenization.",Is this ready now? It looks good to me.,True,{}
explosion/thinc,https://github.com/explosion/thinc,223,2020-01-15T11:32:03Z,2020-01-16T20:22:27Z,2020-01-16T20:22:27Z,MERGED,True,161,0,1,https://github.com/adrianeboyd,Add textcat from config example,3,['examples'],https://github.com/explosion/thinc/pull/223,https://github.com/adrianeboyd,10,https://github.com/explosion/thinc/pull/223#issuecomment-575212079,"A textcat from config example that supports two datasets with mutually exclusive classes:

imdb: 2 classes, from ml-datasets
dbpedia_ontology: 14 classes, loader implemented locally for fast.ai dataset on AWS

Requires syntok for tokenization.","Yes, with the Embed PR it should work.",True,{}
explosion/thinc,https://github.com/explosion/thinc,224,2020-01-15T12:17:17Z,2020-01-15T12:22:20Z,2020-01-15T12:26:14Z,MERGED,True,3,3,3,https://github.com/svlandeg,Pass nI to LayerNorm,1,[],https://github.com/explosion/thinc/pull/224,https://github.com/svlandeg,1,https://github.com/explosion/thinc/pull/224,Pass nI as the previous layer's nO. Had a case where the dimension remained unset if I didn't do this (perhaps also pointing to an issue with dimension inference?). Either way - this makes the code more robust.,Pass nI as the previous layer's nO. Had a case where the dimension remained unset if I didn't do this (perhaps also pointing to an issue with dimension inference?). Either way - this makes the code more robust.,True,{}
explosion/thinc,https://github.com/explosion/thinc,224,2020-01-15T12:17:17Z,2020-01-15T12:22:20Z,2020-01-15T12:26:14Z,MERGED,True,3,3,3,https://github.com/svlandeg,Pass nI to LayerNorm,1,[],https://github.com/explosion/thinc/pull/224,https://github.com/apps/codecov,2,https://github.com/explosion/thinc/pull/224#issuecomment-574635930,Pass nI as the previous layer's nO. Had a case where the dimension remained unset if I didn't do this (perhaps also pointing to an issue with dimension inference?). Either way - this makes the code more robust.,"Codecov Report

Merging #224 into develop will not change coverage.
The diff coverage is 100%.


@@           Coverage Diff            @@
##           develop     #224   +/-   ##
========================================
  Coverage    92.15%   92.15%           
========================================
  Files           63       63           
  Lines         3376     3376           
========================================
  Hits          3111     3111           
  Misses         265      265



Impacted Files
Coverage 





thinc/layers/mish.py
100% <100%> ()



thinc/layers/maxout.py
100% <100%> ()



thinc/layers/relu.py
100% <100%> ()





Continue to review full report at Codecov.

Legend - Click here to learn more
 = absolute <relative> (impact),  = not affected, ? = missing data
Powered by Codecov. Last update c89313e...356a9d9. Read the comment docs.",True,{}
explosion/thinc,https://github.com/explosion/thinc,225,2020-01-15T13:03:24Z,2020-01-15T14:16:42Z,2020-01-15T14:16:44Z,MERGED,True,53,12,4,https://github.com/honnibal,Couple of changes to PR #220 (set dropout rate),3,['enhancement'],https://github.com/explosion/thinc/pull/225,https://github.com/honnibal,1,https://github.com/explosion/thinc/pull/225,"Move to thinc.model to avoid circular imports
Add more general change_attr_values method
Small changes to code: better to put the defaults in the signature
Add tests","Move to thinc.model to avoid circular imports
Add more general change_attr_values method
Small changes to code: better to put the defaults in the signature
Add tests",True,{'THUMBS_UP': ['https://github.com/svlandeg']}
explosion/thinc,https://github.com/explosion/thinc,225,2020-01-15T13:03:24Z,2020-01-15T14:16:42Z,2020-01-15T14:16:44Z,MERGED,True,53,12,4,https://github.com/honnibal,Couple of changes to PR #220 (set dropout rate),3,['enhancement'],https://github.com/explosion/thinc/pull/225,https://github.com/apps/codecov,2,https://github.com/explosion/thinc/pull/225#issuecomment-574651166,"Move to thinc.model to avoid circular imports
Add more general change_attr_values method
Small changes to code: better to put the defaults in the signature
Add tests","Codecov Report

Merging #225 into develop will decrease coverage by 0.13%.
The diff coverage is 33.33%.


@@             Coverage Diff             @@
##           develop     #225      +/-   ##
===========================================
- Coverage    92.07%   91.93%   -0.14%     
===========================================
  Files           63       62       -1     
  Lines         3393     3336      -57     
===========================================
- Hits          3124     3067      -57     
  Misses         269      269



Impacted Files
Coverage 





thinc/mypy.py
100% <> ()



thinc/util.py
100% <> ()



thinc/optimizers.py
55.85% <0%> ()



thinc/layers/add_module.py
100% <100%> ()





Continue to review full report at Codecov.

Legend - Click here to learn more
 = absolute <relative> (impact),  = not affected, ? = missing data
Powered by Codecov. Last update 96428d4...79f2830. Read the comment docs.",True,{}
explosion/thinc,https://github.com/explosion/thinc,226,2020-01-15T14:14:58Z,2020-01-15T16:47:49Z,2020-01-22T00:54:12Z,MERGED,True,5,7,1,https://github.com/ines,Fix setup.py clean,1,"['enhancement', 'install']",https://github.com/explosion/thinc/pull/226,https://github.com/ines,1,https://github.com/explosion/thinc/pull/226,,,True,{}
explosion/thinc,https://github.com/explosion/thinc,226,2020-01-15T14:14:58Z,2020-01-15T16:47:49Z,2020-01-22T00:54:12Z,MERGED,True,5,7,1,https://github.com/ines,Fix setup.py clean,1,"['enhancement', 'install']",https://github.com/explosion/thinc/pull/226,https://github.com/apps/codecov,2,https://github.com/explosion/thinc/pull/226#issuecomment-574678856,,"Codecov Report

Merging #226 into develop will increase coverage by 0.04%.
The diff coverage is n/a.


@@             Coverage Diff             @@
##           develop     #226      +/-   ##
===========================================
+ Coverage    91.73%   91.78%   +0.04%     
===========================================
  Files           62       62              
  Lines         3329     3347      +18     
===========================================
+ Hits          3054     3072      +18     
  Misses         275      275



Impacted Files
Coverage 





thinc/model.py
100% <0%> ()



thinc/api.py
100% <0%> ()



thinc/util.py
94.73% <0%> (+0.04%)





Continue to review full report at Codecov.

Legend - Click here to learn more
 = absolute <relative> (impact),  = not affected, ? = missing data
Powered by Codecov. Last update 6ca6fc2...6327220. Read the comment docs.",True,{}
explosion/thinc,https://github.com/explosion/thinc,227,2020-01-15T16:17:35Z,2020-01-15T16:34:58Z,2020-01-15T16:35:00Z,MERGED,True,131,49,6,https://github.com/ines,Refactor / update optimizers,2,"['enhancement', 'tests']",https://github.com/explosion/thinc/pull/227,https://github.com/ines,1,https://github.com/explosion/thinc/pull/227,"As discussed with @honnibal:

remove lookahead and related code
make Optimizer.__call__ take arguments key, weights, gradient (instead of making key a required keyword argument)

Also tidy up and add tests.","As discussed with @honnibal:

remove lookahead and related code
make Optimizer.__call__ take arguments key, weights, gradient (instead of making key a required keyword argument)

Also tidy up and add tests.",True,{}
explosion/thinc,https://github.com/explosion/thinc,227,2020-01-15T16:17:35Z,2020-01-15T16:34:58Z,2020-01-15T16:35:00Z,MERGED,True,131,49,6,https://github.com/ines,Refactor / update optimizers,2,"['enhancement', 'tests']",https://github.com/explosion/thinc/pull/227,https://github.com/apps/codecov,2,https://github.com/explosion/thinc/pull/227#issuecomment-574735356,"As discussed with @honnibal:

remove lookahead and related code
make Optimizer.__call__ take arguments key, weights, gradient (instead of making key a required keyword argument)

Also tidy up and add tests.","Codecov Report

Merging #227 into develop will increase coverage by 2.2%.
The diff coverage is 100%.


@@            Coverage Diff             @@
##           develop     #227     +/-   ##
==========================================
+ Coverage    91.93%   94.13%   +2.2%     
==========================================
  Files           62       62             
  Lines         3335     3308     -27     
==========================================
+ Hits          3066     3114     +48     
+ Misses         269      194     -75



Impacted Files
Coverage 





thinc/optimizers.py
95.03% <100%> (+39.18%)



thinc/model.py
100% <100%> ()



thinc/config.py
100% <100%> ()





Continue to review full report at Codecov.

Legend - Click here to learn more
 = absolute <relative> (impact),  = not affected, ? = missing data
Powered by Codecov. Last update d3296f4...152e4ce. Read the comment docs.",True,{}
explosion/thinc,https://github.com/explosion/thinc,227,2020-01-15T16:17:35Z,2020-01-15T16:34:58Z,2020-01-15T16:35:00Z,MERGED,True,131,49,6,https://github.com/ines,Refactor / update optimizers,2,"['enhancement', 'tests']",https://github.com/explosion/thinc/pull/227,https://github.com/honnibal,3,https://github.com/explosion/thinc/pull/227#issuecomment-574741546,"As discussed with @honnibal:

remove lookahead and related code
make Optimizer.__call__ take arguments key, weights, gradient (instead of making key a required keyword argument)

Also tidy up and add tests.","Nice! I think the Ray example might need the same change, with the optimizer? Maybe do a quick search for optimizer( to double check for other cases. Otherwise, LGTM!",True,{}
explosion/thinc,https://github.com/explosion/thinc,228,2020-01-15T17:12:51Z,2020-01-15T17:14:19Z,2020-01-15T17:14:19Z,CLOSED,False,521,117,15,https://github.com/honnibal,WIP: Experiment with JaxOps,19,[],https://github.com/explosion/thinc/pull/228,https://github.com/honnibal,1,https://github.com/explosion/thinc/pull/228,,,True,{}
explosion/thinc,https://github.com/explosion/thinc,229,2020-01-15T17:15:02Z,2020-01-20T14:22:04Z,2020-01-20T14:22:07Z,MERGED,True,1974,574,37,https://github.com/honnibal,WIP: JaxOps,157,"['enhancement', 'feat / ops', 'interop / jax']",https://github.com/explosion/thinc/pull/229,https://github.com/honnibal,1,https://github.com/explosion/thinc/pull/229,"Okay so this is miles away from ready, but opening this up to explain what I've learned.
Jax is kind of the new hotness in ML engines. It features:

numpy interface
Autodiff as a function transformation
JIT compilation
Support for CPU, GPU and TPU

It's actually a perfect fit for us, and I've had one eye on Jax-compatibility during the whole redesign. The autodiff is really cool because you can write something like this:
def some_new_forward_function(model, X, is_train):
    W = model.get_param(""W"")
    b = model.get_param(""b"")
    Y, get_gradients = jax.vjp(_do_work, X, W, b)
    def backprop(dY):
        dX, dW, db = get_g(dY)
        model.inc_grad(""W"", dW)
        model.inc_grad(""b"", db)
        return dX

    return Y, backprop

def _do_work(X, W, b):
    # This could be pretty much any maths, with some
    # limitations (but not many)
    return X @ W.T + b

In other words, it does exactly the thing we need if people don't want to write the backprop for some function. The JIT compilation should be very cool too, because it fits quite well into how we do things.
Unlike Tensorflow and PyTorch, Jax support will be as a backend -- so we'll have a JaxOps class. We want to keep Jax as an optional dependency, so we'll need to do the conditional imports etc. I think we probably need a thinc.compat module to manage those, as it's getting a bit out of control.
The JaxOps module should specialise various Ops methods, and call into methods decorated with @jax.jit. The jax.jit function also takes a second argument, static_argnums. I'm not 100% sure of the trade-offs on this but I think it means you get different versions of the function compiled for the different branches of that value, which is nice for stuff like small constants. For instance, it seems good for stuff like window size, padding size, axis args, dtype, etc.
@jax.jit functions can only take certain types of arguments, because it needs to be able to understand the object (? not sure of the inner workings). You can register ""flatten"" functions that let you express some object in terms of a flattened list, sort of like a saner pickle. I've created flatten functions for the Model class. Using it requires that the model instance has an attribute registry_name, which should be the name of a layers registry function that accepts no arguments, and can be used to create a shell of the model. The flattened data is then used to fill in all of the dims, params, attrs etc.
Another trap is that Jax doesn't let you set sub-arrays directly, e.g. you can't do arr[i] = something, although you can do arr += something. I don't really get what's up with that. Anyway, you have to write arr = jax.ops.index_update(arr, jax.ops.index[i], something. It's a completely mechanical transformation: whatever you would put in the index, you move that over to jax.ops.index. The first arg will always be the whole array, and you'll always be assigning to the whole array as the return value. If you're doing something like X[:, n] += Y[:, n] that would become X = index_update(X, index[:, n], Y[:, n] + X[:, n].
It's possible that with JaxOps we could even be faster than PyTorch or Tensorflow for some networks, without our code actually looking any different. It's extremely hard to predict though -- we just have to try it and see.

 Grok Jax
 Basic JaxOps
 Allow jax.jit to take Model instances
 Make some GPU benchmarks of JaxOps vs NumpyOps with JIT
 Benchmark a small network using JaxOps
 Get ops tests passing for JaxOps
 JIT more ops
 Benchmark more realistic networks","Okay so this is miles away from ready, but opening this up to explain what I've learned.
Jax is kind of the new hotness in ML engines. It features:

numpy interface
Autodiff as a function transformation
JIT compilation
Support for CPU, GPU and TPU

It's actually a perfect fit for us, and I've had one eye on Jax-compatibility during the whole redesign. The autodiff is really cool because you can write something like this:
def some_new_forward_function(model, X, is_train):
    W = model.get_param(""W"")
    b = model.get_param(""b"")
    Y, get_gradients = jax.vjp(_do_work, X, W, b)
    def backprop(dY):
        dX, dW, db = get_g(dY)
        model.inc_grad(""W"", dW)
        model.inc_grad(""b"", db)
        return dX

    return Y, backprop

def _do_work(X, W, b):
    # This could be pretty much any maths, with some
    # limitations (but not many)
    return X @ W.T + b

In other words, it does exactly the thing we need if people don't want to write the backprop for some function. The JIT compilation should be very cool too, because it fits quite well into how we do things.
Unlike Tensorflow and PyTorch, Jax support will be as a backend -- so we'll have a JaxOps class. We want to keep Jax as an optional dependency, so we'll need to do the conditional imports etc. I think we probably need a thinc.compat module to manage those, as it's getting a bit out of control.
The JaxOps module should specialise various Ops methods, and call into methods decorated with @jax.jit. The jax.jit function also takes a second argument, static_argnums. I'm not 100% sure of the trade-offs on this but I think it means you get different versions of the function compiled for the different branches of that value, which is nice for stuff like small constants. For instance, it seems good for stuff like window size, padding size, axis args, dtype, etc.
@jax.jit functions can only take certain types of arguments, because it needs to be able to understand the object (? not sure of the inner workings). You can register ""flatten"" functions that let you express some object in terms of a flattened list, sort of like a saner pickle. I've created flatten functions for the Model class. Using it requires that the model instance has an attribute registry_name, which should be the name of a layers registry function that accepts no arguments, and can be used to create a shell of the model. The flattened data is then used to fill in all of the dims, params, attrs etc.
Another trap is that Jax doesn't let you set sub-arrays directly, e.g. you can't do arr[i] = something, although you can do arr += something. I don't really get what's up with that. Anyway, you have to write arr = jax.ops.index_update(arr, jax.ops.index[i], something. It's a completely mechanical transformation: whatever you would put in the index, you move that over to jax.ops.index. The first arg will always be the whole array, and you'll always be assigning to the whole array as the return value. If you're doing something like X[:, n] += Y[:, n] that would become X = index_update(X, index[:, n], Y[:, n] + X[:, n].
It's possible that with JaxOps we could even be faster than PyTorch or Tensorflow for some networks, without our code actually looking any different. It's extremely hard to predict though -- we just have to try it and see.

 Grok Jax
 Basic JaxOps
 Allow jax.jit to take Model instances
 Make some GPU benchmarks of JaxOps vs NumpyOps with JIT
 Benchmark a small network using JaxOps
 Get ops tests passing for JaxOps
 JIT more ops
 Benchmark more realistic networks",True,"{'EYES': ['https://github.com/ines'], 'HEART': ['https://github.com/justindujardin', 'https://github.com/ines', 'https://github.com/Abhijit-2592'], 'HOORAY': ['https://github.com/justindujardin', 'https://github.com/svlandeg', 'https://github.com/ines', 'https://github.com/Abhijit-2592']}"
explosion/thinc,https://github.com/explosion/thinc,229,2020-01-15T17:15:02Z,2020-01-20T14:22:04Z,2020-01-20T14:22:07Z,MERGED,True,1974,574,37,https://github.com/honnibal,WIP: JaxOps,157,"['enhancement', 'feat / ops', 'interop / jax']",https://github.com/explosion/thinc/pull/229,https://github.com/svlandeg,2,https://github.com/explosion/thinc/pull/229#issuecomment-574770004,"Okay so this is miles away from ready, but opening this up to explain what I've learned.
Jax is kind of the new hotness in ML engines. It features:

numpy interface
Autodiff as a function transformation
JIT compilation
Support for CPU, GPU and TPU

It's actually a perfect fit for us, and I've had one eye on Jax-compatibility during the whole redesign. The autodiff is really cool because you can write something like this:
def some_new_forward_function(model, X, is_train):
    W = model.get_param(""W"")
    b = model.get_param(""b"")
    Y, get_gradients = jax.vjp(_do_work, X, W, b)
    def backprop(dY):
        dX, dW, db = get_g(dY)
        model.inc_grad(""W"", dW)
        model.inc_grad(""b"", db)
        return dX

    return Y, backprop

def _do_work(X, W, b):
    # This could be pretty much any maths, with some
    # limitations (but not many)
    return X @ W.T + b

In other words, it does exactly the thing we need if people don't want to write the backprop for some function. The JIT compilation should be very cool too, because it fits quite well into how we do things.
Unlike Tensorflow and PyTorch, Jax support will be as a backend -- so we'll have a JaxOps class. We want to keep Jax as an optional dependency, so we'll need to do the conditional imports etc. I think we probably need a thinc.compat module to manage those, as it's getting a bit out of control.
The JaxOps module should specialise various Ops methods, and call into methods decorated with @jax.jit. The jax.jit function also takes a second argument, static_argnums. I'm not 100% sure of the trade-offs on this but I think it means you get different versions of the function compiled for the different branches of that value, which is nice for stuff like small constants. For instance, it seems good for stuff like window size, padding size, axis args, dtype, etc.
@jax.jit functions can only take certain types of arguments, because it needs to be able to understand the object (? not sure of the inner workings). You can register ""flatten"" functions that let you express some object in terms of a flattened list, sort of like a saner pickle. I've created flatten functions for the Model class. Using it requires that the model instance has an attribute registry_name, which should be the name of a layers registry function that accepts no arguments, and can be used to create a shell of the model. The flattened data is then used to fill in all of the dims, params, attrs etc.
Another trap is that Jax doesn't let you set sub-arrays directly, e.g. you can't do arr[i] = something, although you can do arr += something. I don't really get what's up with that. Anyway, you have to write arr = jax.ops.index_update(arr, jax.ops.index[i], something. It's a completely mechanical transformation: whatever you would put in the index, you move that over to jax.ops.index. The first arg will always be the whole array, and you'll always be assigning to the whole array as the return value. If you're doing something like X[:, n] += Y[:, n] that would become X = index_update(X, index[:, n], Y[:, n] + X[:, n].
It's possible that with JaxOps we could even be faster than PyTorch or Tensorflow for some networks, without our code actually looking any different. It's extremely hard to predict though -- we just have to try it and see.

 Grok Jax
 Basic JaxOps
 Allow jax.jit to take Model instances
 Make some GPU benchmarks of JaxOps vs NumpyOps with JIT
 Benchmark a small network using JaxOps
 Get ops tests passing for JaxOps
 JIT more ops
 Benchmark more realistic networks","Autodiff as a function transformation

This will be awesome to have! Thanks for the walk-through, excited to see how this will come together :-)",True,{}
explosion/thinc,https://github.com/explosion/thinc,229,2020-01-15T17:15:02Z,2020-01-20T14:22:04Z,2020-01-20T14:22:07Z,MERGED,True,1974,574,37,https://github.com/honnibal,WIP: JaxOps,157,"['enhancement', 'feat / ops', 'interop / jax']",https://github.com/explosion/thinc/pull/229,https://github.com/apps/codecov,3,https://github.com/explosion/thinc/pull/229#issuecomment-574938397,"Okay so this is miles away from ready, but opening this up to explain what I've learned.
Jax is kind of the new hotness in ML engines. It features:

numpy interface
Autodiff as a function transformation
JIT compilation
Support for CPU, GPU and TPU

It's actually a perfect fit for us, and I've had one eye on Jax-compatibility during the whole redesign. The autodiff is really cool because you can write something like this:
def some_new_forward_function(model, X, is_train):
    W = model.get_param(""W"")
    b = model.get_param(""b"")
    Y, get_gradients = jax.vjp(_do_work, X, W, b)
    def backprop(dY):
        dX, dW, db = get_g(dY)
        model.inc_grad(""W"", dW)
        model.inc_grad(""b"", db)
        return dX

    return Y, backprop

def _do_work(X, W, b):
    # This could be pretty much any maths, with some
    # limitations (but not many)
    return X @ W.T + b

In other words, it does exactly the thing we need if people don't want to write the backprop for some function. The JIT compilation should be very cool too, because it fits quite well into how we do things.
Unlike Tensorflow and PyTorch, Jax support will be as a backend -- so we'll have a JaxOps class. We want to keep Jax as an optional dependency, so we'll need to do the conditional imports etc. I think we probably need a thinc.compat module to manage those, as it's getting a bit out of control.
The JaxOps module should specialise various Ops methods, and call into methods decorated with @jax.jit. The jax.jit function also takes a second argument, static_argnums. I'm not 100% sure of the trade-offs on this but I think it means you get different versions of the function compiled for the different branches of that value, which is nice for stuff like small constants. For instance, it seems good for stuff like window size, padding size, axis args, dtype, etc.
@jax.jit functions can only take certain types of arguments, because it needs to be able to understand the object (? not sure of the inner workings). You can register ""flatten"" functions that let you express some object in terms of a flattened list, sort of like a saner pickle. I've created flatten functions for the Model class. Using it requires that the model instance has an attribute registry_name, which should be the name of a layers registry function that accepts no arguments, and can be used to create a shell of the model. The flattened data is then used to fill in all of the dims, params, attrs etc.
Another trap is that Jax doesn't let you set sub-arrays directly, e.g. you can't do arr[i] = something, although you can do arr += something. I don't really get what's up with that. Anyway, you have to write arr = jax.ops.index_update(arr, jax.ops.index[i], something. It's a completely mechanical transformation: whatever you would put in the index, you move that over to jax.ops.index. The first arg will always be the whole array, and you'll always be assigning to the whole array as the return value. If you're doing something like X[:, n] += Y[:, n] that would become X = index_update(X, index[:, n], Y[:, n] + X[:, n].
It's possible that with JaxOps we could even be faster than PyTorch or Tensorflow for some networks, without our code actually looking any different. It's extremely hard to predict though -- we just have to try it and see.

 Grok Jax
 Basic JaxOps
 Allow jax.jit to take Model instances
 Make some GPU benchmarks of JaxOps vs NumpyOps with JIT
 Benchmark a small network using JaxOps
 Get ops tests passing for JaxOps
 JIT more ops
 Benchmark more realistic networks","Codecov Report

Merging #229 into develop will decrease coverage by 7.29%.
The diff coverage is 61.55%.


@@            Coverage Diff             @@
##           develop     #229     +/-   ##
==========================================
- Coverage    93.16%   85.86%   -7.3%     
==========================================
  Files           64       65      +1     
  Lines         3407     4039    +632     
==========================================
+ Hits          3174     3468    +294     
- Misses         233      571    +338



Impacted Files
Coverage 





thinc/layers/list2padded_module.py
100% <> ()



thinc/backends/_param_server.py
100% <> ()



thinc/layers/concatenate.py
74.19% <0%> (+3.42%)



thinc/shims/pytorch.py
83.9% <100%> ()



thinc/shims/tensorflow.py
96.96% <100%> ()



thinc/api.py
100% <100%> ()



thinc/layers/__init__.py
100% <100%> ()



thinc/util.py
100% <100%> ()



thinc/layers/lstm.py
100% <100%> ()



thinc/layers/hashembed.py
100% <100%> ()



... and 12 more






Continue to review full report at Codecov.

Legend - Click here to learn more
 = absolute <relative> (impact),  = not affected, ? = missing data
Powered by Codecov. Last update a73c14d...96b23db. Read the comment docs.",True,{}
explosion/thinc,https://github.com/explosion/thinc,229,2020-01-15T17:15:02Z,2020-01-20T14:22:04Z,2020-01-20T14:22:07Z,MERGED,True,1974,574,37,https://github.com/honnibal,WIP: JaxOps,157,"['enhancement', 'feat / ops', 'interop / jax']",https://github.com/explosion/thinc/pull/229,https://github.com/honnibal,4,https://github.com/explosion/thinc/pull/229#issuecomment-575614225,"Okay so this is miles away from ready, but opening this up to explain what I've learned.
Jax is kind of the new hotness in ML engines. It features:

numpy interface
Autodiff as a function transformation
JIT compilation
Support for CPU, GPU and TPU

It's actually a perfect fit for us, and I've had one eye on Jax-compatibility during the whole redesign. The autodiff is really cool because you can write something like this:
def some_new_forward_function(model, X, is_train):
    W = model.get_param(""W"")
    b = model.get_param(""b"")
    Y, get_gradients = jax.vjp(_do_work, X, W, b)
    def backprop(dY):
        dX, dW, db = get_g(dY)
        model.inc_grad(""W"", dW)
        model.inc_grad(""b"", db)
        return dX

    return Y, backprop

def _do_work(X, W, b):
    # This could be pretty much any maths, with some
    # limitations (but not many)
    return X @ W.T + b

In other words, it does exactly the thing we need if people don't want to write the backprop for some function. The JIT compilation should be very cool too, because it fits quite well into how we do things.
Unlike Tensorflow and PyTorch, Jax support will be as a backend -- so we'll have a JaxOps class. We want to keep Jax as an optional dependency, so we'll need to do the conditional imports etc. I think we probably need a thinc.compat module to manage those, as it's getting a bit out of control.
The JaxOps module should specialise various Ops methods, and call into methods decorated with @jax.jit. The jax.jit function also takes a second argument, static_argnums. I'm not 100% sure of the trade-offs on this but I think it means you get different versions of the function compiled for the different branches of that value, which is nice for stuff like small constants. For instance, it seems good for stuff like window size, padding size, axis args, dtype, etc.
@jax.jit functions can only take certain types of arguments, because it needs to be able to understand the object (? not sure of the inner workings). You can register ""flatten"" functions that let you express some object in terms of a flattened list, sort of like a saner pickle. I've created flatten functions for the Model class. Using it requires that the model instance has an attribute registry_name, which should be the name of a layers registry function that accepts no arguments, and can be used to create a shell of the model. The flattened data is then used to fill in all of the dims, params, attrs etc.
Another trap is that Jax doesn't let you set sub-arrays directly, e.g. you can't do arr[i] = something, although you can do arr += something. I don't really get what's up with that. Anyway, you have to write arr = jax.ops.index_update(arr, jax.ops.index[i], something. It's a completely mechanical transformation: whatever you would put in the index, you move that over to jax.ops.index. The first arg will always be the whole array, and you'll always be assigning to the whole array as the return value. If you're doing something like X[:, n] += Y[:, n] that would become X = index_update(X, index[:, n], Y[:, n] + X[:, n].
It's possible that with JaxOps we could even be faster than PyTorch or Tensorflow for some networks, without our code actually looking any different. It's extremely hard to predict though -- we just have to try it and see.

 Grok Jax
 Basic JaxOps
 Allow jax.jit to take Model instances
 Make some GPU benchmarks of JaxOps vs NumpyOps with JIT
 Benchmark a small network using JaxOps
 Get ops tests passing for JaxOps
 JIT more ops
 Benchmark more realistic networks","I've got most things working with the JaxOps, but it's really slow. I don't know whether I'm doing things deeply wrong, or whether I'm just running examples that are too small.
I rebroke everything on the branch trying to get the performance better. I'll refix it later to get it into a state to merge.
I think we should merge this in advance of it actually being useful/performant. This will make it easier for other people to tinker with it later.
I suspect it will be difficult to have code that works well with Jax and with cupy, which is really disappointing --- I'd hoped for a more drop-in experience. Again, I'm not 100% sure: I could just be doing this wrong.",True,{}
explosion/thinc,https://github.com/explosion/thinc,230,2020-01-15T21:48:04Z,2020-01-16T01:31:45Z,2020-01-16T07:49:39Z,MERGED,True,4,3,3,https://github.com/svlandeg,Few small fixes,1,['bug'],https://github.com/explosion/thinc/pull/230,https://github.com/svlandeg,1,https://github.com/explosion/thinc/pull/230,Some small things I ran into while running code from spaCy.,Some small things I ran into while running code from spaCy.,True,{'THUMBS_UP': ['https://github.com/honnibal']}
explosion/thinc,https://github.com/explosion/thinc,230,2020-01-15T21:48:04Z,2020-01-16T01:31:45Z,2020-01-16T07:49:39Z,MERGED,True,4,3,3,https://github.com/svlandeg,Few small fixes,1,['bug'],https://github.com/explosion/thinc/pull/230,https://github.com/apps/codecov,2,https://github.com/explosion/thinc/pull/230#issuecomment-574880005,Some small things I ran into while running code from spaCy.,"Codecov Report

Merging #230 into develop will increase coverage by <.01%.
The diff coverage is 100%.


@@             Coverage Diff             @@
##           develop     #230      +/-   ##
===========================================
+ Coverage    94.13%   94.13%   +<.01%     
===========================================
  Files           62       62              
  Lines         3308     3309       +1     
===========================================
+ Hits          3114     3115       +1     
  Misses         194      194



Impacted Files
Coverage 





thinc/model.py
100% <100%> ()



thinc/layers/maxout.py
100% <100%> ()



thinc/util.py
100% <100%> ()





Continue to review full report at Codecov.

Legend - Click here to learn more
 = absolute <relative> (impact),  = not affected, ? = missing data
Powered by Codecov. Last update a0f06f9...d881c36. Read the comment docs.",True,{}
explosion/thinc,https://github.com/explosion/thinc,231,2020-01-15T22:18:02Z,2020-01-20T17:16:27Z,2020-01-20T17:30:12Z,MERGED,True,330,42,7,https://github.com/justindujardin,Merge pull request #231 from justindujardin/mathy-thinc,13,"['enhancement', 'interop / tensorflow']",https://github.com/explosion/thinc/pull/231,https://github.com/justindujardin,1,https://github.com/explosion/thinc/pull/231,"This adds some experimental changes to support de/serializing subclassed Keras models by decorating tf.keras.Model subclasses with enough information to recreate them reliably.
Here's a minimal example:
import tensorflow as tf
import numpy
from thinc.api import keras_subclass

@keras_subclass(
    ""TestModel"",
    X=numpy.array([0.0, 0.0]),
    Y=numpy.array([0.5]),
    input_shape=(2,)
)
class TestModel(tf.keras.Model):
    def call(self, inputs):
        return inputs
This decorator is not used for Tensorflow Functional/Sequential models which serialize with no special configuration.

 add keras_subclass class decorator
 update TensorFlowWrapper and TensorFlowShim to support decorated classes
 add tests
 Mathy integration

 test prediction with mathy models
 test de/serialization with mathy models
 test training with mathy models
 test restoring optimizer with mathy models","This adds some experimental changes to support de/serializing subclassed Keras models by decorating tf.keras.Model subclasses with enough information to recreate them reliably.
Here's a minimal example:
import tensorflow as tf
import numpy
from thinc.api import keras_subclass

@keras_subclass(
    ""TestModel"",
    X=numpy.array([0.0, 0.0]),
    Y=numpy.array([0.5]),
    input_shape=(2,)
)
class TestModel(tf.keras.Model):
    def call(self, inputs):
        return inputs
This decorator is not used for Tensorflow Functional/Sequential models which serialize with no special configuration.

 add keras_subclass class decorator
 update TensorFlowWrapper and TensorFlowShim to support decorated classes
 add tests
 Mathy integration

 test prediction with mathy models
 test de/serialization with mathy models
 test training with mathy models
 test restoring optimizer with mathy models",True,{'HOORAY': ['https://github.com/ines']}
explosion/thinc,https://github.com/explosion/thinc,231,2020-01-15T22:18:02Z,2020-01-20T17:16:27Z,2020-01-20T17:30:12Z,MERGED,True,330,42,7,https://github.com/justindujardin,Merge pull request #231 from justindujardin/mathy-thinc,13,"['enhancement', 'interop / tensorflow']",https://github.com/explosion/thinc/pull/231,https://github.com/apps/codecov,2,https://github.com/explosion/thinc/pull/231#issuecomment-574938398,"This adds some experimental changes to support de/serializing subclassed Keras models by decorating tf.keras.Model subclasses with enough information to recreate them reliably.
Here's a minimal example:
import tensorflow as tf
import numpy
from thinc.api import keras_subclass

@keras_subclass(
    ""TestModel"",
    X=numpy.array([0.0, 0.0]),
    Y=numpy.array([0.5]),
    input_shape=(2,)
)
class TestModel(tf.keras.Model):
    def call(self, inputs):
        return inputs
This decorator is not used for Tensorflow Functional/Sequential models which serialize with no special configuration.

 add keras_subclass class decorator
 update TensorFlowWrapper and TensorFlowShim to support decorated classes
 add tests
 Mathy integration

 test prediction with mathy models
 test de/serialization with mathy models
 test training with mathy models
 test restoring optimizer with mathy models","Codecov Report

Merging #231 into develop will decrease coverage by 8.34%.
The diff coverage is 68.61%.


@@             Coverage Diff             @@
##           develop     #231      +/-   ##
===========================================
- Coverage    94.15%   85.81%   -8.35%     
===========================================
  Files           62       65       +3     
  Lines         3371     4096     +725     
===========================================
+ Hits          3174     3515     +341     
- Misses         197      581     +384



Impacted Files
Coverage 





thinc/layers/list2padded_module.py
100% <> ()



thinc/backends/_param_server.py
100% <> ()



thinc/layers/with_reshape.py
23.07% <0%> (-0.61%)



thinc/layers/recurrent.py
0% <0%> (-100%)



thinc/layers/remap_ids.py
100% <100%> ()



thinc/util.py
100% <100%> ()



thinc/shims/tensorflow.py
96.1% <100%> ()



thinc/layers/hashembed.py
100% <100%> ()



thinc/model.py
100% <100%> ()



thinc/layers/residual.py
88.88% <100%> (+0.31%)



... and 43 more






Continue to review full report at Codecov.

Legend - Click here to learn more
 = absolute <relative> (impact),  = not affected, ? = missing data
Powered by Codecov. Last update d0d0f9a...cfaa18e. Read the comment docs.",True,{}
explosion/thinc,https://github.com/explosion/thinc,231,2020-01-15T22:18:02Z,2020-01-20T17:16:27Z,2020-01-20T17:30:12Z,MERGED,True,330,42,7,https://github.com/justindujardin,Merge pull request #231 from justindujardin/mathy-thinc,13,"['enhancement', 'interop / tensorflow']",https://github.com/explosion/thinc/pull/231,https://github.com/honnibal,3,https://github.com/explosion/thinc/pull/231#issuecomment-574939249,"This adds some experimental changes to support de/serializing subclassed Keras models by decorating tf.keras.Model subclasses with enough information to recreate them reliably.
Here's a minimal example:
import tensorflow as tf
import numpy
from thinc.api import keras_subclass

@keras_subclass(
    ""TestModel"",
    X=numpy.array([0.0, 0.0]),
    Y=numpy.array([0.5]),
    input_shape=(2,)
)
class TestModel(tf.keras.Model):
    def call(self, inputs):
        return inputs
This decorator is not used for Tensorflow Functional/Sequential models which serialize with no special configuration.

 add keras_subclass class decorator
 update TensorFlowWrapper and TensorFlowShim to support decorated classes
 add tests
 Mathy integration

 test prediction with mathy models
 test de/serialization with mathy models
 test training with mathy models
 test restoring optimizer with mathy models","I like it! I think the decorator really makes the best of the situation, I think it'll be very usable. Let me know when it's ready to merge.",True,{}
explosion/thinc,https://github.com/explosion/thinc,231,2020-01-15T22:18:02Z,2020-01-20T17:16:27Z,2020-01-20T17:30:12Z,MERGED,True,330,42,7,https://github.com/justindujardin,Merge pull request #231 from justindujardin/mathy-thinc,13,"['enhancement', 'interop / tensorflow']",https://github.com/explosion/thinc/pull/231,https://github.com/ines,4,https://github.com/explosion/thinc/pull/231#issuecomment-576350693,"This adds some experimental changes to support de/serializing subclassed Keras models by decorating tf.keras.Model subclasses with enough information to recreate them reliably.
Here's a minimal example:
import tensorflow as tf
import numpy
from thinc.api import keras_subclass

@keras_subclass(
    ""TestModel"",
    X=numpy.array([0.0, 0.0]),
    Y=numpy.array([0.5]),
    input_shape=(2,)
)
class TestModel(tf.keras.Model):
    def call(self, inputs):
        return inputs
This decorator is not used for Tensorflow Functional/Sequential models which serialize with no special configuration.

 add keras_subclass class decorator
 update TensorFlowWrapper and TensorFlowShim to support decorated classes
 add tests
 Mathy integration

 test prediction with mathy models
 test de/serialization with mathy models
 test training with mathy models
 test restoring optimizer with mathy models",@justindujardin Is this ready to merge? ,True,{}
explosion/thinc,https://github.com/explosion/thinc,231,2020-01-15T22:18:02Z,2020-01-20T17:16:27Z,2020-01-20T17:30:12Z,MERGED,True,330,42,7,https://github.com/justindujardin,Merge pull request #231 from justindujardin/mathy-thinc,13,"['enhancement', 'interop / tensorflow']",https://github.com/explosion/thinc/pull/231,https://github.com/justindujardin,5,https://github.com/explosion/thinc/pull/231#issuecomment-576353982,"This adds some experimental changes to support de/serializing subclassed Keras models by decorating tf.keras.Model subclasses with enough information to recreate them reliably.
Here's a minimal example:
import tensorflow as tf
import numpy
from thinc.api import keras_subclass

@keras_subclass(
    ""TestModel"",
    X=numpy.array([0.0, 0.0]),
    Y=numpy.array([0.5]),
    input_shape=(2,)
)
class TestModel(tf.keras.Model):
    def call(self, inputs):
        return inputs
This decorator is not used for Tensorflow Functional/Sequential models which serialize with no special configuration.

 add keras_subclass class decorator
 update TensorFlowWrapper and TensorFlowShim to support decorated classes
 add tests
 Mathy integration

 test prediction with mathy models
 test de/serialization with mathy models
 test training with mathy models
 test restoring optimizer with mathy models","Sure, I have some more changes, but we can follow up with another PR later",True,{}
explosion/thinc,https://github.com/explosion/thinc,232,2020-01-16T09:59:40Z,2020-01-16T11:19:03Z,2020-01-16T11:19:03Z,MERGED,True,4,4,2,https://github.com/adrianeboyd,Make Embed nV optional,2,['feat / layers'],https://github.com/explosion/thinc/pull/232,https://github.com/adrianeboyd,1,https://github.com/explosion/thinc/pull/232,Make Embed nV optional,Make Embed nV optional,True,{'THUMBS_UP': ['https://github.com/honnibal']}
explosion/thinc,https://github.com/explosion/thinc,232,2020-01-16T09:59:40Z,2020-01-16T11:19:03Z,2020-01-16T11:19:03Z,MERGED,True,4,4,2,https://github.com/adrianeboyd,Make Embed nV optional,2,['feat / layers'],https://github.com/explosion/thinc/pull/232,https://github.com/apps/codecov,2,https://github.com/explosion/thinc/pull/232#issuecomment-575077860,Make Embed nV optional,"Codecov Report

Merging #232 into develop will not change coverage.
The diff coverage is 100%.


@@           Coverage Diff            @@
##           develop     #232   +/-   ##
========================================
  Coverage    94.13%   94.13%           
========================================
  Files           62       62           
  Lines         3308     3308           
========================================
  Hits          3114     3114           
  Misses         194      194



Impacted Files
Coverage 





thinc/layers/embed.py
100% <100%> ()





Continue to review full report at Codecov.

Legend - Click here to learn more
 = absolute <relative> (impact),  = not affected, ? = missing data
Powered by Codecov. Last update 0445ace...a648e68. Read the comment docs.",True,{}
explosion/thinc,https://github.com/explosion/thinc,233,2020-01-16T14:29:47Z,2020-01-16T20:33:24Z,2020-01-16T21:44:04Z,MERGED,True,31,0,1,https://github.com/svlandeg,Have concatenate support lists,5,"['enhancement', 'feat / layers']",https://github.com/explosion/thinc/pull/233,https://github.com/svlandeg,1,https://github.com/explosion/thinc/pull/233,"Add in support for concatenating lists in the concatenate layer.
This works, as tested with spaCy. Before we had to do
concatenate_lists(CharacterEmbed(....), FeatureExtracter(...)
now just concatenate works (without messing up the normal concatenate cases)","Add in support for concatenating lists in the concatenate layer.
This works, as tested with spaCy. Before we had to do
concatenate_lists(CharacterEmbed(....), FeatureExtracter(...)
now just concatenate works (without messing up the normal concatenate cases)",True,{'THUMBS_UP': ['https://github.com/honnibal']}
explosion/thinc,https://github.com/explosion/thinc,233,2020-01-16T14:29:47Z,2020-01-16T20:33:24Z,2020-01-16T21:44:04Z,MERGED,True,31,0,1,https://github.com/svlandeg,Have concatenate support lists,5,"['enhancement', 'feat / layers']",https://github.com/explosion/thinc/pull/233,https://github.com/apps/codecov,2,https://github.com/explosion/thinc/pull/233#issuecomment-575178555,"Add in support for concatenating lists in the concatenate layer.
This works, as tested with spaCy. Before we had to do
concatenate_lists(CharacterEmbed(....), FeatureExtracter(...)
now just concatenate works (without messing up the normal concatenate cases)","Codecov Report

Merging #233 into develop will decrease coverage by 0.52%.
The diff coverage is 17.39%.


@@            Coverage Diff             @@
##           develop    #233      +/-   ##
==========================================
- Coverage    94.13%   93.6%   -0.53%     
==========================================
  Files           62      62              
  Lines         3308    3331      +23     
==========================================
+ Hits          3114    3118       +4     
- Misses         194     213      +19



Impacted Files
Coverage 





thinc/layers/concatenate.py
70.31% <17.39%> (-29.69%)





Continue to review full report at Codecov.

Legend - Click here to learn more
 = absolute <relative> (impact),  = not affected, ? = missing data
Powered by Codecov. Last update 09432f6...2bb9394. Read the comment docs.",True,{}
explosion/thinc,https://github.com/explosion/thinc,233,2020-01-16T14:29:47Z,2020-01-16T20:33:24Z,2020-01-16T21:44:04Z,MERGED,True,31,0,1,https://github.com/svlandeg,Have concatenate support lists,5,"['enhancement', 'feat / layers']",https://github.com/explosion/thinc/pull/233,https://github.com/honnibal,3,https://github.com/explosion/thinc/pull/233#issuecomment-575207434,"Add in support for concatenating lists in the concatenate layer.
This works, as tested with spaCy. Before we had to do
concatenate_lists(CharacterEmbed(....), FeatureExtracter(...)
now just concatenate works (without messing up the normal concatenate cases)","It's a bit of a pain, but I think it'll be best if we follow the style from the with_array, with_ragged` etc layers and have different forward functions for the different data-types. Otherwise the conditional logic gets really messy.
Could you change it over to that style?",True,{'THUMBS_UP': ['https://github.com/svlandeg']}
explosion/thinc,https://github.com/explosion/thinc,233,2020-01-16T14:29:47Z,2020-01-16T20:33:24Z,2020-01-16T21:44:04Z,MERGED,True,31,0,1,https://github.com/svlandeg,Have concatenate support lists,5,"['enhancement', 'feat / layers']",https://github.com/explosion/thinc/pull/233,https://github.com/svlandeg,4,https://github.com/explosion/thinc/pull/233#issuecomment-575214269,"Add in support for concatenating lists in the concatenate layer.
This works, as tested with spaCy. Before we had to do
concatenate_lists(CharacterEmbed(....), FeatureExtracter(...)
now just concatenate works (without messing up the normal concatenate cases)","Both ways kind of suck from a code-maintenance perspective, but your suggestion is probably more readable :-)",True,{}
explosion/thinc,https://github.com/explosion/thinc,234,2020-01-16T15:45:01Z,2020-01-17T01:01:56Z,2020-01-22T00:54:08Z,MERGED,True,1802,341,23,https://github.com/ines,"Update examples, add notebooks",24,"['install', 'examples']",https://github.com/explosion/thinc/pull/234,https://github.com/ines,1,https://github.com/explosion/thinc/pull/234,"add installation instructions for dependencies etc. to the top of all example scripts
 create requirements.txt with dependencies for all examples
 remove example-only dependencies (typer, tqdm) from setup
 remove howto directory and add notebooks (see below)
 add datasets registry (came up in example and seems like something users would want)

New content
I want to create more notebooks, since they make it really easy to illustrate the concepts and let users try things live. We can then add them to the README with ""Launch on Colab"" buttons. This also works for the GPU examples, which is pretty nice.

 ""Thinc for beginners"" notebook that walks through MNIST example and explains config system with runnable examples
 Intro to Model class and methods: this is mostly a merged and notebook-ified version of the 2 howto scripts, still needs some more text and also has some errors at the moment
 Transformers tagger example","add installation instructions for dependencies etc. to the top of all example scripts
 create requirements.txt with dependencies for all examples
 remove example-only dependencies (typer, tqdm) from setup
 remove howto directory and add notebooks (see below)
 add datasets registry (came up in example and seems like something users would want)

New content
I want to create more notebooks, since they make it really easy to illustrate the concepts and let users try things live. We can then add them to the README with ""Launch on Colab"" buttons. This also works for the GPU examples, which is pretty nice.

 ""Thinc for beginners"" notebook that walks through MNIST example and explains config system with runnable examples
 Intro to Model class and methods: this is mostly a merged and notebook-ified version of the 2 howto scripts, still needs some more text and also has some errors at the moment
 Transformers tagger example",True,{'HEART': ['https://github.com/svlandeg']}
explosion/thinc,https://github.com/explosion/thinc,234,2020-01-16T15:45:01Z,2020-01-17T01:01:56Z,2020-01-22T00:54:08Z,MERGED,True,1802,341,23,https://github.com/ines,"Update examples, add notebooks",24,"['install', 'examples']",https://github.com/explosion/thinc/pull/234,https://github.com/apps/codecov,2,https://github.com/explosion/thinc/pull/234#issuecomment-575213103,"add installation instructions for dependencies etc. to the top of all example scripts
 create requirements.txt with dependencies for all examples
 remove example-only dependencies (typer, tqdm) from setup
 remove howto directory and add notebooks (see below)
 add datasets registry (came up in example and seems like something users would want)

New content
I want to create more notebooks, since they make it really easy to illustrate the concepts and let users try things live. We can then add them to the README with ""Launch on Colab"" buttons. This also works for the GPU examples, which is pretty nice.

 ""Thinc for beginners"" notebook that walks through MNIST example and explains config system with runnable examples
 Intro to Model class and methods: this is mostly a merged and notebook-ified version of the 2 howto scripts, still needs some more text and also has some errors at the moment
 Transformers tagger example","Codecov Report

Merging #234 into develop will decrease coverage by 0.26%.
The diff coverage is 70.83%.


@@             Coverage Diff             @@
##           develop     #234      +/-   ##
===========================================
- Coverage    94.14%   93.87%   -0.27%     
===========================================
  Files           62       62              
  Lines         3311     3479     +168     
===========================================
+ Hits          3117     3266     +149     
- Misses         194      213      +19



Impacted Files
Coverage 





thinc/model.py
100% <100%> ()



thinc/layers/concatenate.py
70.31% <69.56%> (-29.69%)



thinc/layers/embed.py
100% <0%> ()



thinc/layers/hashembed.py
100% <0%> ()



thinc/layers/maxout.py
100% <0%> ()





Continue to review full report at Codecov.

Legend - Click here to learn more
 = absolute <relative> (impact),  = not affected, ? = missing data
Powered by Codecov. Last update 6b73a7e...bfac636. Read the comment docs.",True,{}
explosion/thinc,https://github.com/explosion/thinc,235,2020-01-16T16:07:30Z,2020-01-16T23:30:33Z,2020-01-16T23:30:54Z,MERGED,True,39,30,4,https://github.com/svlandeg,avoid lambda and nested functions to allow pickling Model,24,['bug'],https://github.com/explosion/thinc/pull/235,https://github.com/svlandeg,1,https://github.com/explosion/thinc/pull/235,"If we run spaCy with multiple processes, the Model needs to be pickled. For this we have to avoid nested functions & lambda functions, so I rewrote these until the final spaCy tests started working ...
(can be tested with test_language in spaCy - I can write some tests for thinc too but just wanted to get this PR out so spaCy can build)","If we run spaCy with multiple processes, the Model needs to be pickled. For this we have to avoid nested functions & lambda functions, so I rewrote these until the final spaCy tests started working ...
(can be tested with test_language in spaCy - I can write some tests for thinc too but just wanted to get this PR out so spaCy can build)",True,{}
explosion/thinc,https://github.com/explosion/thinc,235,2020-01-16T16:07:30Z,2020-01-16T23:30:33Z,2020-01-16T23:30:54Z,MERGED,True,39,30,4,https://github.com/svlandeg,avoid lambda and nested functions to allow pickling Model,24,['bug'],https://github.com/explosion/thinc/pull/235,https://github.com/apps/codecov,2,https://github.com/explosion/thinc/pull/235#issuecomment-575224578,"If we run spaCy with multiple processes, the Model needs to be pickled. For this we have to avoid nested functions & lambda functions, so I rewrote these until the final spaCy tests started working ...
(can be tested with test_language in spaCy - I can write some tests for thinc too but just wanted to get this PR out so spaCy can build)","Codecov Report

Merging #235 into develop will decrease coverage by 0.51%.
The diff coverage is 100%.


@@             Coverage Diff             @@
##           develop     #235      +/-   ##
===========================================
- Coverage    94.13%   93.61%   -0.52%     
===========================================
  Files           62       62              
  Lines         3308     3337      +29     
===========================================
+ Hits          3114     3124      +10     
- Misses         194      213      +19



Impacted Files
Coverage 





thinc/model.py
100% <100%> ()



thinc/layers/hashembed.py
100% <100%> ()



thinc/layers/maxout.py
100% <100%> ()



thinc/layers/embed.py
100% <100%> ()



thinc/layers/concatenate.py
70.31% <0%> (-29.69%)





Continue to review full report at Codecov.

Legend - Click here to learn more
 = absolute <relative> (impact),  = not affected, ? = missing data
Powered by Codecov. Last update 09432f6...7170602. Read the comment docs.",True,{}
explosion/thinc,https://github.com/explosion/thinc,236,2020-01-17T11:16:11Z,2020-01-17T12:09:29Z,2020-01-17T12:23:29Z,MERGED,True,2,2,1,https://github.com/svlandeg,update requirements,1,['install'],https://github.com/explosion/thinc/pull/236,https://github.com/svlandeg,1,https://github.com/explosion/thinc/pull/236,"I ran into issues with older versions which were fixed by updating. I changed the specs in the requirements.txt according to these links:

https://hypothesis.readthedocs.io/en/latest/changes.html#v3-27-0 : deadline since 3.27.0
https://stackoverflow.com/a/58189684/7961860: a certain bug in pytest that got fixed after 5.2.0","I ran into issues with older versions which were fixed by updating. I changed the specs in the requirements.txt according to these links:

https://hypothesis.readthedocs.io/en/latest/changes.html#v3-27-0 : deadline since 3.27.0
https://stackoverflow.com/a/58189684/7961860: a certain bug in pytest that got fixed after 5.2.0",True,{'THUMBS_UP': ['https://github.com/ines']}
explosion/thinc,https://github.com/explosion/thinc,236,2020-01-17T11:16:11Z,2020-01-17T12:09:29Z,2020-01-17T12:23:29Z,MERGED,True,2,2,1,https://github.com/svlandeg,update requirements,1,['install'],https://github.com/explosion/thinc/pull/236,https://github.com/apps/codecov,2,https://github.com/explosion/thinc/pull/236#issuecomment-575586595,"I ran into issues with older versions which were fixed by updating. I changed the specs in the requirements.txt according to these links:

https://hypothesis.readthedocs.io/en/latest/changes.html#v3-27-0 : deadline since 3.27.0
https://stackoverflow.com/a/58189684/7961860: a certain bug in pytest that got fixed after 5.2.0","Codecov Report

 No coverage uploaded for pull request base (develop@6cd628f). Click here to learn what that means.
The diff coverage is n/a.


@@            Coverage Diff             @@
##             develop     #236   +/-   ##
==========================================
  Coverage           ?   93.62%           
==========================================
  Files              ?       62           
  Lines              ?     3340           
  Branches           ?        0           
==========================================
  Hits               ?     3127           
  Misses             ?      213           
  Partials           ?        0

Continue to review full report at Codecov.

Legend - Click here to learn more
 = absolute <relative> (impact),  = not affected, ? = missing data
Powered by Codecov. Last update 6cd628f...2a5ede5. Read the comment docs.",True,{}
explosion/thinc,https://github.com/explosion/thinc,237,2020-01-17T12:49:19Z,2020-01-18T12:55:44Z,2020-01-18T15:03:05Z,MERGED,True,3,6,3,https://github.com/svlandeg,small bugfix in cupy asarray,2,"['bug', 'feat / ops']",https://github.com/explosion/thinc/pull/237,https://github.com/svlandeg,1,https://github.com/explosion/thinc/pull/237,"There was one spot where the cupy function call wasn't yet rewritten to **dtype.
Also added a more user-friendly warning in Model.from_bytes to point to incompatible versions (bound to happen).","There was one spot where the cupy function call wasn't yet rewritten to **dtype.
Also added a more user-friendly warning in Model.from_bytes to point to incompatible versions (bound to happen).",True,{}
explosion/thinc,https://github.com/explosion/thinc,237,2020-01-17T12:49:19Z,2020-01-18T12:55:44Z,2020-01-18T15:03:05Z,MERGED,True,3,6,3,https://github.com/svlandeg,small bugfix in cupy asarray,2,"['bug', 'feat / ops']",https://github.com/explosion/thinc/pull/237,https://github.com/apps/codecov,2,https://github.com/explosion/thinc/pull/237#issuecomment-575614799,"There was one spot where the cupy function call wasn't yet rewritten to **dtype.
Also added a more user-friendly warning in Model.from_bytes to point to incompatible versions (bound to happen).","Codecov Report

Merging #237 into develop will increase coverage by 0.05%.
The diff coverage is 100%.


@@             Coverage Diff             @@
##           develop     #237      +/-   ##
===========================================
+ Coverage    93.62%   93.67%   +0.05%     
===========================================
  Files           62       62              
  Lines         3340     3338       -2     
===========================================
  Hits          3127     3127              
+ Misses         213      211       -2



Impacted Files
Coverage 





thinc/backends/ops.py
69.27% <> (+0.47%)



thinc/model.py
99.74% <100%> (-0.26%)





Continue to review full report at Codecov.

Legend - Click here to learn more
 = absolute <relative> (impact),  = not affected, ? = missing data
Powered by Codecov. Last update 5a7f3e8...a446373. Read the comment docs.",True,{}
explosion/thinc,https://github.com/explosion/thinc,238,2020-01-17T15:13:02Z,2020-01-19T16:26:59Z,2020-01-22T00:54:06Z,MERGED,True,171,110,5,https://github.com/ines,WIP: losses refactor,4,['enhancement'],https://github.com/explosion/thinc/pull/238,https://github.com/ines,1,https://github.com/explosion/thinc/pull/238,"Instead of functions, we now have classes with get_grad and get_loss methods, to allow users to compute gradient and loss separately (while still making it easy to define the class in the registry and config). There's also a __call__ method that returns both.
Not sure if I've been doing the types correctly.","Instead of functions, we now have classes with get_grad and get_loss methods, to allow users to compute gradient and loss separately (while still making it easy to define the class in the registry and config). There's also a __call__ method that returns both.
Not sure if I've been doing the types correctly.",True,{'THUMBS_UP': ['https://github.com/svlandeg']}
explosion/thinc,https://github.com/explosion/thinc,238,2020-01-17T15:13:02Z,2020-01-19T16:26:59Z,2020-01-22T00:54:06Z,MERGED,True,171,110,5,https://github.com/ines,WIP: losses refactor,4,['enhancement'],https://github.com/explosion/thinc/pull/238,https://github.com/apps/codecov,2,https://github.com/explosion/thinc/pull/238#issuecomment-575670574,"Instead of functions, we now have classes with get_grad and get_loss methods, to allow users to compute gradient and loss separately (while still making it easy to define the class in the registry and config). There's also a __call__ method that returns both.
Not sure if I've been doing the types correctly.","Codecov Report

 No coverage uploaded for pull request base (develop@ff2b204). Click here to learn what that means.
The diff coverage is 91.86%.


@@            Coverage Diff             @@
##             develop     #238   +/-   ##
==========================================
  Coverage           ?   93.59%           
==========================================
  Files              ?       62           
  Lines              ?     3468           
  Branches           ?        0           
==========================================
  Hits               ?     3246           
  Misses             ?      222           
  Partials           ?        0



Impacted Files
Coverage 





thinc/api.py
100% <100%> ()



thinc/loss.py
91.86% <91.66%> ()





Continue to review full report at Codecov.

Legend - Click here to learn more
 = absolute <relative> (impact),  = not affected, ? = missing data
Powered by Codecov. Last update ff2b204...9f1a499. Read the comment docs.",True,{}
explosion/thinc,https://github.com/explosion/thinc,239,2020-01-19T15:32:46Z,2020-01-19T19:21:57Z,2020-01-19T19:23:48Z,MERGED,True,406,0,1,https://github.com/ines,WIP: Port over hierarchical textcat example to notebook,5,['examples'],https://github.com/explosion/thinc/pull/239,https://github.com/ines,1,https://github.com/explosion/thinc/pull/239,,,True,{}
explosion/thinc,https://github.com/explosion/thinc,239,2020-01-19T15:32:46Z,2020-01-19T19:21:57Z,2020-01-19T19:23:48Z,MERGED,True,406,0,1,https://github.com/ines,WIP: Port over hierarchical textcat example to notebook,5,['examples'],https://github.com/explosion/thinc/pull/239,https://github.com/apps/codecov,2,https://github.com/explosion/thinc/pull/239#issuecomment-576016855,,"Codecov Report

Merging #239 into develop will decrease coverage by 0.05%.
The diff coverage is n/a.


@@             Coverage Diff             @@
##           develop     #239      +/-   ##
===========================================
- Coverage    93.67%   93.62%   -0.06%     
===========================================
  Files           62       62              
  Lines         3338     3356      +18     
===========================================
+ Hits          3127     3142      +15     
- Misses         211      214       +3



Impacted Files
Coverage 





thinc/layers/siamese.py
38.7% <0%> (-1.3%)



thinc/layers/with_reshape.py
23.07% <0%> (-0.61%)



thinc/model.py
99.46% <0%> (-0.28%)



thinc/layers/mish.py
100% <0%> ()



thinc/layers/add_module.py
100% <0%> ()



thinc/layers/uniqued.py
100% <0%> ()



thinc/layers/embed.py
100% <0%> ()



thinc/layers/lstm.py
100% <0%> ()



thinc/initializers.py
100% <0%> ()



thinc/layers/hashembed.py
100% <0%> ()



... and 19 more






Continue to review full report at Codecov.

Legend - Click here to learn more
 = absolute <relative> (impact),  = not affected, ? = missing data
Powered by Codecov. Last update 32ee20d...798e99a. Read the comment docs.",True,{}
explosion/thinc,https://github.com/explosion/thinc,240,2020-01-19T16:18:12Z,2020-01-19T16:51:09Z,2020-01-19T16:51:11Z,MERGED,True,241,200,34,https://github.com/ines,Update initializers API,7,['enhancement'],https://github.com/explosion/thinc/pull/240,https://github.com/ines,1,https://github.com/explosion/thinc/pull/240,,,True,{}
explosion/thinc,https://github.com/explosion/thinc,240,2020-01-19T16:18:12Z,2020-01-19T16:51:09Z,2020-01-19T16:51:11Z,MERGED,True,241,200,34,https://github.com/ines,Update initializers API,7,['enhancement'],https://github.com/explosion/thinc/pull/240,https://github.com/apps/codecov,2,https://github.com/explosion/thinc/pull/240#issuecomment-576021483,,"Codecov Report

Merging #240 into develop will decrease coverage by 0.13%.
The diff coverage is 91.86%.


@@             Coverage Diff             @@
##           develop     #240      +/-   ##
===========================================
- Coverage    93.61%   93.47%   -0.14%     
===========================================
  Files           62       62              
  Lines         3353     3389      +36     
===========================================
+ Hits          3139     3168      +29     
- Misses         214      221       +7



Impacted Files
Coverage 





thinc/api.py
100% <100%> ()



thinc/loss.py
91.86% <91.66%> (-8.14%)





Continue to review full report at Codecov.

Legend - Click here to learn more
 = absolute <relative> (impact),  = not affected, ? = missing data
Powered by Codecov. Last update 041875c...f3dc5ca. Read the comment docs.",True,{}
explosion/thinc,https://github.com/explosion/thinc,241,2020-01-19T17:21:04Z,2020-01-19T17:42:51Z,2020-01-19T17:42:53Z,MERGED,True,37,58,21,https://github.com/ines,Don't auto-init on model creation,2,"['enhancement', 'feat / layers']",https://github.com/explosion/thinc/pull/241,https://github.com/ines,1,https://github.com/explosion/thinc/pull/241,"TODO

 fix LSTM tests","TODO

 fix LSTM tests",True,{}
explosion/thinc,https://github.com/explosion/thinc,241,2020-01-19T17:21:04Z,2020-01-19T17:42:51Z,2020-01-19T17:42:53Z,MERGED,True,37,58,21,https://github.com/ines,Don't auto-init on model creation,2,"['enhancement', 'feat / layers']",https://github.com/explosion/thinc/pull/241,https://github.com/apps/codecov,2,https://github.com/explosion/thinc/pull/241#issuecomment-576027136,"TODO

 fix LSTM tests","Codecov Report

Merging #241 into develop will decrease coverage by 0.04%.
The diff coverage is 100%.


@@             Coverage Diff             @@
##           develop     #241      +/-   ##
===========================================
- Coverage    93.66%   93.62%   -0.05%     
===========================================
  Files           62       62              
  Lines         3380     3356      -24     
===========================================
- Hits          3166     3142      -24     
  Misses         214      214



Impacted Files
Coverage 





thinc/layers/hashembed.py
100% <> ()



thinc/layers/mish.py
100% <> ()



thinc/layers/maxout.py
100% <> ()



thinc/layers/chain_module.py
100% <> ()



thinc/layers/relu.py
100% <> ()



thinc/layers/recurrent.py
100% <100%> ()



thinc/layers/lstm.py
100% <100%> ()



thinc/layers/linear.py
100% <100%> ()



thinc/layers/embed.py
100% <100%> ()



thinc/layers/softmax.py
100% <100%> ()





Continue to review full report at Codecov.

Legend - Click here to learn more
 = absolute <relative> (impact),  = not affected, ? = missing data
Powered by Codecov. Last update ebbe349...969cfda. Read the comment docs.",True,{}
explosion/thinc,https://github.com/explosion/thinc,242,2020-01-19T17:54:19Z,2020-01-19T18:16:02Z,2020-01-19T18:25:44Z,MERGED,True,145,146,25,https://github.com/ines,Rename layers,1,"['enhancement', 'feat / layers']",https://github.com/explosion/thinc/pull/242,https://github.com/ines,1,https://github.com/explosion/thinc/pull/242,"mean_pool  reduce_mean
 max_pool  reduce_max
 sum_pool  reduce_sum
 ExtractWindow  expand_window","mean_pool  reduce_mean
 max_pool  reduce_max
 sum_pool  reduce_sum
 ExtractWindow  expand_window",True,{}
explosion/thinc,https://github.com/explosion/thinc,242,2020-01-19T17:54:19Z,2020-01-19T18:16:02Z,2020-01-19T18:25:44Z,MERGED,True,145,146,25,https://github.com/ines,Rename layers,1,"['enhancement', 'feat / layers']",https://github.com/explosion/thinc/pull/242,https://github.com/apps/codecov,2,https://github.com/explosion/thinc/pull/242#issuecomment-576029240,"mean_pool  reduce_mean
 max_pool  reduce_max
 sum_pool  reduce_sum
 ExtractWindow  expand_window","Codecov Report

Merging #242 into develop will not change coverage.
The diff coverage is 100%.


@@           Coverage Diff            @@
##           develop     #242   +/-   ##
========================================
  Coverage    93.62%   93.62%           
========================================
  Files           62       62           
  Lines         3356     3356           
========================================
  Hits          3142     3142           
  Misses         214      214



Impacted Files
Coverage 





thinc/layers/reduce_sum.py
100% <100%> ()



thinc/api.py
100% <100%> ()



thinc/layers/__init__.py
100% <100%> ()



thinc/layers/expand_window.py
100% <100%> ()



thinc/layers/reduce_mean.py
100% <100%> ()



thinc/backends/ops.py
69.27% <100%> ()



thinc/layers/reduce_max.py
100% <100%> ()





Continue to review full report at Codecov.

Legend - Click here to learn more
 = absolute <relative> (impact),  = not affected, ? = missing data
Powered by Codecov. Last update 8079763...edb3d80. Read the comment docs.",True,{}
explosion/thinc,https://github.com/explosion/thinc,243,2020-01-19T18:38:06Z,2020-01-19T18:54:21Z,2020-01-19T18:54:24Z,MERGED,True,43,1,4,https://github.com/ines,Add remap_ids layer,1,"['enhancement', 'feat / layers']",https://github.com/explosion/thinc/pull/243,https://github.com/ines,1,https://github.com/explosion/thinc/pull/243,,,True,{'THUMBS_UP': ['https://github.com/honnibal']}
explosion/thinc,https://github.com/explosion/thinc,243,2020-01-19T18:38:06Z,2020-01-19T18:54:21Z,2020-01-19T18:54:24Z,MERGED,True,43,1,4,https://github.com/ines,Add remap_ids layer,1,"['enhancement', 'feat / layers']",https://github.com/explosion/thinc/pull/243,https://github.com/apps/codecov,2,https://github.com/explosion/thinc/pull/243#issuecomment-576033515,,"Codecov Report

Merging #243 into develop will increase coverage by 0.03%.
The diff coverage is 100%.


@@             Coverage Diff             @@
##           develop     #243      +/-   ##
===========================================
+ Coverage    93.62%   93.65%   +0.03%     
===========================================
  Files           62       63       +1     
  Lines         3356     3375      +19     
===========================================
+ Hits          3142     3161      +19     
  Misses         214      214



Impacted Files
Coverage 





thinc/api.py
100% <100%> ()



thinc/layers/__init__.py
100% <100%> ()



thinc/layers/remap_ids.py
100% <100%> ()





Continue to review full report at Codecov.

Legend - Click here to learn more
 = absolute <relative> (impact),  = not affected, ? = missing data
Powered by Codecov. Last update db937e3...1925e03. Read the comment docs.",True,{}
explosion/thinc,https://github.com/explosion/thinc,244,2020-01-19T19:04:12Z,2020-01-19T19:13:49Z,2020-01-19T19:13:53Z,MERGED,True,53,1,3,https://github.com/ines,Add with_flatten,3,"['enhancement', 'feat / layers']",https://github.com/explosion/thinc/pull/244,https://github.com/ines,1,https://github.com/explosion/thinc/pull/244,,,True,{}
explosion/thinc,https://github.com/explosion/thinc,244,2020-01-19T19:04:12Z,2020-01-19T19:13:49Z,2020-01-19T19:13:53Z,MERGED,True,53,1,3,https://github.com/ines,Add with_flatten,3,"['enhancement', 'feat / layers']",https://github.com/explosion/thinc/pull/244,https://github.com/apps/codecov,2,https://github.com/explosion/thinc/pull/244#issuecomment-576035963,,"Codecov Report

Merging #244 into develop will not change coverage.
The diff coverage is n/a.


@@           Coverage Diff            @@
##           develop     #244   +/-   ##
========================================
  Coverage    93.16%   93.16%           
========================================
  Files           64       64           
  Lines         3407     3407           
========================================
  Hits          3174     3174           
  Misses         233      233



Impacted Files
Coverage 





thinc/layers/__init__.py
100% <> ()





Continue to review full report at Codecov.

Legend - Click here to learn more
 = absolute <relative> (impact),  = not affected, ? = missing data
Powered by Codecov. Last update 6a1c4a7...0c3ed0b. Read the comment docs.",True,{}
explosion/thinc,https://github.com/explosion/thinc,245,2020-01-20T09:08:43Z,2020-01-20T14:35:28Z,2020-01-20T14:40:59Z,MERGED,True,13,10,4,https://github.com/svlandeg,Fixing ops,6,"['bug', 'feat / ops']",https://github.com/explosion/thinc/pull/245,https://github.com/svlandeg,1,https://github.com/explosion/thinc/pull/245,"Trying to get spaCy's train_from_config working with GPU. Found some cases in initializers.py that were using numpy instead of ops.xp.
Also note that ops.xp.prod(shape) does not work for cupy, you have to make sure the argument is an array first.","Trying to get spaCy's train_from_config working with GPU. Found some cases in initializers.py that were using numpy instead of ops.xp.
Also note that ops.xp.prod(shape) does not work for cupy, you have to make sure the argument is an array first.",True,{}
explosion/thinc,https://github.com/explosion/thinc,245,2020-01-20T09:08:43Z,2020-01-20T14:35:28Z,2020-01-20T14:40:59Z,MERGED,True,13,10,4,https://github.com/svlandeg,Fixing ops,6,"['bug', 'feat / ops']",https://github.com/explosion/thinc/pull/245,https://github.com/svlandeg,2,https://github.com/explosion/thinc/pull/245#issuecomment-576238295,"Trying to get spaCy's train_from_config working with GPU. Found some cases in initializers.py that were using numpy instead of ops.xp.
Also note that ops.xp.prod(shape) does not work for cupy, you have to make sure the argument is an array first.",Update: added to_numpy from the feature/jax2 branch. Should be an easy-to-resolve conflict because I copy-pasted.,True,{}
explosion/thinc,https://github.com/explosion/thinc,245,2020-01-20T09:08:43Z,2020-01-20T14:35:28Z,2020-01-20T14:40:59Z,MERGED,True,13,10,4,https://github.com/svlandeg,Fixing ops,6,"['bug', 'feat / ops']",https://github.com/explosion/thinc/pull/245,https://github.com/apps/codecov,3,https://github.com/explosion/thinc/pull/245#issuecomment-576299488,"Trying to get spaCy's train_from_config working with GPU. Found some cases in initializers.py that were using numpy instead of ops.xp.
Also note that ops.xp.prod(shape) does not work for cupy, you have to make sure the argument is an array first.","Codecov Report

 No coverage uploaded for pull request base (develop@33a003b). Click here to learn what that means.
The diff coverage is 61.55%.


@@            Coverage Diff             @@
##             develop     #245   +/-   ##
==========================================
  Coverage           ?   85.86%           
==========================================
  Files              ?       65           
  Lines              ?     4039           
  Branches           ?        0           
==========================================
  Hits               ?     3468           
  Misses             ?      571           
  Partials           ?        0



Impacted Files
Coverage 





thinc/layers/list2padded_module.py
100% <> ()



thinc/backends/_param_server.py
100% <> ()



thinc/layers/concatenate.py
74.6% <0%> ()



thinc/shims/pytorch.py
83.9% <100%> ()



thinc/shims/tensorflow.py
96.96% <100%> ()



thinc/api.py
100% <100%> ()



thinc/layers/__init__.py
100% <100%> ()



thinc/util.py
100% <100%> ()



thinc/layers/lstm.py
100% <100%> ()



thinc/layers/hashembed.py
100% <100%> ()



... and 9 more






Continue to review full report at Codecov.

Legend - Click here to learn more
 = absolute <relative> (impact),  = not affected, ? = missing data
Powered by Codecov. Last update 33a003b...c6973ed. Read the comment docs.",True,{}
explosion/thinc,https://github.com/explosion/thinc,246,2020-01-20T13:49:22Z,2020-01-20T14:25:51Z,2020-01-20T14:29:43Z,MERGED,True,2,1,1,https://github.com/svlandeg,Fix setting chained model's output dimension,1,"['bug', 'feat / layers']",https://github.com/explosion/thinc/pull/246,https://github.com/svlandeg,1,https://github.com/explosion/thinc/pull/246,Only set the chain model's nO dim if all the chained layers have an nO set. Popped up in a test with spaCy.,Only set the chain model's nO dim if all the chained layers have an nO set. Popped up in a test with spaCy.,True,{'THUMBS_UP': ['https://github.com/honnibal']}
explosion/thinc,https://github.com/explosion/thinc,246,2020-01-20T13:49:22Z,2020-01-20T14:25:51Z,2020-01-20T14:29:43Z,MERGED,True,2,1,1,https://github.com/svlandeg,Fix setting chained model's output dimension,1,"['bug', 'feat / layers']",https://github.com/explosion/thinc/pull/246,https://github.com/apps/codecov,2,https://github.com/explosion/thinc/pull/246#issuecomment-576284010,Only set the chain model's nO dim if all the chained layers have an nO set. Popped up in a test with spaCy.,"Codecov Report

Merging #246 into develop will increase coverage by <.01%.
The diff coverage is 100%.


@@             Coverage Diff             @@
##           develop     #246      +/-   ##
===========================================
+ Coverage    93.16%   93.16%   +<.01%     
===========================================
  Files           64       64              
  Lines         3407     3408       +1     
===========================================
+ Hits          3174     3175       +1     
  Misses         233      233



Impacted Files
Coverage 





thinc/layers/concatenate.py
71.21% <100%> (+0.44%)





Continue to review full report at Codecov.

Legend - Click here to learn more
 = absolute <relative> (impact),  = not affected, ? = missing data
Powered by Codecov. Last update 946d11d...aece5e6. Read the comment docs.",True,{}
explosion/thinc,https://github.com/explosion/thinc,247,2020-01-20T15:33:35Z,2020-01-29T16:47:32Z,2020-01-29T16:47:39Z,CLOSED,False,3,1,1,https://github.com/tiangolo,Make mypy tests run without requiring pip install -e ./,2,"['enhancement', 'tests', 'interop / mypy']",https://github.com/explosion/thinc/pull/247,https://github.com/tiangolo,1,https://github.com/explosion/thinc/pull/247, Make mypy tests run without requiring pip install -e ./, Make mypy tests run without requiring pip install -e ./,True,{}
explosion/thinc,https://github.com/explosion/thinc,247,2020-01-20T15:33:35Z,2020-01-29T16:47:32Z,2020-01-29T16:47:39Z,CLOSED,False,3,1,1,https://github.com/tiangolo,Make mypy tests run without requiring pip install -e ./,2,"['enhancement', 'tests', 'interop / mypy']",https://github.com/explosion/thinc/pull/247,https://github.com/tiangolo,2,https://github.com/explosion/thinc/pull/247#issuecomment-579850396, Make mypy tests run without requiring pip install -e ./,Superseded by #293,True,{}
explosion/thinc,https://github.com/explosion/thinc,248,2020-01-20T15:42:11Z,2020-01-20T15:59:47Z,2020-01-22T00:54:56Z,MERGED,True,18,31,2,https://github.com/honnibal,Improve chain.initialize() output shape inference,4,"['bug', 'enhancement', 'feat / layers']",https://github.com/explosion/thinc/pull/248,https://github.com/honnibal,1,https://github.com/explosion/thinc/pull/248,"Inferring output shapes is kind of tricky, but it's also really nice if we can get it to work. Previously we used a very complicated approach, based on a sort of bidirectional inference. We tried to set shapes from the end of the network, and from the front, and sort of hoped it worked out.
Instead, this PR goes with a simpler heuristic that's easy to explain, which hopefully will be easier to program against:
If a layer has an unset nO dimension, it receives the Y variable passed in to chain.initialize(). This means that you can leave nO dimensions unset if they can be inferred from the eventual output. If reading the nO dimension from the model output would leave to an incorrect result (for instance if there's a shape transformation at the end), you'll need to specify the nO dimension, as it won't be inferred correctly from the data.","Inferring output shapes is kind of tricky, but it's also really nice if we can get it to work. Previously we used a very complicated approach, based on a sort of bidirectional inference. We tried to set shapes from the end of the network, and from the front, and sort of hoped it worked out.
Instead, this PR goes with a simpler heuristic that's easy to explain, which hopefully will be easier to program against:
If a layer has an unset nO dimension, it receives the Y variable passed in to chain.initialize(). This means that you can leave nO dimensions unset if they can be inferred from the eventual output. If reading the nO dimension from the model output would leave to an incorrect result (for instance if there's a shape transformation at the end), you'll need to specify the nO dimension, as it won't be inferred correctly from the data.",True,{}
explosion/thinc,https://github.com/explosion/thinc,248,2020-01-20T15:42:11Z,2020-01-20T15:59:47Z,2020-01-22T00:54:56Z,MERGED,True,18,31,2,https://github.com/honnibal,Improve chain.initialize() output shape inference,4,"['bug', 'enhancement', 'feat / layers']",https://github.com/explosion/thinc/pull/248,https://github.com/apps/codecov,2,https://github.com/explosion/thinc/pull/248#issuecomment-576331385,"Inferring output shapes is kind of tricky, but it's also really nice if we can get it to work. Previously we used a very complicated approach, based on a sort of bidirectional inference. We tried to set shapes from the end of the network, and from the front, and sort of hoped it worked out.
Instead, this PR goes with a simpler heuristic that's easy to explain, which hopefully will be easier to program against:
If a layer has an unset nO dimension, it receives the Y variable passed in to chain.initialize(). This means that you can leave nO dimensions unset if they can be inferred from the eventual output. If reading the nO dimension from the model output would leave to an incorrect result (for instance if there's a shape transformation at the end), you'll need to specify the nO dimension, as it won't be inferred correctly from the data.","Codecov Report

Merging #248 into develop will decrease coverage by 0.13%.
The diff coverage is 71.42%.


@@             Coverage Diff             @@
##           develop     #248      +/-   ##
===========================================
- Coverage    85.86%   85.73%   -0.14%     
===========================================
  Files           65       65              
  Lines         4039     4030       -9     
===========================================
- Hits          3468     3455      -13     
- Misses         571      575       +4



Impacted Files
Coverage 





thinc/optimizers.py
94.51% <> (-0.04%)



thinc/layers/chain_module.py
92.59% <71.42%> (-7.41%)





Continue to review full report at Codecov.

Legend - Click here to learn more
 = absolute <relative> (impact),  = not affected, ? = missing data
Powered by Codecov. Last update dccc46f...530126e. Read the comment docs.",True,{}
explosion/thinc,https://github.com/explosion/thinc,249,2020-01-20T15:44:57Z,2020-01-20T16:00:23Z,2020-01-20T16:00:23Z,MERGED,True,3,0,1,https://github.com/svlandeg,restore some edits from concatenate,1,['feat / layers'],https://github.com/explosion/thinc/pull/249,https://github.com/svlandeg,1,https://github.com/explosion/thinc/pull/249,"In spaCy, test_tok2vec_configs was failing on calling backprop(vectors) because of the recent changes to concatenate. I reverted part of the edits from here, and that seems to fix it. I'm not sure why this got removed in the first place though.","In spaCy, test_tok2vec_configs was failing on calling backprop(vectors) because of the recent changes to concatenate. I reverted part of the edits from here, and that seems to fix it. I'm not sure why this got removed in the first place though.",True,{}
explosion/thinc,https://github.com/explosion/thinc,249,2020-01-20T15:44:57Z,2020-01-20T16:00:23Z,2020-01-20T16:00:23Z,MERGED,True,3,0,1,https://github.com/svlandeg,restore some edits from concatenate,1,['feat / layers'],https://github.com/explosion/thinc/pull/249,https://github.com/apps/codecov,2,https://github.com/explosion/thinc/pull/249#issuecomment-576331148,"In spaCy, test_tok2vec_configs was failing on calling backprop(vectors) because of the recent changes to concatenate. I reverted part of the edits from here, and that seems to fix it. I'm not sure why this got removed in the first place though.","Codecov Report

 No coverage uploaded for pull request base (develop@f7ebaf7). Click here to learn what that means.
The diff coverage is 0%.


@@            Coverage Diff             @@
##             develop     #249   +/-   ##
==========================================
  Coverage           ?   85.79%           
==========================================
  Files              ?       65           
  Lines              ?     4042           
  Branches           ?        0           
==========================================
  Hits               ?     3468           
  Misses             ?      574           
  Partials           ?        0



Impacted Files
Coverage 





thinc/layers/concatenate.py
71.21% <0%> ()





Continue to review full report at Codecov.

Legend - Click here to learn more
 = absolute <relative> (impact),  = not affected, ? = missing data
Powered by Codecov. Last update f7ebaf7...dfa7fbe. Read the comment docs.",True,{}
explosion/thinc,https://github.com/explosion/thinc,249,2020-01-20T15:44:57Z,2020-01-20T16:00:23Z,2020-01-20T16:00:23Z,MERGED,True,3,0,1,https://github.com/svlandeg,restore some edits from concatenate,1,['feat / layers'],https://github.com/explosion/thinc/pull/249,https://github.com/honnibal,3,https://github.com/explosion/thinc/pull/249#issuecomment-576335772,"In spaCy, test_tok2vec_configs was failing on calling backprop(vectors) because of the recent changes to concatenate. I reverted part of the edits from here, and that seems to fix it. I'm not sure why this got removed in the first place though.","Ah, sorry! I had trouble merging the concatenate changes, because Jax was fussy about it too.",True,{}
explosion/thinc,https://github.com/explosion/thinc,250,2020-01-20T16:31:37Z,2020-01-22T02:16:38Z,2020-01-26T14:37:55Z,CLOSED,False,10,5,3,https://github.com/ines,Hack at mypy plugin,1,"['feat / types', 'examples', 'interop / mypy']",https://github.com/explosion/thinc/pull/250,https://github.com/ines,1,https://github.com/explosion/thinc/pull/250,,,True,{}
explosion/thinc,https://github.com/explosion/thinc,250,2020-01-20T16:31:37Z,2020-01-22T02:16:38Z,2020-01-26T14:37:55Z,CLOSED,False,10,5,3,https://github.com/ines,Hack at mypy plugin,1,"['feat / types', 'examples', 'interop / mypy']",https://github.com/explosion/thinc/pull/250,https://github.com/apps/codecov,2,https://github.com/explosion/thinc/pull/250#issuecomment-576350006,,"Codecov Report

 No coverage uploaded for pull request base (develop@1c6b2ad). Click here to learn what that means.
The diff coverage is 100%.


@@            Coverage Diff             @@
##             develop     #250   +/-   ##
==========================================
  Coverage           ?   85.66%           
==========================================
  Files              ?       65           
  Lines              ?     4033           
  Branches           ?        0           
==========================================
  Hits               ?     3455           
  Misses             ?      578           
  Partials           ?        0



Impacted Files
Coverage 





thinc/mypy.py
100% <100%> ()





Continue to review full report at Codecov.

Legend - Click here to learn more
 = absolute <relative> (impact),  = not affected, ? = missing data
Powered by Codecov. Last update 1c6b2ad...ee3bd85. Read the comment docs.",True,{}
explosion/thinc,https://github.com/explosion/thinc,251,2020-01-21T14:00:57Z,2020-01-21T17:00:40Z,2020-01-21T18:22:47Z,MERGED,True,250,122,6,https://github.com/svlandeg,Fix and test losses,13,"['enhancement', 'feat / ops']",https://github.com/explosion/thinc/pull/251,https://github.com/svlandeg,1,https://github.com/explosion/thinc/pull/251,"Removed L1Distance which wasn't being used and had kind of a complex signature
Created simple L2Distance for use in spacy's pretrain
Implemented all get_loss functions
CategoricalCrossentropy is kind of similar to L2Distance, though the latter can work outside the [0,1] interval and the former can't (on purpose). I thought this might be useful for debugging.
Some usages of these functions assumed normalization, i.e. division by the number of examples given. For instance CategoricalCrossentropy was dividing by guesses.shape[0] while for CosineDistance this was done by the calling code. To make this more homogeneous I added a parameter normalize to all functions which is True by default.
I added tests that ensure the calculations are correct for all these functions.","Removed L1Distance which wasn't being used and had kind of a complex signature
Created simple L2Distance for use in spacy's pretrain
Implemented all get_loss functions
CategoricalCrossentropy is kind of similar to L2Distance, though the latter can work outside the [0,1] interval and the former can't (on purpose). I thought this might be useful for debugging.
Some usages of these functions assumed normalization, i.e. division by the number of examples given. For instance CategoricalCrossentropy was dividing by guesses.shape[0] while for CosineDistance this was done by the calling code. To make this more homogeneous I added a parameter normalize to all functions which is True by default.
I added tests that ensure the calculations are correct for all these functions.",True,"{'HOORAY': ['https://github.com/ines'], 'ROCKET': ['https://github.com/justindujardin']}"
explosion/thinc,https://github.com/explosion/thinc,251,2020-01-21T14:00:57Z,2020-01-21T17:00:40Z,2020-01-21T18:22:47Z,MERGED,True,250,122,6,https://github.com/svlandeg,Fix and test losses,13,"['enhancement', 'feat / ops']",https://github.com/explosion/thinc/pull/251,https://github.com/apps/codecov,2,https://github.com/explosion/thinc/pull/251#issuecomment-576708691,"Removed L1Distance which wasn't being used and had kind of a complex signature
Created simple L2Distance for use in spacy's pretrain
Implemented all get_loss functions
CategoricalCrossentropy is kind of similar to L2Distance, though the latter can work outside the [0,1] interval and the former can't (on purpose). I thought this might be useful for debugging.
Some usages of these functions assumed normalization, i.e. division by the number of examples given. For instance CategoricalCrossentropy was dividing by guesses.shape[0] while for CosineDistance this was done by the calling code. To make this more homogeneous I added a parameter normalize to all functions which is True by default.
I added tests that ensure the calculations are correct for all these functions.","Codecov Report

 No coverage uploaded for pull request base (develop@ef4b17c). Click here to learn what that means.
The diff coverage is 97.98%.


@@            Coverage Diff             @@
##             develop     #251   +/-   ##
==========================================
  Coverage           ?   86.71%           
==========================================
  Files              ?       65           
  Lines              ?     4079           
  Branches           ?        0           
==========================================
  Hits               ?     3537           
  Misses             ?      542           
  Partials           ?        0



Impacted Files
Coverage 





thinc/util.py
100% <> ()



thinc/layers/add.py
100% <> ()



thinc/shims/shim.py
100% <> ()



thinc/layers/list2padded.py
100% <> ()



thinc/layers/chain.py
92.59% <> ()



thinc/backends/ops.py
78.61% <> ()



thinc/backends/jax_ops.py
47.26% <100%> ()



thinc/layers/relu.py
100% <100%> ()



thinc/layers/maxout.py
100% <100%> ()



thinc/layers/mish.py
100% <100%> ()



... and 7 more






Continue to review full report at Codecov.

Legend - Click here to learn more
 = absolute <relative> (impact),  = not affected, ? = missing data
Powered by Codecov. Last update ef4b17c...fedc47f. Read the comment docs.",True,{}
explosion/thinc,https://github.com/explosion/thinc,251,2020-01-21T14:00:57Z,2020-01-21T17:00:40Z,2020-01-21T18:22:47Z,MERGED,True,250,122,6,https://github.com/svlandeg,Fix and test losses,13,"['enhancement', 'feat / ops']",https://github.com/explosion/thinc/pull/251,https://github.com/honnibal,3,https://github.com/explosion/thinc/pull/251#issuecomment-576723152,"Removed L1Distance which wasn't being used and had kind of a complex signature
Created simple L2Distance for use in spacy's pretrain
Implemented all get_loss functions
CategoricalCrossentropy is kind of similar to L2Distance, though the latter can work outside the [0,1] interval and the former can't (on purpose). I thought this might be useful for debugging.
Some usages of these functions assumed normalization, i.e. division by the number of examples given. For instance CategoricalCrossentropy was dividing by guesses.shape[0] while for CosineDistance this was done by the calling code. To make this more homogeneous I added a parameter normalize to all functions which is True by default.
I added tests that ensure the calculations are correct for all these functions.",Looks good!,True,{}
explosion/thinc,https://github.com/explosion/thinc,252,2020-01-21T18:16:52Z,2020-01-22T20:17:23Z,2020-01-22T20:17:23Z,MERGED,True,143,92,10,https://github.com/justindujardin,Fix errors using TensorFlowWrapper for multithreaded training,9,"['bug', 'interop / tensorflow', 'serialization', 'feat / ops']",https://github.com/explosion/thinc/pull/252,https://github.com/justindujardin,1,https://github.com/explosion/thinc/pull/252,"add get_thread_state helper for backends. When called it will get the thread local for the current thread, or create it from the global state attributes.
After some searching I found a blog post that talked about a subtle detail that meant you cannot do mutable thread local data without subclassing threading.local. Update model operators thread local to use a subclass. This prevents errors in the overloaded operators test.

Source: http://slinkp.com/python-thread-locals-20171201.html","add get_thread_state helper for backends. When called it will get the thread local for the current thread, or create it from the global state attributes.
After some searching I found a blog post that talked about a subtle detail that meant you cannot do mutable thread local data without subclassing threading.local. Update model operators thread local to use a subclass. This prevents errors in the overloaded operators test.

Source: http://slinkp.com/python-thread-locals-20171201.html",True,{}
explosion/thinc,https://github.com/explosion/thinc,252,2020-01-21T18:16:52Z,2020-01-22T20:17:23Z,2020-01-22T20:17:23Z,MERGED,True,143,92,10,https://github.com/justindujardin,Fix errors using TensorFlowWrapper for multithreaded training,9,"['bug', 'interop / tensorflow', 'serialization', 'feat / ops']",https://github.com/explosion/thinc/pull/252,https://github.com/apps/codecov,2,https://github.com/explosion/thinc/pull/252#issuecomment-576813663,"add get_thread_state helper for backends. When called it will get the thread local for the current thread, or create it from the global state attributes.
After some searching I found a blog post that talked about a subtle detail that meant you cannot do mutable thread local data without subclassing threading.local. Update model operators thread local to use a subclass. This prevents errors in the overloaded operators test.

Source: http://slinkp.com/python-thread-locals-20171201.html","Codecov Report

 No coverage uploaded for pull request base (develop@f06be13). Click here to learn what that means.
The diff coverage is 85.71%.


@@            Coverage Diff             @@
##             develop     #252   +/-   ##
==========================================
  Coverage           ?   87.83%           
==========================================
  Files              ?       64           
  Lines              ?     4076           
  Branches           ?        0           
==========================================
  Hits               ?     3580           
  Misses             ?      496           
  Partials           ?        0



Impacted Files
Coverage 





thinc/shims/__init__.py
100% <100%> ()



thinc/shims/shim.py
100% <100%> ()



thinc/util.py
100% <100%> ()



thinc/backends/__init__.py
100% <100%> ()



thinc/model.py
100% <100%> ()



thinc/api.py
100% <100%> ()



thinc/layers/tensorflowwrapper.py
97.8% <100%> ()



thinc/shims/tensorflow.py
92% <75.67%> ()





Continue to review full report at Codecov.

Legend - Click here to learn more
 = absolute <relative> (impact),  = not affected, ? = missing data
Powered by Codecov. Last update f06be13...476fd9f. Read the comment docs.",True,{}
explosion/thinc,https://github.com/explosion/thinc,252,2020-01-21T18:16:52Z,2020-01-22T20:17:23Z,2020-01-22T20:17:23Z,MERGED,True,143,92,10,https://github.com/justindujardin,Fix errors using TensorFlowWrapper for multithreaded training,9,"['bug', 'interop / tensorflow', 'serialization', 'feat / ops']",https://github.com/explosion/thinc/pull/252,https://github.com/justindujardin,3,https://github.com/explosion/thinc/pull/252#issuecomment-576881330,"add get_thread_state helper for backends. When called it will get the thread local for the current thread, or create it from the global state attributes.
After some searching I found a blog post that talked about a subtle detail that meant you cannot do mutable thread local data without subclassing threading.local. Update model operators thread local to use a subclass. This prevents errors in the overloaded operators test.

Source: http://slinkp.com/python-thread-locals-20171201.html","I'm getting inconsistent results with this training mathy. Don't merge it yet, I want to do some more testing.",True,{}
explosion/thinc,https://github.com/explosion/thinc,252,2020-01-21T18:16:52Z,2020-01-22T20:17:23Z,2020-01-22T20:17:23Z,MERGED,True,143,92,10,https://github.com/justindujardin,Fix errors using TensorFlowWrapper for multithreaded training,9,"['bug', 'interop / tensorflow', 'serialization', 'feat / ops']",https://github.com/explosion/thinc/pull/252,https://github.com/justindujardin,4,https://github.com/explosion/thinc/pull/252#issuecomment-577364473,"add get_thread_state helper for backends. When called it will get the thread local for the current thread, or create it from the global state attributes.
After some searching I found a blog post that talked about a subtle detail that meant you cannot do mutable thread local data without subclassing threading.local. Update model operators thread local to use a subclass. This prevents errors in the overloaded operators test.

Source: http://slinkp.com/python-thread-locals-20171201.html","Okay, I was able to resolve the inconsistencies by excluding the TensorFlow graph reset (i.e. tf.keras.backend.clear_session()) when performing from_bytes on a subclassed keras model that is only receiving new weights.",True,"{'HOORAY': ['https://github.com/ines', 'https://github.com/honnibal']}"
explosion/thinc,https://github.com/explosion/thinc,253,2020-01-21T23:16:45Z,2020-01-22T00:52:21Z,2020-01-22T00:52:23Z,MERGED,True,480,410,23,https://github.com/ines,Fix data batching and remove arbitrary helper functions,40,['enhancement'],https://github.com/explosion/thinc/pull/253,https://github.com/ines,1,https://github.com/explosion/thinc/pull/253,"make batching more flexible via Ops.minibatch (sequence) and Ops.multibatch (multiple sequences) with optional shuffling (shuffle=True)
simplify batching in training and evaluation code
remove unsatisfying helper functions","make batching more flexible via Ops.minibatch (sequence) and Ops.multibatch (multiple sequences) with optional shuffling (shuffle=True)
simplify batching in training and evaluation code
remove unsatisfying helper functions",True,{}
explosion/thinc,https://github.com/explosion/thinc,253,2020-01-21T23:16:45Z,2020-01-22T00:52:21Z,2020-01-22T00:52:23Z,MERGED,True,480,410,23,https://github.com/ines,Fix data batching and remove arbitrary helper functions,40,['enhancement'],https://github.com/explosion/thinc/pull/253,https://github.com/apps/codecov,2,https://github.com/explosion/thinc/pull/253#issuecomment-576934208,"make batching more flexible via Ops.minibatch (sequence) and Ops.multibatch (multiple sequences) with optional shuffling (shuffle=True)
simplify batching in training and evaluation code
remove unsatisfying helper functions","Codecov Report

 No coverage uploaded for pull request base (develop@a9aedf2). Click here to learn what that means.
The diff coverage is 93.71%.


@@            Coverage Diff            @@
##             develop    #253   +/-   ##
=========================================
  Coverage           ?   87.9%           
=========================================
  Files              ?      64           
  Lines              ?    4034           
  Branches           ?       0           
=========================================
  Hits               ?    3546           
  Misses             ?     488           
  Partials           ?       0



Impacted Files
Coverage 





thinc/optimizers.py
94.51% <> ()



thinc/util.py
100% <> ()



thinc/shims/shim.py
100% <> ()



thinc/layers/add.py
100% <> ()



thinc/layers/list2padded.py
100% <> ()



thinc/layers/concatenate.py
71.21% <0%> ()



thinc/backends/jax_ops.py
47.26% <100%> ()



thinc/layers/maxout.py
100% <100%> ()



thinc/shims/__init__.py
100% <100%> ()



thinc/layers/mish.py
100% <100%> ()



... and 9 more






Continue to review full report at Codecov.

Legend - Click here to learn more
 = absolute <relative> (impact),  = not affected, ? = missing data
Powered by Codecov. Last update a9aedf2...6b31bc1. Read the comment docs.",True,{}
explosion/thinc,https://github.com/explosion/thinc,254,2020-01-22T13:15:19Z,2020-01-22T13:43:41Z,2020-01-22T13:43:43Z,MERGED,True,90,48,3,https://github.com/honnibal,Slightly magical minibatching,1,"['enhancement', 'feat / ops']",https://github.com/explosion/thinc/pull/254,https://github.com/honnibal,1,https://github.com/explosion/thinc/pull/254,"It's really nice to have batches generated, rather than precomputed, because this reduces the time-to-error in a lot of debugging loops. However, it also sucks to have to pass around the size, e.g. for progress bars. Having to rebatch and reshuffle every loop sucks too.
This PR suggests a low-code solution that addresses the problems, but maybe doesn't minimise surprise. We wrap the generator in a SizedGenerator type, which we pass a generator function. The SizedGenerator knows its length, and each time it is invoked it calls its function, allowing it to repeat over the data.","It's really nice to have batches generated, rather than precomputed, because this reduces the time-to-error in a lot of debugging loops. However, it also sucks to have to pass around the size, e.g. for progress bars. Having to rebatch and reshuffle every loop sucks too.
This PR suggests a low-code solution that addresses the problems, but maybe doesn't minimise surprise. We wrap the generator in a SizedGenerator type, which we pass a generator function. The SizedGenerator knows its length, and each time it is invoked it calls its function, allowing it to repeat over the data.",True,{'EYES': ['https://github.com/svlandeg']}
explosion/thinc,https://github.com/explosion/thinc,254,2020-01-22T13:15:19Z,2020-01-22T13:43:41Z,2020-01-22T13:43:43Z,MERGED,True,90,48,3,https://github.com/honnibal,Slightly magical minibatching,1,"['enhancement', 'feat / ops']",https://github.com/explosion/thinc/pull/254,https://github.com/apps/codecov,2,https://github.com/explosion/thinc/pull/254#issuecomment-577176657,"It's really nice to have batches generated, rather than precomputed, because this reduces the time-to-error in a lot of debugging loops. However, it also sucks to have to pass around the size, e.g. for progress bars. Having to rebatch and reshuffle every loop sucks too.
This PR suggests a low-code solution that addresses the problems, but maybe doesn't minimise surprise. We wrap the generator in a SizedGenerator type, which we pass a generator function. The SizedGenerator knows its length, and each time it is invoked it calls its function, allowing it to repeat over the data.","Codecov Report

 No coverage uploaded for pull request base (develop@fe0ac1f). Click here to learn what that means.
The diff coverage is 97.95%.


@@            Coverage Diff             @@
##             develop     #254   +/-   ##
==========================================
  Coverage           ?   87.95%           
==========================================
  Files              ?       64           
  Lines              ?     4043           
  Branches           ?        0           
==========================================
  Hits               ?     3556           
  Misses             ?      487           
  Partials           ?        0



Impacted Files
Coverage 





thinc/backends/ops.py
80.65% <97.95%> ()





Continue to review full report at Codecov.

Legend - Click here to learn more
 = absolute <relative> (impact),  = not affected, ? = missing data
Powered by Codecov. Last update fe0ac1f...a8287b2. Read the comment docs.",True,{}
explosion/thinc,https://github.com/explosion/thinc,255,2020-01-22T18:06:08Z,2020-01-22T20:57:20Z,2020-01-22T20:57:20Z,MERGED,True,128,60,8,https://github.com/tiangolo,"Mypy plugin: chain, add, concatenate",4,"['enhancement', 'tests', 'interop / mypy']",https://github.com/explosion/thinc/pull/255,https://github.com/tiangolo,1,https://github.com/explosion/thinc/pull/255," Update Mypy plugin to correctly handle subtypes with chain, add specific support for add and concatenate."," Update Mypy plugin to correctly handle subtypes with chain, add specific support for add and concatenate.",True,"{'EYES': ['https://github.com/ines', 'https://github.com/svlandeg']}"
explosion/thinc,https://github.com/explosion/thinc,255,2020-01-22T18:06:08Z,2020-01-22T20:57:20Z,2020-01-22T20:57:20Z,MERGED,True,128,60,8,https://github.com/tiangolo,"Mypy plugin: chain, add, concatenate",4,"['enhancement', 'tests', 'interop / mypy']",https://github.com/explosion/thinc/pull/255,https://github.com/apps/codecov,2,https://github.com/explosion/thinc/pull/255#issuecomment-577317932," Update Mypy plugin to correctly handle subtypes with chain, add specific support for add and concatenate.","Codecov Report

 No coverage uploaded for pull request base (develop@f06be13). Click here to learn what that means.
The diff coverage is 91.37%.


@@            Coverage Diff             @@
##             develop     #255   +/-   ##
==========================================
  Coverage           ?   87.77%           
==========================================
  Files              ?       64           
  Lines              ?     4097           
  Branches           ?        0           
==========================================
  Hits               ?     3596           
  Misses             ?      501           
  Partials           ?        0



Impacted Files
Coverage 





thinc/layers/expand_window.py
100% <100%> ()



thinc/layers/add.py
100% <100%> ()



thinc/layers/chain.py
92.45% <100%> ()



thinc/layers/concatenate.py
71.64% <100%> ()



thinc/mypy.py
93.82% <89.58%> ()





Continue to review full report at Codecov.

Legend - Click here to learn more
 = absolute <relative> (impact),  = not affected, ? = missing data
Powered by Codecov. Last update f06be13...26d8607. Read the comment docs.",True,{}
explosion/thinc,https://github.com/explosion/thinc,256,2020-01-22T20:00:48Z,2020-01-22T20:16:11Z,2020-01-23T15:19:08Z,MERGED,True,1,1,1,https://github.com/svlandeg,Fix layernorm ini,6,"['bug', 'feat / layers']",https://github.com/explosion/thinc/pull/256,https://github.com/svlandeg,1,https://github.com/explosion/thinc/pull/256,"Fix by @honnibal - this fixes spaCy's tagger :-)
Also, the performance concerns raised by me in #222 seem to be resolved after this:
(myexplosion) C:\Users\Sofie\Documents\git\spacy>python -m spacy train-from-config C:\Users\Sofie\Documents\data\ud-treebanks-v2.4\UD_Dutch-Alpino\nl-alpino-json\nl_alpino-ud-train.json C:\Users\Sofie\Documents\data\ud-treebanks-v2.4\UD_Dutch-Alpino\nl-alpino-json\nl_alpino-ud-dev.json examples\experiments\ptb-joint-pos-dep\defaults.cfg
 Loading config from: examples\experiments\ptb-joint-pos-dep\defaults.cfg
 Using CPU
 Creating nlp from config
 Loading training corpus
 Initializing the nlp pipeline
 Start training
 Training. Initial learn rate: 0.001
#        LOSS TOK2VEC   LOSS TAGGER   LOSS PARSER   TAGS_ACC   UAS      LAS      SCORE
------   ------------   -----------   -----------   --------   ------   ------   ------
     0          11.71        133.19        311.16       0.88    26.70     5.88     4.88
   200        1309.18      12388.47      33028.07      71.21    52.72    38.73    45.22
   400        1241.05       8852.89      32813.88      79.57    66.22    54.92    59.85
   600        1176.64       8403.21      35156.15      83.73    72.07    62.72    66.92
   800        1165.97       8905.83      38648.57      85.67    74.64    65.67    69.67
  1000        1206.87       9453.02      44569.44      86.85    76.94    68.42    72.10
  2000        1344.12      14189.24      82215.57      90.10    82.41    75.58    78.49
...","Fix by @honnibal - this fixes spaCy's tagger :-)
Also, the performance concerns raised by me in #222 seem to be resolved after this:
(myexplosion) C:\Users\Sofie\Documents\git\spacy>python -m spacy train-from-config C:\Users\Sofie\Documents\data\ud-treebanks-v2.4\UD_Dutch-Alpino\nl-alpino-json\nl_alpino-ud-train.json C:\Users\Sofie\Documents\data\ud-treebanks-v2.4\UD_Dutch-Alpino\nl-alpino-json\nl_alpino-ud-dev.json examples\experiments\ptb-joint-pos-dep\defaults.cfg
 Loading config from: examples\experiments\ptb-joint-pos-dep\defaults.cfg
 Using CPU
 Creating nlp from config
 Loading training corpus
 Initializing the nlp pipeline
 Start training
 Training. Initial learn rate: 0.001
#        LOSS TOK2VEC   LOSS TAGGER   LOSS PARSER   TAGS_ACC   UAS      LAS      SCORE
------   ------------   -----------   -----------   --------   ------   ------   ------
     0          11.71        133.19        311.16       0.88    26.70     5.88     4.88
   200        1309.18      12388.47      33028.07      71.21    52.72    38.73    45.22
   400        1241.05       8852.89      32813.88      79.57    66.22    54.92    59.85
   600        1176.64       8403.21      35156.15      83.73    72.07    62.72    66.92
   800        1165.97       8905.83      38648.57      85.67    74.64    65.67    69.67
  1000        1206.87       9453.02      44569.44      86.85    76.94    68.42    72.10
  2000        1344.12      14189.24      82215.57      90.10    82.41    75.58    78.49
...",True,{'EYES': ['https://github.com/ines']}
explosion/thinc,https://github.com/explosion/thinc,256,2020-01-22T20:00:48Z,2020-01-22T20:16:11Z,2020-01-23T15:19:08Z,MERGED,True,1,1,1,https://github.com/svlandeg,Fix layernorm ini,6,"['bug', 'feat / layers']",https://github.com/explosion/thinc/pull/256,https://github.com/apps/codecov,2,https://github.com/explosion/thinc/pull/256#issuecomment-577362992,"Fix by @honnibal - this fixes spaCy's tagger :-)
Also, the performance concerns raised by me in #222 seem to be resolved after this:
(myexplosion) C:\Users\Sofie\Documents\git\spacy>python -m spacy train-from-config C:\Users\Sofie\Documents\data\ud-treebanks-v2.4\UD_Dutch-Alpino\nl-alpino-json\nl_alpino-ud-train.json C:\Users\Sofie\Documents\data\ud-treebanks-v2.4\UD_Dutch-Alpino\nl-alpino-json\nl_alpino-ud-dev.json examples\experiments\ptb-joint-pos-dep\defaults.cfg
 Loading config from: examples\experiments\ptb-joint-pos-dep\defaults.cfg
 Using CPU
 Creating nlp from config
 Loading training corpus
 Initializing the nlp pipeline
 Start training
 Training. Initial learn rate: 0.001
#        LOSS TOK2VEC   LOSS TAGGER   LOSS PARSER   TAGS_ACC   UAS      LAS      SCORE
------   ------------   -----------   -----------   --------   ------   ------   ------
     0          11.71        133.19        311.16       0.88    26.70     5.88     4.88
   200        1309.18      12388.47      33028.07      71.21    52.72    38.73    45.22
   400        1241.05       8852.89      32813.88      79.57    66.22    54.92    59.85
   600        1176.64       8403.21      35156.15      83.73    72.07    62.72    66.92
   800        1165.97       8905.83      38648.57      85.67    74.64    65.67    69.67
  1000        1206.87       9453.02      44569.44      86.85    76.94    68.42    72.10
  2000        1344.12      14189.24      82215.57      90.10    82.41    75.58    78.49
...","Codecov Report

 No coverage uploaded for pull request base (develop@8987d7c). Click here to learn what that means.
The diff coverage is 98.33%.


@@            Coverage Diff             @@
##             develop     #256   +/-   ##
==========================================
  Coverage           ?   87.98%           
==========================================
  Files              ?       64           
  Lines              ?     4053           
  Branches           ?        0           
==========================================
  Hits               ?     3566           
  Misses             ?      487           
  Partials           ?        0



Impacted Files
Coverage 





thinc/layers/layernorm.py
100% <100%> ()



thinc/config.py
100% <100%> ()



thinc/about.py
100% <100%> ()



thinc/backends/ops.py
80.91% <98.18%> ()





Continue to review full report at Codecov.

Legend - Click here to learn more
 = absolute <relative> (impact),  = not affected, ? = missing data
Powered by Codecov. Last update 8987d7c...d8f026d. Read the comment docs.",True,{}
explosion/thinc,https://github.com/explosion/thinc,257,2020-01-22T22:11:07Z,2020-01-23T00:59:41Z,2020-01-26T14:37:52Z,MERGED,True,50,64,8,https://github.com/ines,Make blis optional,6,"['enhancement', 'install', 'feat / ops']",https://github.com/explosion/thinc/pull/257,https://github.com/ines,1,https://github.com/explosion/thinc/pull/257,,,True,{}
explosion/thinc,https://github.com/explosion/thinc,257,2020-01-22T22:11:07Z,2020-01-23T00:59:41Z,2020-01-26T14:37:52Z,MERGED,True,50,64,8,https://github.com/ines,Make blis optional,6,"['enhancement', 'install', 'feat / ops']",https://github.com/explosion/thinc/pull/257,https://github.com/apps/codecov,2,https://github.com/explosion/thinc/pull/257#issuecomment-577451766,,"Codecov Report

Merging #257 into develop will increase coverage by 0.07%.
The diff coverage is 100%.


@@             Coverage Diff             @@
##           develop     #257      +/-   ##
===========================================
+ Coverage    87.82%   87.89%   +0.07%     
===========================================
  Files           63       63              
  Lines         4017     4016       -1     
===========================================
+ Hits          3528     3530       +2     
+ Misses         489      486       -3



Impacted Files
Coverage 





thinc/backends/jax_ops.py
47.15% <100%> (-0.12%)



thinc/layers/chain.py
98.14% <100%> (+5.69%)



thinc/backends/ops.py
82.21% <100%> (-0.04%)





Continue to review full report at Codecov.

Legend - Click here to learn more
 = absolute <relative> (impact),  = not affected, ? = missing data
Powered by Codecov. Last update 74fa2ea...5ca3a1b. Read the comment docs.",True,{}
explosion/thinc,https://github.com/explosion/thinc,258,2020-01-23T06:19:16Z,2020-01-24T02:58:10Z,2020-01-26T21:29:08Z,MERGED,True,696,0,1,https://github.com/justindujardin,Example: counting like terms with mathy + thinc,9,['examples'],https://github.com/explosion/thinc/pull/258,https://github.com/justindujardin,1,https://github.com/explosion/thinc/pull/258,"I've been curious about uses for mathy outside of RL, so I wrote a dataset generation function to do supervised learning and prediction.
Can a model learn to predict the number of like terms in a large polynomial expression?
Model
Given an input text math expression, predict the total number of like terms as a scalar value. For evaluation, the output is rounded to an integer.
Examples



Text
Answer




12c + -7986c + 7f + 10q + 8k^2 + 0.9k^2 + 5822o + 11c + 9l + 8a + 2387d + 517m^4 + 3.5k^2 + 12y + 11z + 9871t
6


4q + 4s + 8504v + 8d + 7.6x + 9778u^4 + 9.2l^2 + 6k^2 + -8525t + 5n + 7178a^4 + 11p + 4k^2 + 3s + -890k^2
5


-1634m^2 + 1q + 9d + 8w^4 + 7634.8p + -7753v^4 + 11k^3 + 8224x + 10.2n + 8.8c + 1081r + 8.7z + 1.5r + 6.0z + 5884.6r + 11.9z + 8407r
7


-2322m + 5.0l + 12z^3 + -4764j + -7608k + 4297b + 6w^2 + 4a^2 + 4o + 10.1o + 8475v + 7.8p + 5.8x^4
2



Results","I've been curious about uses for mathy outside of RL, so I wrote a dataset generation function to do supervised learning and prediction.
Can a model learn to predict the number of like terms in a large polynomial expression?
Model
Given an input text math expression, predict the total number of like terms as a scalar value. For evaluation, the output is rounded to an integer.
Examples



Text
Answer




12c + -7986c + 7f + 10q + 8k^2 + 0.9k^2 + 5822o + 11c + 9l + 8a + 2387d + 517m^4 + 3.5k^2 + 12y + 11z + 9871t
6


4q + 4s + 8504v + 8d + 7.6x + 9778u^4 + 9.2l^2 + 6k^2 + -8525t + 5n + 7178a^4 + 11p + 4k^2 + 3s + -890k^2
5


-1634m^2 + 1q + 9d + 8w^4 + 7634.8p + -7753v^4 + 11k^3 + 8224x + 10.2n + 8.8c + 1081r + 8.7z + 1.5r + 6.0z + 5884.6r + 11.9z + 8407r
7


-2322m + 5.0l + 12z^3 + -4764j + -7608k + 4297b + 6w^2 + 4a^2 + 4o + 10.1o + 8475v + 7.8p + 5.8x^4
2



Results",True,{'EYES': ['https://github.com/ines']}
explosion/thinc,https://github.com/explosion/thinc,258,2020-01-23T06:19:16Z,2020-01-24T02:58:10Z,2020-01-26T21:29:08Z,MERGED,True,696,0,1,https://github.com/justindujardin,Example: counting like terms with mathy + thinc,9,['examples'],https://github.com/explosion/thinc/pull/258,https://github.com/apps/codecov,2,https://github.com/explosion/thinc/pull/258#issuecomment-577524207,"I've been curious about uses for mathy outside of RL, so I wrote a dataset generation function to do supervised learning and prediction.
Can a model learn to predict the number of like terms in a large polynomial expression?
Model
Given an input text math expression, predict the total number of like terms as a scalar value. For evaluation, the output is rounded to an integer.
Examples



Text
Answer




12c + -7986c + 7f + 10q + 8k^2 + 0.9k^2 + 5822o + 11c + 9l + 8a + 2387d + 517m^4 + 3.5k^2 + 12y + 11z + 9871t
6


4q + 4s + 8504v + 8d + 7.6x + 9778u^4 + 9.2l^2 + 6k^2 + -8525t + 5n + 7178a^4 + 11p + 4k^2 + 3s + -890k^2
5


-1634m^2 + 1q + 9d + 8w^4 + 7634.8p + -7753v^4 + 11k^3 + 8224x + 10.2n + 8.8c + 1081r + 8.7z + 1.5r + 6.0z + 5884.6r + 11.9z + 8407r
7


-2322m + 5.0l + 12z^3 + -4764j + -7608k + 4297b + 6w^2 + 4a^2 + 4o + 10.1o + 8475v + 7.8p + 5.8x^4
2



Results","Codecov Report

Merging #258 into develop will decrease coverage by 0.01%.
The diff coverage is n/a.


@@             Coverage Diff             @@
##           develop     #258      +/-   ##
===========================================
- Coverage    87.88%   87.87%   -0.02%     
===========================================
  Files           63       63              
  Lines         4013     4017       +4     
===========================================
+ Hits          3527     3530       +3     
- Misses         486      487       +1



Impacted Files
Coverage 





thinc/layers/with_flatten.py
37.5% <0%> (-1.21%)



thinc/layers/remap_ids.py
100% <0%> ()



thinc/layers/softmax.py
100% <0%> ()



thinc/layers/relu.py
100% <0%> ()



thinc/optimizers.py
94.44% <0%> (+0.06%)





Continue to review full report at Codecov.

Legend - Click here to learn more
 = absolute <relative> (impact),  = not affected, ? = missing data
Powered by Codecov. Last update 1c81b5a...db77c86. Read the comment docs.",True,{}
explosion/thinc,https://github.com/explosion/thinc,259,2020-01-23T14:59:25Z,2020-01-23T15:21:25Z,2020-01-23T15:30:49Z,MERGED,True,6,3,1,https://github.com/svlandeg,ensure adam is called with 1D arrays,1,['bug'],https://github.com/explosion/thinc/pull/259,https://github.com/svlandeg,1,https://github.com/explosion/thinc/pull/259,"Another fix from @honnibal - I'm just the messenger / tester :-)
The adam function requires a 1D array, so 912a2ac introduced a bug that made spaCy quite unhappy.","Another fix from @honnibal - I'm just the messenger / tester :-)
The adam function requires a 1D array, so 912a2ac introduced a bug that made spaCy quite unhappy.",True,{}
explosion/thinc,https://github.com/explosion/thinc,259,2020-01-23T14:59:25Z,2020-01-23T15:21:25Z,2020-01-23T15:30:49Z,MERGED,True,6,3,1,https://github.com/svlandeg,ensure adam is called with 1D arrays,1,['bug'],https://github.com/explosion/thinc/pull/259,https://github.com/apps/codecov,2,https://github.com/explosion/thinc/pull/259#issuecomment-577720503,"Another fix from @honnibal - I'm just the messenger / tester :-)
The adam function requires a 1D array, so 912a2ac introduced a bug that made spaCy quite unhappy.","Codecov Report

Merging #259 into develop will increase coverage by <.01%.
The diff coverage is 100%.


@@             Coverage Diff             @@
##           develop     #259      +/-   ##
===========================================
+ Coverage    87.88%   87.89%   +<.01%     
===========================================
  Files           63       63              
  Lines         4013     4015       +2     
===========================================
+ Hits          3527     3529       +2     
  Misses         486      486



Impacted Files
Coverage 





thinc/optimizers.py
94.44% <100%> (+0.06%)





Continue to review full report at Codecov.

Legend - Click here to learn more
 = absolute <relative> (impact),  = not affected, ? = missing data
Powered by Codecov. Last update 1c81b5a...4fd1e15. Read the comment docs.",True,{}
explosion/thinc,https://github.com/explosion/thinc,260,2020-01-23T17:14:58Z,2020-01-23T23:51:05Z,2020-01-23T23:51:07Z,MERGED,True,53,53,26,https://github.com/ines,Fix model.ops type and related issues,4,"['bug', 'feat / types', 'feat / ops']",https://github.com/explosion/thinc/pull/260,https://github.com/ines,1,https://github.com/explosion/thinc/pull/260,"Model.ops was incorrectly typed as Union[NumpyOps, CupyOps], which meant that a variety of type issues went unnoticed.
TODO

 fix maxout","Model.ops was incorrectly typed as Union[NumpyOps, CupyOps], which meant that a variety of type issues went unnoticed.
TODO

 fix maxout",True,{}
explosion/thinc,https://github.com/explosion/thinc,260,2020-01-23T17:14:58Z,2020-01-23T23:51:05Z,2020-01-23T23:51:07Z,MERGED,True,53,53,26,https://github.com/ines,Fix model.ops type and related issues,4,"['bug', 'feat / types', 'feat / ops']",https://github.com/explosion/thinc/pull/260,https://github.com/apps/codecov,2,https://github.com/explosion/thinc/pull/260#issuecomment-577928478,"Model.ops was incorrectly typed as Union[NumpyOps, CupyOps], which meant that a variety of type issues went unnoticed.
TODO

 fix maxout","Codecov Report

Merging #260 into develop will decrease coverage by 0.01%.
The diff coverage is 94.11%.


@@             Coverage Diff             @@
##           develop     #260      +/-   ##
===========================================
- Coverage    87.89%   87.87%   -0.02%     
===========================================
  Files           63       63              
  Lines         4015     4017       +2     
===========================================
+ Hits          3529     3530       +1     
- Misses         486      487       +1



Impacted Files
Coverage 





thinc/layers/with_flatten.py
37.5% <0%> (-1.21%)



thinc/layers/list2ragged.py
100% <100%> ()



thinc/layers/residual.py
88.88% <100%> ()



thinc/layers/reduce_sum.py
100% <100%> ()



thinc/layers/remap_ids.py
100% <100%> ()



thinc/layers/with_padded.py
85.33% <100%> ()



thinc/layers/mish.py
100% <100%> ()



thinc/layers/featureextractor.py
100% <100%> ()



thinc/layers/strings2arrays.py
100% <100%> ()



thinc/layers/with_array.py
100% <100%> ()



... and 15 more






Continue to review full report at Codecov.

Legend - Click here to learn more
 = absolute <relative> (impact),  = not affected, ? = missing data
Powered by Codecov. Last update 15ccfbf...34859b3. Read the comment docs.",True,{}
explosion/thinc,https://github.com/explosion/thinc,261,2020-01-24T01:59:24Z,2020-01-24T02:55:31Z,2020-01-26T14:37:50Z,MERGED,True,106,18,12,https://github.com/ines,WIP: add runtime data validation,2,"['enhancement', 'needs tests', 'feat / types']",https://github.com/explosion/thinc/pull/261,https://github.com/ines,1,https://github.com/explosion/thinc/pull/261,"implement validation in Model.initialize where we care less about the extra overhead and have data samples available
validate the inputs and outputs against the type annotations of the forward function (X and return type)

Notes

I xfailed the LSTM test that currently causes a validation error (mismatched dimensions, I think). Not sure if this is a problem with the test, the implementation or the validation.
pydantic raises a RuntimeError if it comes across a Tuple annotation, so I've changed those occurences to Tuple[Any, ...], which should be equivalent
Cython really doesn't like any generic annotations if we want the function to be bound (so it becomes inspectable and isn't treated like a builtin)
needs more tests","implement validation in Model.initialize where we care less about the extra overhead and have data samples available
validate the inputs and outputs against the type annotations of the forward function (X and return type)

Notes

I xfailed the LSTM test that currently causes a validation error (mismatched dimensions, I think). Not sure if this is a problem with the test, the implementation or the validation.
pydantic raises a RuntimeError if it comes across a Tuple annotation, so I've changed those occurences to Tuple[Any, ...], which should be equivalent
Cython really doesn't like any generic annotations if we want the function to be bound (so it becomes inspectable and isn't treated like a builtin)
needs more tests",True,{}
explosion/thinc,https://github.com/explosion/thinc,261,2020-01-24T01:59:24Z,2020-01-24T02:55:31Z,2020-01-26T14:37:50Z,MERGED,True,106,18,12,https://github.com/ines,WIP: add runtime data validation,2,"['enhancement', 'needs tests', 'feat / types']",https://github.com/explosion/thinc/pull/261,https://github.com/apps/codecov,2,https://github.com/explosion/thinc/pull/261#issuecomment-577960944,"implement validation in Model.initialize where we care less about the extra overhead and have data samples available
validate the inputs and outputs against the type annotations of the forward function (X and return type)

Notes

I xfailed the LSTM test that currently causes a validation error (mismatched dimensions, I think). Not sure if this is a problem with the test, the implementation or the validation.
pydantic raises a RuntimeError if it comes across a Tuple annotation, so I've changed those occurences to Tuple[Any, ...], which should be equivalent
Cython really doesn't like any generic annotations if we want the function to be bound (so it becomes inspectable and isn't treated like a builtin)
needs more tests","Codecov Report

 No coverage uploaded for pull request base (develop@f155dab). Click here to learn what that means.
The diff coverage is 94.11%.


@@            Coverage Diff             @@
##             develop     #261   +/-   ##
==========================================
  Coverage           ?   87.83%           
==========================================
  Files              ?       63           
  Lines              ?     4076           
  Branches           ?        0           
==========================================
  Hits               ?     3580           
  Misses             ?      496           
  Partials           ?        0



Impacted Files
Coverage 





thinc/layers/with_getitem.py
100% <100%> ()



thinc/config.py
100% <100%> ()



thinc/layers/dropout.py
100% <100%> ()



thinc/model.py
100% <100%> ()



thinc/util.py
97.29% <92.68%> ()





Continue to review full report at Codecov.

Legend - Click here to learn more
 = absolute <relative> (impact),  = not affected, ? = missing data
Powered by Codecov. Last update f155dab...17dcec5. Read the comment docs.",True,{}
explosion/thinc,https://github.com/explosion/thinc,262,2020-01-24T03:02:13Z,2020-01-24T03:06:31Z,2020-01-24T03:06:31Z,CLOSED,False,70,0,1,https://github.com/justindujardin,Model failure repros,1,[],https://github.com/explosion/thinc/pull/262,https://github.com/justindujardin,1,https://github.com/explosion/thinc/pull/262,"Add a ""model_fails.py"" script in the root to minimally reproduce some failure cases I ran into while working on the mathy demo","Add a ""model_fails.py"" script in the root to minimally reproduce some failure cases I ran into while working on the mathy demo",True,{}
explosion/thinc,https://github.com/explosion/thinc,263,2020-01-24T09:30:16Z,2020-01-24T12:33:34Z,2020-01-24T16:07:00Z,MERGED,True,54,69,7,https://github.com/tiangolo,Replace thread locals with contextvars,5,"['enhancement', 'third-party']",https://github.com/explosion/thinc/pull/263,https://github.com/tiangolo,1,https://github.com/explosion/thinc/pull/263," Replace thread locals with contextvars.
This would allow using Thinc in scenarios that are not purely thread-based, like asyncio (e.g. in FastAPI apps). While still working the same for thread-based scenarios.
From PEP 567: https://www.python.org/dev/peps/pep-0567/#backwards-compatibility

This proposal preserves 100% backwards compatibility.
Libraries that use threading.local() to store context-related values, currently work correctly only for synchronous code. Switching them to use the proposed API will keep their behavior for synchronous code unmodified, but will automatically enable support for asynchronous code.


Python docs: https://docs.python.org/3/library/contextvars.html
PEP 567 ""Converting code that uses threading.local()"": https://www.python.org/dev/peps/pep-0567/#converting-code-that-uses-threading-local"," Replace thread locals with contextvars.
This would allow using Thinc in scenarios that are not purely thread-based, like asyncio (e.g. in FastAPI apps). While still working the same for thread-based scenarios.
From PEP 567: https://www.python.org/dev/peps/pep-0567/#backwards-compatibility

This proposal preserves 100% backwards compatibility.
Libraries that use threading.local() to store context-related values, currently work correctly only for synchronous code. Switching them to use the proposed API will keep their behavior for synchronous code unmodified, but will automatically enable support for asynchronous code.


Python docs: https://docs.python.org/3/library/contextvars.html
PEP 567 ""Converting code that uses threading.local()"": https://www.python.org/dev/peps/pep-0567/#converting-code-that-uses-threading-local",True,{}
explosion/thinc,https://github.com/explosion/thinc,263,2020-01-24T09:30:16Z,2020-01-24T12:33:34Z,2020-01-24T16:07:00Z,MERGED,True,54,69,7,https://github.com/tiangolo,Replace thread locals with contextvars,5,"['enhancement', 'third-party']",https://github.com/explosion/thinc/pull/263,https://github.com/apps/codecov,2,https://github.com/explosion/thinc/pull/263#issuecomment-578057223," Replace thread locals with contextvars.
This would allow using Thinc in scenarios that are not purely thread-based, like asyncio (e.g. in FastAPI apps). While still working the same for thread-based scenarios.
From PEP 567: https://www.python.org/dev/peps/pep-0567/#backwards-compatibility

This proposal preserves 100% backwards compatibility.
Libraries that use threading.local() to store context-related values, currently work correctly only for synchronous code. Switching them to use the proposed API will keep their behavior for synchronous code unmodified, but will automatically enable support for asynchronous code.


Python docs: https://docs.python.org/3/library/contextvars.html
PEP 567 ""Converting code that uses threading.local()"": https://www.python.org/dev/peps/pep-0567/#converting-code-that-uses-threading-local","Codecov Report

 No coverage uploaded for pull request base (develop@5248c11). Click here to learn what that means.
The diff coverage is 100%.


@@            Coverage Diff             @@
##             develop     #263   +/-   ##
==========================================
  Coverage           ?   87.78%           
==========================================
  Files              ?       63           
  Lines              ?     4051           
  Branches           ?        0           
==========================================
  Hits               ?     3556           
  Misses             ?      495           
  Partials           ?        0



Impacted Files
Coverage 





thinc/util.py
97.14% <100%> ()



thinc/model.py
99.75% <100%> ()



thinc/backends/__init__.py
100% <100%> ()





Continue to review full report at Codecov.

Legend - Click here to learn more
 = absolute <relative> (impact),  = not affected, ? = missing data
Powered by Codecov. Last update 5248c11...1bfe8c1. Read the comment docs.",True,{}
explosion/thinc,https://github.com/explosion/thinc,264,2020-01-24T14:24:47Z,2020-01-24T15:38:36Z,2020-01-24T15:44:46Z,MERGED,True,24,11,1,https://github.com/svlandeg,Fixes for notebook0,4,['examples'],https://github.com/explosion/thinc/pull/264,https://github.com/svlandeg,1,https://github.com/explosion/thinc/pull/264,"Done

Added lower bounds for some packages to prevent errors
Few more print statements that gives the user more of a feel of ""something is happening when I execute this cell""
Fixing GPU execution with model.ops.asarray calls.

TODO

The TF wrapper currently does not converge well
The script doesn't actually run yet on GPU, I think because of a bug in thinc that I'm isolating in a different Issue/PR later - cf PR #265","Done

Added lower bounds for some packages to prevent errors
Few more print statements that gives the user more of a feel of ""something is happening when I execute this cell""
Fixing GPU execution with model.ops.asarray calls.

TODO

The TF wrapper currently does not converge well
The script doesn't actually run yet on GPU, I think because of a bug in thinc that I'm isolating in a different Issue/PR later - cf PR #265",True,{}
explosion/thinc,https://github.com/explosion/thinc,264,2020-01-24T14:24:47Z,2020-01-24T15:38:36Z,2020-01-24T15:44:46Z,MERGED,True,24,11,1,https://github.com/svlandeg,Fixes for notebook0,4,['examples'],https://github.com/explosion/thinc/pull/264,https://github.com/apps/codecov,2,https://github.com/explosion/thinc/pull/264#issuecomment-578152894,"Done

Added lower bounds for some packages to prevent errors
Few more print statements that gives the user more of a feel of ""something is happening when I execute this cell""
Fixing GPU execution with model.ops.asarray calls.

TODO

The TF wrapper currently does not converge well
The script doesn't actually run yet on GPU, I think because of a bug in thinc that I'm isolating in a different Issue/PR later - cf PR #265","Codecov Report

 No coverage uploaded for pull request base (develop@5248c11). Click here to learn what that means.
The diff coverage is n/a.


@@            Coverage Diff             @@
##             develop     #264   +/-   ##
==========================================
  Coverage           ?   87.78%           
==========================================
  Files              ?       63           
  Lines              ?     4051           
  Branches           ?        0           
==========================================
  Hits               ?     3556           
  Misses             ?      495           
  Partials           ?        0

Continue to review full report at Codecov.

Legend - Click here to learn more
 = absolute <relative> (impact),  = not affected, ? = missing data
Powered by Codecov. Last update 5248c11...620ad9f. Read the comment docs.",True,{}
explosion/thinc,https://github.com/explosion/thinc,265,2020-01-24T14:31:17Z,2020-01-24T15:44:22Z,2020-01-24T15:44:29Z,MERGED,True,71,14,5,https://github.com/svlandeg,Running MNIST example with GPU,11,"['bug', 'feat / ops']",https://github.com/explosion/thinc/pull/265,https://github.com/svlandeg,1,https://github.com/explosion/thinc/pull/265,"I was trying to run the MNIST example from the 0'th notebook and ran into trouble on the GPU. I added a unit test that has prefer_gpu() - I'm not sure we want that, but at least for now it helps us diagnose things. I fixed a few errors trying to run this, until I got completely stuck at:
python -m pytest model\test_model.py
model\test_model.py:385:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
..\model.py:323: in finish_update
    param, grad = optimizer((node.id, name), param, grad)
..\optimizers.py:248: in __call__
    weights, gradient = self._adam(xp, weights, gradient, lr_scale, key, nr_upd)
..\optimizers.py:351: in _adam
    weights_1D, gradient_1D, mom1, mom2, b1, b2, eps, lr * lr_scale
..\backends\cupy_ops.py:145: in adam
    )(gradient, learn_rate, 1 - beta1, 1 - beta2, eps, weights, mom1, mom2)
cupy\core\_kernel.pyx:591: in cupy.core._kernel.ElementwiseKernel.__call__
    ???
cupy\core\_kernel.pyx:371: in cupy.core._kernel._broadcast
    ???
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

>   ???
E   ValueError: operands could not be broadcast together with shapes (10, 320), (), (), (), (), (10, 320), (10, 32), (10, 32)

cupy\core\_routines_manipulation.pyx:73: ValueError

@honnibal @ines Any pointers on where to look to address this ? cupy_ops.adam looks scary.","I was trying to run the MNIST example from the 0'th notebook and ran into trouble on the GPU. I added a unit test that has prefer_gpu() - I'm not sure we want that, but at least for now it helps us diagnose things. I fixed a few errors trying to run this, until I got completely stuck at:
python -m pytest model\test_model.py
model\test_model.py:385:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
..\model.py:323: in finish_update
    param, grad = optimizer((node.id, name), param, grad)
..\optimizers.py:248: in __call__
    weights, gradient = self._adam(xp, weights, gradient, lr_scale, key, nr_upd)
..\optimizers.py:351: in _adam
    weights_1D, gradient_1D, mom1, mom2, b1, b2, eps, lr * lr_scale
..\backends\cupy_ops.py:145: in adam
    )(gradient, learn_rate, 1 - beta1, 1 - beta2, eps, weights, mom1, mom2)
cupy\core\_kernel.pyx:591: in cupy.core._kernel.ElementwiseKernel.__call__
    ???
cupy\core\_kernel.pyx:371: in cupy.core._kernel._broadcast
    ???
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

>   ???
E   ValueError: operands could not be broadcast together with shapes (10, 320), (), (), (), (), (10, 320), (10, 32), (10, 32)

cupy\core\_routines_manipulation.pyx:73: ValueError

@honnibal @ines Any pointers on where to look to address this ? cupy_ops.adam looks scary.",True,{}
explosion/thinc,https://github.com/explosion/thinc,265,2020-01-24T14:31:17Z,2020-01-24T15:44:22Z,2020-01-24T15:44:29Z,MERGED,True,71,14,5,https://github.com/svlandeg,Running MNIST example with GPU,11,"['bug', 'feat / ops']",https://github.com/explosion/thinc/pull/265,https://github.com/apps/codecov,2,https://github.com/explosion/thinc/pull/265#issuecomment-578156082,"I was trying to run the MNIST example from the 0'th notebook and ran into trouble on the GPU. I added a unit test that has prefer_gpu() - I'm not sure we want that, but at least for now it helps us diagnose things. I fixed a few errors trying to run this, until I got completely stuck at:
python -m pytest model\test_model.py
model\test_model.py:385:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
..\model.py:323: in finish_update
    param, grad = optimizer((node.id, name), param, grad)
..\optimizers.py:248: in __call__
    weights, gradient = self._adam(xp, weights, gradient, lr_scale, key, nr_upd)
..\optimizers.py:351: in _adam
    weights_1D, gradient_1D, mom1, mom2, b1, b2, eps, lr * lr_scale
..\backends\cupy_ops.py:145: in adam
    )(gradient, learn_rate, 1 - beta1, 1 - beta2, eps, weights, mom1, mom2)
cupy\core\_kernel.pyx:591: in cupy.core._kernel.ElementwiseKernel.__call__
    ???
cupy\core\_kernel.pyx:371: in cupy.core._kernel._broadcast
    ???
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

>   ???
E   ValueError: operands could not be broadcast together with shapes (10, 320), (), (), (), (), (10, 320), (10, 32), (10, 32)

cupy\core\_routines_manipulation.pyx:73: ValueError

@honnibal @ines Any pointers on where to look to address this ? cupy_ops.adam looks scary.","Codecov Report

 No coverage uploaded for pull request base (develop@5248c11). Click here to learn what that means.
The diff coverage is 100%.


@@            Coverage Diff             @@
##             develop     #265   +/-   ##
==========================================
  Coverage           ?   87.76%           
==========================================
  Files              ?       63           
  Lines              ?     4045           
  Branches           ?        0           
==========================================
  Hits               ?     3550           
  Misses             ?      495           
  Partials           ?        0



Impacted Files
Coverage 





thinc/optimizers.py
94.23% <100%> ()



thinc/backends/ops.py
82.21% <100%> ()





Continue to review full report at Codecov.

Legend - Click here to learn more
 = absolute <relative> (impact),  = not affected, ? = missing data
Powered by Codecov. Last update 5248c11...2f9e8c3. Read the comment docs.",True,{}
explosion/thinc,https://github.com/explosion/thinc,265,2020-01-24T14:31:17Z,2020-01-24T15:44:22Z,2020-01-24T15:44:29Z,MERGED,True,71,14,5,https://github.com/svlandeg,Running MNIST example with GPU,11,"['bug', 'feat / ops']",https://github.com/explosion/thinc/pull/265,https://github.com/honnibal,3,https://github.com/explosion/thinc/pull/265#issuecomment-578165112,"I was trying to run the MNIST example from the 0'th notebook and ran into trouble on the GPU. I added a unit test that has prefer_gpu() - I'm not sure we want that, but at least for now it helps us diagnose things. I fixed a few errors trying to run this, until I got completely stuck at:
python -m pytest model\test_model.py
model\test_model.py:385:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
..\model.py:323: in finish_update
    param, grad = optimizer((node.id, name), param, grad)
..\optimizers.py:248: in __call__
    weights, gradient = self._adam(xp, weights, gradient, lr_scale, key, nr_upd)
..\optimizers.py:351: in _adam
    weights_1D, gradient_1D, mom1, mom2, b1, b2, eps, lr * lr_scale
..\backends\cupy_ops.py:145: in adam
    )(gradient, learn_rate, 1 - beta1, 1 - beta2, eps, weights, mom1, mom2)
cupy\core\_kernel.pyx:591: in cupy.core._kernel.ElementwiseKernel.__call__
    ???
cupy\core\_kernel.pyx:371: in cupy.core._kernel._broadcast
    ???
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

>   ???
E   ValueError: operands could not be broadcast together with shapes (10, 320), (), (), (), (), (10, 320), (10, 32), (10, 32)

cupy\core\_routines_manipulation.pyx:73: ValueError

@honnibal @ines Any pointers on where to look to address this ? cupy_ops.adam looks scary.",I think the error's probably associated with the array shaping. I'll take a look,True,{}
explosion/thinc,https://github.com/explosion/thinc,265,2020-01-24T14:31:17Z,2020-01-24T15:44:22Z,2020-01-24T15:44:29Z,MERGED,True,71,14,5,https://github.com/svlandeg,Running MNIST example with GPU,11,"['bug', 'feat / ops']",https://github.com/explosion/thinc/pull/265,https://github.com/honnibal,4,https://github.com/explosion/thinc/pull/265#issuecomment-578166487,"I was trying to run the MNIST example from the 0'th notebook and ran into trouble on the GPU. I added a unit test that has prefer_gpu() - I'm not sure we want that, but at least for now it helps us diagnose things. I fixed a few errors trying to run this, until I got completely stuck at:
python -m pytest model\test_model.py
model\test_model.py:385:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
..\model.py:323: in finish_update
    param, grad = optimizer((node.id, name), param, grad)
..\optimizers.py:248: in __call__
    weights, gradient = self._adam(xp, weights, gradient, lr_scale, key, nr_upd)
..\optimizers.py:351: in _adam
    weights_1D, gradient_1D, mom1, mom2, b1, b2, eps, lr * lr_scale
..\backends\cupy_ops.py:145: in adam
    )(gradient, learn_rate, 1 - beta1, 1 - beta2, eps, weights, mom1, mom2)
cupy\core\_kernel.pyx:591: in cupy.core._kernel.ElementwiseKernel.__call__
    ???
cupy\core\_kernel.pyx:371: in cupy.core._kernel._broadcast
    ???
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

>   ???
E   ValueError: operands could not be broadcast together with shapes (10, 320), (), (), (), (), (10, 320), (10, 32), (10, 32)

cupy\core\_routines_manipulation.pyx:73: ValueError

@honnibal @ines Any pointers on where to look to address this ? cupy_ops.adam looks scary.","Yes in Optimizer we're setting mom1 and mom2 to the original weights shape.
The problem here is that we actually want to avoid this reshaping if possible, and do it a bit later. We should change it back to not reshaping in the optimizer.py, and instead only do it within the NumpyOps.adam method.
The JaxOps backend needs to make a copy to reshape, so I was trying to avoid unnecessary reshapings. It's also a bit of an unintuitive way to structure the code, as it doesn't make that much sense in itself.",True,{'EYES': ['https://github.com/svlandeg']}
explosion/thinc,https://github.com/explosion/thinc,265,2020-01-24T14:31:17Z,2020-01-24T15:44:22Z,2020-01-24T15:44:29Z,MERGED,True,71,14,5,https://github.com/svlandeg,Running MNIST example with GPU,11,"['bug', 'feat / ops']",https://github.com/explosion/thinc/pull/265,https://github.com/svlandeg,5,https://github.com/explosion/thinc/pull/265#issuecomment-578175301,"I was trying to run the MNIST example from the 0'th notebook and ran into trouble on the GPU. I added a unit test that has prefer_gpu() - I'm not sure we want that, but at least for now it helps us diagnose things. I fixed a few errors trying to run this, until I got completely stuck at:
python -m pytest model\test_model.py
model\test_model.py:385:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
..\model.py:323: in finish_update
    param, grad = optimizer((node.id, name), param, grad)
..\optimizers.py:248: in __call__
    weights, gradient = self._adam(xp, weights, gradient, lr_scale, key, nr_upd)
..\optimizers.py:351: in _adam
    weights_1D, gradient_1D, mom1, mom2, b1, b2, eps, lr * lr_scale
..\backends\cupy_ops.py:145: in adam
    )(gradient, learn_rate, 1 - beta1, 1 - beta2, eps, weights, mom1, mom2)
cupy\core\_kernel.pyx:591: in cupy.core._kernel.ElementwiseKernel.__call__
    ???
cupy\core\_kernel.pyx:371: in cupy.core._kernel._broadcast
    ???
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

>   ???
E   ValueError: operands could not be broadcast together with shapes (10, 320), (), (), (), (), (10, 320), (10, 32), (10, 32)

cupy\core\_routines_manipulation.pyx:73: ValueError

@honnibal @ines Any pointers on where to look to address this ? cupy_ops.adam looks scary.",UPDATE: also ported the fix from #149 so that I can run the unit tests locally on GPU,True,{}
explosion/thinc,https://github.com/explosion/thinc,266,2020-01-24T15:36:50Z,2020-01-24T19:04:40Z,2020-01-24T19:29:42Z,MERGED,True,69,41,9,https://github.com/honnibal,Support dropout in embedding layers,11,"['enhancement', 'feat / layers']",https://github.com/explosion/thinc/pull/266,https://github.com/honnibal,1,https://github.com/explosion/thinc/pull/266,"Add dropout_rate attr to HashEmbed, Embed and StaticEmbed layers. Dropout is applied by creating one dropout mask vector for the whole batch, and applying it.


Change Dropout layer to use the attr name dropout_rate, rather than rate.


Change the set_dropout_rate helper to just look for the attr name, rather than the nodes.


Remove traces of spaCy from the StaticVectors table. Instead expect vectors as an array, and wrap it in an arbitrary object to prevent msgpack serialization.","Add dropout_rate attr to HashEmbed, Embed and StaticEmbed layers. Dropout is applied by creating one dropout mask vector for the whole batch, and applying it.


Change Dropout layer to use the attr name dropout_rate, rather than rate.


Change the set_dropout_rate helper to just look for the attr name, rather than the nodes.


Remove traces of spaCy from the StaticVectors table. Instead expect vectors as an array, and wrap it in an arbitrary object to prevent msgpack serialization.",True,{}
explosion/thinc,https://github.com/explosion/thinc,266,2020-01-24T15:36:50Z,2020-01-24T19:04:40Z,2020-01-24T19:29:42Z,MERGED,True,69,41,9,https://github.com/honnibal,Support dropout in embedding layers,11,"['enhancement', 'feat / layers']",https://github.com/explosion/thinc/pull/266,https://github.com/apps/codecov,2,https://github.com/explosion/thinc/pull/266#issuecomment-578203170,"Add dropout_rate attr to HashEmbed, Embed and StaticEmbed layers. Dropout is applied by creating one dropout mask vector for the whole batch, and applying it.


Change Dropout layer to use the attr name dropout_rate, rather than rate.


Change the set_dropout_rate helper to just look for the attr name, rather than the nodes.


Remove traces of spaCy from the StaticVectors table. Instead expect vectors as an array, and wrap it in an arbitrary object to prevent msgpack serialization.","Codecov Report

 No coverage uploaded for pull request base (develop@d11968f). Click here to learn what that means.
The diff coverage is 90.9%.


@@            Coverage Diff             @@
##             develop     #266   +/-   ##
==========================================
  Coverage           ?   87.79%           
==========================================
  Files              ?       64           
  Lines              ?     4087           
  Branches           ?        0           
==========================================
  Hits               ?     3588           
  Misses             ?      499           
  Partials           ?        0



Impacted Files
Coverage 





thinc/shims/tensorflow.py
92% <> ()



thinc/layers/embed.py
95.23% <> ()



thinc/layers/__init__.py
100% <> ()



thinc/optimizers.py
94.23% <100%> ()



thinc/layers/hashembed.py
95.74% <100%> ()



thinc/backends/ops.py
82.21% <100%> ()



thinc/shims/pytorch.py
83.9% <50%> ()





Continue to review full report at Codecov.

Legend - Click here to learn more
 = absolute <relative> (impact),  = not affected, ? = missing data
Powered by Codecov. Last update d11968f...712a1d9. Read the comment docs.",True,{}
explosion/thinc,https://github.com/explosion/thinc,267,2020-01-24T18:06:10Z,2020-01-24T18:48:46Z,2020-01-24T19:29:43Z,MERGED,True,65,0,4,https://github.com/ines,Add with_debug layer,3,"['enhancement', 'feat / layers']",https://github.com/explosion/thinc/pull/267,https://github.com/ines,1,https://github.com/explosion/thinc/pull/267,"Wraps any layer and can be called with 3 optional callbacks: on_init, on_forward and on_backprop, which receive the same arguments as the functions they're called in. Can be used to log stuff, inspect inputs/outputs, benchmark etc.","Wraps any layer and can be called with 3 optional callbacks: on_init, on_forward and on_backprop, which receive the same arguments as the functions they're called in. Can be used to log stuff, inspect inputs/outputs, benchmark etc.",True,{'LAUGH': ['https://github.com/svlandeg']}
explosion/thinc,https://github.com/explosion/thinc,267,2020-01-24T18:06:10Z,2020-01-24T18:48:46Z,2020-01-24T19:29:43Z,MERGED,True,65,0,4,https://github.com/ines,Add with_debug layer,3,"['enhancement', 'feat / layers']",https://github.com/explosion/thinc/pull/267,https://github.com/svlandeg,2,https://github.com/explosion/thinc/pull/267#issuecomment-578246881,"Wraps any layer and can be called with 3 optional callbacks: on_init, on_forward and on_backprop, which receive the same arguments as the functions they're called in. Can be used to log stuff, inspect inputs/outputs, benchmark etc.","Ha, awesome!",True,{'LAUGH': ['https://github.com/ines']}
explosion/thinc,https://github.com/explosion/thinc,267,2020-01-24T18:06:10Z,2020-01-24T18:48:46Z,2020-01-24T19:29:43Z,MERGED,True,65,0,4,https://github.com/ines,Add with_debug layer,3,"['enhancement', 'feat / layers']",https://github.com/explosion/thinc/pull/267,https://github.com/apps/codecov,3,https://github.com/explosion/thinc/pull/267#issuecomment-578250429,"Wraps any layer and can be called with 3 optional callbacks: on_init, on_forward and on_backprop, which receive the same arguments as the functions they're called in. Can be used to log stuff, inspect inputs/outputs, benchmark etc.","Codecov Report

 No coverage uploaded for pull request base (develop@499fc87). Click here to learn what that means.
The diff coverage is 100%.


@@            Coverage Diff             @@
##             develop     #267   +/-   ##
==========================================
  Coverage           ?   88.14%           
==========================================
  Files              ?       64           
  Lines              ?     4243           
  Branches           ?        0           
==========================================
  Hits               ?     3740           
  Misses             ?      503           
  Partials           ?        0



Impacted Files
Coverage 





thinc/api.py
100% <100%> ()



thinc/layers/__init__.py
100% <100%> ()



thinc/layers/with_debug.py
100% <100%> ()





Continue to review full report at Codecov.

Legend - Click here to learn more
 = absolute <relative> (impact),  = not affected, ? = missing data
Powered by Codecov. Last update 499fc87...89701f0. Read the comment docs.",True,{}
explosion/thinc,https://github.com/explosion/thinc,268,2020-01-24T18:43:29Z,2020-01-25T02:47:00Z,2020-03-25T10:39:02Z,MERGED,True,4,3,2,https://github.com/svlandeg,Fixing unit tests with GPU,2,"['bug', 'tests']",https://github.com/explosion/thinc/pull/268,https://github.com/svlandeg,1,https://github.com/explosion/thinc/pull/268,"Two small fixes, ensuring all tests run on GPU :-)
Though test_backprop_reduce_sum feels a little flakey. I think perhaps raising the tolerance level helped, as I haven't been able to make it fail anymore, but let's see in the future.","Two small fixes, ensuring all tests run on GPU :-)
Though test_backprop_reduce_sum feels a little flakey. I think perhaps raising the tolerance level helped, as I haven't been able to make it fail anymore, but let's see in the future.",True,{}
explosion/thinc,https://github.com/explosion/thinc,268,2020-01-24T18:43:29Z,2020-01-25T02:47:00Z,2020-03-25T10:39:02Z,MERGED,True,4,3,2,https://github.com/svlandeg,Fixing unit tests with GPU,2,"['bug', 'tests']",https://github.com/explosion/thinc/pull/268,https://github.com/apps/codecov,2,https://github.com/explosion/thinc/pull/268#issuecomment-578255547,"Two small fixes, ensuring all tests run on GPU :-)
Though test_backprop_reduce_sum feels a little flakey. I think perhaps raising the tolerance level helped, as I haven't been able to make it fail anymore, but let's see in the future.","Codecov Report

Merging #268 into develop will increase coverage by 0.06%.
The diff coverage is n/a.


@@             Coverage Diff             @@
##           develop     #268      +/-   ##
===========================================
+ Coverage    87.76%   87.82%   +0.06%     
===========================================
  Files           63       64       +1     
  Lines         4045     4066      +21     
===========================================
+ Hits          3550     3571      +21     
  Misses         495      495



Impacted Files
Coverage 





thinc/initializers.py
100% <0%> ()



thinc/backends/ops.py
82.21% <0%> ()



thinc/api.py
100% <0%> ()



thinc/layers/__init__.py
100% <0%> ()



thinc/layers/with_debug.py
100% <0%> ()





Continue to review full report at Codecov.

Legend - Click here to learn more
 = absolute <relative> (impact),  = not affected, ? = missing data
Powered by Codecov. Last update b6a27e6...d66ef90. Read the comment docs.",True,{}
explosion/thinc,https://github.com/explosion/thinc,269,2020-01-24T19:29:26Z,2020-01-25T02:46:20Z,2020-01-26T14:37:20Z,MERGED,True,75,105,25,https://github.com/ines,Simplify Model attrs,1,['enhancement'],https://github.com/explosion/thinc/pull/269,https://github.com/ines,1,https://github.com/explosion/thinc/pull/269,"As discussed:

make Model.attrs a dict and a property to allow assigning to it directly
remove Model.get_attr, Model.set_attr, Model.has_attr, Model.attr_names","As discussed:

make Model.attrs a dict and a property to allow assigning to it directly
remove Model.get_attr, Model.set_attr, Model.has_attr, Model.attr_names",True,{}
explosion/thinc,https://github.com/explosion/thinc,269,2020-01-24T19:29:26Z,2020-01-25T02:46:20Z,2020-01-26T14:37:20Z,MERGED,True,75,105,25,https://github.com/ines,Simplify Model attrs,1,['enhancement'],https://github.com/explosion/thinc/pull/269,https://github.com/apps/codecov,2,https://github.com/explosion/thinc/pull/269#issuecomment-578272275,"As discussed:

make Model.attrs a dict and a property to allow assigning to it directly
remove Model.get_attr, Model.set_attr, Model.has_attr, Model.attr_names","Codecov Report

Merging #269 into develop will increase coverage by 0.01%.
The diff coverage is 100%.


@@             Coverage Diff             @@
##           develop     #269      +/-   ##
===========================================
+ Coverage    87.78%   87.79%   +0.01%     
===========================================
  Files           64       64              
  Lines         4084     4071      -13     
===========================================
- Hits          3585     3574      -11     
+ Misses         499      497       -2



Impacted Files
Coverage 





thinc/layers/remap_ids.py
100% <100%> ()



thinc/layers/pytorchwrapper.py
100% <100%> ()



thinc/layers/dropout.py
100% <100%> ()



thinc/layers/featureextractor.py
100% <100%> ()



thinc/layers/tensorflowwrapper.py
97.8% <100%> ()



thinc/layers/expand_window.py
100% <100%> ()



thinc/layers/uniqued.py
100% <100%> ()



thinc/layers/with_getitem.py
100% <100%> ()



thinc/layers/hashembed.py
97.77% <100%> (+2.03%)



thinc/layers/embed.py
97.5% <100%> (+2.26%)



... and 4 more






Continue to review full report at Codecov.

Legend - Click here to learn more
 = absolute <relative> (impact),  = not affected, ? = missing data
Powered by Codecov. Last update 23b9b1f...67da17e. Read the comment docs.",True,{}
explosion/thinc,https://github.com/explosion/thinc,270,2020-01-24T20:36:34Z,2020-01-25T02:48:05Z,2020-01-25T11:12:07Z,MERGED,True,44,9,3,https://github.com/tiangolo,Tweak Jupyter/Colab notebooks to fix Colab issues,2,[],https://github.com/explosion/thinc/pull/270,https://github.com/tiangolo,1,https://github.com/explosion/thinc/pull/270, Tweak Jupyter/Colab notebooks to fix Colab issues, Tweak Jupyter/Colab notebooks to fix Colab issues,True,{}
explosion/thinc,https://github.com/explosion/thinc,270,2020-01-24T20:36:34Z,2020-01-25T02:48:05Z,2020-01-25T11:12:07Z,MERGED,True,44,9,3,https://github.com/tiangolo,Tweak Jupyter/Colab notebooks to fix Colab issues,2,[],https://github.com/explosion/thinc/pull/270,https://github.com/apps/codecov,2,https://github.com/explosion/thinc/pull/270#issuecomment-578293687, Tweak Jupyter/Colab notebooks to fix Colab issues,"Codecov Report

Merging #270 into develop will not change coverage.
The diff coverage is n/a.


@@           Coverage Diff            @@
##           develop     #270   +/-   ##
========================================
  Coverage    87.78%   87.78%           
========================================
  Files           64       64           
  Lines         4084     4084           
========================================
  Hits          3585     3585           
  Misses         499      499

Continue to review full report at Codecov.

Legend - Click here to learn more
 = absolute <relative> (impact),  = not affected, ? = missing data
Powered by Codecov. Last update 23b9b1f...e4f542a. Read the comment docs.",True,{}
explosion/thinc,https://github.com/explosion/thinc,271,2020-01-24T20:40:47Z,2020-01-25T02:45:23Z,2020-01-25T11:12:15Z,MERGED,True,0,7,1,https://github.com/tiangolo,"Remove model's ThreadState class, no longer used",1,[],https://github.com/explosion/thinc/pull/271,https://github.com/tiangolo,1,https://github.com/explosion/thinc/pull/271," Remove model's ThreadState class, as it's replaced with context vars and is no longer used."," Remove model's ThreadState class, as it's replaced with context vars and is no longer used.",True,{'HOORAY': ['https://github.com/justindujardin']}
explosion/thinc,https://github.com/explosion/thinc,271,2020-01-24T20:40:47Z,2020-01-25T02:45:23Z,2020-01-25T11:12:15Z,MERGED,True,0,7,1,https://github.com/tiangolo,"Remove model's ThreadState class, no longer used",1,[],https://github.com/explosion/thinc/pull/271,https://github.com/apps/codecov,2,https://github.com/explosion/thinc/pull/271#issuecomment-578294680," Remove model's ThreadState class, as it's replaced with context vars and is no longer used.","Codecov Report

Merging #271 into develop will increase coverage by 0.01%.
The diff coverage is n/a.


@@             Coverage Diff             @@
##           develop     #271      +/-   ##
===========================================
+ Coverage    87.78%   87.79%   +0.01%     
===========================================
  Files           64       64              
  Lines         4084     4080       -4     
===========================================
- Hits          3585     3582       -3     
+ Misses         499      498       -1



Impacted Files
Coverage 





thinc/model.py
100% <> (+0.24%)





Continue to review full report at Codecov.

Legend - Click here to learn more
 = absolute <relative> (impact),  = not affected, ? = missing data
Powered by Codecov. Last update 23b9b1f...d1fc296. Read the comment docs.",True,{}
explosion/thinc,https://github.com/explosion/thinc,272,2020-01-26T17:17:18Z,2020-01-26T18:42:11Z,2020-01-26T18:42:13Z,MERGED,True,122,122,56,https://github.com/ines,Rename registry names: v0 -> v1,2,['feat / config'],https://github.com/explosion/thinc/pull/272,https://github.com/ines,1,https://github.com/explosion/thinc/pull/272,,,True,{}
explosion/thinc,https://github.com/explosion/thinc,272,2020-01-26T17:17:18Z,2020-01-26T18:42:11Z,2020-01-26T18:42:13Z,MERGED,True,122,122,56,https://github.com/ines,Rename registry names: v0 -> v1,2,['feat / config'],https://github.com/explosion/thinc/pull/272,https://github.com/apps/codecov,2,https://github.com/explosion/thinc/pull/272#issuecomment-578523049,,"Codecov Report

Merging #272 into develop will not change coverage.
The diff coverage is 75%.


@@           Coverage Diff            @@
##           develop     #272   +/-   ##
========================================
  Coverage    87.81%   87.81%           
========================================
  Files           64       64           
  Lines         4069     4069           
========================================
  Hits          3573     3573           
  Misses         496      496



Impacted Files
Coverage 





thinc/backends/jax_ops.py
47.15% <0%> ()



thinc/optimizers.py
94.23% <100%> ()



thinc/layers/hashembed.py
97.77% <100%> ()



thinc/layers/parametricattention.py
100% <100%> ()



thinc/layers/embed.py
97.5% <100%> ()



thinc/layers/layernorm.py
100% <100%> ()



thinc/layers/multisoftmax.py
100% <100%> ()



thinc/layers/cauchysimilarity.py
100% <100%> ()



thinc/backends/ops.py
82.21% <71.42%> ()





Continue to review full report at Codecov.

Legend - Click here to learn more
 = absolute <relative> (impact),  = not affected, ? = missing data
Powered by Codecov. Last update 9c9c473...d228657. Read the comment docs.",True,{}
explosion/thinc,https://github.com/explosion/thinc,273,2020-01-26T17:27:51Z,2020-01-26T17:58:09Z,2020-11-19T16:21:45Z,MERGED,True,71,71,24,https://github.com/ines,Rename alloc methods,1,['feat / ops'],https://github.com/explosion/thinc/pull/273,https://github.com/ines,1,https://github.com/explosion/thinc/pull/273,,,True,{}
explosion/thinc,https://github.com/explosion/thinc,273,2020-01-26T17:27:51Z,2020-01-26T17:58:09Z,2020-11-19T16:21:45Z,MERGED,True,71,71,24,https://github.com/ines,Rename alloc methods,1,['feat / ops'],https://github.com/explosion/thinc/pull/273,https://github.com/apps/codecov,2,https://github.com/explosion/thinc/pull/273#issuecomment-578524242,,"Codecov Report

Merging #273 into develop will not change coverage.
The diff coverage is 75%.


@@           Coverage Diff            @@
##           develop     #273   +/-   ##
========================================
  Coverage    87.81%   87.81%           
========================================
  Files           64       64           
  Lines         4069     4069           
========================================
  Hits          3573     3573           
  Misses         496      496



Impacted Files
Coverage 





thinc/backends/jax_ops.py
47.15% <0%> ()



thinc/optimizers.py
94.23% <100%> ()



thinc/layers/hashembed.py
97.77% <100%> ()



thinc/layers/parametricattention.py
100% <100%> ()



thinc/layers/embed.py
97.5% <100%> ()



thinc/layers/layernorm.py
100% <100%> ()



thinc/layers/multisoftmax.py
100% <100%> ()



thinc/layers/cauchysimilarity.py
100% <100%> ()



thinc/backends/ops.py
82.21% <71.42%> ()





Continue to review full report at Codecov.

Legend - Click here to learn more
 = absolute <relative> (impact),  = not affected, ? = missing data
Powered by Codecov. Last update e8bb173...d939fc4. Read the comment docs.",True,{}
explosion/thinc,https://github.com/explosion/thinc,274,2020-01-26T18:43:40Z,2020-01-27T20:23:18Z,2020-11-19T16:21:51Z,MERGED,True,1436,1148,68,https://github.com/ines,Fine-grained array types and ops methods,38,"['enhancement', 'feat / ops']",https://github.com/explosion/thinc/pull/274,https://github.com/ines,1,https://github.com/explosion/thinc/pull/274,,,True,{'HOORAY': ['https://github.com/justindujardin']}
explosion/thinc,https://github.com/explosion/thinc,274,2020-01-26T18:43:40Z,2020-01-27T20:23:18Z,2020-11-19T16:21:51Z,MERGED,True,1436,1148,68,https://github.com/ines,Fine-grained array types and ops methods,38,"['enhancement', 'feat / ops']",https://github.com/explosion/thinc/pull/274,https://github.com/apps/codecov,2,https://github.com/explosion/thinc/pull/274#issuecomment-578530806,,"Codecov Report

Merging #274 into develop will decrease coverage by <.01%.
The diff coverage is 100%.


@@             Coverage Diff             @@
##           develop     #274      +/-   ##
===========================================
- Coverage    87.64%   87.63%   -0.01%     
===========================================
  Files           64       64              
  Lines         4145     4142       -3     
===========================================
- Hits          3633     3630       -3     
  Misses         512      512



Impacted Files
Coverage 





thinc/optimizers.py
94.19% <> (-0.04%)



thinc/model.py
100% <> ()



thinc/config.py
100% <> ()



thinc/layers/chain.py
98.14% <> ()



thinc/about.py
100% <100%> ()



thinc/layers/dropout.py
100% <100%> ()





Continue to review full report at Codecov.

Legend - Click here to learn more
 = absolute <relative> (impact),  = not affected, ? = missing data
Powered by Codecov. Last update b59e2e3...76074d0. Read the comment docs.",True,{}
explosion/thinc,https://github.com/explosion/thinc,275,2020-01-26T19:59:20Z,2020-01-26T20:28:21Z,2020-01-26T20:28:26Z,MERGED,True,116,17,2,https://github.com/ines,WIP: Re-add fine-grained array types,1,"['enhancement', 'feat / types']",https://github.com/explosion/thinc/pull/275,https://github.com/ines,1,https://github.com/explosion/thinc/pull/275,,,True,{}
explosion/thinc,https://github.com/explosion/thinc,275,2020-01-26T19:59:20Z,2020-01-26T20:28:21Z,2020-01-26T20:28:26Z,MERGED,True,116,17,2,https://github.com/ines,WIP: Re-add fine-grained array types,1,"['enhancement', 'feat / types']",https://github.com/explosion/thinc/pull/275,https://github.com/apps/codecov,2,https://github.com/explosion/thinc/pull/275#issuecomment-578537547,,"Codecov Report

Merging #275 into develop will not change coverage.
The diff coverage is n/a.


@@           Coverage Diff            @@
##           develop     #275   +/-   ##
========================================
  Coverage    87.81%   87.81%           
========================================
  Files           64       64           
  Lines         4069     4069           
========================================
  Hits          3573     3573           
  Misses         496      496

Continue to review full report at Codecov.

Legend - Click here to learn more
 = absolute <relative> (impact),  = not affected, ? = missing data
Powered by Codecov. Last update c5e1e10...2bbf52d. Read the comment docs.",True,{}
explosion/thinc,https://github.com/explosion/thinc,276,2020-01-26T21:23:59Z,2020-01-27T02:10:41Z,2020-01-27T16:40:04Z,MERGED,True,293,212,1,https://github.com/justindujardin,Example: fix predict like terms dataset,1,['examples'],https://github.com/explosion/thinc/pull/276,https://github.com/justindujardin,1,https://github.com/explosion/thinc/pull/276,"I thought it was suspicious that it trained to 98% accuracy, and it turns out I was right! 
the dataset included a constant 10 noise terms in its expressions, and the model seemed to be learning to count terms and subtract 10. Maybe it only learned to count "" + ""?
update dataset to remove one-hot encoding after review with @honnibal
add model defaults that can train to ~80% accuracy if given 100 or more epochs.
add intermediate exercise to improve the model training performance by tweaking the architecture.

Related to: #258","I thought it was suspicious that it trained to 98% accuracy, and it turns out I was right! 
the dataset included a constant 10 noise terms in its expressions, and the model seemed to be learning to count terms and subtract 10. Maybe it only learned to count "" + ""?
update dataset to remove one-hot encoding after review with @honnibal
add model defaults that can train to ~80% accuracy if given 100 or more epochs.
add intermediate exercise to improve the model training performance by tweaking the architecture.

Related to: #258",True,{'THUMBS_UP': ['https://github.com/ines']}
explosion/thinc,https://github.com/explosion/thinc,276,2020-01-26T21:23:59Z,2020-01-27T02:10:41Z,2020-01-27T16:40:04Z,MERGED,True,293,212,1,https://github.com/justindujardin,Example: fix predict like terms dataset,1,['examples'],https://github.com/explosion/thinc/pull/276,https://github.com/apps/codecov,2,https://github.com/explosion/thinc/pull/276#issuecomment-578545180,"I thought it was suspicious that it trained to 98% accuracy, and it turns out I was right! 
the dataset included a constant 10 noise terms in its expressions, and the model seemed to be learning to count terms and subtract 10. Maybe it only learned to count "" + ""?
update dataset to remove one-hot encoding after review with @honnibal
add model defaults that can train to ~80% accuracy if given 100 or more epochs.
add intermediate exercise to improve the model training performance by tweaking the architecture.

Related to: #258","Codecov Report

Merging #276 into develop will not change coverage.
The diff coverage is n/a.


@@           Coverage Diff            @@
##           develop     #276   +/-   ##
========================================
  Coverage    87.81%   87.81%           
========================================
  Files           64       64           
  Lines         4069     4069           
========================================
  Hits          3573     3573           
  Misses         496      496

Continue to review full report at Codecov.

Legend - Click here to learn more
 = absolute <relative> (impact),  = not affected, ? = missing data
Powered by Codecov. Last update c5e1e10...de86dd9. Read the comment docs.",True,{}
explosion/thinc,https://github.com/explosion/thinc,277,2020-01-27T02:09:43Z,2020-01-27T03:37:46Z,2020-01-27T03:37:48Z,MERGED,True,6,20,6,https://github.com/ines,Update to Pydantic v1.4,2,"['enhancement', 'feat / config', 'third-party']",https://github.com/explosion/thinc/pull/277,https://github.com/ines,1,https://github.com/explosion/thinc/pull/277,We still need to leave in the Generator workarounds in the config to prevent consuming generators if they're part of a union with Iterator. But we get to remove the ugly hacks around the generics.,We still need to leave in the Generator workarounds in the config to prevent consuming generators if they're part of a union with Iterator. But we get to remove the ugly hacks around the generics.,True,{}
explosion/thinc,https://github.com/explosion/thinc,277,2020-01-27T02:09:43Z,2020-01-27T03:37:46Z,2020-01-27T03:37:48Z,MERGED,True,6,20,6,https://github.com/ines,Update to Pydantic v1.4,2,"['enhancement', 'feat / config', 'third-party']",https://github.com/explosion/thinc/pull/277,https://github.com/apps/codecov,2,https://github.com/explosion/thinc/pull/277#issuecomment-578579688,We still need to leave in the Generator workarounds in the config to prevent consuming generators if they're part of a union with Iterator. But we get to remove the ugly hacks around the generics.,"Codecov Report

Merging #277 into develop will decrease coverage by <.01%.
The diff coverage is 100%.


@@            Coverage Diff             @@
##           develop    #277      +/-   ##
==========================================
- Coverage    87.81%   87.8%   -0.01%     
==========================================
  Files           64      64              
  Lines         4069    4066       -3     
==========================================
- Hits          3573    3570       -3     
  Misses         496     496



Impacted Files
Coverage 





thinc/layers/chain.py
98.14% <> ()



thinc/config.py
100% <> ()



thinc/optimizers.py
94.19% <100%> (-0.04%)





Continue to review full report at Codecov.

Legend - Click here to learn more
 = absolute <relative> (impact),  = not affected, ? = missing data
Powered by Codecov. Last update 0994a6b...523e63f. Read the comment docs.",True,{}
explosion/thinc,https://github.com/explosion/thinc,278,2020-01-27T10:22:41Z,2020-01-27T11:17:02Z,2020-01-27T14:38:14Z,MERGED,True,3,4,3,https://github.com/svlandeg,Merge pull request #278 from svlandeg/fix/tests,2,['tests'],https://github.com/explosion/thinc/pull/278,https://github.com/svlandeg,1,https://github.com/explosion/thinc/pull/278,"Require MNIST example on TF to achieve at least 0.8 in 20 iterations
Fix typo in arrays_BI definition. This seems to have caused flakey behaviour in test_backprop_reduce_sum, which seems to be fixed now, so I lowered the threshold (let's see what the CI thinks)","Require MNIST example on TF to achieve at least 0.8 in 20 iterations
Fix typo in arrays_BI definition. This seems to have caused flakey behaviour in test_backprop_reduce_sum, which seems to be fixed now, so I lowered the threshold (let's see what the CI thinks)",True,{'THUMBS_UP': ['https://github.com/ines']}
explosion/thinc,https://github.com/explosion/thinc,278,2020-01-27T10:22:41Z,2020-01-27T11:17:02Z,2020-01-27T14:38:14Z,MERGED,True,3,4,3,https://github.com/svlandeg,Merge pull request #278 from svlandeg/fix/tests,2,['tests'],https://github.com/explosion/thinc/pull/278,https://github.com/apps/codecov,2,https://github.com/explosion/thinc/pull/278#issuecomment-578684273,"Require MNIST example on TF to achieve at least 0.8 in 20 iterations
Fix typo in arrays_BI definition. This seems to have caused flakey behaviour in test_backprop_reduce_sum, which seems to be fixed now, so I lowered the threshold (let's see what the CI thinks)","Codecov Report

Merging #278 into develop will not change coverage.
The diff coverage is n/a.


@@           Coverage Diff           @@
##           develop    #278   +/-   ##
=======================================
  Coverage     87.8%   87.8%           
=======================================
  Files           64      64           
  Lines         4066    4066           
=======================================
  Hits          3570    3570           
  Misses         496     496

Continue to review full report at Codecov.

Legend - Click here to learn more
 = absolute <relative> (impact),  = not affected, ? = missing data
Powered by Codecov. Last update 697a5b4...bc6ed4e. Read the comment docs.",True,{}
explosion/thinc,https://github.com/explosion/thinc,279,2020-01-27T16:00:01Z,2020-01-27T16:24:16Z,2020-01-27T16:24:33Z,MERGED,True,16,9,3,https://github.com/svlandeg,fix rename to dropout_rate,1,"['bug', 'feat / layers']",https://github.com/explosion/thinc/pull/279,https://github.com/svlandeg,1,https://github.com/explosion/thinc/pull/279,"PR #266 didn't actually change the attr name of the Dropout layer from rate to dropout_rate, which meant that set_dropout_rate didn't work anymore.","PR #266 didn't actually change the attr name of the Dropout layer from rate to dropout_rate, which meant that set_dropout_rate didn't work anymore.",True,{'THUMBS_UP': ['https://github.com/ines']}
explosion/thinc,https://github.com/explosion/thinc,279,2020-01-27T16:00:01Z,2020-01-27T16:24:16Z,2020-01-27T16:24:33Z,MERGED,True,16,9,3,https://github.com/svlandeg,fix rename to dropout_rate,1,"['bug', 'feat / layers']",https://github.com/explosion/thinc/pull/279,https://github.com/apps/codecov,2,https://github.com/explosion/thinc/pull/279#issuecomment-578819259,"PR #266 didn't actually change the attr name of the Dropout layer from rate to dropout_rate, which meant that set_dropout_rate didn't work anymore.","Codecov Report

Merging #279 into develop will not change coverage.
The diff coverage is 100%.


@@           Coverage Diff           @@
##           develop    #279   +/-   ##
=======================================
  Coverage     87.8%   87.8%           
=======================================
  Files           64      64           
  Lines         4066    4066           
=======================================
  Hits          3570    3570           
  Misses         496     496



Impacted Files
Coverage 





thinc/layers/dropout.py
100% <100%> ()





Continue to review full report at Codecov.

Legend - Click here to learn more
 = absolute <relative> (impact),  = not affected, ? = missing data
Powered by Codecov. Last update 0aefdee...c6c6b60. Read the comment docs.",True,{}
explosion/thinc,https://github.com/explosion/thinc,280,2020-01-27T16:37:42Z,2020-01-27T17:06:54Z,2020-01-27T17:07:54Z,MERGED,True,1,1,1,https://github.com/tiangolo,Update docstring for Model.inc_grad(),1,['docs'],https://github.com/explosion/thinc/pull/280,https://github.com/tiangolo,1,https://github.com/explosion/thinc/pull/280, Update docstring for Model.inc_grad(), Update docstring for Model.inc_grad(),True,{'THUMBS_UP': ['https://github.com/ines']}
explosion/thinc,https://github.com/explosion/thinc,280,2020-01-27T16:37:42Z,2020-01-27T17:06:54Z,2020-01-27T17:07:54Z,MERGED,True,1,1,1,https://github.com/tiangolo,Update docstring for Model.inc_grad(),1,['docs'],https://github.com/explosion/thinc/pull/280,https://github.com/apps/codecov,2,https://github.com/explosion/thinc/pull/280#issuecomment-578840479, Update docstring for Model.inc_grad(),"Codecov Report

Merging #280 into develop will not change coverage.
The diff coverage is n/a.


@@           Coverage Diff           @@
##           develop    #280   +/-   ##
=======================================
  Coverage     87.8%   87.8%           
=======================================
  Files           64      64           
  Lines         4066    4066           
=======================================
  Hits          3570    3570           
  Misses         496     496



Impacted Files
Coverage 





thinc/model.py
100% <> ()





Continue to review full report at Codecov.

Legend - Click here to learn more
 = absolute <relative> (impact),  = not affected, ? = missing data
Powered by Codecov. Last update f362d12...f3789ba. Read the comment docs.",True,{}
explosion/thinc,https://github.com/explosion/thinc,281,2020-01-27T20:08:03Z,2020-01-27T20:22:56Z,2020-01-27T20:34:09Z,MERGED,True,1,1,1,https://github.com/svlandeg,Remove type from fwd in FeatureExtractor,1,['feat / types'],https://github.com/explosion/thinc/pull/281,https://github.com/svlandeg,1,https://github.com/explosion/thinc/pull/281,"I'm not sure this is the ""fix"" we want, but the current docs type in FeatureExtractor.forward() does not match with the begin_training(X=...) arguments we're giving in spaCy, making the unit tests fail if we type it as List[Doc].
I don't think I know the historic trail here, and why we're also having this check of if hasattr(doc, ""to_array"") in this layer. Could be that the type should be as currently stated, and we need to refactor elsewhere? I'm not sure.","I'm not sure this is the ""fix"" we want, but the current docs type in FeatureExtractor.forward() does not match with the begin_training(X=...) arguments we're giving in spaCy, making the unit tests fail if we type it as List[Doc].
I don't think I know the historic trail here, and why we're also having this check of if hasattr(doc, ""to_array"") in this layer. Could be that the type should be as currently stated, and we need to refactor elsewhere? I'm not sure.",True,{}
explosion/thinc,https://github.com/explosion/thinc,281,2020-01-27T20:08:03Z,2020-01-27T20:22:56Z,2020-01-27T20:34:09Z,MERGED,True,1,1,1,https://github.com/svlandeg,Remove type from fwd in FeatureExtractor,1,['feat / types'],https://github.com/explosion/thinc/pull/281,https://github.com/honnibal,2,https://github.com/explosion/thinc/pull/281#issuecomment-578935109,"I'm not sure this is the ""fix"" we want, but the current docs type in FeatureExtractor.forward() does not match with the begin_training(X=...) arguments we're giving in spaCy, making the unit tests fail if we type it as List[Doc].
I don't think I know the historic trail here, and why we're also having this check of if hasattr(doc, ""to_array"") in this layer. Could be that the type should be as currently stated, and we need to refactor elsewhere? I'm not sure.","Hm, fair enough. What's the data type we're sending in? Is it a generator?
Anyway, sure, we can remove it for now.",True,{}
explosion/thinc,https://github.com/explosion/thinc,281,2020-01-27T20:08:03Z,2020-01-27T20:22:56Z,2020-01-27T20:34:09Z,MERGED,True,1,1,1,https://github.com/svlandeg,Remove type from fwd in FeatureExtractor,1,['feat / types'],https://github.com/explosion/thinc/pull/281,https://github.com/svlandeg,3,https://github.com/explosion/thinc/pull/281#issuecomment-578939679,"I'm not sure this is the ""fix"" we want, but the current docs type in FeatureExtractor.forward() does not match with the begin_training(X=...) arguments we're giving in spaCy, making the unit tests fail if we type it as List[Doc].
I don't think I know the historic trail here, and why we're also having this check of if hasattr(doc, ""to_array"") in this layer. Could be that the type should be as currently stated, and we need to refactor elsewhere? I'm not sure.","That's the weird part, we are actually passing a List of Doc's: https://github.com/explosion/spaCy/blob/feature/config/spacy/pipeline/pipes.pyx#L1373
docs = [Doc(Vocab(), words=[""hello""])]
self.model.initialize(X=docs)

The tests crash at validate_fwd_input_output called by Model.initialize():
    def validate_fwd_input_output(
        name: str, func: Callable[[Any, Any, bool], Any], X: Any, Y: Any
    ) -> None:
        """"""Validate the input and output of a forward function against the type
        annotations, if available. Used in Model.initialize with the input and
        output samples as they pass through the network.
        """"""
        sig = inspect.signature(func)
        empty = inspect.Signature.empty
        params = list(sig.parameters.values())
        if len(params) != 3:
            bad_params = f""{len(params)} ({', '.join([p.name for p in params])})""
            err = f""Invalid forward function. Expected 3 arguments (model, X , is_train), got {bad_params}""
            raise DataValidationError(name, X, Y, [{""msg"": err}])
        annot_x = params[1].annotation
        annot_y = sig.return_annotation
        sig_args: Dict[str, Any] = {""__config__"": _ArgModelConfig}
        args = {}
        if X is not None and annot_x != empty:
            sig_args[""X""] = (annot_x, ...)
            args[""X""] = X
        if Y is not None and annot_y != empty:
            sig_args[""Y""] = (annot_y, ...)
            args[""Y""] = (Y, lambda x: x)
        ArgModel = create_model(""ArgModel"", **sig_args)
        try:
>           ArgModel.parse_obj(args)

..\thinc\thinc\util.py:367: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

cls = <class 'ArgModel'>, obj = {'X': [hello ]}

    @classmethod
    def parse_obj(cls: Type['Model'], obj: Any) -> 'Model':
        if cls.__custom_root_type__ and (
            not (isinstance(obj, dict) and obj.keys() == {ROOT_KEY}) or cls.__fields__[ROOT_KEY].shape == SHAPE_MAPPING
        ):
            obj = {ROOT_KEY: obj}
        elif not isinstance(obj, dict):
            try:
                obj = dict(obj)
            except (TypeError, ValueError) as e:
                exc = TypeError(f'{cls.__name__} expected dict not {type(obj).__name__}')
                raise ValidationError([ErrorWrapper(exc, loc=ROOT_KEY)], cls) from e
>       return cls(**obj)

..\..\..\Anaconda3\envs\myexplosion\lib\site-packages\pydantic\main.py:402: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

__pydantic_self__ = ArgModel(), data = {'X': [hello ]}, values = {}
fields_set = {'X'}
validation_error = ValidationError(model='ArgModel', errors=[{'loc': ('X', 0), 'msg': 'instance of Doc expected', 'type': 'type_error.arbitrary_type', 'ctx': {'expected_arbitrary_type': 'Doc'}}])

    def __init__(__pydantic_self__, **data: Any) -> None:
        # Uses something other than `self` the first arg to allow ""self"" as a settable attribute
        if TYPE_CHECKING:
            __pydantic_self__.__dict__: Dict[str, Any] = {}
            __pydantic_self__.__fields_set__: 'SetStr' = set()
        values, fields_set, validation_error = validate_model(__pydantic_self__.__class__, data)
        if validation_error:
>           raise validation_error
E           pydantic.error_wrappers.ValidationError: 1 validation error for ArgModel
E           X -> 0
E             instance of Doc expected (type=type_error.arbitrary_type; expected_arbitrary_type=Doc)

..\..\..\Anaconda3\envs\myexplosion\lib\site-packages\pydantic\main.py:283: ValidationError",True,{}
explosion/thinc,https://github.com/explosion/thinc,283,2020-01-28T01:39:24Z,2020-01-28T17:41:10Z,2020-01-28T17:41:28Z,MERGED,True,524,3,9,https://github.com/justindujardin,Feature: MXNet shim,8,"['enhancement', 'interop / mxnet']",https://github.com/explosion/thinc/pull/283,https://github.com/justindujardin,1,https://github.com/explosion/thinc/pull/283,"Add support for wrapping MXNet models:
from thinc.api import MXNetWrapper
import mxnet as mx

mx_model = mx.gluon.nn.Sequential()
mx_model.add(
    mx.gluon.nn.Dense(n_hidden),
    mx.gluon.nn.LayerNorm(),
    mx.gluon.nn.Dense(n_hidden, activation=""relu""),
    mx.gluon.nn.LayerNorm(),
    mx.gluon.nn.Dense(10, activation=""softrelu""),
)
thinc_model = MXNetWrapper(mx_model)
X = thinc_model.ops.alloc2f(1, 784)
Y = thinc_model.predict(X)
assert list(Y.shape) == [1, 10]","Add support for wrapping MXNet models:
from thinc.api import MXNetWrapper
import mxnet as mx

mx_model = mx.gluon.nn.Sequential()
mx_model.add(
    mx.gluon.nn.Dense(n_hidden),
    mx.gluon.nn.LayerNorm(),
    mx.gluon.nn.Dense(n_hidden, activation=""relu""),
    mx.gluon.nn.LayerNorm(),
    mx.gluon.nn.Dense(10, activation=""softrelu""),
)
thinc_model = MXNetWrapper(mx_model)
X = thinc_model.ops.alloc2f(1, 784)
Y = thinc_model.predict(X)
assert list(Y.shape) == [1, 10]",True,{'HEART': ['https://github.com/ines']}
explosion/thinc,https://github.com/explosion/thinc,283,2020-01-28T01:39:24Z,2020-01-28T17:41:10Z,2020-01-28T17:41:28Z,MERGED,True,524,3,9,https://github.com/justindujardin,Feature: MXNet shim,8,"['enhancement', 'interop / mxnet']",https://github.com/explosion/thinc/pull/283,https://github.com/apps/codecov,2,https://github.com/explosion/thinc/pull/283#issuecomment-579334346,"Add support for wrapping MXNet models:
from thinc.api import MXNetWrapper
import mxnet as mx

mx_model = mx.gluon.nn.Sequential()
mx_model.add(
    mx.gluon.nn.Dense(n_hidden),
    mx.gluon.nn.LayerNorm(),
    mx.gluon.nn.Dense(n_hidden, activation=""relu""),
    mx.gluon.nn.LayerNorm(),
    mx.gluon.nn.Dense(10, activation=""softrelu""),
)
thinc_model = MXNetWrapper(mx_model)
X = thinc_model.ops.alloc2f(1, 784)
Y = thinc_model.predict(X)
assert list(Y.shape) == [1, 10]","Codecov Report

Merging #283 into develop will increase coverage by 0.32%.
The diff coverage is 96.81%.


@@             Coverage Diff             @@
##           develop     #283      +/-   ##
===========================================
+ Coverage    87.63%   87.96%   +0.32%     
===========================================
  Files           64       67       +3     
  Lines         4142     4329     +187     
===========================================
+ Hits          3630     3808     +178     
- Misses         512      521       +9



Impacted Files
Coverage 





thinc/layers/mxnetwrapper.py
100% <100%> ()



thinc/shims/__init__.py
100% <100%> ()



thinc/util.py
95.76% <100%> (-1.39%)



thinc/layers/__init__.py
100% <100%> ()



thinc/api.py
100% <100%> ()



thinc/shims/mxnet.py
94.38% <94.38%> ()



thinc/layers/hashembed.py
95.91% <0%> (-1.91%)



thinc/config.py
100% <0%> ()



... and 5 more






Continue to review full report at Codecov.

Legend - Click here to learn more
 = absolute <relative> (impact),  = not affected, ? = missing data
Powered by Codecov. Last update 1b6d879...b492636. Read the comment docs.",True,{}
explosion/thinc,https://github.com/explosion/thinc,284,2020-01-28T03:45:39Z,2020-01-28T08:17:43Z,2020-01-28T15:48:45Z,MERGED,True,35,23,4,https://github.com/justindujardin,Merge pull request #284 from justindujardin/feature/better-type-errors,2,"['feat / types', 'interop / mypy']",https://github.com/explosion/thinc/pull/284,https://github.com/justindujardin,1,https://github.com/explosion/thinc/pull/284,"I noticed I was going to look up types after seeing these errors:

So I added input/output types to the messages:","I noticed I was going to look up types after seeing these errors:

So I added input/output types to the messages:",True,"{'ROCKET': ['https://github.com/tiangolo', 'https://github.com/svlandeg', 'https://github.com/ines']}"
explosion/thinc,https://github.com/explosion/thinc,284,2020-01-28T03:45:39Z,2020-01-28T08:17:43Z,2020-01-28T15:48:45Z,MERGED,True,35,23,4,https://github.com/justindujardin,Merge pull request #284 from justindujardin/feature/better-type-errors,2,"['feat / types', 'interop / mypy']",https://github.com/explosion/thinc/pull/284,https://github.com/apps/codecov,2,https://github.com/explosion/thinc/pull/284#issuecomment-579066616,"I noticed I was going to look up types after seeing these errors:

So I added input/output types to the messages:","Codecov Report

Merging #284 into develop will not change coverage.
The diff coverage is n/a.


@@           Coverage Diff            @@
##           develop     #284   +/-   ##
========================================
  Coverage    87.63%   87.63%           
========================================
  Files           64       64           
  Lines         4142     4142           
========================================
  Hits          3630     3630           
  Misses         512      512

Continue to review full report at Codecov.

Legend - Click here to learn more
 = absolute <relative> (impact),  = not affected, ? = missing data
Powered by Codecov. Last update 1b6d879...55e0b54. Read the comment docs.",True,{}
explosion/thinc,https://github.com/explosion/thinc,284,2020-01-28T03:45:39Z,2020-01-28T08:17:43Z,2020-01-28T15:48:45Z,MERGED,True,35,23,4,https://github.com/justindujardin,Merge pull request #284 from justindujardin/feature/better-type-errors,2,"['feat / types', 'interop / mypy']",https://github.com/explosion/thinc/pull/284,https://github.com/ines,3,https://github.com/explosion/thinc/pull/284#issuecomment-579130543,"I noticed I was going to look up types after seeing these errors:

So I added input/output types to the messages:",This is great! ,True,{'ROCKET': ['https://github.com/justindujardin']}
explosion/thinc,https://github.com/explosion/thinc,285,2020-01-28T11:47:59Z,2020-01-28T12:03:23Z,2020-01-28T12:09:13Z,MERGED,True,43,18,4,https://github.com/svlandeg,Fixes for notebook01,3,['examples'],https://github.com/explosion/thinc/pull/285,https://github.com/svlandeg,1,https://github.com/explosion/thinc/pull/285,"Small fixes to the notebook 01

imports where necessary
Linear was initialized with arguments n_in.ndim and n_out.ndim which seems wrong
more prints for the user to follow what's going on (instead of asserts ?)","Small fixes to the notebook 01

imports where necessary
Linear was initialized with arguments n_in.ndim and n_out.ndim which seems wrong
more prints for the user to follow what's going on (instead of asserts ?)",True,{}
explosion/thinc,https://github.com/explosion/thinc,285,2020-01-28T11:47:59Z,2020-01-28T12:03:23Z,2020-01-28T12:09:13Z,MERGED,True,43,18,4,https://github.com/svlandeg,Fixes for notebook01,3,['examples'],https://github.com/explosion/thinc/pull/285,https://github.com/apps/codecov,2,https://github.com/explosion/thinc/pull/285#issuecomment-579209915,"Small fixes to the notebook 01

imports where necessary
Linear was initialized with arguments n_in.ndim and n_out.ndim which seems wrong
more prints for the user to follow what's going on (instead of asserts ?)","Codecov Report

Merging #285 into develop will not change coverage.
The diff coverage is 100%.


@@           Coverage Diff           @@
##           develop    #285   +/-   ##
=======================================
  Coverage     87.6%   87.6%           
=======================================
  Files           64      64           
  Lines         4146    4146           
=======================================
  Hits          3632    3632           
  Misses         514     514



Impacted Files
Coverage 





thinc/layers/strings2arrays.py
100% <100%> ()





Continue to review full report at Codecov.

Legend - Click here to learn more
 = absolute <relative> (impact),  = not affected, ? = missing data
Powered by Codecov. Last update d542a39...139d7b0. Read the comment docs.",True,{}
explosion/thinc,https://github.com/explosion/thinc,286,2020-01-28T14:23:35Z,2020-01-28T16:52:55Z,2020-01-28T16:52:58Z,MERGED,True,156,77,12,https://github.com/ines,Add array getitem,7,"['enhancement', 'feat / layers']",https://github.com/explosion/thinc/pull/286,https://github.com/ines,1,https://github.com/explosion/thinc/pull/286,,,True,{}
explosion/thinc,https://github.com/explosion/thinc,286,2020-01-28T14:23:35Z,2020-01-28T16:52:55Z,2020-01-28T16:52:58Z,MERGED,True,156,77,12,https://github.com/ines,Add array getitem,7,"['enhancement', 'feat / layers']",https://github.com/explosion/thinc/pull/286,https://github.com/apps/codecov,2,https://github.com/explosion/thinc/pull/286#issuecomment-579273340,,"Codecov Report

Merging #286 into develop will increase coverage by 0.04%.
The diff coverage is 95.83%.


@@             Coverage Diff             @@
##           develop     #286      +/-   ##
===========================================
+ Coverage    87.63%   87.68%   +0.04%     
===========================================
  Files           64       65       +1     
  Lines         4142     4175      +33     
===========================================
+ Hits          3630     3661      +31     
- Misses         512      514       +2



Impacted Files
Coverage 





thinc/backends/ops.py
81.67% <0%> ()



thinc/layers/hashembed.py
95.91% <100%> (-1.91%)



thinc/api.py
100% <100%> ()



thinc/layers/embed.py
97.56% <100%> (+2.43%)



thinc/layers/__init__.py
100% <100%> ()



thinc/config.py
100% <100%> ()



thinc/layers/array_getitem.py
90.47% <90.47%> ()



thinc/util.py
97.24% <0%> (+0.1%)





Continue to review full report at Codecov.

Legend - Click here to learn more
 = absolute <relative> (impact),  = not affected, ? = missing data
Powered by Codecov. Last update 449838c...b685588. Read the comment docs.",True,{}
explosion/thinc,https://github.com/explosion/thinc,287,2020-01-28T14:33:12Z,2020-01-28T16:09:57Z,2020-01-28T16:10:47Z,MERGED,True,29,35,7,https://github.com/svlandeg,Fix notebook 04 & 05,10,['examples'],https://github.com/explosion/thinc/pull/287,https://github.com/svlandeg,1,https://github.com/explosion/thinc/pull/287,"Notebook on configuring GPU: reduced example to use PyTorch instead of TF
& various other small fixes","Notebook on configuring GPU: reduced example to use PyTorch instead of TF
& various other small fixes",True,{}
explosion/thinc,https://github.com/explosion/thinc,287,2020-01-28T14:33:12Z,2020-01-28T16:09:57Z,2020-01-28T16:10:47Z,MERGED,True,29,35,7,https://github.com/svlandeg,Fix notebook 04 & 05,10,['examples'],https://github.com/explosion/thinc/pull/287,https://github.com/apps/codecov,2,https://github.com/explosion/thinc/pull/287#issuecomment-579278612,"Notebook on configuring GPU: reduced example to use PyTorch instead of TF
& various other small fixes","Codecov Report

Merging #287 into develop will not change coverage.
The diff coverage is n/a.


@@           Coverage Diff           @@
##           develop    #287   +/-   ##
=======================================
  Coverage     87.6%   87.6%           
=======================================
  Files           64      64           
  Lines         4146    4146           
=======================================
  Hits          3632    3632           
  Misses         514     514

Continue to review full report at Codecov.

Legend - Click here to learn more
 = absolute <relative> (impact),  = not affected, ? = missing data
Powered by Codecov. Last update 8ec6e58...d2879e4. Read the comment docs.",True,{}
explosion/thinc,https://github.com/explosion/thinc,288,2020-01-28T17:21:25Z,2020-01-28T18:04:59Z,2020-01-28T19:04:23Z,MERGED,True,88,81,14,https://github.com/ines,Bugfix/ragged ints,9,['bug'],https://github.com/explosion/thinc/pull/288,https://github.com/ines,1,https://github.com/explosion/thinc/pull/288,,,True,{}
explosion/thinc,https://github.com/explosion/thinc,288,2020-01-28T17:21:25Z,2020-01-28T18:04:59Z,2020-01-28T19:04:23Z,MERGED,True,88,81,14,https://github.com/ines,Bugfix/ragged ints,9,['bug'],https://github.com/explosion/thinc/pull/288,https://github.com/apps/codecov,2,https://github.com/explosion/thinc/pull/288#issuecomment-579366146,,"Codecov Report

Merging #288 into develop will increase coverage by <.01%.
The diff coverage is 95.89%.


@@             Coverage Diff             @@
##           develop     #288      +/-   ##
===========================================
+ Coverage    87.96%   87.96%   +<.01%     
===========================================
  Files           65       67       +2     
  Lines         4279     4329      +50     
===========================================
+ Hits          3764     3808      +44     
- Misses         515      521       +6



Impacted Files
Coverage 





thinc/backends/ops.py
81.67% <0%> ()



thinc/layers/hashembed.py
95.91% <100%> (-2.57%)



thinc/config.py
100% <100%> ()



thinc/api.py
100% <100%> ()



thinc/about.py
100% <100%> ()



thinc/layers/embed.py
97.56% <100%> (-0.63%)



thinc/layers/__init__.py
100% <100%> ()



thinc/layers/array_getitem.py
90.47% <90.47%> ()



... and 7 more






Continue to review full report at Codecov.

Legend - Click here to learn more
 = absolute <relative> (impact),  = not affected, ? = missing data
Powered by Codecov. Last update 6e4cf20...1f041c1. Read the comment docs.",True,{}
explosion/thinc,https://github.com/explosion/thinc,289,2020-01-28T17:49:37Z,2020-01-28T17:52:46Z,2020-01-28T17:53:21Z,MERGED,True,3,3,1,https://github.com/svlandeg,tiny readme fixes,1,['docs'],https://github.com/explosion/thinc/pull/289,https://github.com/svlandeg,1,https://github.com/explosion/thinc/pull/289,,,True,{}
explosion/thinc,https://github.com/explosion/thinc,290,2020-01-28T18:18:02Z,2020-01-28T18:27:07Z,2020-01-28T18:37:09Z,MERGED,True,85,9,1,https://github.com/tiangolo,Add MXNet example to notebook,2,['examples'],https://github.com/explosion/thinc/pull/290,https://github.com/tiangolo,1,https://github.com/explosion/thinc/pull/290, Add MXNet example to notebook, Add MXNet example to notebook,True,{'ROCKET': ['https://github.com/justindujardin']}
explosion/thinc,https://github.com/explosion/thinc,290,2020-01-28T18:18:02Z,2020-01-28T18:27:07Z,2020-01-28T18:37:09Z,MERGED,True,85,9,1,https://github.com/tiangolo,Add MXNet example to notebook,2,['examples'],https://github.com/explosion/thinc/pull/290,https://github.com/apps/codecov,2,https://github.com/explosion/thinc/pull/290#issuecomment-579388072, Add MXNet example to notebook,"Codecov Report

Merging #290 into develop will decrease coverage by 0.02%.
The diff coverage is n/a.


@@             Coverage Diff             @@
##           develop     #290      +/-   ##
===========================================
- Coverage    87.96%   87.93%   -0.03%     
===========================================
  Files           67       67              
  Lines         4329     4310      -19     
===========================================
- Hits          3808     3790      -18     
+ Misses         521      520       -1




Impacted Files
Coverage 





thinc/backends/jax_ops.py
46.07% <0%> (-1.78%)



thinc/shims/pytorch.py
83.72% <0%> (-0.19%)



thinc/shims/tensorflow.py
92% <0%> (-0.05%)



thinc/layers/tensorflowwrapper.py
98.88% <0%> (+1.08%)





Continue to review full report at Codecov.

Legend - Click here to learn more
 = absolute <relative> (impact),  = not affected, ? = missing data
Powered by Codecov. Last update 271f13f...d92ba9e. Read the comment docs.",True,{}
explosion/thinc,https://github.com/explosion/thinc,291,2020-01-28T18:26:27Z,2020-01-28T18:32:55Z,2020-01-28T18:36:01Z,MERGED,True,1,1,1,https://github.com/tiangolo,Use MXNet forward reference type to avoid requiring it,1,"['bug', 'interop / mxnet']",https://github.com/explosion/thinc/pull/291,https://github.com/tiangolo,1,https://github.com/explosion/thinc/pull/291, Use MXNet forward reference type to avoid requiring it, Use MXNet forward reference type to avoid requiring it,True,{}
explosion/thinc,https://github.com/explosion/thinc,291,2020-01-28T18:26:27Z,2020-01-28T18:32:55Z,2020-01-28T18:36:01Z,MERGED,True,1,1,1,https://github.com/tiangolo,Use MXNet forward reference type to avoid requiring it,1,"['bug', 'interop / mxnet']",https://github.com/explosion/thinc/pull/291,https://github.com/apps/codecov,2,https://github.com/explosion/thinc/pull/291#issuecomment-579391009, Use MXNet forward reference type to avoid requiring it,"Codecov Report

Merging #291 into develop will not change coverage.
The diff coverage is 100%.


@@           Coverage Diff            @@
##           develop     #291   +/-   ##
========================================
  Coverage    87.96%   87.96%           
========================================
  Files           67       67           
  Lines         4329     4329           
========================================
  Hits          3808     3808           
  Misses         521      521



Impacted Files
Coverage 





thinc/shims/mxnet.py
94.38% <100%> ()





Continue to review full report at Codecov.

Legend - Click here to learn more
 = absolute <relative> (impact),  = not affected, ? = missing data
Powered by Codecov. Last update 271f13f...b53a7b7. Read the comment docs.",True,{}
explosion/thinc,https://github.com/explosion/thinc,293,2020-01-29T15:47:18Z,2020-01-29T18:00:26Z,2020-01-29T18:00:26Z,MERGED,True,7,1,1,https://github.com/tiangolo,Use site (site-packages) to detect how to run mypy tests,3,"['tests', 'interop / mypy']",https://github.com/explosion/thinc/pull/293,https://github.com/tiangolo,1,https://github.com/explosion/thinc/pull/293," Use site (site-packages) to detect how to run mypy tests.
This should allow running tests in CI with the package installed:
$ python -m pytest --pyargs thinc --cov=thinc --cov-report=xml
But also with source code without having to install it first:
$ pytest thinc
This supersedes #247"," Use site (site-packages) to detect how to run mypy tests.
This should allow running tests in CI with the package installed:
$ python -m pytest --pyargs thinc --cov=thinc --cov-report=xml
But also with source code without having to install it first:
$ pytest thinc
This supersedes #247",True,{}
explosion/thinc,https://github.com/explosion/thinc,293,2020-01-29T15:47:18Z,2020-01-29T18:00:26Z,2020-01-29T18:00:26Z,MERGED,True,7,1,1,https://github.com/tiangolo,Use site (site-packages) to detect how to run mypy tests,3,"['tests', 'interop / mypy']",https://github.com/explosion/thinc/pull/293,https://github.com/apps/codecov,2,https://github.com/explosion/thinc/pull/293#issuecomment-579823188," Use site (site-packages) to detect how to run mypy tests.
This should allow running tests in CI with the package installed:
$ python -m pytest --pyargs thinc --cov=thinc --cov-report=xml
But also with source code without having to install it first:
$ pytest thinc
This supersedes #247","Codecov Report

Merging #293 into develop will not change coverage.
The diff coverage is n/a.


@@           Coverage Diff            @@
##           develop     #293   +/-   ##
========================================
  Coverage    87.97%   87.97%           
========================================
  Files           67       67           
  Lines         4332     4332           
========================================
  Hits          3811     3811           
  Misses         521      521

Continue to review full report at Codecov.

Legend - Click here to learn more
 = absolute <relative> (impact),  = not affected, ? = missing data
Powered by Codecov. Last update 751fe23...e35980f. Read the comment docs.",True,{}
explosion/thinc,https://github.com/explosion/thinc,294,2020-01-29T17:19:05Z,2020-01-29T17:26:50Z,2020-01-30T09:31:41Z,MERGED,True,4,3,1,https://github.com/justindujardin,fix pos tagger notebook by specifying column for embedding layer,1,"['bug', 'examples']",https://github.com/explosion/thinc/pull/294,https://github.com/justindujardin,1,https://github.com/explosion/thinc/pull/294,"This notebook failed in colab when because it wasn't specifying an Embed column:
DataValidationError: 

Data validation error in 'hashembed'
X: <class 'numpy.ndarray'> Y: <class 'NoneType'>

X   wrong array dimensions (expected 1, got 2)","This notebook failed in colab when because it wasn't specifying an Embed column:
DataValidationError: 

Data validation error in 'hashembed'
X: <class 'numpy.ndarray'> Y: <class 'NoneType'>

X   wrong array dimensions (expected 1, got 2)",True,{'THUMBS_UP': ['https://github.com/honnibal']}
explosion/thinc,https://github.com/explosion/thinc,294,2020-01-29T17:19:05Z,2020-01-29T17:26:50Z,2020-01-30T09:31:41Z,MERGED,True,4,3,1,https://github.com/justindujardin,fix pos tagger notebook by specifying column for embedding layer,1,"['bug', 'examples']",https://github.com/explosion/thinc/pull/294,https://github.com/apps/codecov,2,https://github.com/explosion/thinc/pull/294#issuecomment-579867155,"This notebook failed in colab when because it wasn't specifying an Embed column:
DataValidationError: 

Data validation error in 'hashembed'
X: <class 'numpy.ndarray'> Y: <class 'NoneType'>

X   wrong array dimensions (expected 1, got 2)","Codecov Report

Merging #294 into master will not change coverage.
The diff coverage is n/a.


@@           Coverage Diff           @@
##           master     #294   +/-   ##
=======================================
  Coverage   87.97%   87.97%           
=======================================
  Files          67       67           
  Lines        4332     4332           
=======================================
  Hits         3811     3811           
  Misses        521      521

Continue to review full report at Codecov.

Legend - Click here to learn more
 = absolute <relative> (impact),  = not affected, ? = missing data
Powered by Codecov. Last update 6cd9614...7d2bfe4. Read the comment docs.",True,{}
explosion/thinc,https://github.com/explosion/thinc,295,2020-01-29T20:34:42Z,2020-01-29T20:52:05Z,2020-01-29T20:52:12Z,MERGED,True,163,126,1,https://github.com/justindujardin,fix and cleanup like-terms notebook,1,"['bug', 'examples']",https://github.com/explosion/thinc/pull/295,https://github.com/justindujardin,1,https://github.com/explosion/thinc/pull/295,"update to use more specific Int/Float types
update model to work with more strict data-validation
define specific model types and use them throughout the example","update to use more specific Int/Float types
update model to work with more strict data-validation
define specific model types and use them throughout the example",True,{}
explosion/thinc,https://github.com/explosion/thinc,295,2020-01-29T20:34:42Z,2020-01-29T20:52:05Z,2020-01-29T20:52:12Z,MERGED,True,163,126,1,https://github.com/justindujardin,fix and cleanup like-terms notebook,1,"['bug', 'examples']",https://github.com/explosion/thinc/pull/295,https://github.com/apps/codecov,2,https://github.com/explosion/thinc/pull/295#issuecomment-579948914,"update to use more specific Int/Float types
update model to work with more strict data-validation
define specific model types and use them throughout the example","Codecov Report

Merging #295 into master will not change coverage.
The diff coverage is n/a.


@@           Coverage Diff           @@
##           master     #295   +/-   ##
=======================================
  Coverage   87.97%   87.97%           
=======================================
  Files          67       67           
  Lines        4332     4332           
=======================================
  Hits         3811     3811           
  Misses        521      521



Impacted Files
Coverage 





thinc/backends/jax_ops.py
47.84% <0%> ()



thinc/shims/pytorch.py
83.9% <0%> ()



thinc/shims/tensorflow.py
92.04% <0%> ()



thinc/layers/tensorflowwrapper.py
97.8% <0%> ()





Continue to review full report at Codecov.

Legend - Click here to learn more
 = absolute <relative> (impact),  = not affected, ? = missing data
Powered by Codecov. Last update 4dabba0...5df6e93. Read the comment docs.",True,{}
explosion/thinc,https://github.com/explosion/thinc,296,2020-01-29T22:57:07Z,2020-01-30T00:10:19Z,2020-01-30T09:22:56Z,MERGED,True,3,3,1,https://github.com/BioGeek,String formatting precision fails when run on GPU.,1,"['bug', 'examples']",https://github.com/explosion/thinc/pull/296,https://github.com/BioGeek,1,https://github.com/explosion/thinc/pull/296,"If you run the 00_intro_to_thinc.ipynb notebook on a GPU, you get the following error when you execute the cell where you create an optimizer and do several passes over the data.
TypeError                                 Traceback (most recent call last)

<ipython-input-27-c7aae9724b89> in <module>()
     21         total += Yh.shape[0]
     22     score = correct / total
---> 23     print(f"" {i} {score:.3f}"")

TypeError: unsupported format string passed to cupy.core.core.ndarray.__format__


Also see the related open NumPy issue: ndarray should offer __format__ that can adjust precision #5543 .
This pull request proposes a simple fix/workaround.","If you run the 00_intro_to_thinc.ipynb notebook on a GPU, you get the following error when you execute the cell where you create an optimizer and do several passes over the data.
TypeError                                 Traceback (most recent call last)

<ipython-input-27-c7aae9724b89> in <module>()
     21         total += Yh.shape[0]
     22     score = correct / total
---> 23     print(f"" {i} {score:.3f}"")

TypeError: unsupported format string passed to cupy.core.core.ndarray.__format__


Also see the related open NumPy issue: ndarray should offer __format__ that can adjust precision #5543 .
This pull request proposes a simple fix/workaround.",True,{}
explosion/thinc,https://github.com/explosion/thinc,296,2020-01-29T22:57:07Z,2020-01-30T00:10:19Z,2020-01-30T09:22:56Z,MERGED,True,3,3,1,https://github.com/BioGeek,String formatting precision fails when run on GPU.,1,"['bug', 'examples']",https://github.com/explosion/thinc/pull/296,https://github.com/apps/codecov,2,https://github.com/explosion/thinc/pull/296#issuecomment-580003915,"If you run the 00_intro_to_thinc.ipynb notebook on a GPU, you get the following error when you execute the cell where you create an optimizer and do several passes over the data.
TypeError                                 Traceback (most recent call last)

<ipython-input-27-c7aae9724b89> in <module>()
     21         total += Yh.shape[0]
     22     score = correct / total
---> 23     print(f"" {i} {score:.3f}"")

TypeError: unsupported format string passed to cupy.core.core.ndarray.__format__


Also see the related open NumPy issue: ndarray should offer __format__ that can adjust precision #5543 .
This pull request proposes a simple fix/workaround.","Codecov Report

Merging #296 into master will increase coverage by 4.52%.
The diff coverage is n/a.


@@            Coverage Diff             @@
##           master     #296      +/-   ##
==========================================
+ Coverage   83.44%   87.97%   +4.52%     
==========================================
  Files          67       67              
  Lines        4326     4332       +6     
==========================================
+ Hits         3610     3811     +201     
+ Misses        716      521     -195



Impacted Files
Coverage 





thinc/schedules.py
100% <0%> ()



thinc/shims/tensorflow.py
92.04% <0%> (+71.02%)



thinc/layers/tensorflowwrapper.py
97.8% <0%> (+76.92%)





Continue to review full report at Codecov.

Legend - Click here to learn more
 = absolute <relative> (impact),  = not affected, ? = missing data
Powered by Codecov. Last update a17cd74...f255d29. Read the comment docs.",True,{}
explosion/thinc,https://github.com/explosion/thinc,296,2020-01-29T22:57:07Z,2020-01-30T00:10:19Z,2020-01-30T09:22:56Z,MERGED,True,3,3,1,https://github.com/BioGeek,String formatting precision fails when run on GPU.,1,"['bug', 'examples']",https://github.com/explosion/thinc/pull/296,https://github.com/honnibal,3,https://github.com/explosion/thinc/pull/296#issuecomment-580023260,"If you run the 00_intro_to_thinc.ipynb notebook on a GPU, you get the following error when you execute the cell where you create an optimizer and do several passes over the data.
TypeError                                 Traceback (most recent call last)

<ipython-input-27-c7aae9724b89> in <module>()
     21         total += Yh.shape[0]
     22     score = correct / total
---> 23     print(f"" {i} {score:.3f}"")

TypeError: unsupported format string passed to cupy.core.core.ndarray.__format__


Also see the related open NumPy issue: ndarray should offer __format__ that can adjust precision #5543 .
This pull request proposes a simple fix/workaround.",Thanks!,True,{}
explosion/thinc,https://github.com/explosion/thinc,297,2020-01-29T23:50:38Z,2020-01-29T23:50:45Z,2020-01-30T09:25:33Z,MERGED,True,45,177,1,https://github.com/justindujardin,add tqdm and cleanup like-terms notebook,1,"['enhancement', 'examples']",https://github.com/explosion/thinc/pull/297,https://github.com/justindujardin,1,https://github.com/explosion/thinc/pull/297,,,True,{}
explosion/thinc,https://github.com/explosion/thinc,298,2020-01-30T16:11:20Z,2020-01-31T10:25:11Z,2020-01-31T10:25:11Z,MERGED,True,1,1,1,https://github.com/justindujardin,fix issue where GPU model couldn't be serialized,1,"['bug', 'serialization']",https://github.com/explosion/thinc/pull/298,https://github.com/justindujardin,1,https://github.com/explosion/thinc/pull/298,"Oops, we were converting the model dict to numpy, then passing self.to_dict() to convert_recursive instead of the converted numpy array ","Oops, we were converting the model dict to numpy, then passing self.to_dict() to convert_recursive instead of the converted numpy array ",True,{}
explosion/thinc,https://github.com/explosion/thinc,298,2020-01-30T16:11:20Z,2020-01-31T10:25:11Z,2020-01-31T10:25:11Z,MERGED,True,1,1,1,https://github.com/justindujardin,fix issue where GPU model couldn't be serialized,1,"['bug', 'serialization']",https://github.com/explosion/thinc/pull/298,https://github.com/apps/codecov,2,https://github.com/explosion/thinc/pull/298#issuecomment-580330958,"Oops, we were converting the model dict to numpy, then passing self.to_dict() to convert_recursive instead of the converted numpy array ","Codecov Report

Merging #298 into master will not change coverage.
The diff coverage is 100%.


@@           Coverage Diff           @@
##           master     #298   +/-   ##
=======================================
  Coverage   87.97%   87.97%           
=======================================
  Files          67       67           
  Lines        4332     4332           
=======================================
  Hits         3811     3811           
  Misses        521      521



Impacted Files
Coverage 





thinc/model.py
100% <100%> ()





Continue to review full report at Codecov.

Legend - Click here to learn more
 = absolute <relative> (impact),  = not affected, ? = missing data
Powered by Codecov. Last update ff20143...c2ac309. Read the comment docs.",True,{}
explosion/thinc,https://github.com/explosion/thinc,298,2020-01-30T16:11:20Z,2020-01-31T10:25:11Z,2020-01-31T10:25:11Z,MERGED,True,1,1,1,https://github.com/justindujardin,fix issue where GPU model couldn't be serialized,1,"['bug', 'serialization']",https://github.com/explosion/thinc/pull/298,https://github.com/justindujardin,3,https://github.com/explosion/thinc/pull/298#issuecomment-580333710,"Oops, we were converting the model dict to numpy, then passing self.to_dict() to convert_recursive instead of the converted numpy array ",Possibly related to: https://github.com/explosion/spacy-transformers/issues/99,True,{}
explosion/thinc,https://github.com/explosion/thinc,299,2020-02-03T16:25:36Z,2020-02-03T20:02:15Z,2020-02-03T20:44:21Z,MERGED,True,29,6,2,https://github.com/svlandeg,Small fixes for the optimizers,5,"['bug', 'enhancement']",https://github.com/explosion/thinc/pull/299,https://github.com/svlandeg,1,https://github.com/explosion/thinc/pull/299,"__slots__  for Optimizer
Pass on the value of use_averages consistently instead of overwriting
Have L2_is_weight_decay as a parameter for RAdam","__slots__  for Optimizer
Pass on the value of use_averages consistently instead of overwriting
Have L2_is_weight_decay as a parameter for RAdam",True,{'THUMBS_UP': ['https://github.com/honnibal']}
explosion/thinc,https://github.com/explosion/thinc,299,2020-02-03T16:25:36Z,2020-02-03T20:02:15Z,2020-02-03T20:44:21Z,MERGED,True,29,6,2,https://github.com/svlandeg,Small fixes for the optimizers,5,"['bug', 'enhancement']",https://github.com/explosion/thinc/pull/299,https://github.com/apps/codecov,2,https://github.com/explosion/thinc/pull/299#issuecomment-581499641,"__slots__  for Optimizer
Pass on the value of use_averages consistently instead of overwriting
Have L2_is_weight_decay as a parameter for RAdam","Codecov Report

Merging #299 into master will increase coverage by <.01%.
The diff coverage is 100%.


@@            Coverage Diff             @@
##           master     #299      +/-   ##
==========================================
+ Coverage   87.97%   87.97%   +<.01%     
==========================================
  Files          67       67              
  Lines        4332     4333       +1     
==========================================
+ Hits         3811     3812       +1     
  Misses        521      521



Impacted Files
Coverage 





thinc/optimizers.py
94.23% <100%> (+0.03%)





Continue to review full report at Codecov.

Legend - Click here to learn more
 = absolute <relative> (impact),  = not affected, ? = missing data
Powered by Codecov. Last update 7384056...a02aad4. Read the comment docs.",True,{}
explosion/thinc,https://github.com/explosion/thinc,300,2020-02-03T19:49:55Z,2020-02-04T18:22:23Z,2020-02-04T18:27:21Z,MERGED,True,10,7,3,https://github.com/svlandeg,Rename Unserializable 'data' field,3,['feat / layers'],https://github.com/explosion/thinc/pull/300,https://github.com/svlandeg,1,https://github.com/explosion/thinc/pull/300,"This was a cute little bug:

Unserializable has a data field
StaticVectors expects a vectors argument (e.g. spaCy's Vectors object) that has a data field
So you'd need to do model.attrs[""vectors""].data.data to get to the actual data, but I renamed it instead to avoid future confusion","This was a cute little bug:

Unserializable has a data field
StaticVectors expects a vectors argument (e.g. spaCy's Vectors object) that has a data field
So you'd need to do model.attrs[""vectors""].data.data to get to the actual data, but I renamed it instead to avoid future confusion",True,{}
explosion/thinc,https://github.com/explosion/thinc,300,2020-02-03T19:49:55Z,2020-02-04T18:22:23Z,2020-02-04T18:27:21Z,MERGED,True,10,7,3,https://github.com/svlandeg,Rename Unserializable 'data' field,3,['feat / layers'],https://github.com/explosion/thinc/pull/300,https://github.com/svlandeg,2,https://github.com/explosion/thinc/pull/300#issuecomment-581613030,"This was a cute little bug:

Unserializable has a data field
StaticVectors expects a vectors argument (e.g. spaCy's Vectors object) that has a data field
So you'd need to do model.attrs[""vectors""].data.data to get to the actual data, but I renamed it instead to avoid future confusion","Looking at the failing tests, I guess the original assumption was that the vectors object is directly compatible with Floats2d, in which case the bug was in my spaCy code and not here. So from spaCy we should create this layer with vectors=vectors.data instead. Updated the PR accordingly. Still would like to keep the renamed field in Unserializable which caused this confusion :-)",True,{}
explosion/thinc,https://github.com/explosion/thinc,300,2020-02-03T19:49:55Z,2020-02-04T18:22:23Z,2020-02-04T18:27:21Z,MERGED,True,10,7,3,https://github.com/svlandeg,Rename Unserializable 'data' field,3,['feat / layers'],https://github.com/explosion/thinc/pull/300,https://github.com/apps/codecov,3,https://github.com/explosion/thinc/pull/300#issuecomment-581614336,"This was a cute little bug:

Unserializable has a data field
StaticVectors expects a vectors argument (e.g. spaCy's Vectors object) that has a data field
So you'd need to do model.attrs[""vectors""].data.data to get to the actual data, but I renamed it instead to avoid future confusion","Codecov Report

Merging #300 into master will increase coverage by <.01%.
The diff coverage is 100%.


@@            Coverage Diff             @@
##           master     #300      +/-   ##
==========================================
+ Coverage   87.97%   87.97%   +<.01%     
==========================================
  Files          67       67              
  Lines        4332     4333       +1     
==========================================
+ Hits         3811     3812       +1     
  Misses        521      521



Impacted Files
Coverage 





thinc/layers/concatenate.py
71.64% <100%> ()



thinc/backends/jax_ops.py
47.84% <0%> ()



thinc/shims/pytorch.py
83.9% <0%> ()



thinc/shims/tensorflow.py
92.04% <0%> ()



thinc/optimizers.py
94.23% <0%> (+0.03%)



thinc/layers/tensorflowwrapper.py
97.8% <0%> ()





Continue to review full report at Codecov.

Legend - Click here to learn more
 = absolute <relative> (impact),  = not affected, ? = missing data
Powered by Codecov. Last update 530d434...94df24f. Read the comment docs.",True,{}
explosion/thinc,https://github.com/explosion/thinc,300,2020-02-03T19:49:55Z,2020-02-04T18:22:23Z,2020-02-04T18:27:21Z,MERGED,True,10,7,3,https://github.com/svlandeg,Rename Unserializable 'data' field,3,['feat / layers'],https://github.com/explosion/thinc/pull/300,https://github.com/honnibal,4,https://github.com/explosion/thinc/pull/300#issuecomment-581847037,"This was a cute little bug:

Unserializable has a data field
StaticVectors expects a vectors argument (e.g. spaCy's Vectors object) that has a data field
So you'd need to do model.attrs[""vectors""].data.data to get to the actual data, but I renamed it instead to avoid future confusion","Agree that data is poor. Maybe obj instead of wrapped_object though? The codebase normally prefers shorter names, so wrapped_object is a bit inconsistent.",True,{'THUMBS_UP': ['https://github.com/svlandeg']}
explosion/thinc,https://github.com/explosion/thinc,302,2020-02-04T21:41:55Z,2020-02-04T22:17:56Z,2020-11-19T16:21:56Z,MERGED,True,5,1,1,https://github.com/honnibal,Fix ordering in list2padded op,1,"['bug', 'feat / layers']",https://github.com/explosion/thinc/pull/302,https://github.com/honnibal,1,https://github.com/explosion/thinc/pull/302,"The list2padded operation didn't reorder the sequences, so the batches were entirely incorrect.","The list2padded operation didn't reorder the sequences, so the batches were entirely incorrect.",True,{}
explosion/thinc,https://github.com/explosion/thinc,302,2020-02-04T21:41:55Z,2020-02-04T22:17:56Z,2020-11-19T16:21:56Z,MERGED,True,5,1,1,https://github.com/honnibal,Fix ordering in list2padded op,1,"['bug', 'feat / layers']",https://github.com/explosion/thinc/pull/302,https://github.com/apps/codecov,2,https://github.com/explosion/thinc/pull/302#issuecomment-582130656,"The list2padded operation didn't reorder the sequences, so the batches were entirely incorrect.","Codecov Report

Merging #302 into master will increase coverage by <.01%.
The diff coverage is 100%.


@@            Coverage Diff             @@
##           master     #302      +/-   ##
==========================================
+ Coverage   87.97%   87.97%   +<.01%     
==========================================
  Files          67       67              
  Lines        4333     4334       +1     
==========================================
+ Hits         3812     3813       +1     
  Misses        521      521



Impacted Files
Coverage 





thinc/backends/ops.py
81.7% <100%> (+0.03%)



thinc/backends/jax_ops.py
47.84% <0%> ()



thinc/shims/pytorch.py
83.9% <0%> ()



thinc/shims/tensorflow.py
92.04% <0%> ()



thinc/layers/tensorflowwrapper.py
97.8% <0%> ()





Continue to review full report at Codecov.

Legend - Click here to learn more
 = absolute <relative> (impact),  = not affected, ? = missing data
Powered by Codecov. Last update eef6ade...3dc597c. Read the comment docs.",True,{}
explosion/thinc,https://github.com/explosion/thinc,303,2020-02-04T22:17:45Z,2020-02-11T23:09:35Z,2020-02-11T23:10:11Z,MERGED,True,19,16,2,https://github.com/svlandeg,Fixing StaticVectors,4,"['bug', 'feat / layers']",https://github.com/explosion/thinc/pull/303,https://github.com/svlandeg,1,https://github.com/explosion/thinc/pull/303,"StaticVectors didn't work for me, it got the wrong output dimensions. I shuffled the code around, more or less as it used to be. This seems to at least fix the training step.
A review would be good because I'm not entirely confident.
Also, there's still quite a bit to do on the current spaCy develop branch with respect to StaticVectors, cfg passing, IO etc, so I haven't yet been able to test this end-to-end yet, which is why I'm suggesting to have this as WIP (see also https://github.com/svlandeg/spaCy/tree/feature/static-vectors).","StaticVectors didn't work for me, it got the wrong output dimensions. I shuffled the code around, more or less as it used to be. This seems to at least fix the training step.
A review would be good because I'm not entirely confident.
Also, there's still quite a bit to do on the current spaCy develop branch with respect to StaticVectors, cfg passing, IO etc, so I haven't yet been able to test this end-to-end yet, which is why I'm suggesting to have this as WIP (see also https://github.com/svlandeg/spaCy/tree/feature/static-vectors).",True,{}
explosion/thinc,https://github.com/explosion/thinc,303,2020-02-04T22:17:45Z,2020-02-11T23:09:35Z,2020-02-11T23:10:11Z,MERGED,True,19,16,2,https://github.com/svlandeg,Fixing StaticVectors,4,"['bug', 'feat / layers']",https://github.com/explosion/thinc/pull/303,https://github.com/apps/codecov,2,https://github.com/explosion/thinc/pull/303#issuecomment-582145140,"StaticVectors didn't work for me, it got the wrong output dimensions. I shuffled the code around, more or less as it used to be. This seems to at least fix the training step.
A review would be good because I'm not entirely confident.
Also, there's still quite a bit to do on the current spaCy develop branch with respect to StaticVectors, cfg passing, IO etc, so I haven't yet been able to test this end-to-end yet, which is why I'm suggesting to have this as WIP (see also https://github.com/svlandeg/spaCy/tree/feature/static-vectors).","Codecov Report

Merging #303 into master will increase coverage by <.01%.
The diff coverage is 100%.


@@            Coverage Diff             @@
##           master     #303      +/-   ##
==========================================
+ Coverage   87.97%   87.97%   +<.01%     
==========================================
  Files          67       67              
  Lines        4333     4334       +1     
==========================================
+ Hits         3812     3813       +1     
  Misses        521      521



Impacted Files
Coverage 





thinc/model.py
100% <100%> ()



thinc/backends/ops.py
81.7% <0%> (+0.03%)





Continue to review full report at Codecov.

Legend - Click here to learn more
 = absolute <relative> (impact),  = not affected, ? = missing data
Powered by Codecov. Last update eef6ade...2236902. Read the comment docs.",True,{}
explosion/thinc,https://github.com/explosion/thinc,303,2020-02-04T22:17:45Z,2020-02-11T23:09:35Z,2020-02-11T23:10:11Z,MERGED,True,19,16,2,https://github.com/svlandeg,Fixing StaticVectors,4,"['bug', 'feat / layers']",https://github.com/explosion/thinc/pull/303,https://github.com/svlandeg,3,https://github.com/explosion/thinc/pull/303#issuecomment-583352016,"StaticVectors didn't work for me, it got the wrong output dimensions. I shuffled the code around, more or less as it used to be. This seems to at least fix the training step.
A review would be good because I'm not entirely confident.
Also, there's still quite a bit to do on the current spaCy develop branch with respect to StaticVectors, cfg passing, IO etc, so I haven't yet been able to test this end-to-end yet, which is why I'm suggesting to have this as WIP (see also https://github.com/svlandeg/spaCy/tree/feature/static-vectors).","Ok, after some more testing this seems to run OK at least. Ready for review :-)",True,{}
explosion/thinc,https://github.com/explosion/thinc,303,2020-02-04T22:17:45Z,2020-02-11T23:09:35Z,2020-02-11T23:10:11Z,MERGED,True,19,16,2,https://github.com/svlandeg,Fixing StaticVectors,4,"['bug', 'feat / layers']",https://github.com/explosion/thinc/pull/303,https://github.com/svlandeg,4,https://github.com/explosion/thinc/pull/303#issuecomment-584624892,"StaticVectors didn't work for me, it got the wrong output dimensions. I shuffled the code around, more or less as it used to be. This seems to at least fix the training step.
A review would be good because I'm not entirely confident.
Also, there's still quite a bit to do on the current spaCy develop branch with respect to StaticVectors, cfg passing, IO etc, so I haven't yet been able to test this end-to-end yet, which is why I'm suggesting to have this as WIP (see also https://github.com/svlandeg/spaCy/tree/feature/static-vectors).","Update: also allowed to specify the init_W and set default to glorot_uniform_init. Oddly, that doesn't seem to make much difference in spaCy's model training.",True,{}
explosion/thinc,https://github.com/explosion/thinc,304,2020-02-04T22:54:29Z,2021-01-20T07:40:04Z,2021-01-20T07:40:04Z,CLOSED,False,2973,303,16,https://github.com/honnibal,Faster CPU BiLSTM,105,"['enhancement', 'feat / layers']",https://github.com/explosion/thinc/pull/304,https://github.com/honnibal,1,https://github.com/explosion/thinc/pull/304,"Thinc should have a decent BiLSTM implementation, so we can use it in spaCy. The current one is too slow, so we need to use PyTorch.
For GPU, we can call into CUDNN via cupy, and get fast speeds (although it's not so flexible). It was pretty hard to figure out the API, but I've pretty much worked all of that out (although I'm not ready to push the work on it yet).
The missing piece was a CPU implementation that was compatible with CuDNN (so that you can use a network trained on GPU on CPU). I don't know about full compatibility yet, but the speed is now pretty good.
Benchmark
I ran a quick benchmark with synthetic data against PyTorch on my laptop (at examples/benchmark/lstm_tagger.py). I used three timings each, and picked the best one. Results reported in words per second (higher is better).




Fwd
Fwd+Bwd




Thinc
12.9k
4.0k


PyTorch
17.6k
4.1k



The network size (depth 2, 300d, bidirectional) is about the size we'd be looking to run in spaCy. I set both libraries to single-threaded. I normally get worse performance multi-threading on CPU.
I definitely consider anything better than 50% as fast as PyTorch a victory, especially since this was only a few days work. The backward speed is maybe a bit too good to be true --- possibly I'm making a slight mistake somewhere that could cost accuracy?
Threading, nogil, and blis
The inference loop is almost nogil, so we could make it close to 2x faster by using cython.parallel.prange and using 2 threads. This is sort of a pain in the ass though: it means using openmp, which introduces all sorts of cross-platform headaches. Not worth it.
I used blis.cy.gemm to get matrix multiplication from Cython. This isn't strictly necessary --- we could use numpy instead. The thing is though, to do the implementation efficiently we have to be working on subarrays using inplace operations. That's really messy with numpy --- it's actually a lot clearer and easier with raw pointers. So I would prefer to just add back the blis dependency. We can keep the option of use_blis on NumpyOps, but set it to True.
Variations
There are many variations of LSTMs we can implement as well. For instance, layer norm LSTM is apparently popular now. It's nice to have a decent Cython implementation, so that we know we can do these variants relatively efficiently. However, we need to make sure we have an implementation that remains compatible with CuDNN. Other libraries often end up with two implementations, one CuDNN-compatible, the other not. We might end up with that too.
TODO

 Implement BiLSTM forward pass for NumpyOps
 Implement BiLSTM backward pass for NumpyOps
 Check accuracy against PyTorch using tagger script
 Check speed against PyTorch
 Implement dropout for NumpyOps
 Fix Ops reference implementation
 Add CuDNN implementation for CupyOps
 Verify GPU->serialize->CPU works accurately
 Verify CPU->serialize->GPU works accurately
 Refactor/tidy up","Thinc should have a decent BiLSTM implementation, so we can use it in spaCy. The current one is too slow, so we need to use PyTorch.
For GPU, we can call into CUDNN via cupy, and get fast speeds (although it's not so flexible). It was pretty hard to figure out the API, but I've pretty much worked all of that out (although I'm not ready to push the work on it yet).
The missing piece was a CPU implementation that was compatible with CuDNN (so that you can use a network trained on GPU on CPU). I don't know about full compatibility yet, but the speed is now pretty good.
Benchmark
I ran a quick benchmark with synthetic data against PyTorch on my laptop (at examples/benchmark/lstm_tagger.py). I used three timings each, and picked the best one. Results reported in words per second (higher is better).




Fwd
Fwd+Bwd




Thinc
12.9k
4.0k


PyTorch
17.6k
4.1k



The network size (depth 2, 300d, bidirectional) is about the size we'd be looking to run in spaCy. I set both libraries to single-threaded. I normally get worse performance multi-threading on CPU.
I definitely consider anything better than 50% as fast as PyTorch a victory, especially since this was only a few days work. The backward speed is maybe a bit too good to be true --- possibly I'm making a slight mistake somewhere that could cost accuracy?
Threading, nogil, and blis
The inference loop is almost nogil, so we could make it close to 2x faster by using cython.parallel.prange and using 2 threads. This is sort of a pain in the ass though: it means using openmp, which introduces all sorts of cross-platform headaches. Not worth it.
I used blis.cy.gemm to get matrix multiplication from Cython. This isn't strictly necessary --- we could use numpy instead. The thing is though, to do the implementation efficiently we have to be working on subarrays using inplace operations. That's really messy with numpy --- it's actually a lot clearer and easier with raw pointers. So I would prefer to just add back the blis dependency. We can keep the option of use_blis on NumpyOps, but set it to True.
Variations
There are many variations of LSTMs we can implement as well. For instance, layer norm LSTM is apparently popular now. It's nice to have a decent Cython implementation, so that we know we can do these variants relatively efficiently. However, we need to make sure we have an implementation that remains compatible with CuDNN. Other libraries often end up with two implementations, one CuDNN-compatible, the other not. We might end up with that too.
TODO

 Implement BiLSTM forward pass for NumpyOps
 Implement BiLSTM backward pass for NumpyOps
 Check accuracy against PyTorch using tagger script
 Check speed against PyTorch
 Implement dropout for NumpyOps
 Fix Ops reference implementation
 Add CuDNN implementation for CupyOps
 Verify GPU->serialize->CPU works accurately
 Verify CPU->serialize->GPU works accurately
 Refactor/tidy up",True,"{'EYES': ['https://github.com/svlandeg', 'https://github.com/bdewilde'], 'HOORAY': ['https://github.com/justindujardin', 'https://github.com/bdewilde', 'https://github.com/bratao']}"
explosion/thinc,https://github.com/explosion/thinc,304,2020-02-04T22:54:29Z,2021-01-20T07:40:04Z,2021-01-20T07:40:04Z,CLOSED,False,2973,303,16,https://github.com/honnibal,Faster CPU BiLSTM,105,"['enhancement', 'feat / layers']",https://github.com/explosion/thinc/pull/304,https://github.com/apps/codecov,2,https://github.com/explosion/thinc/pull/304#issuecomment-586146895,"Thinc should have a decent BiLSTM implementation, so we can use it in spaCy. The current one is too slow, so we need to use PyTorch.
For GPU, we can call into CUDNN via cupy, and get fast speeds (although it's not so flexible). It was pretty hard to figure out the API, but I've pretty much worked all of that out (although I'm not ready to push the work on it yet).
The missing piece was a CPU implementation that was compatible with CuDNN (so that you can use a network trained on GPU on CPU). I don't know about full compatibility yet, but the speed is now pretty good.
Benchmark
I ran a quick benchmark with synthetic data against PyTorch on my laptop (at examples/benchmark/lstm_tagger.py). I used three timings each, and picked the best one. Results reported in words per second (higher is better).




Fwd
Fwd+Bwd




Thinc
12.9k
4.0k


PyTorch
17.6k
4.1k



The network size (depth 2, 300d, bidirectional) is about the size we'd be looking to run in spaCy. I set both libraries to single-threaded. I normally get worse performance multi-threading on CPU.
I definitely consider anything better than 50% as fast as PyTorch a victory, especially since this was only a few days work. The backward speed is maybe a bit too good to be true --- possibly I'm making a slight mistake somewhere that could cost accuracy?
Threading, nogil, and blis
The inference loop is almost nogil, so we could make it close to 2x faster by using cython.parallel.prange and using 2 threads. This is sort of a pain in the ass though: it means using openmp, which introduces all sorts of cross-platform headaches. Not worth it.
I used blis.cy.gemm to get matrix multiplication from Cython. This isn't strictly necessary --- we could use numpy instead. The thing is though, to do the implementation efficiently we have to be working on subarrays using inplace operations. That's really messy with numpy --- it's actually a lot clearer and easier with raw pointers. So I would prefer to just add back the blis dependency. We can keep the option of use_blis on NumpyOps, but set it to True.
Variations
There are many variations of LSTMs we can implement as well. For instance, layer norm LSTM is apparently popular now. It's nice to have a decent Cython implementation, so that we know we can do these variants relatively efficiently. However, we need to make sure we have an implementation that remains compatible with CuDNN. Other libraries often end up with two implementations, one CuDNN-compatible, the other not. We might end up with that too.
TODO

 Implement BiLSTM forward pass for NumpyOps
 Implement BiLSTM backward pass for NumpyOps
 Check accuracy against PyTorch using tagger script
 Check speed against PyTorch
 Implement dropout for NumpyOps
 Fix Ops reference implementation
 Add CuDNN implementation for CupyOps
 Verify GPU->serialize->CPU works accurately
 Verify CPU->serialize->GPU works accurately
 Refactor/tidy up","Codecov Report

Merging #304 (37a5607) into master (3dcfdaa) will decrease coverage by 0.12%.
The diff coverage is 74.33%.


@@            Coverage Diff             @@
##           master     #304      +/-   ##
==========================================
- Coverage   59.45%   59.33%   -0.13%     
==========================================
  Files          94       94              
  Lines        6687     6878     +191     
==========================================
+ Hits         3976     4081     +105     
- Misses       2711     2797      +86     



Impacted Files
Coverage 





thinc/layers/with_flatten.py
40.00% <0.00%> (+2.50%)



thinc/tests/backends/test_ops.py
0.00% <0.00%> ()



thinc/tests/layers/test_lstm.py
0.00% <0.00%> ()



thinc/layers/lstm.py
98.19% <97.36%> (-1.81%)



thinc/backends/ops.py
83.03% <98.88%> (+2.24%)



thinc/layers/bidirectional.py
37.50% <0.00%> (-62.50%)



thinc/layers/clone.py
94.11% <0.00%> (-5.89%)





Continue to review full report at Codecov.

Legend - Click here to learn more
 = absolute <relative> (impact),  = not affected, ? = missing data
Powered by Codecov. Last update 3dcfdaa...37a5607. Read the comment docs.",True,{}
explosion/thinc,https://github.com/explosion/thinc,304,2020-02-04T22:54:29Z,2021-01-20T07:40:04Z,2021-01-20T07:40:04Z,CLOSED,False,2973,303,16,https://github.com/honnibal,Faster CPU BiLSTM,105,"['enhancement', 'feat / layers']",https://github.com/explosion/thinc/pull/304,https://github.com/naveenjafer,3,https://github.com/explosion/thinc/pull/304#issuecomment-605967131,"Thinc should have a decent BiLSTM implementation, so we can use it in spaCy. The current one is too slow, so we need to use PyTorch.
For GPU, we can call into CUDNN via cupy, and get fast speeds (although it's not so flexible). It was pretty hard to figure out the API, but I've pretty much worked all of that out (although I'm not ready to push the work on it yet).
The missing piece was a CPU implementation that was compatible with CuDNN (so that you can use a network trained on GPU on CPU). I don't know about full compatibility yet, but the speed is now pretty good.
Benchmark
I ran a quick benchmark with synthetic data against PyTorch on my laptop (at examples/benchmark/lstm_tagger.py). I used three timings each, and picked the best one. Results reported in words per second (higher is better).




Fwd
Fwd+Bwd




Thinc
12.9k
4.0k


PyTorch
17.6k
4.1k



The network size (depth 2, 300d, bidirectional) is about the size we'd be looking to run in spaCy. I set both libraries to single-threaded. I normally get worse performance multi-threading on CPU.
I definitely consider anything better than 50% as fast as PyTorch a victory, especially since this was only a few days work. The backward speed is maybe a bit too good to be true --- possibly I'm making a slight mistake somewhere that could cost accuracy?
Threading, nogil, and blis
The inference loop is almost nogil, so we could make it close to 2x faster by using cython.parallel.prange and using 2 threads. This is sort of a pain in the ass though: it means using openmp, which introduces all sorts of cross-platform headaches. Not worth it.
I used blis.cy.gemm to get matrix multiplication from Cython. This isn't strictly necessary --- we could use numpy instead. The thing is though, to do the implementation efficiently we have to be working on subarrays using inplace operations. That's really messy with numpy --- it's actually a lot clearer and easier with raw pointers. So I would prefer to just add back the blis dependency. We can keep the option of use_blis on NumpyOps, but set it to True.
Variations
There are many variations of LSTMs we can implement as well. For instance, layer norm LSTM is apparently popular now. It's nice to have a decent Cython implementation, so that we know we can do these variants relatively efficiently. However, we need to make sure we have an implementation that remains compatible with CuDNN. Other libraries often end up with two implementations, one CuDNN-compatible, the other not. We might end up with that too.
TODO

 Implement BiLSTM forward pass for NumpyOps
 Implement BiLSTM backward pass for NumpyOps
 Check accuracy against PyTorch using tagger script
 Check speed against PyTorch
 Implement dropout for NumpyOps
 Fix Ops reference implementation
 Add CuDNN implementation for CupyOps
 Verify GPU->serialize->CPU works accurately
 Verify CPU->serialize->GPU works accurately
 Refactor/tidy up",Hi @honnibal any idea as to when this LSTM fix will be merged to master?,True,{}
explosion/thinc,https://github.com/explosion/thinc,304,2020-02-04T22:54:29Z,2021-01-20T07:40:04Z,2021-01-20T07:40:04Z,CLOSED,False,2973,303,16,https://github.com/honnibal,Faster CPU BiLSTM,105,"['enhancement', 'feat / layers']",https://github.com/explosion/thinc/pull/304,https://github.com/naveenjafer,4,https://github.com/explosion/thinc/pull/304#issuecomment-616970214,"Thinc should have a decent BiLSTM implementation, so we can use it in spaCy. The current one is too slow, so we need to use PyTorch.
For GPU, we can call into CUDNN via cupy, and get fast speeds (although it's not so flexible). It was pretty hard to figure out the API, but I've pretty much worked all of that out (although I'm not ready to push the work on it yet).
The missing piece was a CPU implementation that was compatible with CuDNN (so that you can use a network trained on GPU on CPU). I don't know about full compatibility yet, but the speed is now pretty good.
Benchmark
I ran a quick benchmark with synthetic data against PyTorch on my laptop (at examples/benchmark/lstm_tagger.py). I used three timings each, and picked the best one. Results reported in words per second (higher is better).




Fwd
Fwd+Bwd




Thinc
12.9k
4.0k


PyTorch
17.6k
4.1k



The network size (depth 2, 300d, bidirectional) is about the size we'd be looking to run in spaCy. I set both libraries to single-threaded. I normally get worse performance multi-threading on CPU.
I definitely consider anything better than 50% as fast as PyTorch a victory, especially since this was only a few days work. The backward speed is maybe a bit too good to be true --- possibly I'm making a slight mistake somewhere that could cost accuracy?
Threading, nogil, and blis
The inference loop is almost nogil, so we could make it close to 2x faster by using cython.parallel.prange and using 2 threads. This is sort of a pain in the ass though: it means using openmp, which introduces all sorts of cross-platform headaches. Not worth it.
I used blis.cy.gemm to get matrix multiplication from Cython. This isn't strictly necessary --- we could use numpy instead. The thing is though, to do the implementation efficiently we have to be working on subarrays using inplace operations. That's really messy with numpy --- it's actually a lot clearer and easier with raw pointers. So I would prefer to just add back the blis dependency. We can keep the option of use_blis on NumpyOps, but set it to True.
Variations
There are many variations of LSTMs we can implement as well. For instance, layer norm LSTM is apparently popular now. It's nice to have a decent Cython implementation, so that we know we can do these variants relatively efficiently. However, we need to make sure we have an implementation that remains compatible with CuDNN. Other libraries often end up with two implementations, one CuDNN-compatible, the other not. We might end up with that too.
TODO

 Implement BiLSTM forward pass for NumpyOps
 Implement BiLSTM backward pass for NumpyOps
 Check accuracy against PyTorch using tagger script
 Check speed against PyTorch
 Implement dropout for NumpyOps
 Fix Ops reference implementation
 Add CuDNN implementation for CupyOps
 Verify GPU->serialize->CPU works accurately
 Verify CPU->serialize->GPU works accurately
 Refactor/tidy up","Hi @honnibal I pulled this branch feature/lstm and merged it with the latest master and tried evaluating for LSTM, but looks like there are some issues with other layers. With the current master, PyTorchLSTM is working as expected, but native LSTM doesn't. Once I merged Feature/LSTM to it, both the LSTMs are not working as expected. I believe this happened a few months back too, but a fix that you had put in for list2padded fixed it

Pretty sure I've also found a bug in the list2padded method. Should have a fix shortly.
Originally posted by @honnibal in #301 (comment)",True,{}
explosion/thinc,https://github.com/explosion/thinc,304,2020-02-04T22:54:29Z,2021-01-20T07:40:04Z,2021-01-20T07:40:04Z,CLOSED,False,2973,303,16,https://github.com/honnibal,Faster CPU BiLSTM,105,"['enhancement', 'feat / layers']",https://github.com/explosion/thinc/pull/304,https://github.com/honnibal,5,https://github.com/explosion/thinc/pull/304#issuecomment-763403229,"Thinc should have a decent BiLSTM implementation, so we can use it in spaCy. The current one is too slow, so we need to use PyTorch.
For GPU, we can call into CUDNN via cupy, and get fast speeds (although it's not so flexible). It was pretty hard to figure out the API, but I've pretty much worked all of that out (although I'm not ready to push the work on it yet).
The missing piece was a CPU implementation that was compatible with CuDNN (so that you can use a network trained on GPU on CPU). I don't know about full compatibility yet, but the speed is now pretty good.
Benchmark
I ran a quick benchmark with synthetic data against PyTorch on my laptop (at examples/benchmark/lstm_tagger.py). I used three timings each, and picked the best one. Results reported in words per second (higher is better).




Fwd
Fwd+Bwd




Thinc
12.9k
4.0k


PyTorch
17.6k
4.1k



The network size (depth 2, 300d, bidirectional) is about the size we'd be looking to run in spaCy. I set both libraries to single-threaded. I normally get worse performance multi-threading on CPU.
I definitely consider anything better than 50% as fast as PyTorch a victory, especially since this was only a few days work. The backward speed is maybe a bit too good to be true --- possibly I'm making a slight mistake somewhere that could cost accuracy?
Threading, nogil, and blis
The inference loop is almost nogil, so we could make it close to 2x faster by using cython.parallel.prange and using 2 threads. This is sort of a pain in the ass though: it means using openmp, which introduces all sorts of cross-platform headaches. Not worth it.
I used blis.cy.gemm to get matrix multiplication from Cython. This isn't strictly necessary --- we could use numpy instead. The thing is though, to do the implementation efficiently we have to be working on subarrays using inplace operations. That's really messy with numpy --- it's actually a lot clearer and easier with raw pointers. So I would prefer to just add back the blis dependency. We can keep the option of use_blis on NumpyOps, but set it to True.
Variations
There are many variations of LSTMs we can implement as well. For instance, layer norm LSTM is apparently popular now. It's nice to have a decent Cython implementation, so that we know we can do these variants relatively efficiently. However, we need to make sure we have an implementation that remains compatible with CuDNN. Other libraries often end up with two implementations, one CuDNN-compatible, the other not. We might end up with that too.
TODO

 Implement BiLSTM forward pass for NumpyOps
 Implement BiLSTM backward pass for NumpyOps
 Check accuracy against PyTorch using tagger script
 Check speed against PyTorch
 Implement dropout for NumpyOps
 Fix Ops reference implementation
 Add CuDNN implementation for CupyOps
 Verify GPU->serialize->CPU works accurately
 Verify CPU->serialize->GPU works accurately
 Refactor/tidy up","The CuDNN path I went down here is tough, and I need to spend more time to get it working. I'll close this in preference of getting a simpler non-cudnn approach into the library.",True,{}
explosion/thinc,https://github.com/explosion/thinc,305,2020-02-06T16:23:06Z,2020-02-06T16:53:02Z,2020-02-06T16:54:30Z,MERGED,True,93,93,23,https://github.com/svlandeg,rename ReLu to Relu,1,['feat / layers'],https://github.com/explosion/thinc/pull/305,https://github.com/svlandeg,1,https://github.com/explosion/thinc/pull/305,,,True,{}
explosion/thinc,https://github.com/explosion/thinc,305,2020-02-06T16:23:06Z,2020-02-06T16:53:02Z,2020-02-06T16:54:30Z,MERGED,True,93,93,23,https://github.com/svlandeg,rename ReLu to Relu,1,['feat / layers'],https://github.com/explosion/thinc/pull/305,https://github.com/apps/codecov,2,https://github.com/explosion/thinc/pull/305#issuecomment-582988015,,"Codecov Report

Merging #305 into master will not change coverage.
The diff coverage is 100%.


@@           Coverage Diff           @@
##           master     #305   +/-   ##
=======================================
  Coverage   87.97%   87.97%           
=======================================
  Files          67       67           
  Lines        4334     4334           
=======================================
  Hits         3813     3813           
  Misses        521      521



Impacted Files
Coverage 





thinc/model.py
100% <> ()



thinc/api.py
100% <100%> ()



thinc/layers/__init__.py
100% <100%> ()



thinc/layers/relu.py
100% <100%> ()





Continue to review full report at Codecov.

Legend - Click here to learn more
 = absolute <relative> (impact),  = not affected, ? = missing data
Powered by Codecov. Last update afd9fc5...8233a49. Read the comment docs.",True,{}
explosion/thinc,https://github.com/explosion/thinc,309,2020-02-13T06:09:56Z,2020-05-09T22:59:41Z,2020-05-09T22:59:42Z,CLOSED,False,2,0,1,https://github.com/naveenjafer,fixed breaking bug in Embed,1,['feat / layers'],https://github.com/explosion/thinc/pull/309,https://github.com/naveenjafer,1,https://github.com/explosion/thinc/pull/309,"What was the Bug?
Embed was breaking in the previous version due to indexes out of bounds error. #306
Reproduction
Replace HashEmbed in Example 3 pos_tagger_basic_cnn.ipynb of thinc documentation with Embed.
How was it fixed?
A modulo operation with the vocabulary size was needed to keep it within range (Similar to the implementation in hashEmbed)
Additional Comments
Documentation for Embed needs to be updated and further discussion is needed on the input Parameter nV. It does not make sense to default it to 1. nV typically needs to be the size of the vocabulary of the corpus to reduce collisions.
Why does HashEmbed work with small nV values then?
Given the way the hashing has been implemented in HashEmbed, it technically generates nV^4 different combinations for the word to embedding mapping. Hence, even an nV of 16 was sufficient to cover the entire vocabulary. The same does not happen in Embed, hence the need to set nV to size of the vocab. Even in the case of HashEmbed, if someone sets nV too small such that nV^4 << vocab size then the collisions would be high and model accuracy takes a big hit.
Future Discussions
Why is ids parameter generating such large and sparse numbers in the first place? Why dont we have a continuous mapping of the numbers there? It would do away with the need for a modulo operation. In fact the modulo operation is not entirely reliable in handling collisions.","What was the Bug?
Embed was breaking in the previous version due to indexes out of bounds error. #306
Reproduction
Replace HashEmbed in Example 3 pos_tagger_basic_cnn.ipynb of thinc documentation with Embed.
How was it fixed?
A modulo operation with the vocabulary size was needed to keep it within range (Similar to the implementation in hashEmbed)
Additional Comments
Documentation for Embed needs to be updated and further discussion is needed on the input Parameter nV. It does not make sense to default it to 1. nV typically needs to be the size of the vocabulary of the corpus to reduce collisions.
Why does HashEmbed work with small nV values then?
Given the way the hashing has been implemented in HashEmbed, it technically generates nV^4 different combinations for the word to embedding mapping. Hence, even an nV of 16 was sufficient to cover the entire vocabulary. The same does not happen in Embed, hence the need to set nV to size of the vocab. Even in the case of HashEmbed, if someone sets nV too small such that nV^4 << vocab size then the collisions would be high and model accuracy takes a big hit.
Future Discussions
Why is ids parameter generating such large and sparse numbers in the first place? Why dont we have a continuous mapping of the numbers there? It would do away with the need for a modulo operation. In fact the modulo operation is not entirely reliable in handling collisions.",True,{}
explosion/thinc,https://github.com/explosion/thinc,309,2020-02-13T06:09:56Z,2020-05-09T22:59:41Z,2020-05-09T22:59:42Z,CLOSED,False,2,0,1,https://github.com/naveenjafer,fixed breaking bug in Embed,1,['feat / layers'],https://github.com/explosion/thinc/pull/309,https://github.com/apps/codecov,2,https://github.com/explosion/thinc/pull/309#issuecomment-585569487,"What was the Bug?
Embed was breaking in the previous version due to indexes out of bounds error. #306
Reproduction
Replace HashEmbed in Example 3 pos_tagger_basic_cnn.ipynb of thinc documentation with Embed.
How was it fixed?
A modulo operation with the vocabulary size was needed to keep it within range (Similar to the implementation in hashEmbed)
Additional Comments
Documentation for Embed needs to be updated and further discussion is needed on the input Parameter nV. It does not make sense to default it to 1. nV typically needs to be the size of the vocabulary of the corpus to reduce collisions.
Why does HashEmbed work with small nV values then?
Given the way the hashing has been implemented in HashEmbed, it technically generates nV^4 different combinations for the word to embedding mapping. Hence, even an nV of 16 was sufficient to cover the entire vocabulary. The same does not happen in Embed, hence the need to set nV to size of the vocab. Even in the case of HashEmbed, if someone sets nV too small such that nV^4 << vocab size then the collisions would be high and model accuracy takes a big hit.
Future Discussions
Why is ids parameter generating such large and sparse numbers in the first place? Why dont we have a continuous mapping of the numbers there? It would do away with the need for a modulo operation. In fact the modulo operation is not entirely reliable in handling collisions.","Codecov Report

Merging #309 into master will increase coverage by <.01%.
The diff coverage is 100%.


@@            Coverage Diff             @@
##           master     #309      +/-   ##
==========================================
+ Coverage   87.97%   87.98%   +<.01%     
==========================================
  Files          67       67              
  Lines        4334     4336       +2     
==========================================
+ Hits         3813     3815       +2     
  Misses        521      521



Impacted Files
Coverage 





thinc/layers/embed.py
97.72% <100%> (+0.1%)





Continue to review full report at Codecov.

Legend - Click here to learn more
 = absolute <relative> (impact),  = not affected, ? = missing data
Powered by Codecov. Last update da63e95...ff9105d. Read the comment docs.",True,{}
explosion/thinc,https://github.com/explosion/thinc,309,2020-02-13T06:09:56Z,2020-05-09T22:59:41Z,2020-05-09T22:59:42Z,CLOSED,False,2,0,1,https://github.com/naveenjafer,fixed breaking bug in Embed,1,['feat / layers'],https://github.com/explosion/thinc/pull/309,https://github.com/adrianeboyd,3,https://github.com/explosion/thinc/pull/309#issuecomment-586128684,"What was the Bug?
Embed was breaking in the previous version due to indexes out of bounds error. #306
Reproduction
Replace HashEmbed in Example 3 pos_tagger_basic_cnn.ipynb of thinc documentation with Embed.
How was it fixed?
A modulo operation with the vocabulary size was needed to keep it within range (Similar to the implementation in hashEmbed)
Additional Comments
Documentation for Embed needs to be updated and further discussion is needed on the input Parameter nV. It does not make sense to default it to 1. nV typically needs to be the size of the vocabulary of the corpus to reduce collisions.
Why does HashEmbed work with small nV values then?
Given the way the hashing has been implemented in HashEmbed, it technically generates nV^4 different combinations for the word to embedding mapping. Hence, even an nV of 16 was sufficient to cover the entire vocabulary. The same does not happen in Embed, hence the need to set nV to size of the vocab. Even in the case of HashEmbed, if someone sets nV too small such that nV^4 << vocab size then the collisions would be high and model accuracy takes a big hit.
Future Discussions
Why is ids parameter generating such large and sparse numbers in the first place? Why dont we have a continuous mapping of the numbers there? It would do away with the need for a modulo operation. In fact the modulo operation is not entirely reliable in handling collisions.","Hmm, Embed and HashEmbed aren't really intended to be interchangeable. The docs could definitely be improved here, because it's not clear unless you read the code that strings2array uses hashing rather than some other kind of vocab indexing and as a result, it's only intended as input for a layer like HashEmbed. (The background is that a lot of these layers were developed with spacy in mind, which represents all strings as uint64 hashes internally.)
Embed is intended for things like vocab IDs in a small range like 0..len(vocab) and stores a unique vector for each ID, so it seems acceptable to have errors when nV is too small. It's certainly possible to write a custom embedding layer that uses % to handle IDs outside the range 0..nV, but I think it's probably not a good idea to do this in the default Embed layer. Since nV is a parameter that needs to be provided, having a default of nV=1 as the smallest possible size seems fine.
You can see an example with Embed in https://github.com/explosion/thinc/blob/master/examples/03_textcat_basic_neural_bow.ipynb.",True,{}
explosion/thinc,https://github.com/explosion/thinc,309,2020-02-13T06:09:56Z,2020-05-09T22:59:41Z,2020-05-09T22:59:42Z,CLOSED,False,2,0,1,https://github.com/naveenjafer,fixed breaking bug in Embed,1,['feat / layers'],https://github.com/explosion/thinc/pull/309,https://github.com/naveenjafer,4,https://github.com/explosion/thinc/pull/309#issuecomment-588058342,"What was the Bug?
Embed was breaking in the previous version due to indexes out of bounds error. #306
Reproduction
Replace HashEmbed in Example 3 pos_tagger_basic_cnn.ipynb of thinc documentation with Embed.
How was it fixed?
A modulo operation with the vocabulary size was needed to keep it within range (Similar to the implementation in hashEmbed)
Additional Comments
Documentation for Embed needs to be updated and further discussion is needed on the input Parameter nV. It does not make sense to default it to 1. nV typically needs to be the size of the vocabulary of the corpus to reduce collisions.
Why does HashEmbed work with small nV values then?
Given the way the hashing has been implemented in HashEmbed, it technically generates nV^4 different combinations for the word to embedding mapping. Hence, even an nV of 16 was sufficient to cover the entire vocabulary. The same does not happen in Embed, hence the need to set nV to size of the vocab. Even in the case of HashEmbed, if someone sets nV too small such that nV^4 << vocab size then the collisions would be high and model accuracy takes a big hit.
Future Discussions
Why is ids parameter generating such large and sparse numbers in the first place? Why dont we have a continuous mapping of the numbers there? It would do away with the need for a modulo operation. In fact the modulo operation is not entirely reliable in handling collisions.","@adrianeboyd The separation of HashEmbed and Embed makes sense, but I am trying to understand a use case where a default value of nV=1 would be helpful or even work for that matter. For any input vocab size,  it would simply throw an array index error, right? Typically as developers, default value if available in a parameter is still expected to run the code without errors, and if and when needed, that parameter is tweaked to improve performance/results.",True,{}
explosion/thinc,https://github.com/explosion/thinc,309,2020-02-13T06:09:56Z,2020-05-09T22:59:41Z,2020-05-09T22:59:42Z,CLOSED,False,2,0,1,https://github.com/naveenjafer,fixed breaking bug in Embed,1,['feat / layers'],https://github.com/explosion/thinc/pull/309,https://github.com/honnibal,5,https://github.com/explosion/thinc/pull/309#issuecomment-626246615,"What was the Bug?
Embed was breaking in the previous version due to indexes out of bounds error. #306
Reproduction
Replace HashEmbed in Example 3 pos_tagger_basic_cnn.ipynb of thinc documentation with Embed.
How was it fixed?
A modulo operation with the vocabulary size was needed to keep it within range (Similar to the implementation in hashEmbed)
Additional Comments
Documentation for Embed needs to be updated and further discussion is needed on the input Parameter nV. It does not make sense to default it to 1. nV typically needs to be the size of the vocabulary of the corpus to reduce collisions.
Why does HashEmbed work with small nV values then?
Given the way the hashing has been implemented in HashEmbed, it technically generates nV^4 different combinations for the word to embedding mapping. Hence, even an nV of 16 was sufficient to cover the entire vocabulary. The same does not happen in Embed, hence the need to set nV to size of the vocab. Even in the case of HashEmbed, if someone sets nV too small such that nV^4 << vocab size then the collisions would be high and model accuracy takes a big hit.
Future Discussions
Why is ids parameter generating such large and sparse numbers in the first place? Why dont we have a continuous mapping of the numbers there? It would do away with the need for a modulo operation. In fact the modulo operation is not entirely reliable in handling collisions.","@naveenjafer While it's true that you normally want to create objects in a working state, it's not always possible to do everything at once. This is especially true with the config system, where we want to let the objects get created during the create_from_config pass. In this circumstance, we need the object to get created, and we want the members to have their appropriate types. It's not really wonderful, but it's the least bad solution.",True,{'THUMBS_UP': ['https://github.com/naveenjafer']}
explosion/thinc,https://github.com/explosion/thinc,310,2020-02-14T08:12:03Z,2020-02-14T10:05:19Z,2020-02-14T10:05:19Z,MERGED,True,25,8,2,https://github.com/svlandeg,Fix murmur hash on v7.x,3,['bug'],https://github.com/explosion/thinc/pull/310,https://github.com/svlandeg,1,https://github.com/explosion/thinc/pull/310,"Add fix for GPU/Windows processing from PR #149, so we can include it in the release of spaCy 2.2.4","Add fix for GPU/Windows processing from PR #149, so we can include it in the release of spaCy 2.2.4",True,{}
explosion/thinc,https://github.com/explosion/thinc,311,2020-02-14T08:31:49Z,2020-02-14T10:06:26Z,2020-02-14T10:06:26Z,MERGED,True,5,5,3,https://github.com/adrianeboyd,"Set Embed shape to (nV, nO)",2,"['bug', 'feat / layers']",https://github.com/explosion/thinc/pull/311,https://github.com/adrianeboyd,1,https://github.com/explosion/thinc/pull/311,Use nV rather than nV + 1 to remove all whiffs of OOV handling from Embed.,Use nV rather than nV + 1 to remove all whiffs of OOV handling from Embed.,True,{}
explosion/thinc,https://github.com/explosion/thinc,311,2020-02-14T08:31:49Z,2020-02-14T10:06:26Z,2020-02-14T10:06:26Z,MERGED,True,5,5,3,https://github.com/adrianeboyd,"Set Embed shape to (nV, nO)",2,"['bug', 'feat / layers']",https://github.com/explosion/thinc/pull/311,https://github.com/adrianeboyd,2,https://github.com/explosion/thinc/pull/311#issuecomment-586157011,Use nV rather than nV + 1 to remove all whiffs of OOV handling from Embed.,"It is slightly annoying that nV has to be (max(col) + 1), but I think the alternative is worse?",True,{'THUMBS_UP': ['https://github.com/honnibal']}
explosion/thinc,https://github.com/explosion/thinc,311,2020-02-14T08:31:49Z,2020-02-14T10:06:26Z,2020-02-14T10:06:26Z,MERGED,True,5,5,3,https://github.com/adrianeboyd,"Set Embed shape to (nV, nO)",2,"['bug', 'feat / layers']",https://github.com/explosion/thinc/pull/311,https://github.com/apps/codecov,3,https://github.com/explosion/thinc/pull/311#issuecomment-586157979,Use nV rather than nV + 1 to remove all whiffs of OOV handling from Embed.,"Codecov Report

Merging #311 into master will not change coverage.
The diff coverage is 100%.


@@           Coverage Diff           @@
##           master     #311   +/-   ##
=======================================
  Coverage   87.97%   87.97%           
=======================================
  Files          67       67           
  Lines        4334     4334           
=======================================
  Hits         3813     3813           
  Misses        521      521



Impacted Files
Coverage 





thinc/layers/embed.py
97.61% <100%> ()





Continue to review full report at Codecov.

Legend - Click here to learn more
 = absolute <relative> (impact),  = not affected, ? = missing data
Powered by Codecov. Last update da63e95...8f5cc05. Read the comment docs.",True,{}
explosion/thinc,https://github.com/explosion/thinc,312,2020-02-18T20:16:42Z,2020-02-18T22:40:15Z,2020-02-18T22:40:15Z,MERGED,True,1,1,1,https://github.com/adrianeboyd,Cast scale to float in normal_init() for cupy,1,[],https://github.com/explosion/thinc/pull/312,https://github.com/adrianeboyd,1,https://github.com/explosion/thinc/pull/312,Cast scale to float in normal_init() for cupy.,Cast scale to float in normal_init() for cupy.,True,{}
explosion/thinc,https://github.com/explosion/thinc,312,2020-02-18T20:16:42Z,2020-02-18T22:40:15Z,2020-02-18T22:40:15Z,MERGED,True,1,1,1,https://github.com/adrianeboyd,Cast scale to float in normal_init() for cupy,1,[],https://github.com/explosion/thinc/pull/312,https://github.com/apps/codecov,2,https://github.com/explosion/thinc/pull/312#issuecomment-587797295,Cast scale to float in normal_init() for cupy.,"Codecov Report

Merging #312 into master will not change coverage.
The diff coverage is 100%.


@@           Coverage Diff           @@
##           master     #312   +/-   ##
=======================================
  Coverage   87.97%   87.97%           
=======================================
  Files          67       67           
  Lines        4334     4334           
=======================================
  Hits         3813     3813           
  Misses        521      521



Impacted Files
Coverage 





thinc/initializers.py
100% <100%> ()





Continue to review full report at Codecov.

Legend - Click here to learn more
 = absolute <relative> (impact),  = not affected, ? = missing data
Powered by Codecov. Last update 81f8ae0...39b4cc8. Read the comment docs.",True,{}
explosion/thinc,https://github.com/explosion/thinc,314,2020-02-25T21:45:01Z,2020-03-05T12:44:06Z,2020-03-05T12:44:06Z,MERGED,True,7,1,1,https://github.com/floscha,Example Notebook: Make sure data is on the right device after loading from config,1,['examples'],https://github.com/explosion/thinc/pull/314,https://github.com/floscha,1,https://github.com/explosion/thinc/pull/314,"When running the example notebook 00_intro_to_thinc.ipynb on a Colab GPU runtime, the section that loads the data from config will fail with the following error:
<ipython-input-21-be6cf6a02f24> in <module>()
      5 (train_X, train_Y), (dev_X, dev_Y) = loaded_config[""training""][""data""]
      6 
----> 7 model.initialize(X=train_X[:5], Y=train_Y[:5])
      8 train_model(((train_X, train_Y), (dev_X, dev_Y)), model, optimizer, n_iter, batch_size)

ValueError: Encountered a numpy array when processing with cupy. Did you call model.ops.asarray on your data?

To solve this issue, we simply need to explicitly assure that the data is on the correct device, as done earlier in the notebook when first building the dataset.
Another way could be to implicitly move the data to the right device when calling registry.make_from_config(config) but this is obviously a further stretch than fixing the notebook for now.","When running the example notebook 00_intro_to_thinc.ipynb on a Colab GPU runtime, the section that loads the data from config will fail with the following error:
<ipython-input-21-be6cf6a02f24> in <module>()
      5 (train_X, train_Y), (dev_X, dev_Y) = loaded_config[""training""][""data""]
      6 
----> 7 model.initialize(X=train_X[:5], Y=train_Y[:5])
      8 train_model(((train_X, train_Y), (dev_X, dev_Y)), model, optimizer, n_iter, batch_size)

ValueError: Encountered a numpy array when processing with cupy. Did you call model.ops.asarray on your data?

To solve this issue, we simply need to explicitly assure that the data is on the correct device, as done earlier in the notebook when first building the dataset.
Another way could be to implicitly move the data to the right device when calling registry.make_from_config(config) but this is obviously a further stretch than fixing the notebook for now.",True,{}
explosion/thinc,https://github.com/explosion/thinc,314,2020-02-25T21:45:01Z,2020-03-05T12:44:06Z,2020-03-05T12:44:06Z,MERGED,True,7,1,1,https://github.com/floscha,Example Notebook: Make sure data is on the right device after loading from config,1,['examples'],https://github.com/explosion/thinc/pull/314,https://github.com/svlandeg,2,https://github.com/explosion/thinc/pull/314#issuecomment-593541550,"When running the example notebook 00_intro_to_thinc.ipynb on a Colab GPU runtime, the section that loads the data from config will fail with the following error:
<ipython-input-21-be6cf6a02f24> in <module>()
      5 (train_X, train_Y), (dev_X, dev_Y) = loaded_config[""training""][""data""]
      6 
----> 7 model.initialize(X=train_X[:5], Y=train_Y[:5])
      8 train_model(((train_X, train_Y), (dev_X, dev_Y)), model, optimizer, n_iter, batch_size)

ValueError: Encountered a numpy array when processing with cupy. Did you call model.ops.asarray on your data?

To solve this issue, we simply need to explicitly assure that the data is on the correct device, as done earlier in the notebook when first building the dataset.
Another way could be to implicitly move the data to the right device when calling registry.make_from_config(config) but this is obviously a further stretch than fixing the notebook for now.",The failing CI tests will be fixed by #316,True,{}
explosion/thinc,https://github.com/explosion/thinc,314,2020-02-25T21:45:01Z,2020-03-05T12:44:06Z,2020-03-05T12:44:06Z,MERGED,True,7,1,1,https://github.com/floscha,Example Notebook: Make sure data is on the right device after loading from config,1,['examples'],https://github.com/explosion/thinc/pull/314,https://github.com/floscha,3,https://github.com/explosion/thinc/pull/314#issuecomment-593563729,"When running the example notebook 00_intro_to_thinc.ipynb on a Colab GPU runtime, the section that loads the data from config will fail with the following error:
<ipython-input-21-be6cf6a02f24> in <module>()
      5 (train_X, train_Y), (dev_X, dev_Y) = loaded_config[""training""][""data""]
      6 
----> 7 model.initialize(X=train_X[:5], Y=train_Y[:5])
      8 train_model(((train_X, train_Y), (dev_X, dev_Y)), model, optimizer, n_iter, batch_size)

ValueError: Encountered a numpy array when processing with cupy. Did you call model.ops.asarray on your data?

To solve this issue, we simply need to explicitly assure that the data is on the correct device, as done earlier in the notebook when first building the dataset.
Another way could be to implicitly move the data to the right device when calling registry.make_from_config(config) but this is obviously a further stretch than fixing the notebook for now.",Looks like it. All checks are green now ,True,{}
explosion/thinc,https://github.com/explosion/thinc,314,2020-02-25T21:45:01Z,2020-03-05T12:44:06Z,2020-03-05T12:44:06Z,MERGED,True,7,1,1,https://github.com/floscha,Example Notebook: Make sure data is on the right device after loading from config,1,['examples'],https://github.com/explosion/thinc/pull/314,https://github.com/svlandeg,4,https://github.com/explosion/thinc/pull/314#issuecomment-593564898,"When running the example notebook 00_intro_to_thinc.ipynb on a Colab GPU runtime, the section that loads the data from config will fail with the following error:
<ipython-input-21-be6cf6a02f24> in <module>()
      5 (train_X, train_Y), (dev_X, dev_Y) = loaded_config[""training""][""data""]
      6 
----> 7 model.initialize(X=train_X[:5], Y=train_Y[:5])
      8 train_model(((train_X, train_Y), (dev_X, dev_Y)), model, optimizer, n_iter, batch_size)

ValueError: Encountered a numpy array when processing with cupy. Did you call model.ops.asarray on your data?

To solve this issue, we simply need to explicitly assure that the data is on the correct device, as done earlier in the notebook when first building the dataset.
Another way could be to implicitly move the data to the right device when calling registry.make_from_config(config) but this is obviously a further stretch than fixing the notebook for now.",Perfect. Yes I restarted them :-),True,{}
explosion/thinc,https://github.com/explosion/thinc,314,2020-02-25T21:45:01Z,2020-03-05T12:44:06Z,2020-03-05T12:44:06Z,MERGED,True,7,1,1,https://github.com/floscha,Example Notebook: Make sure data is on the right device after loading from config,1,['examples'],https://github.com/explosion/thinc/pull/314,https://github.com/floscha,5,https://github.com/explosion/thinc/pull/314#issuecomment-593570050,"When running the example notebook 00_intro_to_thinc.ipynb on a Colab GPU runtime, the section that loads the data from config will fail with the following error:
<ipython-input-21-be6cf6a02f24> in <module>()
      5 (train_X, train_Y), (dev_X, dev_Y) = loaded_config[""training""][""data""]
      6 
----> 7 model.initialize(X=train_X[:5], Y=train_Y[:5])
      8 train_model(((train_X, train_Y), (dev_X, dev_Y)), model, optimizer, n_iter, batch_size)

ValueError: Encountered a numpy array when processing with cupy. Did you call model.ops.asarray on your data?

To solve this issue, we simply need to explicitly assure that the data is on the correct device, as done earlier in the notebook when first building the dataset.
Another way could be to implicitly move the data to the right device when calling registry.make_from_config(config) but this is obviously a further stretch than fixing the notebook for now.",Thanks then! :),True,{}
explosion/thinc,https://github.com/explosion/thinc,315,2020-02-27T10:45:18Z,2020-03-05T12:43:40Z,2020-03-05T15:32:10Z,MERGED,True,138,45,4,https://github.com/svlandeg,init instead of _init,2,['enhancement'],https://github.com/explosion/thinc/pull/315,https://github.com/svlandeg,1,https://github.com/explosion/thinc/pull/315,"As we may need to copy/set the Model's init function in downstream applications, renaming from _init to init.
Also expanded / structured the gitignore file, similar as how we're doing it in spaCy. Nothing was removed from the original list.","As we may need to copy/set the Model's init function in downstream applications, renaming from _init to init.
Also expanded / structured the gitignore file, similar as how we're doing it in spaCy. Nothing was removed from the original list.",True,{}
explosion/thinc,https://github.com/explosion/thinc,315,2020-02-27T10:45:18Z,2020-03-05T12:43:40Z,2020-03-05T15:32:10Z,MERGED,True,138,45,4,https://github.com/svlandeg,init instead of _init,2,['enhancement'],https://github.com/explosion/thinc/pull/315,https://github.com/svlandeg,2,https://github.com/explosion/thinc/pull/315#issuecomment-593541492,"As we may need to copy/set the Model's init function in downstream applications, renaming from _init to init.
Also expanded / structured the gitignore file, similar as how we're doing it in spaCy. Nothing was removed from the original list.",The failing CI tests will be fixed by #316,True,{}
explosion/thinc,https://github.com/explosion/thinc,316,2020-02-27T17:42:42Z,2020-03-02T18:18:43Z,2020-11-19T16:22:03Z,MERGED,True,2,2,1,https://github.com/justindujardin,Fix CI build on Windows,4,"['install', 'interop / mxnet']",https://github.com/explosion/thinc/pull/316,https://github.com/justindujardin,1,https://github.com/explosion/thinc/pull/316,Only install mxnet on non-windows platforms where it has a valid wheel. The project specifies a generic wheel for the 1.6.0 release that fails to install on our Windows CI build.,Only install mxnet on non-windows platforms where it has a valid wheel. The project specifies a generic wheel for the 1.6.0 release that fails to install on our Windows CI build.,True,{'THUMBS_UP': ['https://github.com/svlandeg']}
explosion/thinc,https://github.com/explosion/thinc,316,2020-02-27T17:42:42Z,2020-03-02T18:18:43Z,2020-11-19T16:22:03Z,MERGED,True,2,2,1,https://github.com/justindujardin,Fix CI build on Windows,4,"['install', 'interop / mxnet']",https://github.com/explosion/thinc/pull/316,https://github.com/apps/codecov,2,https://github.com/explosion/thinc/pull/316#issuecomment-592089420,Only install mxnet on non-windows platforms where it has a valid wheel. The project specifies a generic wheel for the 1.6.0 release that fails to install on our Windows CI build.,"Codecov Report

Merging #316 into master will not change coverage by %.
The diff coverage is n/a.


@@           Coverage Diff           @@
##           master     #316   +/-   ##
=======================================
  Coverage   87.97%   87.97%           
=======================================
  Files          67       67           
  Lines        4334     4334           
=======================================
  Hits         3813     3813           
  Misses        521      521           

Continue to review full report at Codecov.

Legend - Click here to learn more
 = absolute <relative> (impact),  = not affected, ? = missing data
Powered by Codecov. Last update 504f7b1...870029c. Read the comment docs.",True,{}
explosion/thinc,https://github.com/explosion/thinc,317,2020-03-04T14:42:52Z,2020-03-10T13:50:59Z,2020-03-10T13:52:07Z,MERGED,True,29,15,4,https://github.com/svlandeg,Small fixes,16,"['enhancement', 'feat / layers']",https://github.com/explosion/thinc/pull/317,https://github.com/svlandeg,1,https://github.com/explosion/thinc/pull/317,"While experimenting with spaCy's Tok2Vec layers, I came across a case where a Maxout's model nI needed to be set, but I couldn't easily access it because the actual ""maxout"" model was wrapped in a chain layer for normalization & dropout. In that particular case, dimension inference didn't work either.
So I propose here to have this concept of core layer that allows directly accessing the actual functional layer for a specific model.
[UPDATE]
Decided not to go with this design principle.
Did keep a few small fixes:

avoid setting a dimension on a layer that doesn't have it (e.g. spaCy's CharEmbed doesn't have nI)
add and concatenate only get an nI if their children have one (prevents get_width being called unnecessarily)
removed one return statement that was not doing anything.
added Unserializable to the API","While experimenting with spaCy's Tok2Vec layers, I came across a case where a Maxout's model nI needed to be set, but I couldn't easily access it because the actual ""maxout"" model was wrapped in a chain layer for normalization & dropout. In that particular case, dimension inference didn't work either.
So I propose here to have this concept of core layer that allows directly accessing the actual functional layer for a specific model.
[UPDATE]
Decided not to go with this design principle.
Did keep a few small fixes:

avoid setting a dimension on a layer that doesn't have it (e.g. spaCy's CharEmbed doesn't have nI)
add and concatenate only get an nI if their children have one (prevents get_width being called unnecessarily)
removed one return statement that was not doing anything.
added Unserializable to the API",True,{}
explosion/thinc,https://github.com/explosion/thinc,319,2020-03-07T15:30:19Z,2020-04-13T16:30:35Z,2020-04-13T16:30:35Z,MERGED,True,66,4,3,https://github.com/naveenjafer,Support for additional initializers from keras,3,"['enhancement', 'feat / initializers']",https://github.com/explosion/thinc/pull/319,https://github.com/naveenjafer,1,https://github.com/explosion/thinc/pull/319,"Includes Lecun_Unfiorm, He_Uniform and Glorot_Normal initializers - Reference - https://keras.io/initializers/
Completes -
TODO: Harmonize naming with Keras, and fill in missing entries https://keras.io/initializers/ We should also have He normal/uniform and probably lecun normal/uniform.
Will add Lecun and He Normal once Glorot normal is approved","Includes Lecun_Unfiorm, He_Uniform and Glorot_Normal initializers - Reference - https://keras.io/initializers/
Completes -
TODO: Harmonize naming with Keras, and fill in missing entries https://keras.io/initializers/ We should also have He normal/uniform and probably lecun normal/uniform.
Will add Lecun and He Normal once Glorot normal is approved",True,{'THUMBS_UP': ['https://github.com/svlandeg']}
explosion/thinc,https://github.com/explosion/thinc,319,2020-03-07T15:30:19Z,2020-04-13T16:30:35Z,2020-04-13T16:30:35Z,MERGED,True,66,4,3,https://github.com/naveenjafer,Support for additional initializers from keras,3,"['enhancement', 'feat / initializers']",https://github.com/explosion/thinc/pull/319,https://github.com/honnibal,2,https://github.com/explosion/thinc/pull/319#issuecomment-597547457,"Includes Lecun_Unfiorm, He_Uniform and Glorot_Normal initializers - Reference - https://keras.io/initializers/
Completes -
TODO: Harmonize naming with Keras, and fill in missing entries https://keras.io/initializers/ We should also have He normal/uniform and probably lecun normal/uniform.
Will add Lecun and He Normal once Glorot normal is approved","Thanks a lot for these PRs that fill out these extra pieces of the library!
I want to emphasise that we really appreciate the help. We're trying to keep the library very consistent, so some of our comments might end up nitpicky and the process of merging the PRs might get a little tedious sometimes. Sometimes it might be easier for us to merge first and then make some changes. I hope you won't find this discouraging --- we just want to make sure we keep maintaining consistency.",True,{'THUMBS_UP': ['https://github.com/naveenjafer']}
explosion/thinc,https://github.com/explosion/thinc,319,2020-03-07T15:30:19Z,2020-04-13T16:30:35Z,2020-04-13T16:30:35Z,MERGED,True,66,4,3,https://github.com/naveenjafer,Support for additional initializers from keras,3,"['enhancement', 'feat / initializers']",https://github.com/explosion/thinc/pull/319,https://github.com/naveenjafer,3,https://github.com/explosion/thinc/pull/319#issuecomment-597610356,"Includes Lecun_Unfiorm, He_Uniform and Glorot_Normal initializers - Reference - https://keras.io/initializers/
Completes -
TODO: Harmonize naming with Keras, and fill in missing entries https://keras.io/initializers/ We should also have He normal/uniform and probably lecun normal/uniform.
Will add Lecun and He Normal once Glorot normal is approved","Thanks a lot for these PRs that fill out these extra pieces of the library!
I want to emphasise that we really appreciate the help. We're trying to keep the library very consistent, so some of our comments might end up nitpicky and the process of merging the PRs might get a little tedious sometimes. Sometimes it might be easier for us to merge first and then make some changes. I hope you won't find this discouraging --- we just want to make sure we keep maintaining consistency.

Absolutely, I understand :). The least one can do is be patient when using and contributing to an open source repo.",True,{}
explosion/thinc,https://github.com/explosion/thinc,319,2020-03-07T15:30:19Z,2020-04-13T16:30:35Z,2020-04-13T16:30:35Z,MERGED,True,66,4,3,https://github.com/naveenjafer,Support for additional initializers from keras,3,"['enhancement', 'feat / initializers']",https://github.com/explosion/thinc/pull/319,https://github.com/naveenjafer,4,https://github.com/explosion/thinc/pull/319#issuecomment-597611212,"Includes Lecun_Unfiorm, He_Uniform and Glorot_Normal initializers - Reference - https://keras.io/initializers/
Completes -
TODO: Harmonize naming with Keras, and fill in missing entries https://keras.io/initializers/ We should also have He normal/uniform and probably lecun normal/uniform.
Will add Lecun and He Normal once Glorot normal is approved","I think this looks great! You could add the few missing ones in this PR, that keeps it nicely together :-)

Thank you for the update about black :) . I have added the remaining initializers and black checked the syntax.",True,{'THUMBS_UP': ['https://github.com/svlandeg']}
explosion/thinc,https://github.com/explosion/thinc,319,2020-03-07T15:30:19Z,2020-04-13T16:30:35Z,2020-04-13T16:30:35Z,MERGED,True,66,4,3,https://github.com/naveenjafer,Support for additional initializers from keras,3,"['enhancement', 'feat / initializers']",https://github.com/explosion/thinc/pull/319,https://github.com/svlandeg,5,https://github.com/explosion/thinc/pull/319#issuecomment-597616569,"Includes Lecun_Unfiorm, He_Uniform and Glorot_Normal initializers - Reference - https://keras.io/initializers/
Completes -
TODO: Harmonize naming with Keras, and fill in missing entries https://keras.io/initializers/ We should also have He normal/uniform and probably lecun normal/uniform.
Will add Lecun and He Normal once Glorot normal is approved",Looks good! I think this can be merged if Matt agrees :-),True,{'THUMBS_UP': ['https://github.com/naveenjafer']}
explosion/thinc,https://github.com/explosion/thinc,323,2020-03-10T10:51:35Z,2021-01-21T09:09:40Z,2021-01-21T09:09:40Z,CLOSED,False,75,34,5,https://github.com/naveenjafer,leakyRelu feature as a part of Relu layer. ,7,"['enhancement', 'feat / layers']",https://github.com/explosion/thinc/pull/323,https://github.com/naveenjafer,1,https://github.com/explosion/thinc/pull/323,"This is in line with #322. Have implemented Leaky Relu as a parameterized option inbuilt within Relu. Use the optional parameter ""alphaLeaky"" to use it.","This is in line with #322. Have implemented Leaky Relu as a parameterized option inbuilt within Relu. Use the optional parameter ""alphaLeaky"" to use it.",True,{}
explosion/thinc,https://github.com/explosion/thinc,323,2020-03-10T10:51:35Z,2021-01-21T09:09:40Z,2021-01-21T09:09:40Z,CLOSED,False,75,34,5,https://github.com/naveenjafer,leakyRelu feature as a part of Relu layer. ,7,"['enhancement', 'feat / layers']",https://github.com/explosion/thinc/pull/323,https://github.com/honnibal,2,https://github.com/explosion/thinc/pull/323#issuecomment-597548908,"This is in line with #322. Have implemented Leaky Relu as a parameterized option inbuilt within Relu. Use the optional parameter ""alphaLeaky"" to use it.","Thanks for the PR. I'm thinking about the best way to do this: I'm trying to decide whether we should have the parameter in the op or not.
I think I'd probably prefer a different op for the leaky vs non-leaky relu, instead of a parameter. This also makes the naming easier.",True,{}
explosion/thinc,https://github.com/explosion/thinc,323,2020-03-10T10:51:35Z,2021-01-21T09:09:40Z,2021-01-21T09:09:40Z,CLOSED,False,75,34,5,https://github.com/naveenjafer,leakyRelu feature as a part of Relu layer. ,7,"['enhancement', 'feat / layers']",https://github.com/explosion/thinc/pull/323,https://github.com/honnibal,3,https://github.com/explosion/thinc/pull/323#issuecomment-764491691,"This is in line with #322. Have implemented Leaky Relu as a parameterized option inbuilt within Relu. Use the optional parameter ""alphaLeaky"" to use it.","I do think I'd prefer to have this as a separate operation, instead of a parameter, so I'll close this. I'm sure your implementation will be helpful in adding a leaky relu op though, thanks again ",True,{}
explosion/thinc,https://github.com/explosion/thinc,324,2020-03-10T19:18:02Z,2020-03-12T12:40:59Z,2020-03-12T12:45:39Z,MERGED,True,10,2,1,https://github.com/svlandeg,set nI/nO on chained models,6,"['enhancement', 'feat / layers']",https://github.com/explosion/thinc/pull/324,https://github.com/svlandeg,1,https://github.com/explosion/thinc/pull/324,"Ensure that the nO and nI of a model, that gets chained in its constructor, are set appropriately.
Before this PR, you'd have:
width=64
lower = HashEmbed(nO=width, nV=embed_size, column=cols.index(LOWER))
print(lower.get_dim(""nO""))

returning

Cannot get dimension 'nO' for model 'ints-getitem>>hashembed': value unset

which is counter-intuitive and unhelpful...","Ensure that the nO and nI of a model, that gets chained in its constructor, are set appropriately.
Before this PR, you'd have:
width=64
lower = HashEmbed(nO=width, nV=embed_size, column=cols.index(LOWER))
print(lower.get_dim(""nO""))

returning

Cannot get dimension 'nO' for model 'ints-getitem>>hashembed': value unset

which is counter-intuitive and unhelpful...",True,{}
explosion/thinc,https://github.com/explosion/thinc,324,2020-03-10T19:18:02Z,2020-03-12T12:40:59Z,2020-03-12T12:45:39Z,MERGED,True,10,2,1,https://github.com/svlandeg,set nI/nO on chained models,6,"['enhancement', 'feat / layers']",https://github.com/explosion/thinc/pull/324,https://github.com/honnibal,2,https://github.com/explosion/thinc/pull/324#issuecomment-597546056,"Ensure that the nO and nI of a model, that gets chained in its constructor, are set appropriately.
Before this PR, you'd have:
width=64
lower = HashEmbed(nO=width, nV=embed_size, column=cols.index(LOWER))
print(lower.get_dim(""nO""))

returning

Cannot get dimension 'nO' for model 'ints-getitem>>hashembed': value unset

which is counter-intuitive and unhelpful...","It feels like this would be better fixed in chain, right?
https://github.com/explosion/thinc/blob/master/thinc/layers/chain.py#L20
We should have logic in the init that avoids defining nI and nO if the first and last layers (respectively) don't define them, and which uses their values if their values are there. Something like:
dims = {}
if layers[0].has_dim(""nI"") is not False:
    dims[""nI""] = layers[0].get_dim(""nI"")
if layers[-1].has_dim(""nO"") is not False:
    dims[""nO""] = layers[0].get_dim(""nO"")",True,{}
explosion/thinc,https://github.com/explosion/thinc,324,2020-03-10T19:18:02Z,2020-03-12T12:40:59Z,2020-03-12T12:45:39Z,MERGED,True,10,2,1,https://github.com/svlandeg,set nI/nO on chained models,6,"['enhancement', 'feat / layers']",https://github.com/explosion/thinc/pull/324,https://github.com/svlandeg,3,https://github.com/explosion/thinc/pull/324#issuecomment-597546934,"Ensure that the nO and nI of a model, that gets chained in its constructor, are set appropriately.
Before this PR, you'd have:
width=64
lower = HashEmbed(nO=width, nV=embed_size, column=cols.index(LOWER))
print(lower.get_dim(""nO""))

returning

Cannot get dimension 'nO' for model 'ints-getitem>>hashembed': value unset

which is counter-intuitive and unhelpful...","Yes, ofcourse! In a sense it's very similar to the nI stuff in concatenate.",True,{}
explosion/thinc,https://github.com/explosion/thinc,324,2020-03-10T19:18:02Z,2020-03-12T12:40:59Z,2020-03-12T12:45:39Z,MERGED,True,10,2,1,https://github.com/svlandeg,set nI/nO on chained models,6,"['enhancement', 'feat / layers']",https://github.com/explosion/thinc/pull/324,https://github.com/svlandeg,4,https://github.com/explosion/thinc/pull/324#issuecomment-597569081,"Ensure that the nO and nI of a model, that gets chained in its constructor, are set appropriately.
Before this PR, you'd have:
width=64
lower = HashEmbed(nO=width, nV=embed_size, column=cols.index(LOWER))
print(lower.get_dim(""nO""))

returning

Cannot get dimension 'nO' for model 'ints-getitem>>hashembed': value unset

which is counter-intuitive and unhelpful...","This turned out to be a little less straightforward then I'd hoped. If I remove nI/nO entirely from chain when its first/last layer doesn't have that dimension, many tests in spaCy start breaking :|
I've made a minimal change for now that at least fixes the present issue, as I'm in the middle of a different upgrade in spaCy and don't want things to get too mixed up...",True,{}
explosion/thinc,https://github.com/explosion/thinc,325,2020-03-10T20:50:49Z,2020-03-10T21:15:41Z,2020-03-11T16:21:06Z,MERGED,True,2,2,1,https://github.com/justindujardin,chore: update mac os images on azure,1,"['enhancement', 'install']",https://github.com/explosion/thinc/pull/325,https://github.com/justindujardin,1,https://github.com/explosion/thinc/pull/325,"It looks like Azure is deprecating old 10.13 mac images, so try updating them to 10.15

This job was temporarily blocked because it uses a Microsoft hosted agent (MacOS-10.13) that will be permanently removed on March 23rd, 2020.  See https://aka.ms/blocked-hosted-agent for more information.","It looks like Azure is deprecating old 10.13 mac images, so try updating them to 10.15

This job was temporarily blocked because it uses a Microsoft hosted agent (MacOS-10.13) that will be permanently removed on March 23rd, 2020.  See https://aka.ms/blocked-hosted-agent for more information.",True,{'THUMBS_UP': ['https://github.com/svlandeg']}
explosion/thinc,https://github.com/explosion/thinc,326,2020-03-11T14:49:25Z,2020-03-11T15:50:23Z,2020-03-11T16:19:28Z,MERGED,True,27,2,4,https://github.com/svlandeg,Add softmax activation,2,"['enhancement', 'feat / layers']",https://github.com/explosion/thinc/pull/326,https://github.com/svlandeg,1,https://github.com/explosion/thinc/pull/326,"add softmax_activation (cf https://github.com/explosion/spaCy/blob/master/spacy/_ml.py#L724)
bump to 8.0.0a2","add softmax_activation (cf https://github.com/explosion/spaCy/blob/master/spacy/_ml.py#L724)
bump to 8.0.0a2",True,{}
explosion/thinc,https://github.com/explosion/thinc,327,2020-03-18T11:42:47Z,2020-03-24T19:37:15Z,2020-03-24T19:42:59Z,MERGED,True,61,8,6,https://github.com/svlandeg,with_cpu,9,"['enhancement', 'feat / layers']",https://github.com/explosion/thinc/pull/327,https://github.com/svlandeg,1,https://github.com/explosion/thinc/pull/327,"To accomodate for the spaCy BOW text categorizer, I transferred the with_cpu code from spaCy 2.X to here.
There was trouble with running finish_update though, because the optimizer couldn't deal with the Numpy weights from the layers on CPU. Not sure how that used to work. Probably I'm missing an easier/prettier fix, but for now this seems to work.","To accomodate for the spaCy BOW text categorizer, I transferred the with_cpu code from spaCy 2.X to here.
There was trouble with running finish_update though, because the optimizer couldn't deal with the Numpy weights from the layers on CPU. Not sure how that used to work. Probably I'm missing an easier/prettier fix, but for now this seems to work.",True,{}
explosion/thinc,https://github.com/explosion/thinc,329,2020-03-23T19:44:25Z,2020-03-24T15:29:46Z,2020-03-24T15:33:07Z,MERGED,True,5,3,1,https://github.com/svlandeg,multiply by drop_mask in staticvectors.backprop,2,"['bug', 'feat / layers']",https://github.com/explosion/thinc/pull/329,https://github.com/svlandeg,1,https://github.com/explosion/thinc/pull/329,"Also moved some lines. Not sure this actually ever worked correctly before? cf https://github.com/explosion/thinc/blob/v7.x/thinc/neural/_classes/static_vectors.py#L66 - looks like mask was always None previously?
Anyway this fix does boost spaCy's parser performance by 0.5pp.","Also moved some lines. Not sure this actually ever worked correctly before? cf https://github.com/explosion/thinc/blob/v7.x/thinc/neural/_classes/static_vectors.py#L66 - looks like mask was always None previously?
Anyway this fix does boost spaCy's parser performance by 0.5pp.",True,{}
explosion/thinc,https://github.com/explosion/thinc,329,2020-03-23T19:44:25Z,2020-03-24T15:29:46Z,2020-03-24T15:33:07Z,MERGED,True,5,3,1,https://github.com/svlandeg,multiply by drop_mask in staticvectors.backprop,2,"['bug', 'feat / layers']",https://github.com/explosion/thinc/pull/329,https://github.com/svlandeg,2,https://github.com/explosion/thinc/pull/329#issuecomment-603215244,"Also moved some lines. Not sure this actually ever worked correctly before? cf https://github.com/explosion/thinc/blob/v7.x/thinc/neural/_classes/static_vectors.py#L66 - looks like mask was always None previously?
Anyway this fix does boost spaCy's parser performance by 0.5pp.",Looks like the tests fail on the tensorflow-2.2.0rc1 for Python 3.8 (entirely unrelated to the actual contents of this PR),True,{}
explosion/thinc,https://github.com/explosion/thinc,330,2020-03-24T14:42:22Z,2020-03-24T15:29:21Z,2020-03-24T15:49:03Z,MERGED,True,4,1,1,https://github.com/svlandeg,use cupyx for scatter_add,1,"['bug', 'feat / ops']",https://github.com/explosion/thinc/pull/330,https://github.com/svlandeg,1,https://github.com/explosion/thinc/pull/330,"cupy.scatter_add has been moved to cupyx.scatter_add. It was already deprecated for some time, but now this is actually breaking since cupy-cuda v8.
Fixes explosion/spaCy#5193","cupy.scatter_add has been moved to cupyx.scatter_add. It was already deprecated for some time, but now this is actually breaking since cupy-cuda v8.
Fixes explosion/spaCy#5193",True,{}
explosion/thinc,https://github.com/explosion/thinc,331,2020-03-24T19:51:56Z,2020-03-24T21:54:36Z,2020-03-24T22:35:55Z,MERGED,True,8,4,3,https://github.com/justindujardin,fix: support tensorflow 2.2.0 rc,4,[],https://github.com/explosion/thinc/pull/331,https://github.com/justindujardin,1,https://github.com/explosion/thinc/pull/331,"update tensorflow version pin to include versions < 2.3.0
use public make_train_function or  _make_train_function depending on which is present
rename CupyOps.xp2 to CupyOps._xp2 to satisfy tests","update tensorflow version pin to include versions < 2.3.0
use public make_train_function or  _make_train_function depending on which is present
rename CupyOps.xp2 to CupyOps._xp2 to satisfy tests",True,{'HOORAY': ['https://github.com/svlandeg']}
explosion/thinc,https://github.com/explosion/thinc,332,2020-03-26T09:30:25Z,2020-04-13T13:06:22Z,2020-04-13T13:16:38Z,MERGED,True,16,1,3,https://github.com/svlandeg,Fix CPU/GPU IO by checking the layer before setting the param,4,"['bug', 'feat / layers']",https://github.com/explosion/thinc/pull/332,https://github.com/svlandeg,1,https://github.com/explosion/thinc/pull/332,"If we have a mixed model with CPU & GPU layers, the current IO doesn't work because model.from_bytes converts the read data with msg = convert_recursive(is_xp_array, self.ops.asarray, msg) which results in all-cupy or all-numpy arrays across the model.
We used to have layer-specific conversions though, cf. https://github.com/explosion/thinc/blob/v7.x/thinc/neural/_classes/model.py#L357, but this has gotten a bit more tricky with the current design using convert_recursive.
This PR has a quick hack that does the conversion of a param once more before actually setting it. It feels a little inefficient, because an array may be converted from numpy to cupy and back... But this keeps the code change minimal. Happy to discuss other options!","If we have a mixed model with CPU & GPU layers, the current IO doesn't work because model.from_bytes converts the read data with msg = convert_recursive(is_xp_array, self.ops.asarray, msg) which results in all-cupy or all-numpy arrays across the model.
We used to have layer-specific conversions though, cf. https://github.com/explosion/thinc/blob/v7.x/thinc/neural/_classes/model.py#L357, but this has gotten a bit more tricky with the current design using convert_recursive.
This PR has a quick hack that does the conversion of a param once more before actually setting it. It feels a little inefficient, because an array may be converted from numpy to cupy and back... But this keeps the code change minimal. Happy to discuss other options!",True,{}
explosion/thinc,https://github.com/explosion/thinc,332,2020-03-26T09:30:25Z,2020-04-13T13:06:22Z,2020-04-13T13:16:38Z,MERGED,True,16,1,3,https://github.com/svlandeg,Fix CPU/GPU IO by checking the layer before setting the param,4,"['bug', 'feat / layers']",https://github.com/explosion/thinc/pull/332,https://github.com/svlandeg,2,https://github.com/explosion/thinc/pull/332#issuecomment-604347028,"If we have a mixed model with CPU & GPU layers, the current IO doesn't work because model.from_bytes converts the read data with msg = convert_recursive(is_xp_array, self.ops.asarray, msg) which results in all-cupy or all-numpy arrays across the model.
We used to have layer-specific conversions though, cf. https://github.com/explosion/thinc/blob/v7.x/thinc/neural/_classes/model.py#L357, but this has gotten a bit more tricky with the current design using convert_recursive.
This PR has a quick hack that does the conversion of a param once more before actually setting it. It feels a little inefficient, because an array may be converted from numpy to cupy and back... But this keeps the code change minimal. Happy to discuss other options!","Something is not yet quite right. This extra conversion means that the length of bytes is altered when doing from_bytes.to_bytes in spaCy's roundtrip tests.
[EDIT]: fixed !",True,{}
explosion/thinc,https://github.com/explosion/thinc,333,2020-03-31T20:50:40Z,2020-04-13T16:05:28Z,2020-04-13T16:18:34Z,MERGED,True,82,14,6,https://github.com/justindujardin,Add tests for notebook examples,18,"['tests', 'examples']",https://github.com/explosion/thinc/pull/333,https://github.com/justindujardin,1,https://github.com/explosion/thinc/pull/333,"I think I figured out how we can test our ipython notebooks in the examples folder! The nbconvert package provides a way to execute notebooks
When there's an error it looks like this:

The downside to these tests is that they can be slow, or break if you don't have the right system configuration (e.g. no GPU for config notebook). I put the notebooks that execute quickly in one test, and the others in one that is wrapped with @pytest.mark.slow
Changes

run notebooks files in examples folder and assert that there are no errors in cell outputs
add a slow test for the notebooks that take a long time to run","I think I figured out how we can test our ipython notebooks in the examples folder! The nbconvert package provides a way to execute notebooks
When there's an error it looks like this:

The downside to these tests is that they can be slow, or break if you don't have the right system configuration (e.g. no GPU for config notebook). I put the notebooks that execute quickly in one test, and the others in one that is wrapped with @pytest.mark.slow
Changes

run notebooks files in examples folder and assert that there are no errors in cell outputs
add a slow test for the notebooks that take a long time to run",True,{'HOORAY': ['https://github.com/svlandeg']}
explosion/thinc,https://github.com/explosion/thinc,333,2020-03-31T20:50:40Z,2020-04-13T16:05:28Z,2020-04-13T16:18:34Z,MERGED,True,82,14,6,https://github.com/justindujardin,Add tests for notebook examples,18,"['tests', 'examples']",https://github.com/explosion/thinc/pull/333,https://github.com/justindujardin,2,https://github.com/explosion/thinc/pull/333#issuecomment-607347900,"I think I figured out how we can test our ipython notebooks in the examples folder! The nbconvert package provides a way to execute notebooks
When there's an error it looks like this:

The downside to these tests is that they can be slow, or break if you don't have the right system configuration (e.g. no GPU for config notebook). I put the notebooks that execute quickly in one test, and the others in one that is wrapped with @pytest.mark.slow
Changes

run notebooks files in examples folder and assert that there are no errors in cell outputs
add a slow test for the notebooks that take a long time to run","I wired up the notebook tests to run on Azure (and they pass ), but there are a few bits that should probably be improved:

the ""slow"" notebooks aren't executed during test runs. That means any slow notebooks won't get the benefit of regression checks. We could make the notebooks faster, or make sure to run the ""slow"" tests once before each release?
at least one notebook requires a GPU to execute properly. Are there any systems we can execute GPU tests on? I currently put them in the ""slow"" category.

If we resolved the two above items I think that we could test all of the example notebooks. For now, I suppose a few of them are better than none.",True,{}
explosion/thinc,https://github.com/explosion/thinc,334,2020-04-02T13:06:56Z,2020-04-13T16:29:58Z,2020-04-13T17:31:46Z,MERGED,True,8,7,2,https://github.com/svlandeg,max_grad_norm was renamed to grad_clip,4,"['bug', 'feat / layers']",https://github.com/explosion/thinc/pull/334,https://github.com/svlandeg,1,https://github.com/explosion/thinc/pull/334,Found 2 cases where the renaming of the old max_grad_norm to the new grad_clip was not done yet.,Found 2 cases where the renaming of the old max_grad_norm to the new grad_clip was not done yet.,True,{}
explosion/thinc,https://github.com/explosion/thinc,335,2020-04-13T13:05:05Z,2020-04-13T14:31:50Z,2020-11-19T16:22:09Z,MERGED,True,51,61,10,https://github.com/honnibal,Remove ops attribute from Optimizer,6,[],https://github.com/explosion/thinc/pull/335,https://github.com/honnibal,1,https://github.com/explosion/thinc/pull/335,"The Optimizer object can't assume that all layers are using the same backend, because we might have mixed CPU/GPU execution. So the optimizer needs to fetch an ops appropriate to the inputs.
We remove the ops argument to the optimizer, and add a helper get_array_ops.","The Optimizer object can't assume that all layers are using the same backend, because we might have mixed CPU/GPU execution. So the optimizer needs to fetch an ops appropriate to the inputs.
We remove the ops argument to the optimizer, and add a helper get_array_ops.",True,{}
explosion/thinc,https://github.com/explosion/thinc,335,2020-04-13T13:05:05Z,2020-04-13T14:31:50Z,2020-11-19T16:22:09Z,MERGED,True,51,61,10,https://github.com/honnibal,Remove ops attribute from Optimizer,6,[],https://github.com/explosion/thinc/pull/335,https://github.com/apps/codecov,2,https://github.com/explosion/thinc/pull/335#issuecomment-612899713,"The Optimizer object can't assume that all layers are using the same backend, because we might have mixed CPU/GPU execution. So the optimizer needs to fetch an ops appropriate to the inputs.
We remove the ops argument to the optimizer, and add a helper get_array_ops.","Codecov Report

Merging #335 into master will decrease coverage by 0.02%.
The diff coverage is 91.66%.


@@            Coverage Diff             @@
##           master     #335      +/-   ##
==========================================
- Coverage   87.28%   87.25%   -0.03%     
==========================================
  Files          69       69              
  Lines        4403     4409       +6     
==========================================
+ Hits         3843     3847       +4     
- Misses        560      562       +2     



Impacted Files
Coverage 





thinc/backends/__init__.py
94.59% <71.42%> (-5.41%)



thinc/optimizers.py
94.19% <93.75%> (-0.04%)



thinc/layers/add.py
88.63% <100.00%> ()



thinc/model.py
100.00% <100.00%> ()



thinc/shims/mxnet.py
94.44% <100.00%> (+0.06%)



thinc/shims/pytorch.py
84.09% <100.00%> (+0.18%)



thinc/shims/tensorflow.py
92.17% <100.00%> (+0.04%)





Continue to review full report at Codecov.

Legend - Click here to learn more
 = absolute <relative> (impact),  = not affected, ? = missing data
Powered by Codecov. Last update 0824fc2...e59a277. Read the comment docs.",True,{}
explosion/thinc,https://github.com/explosion/thinc,336,2020-04-14T13:16:31Z,2020-04-20T09:42:24Z,2020-04-20T09:42:24Z,MERGED,True,12,7,3,https://github.com/adrianeboyd,Use 0-vector for OOV in StaticVectors,3,[],https://github.com/explosion/thinc/pull/336,https://github.com/adrianeboyd,1,https://github.com/explosion/thinc/pull/336,Use 0-vector for OOV in StaticVectors,Use 0-vector for OOV in StaticVectors,True,{}
explosion/thinc,https://github.com/explosion/thinc,337,2020-05-03T12:07:19Z,2020-05-09T22:50:05Z,2020-11-19T16:22:18Z,MERGED,True,174,78,19,https://github.com/honnibal,Support missing values and string names in CategoricalCrossentropy loss,35,[],https://github.com/explosion/thinc/pull/337,https://github.com/honnibal,1,https://github.com/explosion/thinc/pull/337,"Edit: Ugh, a bunch of formatting etc changes got dragged in here.
In spaCy we have a bunch of code in the components to calculate the gradient of the loss, including when values are missing and when the gold-standard is provided as string names which need to be decoded into the class IDs.
Let's move that functionality into Thinc, by supporting missing values and names in the CategoricalCrossentropy and SequenceCategoricalCrossentropy handlers. The long-term goal is to move to mostly just assembling spaCy pipeline components out of (model, get_loss, set_annotations) pieces, and have fewer Pipe subclasses.
TODO

 Test the 'names' argument
 Update the SequenceCategoricalCrossentropy class too
 Document the changes","Edit: Ugh, a bunch of formatting etc changes got dragged in here.
In spaCy we have a bunch of code in the components to calculate the gradient of the loss, including when values are missing and when the gold-standard is provided as string names which need to be decoded into the class IDs.
Let's move that functionality into Thinc, by supporting missing values and names in the CategoricalCrossentropy and SequenceCategoricalCrossentropy handlers. The long-term goal is to move to mostly just assembling spaCy pipeline components out of (model, get_loss, set_annotations) pieces, and have fewer Pipe subclasses.
TODO

 Test the 'names' argument
 Update the SequenceCategoricalCrossentropy class too
 Document the changes",True,{'HOORAY': ['https://github.com/justindujardin']}
explosion/thinc,https://github.com/explosion/thinc,337,2020-05-03T12:07:19Z,2020-05-09T22:50:05Z,2020-11-19T16:22:18Z,MERGED,True,174,78,19,https://github.com/honnibal,Support missing values and string names in CategoricalCrossentropy loss,35,[],https://github.com/explosion/thinc/pull/337,https://github.com/apps/codecov,2,https://github.com/explosion/thinc/pull/337#issuecomment-626241650,"Edit: Ugh, a bunch of formatting etc changes got dragged in here.
In spaCy we have a bunch of code in the components to calculate the gradient of the loss, including when values are missing and when the gold-standard is provided as string names which need to be decoded into the class IDs.
Let's move that functionality into Thinc, by supporting missing values and names in the CategoricalCrossentropy and SequenceCategoricalCrossentropy handlers. The long-term goal is to move to mostly just assembling spaCy pipeline components out of (model, get_loss, set_annotations) pieces, and have fewer Pipe subclasses.
TODO

 Test the 'names' argument
 Update the SequenceCategoricalCrossentropy class too
 Document the changes","Codecov Report

Merging #337 into master will not change coverage.
The diff coverage is n/a.


@@           Coverage Diff           @@
##           master     #337   +/-   ##
=======================================
  Coverage   57.08%   57.08%           
=======================================
  Files          99       99           
  Lines        6853     6853           
=======================================
  Hits         3912     3912           
  Misses       2941     2941           

Continue to review full report at Codecov.

Legend - Click here to learn more
 = absolute <relative> (impact),  = not affected, ? = missing data
Powered by Codecov. Last update 603c73b...603c73b. Read the comment docs.",True,{}
explosion/thinc,https://github.com/explosion/thinc,338,2020-05-04T18:04:43Z,2020-05-06T12:29:41Z,2020-05-06T12:56:07Z,MERGED,True,1,1,1,https://github.com/svlandeg,add to_numpy to thinc's api,1,['enhancement'],https://github.com/explosion/thinc/pull/338,https://github.com/svlandeg,1,https://github.com/explosion/thinc/pull/338,^^,^^,True,{}
explosion/thinc,https://github.com/explosion/thinc,338,2020-05-04T18:04:43Z,2020-05-06T12:29:41Z,2020-05-06T12:56:07Z,MERGED,True,1,1,1,https://github.com/svlandeg,add to_numpy to thinc's api,1,['enhancement'],https://github.com/explosion/thinc/pull/338,https://github.com/svlandeg,2,https://github.com/explosion/thinc/pull/338#issuecomment-623708152,^^,"Has something changed with Azure pipelines that suddenly makes the Windows tests crash with this error ?
C:\hostedtoolcache\windows\Python\3.6.8\x64\lib\ctypes\__init__.py:348: in __init__
    self._handle = _dlopen(self._name, mode)
E   OSError: [WinError 126] The specified module could not be found",True,{}
explosion/thinc,https://github.com/explosion/thinc,338,2020-05-04T18:04:43Z,2020-05-06T12:29:41Z,2020-05-06T12:56:07Z,MERGED,True,1,1,1,https://github.com/svlandeg,add to_numpy to thinc's api,1,['enhancement'],https://github.com/explosion/thinc/pull/338,https://github.com/honnibal,3,https://github.com/explosion/thinc/pull/338#issuecomment-624620178,^^,"Has something changed with Azure pipelines that suddenly makes the Windows tests crash with this error ?
C:\hostedtoolcache\windows\Python\3.6.8\x64\lib\ctypes\__init__.py:348: in __init__
    self._handle = _dlopen(self._name, mode)
E   OSError: [WinError 126] The specified module could not be found


Ugh, not sure! Either way it won't be this PR, merging.",True,{}
explosion/thinc,https://github.com/explosion/thinc,340,2020-05-07T18:31:13Z,2020-05-09T00:36:11Z,2020-05-12T14:38:07Z,MERGED,True,16,9,4,https://github.com/justindujardin,fix(to_categorical): make consistent with tf/torch one_hot helpers,3,[],https://github.com/explosion/thinc/pull/340,https://github.com/justindujardin,1,https://github.com/explosion/thinc/pull/340,"The to_categorical helper outputs different shapes than PyTorch and TensorFlow equivalent helpers.
This ""fixes"" it so that Thinc/TF/Torch all agree on the output.

resume testing to_categorical helper (lol at xfail reason)
to_categorical restores the input shape before returning
closes #339","The to_categorical helper outputs different shapes than PyTorch and TensorFlow equivalent helpers.
This ""fixes"" it so that Thinc/TF/Torch all agree on the output.

resume testing to_categorical helper (lol at xfail reason)
to_categorical restores the input shape before returning
closes #339",True,{'HOORAY': ['https://github.com/svlandeg']}
explosion/thinc,https://github.com/explosion/thinc,340,2020-05-07T18:31:13Z,2020-05-09T00:36:11Z,2020-05-12T14:38:07Z,MERGED,True,16,9,4,https://github.com/justindujardin,fix(to_categorical): make consistent with tf/torch one_hot helpers,3,[],https://github.com/explosion/thinc/pull/340,https://github.com/honnibal,2,https://github.com/explosion/thinc/pull/340#issuecomment-625480310,"The to_categorical helper outputs different shapes than PyTorch and TensorFlow equivalent helpers.
This ""fixes"" it so that Thinc/TF/Torch all agree on the output.

resume testing to_categorical helper (lol at xfail reason)
to_categorical restores the input shape before returning
closes #339","Nice, thanks! So is it just the behaviour on higher-dimensional arrays?",True,{}
explosion/thinc,https://github.com/explosion/thinc,340,2020-05-07T18:31:13Z,2020-05-09T00:36:11Z,2020-05-12T14:38:07Z,MERGED,True,16,9,4,https://github.com/justindujardin,fix(to_categorical): make consistent with tf/torch one_hot helpers,3,[],https://github.com/explosion/thinc/pull/340,https://github.com/justindujardin,3,https://github.com/explosion/thinc/pull/340#issuecomment-625491029,"The to_categorical helper outputs different shapes than PyTorch and TensorFlow equivalent helpers.
This ""fixes"" it so that Thinc/TF/Torch all agree on the output.

resume testing to_categorical helper (lol at xfail reason)
to_categorical restores the input shape before returning
closes #339","Yeah exactly, it only matters when you're more than 2d",True,{}
explosion/thinc,https://github.com/explosion/thinc,341,2020-05-08T21:11:08Z,2020-05-09T23:49:27Z,2020-11-19T16:22:24Z,MERGED,True,15,12,1,https://github.com/honnibal,"Update learn rate etc in pytorch shim, if schedule",5,[],https://github.com/explosion/thinc/pull/341,https://github.com/honnibal,1,https://github.com/explosion/thinc/pull/341,"Fixes pretty bad bug: if there were schedules set for learning rate etc, the PyTorch shim wasn't receiving the updated values.
We should update the shims to call into Thinc's optimizer. The current way is pretty brittle :(","Fixes pretty bad bug: if there were schedules set for learning rate etc, the PyTorch shim wasn't receiving the updated values.
We should update the shims to call into Thinc's optimizer. The current way is pretty brittle :(",True,{}
explosion/thinc,https://github.com/explosion/thinc,341,2020-05-08T21:11:08Z,2020-05-09T23:49:27Z,2020-11-19T16:22:24Z,MERGED,True,15,12,1,https://github.com/honnibal,"Update learn rate etc in pytorch shim, if schedule",5,[],https://github.com/explosion/thinc/pull/341,https://github.com/apps/codecov,2,https://github.com/explosion/thinc/pull/341#issuecomment-626249612,"Fixes pretty bad bug: if there were schedules set for learning rate etc, the PyTorch shim wasn't receiving the updated values.
We should update the shims to call into Thinc's optimizer. The current way is pretty brittle :(","Codecov Report

Merging #341 into master will increase coverage by 0.01%.
The diff coverage is 81.81%.


@@            Coverage Diff             @@
##           master     #341      +/-   ##
==========================================
+ Coverage   57.08%   57.09%   +0.01%     
==========================================
  Files          99       99              
  Lines        6853     6853              
==========================================
+ Hits         3912     3913       +1     
+ Misses       2941     2940       -1     



Impacted Files
Coverage 





thinc/shims/pytorch.py
82.29% <81.81%> (+1.04%)





Continue to review full report at Codecov.

Legend - Click here to learn more
 = absolute <relative> (impact),  = not affected, ? = missing data
Powered by Codecov. Last update ee8819a...b2e8f5d. Read the comment docs.",True,{}
explosion/thinc,https://github.com/explosion/thinc,342,2020-05-09T15:32:13Z,2020-05-09T21:41:38Z,2020-05-09T21:41:39Z,CLOSED,False,16,1,2,https://github.com/justindujardin,fix(xp2torch): don't explode if input is already a tensor,1,[],https://github.com/explosion/thinc/pull/342,https://github.com/justindujardin,1,https://github.com/explosion/thinc/pull/342,"With the current behavior, you have to be careful to check the input types before calling xp2torch. With this change, xp2torch returns the tensor directly if given one.
Does this fix look alright? If returning a tensor do I need to do any device checking to ensure the tensor is on the current device?","With the current behavior, you have to be careful to check the input types before calling xp2torch. With this change, xp2torch returns the tensor directly if given one.
Does this fix look alright? If returning a tensor do I need to do any device checking to ensure the tensor is on the current device?",True,{}
explosion/thinc,https://github.com/explosion/thinc,342,2020-05-09T15:32:13Z,2020-05-09T21:41:38Z,2020-05-09T21:41:39Z,CLOSED,False,16,1,2,https://github.com/justindujardin,fix(xp2torch): don't explode if input is already a tensor,1,[],https://github.com/explosion/thinc/pull/342,https://github.com/honnibal,2,https://github.com/explosion/thinc/pull/342#issuecomment-626238186,"With the current behavior, you have to be careful to check the input types before calling xp2torch. With this change, xp2torch returns the tensor directly if given one.
Does this fix look alright? If returning a tensor do I need to do any device checking to ensure the tensor is on the current device?","Hmm, I don't know if this is the direction we want to go in. Maybe we can have a to_torch, similar to to_numpy? I generally want to have an approach where you should know what type you have. xp arrays and torch arrays aren't interchangeable, so your code should generally know which you have.",True,{}
explosion/thinc,https://github.com/explosion/thinc,342,2020-05-09T15:32:13Z,2020-05-09T21:41:38Z,2020-05-09T21:41:39Z,CLOSED,False,16,1,2,https://github.com/justindujardin,fix(xp2torch): don't explode if input is already a tensor,1,[],https://github.com/explosion/thinc/pull/342,https://github.com/justindujardin,3,https://github.com/explosion/thinc/pull/342#issuecomment-626239083,"With the current behavior, you have to be careful to check the input types before calling xp2torch. With this change, xp2torch returns the tensor directly if given one.
Does this fix look alright? If returning a tensor do I need to do any device checking to ensure the tensor is on the current device?","Cool, I don't disagree that you should know what kind of array you have. In practice I found that it was often unclear which kind of array to expect when you're mixing Thinc and Torch pieces.",True,{}
explosion/thinc,https://github.com/explosion/thinc,343,2020-05-09T22:18:03Z,2021-01-21T02:08:39Z,2021-01-21T02:08:39Z,CLOSED,False,142,23,1,https://github.com/honnibal,WIP fix transformer example,1,['examples'],https://github.com/explosion/thinc/pull/343,https://github.com/honnibal,1,https://github.com/explosion/thinc/pull/343,,,True,{}
explosion/thinc,https://github.com/explosion/thinc,343,2020-05-09T22:18:03Z,2021-01-21T02:08:39Z,2021-01-21T02:08:39Z,CLOSED,False,142,23,1,https://github.com/honnibal,WIP fix transformer example,1,['examples'],https://github.com/explosion/thinc/pull/343,https://github.com/honnibal,2,https://github.com/explosion/thinc/pull/343#issuecomment-764183659,,Fixed in another commit.,True,{}
explosion/thinc,https://github.com/explosion/thinc,344,2020-05-11T16:45:59Z,2020-05-13T14:47:27Z,2020-05-13T14:47:27Z,CLOSED,False,105,7,2,https://github.com/justindujardin,"add ""PyTorchCrossEntropy.v1"" loss",2,"['enhancement', 'third-party', 'interop / pytorch']",https://github.com/explosion/thinc/pull/344,https://github.com/justindujardin,1,https://github.com/explosion/thinc/pull/344,"The PyTorch cross_entropy loss uses log softmax + negative-log-likelihood and it performs better on certain tasks than the thinc CCE loss:

I'm not sure if this loss is desirable as part of thinc, but I figured it couldn't hurt to have a PR to look at and talk about.
If this is something thinc would like to keep I can add better tests and document the loss class options.","The PyTorch cross_entropy loss uses log softmax + negative-log-likelihood and it performs better on certain tasks than the thinc CCE loss:

I'm not sure if this loss is desirable as part of thinc, but I figured it couldn't hurt to have a PR to look at and talk about.
If this is something thinc would like to keep I can add better tests and document the loss class options.",True,{}
explosion/thinc,https://github.com/explosion/thinc,344,2020-05-11T16:45:59Z,2020-05-13T14:47:27Z,2020-05-13T14:47:27Z,CLOSED,False,105,7,2,https://github.com/justindujardin,"add ""PyTorchCrossEntropy.v1"" loss",2,"['enhancement', 'third-party', 'interop / pytorch']",https://github.com/explosion/thinc/pull/344,https://github.com/honnibal,2,https://github.com/explosion/thinc/pull/344#issuecomment-627809298,"The PyTorch cross_entropy loss uses log softmax + negative-log-likelihood and it performs better on certain tasks than the thinc CCE loss:

I'm not sure if this loss is desirable as part of thinc, but I figured it couldn't hurt to have a PR to look at and talk about.
If this is something thinc would like to keep I can add better tests and document the loss class options.","I think it's important to notice that this calculates something different from the CCE we have, but I wouldn't want to merge literally this. Like, we don't want one loss that outputs torch.Tensor, and we shouldn't need to call into PyTorch for this. We would need to figure out exactly what it's computing to document it, at which point we'd want to implement it.
Btw I think the point about not returning torch.Tensor is worth emphasising. The library will be in pretty bad shape if we start returning a mix of arrays and torch tensors. The two objects serve very similar purposes, but you can't mix them at all. So we should never be accepting or returning torch tensors, you can convert to the numpy/cupy container without a copy and that's what we should always be doing.",True,{}
explosion/thinc,https://github.com/explosion/thinc,344,2020-05-11T16:45:59Z,2020-05-13T14:47:27Z,2020-05-13T14:47:27Z,CLOSED,False,105,7,2,https://github.com/justindujardin,"add ""PyTorchCrossEntropy.v1"" loss",2,"['enhancement', 'third-party', 'interop / pytorch']",https://github.com/explosion/thinc/pull/344,https://github.com/justindujardin,3,https://github.com/explosion/thinc/pull/344#issuecomment-628040414,"The PyTorch cross_entropy loss uses log softmax + negative-log-likelihood and it performs better on certain tasks than the thinc CCE loss:

I'm not sure if this loss is desirable as part of thinc, but I figured it couldn't hurt to have a PR to look at and talk about.
If this is something thinc would like to keep I can add better tests and document the loss class options.","Like, we don't want one loss that outputs torch.Tensor

Just to be clear, it only uses the torch.Tensor internally, both the get_grad and get_loss functions return numpy arrays (which should have been torch2xp). I felt awkward including the torch import for just the type annotation, but.. I can't help it, I  types so much!

and we shouldn't need to call into PyTorch for this.

Yeah, I understand that, but it's surprisingly difficult to find information disambiguation the various kinds of cross entropy losses. It's even more difficult when you have to also find the correct derivations to calculate the grads manually. I could attempt this, but it would be days or more of research for me to get it all right. 
I'll close this for now ",True,{}
explosion/thinc,https://github.com/explosion/thinc,345,2020-05-11T16:59:08Z,2020-05-13T07:38:24Z,2020-05-13T07:38:24Z,MERGED,True,2,13,2,https://github.com/justindujardin,remove graphviz from azure and skip visualizer tests,1,['tests'],https://github.com/explosion/thinc/pull/345,https://github.com/justindujardin,1,https://github.com/explosion/thinc/pull/345,This should allow us to test the stable notebook tests while skipping the ones that need graphviz,This should allow us to test the stable notebook tests while skipping the ones that need graphviz,True,{}
explosion/thinc,https://github.com/explosion/thinc,346,2020-05-11T19:41:56Z,2020-05-13T16:13:12Z,2020-05-13T16:13:24Z,MERGED,True,83,181,5,https://github.com/justindujardin,Use thinc optimizer in Shim classes,12,"['enhancement', 'interop / tensorflow', 'interop / pytorch', 'interop / mxnet']",https://github.com/explosion/thinc/pull/346,https://github.com/justindujardin,1,https://github.com/explosion/thinc/pull/346,"Update the shim classes to use the thinc optimizers instead of creating framework-specific ones.

 Update tensorflow shim
 Update mxnet shim
 Update torch shim
 Verify torch with cuda
 Verify unique parameter ID use in torch/mxnet","Update the shim classes to use the thinc optimizers instead of creating framework-specific ones.

 Update tensorflow shim
 Update mxnet shim
 Update torch shim
 Verify torch with cuda
 Verify unique parameter ID use in torch/mxnet",True,{}
explosion/thinc,https://github.com/explosion/thinc,346,2020-05-11T19:41:56Z,2020-05-13T16:13:12Z,2020-05-13T16:13:24Z,MERGED,True,83,181,5,https://github.com/justindujardin,Use thinc optimizer in Shim classes,12,"['enhancement', 'interop / tensorflow', 'interop / pytorch', 'interop / mxnet']",https://github.com/explosion/thinc/pull/346,https://github.com/honnibal,2,https://github.com/explosion/thinc/pull/346#issuecomment-627806987,"Update the shim classes to use the thinc optimizers instead of creating framework-specific ones.

 Update tensorflow shim
 Update mxnet shim
 Update torch shim
 Verify torch with cuda
 Verify unique parameter ID use in torch/mxnet",Looking good! Just that naming tweak and then we can go ahead and merge.,True,{}
explosion/thinc,https://github.com/explosion/thinc,347,2020-05-15T11:23:35Z,2020-05-15T15:31:57Z,2020-05-15T16:12:16Z,MERGED,True,6,3,2,https://github.com/svlandeg,fix nI in chain,1,"['bug', 'feat / layers']",https://github.com/explosion/thinc/pull/347,https://github.com/svlandeg,1,https://github.com/explosion/thinc/pull/347,"ensure chained layer only has nI if the first layer has one (this was causing failures in spaCy because it would try to run get_width on a Doc otherwise)
bump to 8.0.0a8","ensure chained layer only has nI if the first layer has one (this was causing failures in spaCy because it would try to run get_width on a Doc otherwise)
bump to 8.0.0a8",True,{'THUMBS_UP': ['https://github.com/honnibal']}
explosion/thinc,https://github.com/explosion/thinc,348,2020-05-18T08:20:38Z,2020-05-19T14:14:21Z,2020-05-19T15:00:38Z,MERGED,True,126,41,9,https://github.com/honnibal,Optimize parameters in bulk,15,[],https://github.com/explosion/thinc/pull/348,https://github.com/honnibal,1,https://github.com/explosion/thinc/pull/348,"Most other libraries' optimizers work on a group of parameters at once. This changes the definition of gradient normalization, which causes an annoying mismatch between our hyper-parameter values and e.g. PyTorch. It probably doesn't matter, but it's another thing for people to worry about. Optimizing lots of small arrays also makes our optimizer quite slow.
Instead, we can merge the parameters and gradients together and just call the optimizer once. The question is whether we should do this within the Optimizer class. Maybe? For now the merging and flattening is duplicated in the shims...
We do create an extra copy of the parameters when we do this, which is a bit unideal, but I don't really see a way to help it.","Most other libraries' optimizers work on a group of parameters at once. This changes the definition of gradient normalization, which causes an annoying mismatch between our hyper-parameter values and e.g. PyTorch. It probably doesn't matter, but it's another thing for people to worry about. Optimizing lots of small arrays also makes our optimizer quite slow.
Instead, we can merge the parameters and gradients together and just call the optimizer once. The question is whether we should do this within the Optimizer class. Maybe? For now the merging and flattening is duplicated in the shims...
We do create an extra copy of the parameters when we do this, which is a bit unideal, but I don't really see a way to help it.",True,{}
explosion/thinc,https://github.com/explosion/thinc,348,2020-05-18T08:20:38Z,2020-05-19T14:14:21Z,2020-05-19T15:00:38Z,MERGED,True,126,41,9,https://github.com/honnibal,Optimize parameters in bulk,15,[],https://github.com/explosion/thinc/pull/348,https://github.com/apps/codecov,2,https://github.com/explosion/thinc/pull/348#issuecomment-630827869,"Most other libraries' optimizers work on a group of parameters at once. This changes the definition of gradient normalization, which causes an annoying mismatch between our hyper-parameter values and e.g. PyTorch. It probably doesn't matter, but it's another thing for people to worry about. Optimizing lots of small arrays also makes our optimizer quite slow.
Instead, we can merge the parameters and gradients together and just call the optimizer once. The question is whether we should do this within the Optimizer class. Maybe? For now the merging and flattening is duplicated in the shims...
We do create an extra copy of the parameters when we do this, which is a bit unideal, but I don't really see a way to help it.","Codecov Report

Merging #348 into master will increase coverage by 0.32%.
The diff coverage is 84.37%.


@@            Coverage Diff             @@
##           master     #348      +/-   ##
==========================================
+ Coverage   56.89%   57.21%   +0.32%     
==========================================
  Files          99       99              
  Lines        6792     6863      +71     
==========================================
+ Hits         3864     3927      +63     
- Misses       2928     2936       +8     



Impacted Files
Coverage 





thinc/backends/ops.py
81.42% <0.00%> (-0.29%)



thinc/shims/shim.py
100.00% <> ()



thinc/tests/layers/test_linear.py
0.00% <0.00%> ()



thinc/tests/layers/test_tensorflow_wrapper.py
0.00% <0.00%> ()



thinc/shims/mxnet.py
94.11% <80.95%> (-4.39%)



thinc/shims/pytorch.py
84.88% <90.47%> (+0.82%)



thinc/model.py
100.00% <100.00%> ()



thinc/shims/tensorflow.py
89.61% <100.00%> (+0.86%)





Continue to review full report at Codecov.

Legend - Click here to learn more
 = absolute <relative> (impact),  = not affected, ? = missing data
Powered by Codecov. Last update 787b7de...e68e626. Read the comment docs.",True,{}
explosion/thinc,https://github.com/explosion/thinc,353,2020-06-05T20:21:12Z,2020-06-26T20:09:06Z,2020-06-26T20:17:59Z,MERGED,True,4,1,1,https://github.com/svlandeg,add informative error message when part of the config can't be parsed,3,"['enhancement', 'feat / ux']",https://github.com/explosion/thinc/pull/353,https://github.com/svlandeg,1,https://github.com/explosion/thinc/pull/353,quick fix to help debugging when messing up formatting the config file ... (debugging for a friend),quick fix to help debugging when messing up formatting the config file ... (debugging for a friend),True,{}
explosion/thinc,https://github.com/explosion/thinc,355,2020-06-23T14:26:28Z,2020-06-25T14:46:55Z,2020-11-19T16:22:29Z,MERGED,True,1,1,1,https://github.com/honnibal,Only enable dropout in training mode,1,[],https://github.com/explosion/thinc/pull/355,https://github.com/honnibal,1,https://github.com/explosion/thinc/pull/355,This was a pretty bad bug! Likely affected some of our spaCy v3 experiments.,This was a pretty bad bug! Likely affected some of our spaCy v3 experiments.,True,{}
explosion/thinc,https://github.com/explosion/thinc,355,2020-06-23T14:26:28Z,2020-06-25T14:46:55Z,2020-11-19T16:22:29Z,MERGED,True,1,1,1,https://github.com/honnibal,Only enable dropout in training mode,1,[],https://github.com/explosion/thinc/pull/355,https://github.com/apps/codecov,2,https://github.com/explosion/thinc/pull/355#issuecomment-648198594,This was a pretty bad bug! Likely affected some of our spaCy v3 experiments.,"Codecov Report

Merging #355 into master will decrease coverage by 0.32%.
The diff coverage is 100.00%.


@@            Coverage Diff             @@
##           master     #355      +/-   ##
==========================================
- Coverage   57.21%   56.89%   -0.33%     
==========================================
  Files          99       99              
  Lines        6863     6863              
==========================================
- Hits         3927     3905      -22     
- Misses       2936     2958      +22     



Impacted Files
Coverage 





thinc/layers/dropout.py
55.10% <100.00%> (-44.90%)





Continue to review full report at Codecov.

Legend - Click here to learn more
 = absolute <relative> (impact),  = not affected, ? = missing data
Powered by Codecov. Last update f7707bb...fcf46cb. Read the comment docs.",True,{}
explosion/thinc,https://github.com/explosion/thinc,356,2020-06-23T14:27:03Z,2020-06-25T14:46:39Z,2020-11-19T16:22:34Z,MERGED,True,1,1,1,https://github.com/honnibal,Enable blis by default,1,[],https://github.com/explosion/thinc/pull/356,https://github.com/honnibal,1,https://github.com/explosion/thinc/pull/356,"Leaving it off by default is a big hassle, and caused some bad performance in spaCy that was hard to track down.","Leaving it off by default is a big hassle, and caused some bad performance in spaCy that was hard to track down.",True,{}
explosion/thinc,https://github.com/explosion/thinc,356,2020-06-23T14:27:03Z,2020-06-25T14:46:39Z,2020-11-19T16:22:34Z,MERGED,True,1,1,1,https://github.com/honnibal,Enable blis by default,1,[],https://github.com/explosion/thinc/pull/356,https://github.com/apps/codecov,2,https://github.com/explosion/thinc/pull/356#issuecomment-648199103,"Leaving it off by default is a big hassle, and caused some bad performance in spaCy that was hard to track down.","Codecov Report

Merging #356 into master will not change coverage.
The diff coverage is n/a.


@@           Coverage Diff           @@
##           master     #356   +/-   ##
=======================================
  Coverage   57.21%   57.21%           
=======================================
  Files          99       99           
  Lines        6863     6863           
=======================================
  Hits         3927     3927           
  Misses       2936     2936           

Continue to review full report at Codecov.

Legend - Click here to learn more
 = absolute <relative> (impact),  = not affected, ? = missing data
Powered by Codecov. Last update f7707bb...1266f53. Read the comment docs.",True,{}
explosion/thinc,https://github.com/explosion/thinc,357,2020-06-30T13:54:03Z,2020-07-10T13:23:28Z,2020-07-10T13:23:28Z,MERGED,True,6,6,1,https://github.com/adrianeboyd,Update textcat example to thinc==8.0.0a9,1,"['bug', 'examples']",https://github.com/explosion/thinc/pull/357,https://github.com/adrianeboyd,1,https://github.com/explosion/thinc/pull/357,"Updates vocab size and model definition for changes since 8.0.0a1.
Fixes #354.","Updates vocab size and model definition for changes since 8.0.0a1.
Fixes #354.",True,{}
explosion/thinc,https://github.com/explosion/thinc,357,2020-06-30T13:54:03Z,2020-07-10T13:23:28Z,2020-07-10T13:23:28Z,MERGED,True,6,6,1,https://github.com/adrianeboyd,Update textcat example to thinc==8.0.0a9,1,"['bug', 'examples']",https://github.com/explosion/thinc/pull/357,https://github.com/apps/codecov,2,https://github.com/explosion/thinc/pull/357#issuecomment-651811799,"Updates vocab size and model definition for changes since 8.0.0a1.
Fixes #354.","Codecov Report

Merging #357 into master will not change coverage.
The diff coverage is n/a.


@@           Coverage Diff           @@
##           master     #357   +/-   ##
=======================================
  Coverage   56.89%   56.89%           
=======================================
  Files          99       99           
  Lines        6867     6867           
=======================================
  Hits         3907     3907           
  Misses       2960     2960           

Continue to review full report at Codecov.

Legend - Click here to learn more
 = absolute <relative> (impact),  = not affected, ? = missing data
Powered by Codecov. Last update b997222...47bed53. Read the comment docs.",True,{}
explosion/thinc,https://github.com/explosion/thinc,359,2020-07-09T12:12:43Z,2020-07-09T19:04:16Z,2020-07-31T11:51:39Z,CLOSED,False,27,1,2,https://github.com/svlandeg,Fixing random seed to ensure reproducibility,8,"['bug', 'feat / ops']",https://github.com/explosion/thinc/pull/359,https://github.com/svlandeg,1,https://github.com/explosion/thinc/pull/359,"WIP
Original post, which was quite wrong, below in collapsible section to avoid future confusion...

Background
It looks like we haven't really been supporting fix_random_seed well. This function would call random.seed(value), but the problem is that this call needs to be repeated in between generating random numbers:
random.seed(666)
print(""getting int 0: "", random.randint(0, 100000))
print(""getting int 1: "", random.randint(0, 100000))
print(""getting int 2: "", random.randint(0, 100000))

Gives

getting int 0:  59784
getting int 1:  49417
getting int 2:  56934

But in contrast,
random.seed(666)
print(""getting int 0: "", random.randint(0, 100000))
random.seed(666)
print(""getting int 1: "", random.randint(0, 100000))
random.seed(666)
print(""getting int 2: "", random.randint(0, 100000))

Gives

getting int 0:  59784
getting int 1:  59784
getting int 2:  59784

Note, btw, that this is also a problem for list comprehensions:
random.seed(666)
print(""getting int 666: "", [random.randint(0, 100000) for _ in range(10)])


getting int 666:  [59784, 49417, 56934, 37158, 65618, 1864, 71792, 72571, 93346, 43664]

Fix
We can't expect the user to call fix_random_seed multiple times, because internally we may be generating multiple random numbers in between atomic calls for the user. So we need to do this within Thinc and we need to manage it carefully so we call ensure_fixed_seed EVERY TIME before we generate something random.
Currently this PR solves this with a global variable INTERNAL_SEED in thinc.util. I'm not too happy about that, and worry about consequences for parallellization. But if multiple threads/workers need to reproduce the same results, there will be no other option then to make sure they are initialized with the same seed.
Added also the functionality to remove_random_seed.
Finally, HashEmbed needs to also use this seed if it's available and is not overwritten in its constructor. The new tests in test_hash_embed show the different possibilities. Many of these would be failing on the current master branch.
I was also able to reproduce the original bug with spaCy, and after this fix, those issues seem solved. Will submit the new unit tests for spaCy on develop there.","WIP
Original post, which was quite wrong, below in collapsible section to avoid future confusion...

Background
It looks like we haven't really been supporting fix_random_seed well. This function would call random.seed(value), but the problem is that this call needs to be repeated in between generating random numbers:
random.seed(666)
print(""getting int 0: "", random.randint(0, 100000))
print(""getting int 1: "", random.randint(0, 100000))
print(""getting int 2: "", random.randint(0, 100000))

Gives

getting int 0:  59784
getting int 1:  49417
getting int 2:  56934

But in contrast,
random.seed(666)
print(""getting int 0: "", random.randint(0, 100000))
random.seed(666)
print(""getting int 1: "", random.randint(0, 100000))
random.seed(666)
print(""getting int 2: "", random.randint(0, 100000))

Gives

getting int 0:  59784
getting int 1:  59784
getting int 2:  59784

Note, btw, that this is also a problem for list comprehensions:
random.seed(666)
print(""getting int 666: "", [random.randint(0, 100000) for _ in range(10)])


getting int 666:  [59784, 49417, 56934, 37158, 65618, 1864, 71792, 72571, 93346, 43664]

Fix
We can't expect the user to call fix_random_seed multiple times, because internally we may be generating multiple random numbers in between atomic calls for the user. So we need to do this within Thinc and we need to manage it carefully so we call ensure_fixed_seed EVERY TIME before we generate something random.
Currently this PR solves this with a global variable INTERNAL_SEED in thinc.util. I'm not too happy about that, and worry about consequences for parallellization. But if multiple threads/workers need to reproduce the same results, there will be no other option then to make sure they are initialized with the same seed.
Added also the functionality to remove_random_seed.
Finally, HashEmbed needs to also use this seed if it's available and is not overwritten in its constructor. The new tests in test_hash_embed show the different possibilities. Many of these would be failing on the current master branch.
I was also able to reproduce the original bug with spaCy, and after this fix, those issues seem solved. Will submit the new unit tests for spaCy on develop there.",True,{}
explosion/thinc,https://github.com/explosion/thinc,359,2020-07-09T12:12:43Z,2020-07-09T19:04:16Z,2020-07-31T11:51:39Z,CLOSED,False,27,1,2,https://github.com/svlandeg,Fixing random seed to ensure reproducibility,8,"['bug', 'feat / ops']",https://github.com/explosion/thinc/pull/359,https://github.com/apps/codecov,2,https://github.com/explosion/thinc/pull/359#issuecomment-656096076,"WIP
Original post, which was quite wrong, below in collapsible section to avoid future confusion...

Background
It looks like we haven't really been supporting fix_random_seed well. This function would call random.seed(value), but the problem is that this call needs to be repeated in between generating random numbers:
random.seed(666)
print(""getting int 0: "", random.randint(0, 100000))
print(""getting int 1: "", random.randint(0, 100000))
print(""getting int 2: "", random.randint(0, 100000))

Gives

getting int 0:  59784
getting int 1:  49417
getting int 2:  56934

But in contrast,
random.seed(666)
print(""getting int 0: "", random.randint(0, 100000))
random.seed(666)
print(""getting int 1: "", random.randint(0, 100000))
random.seed(666)
print(""getting int 2: "", random.randint(0, 100000))

Gives

getting int 0:  59784
getting int 1:  59784
getting int 2:  59784

Note, btw, that this is also a problem for list comprehensions:
random.seed(666)
print(""getting int 666: "", [random.randint(0, 100000) for _ in range(10)])


getting int 666:  [59784, 49417, 56934, 37158, 65618, 1864, 71792, 72571, 93346, 43664]

Fix
We can't expect the user to call fix_random_seed multiple times, because internally we may be generating multiple random numbers in between atomic calls for the user. So we need to do this within Thinc and we need to manage it carefully so we call ensure_fixed_seed EVERY TIME before we generate something random.
Currently this PR solves this with a global variable INTERNAL_SEED in thinc.util. I'm not too happy about that, and worry about consequences for parallellization. But if multiple threads/workers need to reproduce the same results, there will be no other option then to make sure they are initialized with the same seed.
Added also the functionality to remove_random_seed.
Finally, HashEmbed needs to also use this seed if it's available and is not overwritten in its constructor. The new tests in test_hash_embed show the different possibilities. Many of these would be failing on the current master branch.
I was also able to reproduce the original bug with spaCy, and after this fix, those issues seem solved. Will submit the new unit tests for spaCy on develop there.","Codecov Report

Merging #359 into master will decrease coverage by 0.14%.
The diff coverage is 0.00%.


@@            Coverage Diff             @@
##           master     #359      +/-   ##
==========================================
- Coverage   56.67%   56.52%   -0.15%     
==========================================
  Files          99       99              
  Lines        6832     6850      +18     
==========================================
  Hits         3872     3872              
- Misses       2960     2978      +18     



Impacted Files
Coverage 





thinc/layers/hashembed.py
98.00% <> ()



thinc/tests/layers/test_hash_embed.py
0.00% <0.00%> ()





Continue to review full report at Codecov.

Legend - Click here to learn more
 = absolute <relative> (impact),  = not affected, ? = missing data
Powered by Codecov. Last update 9a0936f...d64a3e9. Read the comment docs.",True,{}
explosion/thinc,https://github.com/explosion/thinc,359,2020-07-09T12:12:43Z,2020-07-09T19:04:16Z,2020-07-31T11:51:39Z,CLOSED,False,27,1,2,https://github.com/svlandeg,Fixing random seed to ensure reproducibility,8,"['bug', 'feat / ops']",https://github.com/explosion/thinc/pull/359,https://github.com/honnibal,3,https://github.com/explosion/thinc/pull/359#issuecomment-656115976,"WIP
Original post, which was quite wrong, below in collapsible section to avoid future confusion...

Background
It looks like we haven't really been supporting fix_random_seed well. This function would call random.seed(value), but the problem is that this call needs to be repeated in between generating random numbers:
random.seed(666)
print(""getting int 0: "", random.randint(0, 100000))
print(""getting int 1: "", random.randint(0, 100000))
print(""getting int 2: "", random.randint(0, 100000))

Gives

getting int 0:  59784
getting int 1:  49417
getting int 2:  56934

But in contrast,
random.seed(666)
print(""getting int 0: "", random.randint(0, 100000))
random.seed(666)
print(""getting int 1: "", random.randint(0, 100000))
random.seed(666)
print(""getting int 2: "", random.randint(0, 100000))

Gives

getting int 0:  59784
getting int 1:  59784
getting int 2:  59784

Note, btw, that this is also a problem for list comprehensions:
random.seed(666)
print(""getting int 666: "", [random.randint(0, 100000) for _ in range(10)])


getting int 666:  [59784, 49417, 56934, 37158, 65618, 1864, 71792, 72571, 93346, 43664]

Fix
We can't expect the user to call fix_random_seed multiple times, because internally we may be generating multiple random numbers in between atomic calls for the user. So we need to do this within Thinc and we need to manage it carefully so we call ensure_fixed_seed EVERY TIME before we generate something random.
Currently this PR solves this with a global variable INTERNAL_SEED in thinc.util. I'm not too happy about that, and worry about consequences for parallellization. But if multiple threads/workers need to reproduce the same results, there will be no other option then to make sure they are initialized with the same seed.
Added also the functionality to remove_random_seed.
Finally, HashEmbed needs to also use this seed if it's available and is not overwritten in its constructor. The new tests in test_hash_embed show the different possibilities. Many of these would be failing on the current master branch.
I was also able to reproduce the original bug with spaCy, and after this fix, those issues seem solved. Will submit the new unit tests for spaCy on develop there.","It looks like we haven't really been supporting fix_random_seed well. This function would call random.seed(value), but the problem is that this call needs to be repeated in between generating random numbers:

Maybe I'm missing your intention here, but this is what I expected, and I'm not sure the current code is wrong in this respect? Apologies if I've gotten this all wrong, but I think you've tripped yourself up here.
I think the way you've got this working, every time you called dropout for a given size and shape, you'd get the same result (because you're resetting the seed each time). That's not what we want. We want the values to be the same across runs, but within a run, to have no particular correlation. So we want to seed the RNG once at the beginning of execution, we don't want to keep resetting it like this.
The idea is that when you start the program, the RNG starts in an arbitrary state...and then we seed the RNG to put it in the same state each time. If we then call the RNG a deterministic number of times in a deterministic order, we should see the same results between runs.
Note btw that the HashEmbed uses its seed in a different way. The seed there is for its hash function, rather than the RNG.",True,{}
explosion/thinc,https://github.com/explosion/thinc,359,2020-07-09T12:12:43Z,2020-07-09T19:04:16Z,2020-07-31T11:51:39Z,CLOSED,False,27,1,2,https://github.com/svlandeg,Fixing random seed to ensure reproducibility,8,"['bug', 'feat / ops']",https://github.com/explosion/thinc/pull/359,https://github.com/svlandeg,4,https://github.com/explosion/thinc/pull/359#issuecomment-656125495,"WIP
Original post, which was quite wrong, below in collapsible section to avoid future confusion...

Background
It looks like we haven't really been supporting fix_random_seed well. This function would call random.seed(value), but the problem is that this call needs to be repeated in between generating random numbers:
random.seed(666)
print(""getting int 0: "", random.randint(0, 100000))
print(""getting int 1: "", random.randint(0, 100000))
print(""getting int 2: "", random.randint(0, 100000))

Gives

getting int 0:  59784
getting int 1:  49417
getting int 2:  56934

But in contrast,
random.seed(666)
print(""getting int 0: "", random.randint(0, 100000))
random.seed(666)
print(""getting int 1: "", random.randint(0, 100000))
random.seed(666)
print(""getting int 2: "", random.randint(0, 100000))

Gives

getting int 0:  59784
getting int 1:  59784
getting int 2:  59784

Note, btw, that this is also a problem for list comprehensions:
random.seed(666)
print(""getting int 666: "", [random.randint(0, 100000) for _ in range(10)])


getting int 666:  [59784, 49417, 56934, 37158, 65618, 1864, 71792, 72571, 93346, 43664]

Fix
We can't expect the user to call fix_random_seed multiple times, because internally we may be generating multiple random numbers in between atomic calls for the user. So we need to do this within Thinc and we need to manage it carefully so we call ensure_fixed_seed EVERY TIME before we generate something random.
Currently this PR solves this with a global variable INTERNAL_SEED in thinc.util. I'm not too happy about that, and worry about consequences for parallellization. But if multiple threads/workers need to reproduce the same results, there will be no other option then to make sure they are initialized with the same seed.
Added also the functionality to remove_random_seed.
Finally, HashEmbed needs to also use this seed if it's available and is not overwritten in its constructor. The new tests in test_hash_embed show the different possibilities. Many of these would be failing on the current master branch.
I was also able to reproduce the original bug with spaCy, and after this fix, those issues seem solved. Will submit the new unit tests for spaCy on develop there.","Ugh, yea you're right ofcourse. Let me poke at this further.",True,{}
explosion/thinc,https://github.com/explosion/thinc,359,2020-07-09T12:12:43Z,2020-07-09T19:04:16Z,2020-07-31T11:51:39Z,CLOSED,False,27,1,2,https://github.com/svlandeg,Fixing random seed to ensure reproducibility,8,"['bug', 'feat / ops']",https://github.com/explosion/thinc/pull/359,https://github.com/svlandeg,5,https://github.com/explosion/thinc/pull/359#issuecomment-656141124,"WIP
Original post, which was quite wrong, below in collapsible section to avoid future confusion...

Background
It looks like we haven't really been supporting fix_random_seed well. This function would call random.seed(value), but the problem is that this call needs to be repeated in between generating random numbers:
random.seed(666)
print(""getting int 0: "", random.randint(0, 100000))
print(""getting int 1: "", random.randint(0, 100000))
print(""getting int 2: "", random.randint(0, 100000))

Gives

getting int 0:  59784
getting int 1:  49417
getting int 2:  56934

But in contrast,
random.seed(666)
print(""getting int 0: "", random.randint(0, 100000))
random.seed(666)
print(""getting int 1: "", random.randint(0, 100000))
random.seed(666)
print(""getting int 2: "", random.randint(0, 100000))

Gives

getting int 0:  59784
getting int 1:  59784
getting int 2:  59784

Note, btw, that this is also a problem for list comprehensions:
random.seed(666)
print(""getting int 666: "", [random.randint(0, 100000) for _ in range(10)])


getting int 666:  [59784, 49417, 56934, 37158, 65618, 1864, 71792, 72571, 93346, 43664]

Fix
We can't expect the user to call fix_random_seed multiple times, because internally we may be generating multiple random numbers in between atomic calls for the user. So we need to do this within Thinc and we need to manage it carefully so we call ensure_fixed_seed EVERY TIME before we generate something random.
Currently this PR solves this with a global variable INTERNAL_SEED in thinc.util. I'm not too happy about that, and worry about consequences for parallellization. But if multiple threads/workers need to reproduce the same results, there will be no other option then to make sure they are initialized with the same seed.
Added also the functionality to remove_random_seed.
Finally, HashEmbed needs to also use this seed if it's available and is not overwritten in its constructor. The new tests in test_hash_embed show the different possibilities. Many of these would be failing on the current master branch.
I was also able to reproduce the original bug with spaCy, and after this fix, those issues seem solved. Will submit the new unit tests for spaCy on develop there.","So I do think the variation stems from the HashEmbed's seed though?
By calling fix_random_seed, HashEmbed gets the same fixed vectors initialized, but because of the difference in hashing, the output result differs, and this will influence the next layers?",True,{}
explosion/thinc,https://github.com/explosion/thinc,359,2020-07-09T12:12:43Z,2020-07-09T19:04:16Z,2020-07-31T11:51:39Z,CLOSED,False,27,1,2,https://github.com/svlandeg,Fixing random seed to ensure reproducibility,8,"['bug', 'feat / ops']",https://github.com/explosion/thinc/pull/359,https://github.com/honnibal,6,https://github.com/explosion/thinc/pull/359#issuecomment-656143224,"WIP
Original post, which was quite wrong, below in collapsible section to avoid future confusion...

Background
It looks like we haven't really been supporting fix_random_seed well. This function would call random.seed(value), but the problem is that this call needs to be repeated in between generating random numbers:
random.seed(666)
print(""getting int 0: "", random.randint(0, 100000))
print(""getting int 1: "", random.randint(0, 100000))
print(""getting int 2: "", random.randint(0, 100000))

Gives

getting int 0:  59784
getting int 1:  49417
getting int 2:  56934

But in contrast,
random.seed(666)
print(""getting int 0: "", random.randint(0, 100000))
random.seed(666)
print(""getting int 1: "", random.randint(0, 100000))
random.seed(666)
print(""getting int 2: "", random.randint(0, 100000))

Gives

getting int 0:  59784
getting int 1:  59784
getting int 2:  59784

Note, btw, that this is also a problem for list comprehensions:
random.seed(666)
print(""getting int 666: "", [random.randint(0, 100000) for _ in range(10)])


getting int 666:  [59784, 49417, 56934, 37158, 65618, 1864, 71792, 72571, 93346, 43664]

Fix
We can't expect the user to call fix_random_seed multiple times, because internally we may be generating multiple random numbers in between atomic calls for the user. So we need to do this within Thinc and we need to manage it carefully so we call ensure_fixed_seed EVERY TIME before we generate something random.
Currently this PR solves this with a global variable INTERNAL_SEED in thinc.util. I'm not too happy about that, and worry about consequences for parallellization. But if multiple threads/workers need to reproduce the same results, there will be no other option then to make sure they are initialized with the same seed.
Added also the functionality to remove_random_seed.
Finally, HashEmbed needs to also use this seed if it's available and is not overwritten in its constructor. The new tests in test_hash_embed show the different possibilities. Many of these would be failing on the current master branch.
I was also able to reproduce the original bug with spaCy, and after this fix, those issues seem solved. Will submit the new unit tests for spaCy on develop there.","Each HashEmbed table should get a different seed from each other, so that if you have a short word its NORM, PREFIX and SUFFIX vectors end up different from each other. I achieve this by using the model.id as the hash-embed table's seed if none is provided, because that id should be global.
This does have an annoyance though, in that you'll get different results if you run twice within the same process. I actually pushed a change to the Tok2Vec recently that simply sets the seeds explicitly, with values 0-4. But the results are still not reproducible after that.",True,{}
explosion/thinc,https://github.com/explosion/thinc,359,2020-07-09T12:12:43Z,2020-07-09T19:04:16Z,2020-07-31T11:51:39Z,CLOSED,False,27,1,2,https://github.com/svlandeg,Fixing random seed to ensure reproducibility,8,"['bug', 'feat / ops']",https://github.com/explosion/thinc/pull/359,https://github.com/svlandeg,7,https://github.com/explosion/thinc/pull/359#issuecomment-656155530,"WIP
Original post, which was quite wrong, below in collapsible section to avoid future confusion...

Background
It looks like we haven't really been supporting fix_random_seed well. This function would call random.seed(value), but the problem is that this call needs to be repeated in between generating random numbers:
random.seed(666)
print(""getting int 0: "", random.randint(0, 100000))
print(""getting int 1: "", random.randint(0, 100000))
print(""getting int 2: "", random.randint(0, 100000))

Gives

getting int 0:  59784
getting int 1:  49417
getting int 2:  56934

But in contrast,
random.seed(666)
print(""getting int 0: "", random.randint(0, 100000))
random.seed(666)
print(""getting int 1: "", random.randint(0, 100000))
random.seed(666)
print(""getting int 2: "", random.randint(0, 100000))

Gives

getting int 0:  59784
getting int 1:  59784
getting int 2:  59784

Note, btw, that this is also a problem for list comprehensions:
random.seed(666)
print(""getting int 666: "", [random.randint(0, 100000) for _ in range(10)])


getting int 666:  [59784, 49417, 56934, 37158, 65618, 1864, 71792, 72571, 93346, 43664]

Fix
We can't expect the user to call fix_random_seed multiple times, because internally we may be generating multiple random numbers in between atomic calls for the user. So we need to do this within Thinc and we need to manage it carefully so we call ensure_fixed_seed EVERY TIME before we generate something random.
Currently this PR solves this with a global variable INTERNAL_SEED in thinc.util. I'm not too happy about that, and worry about consequences for parallellization. But if multiple threads/workers need to reproduce the same results, there will be no other option then to make sure they are initialized with the same seed.
Added also the functionality to remove_random_seed.
Finally, HashEmbed needs to also use this seed if it's available and is not overwritten in its constructor. The new tests in test_hash_embed show the different possibilities. Many of these would be failing on the current master branch.
I was also able to reproduce the original bug with spaCy, and after this fix, those issues seem solved. Will submit the new unit tests for spaCy on develop there.","Ah, that would explain why the spaCy reproducibility tests (explosion/spaCy#5735) work for Tok2vec but not for the default textcat architecture, which uses its own embedded tok2vec architecture which doesn't set these seeds specifically? (all of that needs to be cleaned up but that's a different matter)",True,{}
explosion/thinc,https://github.com/explosion/thinc,359,2020-07-09T12:12:43Z,2020-07-09T19:04:16Z,2020-07-31T11:51:39Z,CLOSED,False,27,1,2,https://github.com/svlandeg,Fixing random seed to ensure reproducibility,8,"['bug', 'feat / ops']",https://github.com/explosion/thinc/pull/359,https://github.com/svlandeg,8,https://github.com/explosion/thinc/pull/359#issuecomment-656168887,"WIP
Original post, which was quite wrong, below in collapsible section to avoid future confusion...

Background
It looks like we haven't really been supporting fix_random_seed well. This function would call random.seed(value), but the problem is that this call needs to be repeated in between generating random numbers:
random.seed(666)
print(""getting int 0: "", random.randint(0, 100000))
print(""getting int 1: "", random.randint(0, 100000))
print(""getting int 2: "", random.randint(0, 100000))

Gives

getting int 0:  59784
getting int 1:  49417
getting int 2:  56934

But in contrast,
random.seed(666)
print(""getting int 0: "", random.randint(0, 100000))
random.seed(666)
print(""getting int 1: "", random.randint(0, 100000))
random.seed(666)
print(""getting int 2: "", random.randint(0, 100000))

Gives

getting int 0:  59784
getting int 1:  59784
getting int 2:  59784

Note, btw, that this is also a problem for list comprehensions:
random.seed(666)
print(""getting int 666: "", [random.randint(0, 100000) for _ in range(10)])


getting int 666:  [59784, 49417, 56934, 37158, 65618, 1864, 71792, 72571, 93346, 43664]

Fix
We can't expect the user to call fix_random_seed multiple times, because internally we may be generating multiple random numbers in between atomic calls for the user. So we need to do this within Thinc and we need to manage it carefully so we call ensure_fixed_seed EVERY TIME before we generate something random.
Currently this PR solves this with a global variable INTERNAL_SEED in thinc.util. I'm not too happy about that, and worry about consequences for parallellization. But if multiple threads/workers need to reproduce the same results, there will be no other option then to make sure they are initialized with the same seed.
Added also the functionality to remove_random_seed.
Finally, HashEmbed needs to also use this seed if it's available and is not overwritten in its constructor. The new tests in test_hash_embed show the different possibilities. Many of these would be failing on the current master branch.
I was also able to reproduce the original bug with spaCy, and after this fix, those issues seem solved. Will submit the new unit tests for spaCy on develop there.","Ok, so if I fix the seeds also for the textcat architecture, all the tests in that spaCy PR pass. Was there another architecture where you will still seeing irreproducibility (within the same process or not)?",True,{}
explosion/thinc,https://github.com/explosion/thinc,360,2020-07-10T13:16:02Z,2020-07-10T16:01:18Z,2020-07-10T17:44:23Z,MERGED,True,133,5,3,https://github.com/ines,Add overrides option to fill_config and train_from_config,2,"['enhancement', 'feat / config']",https://github.com/explosion/thinc/pull/360,https://github.com/ines,1,https://github.com/explosion/thinc/pull/360,"This PR adds support for a dictionary overrides that can be provided to registry.make_from_config and registry.fill_config. This feature allows implementing workflows that need a mechanism to overwrite config settings without editing the config file, e.g. via a CLI (the intended use in spaCy).
Overrides are applied as the config is resolved and should be passed in as a dictionary, keyed by config properties using the dot notation:
{""section.subsection.value"": 123, ""other_section.foo"": ""bar""}
Values are substituted before a section is validated, so overwriting a typed argument with a value of the wrong type will raise a regular config validation error. It's also possible to overwrite ""promises"" (references to registered functions), e.g. to overwrite a schedule with a single float/int. Overrides can also contain function references for consistency, although this is likely a less common use case.","This PR adds support for a dictionary overrides that can be provided to registry.make_from_config and registry.fill_config. This feature allows implementing workflows that need a mechanism to overwrite config settings without editing the config file, e.g. via a CLI (the intended use in spaCy).
Overrides are applied as the config is resolved and should be passed in as a dictionary, keyed by config properties using the dot notation:
{""section.subsection.value"": 123, ""other_section.foo"": ""bar""}
Values are substituted before a section is validated, so overwriting a typed argument with a value of the wrong type will raise a regular config validation error. It's also possible to overwrite ""promises"" (references to registered functions), e.g. to overwrite a schedule with a single float/int. Overrides can also contain function references for consistency, although this is likely a less common use case.",True,{}
explosion/thinc,https://github.com/explosion/thinc,360,2020-07-10T13:16:02Z,2020-07-10T16:01:18Z,2020-07-10T17:44:23Z,MERGED,True,133,5,3,https://github.com/ines,Add overrides option to fill_config and train_from_config,2,"['enhancement', 'feat / config']",https://github.com/explosion/thinc/pull/360,https://github.com/apps/codecov,2,https://github.com/explosion/thinc/pull/360#issuecomment-656672387,"This PR adds support for a dictionary overrides that can be provided to registry.make_from_config and registry.fill_config. This feature allows implementing workflows that need a mechanism to overwrite config settings without editing the config file, e.g. via a CLI (the intended use in spaCy).
Overrides are applied as the config is resolved and should be passed in as a dictionary, keyed by config properties using the dot notation:
{""section.subsection.value"": 123, ""other_section.foo"": ""bar""}
Values are substituted before a section is validated, so overwriting a typed argument with a value of the wrong type will raise a regular config validation error. It's also possible to overwrite ""promises"" (references to registered functions), e.g. to overwrite a schedule with a single float/int. Overrides can also contain function references for consistency, although this is likely a less common use case.","Codecov Report

Merging #360 into master will increase coverage by 0.15%.
The diff coverage is 100.00%.


@@            Coverage Diff             @@
##           master     #360      +/-   ##
==========================================
+ Coverage   56.67%   56.83%   +0.15%     
==========================================
  Files          99       99              
  Lines        6832     6857      +25     
==========================================
+ Hits         3872     3897      +25     
  Misses       2960     2960              



Impacted Files
Coverage 





thinc/about.py
100.00% <100.00%> ()



thinc/config.py
99.21% <100.00%> (+0.08%)





Continue to review full report at Codecov.

Legend - Click here to learn more
 = absolute <relative> (impact),  = not affected, ? = missing data
Powered by Codecov. Last update 9a0936f...35a5607. Read the comment docs.",True,{}
explosion/thinc,https://github.com/explosion/thinc,361,2020-07-10T17:46:39Z,2020-07-10T19:04:36Z,2020-07-10T19:04:38Z,MERGED,True,1,0,1,https://github.com/ines,Fix config validation error message for overrides,4,"['bug', 'feat / config']",https://github.com/explosion/thinc/pull/361,https://github.com/ines,1,https://github.com/explosion/thinc/pull/361,"Otherwise the reported config in a validation error is wrong and reflects the previous value, so errors are confusing to interpret (and the problematic value is never shown).","Otherwise the reported config in a validation error is wrong and reflects the previous value, so errors are confusing to interpret (and the problematic value is never shown).",True,{}
explosion/thinc,https://github.com/explosion/thinc,361,2020-07-10T17:46:39Z,2020-07-10T19:04:36Z,2020-07-10T19:04:38Z,MERGED,True,1,0,1,https://github.com/ines,Fix config validation error message for overrides,4,"['bug', 'feat / config']",https://github.com/explosion/thinc/pull/361,https://github.com/apps/codecov,2,https://github.com/explosion/thinc/pull/361#issuecomment-656806761,"Otherwise the reported config in a validation error is wrong and reflects the previous value, so errors are confusing to interpret (and the problematic value is never shown).","Codecov Report

Merging #361 into master will increase coverage by 0.00%.
The diff coverage is 100.00%.


@@           Coverage Diff           @@
##           master     #361   +/-   ##
=======================================
  Coverage   56.83%   56.83%           
=======================================
  Files          99       99           
  Lines        6857     6858    +1     
=======================================
+ Hits         3897     3898    +1     
  Misses       2960     2960           



Impacted Files
Coverage 





thinc/config.py
99.22% <100.00%> (+<0.01%)





Continue to review full report at Codecov.

Legend - Click here to learn more
 = absolute <relative> (impact),  = not affected, ? = missing data
Powered by Codecov. Last update 12fe61e...636ff27. Read the comment docs.",True,{}
explosion/thinc,https://github.com/explosion/thinc,362,2020-07-10T19:12:08Z,2020-07-10T20:34:46Z,2020-07-10T20:34:48Z,MERGED,True,31,4,3,https://github.com/honnibal,Raise error if expanding undefined config block,7,[],https://github.com/explosion/thinc/pull/362,https://github.com/honnibal,1,https://github.com/explosion/thinc/pull/362,"This catches a common typo case where you have something like:
[nlp.pipeline]
blah = true

[nlp.pipline.ner]
@architecture = ""tok2vec.v1""

[nlp.pipeline.ner.model]
...

These errors are otherwise difficult to catch, as we'll end up complaining about the validation of the `""tok2vec.v1"" function as it's missing its arguments...Which look like they're provided, unless the user sees the typo.
So you have to open a block, you can't create nested structures all in one section command. The only exception is the * block used for lists, which it wouldn't make sense to create explicitly.
I've made a separate error, ConfigStructureError for this as I didn't want the same error args as the ConfigValidationError. I did inherit from ConfigValidationError though, so people only have to catch the parent class to catch both.","This catches a common typo case where you have something like:
[nlp.pipeline]
blah = true

[nlp.pipline.ner]
@architecture = ""tok2vec.v1""

[nlp.pipeline.ner.model]
...

These errors are otherwise difficult to catch, as we'll end up complaining about the validation of the `""tok2vec.v1"" function as it's missing its arguments...Which look like they're provided, unless the user sees the typo.
So you have to open a block, you can't create nested structures all in one section command. The only exception is the * block used for lists, which it wouldn't make sense to create explicitly.
I've made a separate error, ConfigStructureError for this as I didn't want the same error args as the ConfigValidationError. I did inherit from ConfigValidationError though, so people only have to catch the parent class to catch both.",True,{}
explosion/thinc,https://github.com/explosion/thinc,362,2020-07-10T19:12:08Z,2020-07-10T20:34:46Z,2020-07-10T20:34:48Z,MERGED,True,31,4,3,https://github.com/honnibal,Raise error if expanding undefined config block,7,[],https://github.com/explosion/thinc/pull/362,https://github.com/apps/codecov,2,https://github.com/explosion/thinc/pull/362#issuecomment-656854691,"This catches a common typo case where you have something like:
[nlp.pipeline]
blah = true

[nlp.pipline.ner]
@architecture = ""tok2vec.v1""

[nlp.pipeline.ner.model]
...

These errors are otherwise difficult to catch, as we'll end up complaining about the validation of the `""tok2vec.v1"" function as it's missing its arguments...Which look like they're provided, unless the user sees the typo.
So you have to open a block, you can't create nested structures all in one section command. The only exception is the * block used for lists, which it wouldn't make sense to create explicitly.
I've made a separate error, ConfigStructureError for this as I didn't want the same error args as the ConfigValidationError. I did inherit from ConfigValidationError though, so people only have to catch the parent class to catch both.","Codecov Report

Merging #362 into master will increase coverage by 0.26%.
The diff coverage is 100.00%.


@@            Coverage Diff             @@
##           master     #362      +/-   ##
==========================================
+ Coverage   56.83%   57.09%   +0.26%     
==========================================
  Files          99       99              
  Lines        6857     6899      +42     
==========================================
+ Hits         3897     3939      +42     
  Misses       2960     2960              



Impacted Files
Coverage 





thinc/about.py
100.00% <100.00%> ()



thinc/config.py
99.32% <100.00%> (+0.11%)





Continue to review full report at Codecov.

Legend - Click here to learn more
 = absolute <relative> (impact),  = not affected, ? = missing data
Powered by Codecov. Last update 12fe61e...01e0895. Read the comment docs.",True,{}
explosion/thinc,https://github.com/explosion/thinc,363,2020-07-11T09:35:57Z,2020-07-11T10:17:05Z,2020-07-11T10:17:07Z,MERGED,True,19,4,2,https://github.com/honnibal,Make config.to_str() create intermediate blocks,4,[],https://github.com/explosion/thinc/pull/363,https://github.com/honnibal,1,https://github.com/explosion/thinc/pull/363,"Previously if you had {""optimizer"": {""foo"": {""bar"": 1}}, the printed string would not create the [optimizer] block as it didn't have any leaf values. That now fails parsing due to the change in #362.","Previously if you had {""optimizer"": {""foo"": {""bar"": 1}}, the printed string would not create the [optimizer] block as it didn't have any leaf values. That now fails parsing due to the change in #362.",True,{}
explosion/thinc,https://github.com/explosion/thinc,363,2020-07-11T09:35:57Z,2020-07-11T10:17:05Z,2020-07-11T10:17:07Z,MERGED,True,19,4,2,https://github.com/honnibal,Make config.to_str() create intermediate blocks,4,[],https://github.com/explosion/thinc/pull/363,https://github.com/apps/codecov,2,https://github.com/explosion/thinc/pull/363#issuecomment-657026875,"Previously if you had {""optimizer"": {""foo"": {""bar"": 1}}, the printed string would not create the [optimizer] block as it didn't have any leaf values. That now fails parsing due to the change in #362.","Codecov Report

Merging #363 into master will decrease coverage by 0.00%.
The diff coverage is 100.00%.


@@            Coverage Diff             @@
##           master     #363      +/-   ##
==========================================
- Coverage   56.88%   56.88%   -0.01%     
==========================================
  Files          99       99              
  Lines        6866     6865       -1     
==========================================
- Hits         3906     3905       -1     
  Misses       2960     2960              



Impacted Files
Coverage 





thinc/config.py
99.24% <100.00%> (-0.01%)





Continue to review full report at Codecov.

Legend - Click here to learn more
 = absolute <relative> (impact),  = not affected, ? = missing data
Powered by Codecov. Last update 10c0852...ae24a5f. Read the comment docs.",True,{}
explosion/thinc,https://github.com/explosion/thinc,364,2020-07-12T01:31:11Z,2020-07-13T21:06:53Z,2020-07-13T21:06:53Z,MERGED,True,9,1,1,https://github.com/richardliaw,Add GPU check for unavailable GPU,2,[],https://github.com/explosion/thinc/pull/364,https://github.com/richardliaw,1,https://github.com/explosion/thinc/pull/364,,,True,{}
explosion/thinc,https://github.com/explosion/thinc,364,2020-07-12T01:31:11Z,2020-07-13T21:06:53Z,2020-07-13T21:06:53Z,MERGED,True,9,1,1,https://github.com/richardliaw,Add GPU check for unavailable GPU,2,[],https://github.com/explosion/thinc/pull/364,https://github.com/apps/codecov,2,https://github.com/explosion/thinc/pull/364#issuecomment-657160568,,"Codecov Report

Merging #364 into master will decrease coverage by 0.03%.
The diff coverage is 16.66%.


@@            Coverage Diff             @@
##           master     #364      +/-   ##
==========================================
- Coverage   56.88%   56.84%   -0.04%     
==========================================
  Files          99       99              
  Lines        6865     6871       +6     
==========================================
+ Hits         3905     3906       +1     
- Misses       2960     2965       +5     



Impacted Files
Coverage 





thinc/util.py
92.06% <16.66%> (-3.77%)





Continue to review full report at Codecov.

Legend - Click here to learn more
 = absolute <relative> (impact),  = not affected, ? = missing data
Powered by Codecov. Last update fca3da4...7dbbf8b. Read the comment docs.",True,{}
explosion/thinc,https://github.com/explosion/thinc,365,2020-07-13T20:22:07Z,2020-07-13T21:06:27Z,2020-07-13T21:11:19Z,MERGED,True,2,2,1,https://github.com/svlandeg,fix concatenate (list) on GPU,1,"['bug', 'feat / layers']",https://github.com/explosion/thinc/pull/365,https://github.com/svlandeg,1,https://github.com/explosion/thinc/pull/365,"The backprop calls take List's as input, I think there's no need for the asarray transformations. They don't matter for numpy (I assume because of duck typing), but on GPU cupy throws an error because of the varying lengths and a resulting Unsupported dtype object.
This fix makes CharEmbed / the morphologizer work again on GPU / spaCy (together with explosion/spaCy#5757)","The backprop calls take List's as input, I think there's no need for the asarray transformations. They don't matter for numpy (I assume because of duck typing), but on GPU cupy throws an error because of the varying lengths and a resulting Unsupported dtype object.
This fix makes CharEmbed / the morphologizer work again on GPU / spaCy (together with explosion/spaCy#5757)",True,{}
explosion/thinc,https://github.com/explosion/thinc,365,2020-07-13T20:22:07Z,2020-07-13T21:06:27Z,2020-07-13T21:11:19Z,MERGED,True,2,2,1,https://github.com/svlandeg,fix concatenate (list) on GPU,1,"['bug', 'feat / layers']",https://github.com/explosion/thinc/pull/365,https://github.com/apps/codecov,2,https://github.com/explosion/thinc/pull/365#issuecomment-657777333,"The backprop calls take List's as input, I think there's no need for the asarray transformations. They don't matter for numpy (I assume because of duck typing), but on GPU cupy throws an error because of the varying lengths and a resulting Unsupported dtype object.
This fix makes CharEmbed / the morphologizer work again on GPU / spaCy (together with explosion/spaCy#5757)","Codecov Report

Merging #365 into master will not change coverage.
The diff coverage is 0.00%.


@@           Coverage Diff           @@
##           master     #365   +/-   ##
=======================================
  Coverage   56.88%   56.88%           
=======================================
  Files          99       99           
  Lines        6865     6865           
=======================================
  Hits         3905     3905           
  Misses       2960     2960           



Impacted Files
Coverage 





thinc/layers/concatenate.py
73.23% <0.00%> ()





Continue to review full report at Codecov.

Legend - Click here to learn more
 = absolute <relative> (impact),  = not affected, ? = missing data
Powered by Codecov. Last update fca3da4...7afb55e. Read the comment docs.",True,{}
explosion/thinc,https://github.com/explosion/thinc,366,2020-07-15T21:27:45Z,2020-07-19T11:44:19Z,2020-07-19T19:12:53Z,MERGED,True,23,12,5,https://github.com/svlandeg,robustness against empty X,2,"['enhancement', 'feat / layers', 'feat / ops']",https://github.com/explosion/thinc/pull/366,https://github.com/svlandeg,1,https://github.com/explosion/thinc/pull/366,"Various checks in case X is empty, e.g. in spaCy running with doc = Doc(vocab, words=[])","Various checks in case X is empty, e.g. in spaCy running with doc = Doc(vocab, words=[])",True,{}
explosion/thinc,https://github.com/explosion/thinc,366,2020-07-15T21:27:45Z,2020-07-19T11:44:19Z,2020-07-19T19:12:53Z,MERGED,True,23,12,5,https://github.com/svlandeg,robustness against empty X,2,"['enhancement', 'feat / layers', 'feat / ops']",https://github.com/explosion/thinc/pull/366,https://github.com/apps/codecov,2,https://github.com/explosion/thinc/pull/366#issuecomment-659229275,"Various checks in case X is empty, e.g. in spaCy running with doc = Doc(vocab, words=[])","Codecov Report

Merging #366 into master will increase coverage by 0.01%.
The diff coverage is 84.21%.


@@            Coverage Diff             @@
##           master     #366      +/-   ##
==========================================
+ Coverage   56.84%   56.86%   +0.01%     
==========================================
  Files          99       99              
  Lines        6871     6880       +9     
==========================================
+ Hits         3906     3912       +6     
- Misses       2965     2968       +3     



Impacted Files
Coverage 





thinc/backends/ops.py
81.27% <50.00%> (-0.11%)



thinc/layers/expand_window.py
94.44% <75.00%> (-5.56%)



thinc/layers/hashembed.py
96.15% <90.90%> (-1.85%)



thinc/layers/array_getitem.py
91.30% <100.00%> (+0.82%)





Continue to review full report at Codecov.

Legend - Click here to learn more
 = absolute <relative> (impact),  = not affected, ? = missing data
Powered by Codecov. Last update 6d315c5...466d7af. Read the comment docs.",True,{}
explosion/thinc,https://github.com/explosion/thinc,366,2020-07-15T21:27:45Z,2020-07-19T11:44:19Z,2020-07-19T19:12:53Z,MERGED,True,23,12,5,https://github.com/svlandeg,robustness against empty X,2,"['enhancement', 'feat / layers', 'feat / ops']",https://github.com/explosion/thinc/pull/366,https://github.com/honnibal,3,https://github.com/explosion/thinc/pull/366#issuecomment-660631241,"Various checks in case X is empty, e.g. in spaCy running with doc = Doc(vocab, words=[])","Nice, thanks!",True,{}
explosion/thinc,https://github.com/explosion/thinc,367,2020-07-17T07:55:24Z,2020-07-21T11:50:19Z,2020-07-21T11:50:19Z,MERGED,True,7,7,1,https://github.com/adrianeboyd,Restrict cupy to <8.0.0,1,[],https://github.com/explosion/thinc/pull/367,https://github.com/adrianeboyd,1,https://github.com/explosion/thinc/pull/367,"Similar to explosion/spaCy#5773, I think thinc v7 needs to require cupy <8.0.0.","Similar to explosion/spaCy#5773, I think thinc v7 needs to require cupy <8.0.0.",True,{}
explosion/thinc,https://github.com/explosion/thinc,368,2020-07-19T14:02:06Z,2020-07-19T14:41:37Z,2020-07-19T14:51:24Z,MERGED,True,65,15,3,https://github.com/ines,"Fix handling of dict return values, add registry.resolve",3,"['bug', 'enhancement']",https://github.com/explosion/thinc/pull/368,https://github.com/ines,1,https://github.com/explosion/thinc/pull/368,"Fix handling and validation of registered functions returning dicts when config is filled and validated.
Add registry.resolve shorthand that fills and resolves a config (so we don't have to call into registry._fill from spaCy etc.)","Fix handling and validation of registered functions returning dicts when config is filled and validated.
Add registry.resolve shorthand that fills and resolves a config (so we don't have to call into registry._fill from spaCy etc.)",True,{}
explosion/thinc,https://github.com/explosion/thinc,368,2020-07-19T14:02:06Z,2020-07-19T14:41:37Z,2020-07-19T14:51:24Z,MERGED,True,65,15,3,https://github.com/ines,"Fix handling of dict return values, add registry.resolve",3,"['bug', 'enhancement']",https://github.com/explosion/thinc/pull/368,https://github.com/apps/codecov,2,https://github.com/explosion/thinc/pull/368#issuecomment-660648980,"Fix handling and validation of registered functions returning dicts when config is filled and validated.
Add registry.resolve shorthand that fills and resolves a config (so we don't have to call into registry._fill from spaCy etc.)","Codecov Report

Merging #368 into master will increase coverage by 0.00%.
The diff coverage is 100.00%.


@@           Coverage Diff           @@
##           master     #368   +/-   ##
=======================================
  Coverage   56.86%   56.86%           
=======================================
  Files          99       99           
  Lines        6880     6881    +1     
=======================================
+ Hits         3912     3913    +1     
  Misses       2968     2968           



Impacted Files
Coverage 





thinc/about.py
100.00% <100.00%> ()



thinc/config.py
99.24% <100.00%> (+<0.01%)





Continue to review full report at Codecov.

Legend - Click here to learn more
 = absolute <relative> (impact),  = not affected, ? = missing data
Powered by Codecov. Last update f9bf899...6baac9c. Read the comment docs.",True,{}
explosion/thinc,https://github.com/explosion/thinc,369,2020-07-20T21:45:16Z,2020-07-21T11:46:31Z,2020-07-21T11:46:33Z,MERGED,True,84,8,3,https://github.com/ines,Config enhancements,5,"['enhancement', 'feat / config']",https://github.com/explosion/thinc/pull/369,https://github.com/ines,1,https://github.com/explosion/thinc/pull/369,"Add Config.copy for deepcopying configs.
 Serialize refrences to registered functions with no args as dict

[section]
+ subsection = {""@some_registry"": ""SomeFunction.v1""}

- [section.subsection]
- @some_registry = ""SomeFunction.v1""

 preserve reasonable order in Config.to_str (previously, config was sorted with top-level sections first, followed by all subsections and so on  now sections are ordered by key, so [nlp], followed by [nlp.pipeline], followed by [nlp.pipeline.tagger] and so on)
 improve error messages around config deserialization and interpolation issues","Add Config.copy for deepcopying configs.
 Serialize refrences to registered functions with no args as dict

[section]
+ subsection = {""@some_registry"": ""SomeFunction.v1""}

- [section.subsection]
- @some_registry = ""SomeFunction.v1""

 preserve reasonable order in Config.to_str (previously, config was sorted with top-level sections first, followed by all subsections and so on  now sections are ordered by key, so [nlp], followed by [nlp.pipeline], followed by [nlp.pipeline.tagger] and so on)
 improve error messages around config deserialization and interpolation issues",True,{}
explosion/thinc,https://github.com/explosion/thinc,369,2020-07-20T21:45:16Z,2020-07-21T11:46:31Z,2020-07-21T11:46:33Z,MERGED,True,84,8,3,https://github.com/ines,Config enhancements,5,"['enhancement', 'feat / config']",https://github.com/explosion/thinc/pull/369,https://github.com/apps/codecov,2,https://github.com/explosion/thinc/pull/369#issuecomment-661419061,"Add Config.copy for deepcopying configs.
 Serialize refrences to registered functions with no args as dict

[section]
+ subsection = {""@some_registry"": ""SomeFunction.v1""}

- [section.subsection]
- @some_registry = ""SomeFunction.v1""

 preserve reasonable order in Config.to_str (previously, config was sorted with top-level sections first, followed by all subsections and so on  now sections are ordered by key, so [nlp], followed by [nlp.pipeline], followed by [nlp.pipeline.tagger] and so on)
 improve error messages around config deserialization and interpolation issues","Codecov Report

Merging #369 into master will increase coverage by 0.14%.
The diff coverage is 100.00%.


@@            Coverage Diff             @@
##           master     #369      +/-   ##
==========================================
+ Coverage   56.86%   57.00%   +0.14%     
==========================================
  Files          99       99              
  Lines        6881     6899      +18     
==========================================
+ Hits         3913     3933      +20     
+ Misses       2968     2966       -2     



Impacted Files
Coverage 





thinc/about.py
100.00% <100.00%> ()



thinc/config.py
100.00% <100.00%> (+0.75%)





Continue to review full report at Codecov.

Legend - Click here to learn more
 = absolute <relative> (impact),  = not affected, ? = missing data
Powered by Codecov. Last update 83f247a...b47a58a. Read the comment docs.",True,{}
explosion/thinc,https://github.com/explosion/thinc,370,2020-07-27T16:09:50Z,2020-08-03T09:30:45Z,2020-08-03T09:35:34Z,MERGED,True,4,2,1,https://github.com/svlandeg,Small edit to error msg,2,"['enhancement', 'feat / ux']",https://github.com/explosion/thinc/pull/370,https://github.com/svlandeg,1,https://github.com/explosion/thinc/pull/370,Adding few common causes of errors in the config to the error msg,Adding few common causes of errors in the config to the error msg,True,{}
explosion/thinc,https://github.com/explosion/thinc,370,2020-07-27T16:09:50Z,2020-08-03T09:30:45Z,2020-08-03T09:35:34Z,MERGED,True,4,2,1,https://github.com/svlandeg,Small edit to error msg,2,"['enhancement', 'feat / ux']",https://github.com/explosion/thinc/pull/370,https://github.com/apps/codecov,2,https://github.com/explosion/thinc/pull/370#issuecomment-664495018,Adding few common causes of errors in the config to the error msg,"Codecov Report

Merging #370 into master will not change coverage.
The diff coverage is n/a.


@@           Coverage Diff           @@
##           master     #370   +/-   ##
=======================================
  Coverage   57.00%   57.00%           
=======================================
  Files          99       99           
  Lines        6899     6899           
=======================================
  Hits         3933     3933           
  Misses       2966     2966           



Impacted Files
Coverage 





thinc/config.py
100.00% <> ()





Continue to review full report at Codecov.

Legend - Click here to learn more
 = absolute <relative> (impact),  = not affected, ? = missing data
Powered by Codecov. Last update 53951ce...405ef42. Read the comment docs.",True,{}
explosion/thinc,https://github.com/explosion/thinc,371,2020-07-28T18:13:15Z,2020-07-28T19:16:29Z,2020-07-31T12:55:49Z,MERGED,True,27,0,2,https://github.com/honnibal,"Add maybe_get_{dim, param, grad, ref} methods",4,[],https://github.com/explosion/thinc/pull/371,https://github.com/honnibal,1,https://github.com/explosion/thinc/pull/371,"Writing the conditional logic for the getters is inconvenient, but we don't want to return None from those methods, or the type-guarantees will be greatly weakened. Instead I suggest we have separate methods that are allowed to return None.","Writing the conditional logic for the getters is inconvenient, but we don't want to return None from those methods, or the type-guarantees will be greatly weakened. Instead I suggest we have separate methods that are allowed to return None.",True,{}
explosion/thinc,https://github.com/explosion/thinc,371,2020-07-28T18:13:15Z,2020-07-28T19:16:29Z,2020-07-31T12:55:49Z,MERGED,True,27,0,2,https://github.com/honnibal,"Add maybe_get_{dim, param, grad, ref} methods",4,[],https://github.com/explosion/thinc/pull/371,https://github.com/apps/codecov,2,https://github.com/explosion/thinc/pull/371#issuecomment-665215945,"Writing the conditional logic for the getters is inconvenient, but we don't want to return None from those methods, or the type-guarantees will be greatly weakened. Instead I suggest we have separate methods that are allowed to return None.","Codecov Report

Merging #371 into master will decrease coverage by 0.03%.
The diff coverage is 41.17%.


@@            Coverage Diff             @@
##           master     #371      +/-   ##
==========================================
- Coverage   57.00%   56.96%   -0.04%     
==========================================
  Files          99       99              
  Lines        6899     6916      +17     
==========================================
+ Hits         3933     3940       +7     
- Misses       2966     2976      +10     



Impacted Files
Coverage 





thinc/tests/model/test_model.py
0.00% <0.00%> ()



thinc/model.py
99.75% <87.50%> (-0.25%)





Continue to review full report at Codecov.

Legend - Click here to learn more
 = absolute <relative> (impact),  = not affected, ? = missing data
Powered by Codecov. Last update 53951ce...6bca60b. Read the comment docs.",True,{}
explosion/thinc,https://github.com/explosion/thinc,372,2020-07-31T12:50:36Z,2020-08-03T09:30:25Z,2020-08-03T09:35:56Z,MERGED,True,27,14,6,https://github.com/svlandeg,Softmax init,4,"['enhancement', 'feat / layers']",https://github.com/explosion/thinc/pull/372,https://github.com/svlandeg,1,https://github.com/explosion/thinc/pull/372,"Avoid setting dimensions in softmax when not required
data_validation context mananger
version bump to a21","Avoid setting dimensions in softmax when not required
data_validation context mananger
version bump to a21",True,{'THUMBS_UP': ['https://github.com/honnibal']}
explosion/thinc,https://github.com/explosion/thinc,372,2020-07-31T12:50:36Z,2020-08-03T09:30:25Z,2020-08-03T09:35:56Z,MERGED,True,27,14,6,https://github.com/svlandeg,Softmax init,4,"['enhancement', 'feat / layers']",https://github.com/explosion/thinc/pull/372,https://github.com/apps/codecov,2,https://github.com/explosion/thinc/pull/372#issuecomment-667105911,"Avoid setting dimensions in softmax when not required
data_validation context mananger
version bump to a21","Codecov Report

Merging #372 into master will increase coverage by 0.04%.
The diff coverage is 62.50%.


@@            Coverage Diff             @@
##           master     #372      +/-   ##
==========================================
+ Coverage   56.91%   56.95%   +0.04%     
==========================================
  Files          99       99              
  Lines        6939     6949      +10     
==========================================
+ Hits         3949     3958       +9     
- Misses       2990     2991       +1     



Impacted Files
Coverage 





thinc/tests/layers/test_layers_api.py
0.00% <0.00%> ()



thinc/about.py
100.00% <100.00%> ()



thinc/api.py
100.00% <100.00%> ()



thinc/layers/softmax.py
100.00% <100.00%> ()



thinc/model.py
99.75% <100.00%> (-0.01%)



thinc/util.py
92.64% <100.00%> (+0.58%)





Continue to review full report at Codecov.

Legend - Click here to learn more
 = absolute <relative> (impact),  = not affected, ? = missing data
Powered by Codecov. Last update 3d96011...e95a3e1. Read the comment docs.",True,{}
explosion/thinc,https://github.com/explosion/thinc,373,2020-08-04T14:19:38Z,2020-08-04T19:54:30Z,2020-08-04T19:55:01Z,MERGED,True,163,18,2,https://github.com/ines,Support extended config interpolation,3,"['enhancement', 'feat / config']",https://github.com/explosion/thinc/pull/373,https://github.com/ines,1,https://github.com/explosion/thinc/pull/373,"Resolves #352.
This PR improves how variable interpolation is handled in configs and adds support for referencing blocks. Even the basic interpolation ended up being tricky to support, because we need to make sure that values are loaded with json.loads before they're substituted, so you don't end up with ""hello ${value}"" -> ""hello ""value"""".
Some examples of what's possible now (also see the tests for more):
[paths]
root = ""/some/path""
some_int = 10

[section]
my_path = ""${paths:root}/some/subpath""
my_other_path = ""${paths:root}/some/subpath/${paths:some_int}""
[section]
some_value = ""value""

[other_section]
some_value = ${section}
[section]
some_value = ""value""

[section.subsection]
other_value = 20

[other_section]
some_value = ${section.subsection}
Implementation notes

In order to make this work, I had to implement our own versions of some of the configparser.ExtendedInterpolation methods. _interpolate_some is especially annoying because we really just needed to add a small section, but this was the only way to make it work properly.","Resolves #352.
This PR improves how variable interpolation is handled in configs and adds support for referencing blocks. Even the basic interpolation ended up being tricky to support, because we need to make sure that values are loaded with json.loads before they're substituted, so you don't end up with ""hello ${value}"" -> ""hello ""value"""".
Some examples of what's possible now (also see the tests for more):
[paths]
root = ""/some/path""
some_int = 10

[section]
my_path = ""${paths:root}/some/subpath""
my_other_path = ""${paths:root}/some/subpath/${paths:some_int}""
[section]
some_value = ""value""

[other_section]
some_value = ${section}
[section]
some_value = ""value""

[section.subsection]
other_value = 20

[other_section]
some_value = ${section.subsection}
Implementation notes

In order to make this work, I had to implement our own versions of some of the configparser.ExtendedInterpolation methods. _interpolate_some is especially annoying because we really just needed to add a small section, but this was the only way to make it work properly.",True,"{'EYES': ['https://github.com/svlandeg'], 'HOORAY': ['https://github.com/honnibal']}"
explosion/thinc,https://github.com/explosion/thinc,373,2020-08-04T14:19:38Z,2020-08-04T19:54:30Z,2020-08-04T19:55:01Z,MERGED,True,163,18,2,https://github.com/ines,Support extended config interpolation,3,"['enhancement', 'feat / config']",https://github.com/explosion/thinc/pull/373,https://github.com/apps/codecov,2,https://github.com/explosion/thinc/pull/373#issuecomment-668625748,"Resolves #352.
This PR improves how variable interpolation is handled in configs and adds support for referencing blocks. Even the basic interpolation ended up being tricky to support, because we need to make sure that values are loaded with json.loads before they're substituted, so you don't end up with ""hello ${value}"" -> ""hello ""value"""".
Some examples of what's possible now (also see the tests for more):
[paths]
root = ""/some/path""
some_int = 10

[section]
my_path = ""${paths:root}/some/subpath""
my_other_path = ""${paths:root}/some/subpath/${paths:some_int}""
[section]
some_value = ""value""

[other_section]
some_value = ${section}
[section]
some_value = ""value""

[section.subsection]
other_value = 20

[other_section]
some_value = ${section.subsection}
Implementation notes

In order to make this work, I had to implement our own versions of some of the configparser.ExtendedInterpolation methods. _interpolate_some is especially annoying because we really just needed to add a small section, but this was the only way to make it work properly.","Codecov Report

Merging #373 into master will increase coverage by 0.26%.
The diff coverage is 82.55%.


@@            Coverage Diff             @@
##           master     #373      +/-   ##
==========================================
+ Coverage   56.95%   57.22%   +0.26%     
==========================================
  Files          99       99              
  Lines        6949     7027      +78     
==========================================
+ Hits         3958     4021      +63     
- Misses       2991     3006      +15     



Impacted Files
Coverage 





thinc/config.py
95.84% <82.55%> (-4.16%)





Continue to review full report at Codecov.

Legend - Click here to learn more
 = absolute <relative> (impact),  = not affected, ? = missing data
Powered by Codecov. Last update a29a138...1a0d1c0. Read the comment docs.",True,{}
explosion/thinc,https://github.com/explosion/thinc,374,2020-08-04T21:07:50Z,2020-08-04T21:48:09Z,2020-08-04T21:48:24Z,MERGED,True,54,6,2,https://github.com/ines,Allow config overrides on creation (and used with interpolation),3,"['enhancement', 'feat / config']",https://github.com/explosion/thinc/pull/374,https://github.com/ines,1,https://github.com/explosion/thinc/pull/374,"The Config.from_str, Config.from_bytes and Config.from_disk methods can now take optional overrides that are added to the config parser before the config is interpreted. This allows overwriting values that are referenced as variables in other sections, without breaking the config.
Example
[section]
a = 2
b = 3

[section.subsection]
c = ""hello""

[other_section]
d = ${section:a}
e = ${section.subsection}
overrides = {""section.a"": 20, ""section.subsection.c"": ""world""}
config = Config.from_str(config_str, overrides=overrides)
# Result
{
  ""section"": {
    ""a"": 20,
    ""b"": 3,
    ""subsection"": {
      ""c"": ""world""
    },
  ""other_section"": {
    ""d"": 20,
    ""e"": {
       ""c"": ""world""
    }
  }
}","The Config.from_str, Config.from_bytes and Config.from_disk methods can now take optional overrides that are added to the config parser before the config is interpreted. This allows overwriting values that are referenced as variables in other sections, without breaking the config.
Example
[section]
a = 2
b = 3

[section.subsection]
c = ""hello""

[other_section]
d = ${section:a}
e = ${section.subsection}
overrides = {""section.a"": 20, ""section.subsection.c"": ""world""}
config = Config.from_str(config_str, overrides=overrides)
# Result
{
  ""section"": {
    ""a"": 20,
    ""b"": 3,
    ""subsection"": {
      ""c"": ""world""
    },
  ""other_section"": {
    ""d"": 20,
    ""e"": {
       ""c"": ""world""
    }
  }
}",True,{}
explosion/thinc,https://github.com/explosion/thinc,374,2020-08-04T21:07:50Z,2020-08-04T21:48:09Z,2020-08-04T21:48:24Z,MERGED,True,54,6,2,https://github.com/ines,Allow config overrides on creation (and used with interpolation),3,"['enhancement', 'feat / config']",https://github.com/explosion/thinc/pull/374,https://github.com/apps/codecov,2,https://github.com/explosion/thinc/pull/374#issuecomment-668827827,"The Config.from_str, Config.from_bytes and Config.from_disk methods can now take optional overrides that are added to the config parser before the config is interpreted. This allows overwriting values that are referenced as variables in other sections, without breaking the config.
Example
[section]
a = 2
b = 3

[section.subsection]
c = ""hello""

[other_section]
d = ${section:a}
e = ${section.subsection}
overrides = {""section.a"": 20, ""section.subsection.c"": ""world""}
config = Config.from_str(config_str, overrides=overrides)
# Result
{
  ""section"": {
    ""a"": 20,
    ""b"": 3,
    ""subsection"": {
      ""c"": ""world""
    },
  ""other_section"": {
    ""d"": 20,
    ""e"": {
       ""c"": ""world""
    }
  }
}","Codecov Report

Merging #374 into master will increase coverage by 0.07%.
The diff coverage is 100.00%.


@@            Coverage Diff             @@
##           master     #374      +/-   ##
==========================================
+ Coverage   57.22%   57.29%   +0.07%     
==========================================
  Files          99       99              
  Lines        7027     7039      +12     
==========================================
+ Hits         4021     4033      +12     
  Misses       3006     3006              



Impacted Files
Coverage 





thinc/config.py
95.97% <100.00%> (+0.13%)





Continue to review full report at Codecov.

Legend - Click here to learn more
 = absolute <relative> (impact),  = not affected, ? = missing data
Powered by Codecov. Last update 7a5d2e6...ac1e9ca. Read the comment docs.",True,{}
explosion/thinc,https://github.com/explosion/thinc,375,2020-08-05T21:28:03Z,2020-08-05T22:08:46Z,2020-08-05T22:08:51Z,MERGED,True,10,7,2,https://github.com/ines,"Use ""raise ... from"" in custom errors for better tracebacks",1,"['enhancement', 'feat / ux']",https://github.com/explosion/thinc/pull/375,https://github.com/ines,1,https://github.com/explosion/thinc/pull/375,"Some background on raise with from: https://stackoverflow.com/a/24752607/6400719
It's probably a good code practice we should adopt in spaCy as well? This will:

remove the extensive traceback caused by raising and reraising exceptions during config validation (from None)
in custom exceptions based on other relevant exceptions (like, if copying the config fails because of an error in deepcopy): raising it from the original error means the traceback will read: ""The above exception was the direct cause of the following exception:""","Some background on raise with from: https://stackoverflow.com/a/24752607/6400719
It's probably a good code practice we should adopt in spaCy as well? This will:

remove the extensive traceback caused by raising and reraising exceptions during config validation (from None)
in custom exceptions based on other relevant exceptions (like, if copying the config fails because of an error in deepcopy): raising it from the original error means the traceback will read: ""The above exception was the direct cause of the following exception:""",True,{'HOORAY': ['https://github.com/svlandeg']}
explosion/thinc,https://github.com/explosion/thinc,375,2020-08-05T21:28:03Z,2020-08-05T22:08:46Z,2020-08-05T22:08:51Z,MERGED,True,10,7,2,https://github.com/ines,"Use ""raise ... from"" in custom errors for better tracebacks",1,"['enhancement', 'feat / ux']",https://github.com/explosion/thinc/pull/375,https://github.com/apps/codecov,2,https://github.com/explosion/thinc/pull/375#issuecomment-669523637,"Some background on raise with from: https://stackoverflow.com/a/24752607/6400719
It's probably a good code practice we should adopt in spaCy as well? This will:

remove the extensive traceback caused by raising and reraising exceptions during config validation (from None)
in custom exceptions based on other relevant exceptions (like, if copying the config fails because of an error in deepcopy): raising it from the original error means the traceback will read: ""The above exception was the direct cause of the following exception:""","Codecov Report

Merging #375 into master will increase coverage by 0.00%.
The diff coverage is 80.00%.


@@           Coverage Diff           @@
##           master     #375   +/-   ##
=======================================
  Coverage   57.29%   57.30%           
=======================================
  Files          99       99           
  Lines        7039     7038    -1     
=======================================
  Hits         4033     4033           
+ Misses       3006     3005    -1     



Impacted Files
Coverage 





thinc/config.py
96.23% <75.00%> (+0.25%)



thinc/util.py
92.64% <100.00%> ()





Continue to review full report at Codecov.

Legend - Click here to learn more
 = absolute <relative> (impact),  = not affected, ? = missing data
Powered by Codecov. Last update a4033c8...f6db5e4. Read the comment docs.",True,{}
explosion/thinc,https://github.com/explosion/thinc,376,2020-08-07T10:40:17Z,2020-08-07T11:47:05Z,2020-08-07T11:47:08Z,MERGED,True,54,14,2,https://github.com/ines,Add auto-aliases for reserved field names,1,"['enhancement', 'feat / config']",https://github.com/explosion/thinc/pull/376,https://github.com/ines,1,https://github.com/explosion/thinc/pull/376,"Some field names like validate are ""reserved"" and would otherwise shadow pydantic attributes. Since we auto-generate schemas based on the signatures of registered functions, this means that a function with an argument validate triggers a NameError.
This PR adds a mechanism to automatically alias the field names so they're validated and resolved correctly. Field aliases can be arbitrary strings, so I'm currently using the original name plus a space. This means that validation errors referring to those aliased keys still look correct to the user.","Some field names like validate are ""reserved"" and would otherwise shadow pydantic attributes. Since we auto-generate schemas based on the signatures of registered functions, this means that a function with an argument validate triggers a NameError.
This PR adds a mechanism to automatically alias the field names so they're validated and resolved correctly. Field aliases can be arbitrary strings, so I'm currently using the original name plus a space. This means that validation errors referring to those aliased keys still look correct to the user.",True,{}
explosion/thinc,https://github.com/explosion/thinc,376,2020-08-07T10:40:17Z,2020-08-07T11:47:05Z,2020-08-07T11:47:08Z,MERGED,True,54,14,2,https://github.com/ines,Add auto-aliases for reserved field names,1,"['enhancement', 'feat / config']",https://github.com/explosion/thinc/pull/376,https://github.com/apps/codecov,2,https://github.com/explosion/thinc/pull/376#issuecomment-670455086,"Some field names like validate are ""reserved"" and would otherwise shadow pydantic attributes. Since we auto-generate schemas based on the signatures of registered functions, this means that a function with an argument validate triggers a NameError.
This PR adds a mechanism to automatically alias the field names so they're validated and resolved correctly. Field aliases can be arbitrary strings, so I'm currently using the original name plus a space. This means that validation errors referring to those aliased keys still look correct to the user.","Codecov Report

Merging #376 into master will increase coverage by 0.05%.
The diff coverage is 92.59%.


@@            Coverage Diff             @@
##           master     #376      +/-   ##
==========================================
+ Coverage   57.30%   57.35%   +0.05%     
==========================================
  Files          99       99              
  Lines        7038     7052      +14     
==========================================
+ Hits         4033     4045      +12     
- Misses       3005     3007       +2     



Impacted Files
Coverage 





thinc/config.py
95.85% <92.59%> (-0.39%)





Continue to review full report at Codecov.

Legend - Click here to learn more
 = absolute <relative> (impact),  = not affected, ? = missing data
Powered by Codecov. Last update b7f1d40...59cb578. Read the comment docs.",True,{}
explosion/thinc,https://github.com/explosion/thinc,377,2020-08-12T14:56:07Z,2020-08-12T21:32:17Z,2020-08-12T22:46:42Z,MERGED,True,342,84,3,https://github.com/ines,Support optional variable interpolation and fix config resolution,9,"['enhancement', 'feat / config']",https://github.com/explosion/thinc/pull/377,https://github.com/ines,1,https://github.com/explosion/thinc/pull/377,"Allow setting interpolate=False on Config.to_str, Config.from_str and other serialization methods. This will preserve variables in the configs, and will make it easier for us to provide ""starter configs"" that can be filled without destroying the variables.
Support non-interpolated configs in registry.resolve: in that case, we keep a copy of the original config, fill and resolve the interpolated version (because that's the only way it can work) and then merge the original config with its variable references back into the filled config. This makes it possible to auto-fill a config with variables and add missing defaults etc. while keeping the references intact.
Add Config.interpolate method to interpolate an existing config (returns a copy).
Port config merging logic over from spaCy so it can live in Thinc instead, and add Config.merge that takes a configs and merges it with the current config, wtih some special logic to handle variables and promises.
Make it illegal save or interpolate a config with top-level keys that aren't sections (e.g. {""foo"": 1}, which can only really happen if you construct a Conig from a dict). Otherwise, Python's configparser will add them to a section [DEFAULT] and when the serialized config is loaded back in from a string, those values are added to every section (!) which is almost never what you want and causes very confusing results and validation errors.
Raise better error if a variable references a section within a string, i.e. ""hello ${foo.bar}"". If the section is a regular dict, this would work in theory, and you'd get the string ""hello {'x': 'y'}""  but it most likely indicates a mistake. If the section is a reference to a registered function, it can't work, because registered functions are resolved after the config is created and interpolated.

Open questions

Does it make sense to have interpolate=True as the default? Realistically, we'd be setting it to False a lot in spaCy internally, but from a user's perspective, I think it makes more sense if Config.from_str etc. always returns the interpolated final version and doesn't require calling .interpolate() manually or setting interpolate=True? Or maybe it is actually more logical if it doesn't ""destroy"" variables by default? registry.resolve will always interpolate anyways and can handle variables, so that's no problem. So I might actually be leaning towards default=False now ","Allow setting interpolate=False on Config.to_str, Config.from_str and other serialization methods. This will preserve variables in the configs, and will make it easier for us to provide ""starter configs"" that can be filled without destroying the variables.
Support non-interpolated configs in registry.resolve: in that case, we keep a copy of the original config, fill and resolve the interpolated version (because that's the only way it can work) and then merge the original config with its variable references back into the filled config. This makes it possible to auto-fill a config with variables and add missing defaults etc. while keeping the references intact.
Add Config.interpolate method to interpolate an existing config (returns a copy).
Port config merging logic over from spaCy so it can live in Thinc instead, and add Config.merge that takes a configs and merges it with the current config, wtih some special logic to handle variables and promises.
Make it illegal save or interpolate a config with top-level keys that aren't sections (e.g. {""foo"": 1}, which can only really happen if you construct a Conig from a dict). Otherwise, Python's configparser will add them to a section [DEFAULT] and when the serialized config is loaded back in from a string, those values are added to every section (!) which is almost never what you want and causes very confusing results and validation errors.
Raise better error if a variable references a section within a string, i.e. ""hello ${foo.bar}"". If the section is a regular dict, this would work in theory, and you'd get the string ""hello {'x': 'y'}""  but it most likely indicates a mistake. If the section is a reference to a registered function, it can't work, because registered functions are resolved after the config is created and interpolated.

Open questions

Does it make sense to have interpolate=True as the default? Realistically, we'd be setting it to False a lot in spaCy internally, but from a user's perspective, I think it makes more sense if Config.from_str etc. always returns the interpolated final version and doesn't require calling .interpolate() manually or setting interpolate=True? Or maybe it is actually more logical if it doesn't ""destroy"" variables by default? registry.resolve will always interpolate anyways and can handle variables, so that's no problem. So I might actually be leaning towards default=False now ",True,{}
explosion/thinc,https://github.com/explosion/thinc,377,2020-08-12T14:56:07Z,2020-08-12T21:32:17Z,2020-08-12T22:46:42Z,MERGED,True,342,84,3,https://github.com/ines,Support optional variable interpolation and fix config resolution,9,"['enhancement', 'feat / config']",https://github.com/explosion/thinc/pull/377,https://github.com/apps/codecov,2,https://github.com/explosion/thinc/pull/377#issuecomment-672951952,"Allow setting interpolate=False on Config.to_str, Config.from_str and other serialization methods. This will preserve variables in the configs, and will make it easier for us to provide ""starter configs"" that can be filled without destroying the variables.
Support non-interpolated configs in registry.resolve: in that case, we keep a copy of the original config, fill and resolve the interpolated version (because that's the only way it can work) and then merge the original config with its variable references back into the filled config. This makes it possible to auto-fill a config with variables and add missing defaults etc. while keeping the references intact.
Add Config.interpolate method to interpolate an existing config (returns a copy).
Port config merging logic over from spaCy so it can live in Thinc instead, and add Config.merge that takes a configs and merges it with the current config, wtih some special logic to handle variables and promises.
Make it illegal save or interpolate a config with top-level keys that aren't sections (e.g. {""foo"": 1}, which can only really happen if you construct a Conig from a dict). Otherwise, Python's configparser will add them to a section [DEFAULT] and when the serialized config is loaded back in from a string, those values are added to every section (!) which is almost never what you want and causes very confusing results and validation errors.
Raise better error if a variable references a section within a string, i.e. ""hello ${foo.bar}"". If the section is a regular dict, this would work in theory, and you'd get the string ""hello {'x': 'y'}""  but it most likely indicates a mistake. If the section is a reference to a registered function, it can't work, because registered functions are resolved after the config is created and interpolated.

Open questions

Does it make sense to have interpolate=True as the default? Realistically, we'd be setting it to False a lot in spaCy internally, but from a user's perspective, I think it makes more sense if Config.from_str etc. always returns the interpolated final version and doesn't require calling .interpolate() manually or setting interpolate=True? Or maybe it is actually more logical if it doesn't ""destroy"" variables by default? registry.resolve will always interpolate anyways and can handle variables, so that's no problem. So I might actually be leaning towards default=False now ","Codecov Report

Merging #377 into master will increase coverage by 0.31%.
The diff coverage is 97.64%.


@@            Coverage Diff             @@
##           master     #377      +/-   ##
==========================================
+ Coverage   57.36%   57.68%   +0.31%     
==========================================
  Files          99       99              
  Lines        7051     7109      +58     
==========================================
+ Hits         4045     4101      +56     
- Misses       3006     3008       +2     



Impacted Files
Coverage 





thinc/config.py
95.94% <97.61%> (+0.09%)



thinc/about.py
100.00% <100.00%> ()





Continue to review full report at Codecov.

Legend - Click here to learn more
 = absolute <relative> (impact),  = not affected, ? = missing data
Powered by Codecov. Last update 180162c...7497aa7. Read the comment docs.",True,{}
explosion/thinc,https://github.com/explosion/thinc,377,2020-08-12T14:56:07Z,2020-08-12T21:32:17Z,2020-08-12T22:46:42Z,MERGED,True,342,84,3,https://github.com/ines,Support optional variable interpolation and fix config resolution,9,"['enhancement', 'feat / config']",https://github.com/explosion/thinc/pull/377,https://github.com/honnibal,3,https://github.com/explosion/thinc/pull/377#issuecomment-673109543,"Allow setting interpolate=False on Config.to_str, Config.from_str and other serialization methods. This will preserve variables in the configs, and will make it easier for us to provide ""starter configs"" that can be filled without destroying the variables.
Support non-interpolated configs in registry.resolve: in that case, we keep a copy of the original config, fill and resolve the interpolated version (because that's the only way it can work) and then merge the original config with its variable references back into the filled config. This makes it possible to auto-fill a config with variables and add missing defaults etc. while keeping the references intact.
Add Config.interpolate method to interpolate an existing config (returns a copy).
Port config merging logic over from spaCy so it can live in Thinc instead, and add Config.merge that takes a configs and merges it with the current config, wtih some special logic to handle variables and promises.
Make it illegal save or interpolate a config with top-level keys that aren't sections (e.g. {""foo"": 1}, which can only really happen if you construct a Conig from a dict). Otherwise, Python's configparser will add them to a section [DEFAULT] and when the serialized config is loaded back in from a string, those values are added to every section (!) which is almost never what you want and causes very confusing results and validation errors.
Raise better error if a variable references a section within a string, i.e. ""hello ${foo.bar}"". If the section is a regular dict, this would work in theory, and you'd get the string ""hello {'x': 'y'}""  but it most likely indicates a mistake. If the section is a reference to a registered function, it can't work, because registered functions are resolved after the config is created and interpolated.

Open questions

Does it make sense to have interpolate=True as the default? Realistically, we'd be setting it to False a lot in spaCy internally, but from a user's perspective, I think it makes more sense if Config.from_str etc. always returns the interpolated final version and doesn't require calling .interpolate() manually or setting interpolate=True? Or maybe it is actually more logical if it doesn't ""destroy"" variables by default? registry.resolve will always interpolate anyways and can handle variables, so that's no problem. So I might actually be leaning towards default=False now ","I kind of lean towards interpolate=True I think. I think the most basic thing is like, you want to get the values. So it makes sense to me to handle the interpolation.",True,{}
explosion/thinc,https://github.com/explosion/thinc,378,2020-08-13T09:13:16Z,2020-08-13T13:03:41Z,2020-08-13T13:03:44Z,MERGED,True,106,27,3,https://github.com/ines,Fix reporting of whether config is interpolated,10,"['bug', 'enhancement', 'feat / config']",https://github.com/explosion/thinc/pull/378,https://github.com/ines,1,https://github.com/explosion/thinc/pull/378,"This is relevant, because it impacts how a config is resolved and whether it's interpolated before.
Summary of interpolation / resolving non-interpolated configs
config_str = """"""
[variables]
width = 123

[model]
@architectures = ""my_model""
width = ${variables:width}
""""""

@registry.architectures(""my_model"")
def my_model(width: int, depth: int = 456):
    ...
Auto-fill partial config (e.g. as part of init config command)
config = Config().from_str(config_str)
filled = registry.fill_config(config)
Result:
[variables]
width = 123

[model]
@architectures = ""my_model""
width = 123
depth = 456
Problem: reference to ${variables:width} is gone. A user's CLI script that overwrites --variables.width will now produce different and very misleading results. All the convenience of referencing blocks and values is gone, just because the user wanted to fill in a few defaults.
Solution: we now also support loading configs without interpolating and  resolving the variables on load. Config can be interpolated later by calling config.interpolate().
raw_config = Config().from_str(config_str, interpolate=False)
# filled = registry.fill_config(raw_config)
Next problem: resolving (create functions, check signatures) and filling (add missing defaults) is always done jointly in registry.resolve. In order to do this, we need to know all values so we can create the functions, so we need to interpolate the config.
# internally:
interpolated = raw_config.interpolate()
filled, resolved = registry.resolve(interpolated)
Next problem: The filled config now looks like the config above and doesn't include variables anymore. But we still have our original non-interpolated config, so we can create a final config by putting the two configs together:

the filled config provides the ""truth"", including additional properties
the original non-interpolated config provides the overrides for the fields that were originally variables

# internally:
filled = filled.merge(raw_config)
Result:
[variables]
width = 123

[model]
@architectures = ""my_model""
width = ${variables:width}
depth = 456","This is relevant, because it impacts how a config is resolved and whether it's interpolated before.
Summary of interpolation / resolving non-interpolated configs
config_str = """"""
[variables]
width = 123

[model]
@architectures = ""my_model""
width = ${variables:width}
""""""

@registry.architectures(""my_model"")
def my_model(width: int, depth: int = 456):
    ...
Auto-fill partial config (e.g. as part of init config command)
config = Config().from_str(config_str)
filled = registry.fill_config(config)
Result:
[variables]
width = 123

[model]
@architectures = ""my_model""
width = 123
depth = 456
Problem: reference to ${variables:width} is gone. A user's CLI script that overwrites --variables.width will now produce different and very misleading results. All the convenience of referencing blocks and values is gone, just because the user wanted to fill in a few defaults.
Solution: we now also support loading configs without interpolating and  resolving the variables on load. Config can be interpolated later by calling config.interpolate().
raw_config = Config().from_str(config_str, interpolate=False)
# filled = registry.fill_config(raw_config)
Next problem: resolving (create functions, check signatures) and filling (add missing defaults) is always done jointly in registry.resolve. In order to do this, we need to know all values so we can create the functions, so we need to interpolate the config.
# internally:
interpolated = raw_config.interpolate()
filled, resolved = registry.resolve(interpolated)
Next problem: The filled config now looks like the config above and doesn't include variables anymore. But we still have our original non-interpolated config, so we can create a final config by putting the two configs together:

the filled config provides the ""truth"", including additional properties
the original non-interpolated config provides the overrides for the fields that were originally variables

# internally:
filled = filled.merge(raw_config)
Result:
[variables]
width = 123

[model]
@architectures = ""my_model""
width = ${variables:width}
depth = 456",True,{'ROCKET': ['https://github.com/svlandeg']}
explosion/thinc,https://github.com/explosion/thinc,378,2020-08-13T09:13:16Z,2020-08-13T13:03:41Z,2020-08-13T13:03:44Z,MERGED,True,106,27,3,https://github.com/ines,Fix reporting of whether config is interpolated,10,"['bug', 'enhancement', 'feat / config']",https://github.com/explosion/thinc/pull/378,https://github.com/apps/codecov,2,https://github.com/explosion/thinc/pull/378#issuecomment-673365969,"This is relevant, because it impacts how a config is resolved and whether it's interpolated before.
Summary of interpolation / resolving non-interpolated configs
config_str = """"""
[variables]
width = 123

[model]
@architectures = ""my_model""
width = ${variables:width}
""""""

@registry.architectures(""my_model"")
def my_model(width: int, depth: int = 456):
    ...
Auto-fill partial config (e.g. as part of init config command)
config = Config().from_str(config_str)
filled = registry.fill_config(config)
Result:
[variables]
width = 123

[model]
@architectures = ""my_model""
width = 123
depth = 456
Problem: reference to ${variables:width} is gone. A user's CLI script that overwrites --variables.width will now produce different and very misleading results. All the convenience of referencing blocks and values is gone, just because the user wanted to fill in a few defaults.
Solution: we now also support loading configs without interpolating and  resolving the variables on load. Config can be interpolated later by calling config.interpolate().
raw_config = Config().from_str(config_str, interpolate=False)
# filled = registry.fill_config(raw_config)
Next problem: resolving (create functions, check signatures) and filling (add missing defaults) is always done jointly in registry.resolve. In order to do this, we need to know all values so we can create the functions, so we need to interpolate the config.
# internally:
interpolated = raw_config.interpolate()
filled, resolved = registry.resolve(interpolated)
Next problem: The filled config now looks like the config above and doesn't include variables anymore. But we still have our original non-interpolated config, so we can create a final config by putting the two configs together:

the filled config provides the ""truth"", including additional properties
the original non-interpolated config provides the overrides for the fields that were originally variables

# internally:
filled = filled.merge(raw_config)
Result:
[variables]
width = 123

[model]
@architectures = ""my_model""
width = ${variables:width}
depth = 456","Codecov Report

Merging #378 into master will increase coverage by 0.04%.
The diff coverage is 100.00%.


@@            Coverage Diff             @@
##           master     #378      +/-   ##
==========================================
+ Coverage   57.68%   57.73%   +0.04%     
==========================================
  Files          99       99              
  Lines        7109     7115       +6     
==========================================
+ Hits         4101     4108       +7     
+ Misses       3008     3007       -1     



Impacted Files
Coverage 





thinc/about.py
100.00% <100.00%> ()



thinc/config.py
96.22% <100.00%> (+0.27%)





Continue to review full report at Codecov.

Legend - Click here to learn more
 = absolute <relative> (impact),  = not affected, ? = missing data
Powered by Codecov. Last update e52d25d...d4c5c44. Read the comment docs.",True,{}
explosion/thinc,https://github.com/explosion/thinc,379,2020-08-13T16:51:59Z,2020-08-14T13:37:29Z,2020-08-14T13:55:20Z,MERGED,True,86,11,3,https://github.com/ines,Support custom sort in serialization methods,7,"['enhancement', 'feat / config']",https://github.com/explosion/thinc/pull/379,https://github.com/ines,1,https://github.com/explosion/thinc/pull/379,"Allows passing in a sort_key, a function that takes (key, value) tuples. Only applied to the top-level sections. The default alphabetic order isn't always the best.
TODO

 fix sorting for subsections
 implement sort order setting
 Does it make more sense to allow passing in a complete or incomplete list of section names instead? This is likely the most common use case: you want to specify an exact order of sections, rather than sorting them in reverse or by value or whatever.
 Does it make more sense to set the sort key on Config initialization? This would make it easier to integrate it under the hood because it means you only have to define the sort key once and not every single time you call to_str in various places.","Allows passing in a sort_key, a function that takes (key, value) tuples. Only applied to the top-level sections. The default alphabetic order isn't always the best.
TODO

 fix sorting for subsections
 implement sort order setting
 Does it make more sense to allow passing in a complete or incomplete list of section names instead? This is likely the most common use case: you want to specify an exact order of sections, rather than sorting them in reverse or by value or whatever.
 Does it make more sense to set the sort key on Config initialization? This would make it easier to integrate it under the hood because it means you only have to define the sort key once and not every single time you call to_str in various places.",True,{}
explosion/thinc,https://github.com/explosion/thinc,379,2020-08-13T16:51:59Z,2020-08-14T13:37:29Z,2020-08-14T13:55:20Z,MERGED,True,86,11,3,https://github.com/ines,Support custom sort in serialization methods,7,"['enhancement', 'feat / config']",https://github.com/explosion/thinc/pull/379,https://github.com/apps/codecov,2,https://github.com/explosion/thinc/pull/379#issuecomment-673592769,"Allows passing in a sort_key, a function that takes (key, value) tuples. Only applied to the top-level sections. The default alphabetic order isn't always the best.
TODO

 fix sorting for subsections
 implement sort order setting
 Does it make more sense to allow passing in a complete or incomplete list of section names instead? This is likely the most common use case: you want to specify an exact order of sections, rather than sorting them in reverse or by value or whatever.
 Does it make more sense to set the sort key on Config initialization? This would make it easier to integrate it under the hood because it means you only have to define the sort key once and not every single time you call to_str in various places.","Codecov Report

Merging #379 into master will increase coverage by 0.07%.
The diff coverage is 100.00%.


@@            Coverage Diff             @@
##           master     #379      +/-   ##
==========================================
+ Coverage   57.73%   57.80%   +0.07%     
==========================================
  Files          99       99              
  Lines        7115     7127      +12     
==========================================
+ Hits         4108     4120      +12     
  Misses       3007     3007              



Impacted Files
Coverage 





thinc/about.py
100.00% <100.00%> ()



thinc/config.py
96.32% <100.00%> (+0.09%)





Continue to review full report at Codecov.

Legend - Click here to learn more
 = absolute <relative> (impact),  = not affected, ? = missing data
Powered by Codecov. Last update 97d19be...e0e99a0. Read the comment docs.",True,{}
explosion/thinc,https://github.com/explosion/thinc,379,2020-08-13T16:51:59Z,2020-08-14T13:37:29Z,2020-08-14T13:55:20Z,MERGED,True,86,11,3,https://github.com/ines,Support custom sort in serialization methods,7,"['enhancement', 'feat / config']",https://github.com/explosion/thinc/pull/379,https://github.com/honnibal,3,https://github.com/explosion/thinc/pull/379#issuecomment-673722276,"Allows passing in a sort_key, a function that takes (key, value) tuples. Only applied to the top-level sections. The default alphabetic order isn't always the best.
TODO

 fix sorting for subsections
 implement sort order setting
 Does it make more sense to allow passing in a complete or incomplete list of section names instead? This is likely the most common use case: you want to specify an exact order of sections, rather than sorting them in reverse or by value or whatever.
 Does it make more sense to set the sort key on Config initialization? This would make it easier to integrate it under the hood because it means you only have to define the sort key once and not every single time you call to_str in various places.","Does it make more sense to allow passing in a complete or incomplete list of section names instead?

Yeah, I think it does. You always have the list when you go to serialize, so if you had a function it'd be trivial to get the ordered list. And yeah the basic use-case is knowing what order you want.

Does it make more sense to set the sort key on Config initialization?

That would be okay, yeah. Could a config set it on from_str? So you get the sections back in the order they came in.
Edit: Actually I'm not sure my idea there makes sense. Hm",True,{}
explosion/thinc,https://github.com/explosion/thinc,381,2020-08-14T15:03:53Z,2020-08-14T15:32:58Z,2020-08-14T15:36:06Z,MERGED,True,17,8,3,https://github.com/ines,Fix pickling Config,1,"['bug', 'feat / config']",https://github.com/explosion/thinc/pull/381,https://github.com/ines,1,https://github.com/explosion/thinc/pull/381,,,True,{}
explosion/thinc,https://github.com/explosion/thinc,381,2020-08-14T15:03:53Z,2020-08-14T15:32:58Z,2020-08-14T15:36:06Z,MERGED,True,17,8,3,https://github.com/ines,Fix pickling Config,1,"['bug', 'feat / config']",https://github.com/explosion/thinc/pull/381,https://github.com/apps/codecov,2,https://github.com/explosion/thinc/pull/381#issuecomment-674121306,,"Codecov Report

Merging #381 into master will decrease coverage by 0.00%.
The diff coverage is 100.00%.


@@            Coverage Diff             @@
##           master     #381      +/-   ##
==========================================
- Coverage   57.80%   57.80%   -0.01%     
==========================================
  Files          99       99              
  Lines        7127     7126       -1     
==========================================
- Hits         4120     4119       -1     
  Misses       3007     3007              



Impacted Files
Coverage 





thinc/about.py
100.00% <100.00%> ()



thinc/config.py
96.31% <100.00%> (-0.01%)





Continue to review full report at Codecov.

Legend - Click here to learn more
 = absolute <relative> (impact),  = not affected, ? = missing data
Powered by Codecov. Last update b6f5ea5...a8e7026. Read the comment docs.",True,{}
explosion/thinc,https://github.com/explosion/thinc,382,2020-08-19T12:37:48Z,2020-08-19T14:03:23Z,2020-08-19T14:03:25Z,MERGED,True,74,54,3,https://github.com/ines,Support ${a.b} and ${a:b} in variables for values and sections,2,"['enhancement', 'feat / config']",https://github.com/explosion/thinc/pull/382,https://github.com/ines,1,https://github.com/explosion/thinc/pull/382,"Configs support variables of two different types: references to other values via ${a:b} (like Python's regular configparser) and references to other sections via ${a.b} (requires a custom implementation where we add a placeholder and then substitute the block once the config is interpreted).
Problems

It's very easy to make mistakes here and accidentally write ${a.b} instead of ${a:b}.
The value vs. section distinction makes sense on the implementation level, but not actually from the usage perspective: values in a Thinc config can be set explicitly or using a section (batch_size = 128 vs. [training.batch_size]). There's virtually no difference in terms of the result it produces, and a user can easily swap in a section and vice versa. So having to change a variable reference from : to ., just because it's now a section seems pretty unintuitive and against the config philosophy.

Solution

Treat both ${a.b} and ${a:b} the same and default to the . syntax internally. Don't expect the value to be the string following :  instead, expect it to the the last item following the last ..
When trying to fetch a reference, check if it's a regular value first (default configparser interpolation) and use a fallback value if no value is found. If the fallback is returned, try again as a section. (If no section exists either, the user will see an error regardless.)","Configs support variables of two different types: references to other values via ${a:b} (like Python's regular configparser) and references to other sections via ${a.b} (requires a custom implementation where we add a placeholder and then substitute the block once the config is interpreted).
Problems

It's very easy to make mistakes here and accidentally write ${a.b} instead of ${a:b}.
The value vs. section distinction makes sense on the implementation level, but not actually from the usage perspective: values in a Thinc config can be set explicitly or using a section (batch_size = 128 vs. [training.batch_size]). There's virtually no difference in terms of the result it produces, and a user can easily swap in a section and vice versa. So having to change a variable reference from : to ., just because it's now a section seems pretty unintuitive and against the config philosophy.

Solution

Treat both ${a.b} and ${a:b} the same and default to the . syntax internally. Don't expect the value to be the string following :  instead, expect it to the the last item following the last ..
When trying to fetch a reference, check if it's a regular value first (default configparser interpolation) and use a fallback value if no value is found. If the fallback is returned, try again as a section. (If no section exists either, the user will see an error regardless.)",True,"{'ROCKET': ['https://github.com/svlandeg'], 'THUMBS_UP': ['https://github.com/svlandeg']}"
explosion/thinc,https://github.com/explosion/thinc,382,2020-08-19T12:37:48Z,2020-08-19T14:03:23Z,2020-08-19T14:03:25Z,MERGED,True,74,54,3,https://github.com/ines,Support ${a.b} and ${a:b} in variables for values and sections,2,"['enhancement', 'feat / config']",https://github.com/explosion/thinc/pull/382,https://github.com/apps/codecov,2,https://github.com/explosion/thinc/pull/382#issuecomment-676286341,"Configs support variables of two different types: references to other values via ${a:b} (like Python's regular configparser) and references to other sections via ${a.b} (requires a custom implementation where we add a placeholder and then substitute the block once the config is interpreted).
Problems

It's very easy to make mistakes here and accidentally write ${a.b} instead of ${a:b}.
The value vs. section distinction makes sense on the implementation level, but not actually from the usage perspective: values in a Thinc config can be set explicitly or using a section (batch_size = 128 vs. [training.batch_size]). There's virtually no difference in terms of the result it produces, and a user can easily swap in a section and vice versa. So having to change a variable reference from : to ., just because it's now a section seems pretty unintuitive and against the config philosophy.

Solution

Treat both ${a.b} and ${a:b} the same and default to the . syntax internally. Don't expect the value to be the string following :  instead, expect it to the the last item following the last ..
When trying to fetch a reference, check if it's a regular value first (default configparser interpolation) and use a fallback value if no value is found. If the fallback is returned, try again as a section. (If no section exists either, the user will see an error regardless.)","Codecov Report

Merging #382 into master will increase coverage by 0.01%.
The diff coverage is 100.00%.


@@            Coverage Diff             @@
##           master     #382      +/-   ##
==========================================
+ Coverage   57.80%   57.82%   +0.01%     
==========================================
  Files          99       99              
  Lines        7126     7129       +3     
==========================================
+ Hits         4119     4122       +3     
  Misses       3007     3007              



Impacted Files
Coverage 





thinc/about.py
100.00% <100.00%> ()



thinc/config.py
96.33% <100.00%> (+0.02%)





Continue to review full report at Codecov.

Legend - Click here to learn more
 = absolute <relative> (impact),  = not affected, ? = missing data
Powered by Codecov. Last update de3958a...49143dd. Read the comment docs.",True,{}
explosion/thinc,https://github.com/explosion/thinc,383,2020-08-19T14:14:20Z,2020-08-19T17:41:23Z,2020-08-19T17:49:36Z,MERGED,True,1,1,1,https://github.com/svlandeg,Fix flaky test with higher learning rate,1,['tests'],https://github.com/explosion/thinc/pull/383,https://github.com/svlandeg,1,https://github.com/explosion/thinc/pull/383,Fix flaky test by increasing the learning rate - should converge much faster now.,Fix flaky test by increasing the learning rate - should converge much faster now.,True,{'ROCKET': ['https://github.com/ines']}
explosion/thinc,https://github.com/explosion/thinc,383,2020-08-19T14:14:20Z,2020-08-19T17:41:23Z,2020-08-19T17:49:36Z,MERGED,True,1,1,1,https://github.com/svlandeg,Fix flaky test with higher learning rate,1,['tests'],https://github.com/explosion/thinc/pull/383,https://github.com/apps/codecov,2,https://github.com/explosion/thinc/pull/383#issuecomment-676436280,Fix flaky test by increasing the learning rate - should converge much faster now.,"Codecov Report

Merging #383 into master will increase coverage by 0.01%.
The diff coverage is 0.00%.


@@            Coverage Diff             @@
##           master     #383      +/-   ##
==========================================
+ Coverage   57.80%   57.82%   +0.01%     
==========================================
  Files          99       99              
  Lines        7126     7129       +3     
==========================================
+ Hits         4119     4122       +3     
  Misses       3007     3007              



Impacted Files
Coverage 





thinc/tests/layers/test_pytorch_wrapper.py
0.00% <0.00%> ()



thinc/config.py
96.33% <0.00%> (+0.02%)





Continue to review full report at Codecov.

Legend - Click here to learn more
 = absolute <relative> (impact),  = not affected, ? = missing data
Powered by Codecov. Last update de3958a...7773af1. Read the comment docs.",True,{}
explosion/thinc,https://github.com/explosion/thinc,384,2020-08-23T11:43:37Z,2020-08-23T12:38:27Z,2020-08-23T12:38:29Z,MERGED,True,115,43,3,https://github.com/ines,Improve variable interpolation for lists,5,"['enhancement', 'feat / config']",https://github.com/explosion/thinc/pull/384,https://github.com/ines,1,https://github.com/explosion/thinc/pull/384,"This now works as expected:
[section]
a = 10
b = ""hello""

[other_section]
value = [""hello ${section.a}"", ""${section.b}""]
other_value = [${section}]
{
    ""other_section"": {
        ""value"": [""hello 10"", ""hello""],
        ""other_value"": [{""a"": 10, ""b"": ""hello""}]
    }
}
The mechanism allowing section references in lists will likely also simplify config setups with variable positional arguments, since you can define the objects as blocks and then write * = [${section.block}, ${section.other_block}] etc.
We won't be able to handle values like [${section.a}] because we can't load them as JSON once they're parsed and created. They work if you use interpolate=True, but not if you set interpolate=False and try to interpolate later.","This now works as expected:
[section]
a = 10
b = ""hello""

[other_section]
value = [""hello ${section.a}"", ""${section.b}""]
other_value = [${section}]
{
    ""other_section"": {
        ""value"": [""hello 10"", ""hello""],
        ""other_value"": [{""a"": 10, ""b"": ""hello""}]
    }
}
The mechanism allowing section references in lists will likely also simplify config setups with variable positional arguments, since you can define the objects as blocks and then write * = [${section.block}, ${section.other_block}] etc.
We won't be able to handle values like [${section.a}] because we can't load them as JSON once they're parsed and created. They work if you use interpolate=True, but not if you set interpolate=False and try to interpolate later.",True,{}
explosion/thinc,https://github.com/explosion/thinc,384,2020-08-23T11:43:37Z,2020-08-23T12:38:27Z,2020-08-23T12:38:29Z,MERGED,True,115,43,3,https://github.com/ines,Improve variable interpolation for lists,5,"['enhancement', 'feat / config']",https://github.com/explosion/thinc/pull/384,https://github.com/apps/codecov,2,https://github.com/explosion/thinc/pull/384#issuecomment-678764325,"This now works as expected:
[section]
a = 10
b = ""hello""

[other_section]
value = [""hello ${section.a}"", ""${section.b}""]
other_value = [${section}]
{
    ""other_section"": {
        ""value"": [""hello 10"", ""hello""],
        ""other_value"": [{""a"": 10, ""b"": ""hello""}]
    }
}
The mechanism allowing section references in lists will likely also simplify config setups with variable positional arguments, since you can define the objects as blocks and then write * = [${section.block}, ${section.other_block}] etc.
We won't be able to handle values like [${section.a}] because we can't load them as JSON once they're parsed and created. They work if you use interpolate=True, but not if you set interpolate=False and try to interpolate later.","Codecov Report

Merging #384 into master will increase coverage by 0.07%.
The diff coverage is 88.88%.


@@            Coverage Diff             @@
##           master     #384      +/-   ##
==========================================
+ Coverage   57.82%   57.89%   +0.07%     
==========================================
  Files          99       99              
  Lines        7129     7144      +15     
==========================================
+ Hits         4122     4136      +14     
- Misses       3007     3008       +1     



Impacted Files
Coverage 





thinc/config.py
96.24% <88.63%> (-0.10%)



thinc/about.py
100.00% <100.00%> ()





Continue to review full report at Codecov.

Legend - Click here to learn more
 = absolute <relative> (impact),  = not affected, ? = missing data
Powered by Codecov. Last update a63ad1e...187098e. Read the comment docs.",True,{}
explosion/thinc,https://github.com/explosion/thinc,385,2020-08-24T13:52:58Z,2020-08-24T15:01:55Z,2020-08-24T17:47:39Z,MERGED,True,72,26,3,https://github.com/ines,Support remove extra fields from config when filling,4,"['enhancement', 'feat / config']",https://github.com/explosion/thinc/pull/385,https://github.com/ines,1,https://github.com/explosion/thinc/pull/385,"When filling and resolving a config, there are currently two options:

with validation: will raise an error if values are wrong or missing
without validation: will fill in all gaps and try to fix problems but not raise errors

However, we didn't handle extra values if validation was disabled. In that case, we're only constructing the config from the schema (in the most efficient way possible, using pydantic). So we need to filter all fields that are not in the original schema manually, if the schema says it doesn't allow extra fields.
To make this work consistently with both interpolated and non-interpolated configs, we need a little hack: to restore the variabels, we're merging the original non-interpolated config into the final filled config. This works fine, unless the original config includes extra values that have been removed during filling. So for that particular merge operation, we need to remove all extra values (which should only happen in this context  a filled config that removed extra settings).","When filling and resolving a config, there are currently two options:

with validation: will raise an error if values are wrong or missing
without validation: will fill in all gaps and try to fix problems but not raise errors

However, we didn't handle extra values if validation was disabled. In that case, we're only constructing the config from the schema (in the most efficient way possible, using pydantic). So we need to filter all fields that are not in the original schema manually, if the schema says it doesn't allow extra fields.
To make this work consistently with both interpolated and non-interpolated configs, we need a little hack: to restore the variabels, we're merging the original non-interpolated config into the final filled config. This works fine, unless the original config includes extra values that have been removed during filling. So for that particular merge operation, we need to remove all extra values (which should only happen in this context  a filled config that removed extra settings).",True,{}
explosion/thinc,https://github.com/explosion/thinc,385,2020-08-24T13:52:58Z,2020-08-24T15:01:55Z,2020-08-24T17:47:39Z,MERGED,True,72,26,3,https://github.com/ines,Support remove extra fields from config when filling,4,"['enhancement', 'feat / config']",https://github.com/explosion/thinc/pull/385,https://github.com/apps/codecov,2,https://github.com/explosion/thinc/pull/385#issuecomment-679142128,"When filling and resolving a config, there are currently two options:

with validation: will raise an error if values are wrong or missing
without validation: will fill in all gaps and try to fix problems but not raise errors

However, we didn't handle extra values if validation was disabled. In that case, we're only constructing the config from the schema (in the most efficient way possible, using pydantic). So we need to filter all fields that are not in the original schema manually, if the schema says it doesn't allow extra fields.
To make this work consistently with both interpolated and non-interpolated configs, we need a little hack: to restore the variabels, we're merging the original non-interpolated config into the final filled config. This works fine, unless the original config includes extra values that have been removed during filling. So for that particular merge operation, we need to remove all extra values (which should only happen in this context  a filled config that removed extra settings).","Codecov Report

Merging #385 into master will increase coverage by 0.08%.
The diff coverage is 100.00%.


@@            Coverage Diff             @@
##           master     #385      +/-   ##
==========================================
+ Coverage   57.89%   57.98%   +0.08%     
==========================================
  Files          99       99              
  Lines        7144     7159      +15     
==========================================
+ Hits         4136     4151      +15     
  Misses       3008     3008              



Impacted Files
Coverage 





thinc/about.py
100.00% <100.00%> ()



thinc/config.py
96.35% <100.00%> (+0.11%)





Continue to review full report at Codecov.

Legend - Click here to learn more
 = absolute <relative> (impact),  = not affected, ? = missing data
Powered by Codecov. Last update dbf9053...2193817. Read the comment docs.",True,{}
explosion/thinc,https://github.com/explosion/thinc,386,2020-08-28T07:26:41Z,2020-08-28T15:16:10Z,2020-08-31T13:42:43Z,CLOSED,False,10,2,4,https://github.com/svlandeg,Listener flag,3,"['bug', 'feat / layers']",https://github.com/explosion/thinc/pull/386,https://github.com/svlandeg,1,https://github.com/explosion/thinc/pull/386,"Add is_listener flag in model.attrs, default False. If this flag is True, prevent calling model.predict directly for initialization/shape inference, as that won't work (the model is not independent)","Add is_listener flag in model.attrs, default False. If this flag is True, prevent calling model.predict directly for initialization/shape inference, as that won't work (the model is not independent)",True,{}
explosion/thinc,https://github.com/explosion/thinc,386,2020-08-28T07:26:41Z,2020-08-28T15:16:10Z,2020-08-31T13:42:43Z,CLOSED,False,10,2,4,https://github.com/svlandeg,Listener flag,3,"['bug', 'feat / layers']",https://github.com/explosion/thinc/pull/386,https://github.com/apps/codecov,2,https://github.com/explosion/thinc/pull/386#issuecomment-682376466,"Add is_listener flag in model.attrs, default False. If this flag is True, prevent calling model.predict directly for initialization/shape inference, as that won't work (the model is not independent)","Codecov Report

Merging #386 into master will decrease coverage by 0.00%.
The diff coverage is 50.00%.


@@            Coverage Diff             @@
##           master     #386      +/-   ##
==========================================
- Coverage   57.98%   57.97%   -0.01%     
==========================================
  Files          99       99              
  Lines        7159     7165       +6     
==========================================
+ Hits         4151     4154       +3     
- Misses       3008     3011       +3     



Impacted Files
Coverage 





thinc/layers/siamese.py
38.23% <0.00%> (-2.39%)



thinc/layers/chain.py
92.06% <50.00%> (-1.38%)



thinc/about.py
100.00% <100.00%> ()



thinc/model.py
99.75% <100.00%> (+<0.01%)





Continue to review full report at Codecov.

Legend - Click here to learn more
 = absolute <relative> (impact),  = not affected, ? = missing data
Powered by Codecov. Last update 5e90b03...d2e340b. Read the comment docs.",True,{}
explosion/thinc,https://github.com/explosion/thinc,386,2020-08-28T07:26:41Z,2020-08-28T15:16:10Z,2020-08-31T13:42:43Z,CLOSED,False,10,2,4,https://github.com/svlandeg,Listener flag,3,"['bug', 'feat / layers']",https://github.com/explosion/thinc/pull/386,https://github.com/honnibal,3,https://github.com/explosion/thinc/pull/386#issuecomment-682512347,"Add is_listener flag in model.attrs, default False. If this flag is True, prevent calling model.predict directly for initialization/shape inference, as that won't work (the model is not independent)","Ugh, I'm not sure how to solve this but this will be a nightmare :(. If we accept this as the solution, then every layer has to think about listeners. We need to find another way.",True,{'THUMBS_UP': ['https://github.com/svlandeg']}
explosion/thinc,https://github.com/explosion/thinc,387,2020-09-04T14:24:42Z,2020-09-05T10:03:34Z,2020-09-05T21:39:16Z,MERGED,True,15,4,3,https://github.com/svlandeg,fix with_cpu shape inference,2,"['bug', 'feat / layers']",https://github.com/explosion/thinc/pull/387,https://github.com/svlandeg,1,https://github.com/explosion/thinc/pull/387,"I couldn't get the textcat shape inference to work properly in spaCy. Ultimately tracked it down to with_cpu that was not properly hooked up with the right dimensions. This should fix it.
Also changed the name to include the encapsulated layer, as that simplifies debugging.
Bump to 8.0.0a31","I couldn't get the textcat shape inference to work properly in spaCy. Ultimately tracked it down to with_cpu that was not properly hooked up with the right dimensions. This should fix it.
Also changed the name to include the encapsulated layer, as that simplifies debugging.
Bump to 8.0.0a31",True,{}
explosion/thinc,https://github.com/explosion/thinc,387,2020-09-04T14:24:42Z,2020-09-05T10:03:34Z,2020-09-05T21:39:16Z,MERGED,True,15,4,3,https://github.com/svlandeg,fix with_cpu shape inference,2,"['bug', 'feat / layers']",https://github.com/explosion/thinc/pull/387,https://github.com/apps/codecov,2,https://github.com/explosion/thinc/pull/387#issuecomment-687181843,"I couldn't get the textcat shape inference to work properly in spaCy. Ultimately tracked it down to with_cpu that was not properly hooked up with the right dimensions. This should fix it.
Also changed the name to include the encapsulated layer, as that simplifies debugging.
Bump to 8.0.0a31","Codecov Report

Merging #387 into master will decrease coverage by 0.05%.
The diff coverage is 20.00%.


@@            Coverage Diff             @@
##           master     #387      +/-   ##
==========================================
- Coverage   57.98%   57.92%   -0.06%     
==========================================
  Files          99       99              
  Lines        7159     7166       +7     
==========================================
  Hits         4151     4151              
- Misses       3008     3015       +7     



Impacted Files
Coverage 





thinc/shims/pytorch.py
83.82% <> ()



thinc/layers/with_cpu.py
26.82% <11.11%> (-5.53%)



thinc/about.py
100.00% <100.00%> ()





Continue to review full report at Codecov.

Legend - Click here to learn more
 = absolute <relative> (impact),  = not affected, ? = missing data
Powered by Codecov. Last update 5e90b03...1c87688. Read the comment docs.",True,{}
explosion/thinc,https://github.com/explosion/thinc,388,2020-09-09T14:48:06Z,2020-09-10T12:43:47Z,2020-09-10T12:49:06Z,MERGED,True,11,11,11,https://github.com/svlandeg,nested names,4,['feat / layers'],https://github.com/explosion/thinc/pull/388,https://github.com/svlandeg,1,https://github.com/explosion/thinc/pull/388,"Maybe it's just me, but I kind of like analyzing a layer/model's architecture using its name. The ""syntax"" using - gets unusable with many nested layers though, so proposing () here instead.","Maybe it's just me, but I kind of like analyzing a layer/model's architecture using its name. The ""syntax"" using - gets unusable with many nested layers though, so proposing () here instead.",True,{}
explosion/thinc,https://github.com/explosion/thinc,388,2020-09-09T14:48:06Z,2020-09-10T12:43:47Z,2020-09-10T12:49:06Z,MERGED,True,11,11,11,https://github.com/svlandeg,nested names,4,['feat / layers'],https://github.com/explosion/thinc/pull/388,https://github.com/apps/codecov,2,https://github.com/explosion/thinc/pull/388#issuecomment-690137543,"Maybe it's just me, but I kind of like analyzing a layer/model's architecture using its name. The ""syntax"" using - gets unusable with many nested layers though, so proposing () here instead.","Codecov Report

Merging #388 into master will not change coverage.
The diff coverage is 66.66%.


@@           Coverage Diff           @@
##           master     #388   +/-   ##
=======================================
  Coverage   57.94%   57.94%           
=======================================
  Files          99       99           
  Lines        7179     7179           
=======================================
  Hits         4160     4160           
  Misses       3019     3019           



Impacted Files
Coverage 





thinc/layers/residual.py
88.88% <> ()



thinc/layers/uniqued.py
100.00% <> ()



thinc/layers/with_array.py
100.00% <> ()



thinc/layers/with_flatten.py
37.50% <0.00%> ()



thinc/layers/with_getitem.py
100.00% <> ()



thinc/layers/with_reshape.py
23.07% <> ()



thinc/tests/layers/test_lstm.py
0.00% <0.00%> ()



thinc/layers/with_debug.py
100.00% <100.00%> ()



thinc/layers/with_list.py
100.00% <100.00%> ()



thinc/layers/with_padded.py
80.51% <100.00%> ()



... and 1 more






Continue to review full report at Codecov.

Legend - Click here to learn more
 = absolute <relative> (impact),  = not affected, ? = missing data
Powered by Codecov. Last update fce1722...a458809. Read the comment docs.",True,{}
explosion/thinc,https://github.com/explosion/thinc,389,2020-09-16T13:59:17Z,2020-09-16T15:55:19Z,2020-09-16T20:04:36Z,MERGED,True,30,6,3,https://github.com/svlandeg,check for conflicting promise before merging,2,"['bug', 'feat / config']",https://github.com/explosion/thinc/pull/389,https://github.com/svlandeg,1,https://github.com/explosion/thinc/pull/389,"There was an issue when merging a config with a default. If the original config has a promise (@foo) but the default has something else that doesn't match, the default values would still get added in the merged version. This was because the continue in the code started from the viewpoint of the default config. So I think we need both viewpoints - hence the second condition for another continue.
Hopefully the new unit tests make it clear. We need this behaviour to be able to implement different version of training.corpus in spaCy.","There was an issue when merging a config with a default. If the original config has a promise (@foo) but the default has something else that doesn't match, the default values would still get added in the merged version. This was because the continue in the code started from the viewpoint of the default config. So I think we need both viewpoints - hence the second condition for another continue.
Hopefully the new unit tests make it clear. We need this behaviour to be able to implement different version of training.corpus in spaCy.",True,{'THUMBS_UP': ['https://github.com/ines']}
explosion/thinc,https://github.com/explosion/thinc,389,2020-09-16T13:59:17Z,2020-09-16T15:55:19Z,2020-09-16T20:04:36Z,MERGED,True,30,6,3,https://github.com/svlandeg,check for conflicting promise before merging,2,"['bug', 'feat / config']",https://github.com/explosion/thinc/pull/389,https://github.com/apps/codecov,2,https://github.com/explosion/thinc/pull/389#issuecomment-693429011,"There was an issue when merging a config with a default. If the original config has a promise (@foo) but the default has something else that doesn't match, the default values would still get added in the merged version. This was because the continue in the code started from the viewpoint of the default config. So I think we need both viewpoints - hence the second condition for another continue.
Hopefully the new unit tests make it clear. We need this behaviour to be able to implement different version of training.corpus in spaCy.","Codecov Report

Merging #389 into master will increase coverage by 0.00%.
The diff coverage is 100.00%.


@@           Coverage Diff           @@
##           master     #389   +/-   ##
=======================================
  Coverage   57.94%   57.95%           
=======================================
  Files          99       99           
  Lines        7179     7183    +4     
=======================================
+ Hits         4160     4163    +3     
- Misses       3019     3020    +1     



Impacted Files
Coverage 





thinc/config.py
96.18% <100.00%> (-0.18%)





Continue to review full report at Codecov.

Legend - Click here to learn more
 = absolute <relative> (impact),  = not affected, ? = missing data
Powered by Codecov. Last update f0f1533...7cf2c1f. Read the comment docs.",True,{}
explosion/thinc,https://github.com/explosion/thinc,389,2020-09-16T13:59:17Z,2020-09-16T15:55:19Z,2020-09-16T20:04:36Z,MERGED,True,30,6,3,https://github.com/svlandeg,check for conflicting promise before merging,2,"['bug', 'feat / config']",https://github.com/explosion/thinc/pull/389,https://github.com/honnibal,3,https://github.com/explosion/thinc/pull/389#issuecomment-693499054,"There was an issue when merging a config with a default. If the original config has a promise (@foo) but the default has something else that doesn't match, the default values would still get added in the merged version. This was because the continue in the code started from the viewpoint of the default config. So I think we need both viewpoints - hence the second condition for another continue.
Hopefully the new unit tests make it clear. We need this behaviour to be able to implement different version of training.corpus in spaCy.",Looks good to me!,True,{}
explosion/thinc,https://github.com/explosion/thinc,390,2020-09-17T15:40:32Z,2020-09-18T13:36:29Z,2021-01-21T14:14:11Z,MERGED,True,15,16,8,https://github.com/svlandeg,ml datasets upgrade to 0.2,4,['install'],https://github.com/explosion/thinc/pull/390,https://github.com/svlandeg,1,https://github.com/explosion/thinc/pull/390,,,True,{}
explosion/thinc,https://github.com/explosion/thinc,390,2020-09-17T15:40:32Z,2020-09-18T13:36:29Z,2021-01-21T14:14:11Z,MERGED,True,15,16,8,https://github.com/svlandeg,ml datasets upgrade to 0.2,4,['install'],https://github.com/explosion/thinc/pull/390,https://github.com/apps/codecov,2,https://github.com/explosion/thinc/pull/390#issuecomment-694347747,,"Codecov Report

Merging #390 into master will increase coverage by 0.00%.
The diff coverage is 100.00%.


@@           Coverage Diff           @@
##           master     #390   +/-   ##
=======================================
  Coverage   57.94%   57.95%           
=======================================
  Files          99       99           
  Lines        7179     7183    +4     
=======================================
+ Hits         4160     4163    +3     
- Misses       3019     3020    +1     



Impacted Files
Coverage 





thinc/about.py
100.00% <100.00%> ()



thinc/config.py
96.18% <100.00%> (-0.18%)





Continue to review full report at Codecov.

Legend - Click here to learn more
 = absolute <relative> (impact),  = not affected, ? = missing data
Powered by Codecov. Last update 1578547...3113eaa. Read the comment docs.",True,{}
explosion/thinc,https://github.com/explosion/thinc,391,2020-09-18T08:33:24Z,2020-09-18T11:34:27Z,2020-09-28T08:16:03Z,CLOSED,False,46,74,8,https://github.com/svlandeg,allocator argument ,3,['performance'],https://github.com/explosion/thinc/pull/391,https://github.com/svlandeg,1,https://github.com/explosion/thinc/pull/391,"Incorporate allocator argument in prefer_gpu and require_gpu that can be ""tensorflow"" or ""pytorch"".
Version bump to 8.0.0a34
TODO after merging: Thinc docs","Incorporate allocator argument in prefer_gpu and require_gpu that can be ""tensorflow"" or ""pytorch"".
Version bump to 8.0.0a34
TODO after merging: Thinc docs",True,{}
explosion/thinc,https://github.com/explosion/thinc,391,2020-09-18T08:33:24Z,2020-09-18T11:34:27Z,2020-09-28T08:16:03Z,CLOSED,False,46,74,8,https://github.com/svlandeg,allocator argument ,3,['performance'],https://github.com/explosion/thinc/pull/391,https://github.com/apps/codecov,2,https://github.com/explosion/thinc/pull/391#issuecomment-694738592,"Incorporate allocator argument in prefer_gpu and require_gpu that can be ""tensorflow"" or ""pytorch"".
Version bump to 8.0.0a34
TODO after merging: Thinc docs","Codecov Report

Merging #391 into master will increase coverage by 0.00%.
The diff coverage is 88.88%.


@@           Coverage Diff           @@
##           master     #391   +/-   ##
=======================================
  Coverage   57.94%   57.95%           
=======================================
  Files          99       99           
  Lines        7179     7183    +4     
=======================================
+ Hits         4160     4163    +3     
- Misses       3019     3020    +1     



Impacted Files
Coverage 





thinc/api.py
100.00% <> ()



thinc/backends/__init__.py
94.59% <> ()



thinc/tests/model/test_model.py
0.00% <0.00%> ()



thinc/about.py
100.00% <100.00%> ()



thinc/config.py
96.18% <100.00%> (-0.18%)



thinc/util.py
92.70% <100.00%> (+0.05%)





Continue to review full report at Codecov.

Legend - Click here to learn more
 = absolute <relative> (impact),  = not affected, ? = missing data
Powered by Codecov. Last update 1578547...6b1eba4. Read the comment docs.",True,{}
explosion/thinc,https://github.com/explosion/thinc,392,2020-09-18T14:51:54Z,2020-09-18T18:48:49Z,2020-09-18T19:08:09Z,MERGED,True,14,2,3,https://github.com/svlandeg,set_gpu_allocator,4,"['enhancement', 'performance']",https://github.com/explosion/thinc/pull/392,https://github.com/svlandeg,1,https://github.com/explosion/thinc/pull/392,"introduce method set_gpu_allocator
version bump to 8.0.0a34","introduce method set_gpu_allocator
version bump to 8.0.0a34",True,{}
explosion/thinc,https://github.com/explosion/thinc,392,2020-09-18T14:51:54Z,2020-09-18T18:48:49Z,2020-09-18T19:08:09Z,MERGED,True,14,2,3,https://github.com/svlandeg,set_gpu_allocator,4,"['enhancement', 'performance']",https://github.com/explosion/thinc/pull/392,https://github.com/apps/codecov,2,https://github.com/explosion/thinc/pull/392#issuecomment-694920311,"introduce method set_gpu_allocator
version bump to 8.0.0a34","Codecov Report

Merging #392 into master will not change coverage.
The diff coverage is 100.00%.


@@           Coverage Diff           @@
##           master     #392   +/-   ##
=======================================
  Coverage   57.95%   57.95%           
=======================================
  Files          99       99           
  Lines        7183     7183           
=======================================
  Hits         4163     4163           
  Misses       3020     3020           



Impacted Files
Coverage 





thinc/backends/__init__.py
94.59% <> ()



thinc/about.py
100.00% <100.00%> ()



thinc/api.py
100.00% <100.00%> ()





Continue to review full report at Codecov.

Legend - Click here to learn more
 = absolute <relative> (impact),  = not affected, ? = missing data
Powered by Codecov. Last update bc80ed8...7df9d4f. Read the comment docs.",True,{}
explosion/thinc,https://github.com/explosion/thinc,393,2020-09-24T19:58:38Z,2020-09-24T22:04:44Z,2020-09-24T22:04:44Z,MERGED,True,2,2,2,https://github.com/svlandeg,fix model.from_dict,5,['bug'],https://github.com/explosion/thinc/pull/393,https://github.com/svlandeg,1,https://github.com/explosion/thinc/pull/393,"Fixed required to make spacy's pretraining work

bump 8.0.0a35","Fixed required to make spacy's pretraining work

bump 8.0.0a35",True,{}
explosion/thinc,https://github.com/explosion/thinc,393,2020-09-24T19:58:38Z,2020-09-24T22:04:44Z,2020-09-24T22:04:44Z,MERGED,True,2,2,2,https://github.com/svlandeg,fix model.from_dict,5,['bug'],https://github.com/explosion/thinc/pull/393,https://github.com/apps/codecov,2,https://github.com/explosion/thinc/pull/393#issuecomment-698575365,"Fixed required to make spacy's pretraining work

bump 8.0.0a35","Codecov Report

Merging #393 into master will decrease coverage by 3.10%.
The diff coverage is 100.00%.


@@            Coverage Diff             @@
##           master     #393      +/-   ##
==========================================
- Coverage   57.95%   54.85%   -3.11%     
==========================================
  Files          99       99              
  Lines        7183     7183              
==========================================
- Hits         4163     3940     -223     
- Misses       3020     3243     +223     



Impacted Files
Coverage 





thinc/about.py
100.00% <100.00%> ()



thinc/model.py
95.59% <100.00%> (-4.16%)



thinc/layers/mxnetwrapper.py
22.22% <0.00%> (-77.78%)



thinc/shims/mxnet.py
23.52% <0.00%> (-70.59%)



thinc/backends/jax_ops.py
27.92% <0.00%> (-19.92%)



thinc/util.py
89.70% <0.00%> (-2.95%)



thinc/backends/ops.py
80.78% <0.00%> (-0.52%)





Continue to review full report at Codecov.

Legend - Click here to learn more
 = absolute <relative> (impact),  = not affected, ? = missing data
Powered by Codecov. Last update 7a3fea3...86818ca. Read the comment docs.",True,{}
explosion/thinc,https://github.com/explosion/thinc,394,2020-09-24T20:58:28Z,2020-09-24T22:00:00Z,2021-01-21T14:13:52Z,MERGED,True,7,1048,10,https://github.com/honnibal,Remove jax for now,3,['feat / ops'],https://github.com/explosion/thinc/pull/394,https://github.com/honnibal,1,https://github.com/explosion/thinc/pull/394,It went out of date and is pretty far from working for now.,It went out of date and is pretty far from working for now.,True,{}
explosion/thinc,https://github.com/explosion/thinc,394,2020-09-24T20:58:28Z,2020-09-24T22:00:00Z,2021-01-21T14:13:52Z,MERGED,True,7,1048,10,https://github.com/honnibal,Remove jax for now,3,['feat / ops'],https://github.com/explosion/thinc/pull/394,https://github.com/apps/codecov,2,https://github.com/explosion/thinc/pull/394#issuecomment-698587693,It went out of date and is pretty far from working for now.,"Codecov Report

Merging #394 into master will increase coverage by 1.45%.
The diff coverage is 66.66%.


@@            Coverage Diff             @@
##           master     #394      +/-   ##
==========================================
+ Coverage   57.95%   59.41%   +1.45%     
==========================================
  Files          99       96       -3     
  Lines        7183     6574     -609     
==========================================
- Hits         4163     3906     -257     
+ Misses       3020     2668     -352     



Impacted Files
Coverage 





thinc/backends/ops.py
80.78% <> (-0.52%)



thinc/model.py
99.74% <> (-0.02%)



thinc/tests/backends/test_ops.py
0.00% <0.00%> ()



thinc/api.py
100.00% <100.00%> ()



thinc/backends/__init__.py
97.05% <100.00%> (+2.46%)



thinc/util.py
92.64% <100.00%> ()





Continue to review full report at Codecov.

Legend - Click here to learn more
 = absolute <relative> (impact),  = not affected, ? = missing data
Powered by Codecov. Last update 7a3fea3...c4e19e5. Read the comment docs.",True,{}
explosion/thinc,https://github.com/explosion/thinc,395,2020-09-24T22:37:53Z,2020-09-24T22:39:55Z,2020-09-24T22:39:56Z,MERGED,True,27898,0,87,https://github.com/ines,Add website & docs to main repo,1,['docs'],https://github.com/explosion/thinc/pull/395,https://github.com/ines,1,https://github.com/explosion/thinc/pull/395,,,True,{}
explosion/thinc,https://github.com/explosion/thinc,397,2020-09-26T10:12:12Z,2020-09-26T10:55:08Z,2020-09-26T10:55:10Z,MERGED,True,153,31,3,https://github.com/ines,Refactor ConfigValidationError and make it more extensible,3,"['enhancement', 'feat / config']",https://github.com/explosion/thinc/pull/397,https://github.com/ines,1,https://github.com/explosion/thinc/pull/397,This makes it easier to test for specific config validation errors (because we can easily inspect the errors in a structured way) and re-raise modified versions of the error (e.g. in spaCy).,This makes it easier to test for specific config validation errors (because we can easily inspect the errors in a structured way) and re-raise modified versions of the error (e.g. in spaCy).,True,{}
explosion/thinc,https://github.com/explosion/thinc,397,2020-09-26T10:12:12Z,2020-09-26T10:55:08Z,2020-09-26T10:55:10Z,MERGED,True,153,31,3,https://github.com/ines,Refactor ConfigValidationError and make it more extensible,3,"['enhancement', 'feat / config']",https://github.com/explosion/thinc/pull/397,https://github.com/apps/codecov,2,https://github.com/explosion/thinc/pull/397#issuecomment-699475132,This makes it easier to test for specific config validation errors (because we can easily inspect the errors in a structured way) and re-raise modified versions of the error (e.g. in spaCy).,"Codecov Report

Merging #397 into master will increase coverage by 0.15%.
The diff coverage is 97.95%.


@@            Coverage Diff             @@
##           master     #397      +/-   ##
==========================================
+ Coverage   59.41%   59.56%   +0.15%     
==========================================
  Files          96       96              
  Lines        6574     6601      +27     
==========================================
+ Hits         3906     3932      +26     
- Misses       2668     2669       +1     



Impacted Files
Coverage 





thinc/initializers.py
75.80% <> ()



thinc/config.py
96.19% <97.91%> (+<0.01%)



thinc/about.py
100.00% <100.00%> ()





Continue to review full report at Codecov.

Legend - Click here to learn more
 = absolute <relative> (impact),  = not affected, ? = missing data
Powered by Codecov. Last update 8c378d8...5ec10ed. Read the comment docs.",True,{}
explosion/thinc,https://github.com/explosion/thinc,398,2020-09-26T14:16:17Z,2020-09-26T14:48:52Z,2020-09-26T14:48:54Z,MERGED,True,15,3,2,https://github.com/ines,Improve config errors,1,"['enhancement', 'feat / config']",https://github.com/explosion/thinc/pull/398,https://github.com/ines,1,https://github.com/explosion/thinc/pull/398,Minor improvements to error handling in config.,Minor improvements to error handling in config.,True,{}
explosion/thinc,https://github.com/explosion/thinc,398,2020-09-26T14:16:17Z,2020-09-26T14:48:52Z,2020-09-26T14:48:54Z,MERGED,True,15,3,2,https://github.com/ines,Improve config errors,1,"['enhancement', 'feat / config']",https://github.com/explosion/thinc/pull/398,https://github.com/apps/codecov,2,https://github.com/explosion/thinc/pull/398#issuecomment-699502352,Minor improvements to error handling in config.,"Codecov Report

Merging #398 into master will increase coverage by 0.03%.
The diff coverage is 100.00%.


@@            Coverage Diff             @@
##           master     #398      +/-   ##
==========================================
+ Coverage   59.56%   59.60%   +0.03%     
==========================================
  Files          96       96              
  Lines        6601     6607       +6     
==========================================
+ Hits         3932     3938       +6     
  Misses       2669     2669              



Impacted Files
Coverage 





thinc/config.py
96.23% <100.00%> (+0.04%)





Continue to review full report at Codecov.

Legend - Click here to learn more
 = absolute <relative> (impact),  = not affected, ? = missing data
Powered by Codecov. Last update 824ddca...5f65e03. Read the comment docs.",True,{}
explosion/thinc,https://github.com/explosion/thinc,399,2020-09-27T16:09:14Z,2020-09-27T17:28:59Z,2020-09-27T17:29:01Z,MERGED,True,271,262,20,https://github.com/ines,Refactor filling and resolving config,4,"['enhancement', 'feat / config']",https://github.com/explosion/thinc/pull/399,https://github.com/ines,1,https://github.com/explosion/thinc/pull/399,"Allow configs to be filled without resolving. In this case, registered functions (promises) aren't called or even loaded and their return values aren't validated against any schema.
Only use two descriptive methods: registry.fill returns a filled, unresolved config and registry.resolve returns the resolved config","Allow configs to be filled without resolving. In this case, registered functions (promises) aren't called or even loaded and their return values aren't validated against any schema.
Only use two descriptive methods: registry.fill returns a filled, unresolved config and registry.resolve returns the resolved config",True,{}
explosion/thinc,https://github.com/explosion/thinc,399,2020-09-27T16:09:14Z,2020-09-27T17:28:59Z,2020-09-27T17:29:01Z,MERGED,True,271,262,20,https://github.com/ines,Refactor filling and resolving config,4,"['enhancement', 'feat / config']",https://github.com/explosion/thinc/pull/399,https://github.com/apps/codecov,2,https://github.com/explosion/thinc/pull/399#issuecomment-699658340,"Allow configs to be filled without resolving. In this case, registered functions (promises) aren't called or even loaded and their return values aren't validated against any schema.
Only use two descriptive methods: registry.fill returns a filled, unresolved config and registry.resolve returns the resolved config","Codecov Report

Merging #399 into master will increase coverage by 0.10%.
The diff coverage is 91.89%.


@@            Coverage Diff             @@
##           master     #399      +/-   ##
==========================================
+ Coverage   59.60%   59.71%   +0.10%     
==========================================
  Files          96       96              
  Lines        6607     6622      +15     
==========================================
+ Hits         3938     3954      +16     
+ Misses       2669     2668       -1     



Impacted Files
Coverage 





thinc/tests/layers/test_layers_api.py
0.00% <0.00%> ()



thinc/tests/layers/test_transforms.py
0.00% <0.00%> ()



thinc/about.py
100.00% <100.00%> ()



thinc/config.py
96.16% <100.00%> (-0.07%)





Continue to review full report at Codecov.

Legend - Click here to learn more
 = absolute <relative> (impact),  = not affected, ? = missing data
Powered by Codecov. Last update 8052c80...26fd018. Read the comment docs.",True,{}
explosion/thinc,https://github.com/explosion/thinc,400,2020-09-27T18:27:36Z,2020-09-27T20:49:03Z,2020-09-27T20:49:05Z,MERGED,True,25,4,3,https://github.com/ines,Handle promises with unavailable references if resolve=False,3,"['bug', 'feat / config']",https://github.com/explosion/thinc/pull/400,https://github.com/ines,1,https://github.com/explosion/thinc/pull/400,"If we're just filling a config, promises that refer to registered functions that are not available should be ignored.","If we're just filling a config, promises that refer to registered functions that are not available should be ignored.",True,{'THUMBS_UP': ['https://github.com/svlandeg']}
explosion/thinc,https://github.com/explosion/thinc,400,2020-09-27T18:27:36Z,2020-09-27T20:49:03Z,2020-09-27T20:49:05Z,MERGED,True,25,4,3,https://github.com/ines,Handle promises with unavailable references if resolve=False,3,"['bug', 'feat / config']",https://github.com/explosion/thinc/pull/400,https://github.com/apps/codecov,2,https://github.com/explosion/thinc/pull/400#issuecomment-699683719,"If we're just filling a config, promises that refer to registered functions that are not available should be ignored.","Codecov Report

Merging #400 into master will increase coverage by 0.03%.
The diff coverage is 90.90%.


@@            Coverage Diff             @@
##           master     #400      +/-   ##
==========================================
+ Coverage   59.71%   59.74%   +0.03%     
==========================================
  Files          96       96              
  Lines        6622     6630       +8     
==========================================
+ Hits         3954     3961       +7     
- Misses       2668     2669       +1     



Impacted Files
Coverage 





thinc/config.py
96.04% <90.00%> (-0.13%)



thinc/about.py
100.00% <100.00%> ()





Continue to review full report at Codecov.

Legend - Click here to learn more
 = absolute <relative> (impact),  = not affected, ? = missing data
Powered by Codecov. Last update ca1f304...27e56aa. Read the comment docs.",True,{}
explosion/thinc,https://github.com/explosion/thinc,402,2020-09-28T12:38:39Z,2020-09-28T16:11:40Z,2021-01-21T14:13:24Z,MERGED,True,128,3,3,https://github.com/honnibal,Add Model methods to check deserialization compatibility,6,"['enhancement', 'serialization', 'feat / ux']",https://github.com/explosion/thinc/pull/402,https://github.com/honnibal,1,https://github.com/explosion/thinc/pull/402,"This should allow us to look before we leap to avoid confusing errors. I've called the methods model.can_from_disk(path), model.can_from_bytes(bytes_data) and model.can_from_dict(msg). By default, if the model is initialized and the data would cause us to change an attribute value that's already been set, we return False, even though that would be legal. This can be changed by setting strict=False.","This should allow us to look before we leap to avoid confusing errors. I've called the methods model.can_from_disk(path), model.can_from_bytes(bytes_data) and model.can_from_dict(msg). By default, if the model is initialized and the data would cause us to change an attribute value that's already been set, we return False, even though that would be legal. This can be changed by setting strict=False.",True,{}
explosion/thinc,https://github.com/explosion/thinc,402,2020-09-28T12:38:39Z,2020-09-28T16:11:40Z,2021-01-21T14:13:24Z,MERGED,True,128,3,3,https://github.com/honnibal,Add Model methods to check deserialization compatibility,6,"['enhancement', 'serialization', 'feat / ux']",https://github.com/explosion/thinc/pull/402,https://github.com/apps/codecov,2,https://github.com/explosion/thinc/pull/402#issuecomment-699982069,"This should allow us to look before we leap to avoid confusing errors. I've called the methods model.can_from_disk(path), model.can_from_bytes(bytes_data) and model.can_from_dict(msg). By default, if the model is initialized and the data would cause us to change an attribute value that's already been set, we return False, even though that would be legal. This can be changed by setting strict=False.","Codecov Report

Merging #402 into master will decrease coverage by 0.07%.
The diff coverage is 48.97%.


@@            Coverage Diff             @@
##           master     #402      +/-   ##
==========================================
- Coverage   59.74%   59.66%   -0.08%     
==========================================
  Files          96       96              
  Lines        6630     6679      +49     
==========================================
+ Hits         3961     3985      +24     
- Misses       2669     2694      +25     



Impacted Files
Coverage 





thinc/model.py
94.07% <48.97%> (-5.67%)





Continue to review full report at Codecov.

Legend - Click here to learn more
 = absolute <relative> (impact),  = not affected, ? = missing data
Powered by Codecov. Last update 32512f4...57ecce7. Read the comment docs.",True,{}
explosion/thinc,https://github.com/explosion/thinc,403,2020-09-28T14:49:45Z,2020-09-28T18:47:38Z,2021-01-21T14:12:47Z,MERGED,True,2,2,2,https://github.com/svlandeg,upgrade pydantic pin to use field.default_factory,1,['install'],https://github.com/explosion/thinc/pull/403,https://github.com/svlandeg,1,https://github.com/explosion/thinc/pull/403,"Need to have pydantic>=1.5 to use field.default_factory
cf https://pydantic-docs.helpmanual.io/changelog/#v15-2020-04-18","Need to have pydantic>=1.5 to use field.default_factory
cf https://pydantic-docs.helpmanual.io/changelog/#v15-2020-04-18",True,{}
explosion/thinc,https://github.com/explosion/thinc,403,2020-09-28T14:49:45Z,2020-09-28T18:47:38Z,2021-01-21T14:12:47Z,MERGED,True,2,2,2,https://github.com/svlandeg,upgrade pydantic pin to use field.default_factory,1,['install'],https://github.com/explosion/thinc/pull/403,https://github.com/apps/codecov,2,https://github.com/explosion/thinc/pull/403#issuecomment-700060368,"Need to have pydantic>=1.5 to use field.default_factory
cf https://pydantic-docs.helpmanual.io/changelog/#v15-2020-04-18","Codecov Report

Merging #403 into master will not change coverage.
The diff coverage is n/a.


@@           Coverage Diff           @@
##           master     #403   +/-   ##
=======================================
  Coverage   59.74%   59.74%           
=======================================
  Files          96       96           
  Lines        6630     6630           
=======================================
  Hits         3961     3961           
  Misses       2669     2669           

Continue to review full report at Codecov.

Legend - Click here to learn more
 = absolute <relative> (impact),  = not affected, ? = missing data
Powered by Codecov. Last update 32512f4...0019c27. Read the comment docs.",True,{}
explosion/thinc,https://github.com/explosion/thinc,404,2020-10-01T08:11:28Z,2020-10-01T11:29:01Z,2020-10-01T13:28:31Z,MERGED,True,2,98,8,https://github.com/svlandeg,Remove FeatureExtractor,3,['feat / layers'],https://github.com/explosion/thinc/pull/404,https://github.com/svlandeg,1,https://github.com/explosion/thinc/pull/404,"As this layer is spaCy-specific, it makes more sense to have it in spaCy instead: explosion/spaCy#6170
Version bump to 8.0.0a43","As this layer is spaCy-specific, it makes more sense to have it in spaCy instead: explosion/spaCy#6170
Version bump to 8.0.0a43",True,{}
explosion/thinc,https://github.com/explosion/thinc,404,2020-10-01T08:11:28Z,2020-10-01T11:29:01Z,2020-10-01T13:28:31Z,MERGED,True,2,98,8,https://github.com/svlandeg,Remove FeatureExtractor,3,['feat / layers'],https://github.com/explosion/thinc/pull/404,https://github.com/apps/codecov,2,https://github.com/explosion/thinc/pull/404#issuecomment-701972375,"As this layer is spaCy-specific, it makes more sense to have it in spaCy instead: explosion/spaCy#6170
Version bump to 8.0.0a43","Codecov Report

Merging #404 into master will increase coverage by 0.02%.
The diff coverage is 100.00%.


@@            Coverage Diff             @@
##           master     #404      +/-   ##
==========================================
+ Coverage   59.67%   59.70%   +0.02%     
==========================================
  Files          96       94       -2     
  Lines        6681     6641      -40     
==========================================
- Hits         3987     3965      -22     
+ Misses       2694     2676      -18     



Impacted Files
Coverage 





thinc/layers/__init__.py
100.00% <> ()



thinc/tests/layers/test_layers_api.py
0.00% <> ()



thinc/about.py
100.00% <100.00%> ()



thinc/api.py
100.00% <100.00%> ()





Continue to review full report at Codecov.

Legend - Click here to learn more
 = absolute <relative> (impact),  = not affected, ? = missing data
Powered by Codecov. Last update 301bf3a...f2626f0. Read the comment docs.",True,{}
explosion/thinc,https://github.com/explosion/thinc,405,2020-10-05T10:22:50Z,2020-11-03T14:05:04Z,2020-11-03T14:22:32Z,MERGED,True,4,0,1,https://github.com/svlandeg,check dims before calling blis.gemm,3,['feat / ux'],https://github.com/explosion/thinc/pull/405,https://github.com/svlandeg,1,https://github.com/explosion/thinc/pull/405,Avoid a more cryptic Cython error downstream,Avoid a more cryptic Cython error downstream,True,{}
explosion/thinc,https://github.com/explosion/thinc,405,2020-10-05T10:22:50Z,2020-11-03T14:05:04Z,2020-11-03T14:22:32Z,MERGED,True,4,0,1,https://github.com/svlandeg,check dims before calling blis.gemm,3,['feat / ux'],https://github.com/explosion/thinc/pull/405,https://github.com/apps/codecov,2,https://github.com/explosion/thinc/pull/405#issuecomment-703544753,Avoid a more cryptic Cython error downstream,"Codecov Report

Merging #405 into master will not change coverage.
The diff coverage is n/a.


@@           Coverage Diff           @@
##           master     #405   +/-   ##
=======================================
  Coverage   59.70%   59.70%           
=======================================
  Files          94       94           
  Lines        6639     6639           
=======================================
  Hits         3964     3964           
  Misses       2675     2675           

Continue to review full report at Codecov.

Legend - Click here to learn more
 = absolute <relative> (impact),  = not affected, ? = missing data
Powered by Codecov. Last update 28c59aa...07c9cb9. Read the comment docs.",True,{}
explosion/thinc,https://github.com/explosion/thinc,406,2020-10-10T15:33:30Z,2020-10-10T16:27:52Z,2020-10-10T16:27:54Z,MERGED,True,17,2,3,https://github.com/ines,Handle error for nested invalid sections,2,['bug'],https://github.com/explosion/thinc/pull/406,https://github.com/ines,1,https://github.com/explosion/thinc/pull/406,,,True,{}
explosion/thinc,https://github.com/explosion/thinc,406,2020-10-10T15:33:30Z,2020-10-10T16:27:52Z,2020-10-10T16:27:54Z,MERGED,True,17,2,3,https://github.com/ines,Handle error for nested invalid sections,2,['bug'],https://github.com/explosion/thinc/pull/406,https://github.com/apps/codecov,2,https://github.com/explosion/thinc/pull/406#issuecomment-706567776,,"Codecov Report

Merging #406 into master will increase coverage by 0.02%.
The diff coverage is 100.00%.


@@            Coverage Diff             @@
##           master     #406      +/-   ##
==========================================
+ Coverage   59.70%   59.73%   +0.02%     
==========================================
  Files          94       94              
  Lines        6639     6643       +4     
==========================================
+ Hits         3964     3968       +4     
  Misses       2675     2675              



Impacted Files
Coverage 





thinc/about.py
100.00% <100.00%> ()



thinc/config.py
96.25% <100.00%> (+0.02%)





Continue to review full report at Codecov.

Legend - Click here to learn more
 = absolute <relative> (impact),  = not affected, ? = missing data
Powered by Codecov. Last update 28c59aa...9eb1eeb. Read the comment docs.",True,{}
explosion/thinc,https://github.com/explosion/thinc,407,2020-10-13T16:57:21Z,2020-11-03T14:05:25Z,2020-11-03T14:22:25Z,MERGED,True,85,110,6,https://github.com/svlandeg,Remove StaticVectors + dropout fix,12,"['bug', 'enhancement', 'tests', 'feat / layers']",https://github.com/explosion/thinc/pull/407,https://github.com/svlandeg,1,https://github.com/explosion/thinc/pull/407,"Bugfix for dropout in Embed - should not be applied at runtime
Added additional check that model.predict doesn't use dropout via the NoDropoutOps
Removed StaticVectors (moved to spaCy)
Added unit tests to check batch predictions vs. single predictions for the most common Model types","Bugfix for dropout in Embed - should not be applied at runtime
Added additional check that model.predict doesn't use dropout via the NoDropoutOps
Removed StaticVectors (moved to spaCy)
Added unit tests to check batch predictions vs. single predictions for the most common Model types",True,{}
explosion/thinc,https://github.com/explosion/thinc,407,2020-10-13T16:57:21Z,2020-11-03T14:05:25Z,2020-11-03T14:22:25Z,MERGED,True,85,110,6,https://github.com/svlandeg,Remove StaticVectors + dropout fix,12,"['bug', 'enhancement', 'tests', 'feat / layers']",https://github.com/explosion/thinc/pull/407,https://github.com/apps/codecov,2,https://github.com/explosion/thinc/pull/407#issuecomment-707886166,"Bugfix for dropout in Embed - should not be applied at runtime
Added additional check that model.predict doesn't use dropout via the NoDropoutOps
Removed StaticVectors (moved to spaCy)
Added unit tests to check batch predictions vs. single predictions for the most common Model types","Codecov Report

Merging #407 into master will decrease coverage by 0.34%.
The diff coverage is 22.22%.


@@            Coverage Diff             @@
##           master     #407      +/-   ##
==========================================
- Coverage   59.73%   59.39%   -0.35%     
==========================================
  Files          94       94              
  Lines        6643     6693      +50     
==========================================
+ Hits         3968     3975       +7     
- Misses       2675     2718      +43     



Impacted Files
Coverage 





thinc/layers/__init__.py
100.00% <> ()



thinc/tests/layers/test_layers_api.py
0.00% <0.00%> ()



thinc/api.py
100.00% <100.00%> ()



thinc/backends/__init__.py
97.14% <100.00%> (+0.08%)



thinc/layers/embed.py
100.00% <100.00%> (+2.38%)



thinc/layers/hashembed.py
98.14% <100.00%> (+1.99%)



thinc/config.py
96.07% <0.00%> (-0.18%)





Continue to review full report at Codecov.

Legend - Click here to learn more
 = absolute <relative> (impact),  = not affected, ? = missing data
Powered by Codecov. Last update a6578f3...7c2092a. Read the comment docs.",True,{}
explosion/thinc,https://github.com/explosion/thinc,409,2020-10-16T14:57:17Z,2020-11-03T14:03:25Z,2020-11-03T14:21:55Z,MERGED,True,27,3,5,https://github.com/svlandeg,require_cpu,7,"['enhancement', 'feat / ops']",https://github.com/explosion/thinc/pull/409,https://github.com/svlandeg,1,https://github.com/explosion/thinc/pull/409,"Add convenience method require_cpu that makes sure to use NumpyOps.
Implemented after reading the discussion here: explosion/spaCy#5018","Add convenience method require_cpu that makes sure to use NumpyOps.
Implemented after reading the discussion here: explosion/spaCy#5018",True,{}
explosion/thinc,https://github.com/explosion/thinc,409,2020-10-16T14:57:17Z,2020-11-03T14:03:25Z,2020-11-03T14:21:55Z,MERGED,True,27,3,5,https://github.com/svlandeg,require_cpu,7,"['enhancement', 'feat / ops']",https://github.com/explosion/thinc/pull/409,https://github.com/apps/codecov,2,https://github.com/explosion/thinc/pull/409#issuecomment-710101349,"Add convenience method require_cpu that makes sure to use NumpyOps.
Implemented after reading the discussion here: explosion/spaCy#5018","Codecov Report

Merging #409 into master will increase coverage by 0.00%.
The diff coverage is 100.00%.


@@           Coverage Diff           @@
##           master     #409   +/-   ##
=======================================
  Coverage   59.73%   59.73%           
=======================================
  Files          94       94           
  Lines        6643     6644    +1     
=======================================
+ Hits         3968     3969    +1     
  Misses       2675     2675           



Impacted Files
Coverage 





thinc/config.py
96.25% <> ()



thinc/util.py
92.64% <> ()



thinc/about.py
100.00% <100.00%> ()



thinc/api.py
100.00% <100.00%> ()



thinc/backends/__init__.py
97.14% <100.00%> (+0.08%)





Continue to review full report at Codecov.

Legend - Click here to learn more
 = absolute <relative> (impact),  = not affected, ? = missing data
Powered by Codecov. Last update 36e20ce...aad5211. Read the comment docs.",True,{}
explosion/thinc,https://github.com/explosion/thinc,412,2020-10-20T18:51:15Z,2020-11-03T14:04:35Z,2020-11-03T14:22:03Z,MERGED,True,6,0,1,https://github.com/svlandeg,Fix random seed for pytorch,1,"['enhancement', 'interop / pytorch']",https://github.com/explosion/thinc/pull/412,https://github.com/svlandeg,1,https://github.com/explosion/thinc/pull/412,Ensure reproducibility when working with PyTorch models like in spacy_transformers.,Ensure reproducibility when working with PyTorch models like in spacy_transformers.,True,{}
explosion/thinc,https://github.com/explosion/thinc,412,2020-10-20T18:51:15Z,2020-11-03T14:04:35Z,2020-11-03T14:22:03Z,MERGED,True,6,0,1,https://github.com/svlandeg,Fix random seed for pytorch,1,"['enhancement', 'interop / pytorch']",https://github.com/explosion/thinc/pull/412,https://github.com/apps/codecov,2,https://github.com/explosion/thinc/pull/412#issuecomment-713071775,Ensure reproducibility when working with PyTorch models like in spacy_transformers.,"Codecov Report

Merging #412 into master will increase coverage by 0.00%.
The diff coverage is 100.00%.


@@           Coverage Diff           @@
##           master     #412   +/-   ##
=======================================
  Coverage   59.73%   59.73%           
=======================================
  Files          94       94           
  Lines        6643     6644    +1     
=======================================
+ Hits         3968     3969    +1     
  Misses       2675     2675           



Impacted Files
Coverage 





thinc/util.py
92.64% <> ()



thinc/about.py
100.00% <100.00%> ()



thinc/backends/__init__.py
97.14% <100.00%> (+0.08%)





Continue to review full report at Codecov.

Legend - Click here to learn more
 = absolute <relative> (impact),  = not affected, ? = missing data
Powered by Codecov. Last update 36e20ce...cbff512. Read the comment docs.",True,{}
explosion/thinc,https://github.com/explosion/thinc,413,2020-10-26T21:42:05Z,2020-10-26T21:42:49Z,2020-10-26T21:42:52Z,MERGED,True,3,3,3,https://github.com/svlandeg,pydantic upper pin 1.7.0 for now,1,['bug'],https://github.com/explosion/thinc/pull/413,https://github.com/svlandeg,1,https://github.com/explosion/thinc/pull/413,bump to rc1,bump to rc1,True,{}
explosion/thinc,https://github.com/explosion/thinc,414,2020-10-27T07:54:06Z,2020-11-03T13:25:04Z,2021-01-21T14:12:32Z,MERGED,True,10,10,5,https://github.com/adrianeboyd,Specify ml_datasets>=0.2.0a0 in examples,1,['install'],https://github.com/explosion/thinc/pull/414,https://github.com/adrianeboyd,1,https://github.com/explosion/thinc/pull/414,,,True,{}
explosion/thinc,https://github.com/explosion/thinc,414,2020-10-27T07:54:06Z,2020-11-03T13:25:04Z,2021-01-21T14:12:32Z,MERGED,True,10,10,5,https://github.com/adrianeboyd,Specify ml_datasets>=0.2.0a0 in examples,1,['install'],https://github.com/explosion/thinc/pull/414,https://github.com/apps/codecov,2,https://github.com/explosion/thinc/pull/414#issuecomment-717061049,,"Codecov Report

Merging #414 into master will not change coverage.
The diff coverage is n/a.


@@           Coverage Diff           @@
##           master     #414   +/-   ##
=======================================
  Coverage   59.73%   59.73%           
=======================================
  Files          94       94           
  Lines        6644     6644           
=======================================
  Hits         3969     3969           
  Misses       2675     2675           

Continue to review full report at Codecov.

Legend - Click here to learn more
 = absolute <relative> (impact),  = not affected, ? = missing data
Powered by Codecov. Last update c1f5b51...225d598. Read the comment docs.",True,{}
explosion/thinc,https://github.com/explosion/thinc,415,2020-11-03T12:03:52Z,2020-11-03T14:06:02Z,2021-01-21T14:12:21Z,MERGED,True,12,2,3,https://github.com/adrianeboyd,Updates for python 3.9,7,['install'],https://github.com/explosion/thinc/pull/415,https://github.com/adrianeboyd,1,https://github.com/explosion/thinc/pull/415,,,True,{}
explosion/thinc,https://github.com/explosion/thinc,415,2020-11-03T12:03:52Z,2020-11-03T14:06:02Z,2021-01-21T14:12:21Z,MERGED,True,12,2,3,https://github.com/adrianeboyd,Updates for python 3.9,7,['install'],https://github.com/explosion/thinc/pull/415,https://github.com/apps/codecov,2,https://github.com/explosion/thinc/pull/415#issuecomment-721078398,,"Codecov Report

Merging #415 into master will not change coverage.
The diff coverage is 100.00%.


@@           Coverage Diff           @@
##           master     #415   +/-   ##
=======================================
  Coverage   59.73%   59.73%           
=======================================
  Files          94       94           
  Lines        6644     6644           
=======================================
  Hits         3969     3969           
  Misses       2675     2675           



Impacted Files
Coverage 





thinc/about.py
100.00% <100.00%> ()





Continue to review full report at Codecov.

Legend - Click here to learn more
 = absolute <relative> (impact),  = not affected, ? = missing data
Powered by Codecov. Last update 0ac28ad...1721d19. Read the comment docs.",True,{}
explosion/thinc,https://github.com/explosion/thinc,416,2020-11-03T14:18:25Z,2020-11-03T14:18:39Z,2021-01-21T14:12:13Z,MERGED,True,1,1,1,https://github.com/adrianeboyd,Update torch extras range,1,['install'],https://github.com/explosion/thinc/pull/416,https://github.com/adrianeboyd,1,https://github.com/explosion/thinc/pull/416,,,True,{}
explosion/thinc,https://github.com/explosion/thinc,417,2020-11-04T08:07:23Z,2020-11-04T09:39:44Z,2021-01-21T14:12:07Z,MERGED,True,27,16,5,https://github.com/adrianeboyd,Updates for python 3.9 for v7.x,8,['install'],https://github.com/explosion/thinc/pull/417,https://github.com/adrianeboyd,1,https://github.com/explosion/thinc/pull/417,,,True,{}
explosion/thinc,https://github.com/explosion/thinc,418,2020-11-09T14:26:32Z,2020-12-04T01:59:16Z,2020-12-04T08:55:40Z,MERGED,True,16,19,2,https://github.com/svlandeg,update Model documentation,1,['docs'],https://github.com/explosion/thinc/pull/418,https://github.com/svlandeg,1,https://github.com/explosion/thinc/pull/418,"The documentation of the Model constructor seems to have gone out of date: grads is not a parameter anymore, but refs is.
Also removed some dead code from Model.copy","The documentation of the Model constructor seems to have gone out of date: grads is not a parameter anymore, but refs is.
Also removed some dead code from Model.copy",True,{}
explosion/thinc,https://github.com/explosion/thinc,418,2020-11-09T14:26:32Z,2020-12-04T01:59:16Z,2020-12-04T08:55:40Z,MERGED,True,16,19,2,https://github.com/svlandeg,update Model documentation,1,['docs'],https://github.com/explosion/thinc/pull/418,https://github.com/apps/codecov,2,https://github.com/explosion/thinc/pull/418#issuecomment-724050595,"The documentation of the Model constructor seems to have gone out of date: grads is not a parameter anymore, but refs is.
Also removed some dead code from Model.copy","Codecov Report

Merging #418 (a7692b0) into master (6e9c13d) will decrease coverage by 0.01%.
The diff coverage is n/a.


@@            Coverage Diff             @@
##           master     #418      +/-   ##
==========================================
- Coverage   59.39%   59.37%   -0.02%     
==========================================
  Files          94       94              
  Lines        6693     6690       -3     
==========================================
- Hits         3975     3972       -3     
  Misses       2718     2718              



Impacted Files
Coverage 





thinc/model.py
94.03% <> (-0.05%)





Continue to review full report at Codecov.

Legend - Click here to learn more
 = absolute <relative> (impact),  = not affected, ? = missing data
Powered by Codecov. Last update 6e9c13d...a7692b0. Read the comment docs.",True,{}
explosion/thinc,https://github.com/explosion/thinc,419,2020-11-12T08:40:25Z,2020-11-12T09:37:40Z,2021-01-21T14:11:30Z,MERGED,True,4,4,3,https://github.com/adrianeboyd,Support hypothesis 5 in tests,1,"['tests', 'install']",https://github.com/explosion/thinc/pull/419,https://github.com/adrianeboyd,1,https://github.com/explosion/thinc/pull/419,,,True,{}
explosion/thinc,https://github.com/explosion/thinc,420,2020-11-12T09:42:36Z,2020-11-12T10:16:02Z,2021-01-21T14:11:14Z,MERGED,True,1,1,1,https://github.com/adrianeboyd,Extend hypothesis range to <6.0.0,1,['install'],https://github.com/explosion/thinc/pull/420,https://github.com/adrianeboyd,1,https://github.com/explosion/thinc/pull/420,,,True,{}
explosion/thinc,https://github.com/explosion/thinc,420,2020-11-12T09:42:36Z,2020-11-12T10:16:02Z,2021-01-21T14:11:14Z,MERGED,True,1,1,1,https://github.com/adrianeboyd,Extend hypothesis range to <6.0.0,1,['install'],https://github.com/explosion/thinc/pull/420,https://github.com/apps/codecov,2,https://github.com/explosion/thinc/pull/420#issuecomment-725968868,,"Codecov Report

Merging #420 (be40d6d) into master (6e9c13d) will not change coverage.
The diff coverage is n/a.


@@           Coverage Diff           @@
##           master     #420   +/-   ##
=======================================
  Coverage   59.39%   59.39%           
=======================================
  Files          94       94           
  Lines        6693     6693           
=======================================
  Hits         3975     3975           
  Misses       2718     2718           

Continue to review full report at Codecov.

Legend - Click here to learn more
 = absolute <relative> (impact),  = not affected, ? = missing data
Powered by Codecov. Last update 6e9c13d...be40d6d. Read the comment docs.",True,{}
explosion/thinc,https://github.com/explosion/thinc,421,2020-11-13T11:07:52Z,2020-11-14T05:07:01Z,2020-11-14T05:07:01Z,MERGED,True,1,1,1,https://github.com/adrianeboyd,Fix finalization method name for Beam,1,['bug'],https://github.com/explosion/thinc/pull/421,https://github.com/adrianeboyd,1,https://github.com/explosion/thinc/pull/421,Rename Beam.__del__ to Beam.__dealloc__.,Rename Beam.__del__ to Beam.__dealloc__.,True,{}
explosion/thinc,https://github.com/explosion/thinc,422,2020-11-13T12:28:27Z,2020-11-14T05:07:21Z,2020-11-14T05:07:21Z,MERGED,True,1,1,1,https://github.com/adrianeboyd,Fix finalization method name for Beam,1,['bug'],https://github.com/explosion/thinc/pull/422,https://github.com/adrianeboyd,1,https://github.com/explosion/thinc/pull/422,,,True,{}
explosion/thinc,https://github.com/explosion/thinc,422,2020-11-13T12:28:27Z,2020-11-14T05:07:21Z,2020-11-14T05:07:21Z,MERGED,True,1,1,1,https://github.com/adrianeboyd,Fix finalization method name for Beam,1,['bug'],https://github.com/explosion/thinc/pull/422,https://github.com/apps/codecov,2,https://github.com/explosion/thinc/pull/422#issuecomment-726740634,,"Codecov Report

Merging #422 (8d6e7dd) into master (6e9c13d) will not change coverage.
The diff coverage is n/a.


@@           Coverage Diff           @@
##           master     #422   +/-   ##
=======================================
  Coverage   59.39%   59.39%           
=======================================
  Files          94       94           
  Lines        6693     6693           
=======================================
  Hits         3975     3975           
  Misses       2718     2718           

Continue to review full report at Codecov.

Legend - Click here to learn more
 = absolute <relative> (impact),  = not affected, ? = missing data
Powered by Codecov. Last update 6e9c13d...8d6e7dd. Read the comment docs.",True,{}
explosion/thinc,https://github.com/explosion/thinc,423,2020-11-16T09:32:59Z,2020-11-19T12:52:59Z,2021-01-21T14:11:08Z,MERGED,True,0,1,1,https://github.com/adrianeboyd,Remove wheel from setup_requires,1,['install'],https://github.com/explosion/thinc/pull/423,https://github.com/adrianeboyd,1,https://github.com/explosion/thinc/pull/423,,,True,{}
explosion/thinc,https://github.com/explosion/thinc,423,2020-11-16T09:32:59Z,2020-11-19T12:52:59Z,2021-01-21T14:11:08Z,MERGED,True,0,1,1,https://github.com/adrianeboyd,Remove wheel from setup_requires,1,['install'],https://github.com/explosion/thinc/pull/423,https://github.com/apps/codecov,2,https://github.com/explosion/thinc/pull/423#issuecomment-727860420,,"Codecov Report

Merging #423 (ddd494f) into master (033d9fb) will not change coverage.
The diff coverage is n/a.


@@           Coverage Diff           @@
##           master     #423   +/-   ##
=======================================
  Coverage   59.39%   59.39%           
=======================================
  Files          94       94           
  Lines        6693     6693           
=======================================
  Hits         3975     3975           
  Misses       2718     2718           

Continue to review full report at Codecov.

Legend - Click here to learn more
 = absolute <relative> (impact),  = not affected, ? = missing data
Powered by Codecov. Last update 033d9fb...ddd494f. Read the comment docs.",True,{}
explosion/thinc,https://github.com/explosion/thinc,424,2020-11-16T09:45:37Z,2020-12-04T01:58:51Z,2020-12-04T01:58:51Z,MERGED,True,17,1,2,https://github.com/adrianeboyd,Add pyproject.toml and improve CI sdist install test,5,['install'],https://github.com/explosion/thinc/pull/424,https://github.com/adrianeboyd,1,https://github.com/explosion/thinc/pull/424,,,True,{}
explosion/thinc,https://github.com/explosion/thinc,424,2020-11-16T09:45:37Z,2020-12-04T01:58:51Z,2020-12-04T01:58:51Z,MERGED,True,17,1,2,https://github.com/adrianeboyd,Add pyproject.toml and improve CI sdist install test,5,['install'],https://github.com/explosion/thinc/pull/424,https://github.com/apps/codecov,2,https://github.com/explosion/thinc/pull/424#issuecomment-728952458,,"Codecov Report

Merging #424 (d72c12f) into master (033d9fb) will not change coverage.
The diff coverage is n/a.


@@           Coverage Diff           @@
##           master     #424   +/-   ##
=======================================
  Coverage   59.39%   59.39%           
=======================================
  Files          94       94           
  Lines        6693     6693           
=======================================
  Hits         3975     3975           
  Misses       2718     2718           

Continue to review full report at Codecov.

Legend - Click here to learn more
 = absolute <relative> (impact),  = not affected, ? = missing data
Powered by Codecov. Last update 033d9fb...d72c12f. Read the comment docs.",True,{}
explosion/thinc,https://github.com/explosion/thinc,425,2020-11-18T08:59:47Z,2020-11-18T10:52:36Z,2020-11-18T10:52:36Z,CLOSED,False,55,10238,29,https://github.com/adrianeboyd,TEST/WIP: First hacky test for numpy header updates,8,[],https://github.com/explosion/thinc/pull/425,https://github.com/adrianeboyd,1,https://github.com/explosion/thinc/pull/425,,,True,{}
explosion/thinc,https://github.com/explosion/thinc,426,2020-11-18T10:53:22Z,2020-11-20T16:26:47Z,2020-11-20T16:26:47Z,CLOSED,False,33,10213,31,https://github.com/adrianeboyd,TEST/WIP: First hacky test for numpy header updates,11,[],https://github.com/explosion/thinc/pull/426,https://github.com/adrianeboyd,1,https://github.com/explosion/thinc/pull/426,,,True,{}
explosion/thinc,https://github.com/explosion/thinc,427,2020-11-20T10:48:42Z,2020-12-04T01:58:13Z,2020-12-04T01:58:13Z,MERGED,True,1,1,1,https://github.com/adrianeboyd,Fix relative import in tests,1,"['bug', 'tests']",https://github.com/explosion/thinc/pull/427,https://github.com/adrianeboyd,1,https://github.com/explosion/thinc/pull/427,,,True,{}
explosion/thinc,https://github.com/explosion/thinc,427,2020-11-20T10:48:42Z,2020-12-04T01:58:13Z,2020-12-04T01:58:13Z,MERGED,True,1,1,1,https://github.com/adrianeboyd,Fix relative import in tests,1,"['bug', 'tests']",https://github.com/explosion/thinc/pull/427,https://github.com/apps/codecov,2,https://github.com/explosion/thinc/pull/427#issuecomment-731100448,,"Codecov Report

Merging #427 (2dee9de) into master (033d9fb) will not change coverage.
The diff coverage is 0.00%.


@@           Coverage Diff           @@
##           master     #427   +/-   ##
=======================================
  Coverage   59.39%   59.39%           
=======================================
  Files          94       94           
  Lines        6693     6693           
=======================================
  Hits         3975     3975           
  Misses       2718     2718           



Impacted Files
Coverage 





thinc/tests/layers/test_uniqued.py
0.00% <0.00%> ()





Continue to review full report at Codecov.

Legend - Click here to learn more
 = absolute <relative> (impact),  = not affected, ? = missing data
Powered by Codecov. Last update 033d9fb...2dee9de. Read the comment docs.",True,{}
explosion/thinc,https://github.com/explosion/thinc,428,2020-11-20T11:16:54Z,2020-11-20T16:26:55Z,2020-11-20T16:26:55Z,CLOSED,False,37,6,6,https://github.com/adrianeboyd,TEST/WIP: Test numpy headers for v8,1,[],https://github.com/explosion/thinc/pull/428,https://github.com/adrianeboyd,1,https://github.com/explosion/thinc/pull/428,,,True,{}
explosion/thinc,https://github.com/explosion/thinc,428,2020-11-20T11:16:54Z,2020-11-20T16:26:55Z,2020-11-20T16:26:55Z,CLOSED,False,37,6,6,https://github.com/adrianeboyd,TEST/WIP: Test numpy headers for v8,1,[],https://github.com/explosion/thinc/pull/428,https://github.com/apps/codecov,2,https://github.com/explosion/thinc/pull/428#issuecomment-731112499,,"Codecov Report

Merging #428 (7f4492a) into master (033d9fb) will not change coverage.
The diff coverage is n/a.


@@           Coverage Diff           @@
##           master     #428   +/-   ##
=======================================
  Coverage   59.39%   59.39%           
=======================================
  Files          94       94           
  Lines        6693     6693           
=======================================
  Hits         3975     3975           
  Misses       2718     2718           

Continue to review full report at Codecov.

Legend - Click here to learn more
 = absolute <relative> (impact),  = not affected, ? = missing data
Powered by Codecov. Last update 033d9fb...7f4492a. Read the comment docs.",True,{}
explosion/thinc,https://github.com/explosion/thinc,429,2020-11-20T14:57:35Z,2020-11-23T09:20:38Z,2021-01-21T14:10:52Z,MERGED,True,41,10217,33,https://github.com/adrianeboyd,Dynamically include numpy headers,7,['feat / ops'],https://github.com/explosion/thinc/pull/429,https://github.com/adrianeboyd,1,https://github.com/explosion/thinc/pull/429,"Add pyproject.toml and build-constraints.txt with numpy version pins for building wheels with pip and wheelwright
Update setup.py to add current numpy include directory
Assume cython and numpy are installed for setup.py
Remove included numpy headers","Add pyproject.toml and build-constraints.txt with numpy version pins for building wheels with pip and wheelwright
Update setup.py to add current numpy include directory
Assume cython and numpy are installed for setup.py
Remove included numpy headers",True,{}
explosion/thinc,https://github.com/explosion/thinc,430,2020-11-20T15:27:37Z,2020-12-15T14:42:17Z,2020-12-15T14:42:17Z,MERGED,True,19,9,6,https://github.com/adrianeboyd,Update numpy headers and setup,9,['install'],https://github.com/explosion/thinc/pull/430,https://github.com/adrianeboyd,1,https://github.com/explosion/thinc/pull/430,"Add build-constraints.txt with numpy version pins for building wheels with pip and wheelwright
Update setup and version pins","Add build-constraints.txt with numpy version pins for building wheels with pip and wheelwright
Update setup and version pins",True,{}
explosion/thinc,https://github.com/explosion/thinc,430,2020-11-20T15:27:37Z,2020-12-15T14:42:17Z,2020-12-15T14:42:17Z,MERGED,True,19,9,6,https://github.com/adrianeboyd,Update numpy headers and setup,9,['install'],https://github.com/explosion/thinc/pull/430,https://github.com/apps/codecov,2,https://github.com/explosion/thinc/pull/430#issuecomment-731240609,"Add build-constraints.txt with numpy version pins for building wheels with pip and wheelwright
Update setup and version pins","Codecov Report

Merging #430 (7b733f1) into master (76d0a78) will not change coverage.
The diff coverage is n/a.


@@           Coverage Diff           @@
##           master     #430   +/-   ##
=======================================
  Coverage   59.45%   59.45%           
=======================================
  Files          94       94           
  Lines        6687     6687           
=======================================
  Hits         3976     3976           
  Misses       2711     2711           

Continue to review full report at Codecov.

Legend - Click here to learn more
 = absolute <relative> (impact),  = not affected, ? = missing data
Powered by Codecov. Last update 76d0a78...7b733f1. Read the comment docs.",True,{}
explosion/thinc,https://github.com/explosion/thinc,431,2020-11-23T17:42:14Z,2020-12-04T01:57:20Z,2020-12-04T08:53:53Z,MERGED,True,5,2,2,https://github.com/svlandeg,avoid initializing with Y if not necessary,1,"['bug', 'feat / layers']",https://github.com/explosion/thinc/pull/431,https://github.com/svlandeg,1,https://github.com/explosion/thinc/pull/431,"Only initialize the internal layer of residual with Y if the output dim nO is yet to be determined.
This is necessary to avoid wrong shape inferences stemming from the chain layer. This is all a bit brittle, and perhaps we want to rethink the whole shape inference mechanism at some point, but for now I think this works.","Only initialize the internal layer of residual with Y if the output dim nO is yet to be determined.
This is necessary to avoid wrong shape inferences stemming from the chain layer. This is all a bit brittle, and perhaps we want to rethink the whole shape inference mechanism at some point, but for now I think this works.",True,{}
explosion/thinc,https://github.com/explosion/thinc,431,2020-11-23T17:42:14Z,2020-12-04T01:57:20Z,2020-12-04T08:53:53Z,MERGED,True,5,2,2,https://github.com/svlandeg,avoid initializing with Y if not necessary,1,"['bug', 'feat / layers']",https://github.com/explosion/thinc/pull/431,https://github.com/apps/codecov,2,https://github.com/explosion/thinc/pull/431#issuecomment-732321823,"Only initialize the internal layer of residual with Y if the output dim nO is yet to be determined.
This is necessary to avoid wrong shape inferences stemming from the chain layer. This is all a bit brittle, and perhaps we want to rethink the whole shape inference mechanism at some point, but for now I think this works.","Codecov Report

Merging #431 (d4229a7) into master (adeb99a) will increase coverage by 0.01%.
The diff coverage is 100.00%.


@@            Coverage Diff             @@
##           master     #431      +/-   ##
==========================================
+ Coverage   59.39%   59.40%   +0.01%     
==========================================
  Files          94       94              
  Lines        6693     6695       +2     
==========================================
+ Hits         3975     3977       +2     
  Misses       2718     2718              



Impacted Files
Coverage 





thinc/layers/parametricattention.py
100.00% <100.00%> ()



thinc/layers/residual.py
89.47% <100.00%> (+0.58%)





Continue to review full report at Codecov.

Legend - Click here to learn more
 = absolute <relative> (impact),  = not affected, ? = missing data
Powered by Codecov. Last update adeb99a...d4229a7. Read the comment docs.",True,{}
explosion/thinc,https://github.com/explosion/thinc,431,2020-11-23T17:42:14Z,2020-12-04T01:57:20Z,2020-12-04T08:53:53Z,MERGED,True,5,2,2,https://github.com/svlandeg,avoid initializing with Y if not necessary,1,"['bug', 'feat / layers']",https://github.com/explosion/thinc/pull/431,https://github.com/honnibal,3,https://github.com/explosion/thinc/pull/431#issuecomment-733383360,"Only initialize the internal layer of residual with Y if the output dim nO is yet to be determined.
This is necessary to avoid wrong shape inferences stemming from the chain layer. This is all a bit brittle, and perhaps we want to rethink the whole shape inference mechanism at some point, but for now I think this works.",Can you point me to the code where this comes up? Is it in the rel model?,True,{}
explosion/thinc,https://github.com/explosion/thinc,431,2020-11-23T17:42:14Z,2020-12-04T01:57:20Z,2020-12-04T08:53:53Z,MERGED,True,5,2,2,https://github.com/svlandeg,avoid initializing with Y if not necessary,1,"['bug', 'feat / layers']",https://github.com/explosion/thinc/pull/431,https://github.com/svlandeg,4,https://github.com/explosion/thinc/pull/431#issuecomment-733583195,"Only initialize the internal layer of residual with Y if the output dim nO is yet to be determined.
This is necessary to avoid wrong shape inferences stemming from the chain layer. This is all a bit brittle, and perhaps we want to rethink the whole shape inference mechanism at some point, but for now I think this works.","The original issue that instigated this was explosion/spaCy#6401 (comment). Some more background on the analysis of what goes wrong in that report is here: explosion/spaCy#6431
This is about combining a textcat with a TransformerListener. The latter doesn't know its nO dimension straight away, so the chain layer gets into shape inference and tries to propagate the output Y through its layers: https://github.com/explosion/thinc/blob/master/thinc/layers/chain.py#L79. In this code, we specifically prevent propagating Y if nO is already set, because for the intermediate layers there's no guarantee that the given Y is actually compatible. But for the residual layer, it may be the case that its own nO is unset, but its encapsulated layer does have a proper nO. So in that case we need to prevent the Y being used to (re)initialize that encapsulated layer.",True,{}
explosion/thinc,https://github.com/explosion/thinc,431,2020-11-23T17:42:14Z,2020-12-04T01:57:20Z,2020-12-04T08:53:53Z,MERGED,True,5,2,2,https://github.com/svlandeg,avoid initializing with Y if not necessary,1,"['bug', 'feat / layers']",https://github.com/explosion/thinc/pull/431,https://github.com/honnibal,5,https://github.com/explosion/thinc/pull/431#issuecomment-738505122,"Only initialize the internal layer of residual with Y if the output dim nO is yet to be determined.
This is necessary to avoid wrong shape inferences stemming from the chain layer. This is all a bit brittle, and perhaps we want to rethink the whole shape inference mechanism at some point, but for now I think this works.","Ugh, this is ugly. Okay I'll merge this, but you're right that the policy is quite unclear.",True,{}
explosion/thinc,https://github.com/explosion/thinc,432,2020-11-24T16:35:13Z,2020-12-04T09:24:15Z,2020-12-04T09:24:18Z,MERGED,True,28,17,5,https://github.com/svlandeg,Propagate dims,3,"['bug', 'feat / layers']",https://github.com/explosion/thinc/pull/432,https://github.com/svlandeg,1,https://github.com/explosion/thinc/pull/432,"spaCy issue explosion/spaCy#6318 pointed towards some issues with inferring dimensions. To better support chaining layers and shape inference, this PR:

transfers dimensions from the encapsulated layer in the with_X type-transformation layers
sets an appropriate nO and nI dimension on the PyTorchLSTM","spaCy issue explosion/spaCy#6318 pointed towards some issues with inferring dimensions. To better support chaining layers and shape inference, this PR:

transfers dimensions from the encapsulated layer in the with_X type-transformation layers
sets an appropriate nO and nI dimension on the PyTorchLSTM",True,{}
explosion/thinc,https://github.com/explosion/thinc,432,2020-11-24T16:35:13Z,2020-12-04T09:24:15Z,2020-12-04T09:24:18Z,MERGED,True,28,17,5,https://github.com/svlandeg,Propagate dims,3,"['bug', 'feat / layers']",https://github.com/explosion/thinc/pull/432,https://github.com/apps/codecov,2,https://github.com/explosion/thinc/pull/432#issuecomment-733098997,"spaCy issue explosion/spaCy#6318 pointed towards some issues with inferring dimensions. To better support chaining layers and shape inference, this PR:

transfers dimensions from the encapsulated layer in the with_X type-transformation layers
sets an appropriate nO and nI dimension on the PyTorchLSTM","Codecov Report

Merging #432 (5c8cfcb) into master (adeb99a) will increase coverage by 0.06%.
The diff coverage is 87.50%.


@@            Coverage Diff             @@
##           master     #432      +/-   ##
==========================================
+ Coverage   59.39%   59.45%   +0.06%     
==========================================
  Files          94       94              
  Lines        6693     6687       -6     
==========================================
+ Hits         3975     3976       +1     
+ Misses       2718     2711       -7     



Impacted Files
Coverage 





thinc/layers/pytorchwrapper.py
100.00% <> ()



thinc/layers/with_cpu.py
32.35% <50.00%> (+5.52%)



thinc/layers/lstm.py
100.00% <100.00%> ()



thinc/layers/with_list.py
100.00% <100.00%> ()



thinc/layers/with_padded.py
80.26% <100.00%> (-0.26%)



thinc/model.py
94.03% <0.00%> (-0.05%)



thinc/layers/residual.py
89.47% <0.00%> (+0.58%)





Continue to review full report at Codecov.

Legend - Click here to learn more
 = absolute <relative> (impact),  = not affected, ? = missing data
Powered by Codecov. Last update adeb99a...5c8cfcb. Read the comment docs.",True,{}
explosion/thinc,https://github.com/explosion/thinc,433,2020-11-30T14:10:14Z,2020-12-04T01:48:18Z,2021-01-21T14:10:32Z,MERGED,True,1,4,1,https://github.com/adrianeboyd,Remove detailed numpy build constraints,1,['install'],https://github.com/explosion/thinc/pull/433,https://github.com/adrianeboyd,1,https://github.com/explosion/thinc/pull/433,"Remove detailed numpy build constraints from pyproject.toml because
it is too difficult to maintain for many architectures

These constraints are more a reflection of what is available on
pypi as binary wheels rather than any real build requirements that
it is necessary for users to follow when building from source
Users building their own binary packages will need to enforce the
constraints that make sense in their environments, e.g., the conda
compatible numpy pins



Keep the build constraints in build-constraints.txt for use with our
builds

Our builds with wheelwright are built against the earliest
compatible binary versions of numpy on pypi
These constraints are documented within the distribution","Remove detailed numpy build constraints from pyproject.toml because
it is too difficult to maintain for many architectures

These constraints are more a reflection of what is available on
pypi as binary wheels rather than any real build requirements that
it is necessary for users to follow when building from source
Users building their own binary packages will need to enforce the
constraints that make sense in their environments, e.g., the conda
compatible numpy pins



Keep the build constraints in build-constraints.txt for use with our
builds

Our builds with wheelwright are built against the earliest
compatible binary versions of numpy on pypi
These constraints are documented within the distribution",True,{}
explosion/thinc,https://github.com/explosion/thinc,434,2020-11-30T14:15:32Z,2020-12-07T13:10:27Z,2021-01-21T14:10:17Z,MERGED,True,18,232,6,https://github.com/adrianeboyd,Update setup,7,['install'],https://github.com/explosion/thinc/pull/434,https://github.com/adrianeboyd,1,https://github.com/explosion/thinc/pull/434,"Update cythonize in setup
Remove python version-related blis pins now that blis is easier to build from source in v0.7.4","Update cythonize in setup
Remove python version-related blis pins now that blis is easier to build from source in v0.7.4",True,{}
explosion/thinc,https://github.com/explosion/thinc,434,2020-11-30T14:15:32Z,2020-12-07T13:10:27Z,2021-01-21T14:10:17Z,MERGED,True,18,232,6,https://github.com/adrianeboyd,Update setup,7,['install'],https://github.com/explosion/thinc/pull/434,https://github.com/adrianeboyd,2,https://github.com/explosion/thinc/pull/434#issuecomment-735886658,"Update cythonize in setup
Remove python version-related blis pins now that blis is easier to build from source in v0.7.4",I think the updated version of of preshed (with explosion/preshed#28) should fix this problem.,True,{}
explosion/thinc,https://github.com/explosion/thinc,435,2020-12-02T08:52:16Z,2020-12-02T09:14:04Z,2020-12-02T09:14:04Z,MERGED,True,2,2,1,https://github.com/adrianeboyd,Remove f-strings in pytorch wrappers,1,[],https://github.com/explosion/thinc/pull/435,https://github.com/adrianeboyd,1,https://github.com/explosion/thinc/pull/435,,,True,{}
explosion/thinc,https://github.com/explosion/thinc,436,2020-12-02T09:01:25Z,2020-12-02T09:10:06Z,2021-01-21T14:09:41Z,CLOSED,False,3,2,2,https://github.com/adrianeboyd,Remove upper pin for torch,1,['install'],https://github.com/explosion/thinc/pull/436,https://github.com/adrianeboyd,1,https://github.com/explosion/thinc/pull/436,dataclasses 0.8 should fix the python version compatibility issues for torch 1.7.0.,dataclasses 0.8 should fix the python version compatibility issues for torch 1.7.0.,True,{}
explosion/thinc,https://github.com/explosion/thinc,436,2020-12-02T09:01:25Z,2020-12-02T09:10:06Z,2021-01-21T14:09:41Z,CLOSED,False,3,2,2,https://github.com/adrianeboyd,Remove upper pin for torch,1,['install'],https://github.com/explosion/thinc/pull/436,https://github.com/apps/codecov,2,https://github.com/explosion/thinc/pull/436#issuecomment-737095785,dataclasses 0.8 should fix the python version compatibility issues for torch 1.7.0.,"Codecov Report

Merging #436 (fc40976) into master (adeb99a) will increase coverage by 1.07%.
The diff coverage is n/a.


@@            Coverage Diff             @@
##           master     #436      +/-   ##
==========================================
+ Coverage   59.39%   60.46%   +1.07%     
==========================================
  Files          94       94              
  Lines        6693     6566     -127     
==========================================
- Hits         3975     3970       -5     
+ Misses       2718     2596     -122     



Impacted Files
Coverage 





thinc/shims/pytorch.py
83.58% <0.00%> (-0.25%)



thinc/shims/tensorflow.py
89.50% <0.00%> (-0.06%)



thinc/util.py
92.59% <0.00%> (-0.06%)



thinc/layers/tensorflowwrapper.py
98.88% <0.00%> (-0.02%)



thinc/config.py
96.06% <0.00%> (-0.01%)



thinc/tests/mypy/test_mypy.py
0.00% <0.00%> ()



thinc/tests/layers/test_lstm.py
0.00% <0.00%> ()



thinc/tests/model/test_model.py
0.00% <0.00%> ()



thinc/tests/backends/test_ops.py
0.00% <0.00%> ()



thinc/tests/layers/test_mnist.py
0.00% <0.00%> ()



... and 12 more






Continue to review full report at Codecov.

Legend - Click here to learn more
 = absolute <relative> (impact),  = not affected, ? = missing data
Powered by Codecov. Last update adeb99a...fc40976. Read the comment docs.",True,{}
explosion/thinc,https://github.com/explosion/thinc,436,2020-12-02T09:01:25Z,2020-12-02T09:10:06Z,2021-01-21T14:09:41Z,CLOSED,False,3,2,2,https://github.com/adrianeboyd,Remove upper pin for torch,1,['install'],https://github.com/explosion/thinc/pull/436,https://github.com/adrianeboyd,3,https://github.com/explosion/thinc/pull/436#issuecomment-737096518,dataclasses 0.8 should fix the python version compatibility issues for torch 1.7.0.,Nevermind...,True,{}
explosion/thinc,https://github.com/explosion/thinc,437,2020-12-02T09:03:07Z,2020-12-02T09:41:26Z,2021-01-21T14:09:34Z,MERGED,True,72,72,30,https://github.com/adrianeboyd,Switch to absolute imports in tests,1,['tests'],https://github.com/explosion/thinc/pull/437,https://github.com/adrianeboyd,1,https://github.com/explosion/thinc/pull/437,,,True,{}
explosion/thinc,https://github.com/explosion/thinc,438,2020-12-02T09:12:53Z,2020-12-04T07:53:30Z,2021-01-21T14:09:15Z,CLOSED,False,5,2,2,https://github.com/adrianeboyd,Forbid torch==1.7.0,1,['install'],https://github.com/explosion/thinc/pull/438,https://github.com/adrianeboyd,1,https://github.com/explosion/thinc/pull/438,"The next release of torch after 1.7.0 should fix this, so just forbid 1.7.0 specifically.","The next release of torch after 1.7.0 should fix this, so just forbid 1.7.0 specifically.",True,{}
explosion/thinc,https://github.com/explosion/thinc,438,2020-12-02T09:12:53Z,2020-12-04T07:53:30Z,2021-01-21T14:09:15Z,CLOSED,False,5,2,2,https://github.com/adrianeboyd,Forbid torch==1.7.0,1,['install'],https://github.com/explosion/thinc/pull/438,https://github.com/adrianeboyd,2,https://github.com/explosion/thinc/pull/438#issuecomment-737098322,"The next release of torch after 1.7.0 should fix this, so just forbid 1.7.0 specifically.",I think the other alternative is pinning dataclasses to ==0.6.,True,{}
explosion/thinc,https://github.com/explosion/thinc,438,2020-12-02T09:12:53Z,2020-12-04T07:53:30Z,2021-01-21T14:09:15Z,CLOSED,False,5,2,2,https://github.com/adrianeboyd,Forbid torch==1.7.0,1,['install'],https://github.com/explosion/thinc/pull/438,https://github.com/adrianeboyd,3,https://github.com/explosion/thinc/pull/438#issuecomment-737100351,"The next release of torch after 1.7.0 should fix this, so just forbid 1.7.0 specifically.","But ugh, I don't really like any of this. Maybe only do this for the affected versions of python?",True,{}
explosion/thinc,https://github.com/explosion/thinc,438,2020-12-02T09:12:53Z,2020-12-04T07:53:30Z,2021-01-21T14:09:15Z,CLOSED,False,5,2,2,https://github.com/adrianeboyd,Forbid torch==1.7.0,1,['install'],https://github.com/explosion/thinc/pull/438,https://github.com/apps/codecov,4,https://github.com/explosion/thinc/pull/438#issuecomment-737103896,"The next release of torch after 1.7.0 should fix this, so just forbid 1.7.0 specifically.","Codecov Report

Merging #438 (d4ee1d9) into master (adeb99a) will not change coverage.
The diff coverage is n/a.


@@           Coverage Diff           @@
##           master     #438   +/-   ##
=======================================
  Coverage   59.39%   59.39%           
=======================================
  Files          94       94           
  Lines        6693     6693           
=======================================
  Hits         3975     3975           
  Misses       2718     2718           

Continue to review full report at Codecov.

Legend - Click here to learn more
 = absolute <relative> (impact),  = not affected, ? = missing data
Powered by Codecov. Last update adeb99a...d4ee1d9. Read the comment docs.",True,{}
explosion/thinc,https://github.com/explosion/thinc,438,2020-12-02T09:12:53Z,2020-12-04T07:53:30Z,2021-01-21T14:09:15Z,CLOSED,False,5,2,2,https://github.com/adrianeboyd,Forbid torch==1.7.0,1,['install'],https://github.com/explosion/thinc/pull/438,https://github.com/bratao,5,https://github.com/explosion/thinc/pull/438#issuecomment-738023316,"The next release of torch after 1.7.0 should fix this, so just forbid 1.7.0 specifically.",Pinning dataclasses looks more elegant then pinning the actual pytorch version,True,{}
explosion/thinc,https://github.com/explosion/thinc,438,2020-12-02T09:12:53Z,2020-12-04T07:53:30Z,2021-01-21T14:09:15Z,CLOSED,False,5,2,2,https://github.com/adrianeboyd,Forbid torch==1.7.0,1,['install'],https://github.com/explosion/thinc/pull/438,https://github.com/adrianeboyd,6,https://github.com/explosion/thinc/pull/438#issuecomment-738275109,"The next release of torch after 1.7.0 should fix this, so just forbid 1.7.0 specifically.","I mixed up two different problems here.
Pinning dataclasses works for the reported install problem where no version of dataclasses is found (which I can't manage to replicate, it's some kind of pip + -f bug for particular versions of pip, maybe?).
The problem is that having dataclasses installed at all for python 3.7+ leads to the ""typing has no attribute _ClassVar error"" where you can't run the test suite as we do in the CI. Whether you get the built-in dataclasses or the one in site-packages seems to depend on exactly how/where things are imported and when only one of them works, this is no good.
It's extremely frustrating that the torch 1.7.0 wheels are broken like this and that they haven't published an update. There's no way for the thinc package to specify that a module be uninstalled or that a module is forbidden. There's no good way to express that the version is compatible but install it with --no-deps.
You can always install torch 1.7.0 by hand and uninstall dataclasses if you know what you're doing?
Again, argh.
I guess this is possible
torch>=1.5.0 ; python_version == '3.6'
torch>=1.5.0,!=1.7.0 ; python_version >= '3.6'

The other problem is that installing with thinc[torch] probably doesn't get the exact CPU/CUDA version you want anyway, so you're better off installing it separately following their install instructions, and then none of this is checked anyway, so on the one hand who cares about our specifications here, and on the other, why bother add this confusion at all.",True,{}
explosion/thinc,https://github.com/explosion/thinc,438,2020-12-02T09:12:53Z,2020-12-04T07:53:30Z,2021-01-21T14:09:15Z,CLOSED,False,5,2,2,https://github.com/adrianeboyd,Forbid torch==1.7.0,1,['install'],https://github.com/explosion/thinc/pull/438,https://github.com/honnibal,7,https://github.com/explosion/thinc/pull/438#issuecomment-738499577,"The next release of torch after 1.7.0 should fix this, so just forbid 1.7.0 specifically.","Ugh.
Maybe thinc[torch] and spacy[torch] were a mistake? They're just nowhere near fine-grained enough to be helpful. Should we remove them?",True,{}
explosion/thinc,https://github.com/explosion/thinc,438,2020-12-02T09:12:53Z,2020-12-04T07:53:30Z,2021-01-21T14:09:15Z,CLOSED,False,5,2,2,https://github.com/adrianeboyd,Forbid torch==1.7.0,1,['install'],https://github.com/explosion/thinc/pull/438,https://github.com/adrianeboyd,8,https://github.com/explosion/thinc/pull/438#issuecomment-738625684,"The next release of torch after 1.7.0 should fix this, so just forbid 1.7.0 specifically.","I don't think we have spacy[torch]? Probably we should just have >=1.5.0 here and not care too much since it's just an extra.
The requirement that probably matters more is the one in spacy-transformers, since that will be checked on install no matter how you've installed torch.",True,{}
explosion/thinc,https://github.com/explosion/thinc,438,2020-12-02T09:12:53Z,2020-12-04T07:53:30Z,2021-01-21T14:09:15Z,CLOSED,False,5,2,2,https://github.com/adrianeboyd,Forbid torch==1.7.0,1,['install'],https://github.com/explosion/thinc/pull/438,https://github.com/adrianeboyd,9,https://github.com/explosion/thinc/pull/438#issuecomment-738625957,"The next release of torch after 1.7.0 should fix this, so just forbid 1.7.0 specifically.","The CI should avoid 1.7.0 and use the +cpu versions, though, I can at least do that here.",True,{}
explosion/thinc,https://github.com/explosion/thinc,440,2020-12-03T09:20:20Z,2020-12-03T09:28:39Z,2020-12-03T09:28:39Z,CLOSED,False,14,10,3,https://github.com/adrianeboyd,Backport cupyx.scatter_add fix to v7,2,[],https://github.com/explosion/thinc/pull/440,https://github.com/adrianeboyd,1,https://github.com/explosion/thinc/pull/440,Backport #330 to v7.,Backport #330 to v7.,True,{}
explosion/thinc,https://github.com/explosion/thinc,441,2020-12-03T09:57:04Z,2020-12-04T01:33:51Z,2021-01-21T14:08:33Z,MERGED,True,16,18,3,https://github.com/adrianeboyd,Backport cupyx.scatter_add fix to v7,3,"['bug', 'feat / ops']",https://github.com/explosion/thinc/pull/441,https://github.com/adrianeboyd,1,https://github.com/explosion/thinc/pull/441,"Backport #330 to v7.
Switch older references to ops.xp.scatter_add to use ops.scatter_add, which is present in all supported versions of cupy.
Remove upper cupy pins from extras.","Backport #330 to v7.
Switch older references to ops.xp.scatter_add to use ops.scatter_add, which is present in all supported versions of cupy.
Remove upper cupy pins from extras.",True,{}
explosion/thinc,https://github.com/explosion/thinc,442,2020-12-04T07:54:22Z,2020-12-08T14:26:44Z,2021-01-21T14:07:50Z,MERGED,True,23,4,4,https://github.com/adrianeboyd,Update torch pins,4,['install'],https://github.com/explosion/thinc/pull/442,https://github.com/adrianeboyd,1,https://github.com/explosion/thinc/pull/442,"Remove upper torch pin in extras, users are on their own to uninstall dataclasses
Switch to +cpu in CI tests for smaller downloads","Remove upper torch pin in extras, users are on their own to uninstall dataclasses
Switch to +cpu in CI tests for smaller downloads",True,{}
explosion/thinc,https://github.com/explosion/thinc,442,2020-12-04T07:54:22Z,2020-12-08T14:26:44Z,2021-01-21T14:07:50Z,MERGED,True,23,4,4,https://github.com/adrianeboyd,Update torch pins,4,['install'],https://github.com/explosion/thinc/pull/442,https://github.com/apps/codecov,2,https://github.com/explosion/thinc/pull/442#issuecomment-738630098,"Remove upper torch pin in extras, users are on their own to uninstall dataclasses
Switch to +cpu in CI tests for smaller downloads","Codecov Report

Merging #442 (34ab7b1) into master (48d7161) will increase coverage by 0.07%.
The diff coverage is n/a.


@@            Coverage Diff             @@
##           master     #442      +/-   ##
==========================================
+ Coverage   59.38%   59.45%   +0.07%     
==========================================
  Files          94       94              
  Lines        6692     6687       -5     
==========================================
+ Hits         3974     3976       +2     
+ Misses       2718     2711       -7     



Impacted Files
Coverage 





thinc/layers/with_padded.py
80.26% <0.00%> (-0.26%)



thinc/layers/lstm.py
100.00% <0.00%> ()



thinc/layers/with_list.py
100.00% <0.00%> ()



thinc/layers/pytorchwrapper.py
100.00% <0.00%> ()



thinc/layers/with_cpu.py
32.35% <0.00%> (+5.52%)





Continue to review full report at Codecov.

Legend - Click here to learn more
 = absolute <relative> (impact),  = not affected, ? = missing data
Powered by Codecov. Last update 48d7161...34ab7b1. Read the comment docs.",True,{}
explosion/thinc,https://github.com/explosion/thinc,443,2020-12-04T15:02:19Z,2020-12-15T10:05:39Z,2021-01-21T14:07:32Z,MERGED,True,6,6,3,https://github.com/adrianeboyd,Update for pydantic 1.7 and python 3.9,1,['feat / types'],https://github.com/explosion/thinc/pull/443,https://github.com/adrianeboyd,1,https://github.com/explosion/thinc/pull/443,Rename Ragged._cumsums to Ragged.cumsums so it's not flagged as an invalid field.,Rename Ragged._cumsums to Ragged.cumsums so it's not flagged as an invalid field.,True,{}
explosion/thinc,https://github.com/explosion/thinc,443,2020-12-04T15:02:19Z,2020-12-15T10:05:39Z,2021-01-21T14:07:32Z,MERGED,True,6,6,3,https://github.com/adrianeboyd,Update for pydantic 1.7 and python 3.9,1,['feat / types'],https://github.com/explosion/thinc/pull/443,https://github.com/adrianeboyd,2,https://github.com/explosion/thinc/pull/443#issuecomment-738832513,Rename Ragged._cumsums to Ragged.cumsums so it's not flagged as an invalid field.,"There is maybe some way to get this to work as a private attribute or having it ignore fields that start with underscore, but I couldn't get it to work.",True,{}
explosion/thinc,https://github.com/explosion/thinc,443,2020-12-04T15:02:19Z,2020-12-15T10:05:39Z,2021-01-21T14:07:32Z,MERGED,True,6,6,3,https://github.com/adrianeboyd,Update for pydantic 1.7 and python 3.9,1,['feat / types'],https://github.com/explosion/thinc/pull/443,https://github.com/apps/codecov,3,https://github.com/explosion/thinc/pull/443#issuecomment-738836198,Rename Ragged._cumsums to Ragged.cumsums so it's not flagged as an invalid field.,"Codecov Report

Merging #443 (4e8cf6f) into master (48d7161) will increase coverage by 0.07%.
The diff coverage is n/a.


@@            Coverage Diff             @@
##           master     #443      +/-   ##
==========================================
+ Coverage   59.38%   59.45%   +0.07%     
==========================================
  Files          94       94              
  Lines        6692     6687       -5     
==========================================
+ Hits         3974     3976       +2     
+ Misses       2718     2711       -7     



Impacted Files
Coverage 





thinc/layers/with_padded.py
80.26% <0.00%> (-0.26%)



thinc/layers/lstm.py
100.00% <0.00%> ()



thinc/layers/with_list.py
100.00% <0.00%> ()



thinc/layers/pytorchwrapper.py
100.00% <0.00%> ()



thinc/layers/with_cpu.py
32.35% <0.00%> (+5.52%)





Continue to review full report at Codecov.

Legend - Click here to learn more
 = absolute <relative> (impact),  = not affected, ? = missing data
Powered by Codecov. Last update 48d7161...4e8cf6f. Read the comment docs.",True,{}
explosion/thinc,https://github.com/explosion/thinc,443,2020-12-04T15:02:19Z,2020-12-15T10:05:39Z,2021-01-21T14:07:32Z,MERGED,True,6,6,3,https://github.com/adrianeboyd,Update for pydantic 1.7 and python 3.9,1,['feat / types'],https://github.com/explosion/thinc/pull/443,https://github.com/adrianeboyd,4,https://github.com/explosion/thinc/pull/443#issuecomment-740649917,Rename Ragged._cumsums to Ragged.cumsums so it's not flagged as an invalid field.,Maybe this should be 1.7.1 instead because of the dataclasses arbitrary types thing?,True,{}
explosion/thinc,https://github.com/explosion/thinc,443,2020-12-04T15:02:19Z,2020-12-15T10:05:39Z,2021-01-21T14:07:32Z,MERGED,True,6,6,3,https://github.com/adrianeboyd,Update for pydantic 1.7 and python 3.9,1,['feat / types'],https://github.com/explosion/thinc/pull/443,https://github.com/honnibal,5,https://github.com/explosion/thinc/pull/443#issuecomment-741082446,Rename Ragged._cumsums to Ragged.cumsums so it's not flagged as an invalid field.,"What's the deal with this invalid field thing? You can't have private fields on a dataclass?
It's a bit annoying, because I don't want that to be part of the public interface =/",True,{}
explosion/thinc,https://github.com/explosion/thinc,443,2020-12-04T15:02:19Z,2020-12-15T10:05:39Z,2021-01-21T14:07:32Z,MERGED,True,6,6,3,https://github.com/adrianeboyd,Update for pydantic 1.7 and python 3.9,1,['feat / types'],https://github.com/explosion/thinc/pull/443,https://github.com/adrianeboyd,6,https://github.com/explosion/thinc/pull/443#issuecomment-741618333,Rename Ragged._cumsums to Ragged.cumsums so it's not flagged as an invalid field.,"I know, but if this is the only thing holding up python 3.9 support maybe we could at least temporarily modify it. But maybe someone more knowledgeable (@tiangolo?) knows how to configure this correctly?
You get this warning from the is_valid_field check:
RuntimeWarning: fields may not start with an underscore, ignoring ""_cumsums""

and then the error:
X -> _cumsums   extra fields not permitted

I tried things like adding underscore_attrs_are_private to _ArgModelConfig and it didn't seem to do what I expected, but I don't understand all the details here.",True,{}
explosion/thinc,https://github.com/explosion/thinc,443,2020-12-04T15:02:19Z,2020-12-15T10:05:39Z,2021-01-21T14:07:32Z,MERGED,True,6,6,3,https://github.com/adrianeboyd,Update for pydantic 1.7 and python 3.9,1,['feat / types'],https://github.com/explosion/thinc/pull/443,https://github.com/tiangolo,7,https://github.com/explosion/thinc/pull/443#issuecomment-752539634,Rename Ragged._cumsums to Ragged.cumsums so it's not flagged as an invalid field.,"I'm sorry for the delay, my GitHub notifications are a mess, I have 1500 unread emails. ",True,{}
explosion/thinc,https://github.com/explosion/thinc,444,2020-12-08T11:46:16Z,2020-12-08T14:27:14Z,2021-01-21T14:06:30Z,MERGED,True,1,1,1,https://github.com/adrianeboyd,Fix cupy-cuda111 extra,1,['install'],https://github.com/explosion/thinc/pull/444,https://github.com/adrianeboyd,1,https://github.com/explosion/thinc/pull/444,,,True,{}
explosion/thinc,https://github.com/explosion/thinc,445,2020-12-08T11:52:14Z,2020-12-08T14:27:31Z,2021-01-21T14:06:20Z,MERGED,True,6,0,1,https://github.com/adrianeboyd,Update cupy extras,1,['install'],https://github.com/explosion/thinc/pull/445,https://github.com/adrianeboyd,1,https://github.com/explosion/thinc/pull/445,,,True,{}
explosion/thinc,https://github.com/explosion/thinc,445,2020-12-08T11:52:14Z,2020-12-08T14:27:31Z,2021-01-21T14:06:20Z,MERGED,True,6,0,1,https://github.com/adrianeboyd,Update cupy extras,1,['install'],https://github.com/explosion/thinc/pull/445,https://github.com/apps/codecov,2,https://github.com/explosion/thinc/pull/445#issuecomment-740577440,,"Codecov Report

Merging #445 (67e3662) into master (9eb1eeb) will decrease coverage by 0.27%.
The diff coverage is 34.61%.


@@            Coverage Diff             @@
##           master     #445      +/-   ##
==========================================
- Coverage   59.73%   59.45%   -0.28%     
==========================================
  Files          94       94              
  Lines        6643     6687      +44     
==========================================
+ Hits         3968     3976       +8     
- Misses       2675     2711      +36     



Impacted Files
Coverage 





thinc/config.py
96.07% <> (-0.18%)



thinc/layers/__init__.py
100.00% <> ()



thinc/layers/pytorchwrapper.py
100.00% <> ()



thinc/model.py
94.03% <> (-0.05%)



thinc/tests/layers/test_layers_api.py
0.00% <0.00%> ()



thinc/tests/layers/test_uniqued.py
0.00% <0.00%> ()



thinc/util.py
92.64% <> ()



thinc/layers/with_cpu.py
32.35% <50.00%> (+5.52%)



thinc/about.py
100.00% <100.00%> ()



thinc/api.py
100.00% <100.00%> ()



... and 11 more






Continue to review full report at Codecov.

Legend - Click here to learn more
 = absolute <relative> (impact),  = not affected, ? = missing data
Powered by Codecov. Last update 40360fa...67e3662. Read the comment docs.",True,{}
explosion/thinc,https://github.com/explosion/thinc,447,2020-12-15T09:14:40Z,2020-12-15T10:48:14Z,2020-12-15T10:48:16Z,MERGED,True,17,3,3,https://github.com/ines,Make dataclasses work with Pydantic >= 1.7,3,"['enhancement', 'feat / config', 'third-party']",https://github.com/explosion/thinc/pull/447,https://github.com/ines,1,https://github.com/explosion/thinc/pull/447,Don't use private dataclass fields for now (which conflict with the way Pydantic validates dataclasses in > 1.7).  This should make Thinc and by extension spaCy work with the latest Pydantic and Python 3.9.,Don't use private dataclass fields for now (which conflict with the way Pydantic validates dataclasses in > 1.7).  This should make Thinc and by extension spaCy work with the latest Pydantic and Python 3.9.,True,{}
explosion/thinc,https://github.com/explosion/thinc,447,2020-12-15T09:14:40Z,2020-12-15T10:48:14Z,2020-12-15T10:48:16Z,MERGED,True,17,3,3,https://github.com/ines,Make dataclasses work with Pydantic >= 1.7,3,"['enhancement', 'feat / config', 'third-party']",https://github.com/explosion/thinc/pull/447,https://github.com/apps/codecov,2,https://github.com/explosion/thinc/pull/447#issuecomment-745161465,Don't use private dataclass fields for now (which conflict with the way Pydantic validates dataclasses in > 1.7).  This should make Thinc and by extension spaCy work with the latest Pydantic and Python 3.9.,"Codecov Report

Merging #447 (f5976b9) into master (94beadc) will not change coverage.
The diff coverage is n/a.


@@           Coverage Diff           @@
##           master     #447   +/-   ##
=======================================
  Coverage   59.45%   59.45%           
=======================================
  Files          94       94           
  Lines        6687     6687           
=======================================
  Hits         3976     3976           
  Misses       2711     2711           

Continue to review full report at Codecov.

Legend - Click here to learn more
 = absolute <relative> (impact),  = not affected, ? = missing data
Powered by Codecov. Last update 94beadc...f5976b9. Read the comment docs.",True,{}
explosion/thinc,https://github.com/explosion/thinc,448,2020-12-15T14:57:14Z,2020-12-16T10:54:55Z,2020-12-16T10:54:55Z,MERGED,True,27,4,3,https://github.com/adrianeboyd,Update install notes for v8,1,['docs'],https://github.com/explosion/thinc/pull/448,https://github.com/adrianeboyd,1,https://github.com/explosion/thinc/pull/448,,,True,{}
explosion/thinc,https://github.com/explosion/thinc,448,2020-12-15T14:57:14Z,2020-12-16T10:54:55Z,2020-12-16T10:54:55Z,MERGED,True,27,4,3,https://github.com/adrianeboyd,Update install notes for v8,1,['docs'],https://github.com/explosion/thinc/pull/448,https://github.com/apps/codecov,2,https://github.com/explosion/thinc/pull/448#issuecomment-745351318,,"Codecov Report

Merging #448 (7feea5d) into master (94beadc) will not change coverage.
The diff coverage is n/a.


@@           Coverage Diff           @@
##           master     #448   +/-   ##
=======================================
  Coverage   59.45%   59.45%           
=======================================
  Files          94       94           
  Lines        6687     6687           
=======================================
  Hits         3976     3976           
  Misses       2711     2711           

Continue to review full report at Codecov.

Legend - Click here to learn more
 = absolute <relative> (impact),  = not affected, ? = missing data
Powered by Codecov. Last update 94beadc...7feea5d. Read the comment docs.",True,{}
explosion/thinc,https://github.com/explosion/thinc,449,2020-12-16T10:55:07Z,2020-12-16T11:21:55Z,2020-12-16T11:21:55Z,MERGED,True,1,1,1,https://github.com/adrianeboyd,Set version to v8.0.0rc3,1,[],https://github.com/explosion/thinc/pull/449,https://github.com/adrianeboyd,1,https://github.com/explosion/thinc/pull/449,,,True,{}
explosion/thinc,https://github.com/explosion/thinc,449,2020-12-16T10:55:07Z,2020-12-16T11:21:55Z,2020-12-16T11:21:55Z,MERGED,True,1,1,1,https://github.com/adrianeboyd,Set version to v8.0.0rc3,1,[],https://github.com/explosion/thinc/pull/449,https://github.com/apps/codecov,2,https://github.com/explosion/thinc/pull/449#issuecomment-746104807,,"Codecov Report

Merging #449 (6bb6560) into master (94beadc) will not change coverage.
The diff coverage is 100.00%.


@@           Coverage Diff           @@
##           master     #449   +/-   ##
=======================================
  Coverage   59.45%   59.45%           
=======================================
  Files          94       94           
  Lines        6687     6687           
=======================================
  Hits         3976     3976           
  Misses       2711     2711           



Impacted Files
Coverage 





thinc/about.py
100.00% <100.00%> ()





Continue to review full report at Codecov.

Legend - Click here to learn more
 = absolute <relative> (impact),  = not affected, ? = missing data
Powered by Codecov. Last update 94beadc...6bb6560. Read the comment docs.",True,{}
explosion/thinc,https://github.com/explosion/thinc,451,2020-12-23T02:46:36Z,2020-12-27T20:32:54Z,2020-12-28T00:11:14Z,MERGED,True,1,1,1,https://github.com/kigawas,Fixes indent error in concept.md,1,['docs'],https://github.com/explosion/thinc/pull/451,https://github.com/kigawas,1,https://github.com/explosion/thinc/pull/451,,,True,{'THUMBS_UP': ['https://github.com/svlandeg']}
explosion/thinc,https://github.com/explosion/thinc,451,2020-12-23T02:46:36Z,2020-12-27T20:32:54Z,2020-12-28T00:11:14Z,MERGED,True,1,1,1,https://github.com/kigawas,Fixes indent error in concept.md,1,['docs'],https://github.com/explosion/thinc/pull/451,https://github.com/svlandeg,2,https://github.com/explosion/thinc/pull/451#issuecomment-751512743,,Thanks!,True,{}
explosion/thinc,https://github.com/explosion/thinc,452,2020-12-23T17:07:53Z,2021-01-05T02:39:05Z,2021-01-05T02:39:05Z,MERGED,True,13,7,1,https://github.com/bratao,Fix radam,1,"['bug', 'feat / optimizers']",https://github.com/explosion/thinc/pull/452,https://github.com/bratao,1,https://github.com/explosion/thinc/pull/452,"Radam was broken in latest version.
This PR fixes #450 by mimicking _adam behavior and to work with a 1d array.
Radam is much more resistant to the learning rate. + Lookahead = Ranger is the best optimizer for me!","Radam was broken in latest version.
This PR fixes #450 by mimicking _adam behavior and to work with a 1d array.
Radam is much more resistant to the learning rate. + Lookahead = Ranger is the best optimizer for me!",True,{'THUMBS_UP': ['https://github.com/svlandeg']}
explosion/thinc,https://github.com/explosion/thinc,452,2020-12-23T17:07:53Z,2021-01-05T02:39:05Z,2021-01-05T02:39:05Z,MERGED,True,13,7,1,https://github.com/bratao,Fix radam,1,"['bug', 'feat / optimizers']",https://github.com/explosion/thinc/pull/452,https://github.com/apps/codecov,2,https://github.com/explosion/thinc/pull/452#issuecomment-750394005,"Radam was broken in latest version.
This PR fixes #450 by mimicking _adam behavior and to work with a 1d array.
Radam is much more resistant to the learning rate. + Lookahead = Ranger is the best optimizer for me!","Codecov Report

Merging #452 (553044a) into master (115da8d) will increase coverage by 0.01%.
The diff coverage is 66.66%.


@@            Coverage Diff             @@
##           master     #452      +/-   ##
==========================================
+ Coverage   59.45%   59.47%   +0.01%     
==========================================
  Files          94       94              
  Lines        6687     6689       +2     
==========================================
+ Hits         3976     3978       +2     
  Misses       2711     2711              



Impacted Files
Coverage 





thinc/optimizers.py
94.26% <66.66%> (+0.07%)





Continue to review full report at Codecov.

Legend - Click here to learn more
 = absolute <relative> (impact),  = not affected, ? = missing data
Powered by Codecov. Last update 115da8d...553044a. Read the comment docs.",True,{}
explosion/thinc,https://github.com/explosion/thinc,452,2020-12-23T17:07:53Z,2021-01-05T02:39:05Z,2021-01-05T02:39:05Z,MERGED,True,13,7,1,https://github.com/bratao,Fix radam,1,"['bug', 'feat / optimizers']",https://github.com/explosion/thinc/pull/452,https://github.com/honnibal,3,https://github.com/explosion/thinc/pull/452#issuecomment-754349117,"Radam was broken in latest version.
This PR fixes #450 by mimicking _adam behavior and to work with a 1d array.
Radam is much more resistant to the learning rate. + Lookahead = Ranger is the best optimizer for me!",Thanks!,True,{}
explosion/thinc,https://github.com/explosion/thinc,453,2021-01-03T16:35:12Z,2021-01-05T02:39:30Z,2021-01-05T02:39:31Z,MERGED,True,2,2,1,https://github.com/wavenator,fix ResourceWarning for opening a file without explicitly closing it,1,['bug'],https://github.com/explosion/thinc/pull/453,https://github.com/wavenator,1,https://github.com/explosion/thinc/pull/453,"Running pytest on my project results with the following error:
/usr/local/lib64/python3.8/site-packages/thinc/neural/_custom_kernels.py:36                                                                                                                                      
  /usr/local/lib64/python3.8/site-packages/thinc/neural/_custom_kernels.py:36: ResourceWarning: unclosed file <_io.TextIOWrapper name='/usr/local/lib64/python3.8/site-packages/thinc/neural/_custom_kernels.cu' mo
de='r' encoding='utf8'>                                                                                                                                                                                            
    SRC = (PWD / ""_custom_kernels.cu"").open(""r"", encoding=""utf8"").read() 
This happens because in _custom_kernels.py file you open a file without explicitly closing it. I replaced your open call with a read_text call.","Running pytest on my project results with the following error:
/usr/local/lib64/python3.8/site-packages/thinc/neural/_custom_kernels.py:36                                                                                                                                      
  /usr/local/lib64/python3.8/site-packages/thinc/neural/_custom_kernels.py:36: ResourceWarning: unclosed file <_io.TextIOWrapper name='/usr/local/lib64/python3.8/site-packages/thinc/neural/_custom_kernels.cu' mo
de='r' encoding='utf8'>                                                                                                                                                                                            
    SRC = (PWD / ""_custom_kernels.cu"").open(""r"", encoding=""utf8"").read() 
This happens because in _custom_kernels.py file you open a file without explicitly closing it. I replaced your open call with a read_text call.",True,{'THUMBS_UP': ['https://github.com/svlandeg']}
explosion/thinc,https://github.com/explosion/thinc,453,2021-01-03T16:35:12Z,2021-01-05T02:39:30Z,2021-01-05T02:39:31Z,MERGED,True,2,2,1,https://github.com/wavenator,fix ResourceWarning for opening a file without explicitly closing it,1,['bug'],https://github.com/explosion/thinc/pull/453,https://github.com/apps/codecov,2,https://github.com/explosion/thinc/pull/453#issuecomment-753643751,"Running pytest on my project results with the following error:
/usr/local/lib64/python3.8/site-packages/thinc/neural/_custom_kernels.py:36                                                                                                                                      
  /usr/local/lib64/python3.8/site-packages/thinc/neural/_custom_kernels.py:36: ResourceWarning: unclosed file <_io.TextIOWrapper name='/usr/local/lib64/python3.8/site-packages/thinc/neural/_custom_kernels.cu' mo
de='r' encoding='utf8'>                                                                                                                                                                                            
    SRC = (PWD / ""_custom_kernels.cu"").open(""r"", encoding=""utf8"").read() 
This happens because in _custom_kernels.py file you open a file without explicitly closing it. I replaced your open call with a read_text call.","Codecov Report

Merging #453 (a45eefe) into master (115da8d) will not change coverage.
The diff coverage is n/a.


@@           Coverage Diff           @@
##           master     #453   +/-   ##
=======================================
  Coverage   59.45%   59.45%           
=======================================
  Files          94       94           
  Lines        6687     6687           
=======================================
  Hits         3976     3976           
  Misses       2711     2711           

Continue to review full report at Codecov.

Legend - Click here to learn more
 = absolute <relative> (impact),  = not affected, ? = missing data
Powered by Codecov. Last update e457f1a...a45eefe. Read the comment docs.",True,{}
explosion/thinc,https://github.com/explosion/thinc,453,2021-01-03T16:35:12Z,2021-01-05T02:39:30Z,2021-01-05T02:39:31Z,MERGED,True,2,2,1,https://github.com/wavenator,fix ResourceWarning for opening a file without explicitly closing it,1,['bug'],https://github.com/explosion/thinc/pull/453,https://github.com/honnibal,3,https://github.com/explosion/thinc/pull/453#issuecomment-754349255,"Running pytest on my project results with the following error:
/usr/local/lib64/python3.8/site-packages/thinc/neural/_custom_kernels.py:36                                                                                                                                      
  /usr/local/lib64/python3.8/site-packages/thinc/neural/_custom_kernels.py:36: ResourceWarning: unclosed file <_io.TextIOWrapper name='/usr/local/lib64/python3.8/site-packages/thinc/neural/_custom_kernels.cu' mo
de='r' encoding='utf8'>                                                                                                                                                                                            
    SRC = (PWD / ""_custom_kernels.cu"").open(""r"", encoding=""utf8"").read() 
This happens because in _custom_kernels.py file you open a file without explicitly closing it. I replaced your open call with a read_text call.",Thanks!,True,{}
explosion/thinc,https://github.com/explosion/thinc,454,2021-01-04T14:07:24Z,2021-01-05T02:38:22Z,2021-01-21T14:04:33Z,MERGED,True,13,1,2,https://github.com/svlandeg,Adding a few assert statements for robustness,2,['performance'],https://github.com/explosion/thinc/pull/454,https://github.com/svlandeg,1,https://github.com/explosion/thinc/pull/454,Adding sanity checks to avoid 0-length memory allocations,Adding sanity checks to avoid 0-length memory allocations,True,{}
explosion/thinc,https://github.com/explosion/thinc,454,2021-01-04T14:07:24Z,2021-01-05T02:38:22Z,2021-01-21T14:04:33Z,MERGED,True,13,1,2,https://github.com/svlandeg,Adding a few assert statements for robustness,2,['performance'],https://github.com/explosion/thinc/pull/454,https://github.com/apps/codecov,2,https://github.com/explosion/thinc/pull/454#issuecomment-753998015,Adding sanity checks to avoid 0-length memory allocations,"Codecov Report

Merging #454 (5452cb0) into master (115da8d) will not change coverage.
The diff coverage is n/a.


@@           Coverage Diff           @@
##           master     #454   +/-   ##
=======================================
  Coverage   59.45%   59.45%           
=======================================
  Files          94       94           
  Lines        6687     6687           
=======================================
  Hits         3976     3976           
  Misses       2711     2711           

Continue to review full report at Codecov.

Legend - Click here to learn more
 = absolute <relative> (impact),  = not affected, ? = missing data
Powered by Codecov. Last update e457f1a...5452cb0. Read the comment docs.",True,{}
explosion/thinc,https://github.com/explosion/thinc,456,2021-01-18T01:54:02Z,2021-01-18T05:01:23Z,2021-01-21T14:04:12Z,MERGED,True,112,3,7,https://github.com/honnibal,Add reduce_first and reduce_last layers,6,"['enhancement', 'feat / layers']",https://github.com/explosion/thinc/pull/456,https://github.com/honnibal,1,https://github.com/explosion/thinc/pull/456,"I've wanted these for a while. They're especially useful after LSTM and transformer models, where it makes sense to interpret the first or last vector as the representation of the whole sequence.","I've wanted these for a while. They're especially useful after LSTM and transformer models, where it makes sense to interpret the first or last vector as the representation of the whole sequence.",True,{}
explosion/thinc,https://github.com/explosion/thinc,456,2021-01-18T01:54:02Z,2021-01-18T05:01:23Z,2021-01-21T14:04:12Z,MERGED,True,112,3,7,https://github.com/honnibal,Add reduce_first and reduce_last layers,6,"['enhancement', 'feat / layers']",https://github.com/explosion/thinc/pull/456,https://github.com/apps/codecov,2,https://github.com/explosion/thinc/pull/456#issuecomment-761936046,"I've wanted these for a while. They're especially useful after LSTM and transformer models, where it makes sense to interpret the first or last vector as the representation of the whole sequence.","Codecov Report

Merging #456 (edb6c39) into master (90de1ff) will decrease coverage by 0.09%.
The diff coverage is 52.94%.


@@            Coverage Diff             @@
##           master     #456      +/-   ##
==========================================
- Coverage   59.47%   59.37%   -0.10%     
==========================================
  Files          94       97       +3     
  Lines        6689     6771      +82     
==========================================
+ Hits         3978     4020      +42     
- Misses       2711     2751      +40     



Impacted Files
Coverage 





thinc/tests/layers/test_reduce_first_last.py
0.00% <0.00%> ()



thinc/layers/concatenate.py
61.79% <66.66%> (-0.28%)



thinc/api.py
100.00% <100.00%> ()



thinc/layers/__init__.py
100.00% <100.00%> ()



thinc/layers/reduce_first.py
100.00% <100.00%> ()



thinc/layers/reduce_last.py
100.00% <100.00%> ()



thinc/util.py
92.64% <100.00%> ()





Continue to review full report at Codecov.

Legend - Click here to learn more
 = absolute <relative> (impact),  = not affected, ? = missing data
Powered by Codecov. Last update 90de1ff...edb6c39. Read the comment docs.",True,{}
explosion/thinc,https://github.com/explosion/thinc,457,2021-01-18T05:06:57Z,2021-01-18T07:15:47Z,2021-01-21T14:03:45Z,MERGED,True,10,7,1,https://github.com/honnibal,Fix ParametricAttention layer,1,"['bug', 'feat / layers']",https://github.com/explosion/thinc/pull/457,https://github.com/honnibal,1,https://github.com/explosion/thinc/pull/457,The ParametricAttention layer had fallen behind and wasn't working.,The ParametricAttention layer had fallen behind and wasn't working.,True,{}
explosion/thinc,https://github.com/explosion/thinc,457,2021-01-18T05:06:57Z,2021-01-18T07:15:47Z,2021-01-21T14:03:45Z,MERGED,True,10,7,1,https://github.com/honnibal,Fix ParametricAttention layer,1,"['bug', 'feat / layers']",https://github.com/explosion/thinc/pull/457,https://github.com/apps/codecov,2,https://github.com/explosion/thinc/pull/457#issuecomment-761984221,The ParametricAttention layer had fallen behind and wasn't working.,"Codecov Report

Merging #457 (964bbb5) into master (3d0e946) will increase coverage by 0.01%.
The diff coverage is 100.00%.


@@            Coverage Diff             @@
##           master     #457      +/-   ##
==========================================
+ Coverage   59.37%   59.38%   +0.01%     
==========================================
  Files          97       97              
  Lines        6771     6773       +2     
==========================================
+ Hits         4020     4022       +2     
  Misses       2751     2751              



Impacted Files
Coverage 





thinc/layers/parametricattention.py
100.00% <100.00%> ()





Continue to review full report at Codecov.

Legend - Click here to learn more
 = absolute <relative> (impact),  = not affected, ? = missing data
Powered by Codecov. Last update 3d0e946...964bbb5. Read the comment docs.",True,{}
explosion/thinc,https://github.com/explosion/thinc,458,2021-01-20T04:28:28Z,2021-01-20T08:04:58Z,2021-01-21T14:03:16Z,MERGED,True,143,30,5,https://github.com/honnibal,Separate with_array and with_array2d layers,5,"['enhancement', 'feat / layers']",https://github.com/explosion/thinc/pull/458,https://github.com/honnibal,1,https://github.com/explosion/thinc/pull/458,"When working on more examples, I've found I often need the with_array layer to behave differently than how we had it. I wanted it to not reshape the data, so that I can easily get at the Array3d that's behind a Padded layer. But other times you do want to get the concatenated sequence representation back out.
My answer is to have two layers, with_array and with_array2d. I think it's better to have a different wrapper than to have an argument, because with an argument the type signatures get out of control.","When working on more examples, I've found I often need the with_array layer to behave differently than how we had it. I wanted it to not reshape the data, so that I can easily get at the Array3d that's behind a Padded layer. But other times you do want to get the concatenated sequence representation back out.
My answer is to have two layers, with_array and with_array2d. I think it's better to have a different wrapper than to have an argument, because with an argument the type signatures get out of control.",True,{}
explosion/thinc,https://github.com/explosion/thinc,458,2021-01-20T04:28:28Z,2021-01-20T08:04:58Z,2021-01-21T14:03:16Z,MERGED,True,143,30,5,https://github.com/honnibal,Separate with_array and with_array2d layers,5,"['enhancement', 'feat / layers']",https://github.com/explosion/thinc/pull/458,https://github.com/apps/codecov,2,https://github.com/explosion/thinc/pull/458#issuecomment-763400254,"When working on more examples, I've found I often need the with_array layer to behave differently than how we had it. I wanted it to not reshape the data, so that I can easily get at the Array3d that's behind a Padded layer. But other times you do want to get the concatenated sequence representation back out.
My answer is to have two layers, with_array and with_array2d. I think it's better to have a different wrapper than to have an argument, because with an argument the type signatures get out of control.","Codecov Report

Merging #458 (1c29637) into master (47b0081) will increase coverage by 0.01%.
The diff coverage is 85.18%.


@@            Coverage Diff             @@
##           master     #458      +/-   ##
==========================================
+ Coverage   59.38%   59.39%   +0.01%     
==========================================
  Files          97       98       +1     
  Lines        6773     6834      +61     
==========================================
+ Hits         4022     4059      +37     
- Misses       2751     2775      +24     



Impacted Files
Coverage 





thinc/tests/layers/test_with_transforms.py
0.00% <0.00%> ()



thinc/layers/with_array.py
63.79% <41.66%> (-36.21%)



thinc/layers/with_array2d.py
95.31% <95.31%> ()



thinc/api.py
100.00% <100.00%> ()



thinc/layers/__init__.py
100.00% <100.00%> ()





Continue to review full report at Codecov.

Legend - Click here to learn more
 = absolute <relative> (impact),  = not affected, ? = missing data
Powered by Codecov. Last update 47b0081...1c29637. Read the comment docs.",True,{}
explosion/thinc,https://github.com/explosion/thinc,459,2021-01-20T07:40:53Z,2021-01-20T08:57:04Z,2022-02-28T15:05:17Z,MERGED,True,1097,303,9,https://github.com/honnibal,LSTM layers,96,"['enhancement', 'feat / layers']",https://github.com/explosion/thinc/pull/459,https://github.com/honnibal,1,https://github.com/explosion/thinc/pull/459,Cleaned up version of #304,Cleaned up version of #304,True,{}
explosion/thinc,https://github.com/explosion/thinc,459,2021-01-20T07:40:53Z,2021-01-20T08:57:04Z,2022-02-28T15:05:17Z,MERGED,True,1097,303,9,https://github.com/honnibal,LSTM layers,96,"['enhancement', 'feat / layers']",https://github.com/explosion/thinc/pull/459,https://github.com/apps/codecov,2,https://github.com/explosion/thinc/pull/459#issuecomment-763407836,Cleaned up version of #304,"Codecov Report

Merging #459 (708803a) into master (47b0081) will increase coverage by 0.25%.
The diff coverage is 74.33%.


@@            Coverage Diff             @@
##           master     #459      +/-   ##
==========================================
+ Coverage   59.38%   59.63%   +0.25%     
==========================================
  Files          97       98       +1     
  Lines        6773     7051     +278     
==========================================
+ Hits         4022     4205     +183     
- Misses       2751     2846      +95     



Impacted Files
Coverage 





thinc/layers/with_flatten.py
40.00% <0.00%> (+2.50%)



thinc/tests/backends/test_ops.py
0.00% <0.00%> ()



thinc/tests/layers/test_lstm.py
0.00% <0.00%> ()



thinc/layers/lstm.py
98.19% <97.36%> (-1.81%)



thinc/backends/ops.py
83.03% <98.88%> (+2.24%)



thinc/layers/bidirectional.py
37.50% <0.00%> (-62.50%)



thinc/layers/with_array.py
92.59% <0.00%> (-7.41%)



thinc/layers/clone.py
94.11% <0.00%> (-5.89%)



thinc/api.py
100.00% <0.00%> ()



thinc/layers/__init__.py
100.00% <0.00%> ()



... and 1 more






Continue to review full report at Codecov.

Legend - Click here to learn more
 = absolute <relative> (impact),  = not affected, ? = missing data
Powered by Codecov. Last update 47b0081...708803a. Read the comment docs.",True,{}
explosion/thinc,https://github.com/explosion/thinc,460,2021-01-20T09:20:41Z,2021-01-21T02:08:54Z,2021-01-21T14:01:18Z,MERGED,True,12,10,2,https://github.com/honnibal,Disable default input/output validation on initialize,3,['feat / types'],https://github.com/explosion/thinc/pull/460,https://github.com/honnibal,1,https://github.com/explosion/thinc/pull/460,"This PR disables the default type validation that would happen on model.initialize(), as it causes runtime errors if types are wrong. Specifying types entirely correctly can be fiddly, so raising runtime errors from the types can lead to problems.
A particular problem at the moment is that the chain() combinator cheats and passes in the Y output into non-final layers to assist with shape inference. This means that layers which define a type on that Y variable during init can fail validation. While the current state is a hack, more generally I think it's a bad idea to have a default that raises a runtime error on the types. The type errors also are not necessarily easier to debug than the runtime error.","This PR disables the default type validation that would happen on model.initialize(), as it causes runtime errors if types are wrong. Specifying types entirely correctly can be fiddly, so raising runtime errors from the types can lead to problems.
A particular problem at the moment is that the chain() combinator cheats and passes in the Y output into non-final layers to assist with shape inference. This means that layers which define a type on that Y variable during init can fail validation. While the current state is a hack, more generally I think it's a bad idea to have a default that raises a runtime error on the types. The type errors also are not necessarily easier to debug than the runtime error.",True,{}
explosion/thinc,https://github.com/explosion/thinc,460,2021-01-20T09:20:41Z,2021-01-21T02:08:54Z,2021-01-21T14:01:18Z,MERGED,True,12,10,2,https://github.com/honnibal,Disable default input/output validation on initialize,3,['feat / types'],https://github.com/explosion/thinc/pull/460,https://github.com/apps/codecov,2,https://github.com/explosion/thinc/pull/460#issuecomment-763496036,"This PR disables the default type validation that would happen on model.initialize(), as it causes runtime errors if types are wrong. Specifying types entirely correctly can be fiddly, so raising runtime errors from the types can lead to problems.
A particular problem at the moment is that the chain() combinator cheats and passes in the Y output into non-final layers to assist with shape inference. This means that layers which define a type on that Y variable during init can fail validation. While the current state is a hack, more generally I think it's a bad idea to have a default that raises a runtime error on the types. The type errors also are not necessarily easier to debug than the runtime error.","Codecov Report

Merging #460 (7bdd820) into master (089e210) will decrease coverage by 0.01%.
The diff coverage is 8.33%.


@@            Coverage Diff             @@
##           master     #460      +/-   ##
==========================================
- Coverage   59.26%   59.25%   -0.02%     
==========================================
  Files          98       98              
  Lines        7024     7026       +2     
==========================================
  Hits         4163     4163              
- Misses       2861     2863       +2     



Impacted Files
Coverage 





thinc/tests/model/test_validation.py
0.00% <0.00%> ()



thinc/util.py
92.64% <100.00%> ()





Continue to review full report at Codecov.

Legend - Click here to learn more
 = absolute <relative> (impact),  = not affected, ? = missing data
Powered by Codecov. Last update 089e210...7bdd820. Read the comment docs.",True,{}
explosion/thinc,https://github.com/explosion/thinc,462,2021-01-21T08:26:07Z,2021-01-21T09:08:04Z,2021-01-21T14:00:59Z,MERGED,True,12,6,1,https://github.com/honnibal,Fix types on add() combinator,2,['feat / layers'],https://github.com/explosion/thinc/pull/462,https://github.com/honnibal,1,https://github.com/explosion/thinc/pull/462,,,True,{}
explosion/thinc,https://github.com/explosion/thinc,462,2021-01-21T08:26:07Z,2021-01-21T09:08:04Z,2021-01-21T14:00:59Z,MERGED,True,12,6,1,https://github.com/honnibal,Fix types on add() combinator,2,['feat / layers'],https://github.com/explosion/thinc/pull/462,https://github.com/apps/codecov,2,https://github.com/explosion/thinc/pull/462#issuecomment-764471533,,"Codecov Report

Merging #462 (370840e) into master (6fbf95e) will increase coverage by 0.03%.
The diff coverage is 100.00%.


@@            Coverage Diff             @@
##           master     #462      +/-   ##
==========================================
+ Coverage   59.25%   59.28%   +0.03%     
==========================================
  Files          98       98              
  Lines        7026     7032       +6     
==========================================
+ Hits         4163     4169       +6     
  Misses       2863     2863              



Impacted Files
Coverage 





thinc/layers/add.py
100.00% <100.00%> ()





Continue to review full report at Codecov.

Legend - Click here to learn more
 = absolute <relative> (impact),  = not affected, ? = missing data
Powered by Codecov. Last update 6fbf95e...370840e. Read the comment docs.",True,{}
explosion/thinc,https://github.com/explosion/thinc,463,2021-01-21T08:55:53Z,2021-01-21T10:09:38Z,2021-01-21T13:57:29Z,MERGED,True,116,9,8,https://github.com/honnibal,Deprecate name 'Logistic' in favour of Sigmoid and sigmoid_activation layers,6,['feat / layers'],https://github.com/explosion/thinc/pull/463,https://github.com/honnibal,1,https://github.com/explosion/thinc/pull/463,"The current Logistic layer has a name that's inconsistent with the rest of the layers. You'd expect based on its name that it has a dense weight layer, like Softmax and Relu do. Instead Logistic is activation-only.
Changing the behaviour of the layer will be too breaking, and the name isn't great anyway. We should just follow Keras and call this the ""sigmoid"" activation (like it actually is in the Ops class anyway!).
I've added two layers: sigmoid_activation() does what Logistic currently does, and Sigmoid provides a linear layer in addition to the activation.","The current Logistic layer has a name that's inconsistent with the rest of the layers. You'd expect based on its name that it has a dense weight layer, like Softmax and Relu do. Instead Logistic is activation-only.
Changing the behaviour of the layer will be too breaking, and the name isn't great anyway. We should just follow Keras and call this the ""sigmoid"" activation (like it actually is in the Ops class anyway!).
I've added two layers: sigmoid_activation() does what Logistic currently does, and Sigmoid provides a linear layer in addition to the activation.",True,{}
explosion/thinc,https://github.com/explosion/thinc,463,2021-01-21T08:55:53Z,2021-01-21T10:09:38Z,2021-01-21T13:57:29Z,MERGED,True,116,9,8,https://github.com/honnibal,Deprecate name 'Logistic' in favour of Sigmoid and sigmoid_activation layers,6,['feat / layers'],https://github.com/explosion/thinc/pull/463,https://github.com/apps/codecov,2,https://github.com/explosion/thinc/pull/463#issuecomment-764490441,"The current Logistic layer has a name that's inconsistent with the rest of the layers. You'd expect based on its name that it has a dense weight layer, like Softmax and Relu do. Instead Logistic is activation-only.
Changing the behaviour of the layer will be too breaking, and the name isn't great anyway. We should just follow Keras and call this the ""sigmoid"" activation (like it actually is in the Ops class anyway!).
I've added two layers: sigmoid_activation() does what Logistic currently does, and Sigmoid provides a linear layer in addition to the activation.","Codecov Report

Merging #463 (ca42188) into master (6fbf95e) will increase coverage by 0.41%.
The diff coverage is 94.44%.


@@            Coverage Diff             @@
##           master     #463      +/-   ##
==========================================
+ Coverage   59.25%   59.66%   +0.41%     
==========================================
  Files          98      100       +2     
  Lines        7026     7079      +53     
==========================================
+ Hits         4163     4224      +61     
+ Misses       2863     2855       -8     



Impacted Files
Coverage 





thinc/layers/hashembed.py
98.14% <> ()



thinc/layers/logistic.py
100.00% <> ()



thinc/tests/layers/test_layers_api.py
0.00% <> ()



thinc/backends/ops.py
83.50% <57.14%> (+0.47%)



thinc/api.py
100.00% <100.00%> ()



thinc/layers/__init__.py
100.00% <100.00%> ()



thinc/layers/sigmoid.py
100.00% <100.00%> ()



thinc/layers/sigmoid_activation.py
100.00% <100.00%> ()



thinc/layers/add.py
100.00% <0.00%> ()



... and 3 more






Continue to review full report at Codecov.

Legend - Click here to learn more
 = absolute <relative> (impact),  = not affected, ? = missing data
Powered by Codecov. Last update 6fbf95e...ca42188. Read the comment docs.",True,{}
explosion/thinc,https://github.com/explosion/thinc,464,2021-01-22T04:41:07Z,2021-01-22T10:19:20Z,2021-02-06T00:06:44Z,MERGED,True,68,6,3,https://github.com/honnibal,Fix list2padded op,3,"['bug', 'feat / layers']",https://github.com/explosion/thinc/pull/464,https://github.com/honnibal,1,https://github.com/explosion/thinc/pull/464,"The list2padded op was doing the ""get indexes by length"" step twice, which left the sequences unsorted by length. This messes up a bunch of logic, meaning that layers wrapped with with_padded wouldn't converge (oops).
The fix comes with a better test for these transforms, to check that they don't disrupt the values when wrapping a noop layer.","The list2padded op was doing the ""get indexes by length"" step twice, which left the sequences unsorted by length. This messes up a bunch of logic, meaning that layers wrapped with with_padded wouldn't converge (oops).
The fix comes with a better test for these transforms, to check that they don't disrupt the values when wrapping a noop layer.",True,{'EYES': ['https://github.com/svlandeg']}
explosion/thinc,https://github.com/explosion/thinc,464,2021-01-22T04:41:07Z,2021-01-22T10:19:20Z,2021-02-06T00:06:44Z,MERGED,True,68,6,3,https://github.com/honnibal,Fix list2padded op,3,"['bug', 'feat / layers']",https://github.com/explosion/thinc/pull/464,https://github.com/apps/codecov,2,https://github.com/explosion/thinc/pull/464#issuecomment-765247503,"The list2padded op was doing the ""get indexes by length"" step twice, which left the sequences unsorted by length. This messes up a bunch of logic, meaning that layers wrapped with with_padded wouldn't converge (oops).
The fix comes with a better test for these transforms, to check that they don't disrupt the values when wrapping a noop layer.","Codecov Report

Merging #464 (be66c77) into master (fa80c06) will decrease coverage by 0.04%.
The diff coverage is 13.33%.


@@            Coverage Diff             @@
##           master     #464      +/-   ##
==========================================
- Coverage   59.66%   59.62%   -0.05%     
==========================================
  Files         100      100              
  Lines        7079     7118      +39     
==========================================
+ Hits         4224     4244      +20     
- Misses       2855     2874      +19     



Impacted Files
Coverage 





thinc/tests/layers/test_with_transforms.py
0.00% <0.00%> ()



thinc/backends/ops.py
83.58% <100.00%> (+0.07%)



thinc/layers/with_array.py
93.10% <0.00%> (+29.31%)





Continue to review full report at Codecov.

Legend - Click here to learn more
 = absolute <relative> (impact),  = not affected, ? = missing data
Powered by Codecov. Last update fa80c06...be66c77. Read the comment docs.",True,{}
explosion/thinc,https://github.com/explosion/thinc,465,2021-02-01T23:47:59Z,2021-02-09T12:04:12Z,2021-02-09T21:35:28Z,MERGED,True,90,18,11,https://github.com/justindujardin,fix(pathy): don't force input paths to Path when de/serializing objects,13,[],https://github.com/explosion/thinc/pull/465,https://github.com/justindujardin,1,https://github.com/explosion/thinc/pull/465,"With spaCy 3, saving models to Pathy paths has stopped working.
It turns out that thinc is forcing the input paths to be a Path class by wrapping them ""just to be sure"". It's friendly and concise, but it messes with inputs that are already path-like objects.
I added tests to demonstrate the failure, and as a fix, I only coerce the type to Path if it is a string.
Related to: justindujardin/pathy#44","With spaCy 3, saving models to Pathy paths has stopped working.
It turns out that thinc is forcing the input paths to be a Path class by wrapping them ""just to be sure"". It's friendly and concise, but it messes with inputs that are already path-like objects.
I added tests to demonstrate the failure, and as a fix, I only coerce the type to Path if it is a string.
Related to: justindujardin/pathy#44",True,{}
explosion/thinc,https://github.com/explosion/thinc,465,2021-02-01T23:47:59Z,2021-02-09T12:04:12Z,2021-02-09T21:35:28Z,MERGED,True,90,18,11,https://github.com/justindujardin,fix(pathy): don't force input paths to Path when de/serializing objects,13,[],https://github.com/explosion/thinc/pull/465,https://github.com/apps/codecov,2,https://github.com/explosion/thinc/pull/465#issuecomment-771243084,"With spaCy 3, saving models to Pathy paths has stopped working.
It turns out that thinc is forcing the input paths to be a Path class by wrapping them ""just to be sure"". It's friendly and concise, but it messes with inputs that are already path-like objects.
I added tests to demonstrate the failure, and as a fix, I only coerce the type to Path if it is a string.
Related to: justindujardin/pathy#44","Codecov Report

Merging #465 (a74c947) into master (ca42188) will decrease coverage by 0.29%.
The diff coverage is 51.31%.


@@            Coverage Diff             @@
##           master     #465      +/-   ##
==========================================
- Coverage   59.66%   59.37%   -0.30%     
==========================================
  Files         100      101       +1     
  Lines        7079     7148      +69     
==========================================
+ Hits         4224     4244      +20     
- Misses       2855     2904      +49     



Impacted Files
Coverage 





thinc/layers/logistic.py
100.00% <> ()



thinc/layers/reduce_first.py
100.00% <> ()



thinc/layers/reduce_last.py
100.00% <> ()



thinc/shims/shim.py
100.00% <> ()



thinc/tests/backends/test_ops.py
0.00% <0.00%> ()



thinc/tests/layers/test_layers_api.py
0.00% <> ()



thinc/tests/layers/test_shim.py
0.00% <0.00%> ()



thinc/tests/layers/test_with_transforms.py
0.00% <0.00%> ()



thinc/tests/model/test_model.py
0.00% <0.00%> ()



thinc/model.py
94.03% <66.66%> ()



... and 17 more






Continue to review full report at Codecov.

Legend - Click here to learn more
 = absolute <relative> (impact),  = not affected, ? = missing data
Powered by Codecov. Last update 3d5ab74...a74c947. Read the comment docs.",True,{}
explosion/thinc,https://github.com/explosion/thinc,466,2021-02-05T20:04:34Z,2021-02-17T13:12:41Z,2021-02-17T13:13:31Z,MERGED,True,1,1,1,https://github.com/svlandeg,avoid initializing with Y if X is set,1,[],https://github.com/explosion/thinc/pull/466,https://github.com/svlandeg,1,https://github.com/explosion/thinc/pull/466,"Inspired by this issue. The ""trouble"" is being caused by the shape inference code in chain, but it can't hurt to add some additional robustness into layernorm to avoid using Y if not necessary.","Inspired by this issue. The ""trouble"" is being caused by the shape inference code in chain, but it can't hurt to add some additional robustness into layernorm to avoid using Y if not necessary.",True,{}
explosion/thinc,https://github.com/explosion/thinc,466,2021-02-05T20:04:34Z,2021-02-17T13:12:41Z,2021-02-17T13:13:31Z,MERGED,True,1,1,1,https://github.com/svlandeg,avoid initializing with Y if X is set,1,[],https://github.com/explosion/thinc/pull/466,https://github.com/apps/codecov,2,https://github.com/explosion/thinc/pull/466#issuecomment-774262221,"Inspired by this issue. The ""trouble"" is being caused by the shape inference code in chain, but it can't hurt to add some additional robustness into layernorm to avoid using Y if not necessary.","Codecov Report

Merging #466 (c7cae55) into master (ca42188) will decrease coverage by 0.19%.
The diff coverage is 61.86%.


@@            Coverage Diff             @@
##           master     #466      +/-   ##
==========================================
- Coverage   59.66%   59.47%   -0.20%     
==========================================
  Files         100      101       +1     
  Lines        7079     7285     +206     
==========================================
+ Hits         4224     4333     +109     
- Misses       2855     2952      +97     



Impacted Files
Coverage 





thinc/layers/logistic.py
100.00% <> ()



thinc/layers/reduce_first.py
100.00% <> ()



thinc/layers/reduce_last.py
100.00% <> ()



thinc/tests/layers/test_layers_api.py
0.00% <> ()



thinc/tests/layers/test_with_transforms.py
0.00% <0.00%> ()



thinc/layers/layernorm.py
90.90% <50.00%> (-9.10%)



thinc/backends/ops.py
83.58% <76.92%> (+0.07%)



thinc/about.py
100.00% <100.00%> ()



thinc/api.py
100.00% <100.00%> ()



thinc/config.py
97.05% <100.00%> (+0.98%)



... and 14 more






Continue to review full report at Codecov.

Legend - Click here to learn more
 = absolute <relative> (impact),  = not affected, ? = missing data
Powered by Codecov. Last update 3d5ab74...c7cae55. Read the comment docs.",True,{}
explosion/thinc,https://github.com/explosion/thinc,466,2021-02-05T20:04:34Z,2021-02-17T13:12:41Z,2021-02-17T13:13:31Z,MERGED,True,1,1,1,https://github.com/svlandeg,avoid initializing with Y if X is set,1,[],https://github.com/explosion/thinc/pull/466,https://github.com/svlandeg,3,https://github.com/explosion/thinc/pull/466#issuecomment-774353381,"Inspired by this issue. The ""trouble"" is being caused by the shape inference code in chain, but it can't hurt to add some additional robustness into layernorm to avoid using Y if not necessary.","How is this little PR suddenly triggering mypy issues? mypy 0.8 was out already 15 days ago, and master was still building correctly 13 days ago (with the same mypy version), so that can't be it?",True,{}
explosion/thinc,https://github.com/explosion/thinc,466,2021-02-05T20:04:34Z,2021-02-17T13:12:41Z,2021-02-17T13:13:31Z,MERGED,True,1,1,1,https://github.com/svlandeg,avoid initializing with Y if X is set,1,[],https://github.com/explosion/thinc/pull/466,https://github.com/adrianeboyd,4,https://github.com/explosion/thinc/pull/466#issuecomment-774438843,"Inspired by this issue. The ""trouble"" is being caused by the shape inference code in chain, but it can't hurt to add some additional robustness into layernorm to avoid using Y if not necessary.","This is probably also related to the newest version of numpy, but I'm not sure why it looks different than in #465, which didn't fail until the test suite.",True,{}
explosion/thinc,https://github.com/explosion/thinc,466,2021-02-05T20:04:34Z,2021-02-17T13:12:41Z,2021-02-17T13:13:31Z,MERGED,True,1,1,1,https://github.com/svlandeg,avoid initializing with Y if X is set,1,[],https://github.com/explosion/thinc/pull/466,https://github.com/svlandeg,5,https://github.com/explosion/thinc/pull/466#issuecomment-774471049,"Inspired by this issue. The ""trouble"" is being caused by the shape inference code in chain, but it can't hurt to add some additional robustness into layernorm to avoid using Y if not necessary.","Hm, I guess we'll see when the other PR is merged whether it also fixes this one ",True,{}
explosion/thinc,https://github.com/explosion/thinc,466,2021-02-05T20:04:34Z,2021-02-17T13:12:41Z,2021-02-17T13:13:31Z,MERGED,True,1,1,1,https://github.com/svlandeg,avoid initializing with Y if X is set,1,[],https://github.com/explosion/thinc/pull/466,https://github.com/justindujardin,6,https://github.com/explosion/thinc/pull/466#issuecomment-779422319,"Inspired by this issue. The ""trouble"" is being caused by the shape inference code in chain, but it can't hurt to add some additional robustness into layernorm to avoid using Y if not necessary.","I'm late to this conversation, but you're right, the failures were related to the latest numpy release.
I added some # type: ignore comments in the test suite to fix them
The latest numpy release has its own type hints, so our usages in thinc don't completely line up.
At some point we could probably benefit from using the built-in numpy types in thinc, but for now a few comments did the trick. ",True,{'THUMBS_UP': ['https://github.com/svlandeg']}
explosion/thinc,https://github.com/explosion/thinc,469,2021-02-16T12:26:34Z,2021-03-03T09:45:25Z,2021-03-03T09:45:25Z,MERGED,True,33,2,1,https://github.com/adrianeboyd,Add check for contextvars ops vs. thread-local storage,1,[],https://github.com/explosion/thinc/pull/469,https://github.com/adrianeboyd,1,https://github.com/explosion/thinc/pull/469,"In order to detect cases in Jupyter notebooks where contextvars are
not preserved across cells, additionally save the ops settings in
thread-local storage as in thinc v7.
The new method contextvars_eq_thread_ops returns whether both are the
same ops type. The thread-local storage is not intended for general
use, only for this check in a restricted set of configurations.","In order to detect cases in Jupyter notebooks where contextvars are
not preserved across cells, additionally save the ops settings in
thread-local storage as in thinc v7.
The new method contextvars_eq_thread_ops returns whether both are the
same ops type. The thread-local storage is not intended for general
use, only for this check in a restricted set of configurations.",True,{}
explosion/thinc,https://github.com/explosion/thinc,469,2021-02-16T12:26:34Z,2021-03-03T09:45:25Z,2021-03-03T09:45:25Z,MERGED,True,33,2,1,https://github.com/adrianeboyd,Add check for contextvars ops vs. thread-local storage,1,[],https://github.com/explosion/thinc/pull/469,https://github.com/adrianeboyd,2,https://github.com/explosion/thinc/pull/469#issuecomment-779808315,"In order to detect cases in Jupyter notebooks where contextvars are
not preserved across cells, additionally save the ops settings in
thread-local storage as in thinc v7.
The new method contextvars_eq_thread_ops returns whether both are the
same ops type. The thread-local storage is not intended for general
use, only for this check in a restricted set of configurations.",When/how/where should thinc test for this in notebooks?,True,{}
explosion/thinc,https://github.com/explosion/thinc,469,2021-02-16T12:26:34Z,2021-03-03T09:45:25Z,2021-03-03T09:45:25Z,MERGED,True,33,2,1,https://github.com/adrianeboyd,Add check for contextvars ops vs. thread-local storage,1,[],https://github.com/explosion/thinc/pull/469,https://github.com/apps/codecov,3,https://github.com/explosion/thinc/pull/469#issuecomment-779808375,"In order to detect cases in Jupyter notebooks where contextvars are
not preserved across cells, additionally save the ops settings in
thread-local storage as in thinc v7.
The new method contextvars_eq_thread_ops returns whether both are the
same ops type. The thread-local storage is not intended for general
use, only for this check in a restricted set of configurations.","Codecov Report

Merging #469 (3a0b885) into master (816dbb1) will increase coverage by 0.03%.
The diff coverage is 75.00%.


@@            Coverage Diff             @@
##           master     #469      +/-   ##
==========================================
+ Coverage   59.37%   59.40%   +0.03%     
==========================================
  Files         101      101              
  Lines        7148     7166      +18     
==========================================
+ Hits         4244     4257      +13     
- Misses       2904     2909       +5     



Impacted Files
Coverage 





thinc/backends/__init__.py
88.67% <75.00%> (-8.47%)





Continue to review full report at Codecov.

Legend - Click here to learn more
 = absolute <relative> (impact),  = not affected, ? = missing data
Powered by Codecov. Last update 816dbb1...3a0b885. Read the comment docs.",True,{}
explosion/thinc,https://github.com/explosion/thinc,470,2021-02-16T12:37:55Z,2021-02-23T12:42:25Z,2021-02-23T14:38:50Z,MERGED,True,6,0,1,https://github.com/adrianeboyd,Reset torch tensor type in require_cpu,1,[],https://github.com/explosion/thinc/pull/470,https://github.com/adrianeboyd,1,https://github.com/explosion/thinc/pull/470,,,True,{}
explosion/thinc,https://github.com/explosion/thinc,470,2021-02-16T12:37:55Z,2021-02-23T12:42:25Z,2021-02-23T14:38:50Z,MERGED,True,6,0,1,https://github.com/adrianeboyd,Reset torch tensor type in require_cpu,1,[],https://github.com/explosion/thinc/pull/470,https://github.com/apps/codecov,2,https://github.com/explosion/thinc/pull/470#issuecomment-779813694,,"Codecov Report

Merging #470 (145c8e5) into master (816dbb1) will not change coverage.
The diff coverage is n/a.


@@           Coverage Diff           @@
##           master     #470   +/-   ##
=======================================
  Coverage   59.37%   59.37%           
=======================================
  Files         101      101           
  Lines        7148     7148           
=======================================
  Hits         4244     4244           
  Misses       2904     2904           



Impacted Files
Coverage 





thinc/util.py
92.64% <> ()





Continue to review full report at Codecov.

Legend - Click here to learn more
 = absolute <relative> (impact),  = not affected, ? = missing data
Powered by Codecov. Last update 816dbb1...145c8e5. Read the comment docs.",True,{}
explosion/thinc,https://github.com/explosion/thinc,470,2021-02-16T12:37:55Z,2021-02-23T12:42:25Z,2021-02-23T14:38:50Z,MERGED,True,6,0,1,https://github.com/adrianeboyd,Reset torch tensor type in require_cpu,1,[],https://github.com/explosion/thinc/pull/470,https://github.com/honnibal,3,https://github.com/explosion/thinc/pull/470#issuecomment-784248164,,I'll try reverting this to see if it's behind the recent failures.,True,{}
explosion/thinc,https://github.com/explosion/thinc,472,2021-02-23T11:42:04Z,2021-03-03T09:45:34Z,2021-03-03T09:45:34Z,MERGED,True,76,1,5,https://github.com/honnibal,Add map_list layer,5,[],https://github.com/explosion/thinc/pull/472,https://github.com/honnibal,1,https://github.com/explosion/thinc/pull/472,"Add a combinator layer that maps a child layer across list inputs. I happened to need this to show how to do something for spaCy, and I thought I'd added this already but it wasn't there.
I think in Thinc v7 there was a similar layer called foreach. A name with map seems better, and apparently in list it's called maplist. I hate arbitrary non-underscore names, hence map_list.","Add a combinator layer that maps a child layer across list inputs. I happened to need this to show how to do something for spaCy, and I thought I'd added this already but it wasn't there.
I think in Thinc v7 there was a similar layer called foreach. A name with map seems better, and apparently in list it's called maplist. I hate arbitrary non-underscore names, hence map_list.",True,{}
explosion/thinc,https://github.com/explosion/thinc,472,2021-02-23T11:42:04Z,2021-03-03T09:45:34Z,2021-03-03T09:45:34Z,MERGED,True,76,1,5,https://github.com/honnibal,Add map_list layer,5,[],https://github.com/explosion/thinc/pull/472,https://github.com/apps/codecov,2,https://github.com/explosion/thinc/pull/472#issuecomment-784145173,"Add a combinator layer that maps a child layer across list inputs. I happened to need this to show how to do something for spaCy, and I thought I'd added this already but it wasn't there.
I think in Thinc v7 there was a similar layer called foreach. A name with map seems better, and apparently in list it's called maplist. I hate arbitrary non-underscore names, hence map_list.","Codecov Report

Merging #472 (b81b708) into master (71d285b) will increase coverage by 0.22%.
The diff coverage is 51.21%.


@@            Coverage Diff             @@
##           master     #472      +/-   ##
==========================================
+ Coverage   59.31%   59.54%   +0.22%     
==========================================
  Files         101      102       +1     
  Lines        7148     7227      +79     
==========================================
+ Hits         4240     4303      +63     
- Misses       2908     2924      +16     



Impacted Files
Coverage 





thinc/tests/layers/test_combinators.py
0.00% <0.00%> ()



thinc/api.py
100.00% <100.00%> ()



thinc/layers/__init__.py
100.00% <100.00%> ()



thinc/layers/map_list.py
100.00% <100.00%> ()



thinc/util.py
96.00% <0.00%> (+3.35%)





Continue to review full report at Codecov.

Legend - Click here to learn more
 = absolute <relative> (impact),  = not affected, ? = missing data
Powered by Codecov. Last update 71d285b...b81b708. Read the comment docs.",True,{}
explosion/thinc,https://github.com/explosion/thinc,473,2021-02-23T14:38:58Z,2021-03-01T15:42:07Z,2021-03-01T15:42:07Z,CLOSED,False,4,9,2,https://github.com/honnibal,"Revert ""Reset torch tensor type in require_cpu""",2,[],https://github.com/explosion/thinc/pull/473,https://github.com/honnibal,1,https://github.com/explosion/thinc/pull/473,Reverts #470,Reverts #470,True,{}
explosion/thinc,https://github.com/explosion/thinc,473,2021-02-23T14:38:58Z,2021-03-01T15:42:07Z,2021-03-01T15:42:07Z,CLOSED,False,4,9,2,https://github.com/honnibal,"Revert ""Reset torch tensor type in require_cpu""",2,[],https://github.com/explosion/thinc/pull/473,https://github.com/honnibal,2,https://github.com/explosion/thinc/pull/473#issuecomment-784248416,Reverts #470,We're getting failures so I just want to see if this passes,True,{}
explosion/thinc,https://github.com/explosion/thinc,473,2021-02-23T14:38:58Z,2021-03-01T15:42:07Z,2021-03-01T15:42:07Z,CLOSED,False,4,9,2,https://github.com/honnibal,"Revert ""Reset torch tensor type in require_cpu""",2,[],https://github.com/explosion/thinc/pull/473,https://github.com/apps/codecov,3,https://github.com/explosion/thinc/pull/473#issuecomment-784253228,Reverts #470,"Codecov Report

Merging #473 (df91860) into master (a69d05e) will not change coverage.
The diff coverage is n/a.


@@           Coverage Diff           @@
##           master     #473   +/-   ##
=======================================
  Coverage   59.31%   59.31%           
=======================================
  Files         101      101           
  Lines        7148     7148           
=======================================
  Hits         4240     4240           
  Misses       2908     2908           



Impacted Files
Coverage 





thinc/util.py
92.64% <> ()





Continue to review full report at Codecov.

Legend - Click here to learn more
 = absolute <relative> (impact),  = not affected, ? = missing data
Powered by Codecov. Last update a69d05e...ebf8b8a. Read the comment docs.",True,{}
explosion/thinc,https://github.com/explosion/thinc,473,2021-02-23T14:38:58Z,2021-03-01T15:42:07Z,2021-03-01T15:42:07Z,CLOSED,False,4,9,2,https://github.com/honnibal,"Revert ""Reset torch tensor type in require_cpu""",2,[],https://github.com/explosion/thinc/pull/473,https://github.com/adrianeboyd,4,https://github.com/explosion/thinc/pull/473#issuecomment-788045762,Reverts #470,"I don't think this is the culprit, so closing for now.",True,{}
explosion/thinc,https://github.com/explosion/thinc,476,2021-03-01T09:07:44Z,2021-03-01T11:53:17Z,2021-03-01T11:53:17Z,MERGED,True,1,4,1,https://github.com/adrianeboyd,Disable code cov in CI,1,[],https://github.com/explosion/thinc/pull/476,https://github.com/adrianeboyd,1,https://github.com/explosion/thinc/pull/476,"Seems to currently cause segfaults for python 3.9 in linux in the CI, although no idea what the underlying cause might be.","Seems to currently cause segfaults for python 3.9 in linux in the CI, although no idea what the underlying cause might be.",True,{}
explosion/thinc,https://github.com/explosion/thinc,477,2021-03-02T09:59:08Z,2021-03-02T11:26:54Z,2021-03-02T11:26:57Z,MERGED,True,29,0,1,https://github.com/svlandeg,Bot for resolved issues,1,['enhancement'],https://github.com/explosion/thinc/pull/477,https://github.com/svlandeg,1,https://github.com/explosion/thinc/pull/477,Adding @tiangolo's issue manager for 'resolved' issues,Adding @tiangolo's issue manager for 'resolved' issues,True,{}
explosion/thinc,https://github.com/explosion/thinc,478,2021-03-02T11:16:02Z,2021-03-02T14:14:05Z,2021-03-02T14:37:17Z,MERGED,True,11,11,2,https://github.com/svlandeg,sync pins with spacy,2,['install'],https://github.com/explosion/thinc/pull/478,https://github.com/svlandeg,1,https://github.com/explosion/thinc/pull/478,"sync pins with spacy - cf explosion/spaCy#7247
Triggered by the fact that ml_datasets still referred to a specific alpha version.","sync pins with spacy - cf explosion/spaCy#7247
Triggered by the fact that ml_datasets still referred to a specific alpha version.",True,{}
explosion/thinc,https://github.com/explosion/thinc,479,2021-03-03T09:47:45Z,2021-08-16T10:00:55Z,2021-08-16T10:00:55Z,CLOSED,False,29,26,2,https://github.com/adrianeboyd,WIP: python 3.9 CI segfaults,11,[],https://github.com/explosion/thinc/pull/479,https://github.com/adrianeboyd,1,https://github.com/explosion/thinc/pull/479,,,True,{}
explosion/thinc,https://github.com/explosion/thinc,479,2021-03-03T09:47:45Z,2021-08-16T10:00:55Z,2021-08-16T10:00:55Z,CLOSED,False,29,26,2,https://github.com/adrianeboyd,WIP: python 3.9 CI segfaults,11,[],https://github.com/explosion/thinc/pull/479,https://github.com/apps/codecov,2,https://github.com/explosion/thinc/pull/479#issuecomment-789611617,,"Codecov Report

Merging #479 (eb4df78) into master (a69d05e) will decrease coverage by 2.77%.
The diff coverage is 59.67%.


@@            Coverage Diff             @@
##           master     #479      +/-   ##
==========================================
- Coverage   59.31%   56.54%   -2.78%     
==========================================
  Files         101      102       +1     
  Lines        7148     7200      +52     
==========================================
- Hits         4240     4071     -169     
- Misses       2908     3129     +221     



Impacted Files
Coverage 





thinc/tests/layers/test_combinators.py
0.00% <0.00%> ()



thinc/backends/__init__.py
88.67% <75.00%> (-8.47%)



thinc/about.py
100.00% <100.00%> ()



thinc/api.py
100.00% <100.00%> ()



thinc/layers/__init__.py
100.00% <100.00%> ()



thinc/layers/map_list.py
100.00% <100.00%> ()



thinc/layers/tensorflowwrapper.py
20.87% <0.00%> (-78.03%)



thinc/shims/tensorflow.py
20.32% <0.00%> (-69.24%)



thinc/schedules.py
100.00% <0.00%> ()





Continue to review full report at Codecov.

Legend - Click here to learn more
 = absolute <relative> (impact),  = not affected, ? = missing data
Powered by Codecov. Last update 4f2723e...eb4df78. Read the comment docs.",True,{}
explosion/thinc,https://github.com/explosion/thinc,479,2021-03-03T09:47:45Z,2021-08-16T10:00:55Z,2021-08-16T10:00:55Z,CLOSED,False,29,26,2,https://github.com/adrianeboyd,WIP: python 3.9 CI segfaults,11,[],https://github.com/explosion/thinc/pull/479,https://github.com/adrianeboyd,3,https://github.com/explosion/thinc/pull/479#issuecomment-789693467,,**throws hands up in the air**,True,{}
explosion/thinc,https://github.com/explosion/thinc,480,2021-03-03T12:55:06Z,2021-03-06T09:18:58Z,2021-03-06T09:18:58Z,CLOSED,False,29,25,2,https://github.com/adrianeboyd,"Revert ""Disable code cov in CI (#476)""",6,[],https://github.com/explosion/thinc/pull/480,https://github.com/adrianeboyd,1,https://github.com/explosion/thinc/pull/480,This reverts commit fda5e5f.,This reverts commit fda5e5f.,True,{}
explosion/thinc,https://github.com/explosion/thinc,480,2021-03-03T12:55:06Z,2021-03-06T09:18:58Z,2021-03-06T09:18:58Z,CLOSED,False,29,25,2,https://github.com/adrianeboyd,"Revert ""Disable code cov in CI (#476)""",6,[],https://github.com/explosion/thinc/pull/480,https://github.com/apps/codecov,2,https://github.com/explosion/thinc/pull/480#issuecomment-789704524,This reverts commit fda5e5f.,"Codecov Report

Merging #480 (d463aef) into master (a69d05e) will decrease coverage by 2.77%.
The diff coverage is 59.01%.


@@            Coverage Diff             @@
##           master     #480      +/-   ##
==========================================
- Coverage   59.31%   56.54%   -2.78%     
==========================================
  Files         101      102       +1     
  Lines        7148     7200      +52     
==========================================
- Hits         4240     4071     -169     
- Misses       2908     3129     +221     



Impacted Files
Coverage 





thinc/tests/layers/test_combinators.py
0.00% <0.00%> ()



thinc/backends/__init__.py
88.67% <75.00%> (-8.47%)



thinc/api.py
100.00% <100.00%> ()



thinc/layers/__init__.py
100.00% <100.00%> ()



thinc/layers/map_list.py
100.00% <100.00%> ()



thinc/layers/tensorflowwrapper.py
20.87% <0.00%> (-78.03%)



thinc/shims/tensorflow.py
20.32% <0.00%> (-69.24%)



thinc/schedules.py
100.00% <0.00%> ()





Continue to review full report at Codecov.

Legend - Click here to learn more
 = absolute <relative> (impact),  = not affected, ? = missing data
Powered by Codecov. Last update 4f2723e...d463aef. Read the comment docs.",True,{}
explosion/thinc,https://github.com/explosion/thinc,480,2021-03-03T12:55:06Z,2021-03-06T09:18:58Z,2021-03-06T09:18:58Z,CLOSED,False,29,25,2,https://github.com/adrianeboyd,"Revert ""Disable code cov in CI (#476)""",6,[],https://github.com/explosion/thinc/pull/480,https://github.com/adrianeboyd,3,https://github.com/explosion/thinc/pull/480#issuecomment-789716917,This reverts commit fda5e5f.,**shakes fist at the sky**,True,{}
explosion/thinc,https://github.com/explosion/thinc,480,2021-03-03T12:55:06Z,2021-03-06T09:18:58Z,2021-03-06T09:18:58Z,CLOSED,False,29,25,2,https://github.com/adrianeboyd,"Revert ""Disable code cov in CI (#476)""",6,[],https://github.com/explosion/thinc/pull/480,https://github.com/adrianeboyd,4,https://github.com/explosion/thinc/pull/480#issuecomment-789831684,This reverts commit fda5e5f.,I guess I can try to see if I can reproduce on a local build agent so it's easier to debug?,True,{}
explosion/thinc,https://github.com/explosion/thinc,481,2021-03-04T12:45:21Z,2022-04-04T08:47:01Z,2022-04-04T08:47:01Z,CLOSED,False,113,73,2,https://github.com/adrianeboyd,WIP: Test on self-hosted azure agent with GPU,8,"['tests', ' wip']",https://github.com/explosion/thinc/pull/481,https://github.com/adrianeboyd,1,https://github.com/explosion/thinc/pull/481,,,True,{}
explosion/thinc,https://github.com/explosion/thinc,483,2021-03-06T09:18:36Z,2021-03-06T09:56:55Z,2021-03-06T09:56:55Z,MERGED,True,4,1,1,https://github.com/adrianeboyd,"Revert ""Disable code cov in CI (#476)""",1,[],https://github.com/explosion/thinc/pull/483,https://github.com/adrianeboyd,1,https://github.com/explosion/thinc/pull/483,This reverts commit fda5e5f.,This reverts commit fda5e5f.,True,{}
explosion/thinc,https://github.com/explosion/thinc,483,2021-03-06T09:18:36Z,2021-03-06T09:56:55Z,2021-03-06T09:56:55Z,MERGED,True,4,1,1,https://github.com/adrianeboyd,"Revert ""Disable code cov in CI (#476)""",1,[],https://github.com/explosion/thinc/pull/483,https://github.com/apps/codecov,2,https://github.com/explosion/thinc/pull/483#issuecomment-791901821,This reverts commit fda5e5f.,"Codecov Report

Merging #483 (5716726) into master (a69d05e) will decrease coverage by 0.00%.
The diff coverage is 59.01%.


@@            Coverage Diff             @@
##           master     #483      +/-   ##
==========================================
- Coverage   59.31%   59.31%   -0.01%     
==========================================
  Files         101      102       +1     
  Lines        7148     7206      +58     
==========================================
+ Hits         4240     4274      +34     
- Misses       2908     2932      +24     



Impacted Files
Coverage 





thinc/tests/layers/test_combinators.py
0.00% <0.00%> ()



thinc/backends/__init__.py
88.67% <75.00%> (-8.47%)



thinc/api.py
100.00% <100.00%> ()



thinc/layers/__init__.py
100.00% <100.00%> ()



thinc/layers/map_list.py
100.00% <100.00%> ()





Continue to review full report at Codecov.

Legend - Click here to learn more
 = absolute <relative> (impact),  = not affected, ? = missing data
Powered by Codecov. Last update 4f2723e...5716726. Read the comment docs.",True,{}
explosion/thinc,https://github.com/explosion/thinc,484,2021-03-06T22:41:54Z,2021-03-09T12:00:13Z,2021-03-09T12:06:16Z,MERGED,True,28,27,2,https://github.com/svlandeg,ensure consistency of nO dim for biLSTM,2,"['bug', 'feat / layers']",https://github.com/explosion/thinc/pull/484,https://github.com/svlandeg,1,https://github.com/explosion/thinc/pull/484,"Fixes explosion/spaCy#7088
When using a biLSTM, the given nO dimension of the Thinc model would be divided by half and stored as such. I think this behaviour is unexpected, and it clashes with some of the shape inference & listener code in spaCy.
I think it makes more sense to keep the given nO, but provide the Torch model with half the value for its hidden features, so that the final output matches the required nO dimension.
This PR changes the behaviour both for PyTorchLSTM and LSTM (if bi is True) - both would otherwise predict vectors that were incompatible with their internal nO value - cf test and spaCy issue linked above.
The additional line in the test makes CauchySimilarity error as well. Not sure (yet) whether this is due to an error in get_width or the model itself, or whether this model should just be excluded from this particular line in the test. Suggest to address this in a follow-up PR.","Fixes explosion/spaCy#7088
When using a biLSTM, the given nO dimension of the Thinc model would be divided by half and stored as such. I think this behaviour is unexpected, and it clashes with some of the shape inference & listener code in spaCy.
I think it makes more sense to keep the given nO, but provide the Torch model with half the value for its hidden features, so that the final output matches the required nO dimension.
This PR changes the behaviour both for PyTorchLSTM and LSTM (if bi is True) - both would otherwise predict vectors that were incompatible with their internal nO value - cf test and spaCy issue linked above.
The additional line in the test makes CauchySimilarity error as well. Not sure (yet) whether this is due to an error in get_width or the model itself, or whether this model should just be excluded from this particular line in the test. Suggest to address this in a follow-up PR.",True,{}
explosion/thinc,https://github.com/explosion/thinc,484,2021-03-06T22:41:54Z,2021-03-09T12:00:13Z,2021-03-09T12:06:16Z,MERGED,True,28,27,2,https://github.com/svlandeg,ensure consistency of nO dim for biLSTM,2,"['bug', 'feat / layers']",https://github.com/explosion/thinc/pull/484,https://github.com/apps/codecov,2,https://github.com/explosion/thinc/pull/484#issuecomment-792097550,"Fixes explosion/spaCy#7088
When using a biLSTM, the given nO dimension of the Thinc model would be divided by half and stored as such. I think this behaviour is unexpected, and it clashes with some of the shape inference & listener code in spaCy.
I think it makes more sense to keep the given nO, but provide the Torch model with half the value for its hidden features, so that the final output matches the required nO dimension.
This PR changes the behaviour both for PyTorchLSTM and LSTM (if bi is True) - both would otherwise predict vectors that were incompatible with their internal nO value - cf test and spaCy issue linked above.
The additional line in the test makes CauchySimilarity error as well. Not sure (yet) whether this is due to an error in get_width or the model itself, or whether this model should just be excluded from this particular line in the test. Suggest to address this in a follow-up PR.","Codecov Report

Merging #484 (a35ad58) into master (1d998ad) will decrease coverage by 0.36%.
The diff coverage is 88.46%.


@@            Coverage Diff             @@
##           master     #484      +/-   ##
==========================================
- Coverage   59.31%   58.94%   -0.37%     
==========================================
  Files         102      102              
  Lines        7206     7207       +1     
==========================================
- Hits         4274     4248      -26     
- Misses       2932     2959      +27     



Impacted Files
Coverage 





thinc/tests/layers/test_layers_api.py
0.00% <0.00%> ()



thinc/layers/lstm.py
98.18% <100.00%> (-0.02%)



thinc/layers/cauchysimilarity.py
32.43% <0.00%> (-67.57%)





Continue to review full report at Codecov.

Legend - Click here to learn more
 = absolute <relative> (impact),  = not affected, ? = missing data
Powered by Codecov. Last update 1d998ad...a35ad58. Read the comment docs.",True,{}
explosion/thinc,https://github.com/explosion/thinc,484,2021-03-06T22:41:54Z,2021-03-09T12:00:13Z,2021-03-09T12:06:16Z,MERGED,True,28,27,2,https://github.com/svlandeg,ensure consistency of nO dim for biLSTM,2,"['bug', 'feat / layers']",https://github.com/explosion/thinc/pull/484,https://github.com/honnibal,3,https://github.com/explosion/thinc/pull/484#issuecomment-793784103,"Fixes explosion/spaCy#7088
When using a biLSTM, the given nO dimension of the Thinc model would be divided by half and stored as such. I think this behaviour is unexpected, and it clashes with some of the shape inference & listener code in spaCy.
I think it makes more sense to keep the given nO, but provide the Torch model with half the value for its hidden features, so that the final output matches the required nO dimension.
This PR changes the behaviour both for PyTorchLSTM and LSTM (if bi is True) - both would otherwise predict vectors that were incompatible with their internal nO value - cf test and spaCy issue linked above.
The additional line in the test makes CauchySimilarity error as well. Not sure (yet) whether this is due to an error in get_width or the model itself, or whether this model should just be excluded from this particular line in the test. Suggest to address this in a follow-up PR.","Yeah this is sensible, I agree that a bunch of places expect nO to behave a certain way.",True,{}
explosion/thinc,https://github.com/explosion/thinc,485,2021-03-08T10:31:49Z,2021-03-08T10:59:01Z,2021-03-08T10:59:01Z,MERGED,True,1,1,1,https://github.com/adrianeboyd,Set version to v8.0.2,1,[],https://github.com/explosion/thinc/pull/485,https://github.com/adrianeboyd,1,https://github.com/explosion/thinc/pull/485,Let's try this again...,Let's try this again...,True,{}
explosion/thinc,https://github.com/explosion/thinc,485,2021-03-08T10:31:49Z,2021-03-08T10:59:01Z,2021-03-08T10:59:01Z,MERGED,True,1,1,1,https://github.com/adrianeboyd,Set version to v8.0.2,1,[],https://github.com/explosion/thinc/pull/485,https://github.com/apps/codecov,2,https://github.com/explosion/thinc/pull/485#issuecomment-792662381,Let's try this again...,"Codecov Report

Merging #485 (30d5771) into master (1d998ad) will not change coverage.
The diff coverage is 100.00%.


@@           Coverage Diff           @@
##           master     #485   +/-   ##
=======================================
  Coverage   59.31%   59.31%           
=======================================
  Files         102      102           
  Lines        7206     7206           
=======================================
  Hits         4274     4274           
  Misses       2932     2932           



Impacted Files
Coverage 





thinc/about.py
100.00% <100.00%> ()





Continue to review full report at Codecov.

Legend - Click here to learn more
 = absolute <relative> (impact),  = not affected, ? = missing data
Powered by Codecov. Last update 1d998ad...30d5771. Read the comment docs.",True,{}
explosion/thinc,https://github.com/explosion/thinc,486,2021-03-30T22:43:57Z,2021-04-19T08:31:41Z,2021-04-19T08:39:05Z,MERGED,True,6,2,2,https://github.com/svlandeg,Fix expand_window,2,"['bug', 'feat / layers']",https://github.com/explosion/thinc/pull/486,https://github.com/svlandeg,1,https://github.com/explosion/thinc/pull/486,"I think there was a bug in expand_window when it was run on empty input. In this case, X was just returned as is, with shape (0, x). But surrounding layers would be expecting a different shape:  (0, x*((nW*2)+1).
This bug only became apparent when running test_empty_doc in spaCy on GPU - cf explosion/spaCy#7293 (it seems like numpy is more forgiving?).
By adding a tile method in Ops, the correct shape is obtained to avoid downstream layers crashing.","I think there was a bug in expand_window when it was run on empty input. In this case, X was just returned as is, with shape (0, x). But surrounding layers would be expecting a different shape:  (0, x*((nW*2)+1).
This bug only became apparent when running test_empty_doc in spaCy on GPU - cf explosion/spaCy#7293 (it seems like numpy is more forgiving?).
By adding a tile method in Ops, the correct shape is obtained to avoid downstream layers crashing.",True,{}
explosion/thinc,https://github.com/explosion/thinc,486,2021-03-30T22:43:57Z,2021-04-19T08:31:41Z,2021-04-19T08:39:05Z,MERGED,True,6,2,2,https://github.com/svlandeg,Fix expand_window,2,"['bug', 'feat / layers']",https://github.com/explosion/thinc/pull/486,https://github.com/apps/codecov,2,https://github.com/explosion/thinc/pull/486#issuecomment-810632028,"I think there was a bug in expand_window when it was run on empty input. In this case, X was just returned as is, with shape (0, x). But surrounding layers would be expecting a different shape:  (0, x*((nW*2)+1).
This bug only became apparent when running test_empty_doc in spaCy on GPU - cf explosion/spaCy#7293 (it seems like numpy is more forgiving?).
By adding a tile method in Ops, the correct shape is obtained to avoid downstream layers crashing.","Codecov Report

Merging #486 (8fd45c8) into master (f92bb9c) will decrease coverage by 0.01%.
The diff coverage is 25.00%.


@@            Coverage Diff             @@
##           master     #486      +/-   ##
==========================================
- Coverage   58.94%   58.92%   -0.02%     
==========================================
  Files         102      102              
  Lines        7207     7209       +2     
==========================================
  Hits         4248     4248              
- Misses       2959     2961       +2     



Impacted Files
Coverage 





thinc/layers/expand_window.py
88.88% <0.00%> (-5.56%)



thinc/backends/ops.py
83.48% <50.00%> (-0.10%)





Continue to review full report at Codecov.

Legend - Click here to learn more
 = absolute <relative> (impact),  = not affected, ? = missing data
Powered by Codecov. Last update f92bb9c...8fd45c8. Read the comment docs.",True,{}
explosion/thinc,https://github.com/explosion/thinc,487,2021-04-13T11:23:42Z,2021-04-19T09:37:32Z,2021-04-19T09:37:32Z,MERGED,True,2,2,2,https://github.com/adrianeboyd,Set catalogue lower pin to v2.0.3,2,[],https://github.com/explosion/thinc/pull/487,https://github.com/adrianeboyd,1,https://github.com/explosion/thinc/pull/487,,,True,{}
explosion/thinc,https://github.com/explosion/thinc,487,2021-04-13T11:23:42Z,2021-04-19T09:37:32Z,2021-04-19T09:37:32Z,MERGED,True,2,2,2,https://github.com/adrianeboyd,Set catalogue lower pin to v2.0.3,2,[],https://github.com/explosion/thinc/pull/487,https://github.com/apps/codecov,2,https://github.com/explosion/thinc/pull/487#issuecomment-818759101,,"Codecov Report

Merging #487 (08240e0) into master (f92bb9c) will not change coverage.
The diff coverage is n/a.


@@           Coverage Diff           @@
##           master     #487   +/-   ##
=======================================
  Coverage   58.94%   58.94%           
=======================================
  Files         102      102           
  Lines        7207     7207           
=======================================
  Hits         4248     4248           
  Misses       2959     2959           

Continue to review full report at Codecov.

Legend - Click here to learn more
 = absolute <relative> (impact),  = not affected, ? = missing data
Powered by Codecov. Last update f92bb9c...08240e0. Read the comment docs.",True,{}
explosion/thinc,https://github.com/explosion/thinc,488,2021-04-15T11:30:53Z,2021-04-19T09:29:42Z,2021-04-19T09:36:17Z,MERGED,True,75,1,3,https://github.com/svlandeg,Fix config override & interpolate interaction,5,"['bug', 'feat / config']",https://github.com/explosion/thinc/pull/488,https://github.com/svlandeg,1,https://github.com/explosion/thinc/pull/488,"Before this PR, the config.from_str method would interpolate before setting the overrides. This meant that a string such as ""hello ${vars.a}"" would get interpolated first - i.e. changing vars.a with the value from the config, and only afterwards would this value be changed again into the correct one from overrides.
This mechanism resulted in unnecessary quotes when strings were being inserted into other strings, as examplified by the unit tests. Before this PR, most of the test_config_overrides configurations would fail (especially the ones more down the bottom).
Instead, this PR ensures that interpolation only happens after setting the overrides, circumventing the quoting behaviour.
While that fixes most of the tests, one additional issue couldn't be resolved: if you have an integer value in quotes like ""42"", internally this will be parsed as an integer first, then converted into a backslashed version to ensure the value remains a string, and all this does not play well when that string is again inserted in a larger string like ""hello ${vars.a}"". Long story short - this can be resolved by just putting 42 instead of ""42"" and I've probably already spent too much time on this to keep digging here. I kept the unit tests on xfail in case someone else wants to have a stab at it though :-)
Releasing this, including the version bump to 8.0.3, should make explosion/spaCy#7755 go green.","Before this PR, the config.from_str method would interpolate before setting the overrides. This meant that a string such as ""hello ${vars.a}"" would get interpolated first - i.e. changing vars.a with the value from the config, and only afterwards would this value be changed again into the correct one from overrides.
This mechanism resulted in unnecessary quotes when strings were being inserted into other strings, as examplified by the unit tests. Before this PR, most of the test_config_overrides configurations would fail (especially the ones more down the bottom).
Instead, this PR ensures that interpolation only happens after setting the overrides, circumventing the quoting behaviour.
While that fixes most of the tests, one additional issue couldn't be resolved: if you have an integer value in quotes like ""42"", internally this will be parsed as an integer first, then converted into a backslashed version to ensure the value remains a string, and all this does not play well when that string is again inserted in a larger string like ""hello ${vars.a}"". Long story short - this can be resolved by just putting 42 instead of ""42"" and I've probably already spent too much time on this to keep digging here. I kept the unit tests on xfail in case someone else wants to have a stab at it though :-)
Releasing this, including the version bump to 8.0.3, should make explosion/spaCy#7755 go green.",True,{'THUMBS_UP': ['https://github.com/ines']}
explosion/thinc,https://github.com/explosion/thinc,488,2021-04-15T11:30:53Z,2021-04-19T09:29:42Z,2021-04-19T09:36:17Z,MERGED,True,75,1,3,https://github.com/svlandeg,Fix config override & interpolate interaction,5,"['bug', 'feat / config']",https://github.com/explosion/thinc/pull/488,https://github.com/apps/codecov,2,https://github.com/explosion/thinc/pull/488#issuecomment-820355817,"Before this PR, the config.from_str method would interpolate before setting the overrides. This meant that a string such as ""hello ${vars.a}"" would get interpolated first - i.e. changing vars.a with the value from the config, and only afterwards would this value be changed again into the correct one from overrides.
This mechanism resulted in unnecessary quotes when strings were being inserted into other strings, as examplified by the unit tests. Before this PR, most of the test_config_overrides configurations would fail (especially the ones more down the bottom).
Instead, this PR ensures that interpolation only happens after setting the overrides, circumventing the quoting behaviour.
While that fixes most of the tests, one additional issue couldn't be resolved: if you have an integer value in quotes like ""42"", internally this will be parsed as an integer first, then converted into a backslashed version to ensure the value remains a string, and all this does not play well when that string is again inserted in a larger string like ""hello ${vars.a}"". Long story short - this can be resolved by just putting 42 instead of ""42"" and I've probably already spent too much time on this to keep digging here. I kept the unit tests on xfail in case someone else wants to have a stab at it though :-)
Releasing this, including the version bump to 8.0.3, should make explosion/spaCy#7755 go green.","Codecov Report

Merging #488 (c5e0dbc) into master (f92bb9c) will increase coverage by 0.02%.
The diff coverage is 100.00%.


@@            Coverage Diff             @@
##           master     #488      +/-   ##
==========================================
+ Coverage   58.94%   58.96%   +0.02%     
==========================================
  Files         102      102              
  Lines        7207     7211       +4     
==========================================
+ Hits         4248     4252       +4     
  Misses       2959     2959              



Impacted Files
Coverage 





thinc/about.py
100.00% <100.00%> ()



thinc/config.py
96.07% <100.00%> (+0.02%)





Continue to review full report at Codecov.

Legend - Click here to learn more
 = absolute <relative> (impact),  = not affected, ? = missing data
Powered by Codecov. Last update f92bb9c...c5e0dbc. Read the comment docs.",True,{}
explosion/thinc,https://github.com/explosion/thinc,489,2021-04-19T12:28:42Z,2021-04-20T08:27:03Z,2021-04-20T08:27:06Z,MERGED,True,1,26,3,https://github.com/svlandeg,Remove codevoc from CI,6,[],https://github.com/explosion/thinc/pull/489,https://github.com/svlandeg,1,https://github.com/explosion/thinc/pull/489,Remove codevoc from CI,Remove codevoc from CI,True,{}
explosion/thinc,https://github.com/explosion/thinc,490,2021-04-21T09:57:20Z,2021-05-27T08:02:38Z,2021-05-27T08:02:38Z,MERGED,True,133,1,5,https://github.com/polm,Add tuplify,4,"['enhancement', 'feat / layers']",https://github.com/explosion/thinc/pull/490,https://github.com/polm,1,https://github.com/explosion/thinc/pull/490,"Tuplify replicates the input so that it can be modified separately by
downstream layers.
This needs tests and probably other metadata settings added still.","Tuplify replicates the input so that it can be modified separately by
downstream layers.
This needs tests and probably other metadata settings added still.",True,{}
explosion/thinc,https://github.com/explosion/thinc,490,2021-04-21T09:57:20Z,2021-05-27T08:02:38Z,2021-05-27T08:02:38Z,MERGED,True,133,1,5,https://github.com/polm,Add tuplify,4,"['enhancement', 'feat / layers']",https://github.com/explosion/thinc/pull/490,https://github.com/polm,2,https://github.com/explosion/thinc/pull/490#issuecomment-843743493,"Tuplify replicates the input so that it can be modified separately by
downstream layers.
This needs tests and probably other metadata settings added still.","So working with this more there are actually a few problems that should be fixed before it's merged.

There needs to be init handling for sublayers.
If the input is a generator that has to be handled appropriately.",True,{}
explosion/thinc,https://github.com/explosion/thinc,490,2021-04-21T09:57:20Z,2021-05-27T08:02:38Z,2021-05-27T08:02:38Z,MERGED,True,133,1,5,https://github.com/polm,Add tuplify,4,"['enhancement', 'feat / layers']",https://github.com/explosion/thinc/pull/490,https://github.com/honnibal,3,https://github.com/explosion/thinc/pull/490#issuecomment-846840164,"Tuplify replicates the input so that it can be modified separately by
downstream layers.
This needs tests and probably other metadata settings added still.","Regarding 2, I don't think we need to support inputs as generators. Our own batching utilities will always be producing a list or a tuple or an array, and I don't think it's reasonable for people to expect a generator as a batch to work properly.",True,{'THUMBS_UP': ['https://github.com/polm']}
explosion/thinc,https://github.com/explosion/thinc,491,2021-04-21T11:57:49Z,2021-05-31T08:21:47Z,2021-05-31T09:30:59Z,MERGED,True,165,47,7,https://github.com/svlandeg,Resizable container & force set_dim,16,['enhancement'],https://github.com/explosion/thinc/pull/491,https://github.com/svlandeg,1,https://github.com/explosion/thinc/pull/491,"Introduce resizable container layer with a resize method that can be used in model.attrs[""resize_output""] that is checked in spaCy's pipe.is_resizable method. See also explosion/spaCy#7862
Add argument force that is False by default to allow overriding an existing dim value with model.set_dim() (to be used with caution ;-))","Introduce resizable container layer with a resize method that can be used in model.attrs[""resize_output""] that is checked in spaCy's pipe.is_resizable method. See also explosion/spaCy#7862
Add argument force that is False by default to allow overriding an existing dim value with model.set_dim() (to be used with caution ;-))",True,{}
explosion/thinc,https://github.com/explosion/thinc,491,2021-04-21T11:57:49Z,2021-05-31T08:21:47Z,2021-05-31T09:30:59Z,MERGED,True,165,47,7,https://github.com/svlandeg,Resizable container & force set_dim,16,['enhancement'],https://github.com/explosion/thinc/pull/491,https://github.com/honnibal,2,https://github.com/explosion/thinc/pull/491#issuecomment-846843199,"Introduce resizable container layer with a resize method that can be used in model.attrs[""resize_output""] that is checked in spaCy's pipe.is_resizable method. See also explosion/spaCy#7862
Add argument force that is False by default to allow overriding an existing dim value with model.set_dim() (to be used with caution ;-))","Do we need the force flag on set_dim still? If we don't need it for this feature I wouldn't add it, as it does cause problems if parameters are already set.",True,{}
explosion/thinc,https://github.com/explosion/thinc,491,2021-04-21T11:57:49Z,2021-05-31T08:21:47Z,2021-05-31T09:30:59Z,MERGED,True,165,47,7,https://github.com/svlandeg,Resizable container & force set_dim,16,['enhancement'],https://github.com/explosion/thinc/pull/491,https://github.com/svlandeg,3,https://github.com/explosion/thinc/pull/491#issuecomment-847325023,"Introduce resizable container layer with a resize method that can be used in model.attrs[""resize_output""] that is checked in spaCy's pipe.is_resizable method. See also explosion/spaCy#7862
Add argument force that is False by default to allow overriding an existing dim value with model.set_dim() (to be used with caution ;-))","Yea I think it's still needed in the ""container"" layer to allow for proper dimension inference...
[EDIT: made it so that force only works when no parameters are set]",True,{}
explosion/thinc,https://github.com/explosion/thinc,492,2021-04-26T09:15:39Z,2021-05-06T08:24:34Z,2021-05-06T08:24:34Z,MERGED,True,1,1,1,https://github.com/polm,Fix backprop in with_getitem,2,"['bug', 'feat / layers']",https://github.com/explosion/thinc/pull/492,https://github.com/polm,1,https://github.com/explosion/thinc/pull/492,The return value in backprop is pulling from the forward input rather than the backprop input.,The return value in backprop is pulling from the forward input rather than the backprop input.,True,{}
explosion/thinc,https://github.com/explosion/thinc,492,2021-04-26T09:15:39Z,2021-05-06T08:24:34Z,2021-05-06T08:24:34Z,MERGED,True,1,1,1,https://github.com/polm,Fix backprop in with_getitem,2,"['bug', 'feat / layers']",https://github.com/explosion/thinc/pull/492,https://github.com/apps/azure-pipelines,2,https://github.com/explosion/thinc/pull/492#issuecomment-829101417,The return value in backprop is pulling from the forward input rather than the backprop input.,There was an error handling pipeline event 8f87ca8a-3ea0-47b7-ab69-4b76511b9703.,True,{'EYES': ['https://github.com/svlandeg']}
explosion/thinc,https://github.com/explosion/thinc,493,2021-04-26T13:23:21Z,2021-04-29T09:28:58Z,2021-04-29T09:30:02Z,MERGED,True,2,1,1,https://github.com/svlandeg,CI failing with pip 21.1,3,"['bug', 'tests']",https://github.com/explosion/thinc/pull/493,https://github.com/svlandeg,1,https://github.com/explosion/thinc/pull/493,"It turns out that the recently released pip 21.1 makes our CI fail on Windows. The pip uninstall command runs into permission issues with shutil, stopping the CI early.
Pinning pip to 21.0 on the CI, as in this PR, resolves the failures, but this solution is obviously not great for longer term. Still, we might consider patching this for now so we can get on with Thinc PRs.","It turns out that the recently released pip 21.1 makes our CI fail on Windows. The pip uninstall command runs into permission issues with shutil, stopping the CI early.
Pinning pip to 21.0 on the CI, as in this PR, resolves the failures, but this solution is obviously not great for longer term. Still, we might consider patching this for now so we can get on with Thinc PRs.",True,{}
explosion/thinc,https://github.com/explosion/thinc,494,2021-05-03T21:18:50Z,2021-05-04T23:31:57Z,2021-05-04T23:31:57Z,MERGED,True,10,7,3,https://github.com/Kludex, Add update_forward_refs with local models,2,[],https://github.com/explosion/thinc/pull/494,https://github.com/Kludex,1,https://github.com/explosion/thinc/pull/494,"Hi there! 
Let me start with: I don't know exactly how this start failing. 
Maybe you'll understand better looking at the explanation: samuelcolvin/pydantic#2678 (comment)
The solution I proposed is the one presented in the link above. An alternative could be:
ArgModel.update_forward_refs(**globals())
(but you'll get more than you need... and I'm not sure it works in all cases)","Hi there! 
Let me start with: I don't know exactly how this start failing. 
Maybe you'll understand better looking at the explanation: samuelcolvin/pydantic#2678 (comment)
The solution I proposed is the one presented in the link above. An alternative could be:
ArgModel.update_forward_refs(**globals())
(but you'll get more than you need... and I'm not sure it works in all cases)",True,"{'HOORAY': ['https://github.com/svlandeg', 'https://github.com/ines']}"
explosion/thinc,https://github.com/explosion/thinc,494,2021-05-03T21:18:50Z,2021-05-04T23:31:57Z,2021-05-04T23:31:57Z,MERGED,True,10,7,3,https://github.com/Kludex, Add update_forward_refs with local models,2,[],https://github.com/explosion/thinc/pull/494,https://github.com/ines,2,https://github.com/explosion/thinc/pull/494#issuecomment-831619815,"Hi there! 
Let me start with: I don't know exactly how this start failing. 
Maybe you'll understand better looking at the explanation: samuelcolvin/pydantic#2678 (comment)
The solution I proposed is the one presented in the link above. An alternative could be:
ArgModel.update_forward_refs(**globals())
(but you'll get more than you need... and I'm not sure it works in all cases)","Omg, thanks!  (Also, I feel kinda stupid for not seeing this... I even vaguely remember thinking ""hmm, I wish there was a way to just tell it what the modules are "" hahaha)
So I'm assuming the reason it doesn't work out-of-the-box, even with the imports in the top scope of util.py, is because the actual forward refs are defined outside of that scope? And that's what actually matters, not the scope updating the forward refs?
I'll add some small comments for our team that are mostly about how we want to structure the call to update_forward_refs, now that we know it's possible to just pass in the refs like this.",True,{}
explosion/thinc,https://github.com/explosion/thinc,495,2021-05-05T17:44:17Z,2021-05-29T03:54:51Z,2021-05-31T07:51:02Z,MERGED,True,7,4,1,https://github.com/svlandeg,Avoid concatting `None` gradient,2,"['bug', 'feat / layers']",https://github.com/explosion/thinc/pull/495,https://github.com/svlandeg,1,https://github.com/explosion/thinc/pull/495,"Originally opened to address explosion/spaCy#8012
That was solved within spacy. This PR now only involves a more general implementation of the current functionality.","Originally opened to address explosion/spaCy#8012
That was solved within spacy. This PR now only involves a more general implementation of the current functionality.",True,{}
explosion/thinc,https://github.com/explosion/thinc,498,2021-05-18T06:56:38Z,2021-05-24T02:26:48Z,2021-05-24T02:26:48Z,MERGED,True,15,7,5,https://github.com/adrianeboyd,Skip tests if packages are missing,2,['tests'],https://github.com/explosion/thinc/pull/498,https://github.com/adrianeboyd,1,https://github.com/explosion/thinc/pull/498,"Skip mypy, ml_datasets and notebook tests if those (dev-only) requirements are missing. (Primarily due to problems running the test suite on conda-forge.)","Skip mypy, ml_datasets and notebook tests if those (dev-only) requirements are missing. (Primarily due to problems running the test suite on conda-forge.)",True,{}
explosion/thinc,https://github.com/explosion/thinc,499,2021-05-18T07:02:26Z,2021-05-18T08:01:05Z,2021-05-18T08:01:05Z,MERGED,True,2,2,2,https://github.com/adrianeboyd,Update pydantic requirement,1,[],https://github.com/explosion/thinc/pull/499,https://github.com/adrianeboyd,1,https://github.com/explosion/thinc/pull/499,"Increase upper pin to <1.9.0. Forbid versions with security vulnerability:
GHSA-5jqp-qgf6-3pvh","Increase upper pin to <1.9.0. Forbid versions with security vulnerability:
GHSA-5jqp-qgf6-3pvh",True,{}
explosion/thinc,https://github.com/explosion/thinc,499,2021-05-18T07:02:26Z,2021-05-18T08:01:05Z,2021-05-18T08:01:05Z,MERGED,True,2,2,2,https://github.com/adrianeboyd,Update pydantic requirement,1,[],https://github.com/explosion/thinc/pull/499,https://github.com/svlandeg,2,https://github.com/explosion/thinc/pull/499#issuecomment-842948865,"Increase upper pin to <1.9.0. Forbid versions with security vulnerability:
GHSA-5jqp-qgf6-3pvh",I guess we need to do spacy too?,True,{'THUMBS_UP': ['https://github.com/Kludex']}
explosion/thinc,https://github.com/explosion/thinc,501,2021-05-28T12:51:29Z,2021-05-31T07:52:59Z,2021-05-31T07:53:02Z,MERGED,True,0,10,2,https://github.com/svlandeg,remove alpha release warning,1,[],https://github.com/explosion/thinc/pull/501,https://github.com/svlandeg,1,https://github.com/explosion/thinc/pull/501,,,True,{}
explosion/thinc,https://github.com/explosion/thinc,502,2021-05-31T09:19:16Z,2021-05-31T09:53:12Z,2021-05-31T09:53:12Z,MERGED,True,3,1,1,https://github.com/thomashacker,add more info to 'ValueError' exception message,2,[],https://github.com/explosion/thinc/pull/502,https://github.com/thomashacker,1,https://github.com/explosion/thinc/pull/502,,,True,"{'HOORAY': ['https://github.com/svlandeg', 'https://github.com/ines', 'https://github.com/damian-romero']}"
explosion/thinc,https://github.com/explosion/thinc,503,2021-06-01T13:09:46Z,2021-06-01T15:19:28Z,2021-06-01T15:19:28Z,MERGED,True,1,1,1,https://github.com/adrianeboyd,Exclude generated .cpp files from package,1,[],https://github.com/explosion/thinc/pull/503,https://github.com/adrianeboyd,1,https://github.com/explosion/thinc/pull/503,,,True,{}
explosion/thinc,https://github.com/explosion/thinc,504,2021-06-01T13:12:00Z,2021-08-16T09:59:58Z,2021-08-16T09:59:58Z,CLOSED,False,8,6,5,https://github.com/adrianeboyd,Test Cython==3.0a7,3,[],https://github.com/explosion/thinc/pull/504,https://github.com/adrianeboyd,1,https://github.com/explosion/thinc/pull/504,,,True,{}
explosion/thinc,https://github.com/explosion/thinc,504,2021-06-01T13:12:00Z,2021-08-16T09:59:58Z,2021-08-16T09:59:58Z,CLOSED,False,8,6,5,https://github.com/adrianeboyd,Test Cython==3.0a7,3,[],https://github.com/explosion/thinc/pull/504,https://github.com/adrianeboyd,2,https://github.com/explosion/thinc/pull/504#issuecomment-852115160,,(I expect this to compile but have a few test failures.),True,{}
explosion/thinc,https://github.com/explosion/thinc,505,2021-06-01T13:31:37Z,2021-06-01T15:19:12Z,2021-06-01T15:19:12Z,MERGED,True,2,2,2,https://github.com/adrianeboyd,Require catalogue>=2.0.4,1,[],https://github.com/explosion/thinc/pull/505,https://github.com/adrianeboyd,1,https://github.com/explosion/thinc/pull/505,Require catalogue>=2.0.4 due to bugs in v2.0.3 related to importlib-metadata.,Require catalogue>=2.0.4 due to bugs in v2.0.3 related to importlib-metadata.,True,{}
explosion/thinc,https://github.com/explosion/thinc,506,2021-06-08T07:39:03Z,2021-08-10T11:37:13Z,2021-08-10T11:37:13Z,CLOSED,False,10,1,1,https://github.com/adrianeboyd,Add hacky AppleOps,1,[],https://github.com/explosion/thinc/pull/506,https://github.com/adrianeboyd,1,https://github.com/explosion/thinc/pull/506,"Submitted for reference, but I think it would be better to handle this through entry points as noted here:
https://github.com/honnibal/thinc-apple-ops/blob/841ebc1ac42b0639bbc7d89b3b56c5a191ccfeb2/setup.cfg#L36-L39
This version requires the user to manually manage the ops with a context manager, etc., which is probably also not what we want in the future.","Submitted for reference, but I think it would be better to handle this through entry points as noted here:
https://github.com/honnibal/thinc-apple-ops/blob/841ebc1ac42b0639bbc7d89b3b56c5a191ccfeb2/setup.cfg#L36-L39
This version requires the user to manually manage the ops with a context manager, etc., which is probably also not what we want in the future.",True,{}
explosion/thinc,https://github.com/explosion/thinc,507,2021-06-08T07:48:43Z,2021-06-08T11:42:23Z,2021-06-08T11:42:23Z,MERGED,True,4,4,1,https://github.com/adrianeboyd,Update CI,1,[],https://github.com/explosion/thinc/pull/507,https://github.com/adrianeboyd,1,https://github.com/explosion/thinc/pull/507,"Update to ubuntu-18.04 image
Update to test with torch==1.8.1","Update to ubuntu-18.04 image
Update to test with torch==1.8.1",True,{}
explosion/thinc,https://github.com/explosion/thinc,508,2021-06-10T08:03:53Z,2021-06-10T10:50:18Z,2021-06-10T10:50:18Z,MERGED,True,3,3,3,https://github.com/adrianeboyd,Restrict cython to <3.0,1,[],https://github.com/explosion/thinc/pull/508,https://github.com/adrianeboyd,1,https://github.com/explosion/thinc/pull/508,,,True,{}
explosion/thinc,https://github.com/explosion/thinc,508,2021-06-10T08:03:53Z,2021-06-10T10:50:18Z,2021-06-10T10:50:18Z,MERGED,True,3,3,3,https://github.com/adrianeboyd,Restrict cython to <3.0,1,[],https://github.com/explosion/thinc/pull/508,https://github.com/adrianeboyd,2,https://github.com/explosion/thinc/pull/508#issuecomment-858418478,,"The failures are related to mypy, will rerun after #509 #510.",True,{}
explosion/thinc,https://github.com/explosion/thinc,509,2021-06-10T08:12:22Z,2021-06-10T09:04:14Z,2021-06-10T09:04:14Z,CLOSED,False,1,1,1,https://github.com/adrianeboyd,Use mypy<0.820 for testing,1,[],https://github.com/explosion/thinc/pull/509,https://github.com/adrianeboyd,1,https://github.com/explosion/thinc/pull/509,,,True,{}
explosion/thinc,https://github.com/explosion/thinc,510,2021-06-10T08:43:44Z,2021-06-10T09:54:37Z,2021-06-10T09:54:37Z,MERGED,True,20,17,4,https://github.com/Kludex, Fix mypy tests due to wrong quoting,4,[],https://github.com/explosion/thinc/pull/510,https://github.com/Kludex,1,https://github.com/explosion/thinc/pull/510,,,True,{}
explosion/thinc,https://github.com/explosion/thinc,511,2021-06-16T11:42:45Z,2021-06-16T12:31:32Z,2021-06-16T12:31:32Z,MERGED,True,3,3,3,https://github.com/adrianeboyd,Update for torch 1.9.0,2,[],https://github.com/explosion/thinc/pull/511,https://github.com/adrianeboyd,1,https://github.com/explosion/thinc/pull/511,,,True,{}
explosion/thinc,https://github.com/explosion/thinc,513,2021-06-21T08:54:04Z,2021-06-21T09:50:17Z,2021-06-21T09:50:17Z,MERGED,True,1,1,1,https://github.com/adrianeboyd,Set version to v8.0.6,1,[],https://github.com/explosion/thinc/pull/513,https://github.com/adrianeboyd,1,https://github.com/explosion/thinc/pull/513,,,True,{}
explosion/thinc,https://github.com/explosion/thinc,514,2021-06-24T09:18:15Z,2021-07-01T09:01:15Z,2021-07-01T09:01:15Z,MERGED,True,13,2,2,https://github.com/adrianeboyd,Include final ngram in NumpyOps.ngrams,4,[],https://github.com/explosion/thinc/pull/514,https://github.com/adrianeboyd,1,https://github.com/explosion/thinc/pull/514,Additionally return empty lists for invalid n-gram lengths,Additionally return empty lists for invalid n-gram lengths,True,{}
explosion/thinc,https://github.com/explosion/thinc,516,2021-06-30T11:22:30Z,2021-07-01T08:13:25Z,2021-07-01T08:13:25Z,MERGED,True,9,9,1,https://github.com/adrianeboyd,Update initializers for typing in numpy 1.21+,1,['feat / types'],https://github.com/explosion/thinc/pull/516,https://github.com/adrianeboyd,1,https://github.com/explosion/thinc/pull/516,Update initializers to cast values to the thinc-specific types supported by the ops.,Update initializers to cast values to the thinc-specific types supported by the ops.,True,{'THUMBS_UP': ['https://github.com/honnibal']}
explosion/thinc,https://github.com/explosion/thinc,517,2021-07-01T09:03:22Z,2021-07-01T11:05:13Z,2021-07-01T11:05:13Z,MERGED,True,1,1,1,https://github.com/adrianeboyd,Set version to v8.0.7,1,[],https://github.com/explosion/thinc/pull/517,https://github.com/adrianeboyd,1,https://github.com/explosion/thinc/pull/517,,,True,{}
explosion/thinc,https://github.com/explosion/thinc,518,2021-07-15T16:13:29Z,2021-07-19T08:03:19Z,2021-07-19T08:03:36Z,MERGED,True,100,13,2,https://github.com/svlandeg,allow negated values in CategoricalCrossentropy,6,"['enhancement', 'feat / ops']",https://github.com/explosion/thinc/pull/518,https://github.com/svlandeg,1,https://github.com/explosion/thinc/pull/518,"SequenceCategoricalCrossentropy can be called with a truths array consisting of strings. This PR allows a neg_prefix to be defined that signals to the loss function that a given string is negated - i.e. it is a negative annotation.
This way, calculation of the loss can directly take into account a negated value.
Goal
This will allow us to train the tagger on labels such as ""!NN"", without changing much on the spaCy side. The only requirement will be to define the neg_prefix. I'll be adding a unit test to spaCy that nicely illustrates this.
How it works

input: truths array, e.g. [['!N', 'V', 'J', 'N']]
ignoring the negative prefix !, this will be converted to indices according to the known labels: [1, 2, 0, 1]
this PR additionally keeps track of a negatives_mask which is 1 for all normal values, -1 for the negated value, and 0 for the other values in the same row. These other values will become masked because !N does not tell us whether V or J is correct, so we don't calculate a loss value on those.
In this example, the negatives_mask will be:

[[ 0. -1.  0.]
 [ 1.  1.  1.]
 [ 1.  1.  1.]
 [ 1.  1.  1.]]

the final truths array will be
[[0. 0. 0.]
 [0. 0. 1.]
 [1. 0. 0.]
 [0. 1. 0.]]

(instead of [0. 1. 0.] on the first row)
and the final mask will be
[[0. 1. 0.]
 [1. 1. 1.]
 [1. 1. 1.]
 [1. 1. 1.]]

effectively ignoring V and J for the first instance.","SequenceCategoricalCrossentropy can be called with a truths array consisting of strings. This PR allows a neg_prefix to be defined that signals to the loss function that a given string is negated - i.e. it is a negative annotation.
This way, calculation of the loss can directly take into account a negated value.
Goal
This will allow us to train the tagger on labels such as ""!NN"", without changing much on the spaCy side. The only requirement will be to define the neg_prefix. I'll be adding a unit test to spaCy that nicely illustrates this.
How it works

input: truths array, e.g. [['!N', 'V', 'J', 'N']]
ignoring the negative prefix !, this will be converted to indices according to the known labels: [1, 2, 0, 1]
this PR additionally keeps track of a negatives_mask which is 1 for all normal values, -1 for the negated value, and 0 for the other values in the same row. These other values will become masked because !N does not tell us whether V or J is correct, so we don't calculate a loss value on those.
In this example, the negatives_mask will be:

[[ 0. -1.  0.]
 [ 1.  1.  1.]
 [ 1.  1.  1.]
 [ 1.  1.  1.]]

the final truths array will be
[[0. 0. 0.]
 [0. 0. 1.]
 [1. 0. 0.]
 [0. 1. 0.]]

(instead of [0. 1. 0.] on the first row)
and the final mask will be
[[0. 1. 0.]
 [1. 1. 1.]
 [1. 1. 1.]
 [1. 1. 1.]]

effectively ignoring V and J for the first instance.",True,{}
explosion/thinc,https://github.com/explosion/thinc,520,2021-08-06T07:56:24Z,2021-08-17T08:40:22Z,2021-08-17T08:40:22Z,MERGED,True,17,12,7,https://github.com/adrianeboyd,Add ops registry,4,['feat / ops'],https://github.com/explosion/thinc/pull/520,https://github.com/adrianeboyd,1,https://github.com/explosion/thinc/pull/520,,,True,{'THUMBS_UP': ['https://github.com/svlandeg']}
explosion/thinc,https://github.com/explosion/thinc,520,2021-08-06T07:56:24Z,2021-08-17T08:40:22Z,2021-08-17T08:40:22Z,MERGED,True,17,12,7,https://github.com/adrianeboyd,Add ops registry,4,['feat / ops'],https://github.com/explosion/thinc/pull/520,https://github.com/adrianeboyd,2,https://github.com/explosion/thinc/pull/520#issuecomment-894193505,,"To prefer AppleOps over NumpyOps you could something like this, although there may be a better place for it:
diff --git a/thinc/__init__.py b/thinc/__init__.py
index a0e72b2a..3c4f2c9e 100644
--- a/thinc/__init__.py
+++ b/thinc/__init__.py
@@ -3,3 +3,11 @@ import numpy
 
 from .about import __version__
 from .config import registry
+
+
+try:
+    from thinc_apple_ops import AppleOps
+    from .api import set_current_ops
+    set_current_ops(AppleOps())
+except ImportError:
+    pass

require_cpu would also potentially need to be updated.

Edited: the following does not work due to circular imports:
A follow-up PR would do this to prefer AppleOps over NumpyOps if present:
diff --git a/thinc/backends/__init__.py b/thinc/backends/__init__.py
index 5991a6ea..6d3d3589 100644
--- a/thinc/backends/__init__.py
+++ b/thinc/backends/__init__.py
@@ -15,12 +15,20 @@ from ..types import OpsNames
 from .. import registry
 
 
-context_ops: ContextVar[NumpyOps] = ContextVar(""context_ops"", default=NumpyOps())
+DefaultOps = NumpyOps
+try:
+    from thinc_apple_ops import AppleOps
+
+    DefaultOps = AppleOps
+except ImportError:
+    pass
+
+context_ops: ContextVar[DefaultOps] = ContextVar(""context_ops"", default=DefaultOps())
 context_pools: ContextVar[dict] = ContextVar(""context_pools"", default={})
 
 # Internal use of thread-local storage only for detecting cases where a Jupyter
 # notebook might not have preserved contextvars across cells.
-_GLOBAL_STATE = {""ops"": NumpyOps()}
+_GLOBAL_STATE = {""ops"": DefaultOps()}
 
 
 def set_gpu_allocator(allocator: str) -> None:  # pragma: no cover",True,{}
explosion/thinc,https://github.com/explosion/thinc,520,2021-08-06T07:56:24Z,2021-08-17T08:40:22Z,2021-08-17T08:40:22Z,MERGED,True,17,12,7,https://github.com/adrianeboyd,Add ops registry,4,['feat / ops'],https://github.com/explosion/thinc/pull/520,https://github.com/adrianeboyd,3,https://github.com/explosion/thinc/pull/520#issuecomment-900074115,,"Ah, wait, maybe I understand the mypy error now. The OpsNames typing with a hard-coded list isn't going to work.",True,{}
explosion/thinc,https://github.com/explosion/thinc,520,2021-08-06T07:56:24Z,2021-08-17T08:40:22Z,2021-08-17T08:40:22Z,MERGED,True,17,12,7,https://github.com/adrianeboyd,Add ops registry,4,['feat / ops'],https://github.com/explosion/thinc/pull/520,https://github.com/adrianeboyd,4,https://github.com/explosion/thinc/pull/520#issuecomment-900085495,,"I still don't understand how to fix the registry-related mypy error, and I'm seeing new unrelated mypy errors, so let's see...",True,{}
explosion/thinc,https://github.com/explosion/thinc,521,2021-08-11T18:44:31Z,2021-08-16T16:01:05Z,2021-08-16T16:01:05Z,MERGED,True,9,1,2,https://github.com/danieldk,Fix numpy_ops gemm output semantics when BLIS is used,1,"['bug', 'feat / ops']",https://github.com/explosion/thinc/pull/521,https://github.com/danieldk,1,https://github.com/explosion/thinc/pull/521,"I am reading various parts of the thinc source code to see how things work, I found a small inconsistency in the numpy gemm op:
The out keyword argument of the gemm op specifies an output array. However, the semantics were different depending on whether BLIS is used:

use_blis==False: the values of out array are overwritten.
use_blis==True: the values of out are added to the result.

With this change, the values of out are also overwritten with use_blis=True.","I am reading various parts of the thinc source code to see how things work, I found a small inconsistency in the numpy gemm op:
The out keyword argument of the gemm op specifies an output array. However, the semantics were different depending on whether BLIS is used:

use_blis==False: the values of out array are overwritten.
use_blis==True: the values of out are added to the result.

With this change, the values of out are also overwritten with use_blis=True.",True,{'EYES': ['https://github.com/svlandeg']}
explosion/thinc,https://github.com/explosion/thinc,523,2021-08-24T08:47:40Z,2021-08-27T10:51:39Z,2021-08-27T10:51:39Z,MERGED,True,11,2,2,https://github.com/adrianeboyd,Prefer AppleOps to NumpyOps if available,2,['feat / ops'],https://github.com/explosion/thinc/pull/523,https://github.com/adrianeboyd,1,https://github.com/explosion/thinc/pull/523,"I am not sure this is the best place to do this, but didn't see any more suitable alternatives, since you run into circular imports if it's in thinc.backends.","I am not sure this is the best place to do this, but didn't see any more suitable alternatives, since you run into circular imports if it's in thinc.backends.",True,{}
explosion/thinc,https://github.com/explosion/thinc,523,2021-08-24T08:47:40Z,2021-08-27T10:51:39Z,2021-08-27T10:51:39Z,MERGED,True,11,2,2,https://github.com/adrianeboyd,Prefer AppleOps to NumpyOps if available,2,['feat / ops'],https://github.com/explosion/thinc/pull/523,https://github.com/honnibal,2,https://github.com/explosion/thinc/pull/523#issuecomment-906314934,"I am not sure this is the best place to do this, but didn't see any more suitable alternatives, since you run into circular imports if it's in thinc.backends.",Looks good to me. I agree there's no more sensible place to do it.,True,{}
explosion/thinc,https://github.com/explosion/thinc,524,2021-08-24T10:47:09Z,2021-08-24T11:41:53Z,2021-08-24T11:41:54Z,MERGED,True,2,2,1,https://github.com/adrianeboyd,Use newer releases of nbconvert and nbformat,1,[],https://github.com/explosion/thinc/pull/524,https://github.com/adrianeboyd,1,https://github.com/explosion/thinc/pull/524,"I have only increased the upper ranges. Some combination of older versions and newer dependencies (nbclient?) is incompatible, but I'm not sure exactly what.","I have only increased the upper ranges. Some combination of older versions and newer dependencies (nbclient?) is incompatible, but I'm not sure exactly what.",True,{}
explosion/thinc,https://github.com/explosion/thinc,525,2021-08-25T22:22:48Z,2021-08-31T13:16:04Z,2021-08-31T13:16:27Z,MERGED,True,162,8,5,https://github.com/connorbrinton, Fix Mypy plugin crash on variadic arguments,7,"['bug', 'feat / types']",https://github.com/explosion/thinc/pull/525,https://github.com/connorbrinton,1,https://github.com/explosion/thinc/pull/525,"These changes fix a crash that would occur in the Thinc Mypy plugin whenever a Thinc API function was called with variadic arguments.
Previously the Thinc Mypy plugin would treat any argument with a generic type as a Model instance and attempt to infer a more specific type for the return type of the function call. However, the Thinc Mypy plugin was intended to only be used on functions that exclusively accept and return Model instances. Since the plugin accepted overly broad input, it would sometimes end up trying to access the second generic type argument for a generic type instance with only a single argument. This resulted in the plugin crashing.
This specifically manifested itself whenever variadic arguments were passed to a Thinc API function. Mypy passes the type of variadic arguments as List[Model[InT, OutT]]. The Thinc Mypy plugin would look at this type and attempt to treat it as an unwrapped Model type since the plugin would allow any generic type as an argument type annotation. Later on the plugin would attempt to retrieve the output type of the ""Model"". Since List accepts only a single generic argument, attempting to access type.args[1] would cause the crash.
These changes fix this by restricting the input on which the Thinc Mypy plugin operates. Specifically each argument and the return type must:

Be a generic type Instance,
Refer to the thinc.model.Model type and
Have two generic type arguments

Notably these restrictions prevent the Thinc Mypy plugin from working on subclasses of thinc.model.Model. While this plugin previously had some support for Model subclasses, this behavior was not documented and unintentional. Model subclasses were only properly handled if their first two generic type arguments had the same meaning as Model's generic type arguments.
Support for Model subclasses may be restored in the future through the use of Mypy's map_instance_to_supertype function. I just haven't figured out how to use it yet 
Fixes #519","These changes fix a crash that would occur in the Thinc Mypy plugin whenever a Thinc API function was called with variadic arguments.
Previously the Thinc Mypy plugin would treat any argument with a generic type as a Model instance and attempt to infer a more specific type for the return type of the function call. However, the Thinc Mypy plugin was intended to only be used on functions that exclusively accept and return Model instances. Since the plugin accepted overly broad input, it would sometimes end up trying to access the second generic type argument for a generic type instance with only a single argument. This resulted in the plugin crashing.
This specifically manifested itself whenever variadic arguments were passed to a Thinc API function. Mypy passes the type of variadic arguments as List[Model[InT, OutT]]. The Thinc Mypy plugin would look at this type and attempt to treat it as an unwrapped Model type since the plugin would allow any generic type as an argument type annotation. Later on the plugin would attempt to retrieve the output type of the ""Model"". Since List accepts only a single generic argument, attempting to access type.args[1] would cause the crash.
These changes fix this by restricting the input on which the Thinc Mypy plugin operates. Specifically each argument and the return type must:

Be a generic type Instance,
Refer to the thinc.model.Model type and
Have two generic type arguments

Notably these restrictions prevent the Thinc Mypy plugin from working on subclasses of thinc.model.Model. While this plugin previously had some support for Model subclasses, this behavior was not documented and unintentional. Model subclasses were only properly handled if their first two generic type arguments had the same meaning as Model's generic type arguments.
Support for Model subclasses may be restored in the future through the use of Mypy's map_instance_to_supertype function. I just haven't figured out how to use it yet 
Fixes #519",True,"{'EYES': ['https://github.com/svlandeg'], 'HOORAY': ['https://github.com/svlandeg']}"
explosion/thinc,https://github.com/explosion/thinc,525,2021-08-25T22:22:48Z,2021-08-31T13:16:04Z,2021-08-31T13:16:27Z,MERGED,True,162,8,5,https://github.com/connorbrinton, Fix Mypy plugin crash on variadic arguments,7,"['bug', 'feat / types']",https://github.com/explosion/thinc/pull/525,https://github.com/adrianeboyd,2,https://github.com/explosion/thinc/pull/525#issuecomment-906377379,"These changes fix a crash that would occur in the Thinc Mypy plugin whenever a Thinc API function was called with variadic arguments.
Previously the Thinc Mypy plugin would treat any argument with a generic type as a Model instance and attempt to infer a more specific type for the return type of the function call. However, the Thinc Mypy plugin was intended to only be used on functions that exclusively accept and return Model instances. Since the plugin accepted overly broad input, it would sometimes end up trying to access the second generic type argument for a generic type instance with only a single argument. This resulted in the plugin crashing.
This specifically manifested itself whenever variadic arguments were passed to a Thinc API function. Mypy passes the type of variadic arguments as List[Model[InT, OutT]]. The Thinc Mypy plugin would look at this type and attempt to treat it as an unwrapped Model type since the plugin would allow any generic type as an argument type annotation. Later on the plugin would attempt to retrieve the output type of the ""Model"". Since List accepts only a single generic argument, attempting to access type.args[1] would cause the crash.
These changes fix this by restricting the input on which the Thinc Mypy plugin operates. Specifically each argument and the return type must:

Be a generic type Instance,
Refer to the thinc.model.Model type and
Have two generic type arguments

Notably these restrictions prevent the Thinc Mypy plugin from working on subclasses of thinc.model.Model. While this plugin previously had some support for Model subclasses, this behavior was not documented and unintentional. Model subclasses were only properly handled if their first two generic type arguments had the same meaning as Model's generic type arguments.
Support for Model subclasses may be restored in the future through the use of Mypy's map_instance_to_supertype function. I just haven't figured out how to use it yet 
Fixes #519",Thanks for the PR! I'll go ahead and try to fix the python version compat issues from our end.,True,{'THUMBS_UP': ['https://github.com/connorbrinton']}
explosion/thinc,https://github.com/explosion/thinc,525,2021-08-25T22:22:48Z,2021-08-31T13:16:04Z,2021-08-31T13:16:27Z,MERGED,True,162,8,5,https://github.com/connorbrinton, Fix Mypy plugin crash on variadic arguments,7,"['bug', 'feat / types']",https://github.com/explosion/thinc/pull/525,https://github.com/connorbrinton,3,https://github.com/explosion/thinc/pull/525#issuecomment-908377540,"These changes fix a crash that would occur in the Thinc Mypy plugin whenever a Thinc API function was called with variadic arguments.
Previously the Thinc Mypy plugin would treat any argument with a generic type as a Model instance and attempt to infer a more specific type for the return type of the function call. However, the Thinc Mypy plugin was intended to only be used on functions that exclusively accept and return Model instances. Since the plugin accepted overly broad input, it would sometimes end up trying to access the second generic type argument for a generic type instance with only a single argument. This resulted in the plugin crashing.
This specifically manifested itself whenever variadic arguments were passed to a Thinc API function. Mypy passes the type of variadic arguments as List[Model[InT, OutT]]. The Thinc Mypy plugin would look at this type and attempt to treat it as an unwrapped Model type since the plugin would allow any generic type as an argument type annotation. Later on the plugin would attempt to retrieve the output type of the ""Model"". Since List accepts only a single generic argument, attempting to access type.args[1] would cause the crash.
These changes fix this by restricting the input on which the Thinc Mypy plugin operates. Specifically each argument and the return type must:

Be a generic type Instance,
Refer to the thinc.model.Model type and
Have two generic type arguments

Notably these restrictions prevent the Thinc Mypy plugin from working on subclasses of thinc.model.Model. While this plugin previously had some support for Model subclasses, this behavior was not documented and unintentional. Model subclasses were only properly handled if their first two generic type arguments had the same meaning as Model's generic type arguments.
Support for Model subclasses may be restored in the future through the use of Mypy's map_instance_to_supertype function. I just haven't figured out how to use it yet 
Fixes #519","@svlandeg Thanks for those suggestions, they've been committed ",True,{'THUMBS_UP': ['https://github.com/svlandeg']}
explosion/thinc,https://github.com/explosion/thinc,525,2021-08-25T22:22:48Z,2021-08-31T13:16:04Z,2021-08-31T13:16:27Z,MERGED,True,162,8,5,https://github.com/connorbrinton, Fix Mypy plugin crash on variadic arguments,7,"['bug', 'feat / types']",https://github.com/explosion/thinc/pull/525,https://github.com/svlandeg,4,https://github.com/explosion/thinc/pull/525#issuecomment-908428363,"These changes fix a crash that would occur in the Thinc Mypy plugin whenever a Thinc API function was called with variadic arguments.
Previously the Thinc Mypy plugin would treat any argument with a generic type as a Model instance and attempt to infer a more specific type for the return type of the function call. However, the Thinc Mypy plugin was intended to only be used on functions that exclusively accept and return Model instances. Since the plugin accepted overly broad input, it would sometimes end up trying to access the second generic type argument for a generic type instance with only a single argument. This resulted in the plugin crashing.
This specifically manifested itself whenever variadic arguments were passed to a Thinc API function. Mypy passes the type of variadic arguments as List[Model[InT, OutT]]. The Thinc Mypy plugin would look at this type and attempt to treat it as an unwrapped Model type since the plugin would allow any generic type as an argument type annotation. Later on the plugin would attempt to retrieve the output type of the ""Model"". Since List accepts only a single generic argument, attempting to access type.args[1] would cause the crash.
These changes fix this by restricting the input on which the Thinc Mypy plugin operates. Specifically each argument and the return type must:

Be a generic type Instance,
Refer to the thinc.model.Model type and
Have two generic type arguments

Notably these restrictions prevent the Thinc Mypy plugin from working on subclasses of thinc.model.Model. While this plugin previously had some support for Model subclasses, this behavior was not documented and unintentional. Model subclasses were only properly handled if their first two generic type arguments had the same meaning as Model's generic type arguments.
Support for Model subclasses may be restored in the future through the use of Mypy's map_instance_to_supertype function. I just haven't figured out how to use it yet 
Fixes #519",We'll get this released as part of 8.0.9 so we can continue with explosion/spaCy#8773!,True,{'HOORAY': ['https://github.com/connorbrinton']}
explosion/thinc,https://github.com/explosion/thinc,526,2021-08-27T09:45:49Z,2021-08-31T15:21:01Z,2021-08-31T15:21:53Z,MERGED,True,85,1,5,https://github.com/danieldk,Add context manager and layer wrapper for marking NVTX ranges,4,"['enhancement', 'feat / layers']",https://github.com/explosion/thinc/pull/526,https://github.com/danieldk,1,https://github.com/explosion/thinc/pull/526,"This PR adds the following:

the with_nvtx_range layer that wraps any layer and marks the forward and backprop passes as an NVTX range. This can be helpful when profiling GPU performance of one or more layers.
the use_nvtx_range context manager, which can be used to mark a scope as an NVTX range.

The ranges are used to mark executions in nvprof or NSight profiles and make it easier to see to which layers/code CUDA operations are associated.","This PR adds the following:

the with_nvtx_range layer that wraps any layer and marks the forward and backprop passes as an NVTX range. This can be helpful when profiling GPU performance of one or more layers.
the use_nvtx_range context manager, which can be used to mark a scope as an NVTX range.

The ranges are used to mark executions in nvprof or NSight profiles and make it easier to see to which layers/code CUDA operations are associated.",True,"{'EYES': ['https://github.com/svlandeg'], 'HOORAY': ['https://github.com/svlandeg']}"
explosion/thinc,https://github.com/explosion/thinc,526,2021-08-27T09:45:49Z,2021-08-31T15:21:01Z,2021-08-31T15:21:53Z,MERGED,True,85,1,5,https://github.com/danieldk,Add context manager and layer wrapper for marking NVTX ranges,4,"['enhancement', 'feat / layers']",https://github.com/explosion/thinc/pull/526,https://github.com/danieldk,2,https://github.com/explosion/thinc/pull/526#issuecomment-909001715,"This PR adds the following:

the with_nvtx_range layer that wraps any layer and marks the forward and backprop passes as an NVTX range. This can be helpful when profiling GPU performance of one or more layers.
the use_nvtx_range context manager, which can be used to mark a scope as an NVTX range.

The ranges are used to mark executions in nvprof or NSight profiles and make it easier to see to which layers/code CUDA operations are associated.","Looks good to me! I only marked some very minor typo stuff.

Thanks! All fixed.",True,{}
explosion/thinc,https://github.com/explosion/thinc,527,2021-08-31T11:46:47Z,2021-09-03T07:43:43Z,2021-09-03T07:43:46Z,MERGED,True,8,5,3,https://github.com/ines,Allow config overrides to add new keys,1,"['enhancement', 'feat / config']",https://github.com/explosion/thinc/pull/527,https://github.com/ines,1,https://github.com/explosion/thinc/pull/527,"Still need to test this against spaCy to check if we need to adjust any tests here. Also a bit worried about potential unitended side-effects (or some important reason why we included this feature that I forgot).
Related: explosion/spacy-transformers#283 (comment)

The config system currently only allows passing on overrides for existing settings, not new keys. For instance, given a config like this:
[section]
foo = 1
bar = 2
Passing in the overrides {""section.foo"": 3} is valid, whereas {""section.baz"": 3} would raise an error. In a lot of scenarios, this makes sense, because you'd expect the config to be complete and describing all available settings  so anything unexpected is likely a problem. But there are other cases where the config is more flexible (e.g. a dictionary with arbitrary keys passed to a function as an argument).","Still need to test this against spaCy to check if we need to adjust any tests here. Also a bit worried about potential unitended side-effects (or some important reason why we included this feature that I forgot).
Related: explosion/spacy-transformers#283 (comment)

The config system currently only allows passing on overrides for existing settings, not new keys. For instance, given a config like this:
[section]
foo = 1
bar = 2
Passing in the overrides {""section.foo"": 3} is valid, whereas {""section.baz"": 3} would raise an error. In a lot of scenarios, this makes sense, because you'd expect the config to be complete and describing all available settings  so anything unexpected is likely a problem. But there are other cases where the config is more flexible (e.g. a dictionary with arbitrary keys passed to a function as an argument).",True,{}
explosion/thinc,https://github.com/explosion/thinc,527,2021-08-31T11:46:47Z,2021-09-03T07:43:43Z,2021-09-03T07:43:46Z,MERGED,True,8,5,3,https://github.com/ines,Allow config overrides to add new keys,1,"['enhancement', 'feat / config']",https://github.com/explosion/thinc/pull/527,https://github.com/svlandeg,2,https://github.com/explosion/thinc/pull/527#issuecomment-909237379,"Still need to test this against spaCy to check if we need to adjust any tests here. Also a bit worried about potential unitended side-effects (or some important reason why we included this feature that I forgot).
Related: explosion/spacy-transformers#283 (comment)

The config system currently only allows passing on overrides for existing settings, not new keys. For instance, given a config like this:
[section]
foo = 1
bar = 2
Passing in the overrides {""section.foo"": 3} is valid, whereas {""section.baz"": 3} would raise an error. In a lot of scenarios, this makes sense, because you'd expect the config to be complete and describing all available settings  so anything unexpected is likely a problem. But there are other cases where the config is more flexible (e.g. a dictionary with arbitrary keys passed to a function as an argument).","I had a look into the history and apparently you had it like this in your original PR, but then changed it: ac1e9ca
There wasn't another reason discussed other than structuring the code differently.
I wonder whether it would make sense to warn when a new key is defined though. To guard the user against making a typo, accidentally adding a new key and not updating the old one as they would have expected.",True,{}
explosion/thinc,https://github.com/explosion/thinc,528,2021-08-31T21:49:04Z,2021-08-31T22:16:41Z,2021-08-31T22:16:43Z,MERGED,True,1,2,1,https://github.com/svlandeg,try unpinning pip,1,['tests'],https://github.com/explosion/thinc/pull/528,https://github.com/svlandeg,1,https://github.com/explosion/thinc/pull/528,"Testing the CI to see whether we can unpin pip, cf #493","Testing the CI to see whether we can unpin pip, cf #493",True,{}
explosion/thinc,https://github.com/explosion/thinc,528,2021-08-31T21:49:04Z,2021-08-31T22:16:41Z,2021-08-31T22:16:43Z,MERGED,True,1,2,1,https://github.com/svlandeg,try unpinning pip,1,['tests'],https://github.com/explosion/thinc/pull/528,https://github.com/svlandeg,2,https://github.com/explosion/thinc/pull/528#issuecomment-909685690,"Testing the CI to see whether we can unpin pip, cf #493","Hooray, it magically works again ",True,{}
explosion/thinc,https://github.com/explosion/thinc,529,2021-09-01T09:31:40Z,2021-09-02T14:06:02Z,2021-09-02T14:10:06Z,MERGED,True,296,12,8,https://github.com/danieldk,PyTorchShim: add support for mixed-precision,3,"['enhancement', 'feat / shims', 'interop / pytorch']",https://github.com/explosion/thinc/pull/529,https://github.com/danieldk,1,https://github.com/explosion/thinc/pull/529,"This change adds support for mixed-precision training and prediction to
the PyTorch shim. Mixed-precision support consists of two parts:

Autocasting of the forward pass. This changes whitelisted ops to run
in half precision rather than single precision.
Gradient scaling. Gradients underflow quickly in half-precision.
Gradient scaling increases the magnitude of the gradients during
backprop to avoid underflow.

A separate class, PyTorchGradScaler is introduced to perform gradient
scaling and store the associated state.

I hope that I correctly implemented multi-GPU training, but I couldn't test it with my current single GPU workstation.","This change adds support for mixed-precision training and prediction to
the PyTorch shim. Mixed-precision support consists of two parts:

Autocasting of the forward pass. This changes whitelisted ops to run
in half precision rather than single precision.
Gradient scaling. Gradients underflow quickly in half-precision.
Gradient scaling increases the magnitude of the gradients during
backprop to avoid underflow.

A separate class, PyTorchGradScaler is introduced to perform gradient
scaling and store the associated state.

I hope that I correctly implemented multi-GPU training, but I couldn't test it with my current single GPU workstation.",True,{}
explosion/thinc,https://github.com/explosion/thinc,529,2021-09-01T09:31:40Z,2021-09-02T14:06:02Z,2021-09-02T14:10:06Z,MERGED,True,296,12,8,https://github.com/danieldk,PyTorchShim: add support for mixed-precision,3,"['enhancement', 'feat / shims', 'interop / pytorch']",https://github.com/explosion/thinc/pull/529,https://github.com/honnibal,2,https://github.com/explosion/thinc/pull/529#issuecomment-911487002,"This change adds support for mixed-precision training and prediction to
the PyTorch shim. Mixed-precision support consists of two parts:

Autocasting of the forward pass. This changes whitelisted ops to run
in half precision rather than single precision.
Gradient scaling. Gradients underflow quickly in half-precision.
Gradient scaling increases the magnitude of the gradients during
backprop to avoid underflow.

A separate class, PyTorchGradScaler is introduced to perform gradient
scaling and store the associated state.

I hope that I correctly implemented multi-GPU training, but I couldn't test it with my current single GPU workstation.","It's hard to love the mechanism here.   It's just sort of disappointing to need this helper class.
I don't really have a better suggestion though, and it doesn't affect the rest of the API, it's specific to the PyTorchShim. So I'm okay with merging this as-is. I'd definitely try to find some other way of doing things if it were on the main Model class though!",True,{}
explosion/thinc,https://github.com/explosion/thinc,529,2021-09-01T09:31:40Z,2021-09-02T14:06:02Z,2021-09-02T14:10:06Z,MERGED,True,296,12,8,https://github.com/danieldk,PyTorchShim: add support for mixed-precision,3,"['enhancement', 'feat / shims', 'interop / pytorch']",https://github.com/explosion/thinc/pull/529,https://github.com/svlandeg,3,https://github.com/explosion/thinc/pull/529#issuecomment-911722979,"This change adds support for mixed-precision training and prediction to
the PyTorch shim. Mixed-precision support consists of two parts:

Autocasting of the forward pass. This changes whitelisted ops to run
in half precision rather than single precision.
Gradient scaling. Gradients underflow quickly in half-precision.
Gradient scaling increases the magnitude of the gradients during
backprop to avoid underflow.

A separate class, PyTorchGradScaler is introduced to perform gradient
scaling and store the associated state.

I hope that I correctly implemented multi-GPU training, but I couldn't test it with my current single GPU workstation.","We can have it as an ondocumented beta feature for now, allowing us to still revisit the implementation details at some point if we want. So I'll go ahead and merge!",True,{}
explosion/thinc,https://github.com/explosion/thinc,530,2021-09-01T13:54:43Z,2021-09-01T18:47:51Z,2021-09-02T10:21:48Z,MERGED,True,63,11,3,https://github.com/danieldk,Add wrap_model_recursive,3,['enhancement'],https://github.com/explosion/thinc/pull/530,https://github.com/danieldk,1,https://github.com/explosion/thinc/pull/530,"This is a simple wrapper function that recursively wraps a Model and its layers recursively.
Useful for e.g. recursively wrapping forward/backprop passes in NVTX ranges.","This is a simple wrapper function that recursively wraps a Model and its layers recursively.
Useful for e.g. recursively wrapping forward/backprop passes in NVTX ranges.",True,{'THUMBS_UP': ['https://github.com/svlandeg']}
explosion/thinc,https://github.com/explosion/thinc,531,2021-09-03T08:31:22Z,2021-09-03T09:51:25Z,2021-09-03T09:51:25Z,MERGED,True,1495,270,62,https://github.com/svlandeg,Update docs branch,60,[],https://github.com/explosion/thinc/pull/531,https://github.com/svlandeg,1,https://github.com/explosion/thinc/pull/531,Syncing with 8.0.9,Syncing with 8.0.9,True,{}
explosion/thinc,https://github.com/explosion/thinc,532,2021-09-06T13:37:50Z,2021-09-13T09:22:45Z,2021-09-13T09:22:45Z,MERGED,True,8,1,1,https://github.com/danieldk,Speed up clip_gradient with GPU training,1,"['enhancement', 'performance']",https://github.com/explosion/thinc/pull/532,https://github.com/danieldk,1,https://github.com/explosion/thinc/pull/532,"Roughly 25% of GPU time when training a transformer tagger with mixed-precision was spent on computing Frobenius norms for gradient clipping. The reason turns out to be that CuPy only uses a single block for scalar reductions (unless another backend such as CUB is used), resulting in a slow sum reduction when computing a norm.
This change replaces cupy.linalg.norm by cupy.cublas.nrm2, the optimized cuBLAS implementation for computing l2 norms. This reduces training time about by ~24%.
Top-5 cost before this change:
             Type  Time(%)      Time     Calls       Avg       Min       Max  Name
 GPU activities:   28.49%  23.0423s    185554  124.18us  1.0550us  19.842ms  cupy_sum
                    5.68%  4.59333s     42324  108.53us  29.151us  2.6026ms  turing_fp16_s1688gemm_fp16_128x128_ldg8_f2f_tn
                    4.69%  3.78947s    202426  18.720us  1.6320us  1.1343ms  _ZN2at6native27unrolled_elementwise_kernelIZZZNS0_21copy_device_to_deviceERNS_14TensorIteratorEbENKUlvE0_clEvENKUlvE18_clEvEUlN3c104HalfEE_NS_6detail5ArrayIPcLi2EEE23TrivialOffsetCalculatorILi1EjESE_NS0_6memory12LoadWithCastILi1EEENSF_13StoreWithCastEEEviT_T0_T1_T2_T3_T4_
                    4.66%  3.77189s     31488  119.79us  4.5760us  354.82us  turing_fp16_s1688gemm_fp16_128x128_ldg8_f2f_stages_32x1_nt
                    4.51%  3.64793s     79206  46.056us  1.6000us  2.8409ms  adam

After:
             Type  Time(%)      Time     Calls       Avg       Min       Max  Name
 GPU activities:    7.95%  4.59139s     42324  108.48us  29.184us  2.5801ms  turing_fp16_s1688gemm_fp16_128x128_ldg8_f2f_tn
                    6.56%  3.78955s    202426  18.720us  1.6320us  1.1526ms  _ZN2at6native27unrolled_elementwise_kernelIZZZNS0_21copy_device_to_deviceERNS_14TensorIteratorEbENKUlvE0_clEvENKUlvE18_clEvEUlN3c104HalfEE_NS_6detail5ArrayIPcLi2EEE23TrivialOffsetCalculatorILi1EjESE_NS0_6memory12LoadWithCastILi1EEENSF_13StoreWithCastEEEviT_T0_T1_T2_T3_T4_
                    6.53%  3.77162s     31488  119.78us  4.5760us  354.98us  turing_fp16_s1688gemm_fp16_128x128_ldg8_f2f_stages_32x1_nt
                    6.25%  3.61247s     78410  46.071us  1.6640us  2.8498ms  adam
                    6.21%  3.58839s    186237  19.267us     800ns  1.1974ms  void at::native::vectorized_elementwise_kernel<int=4, at::native::AddFunctor<float>, at::detail::Array<char*, int=3>>(int, float, at::native::AddFunctor<float>)","Roughly 25% of GPU time when training a transformer tagger with mixed-precision was spent on computing Frobenius norms for gradient clipping. The reason turns out to be that CuPy only uses a single block for scalar reductions (unless another backend such as CUB is used), resulting in a slow sum reduction when computing a norm.
This change replaces cupy.linalg.norm by cupy.cublas.nrm2, the optimized cuBLAS implementation for computing l2 norms. This reduces training time about by ~24%.
Top-5 cost before this change:
             Type  Time(%)      Time     Calls       Avg       Min       Max  Name
 GPU activities:   28.49%  23.0423s    185554  124.18us  1.0550us  19.842ms  cupy_sum
                    5.68%  4.59333s     42324  108.53us  29.151us  2.6026ms  turing_fp16_s1688gemm_fp16_128x128_ldg8_f2f_tn
                    4.69%  3.78947s    202426  18.720us  1.6320us  1.1343ms  _ZN2at6native27unrolled_elementwise_kernelIZZZNS0_21copy_device_to_deviceERNS_14TensorIteratorEbENKUlvE0_clEvENKUlvE18_clEvEUlN3c104HalfEE_NS_6detail5ArrayIPcLi2EEE23TrivialOffsetCalculatorILi1EjESE_NS0_6memory12LoadWithCastILi1EEENSF_13StoreWithCastEEEviT_T0_T1_T2_T3_T4_
                    4.66%  3.77189s     31488  119.79us  4.5760us  354.82us  turing_fp16_s1688gemm_fp16_128x128_ldg8_f2f_stages_32x1_nt
                    4.51%  3.64793s     79206  46.056us  1.6000us  2.8409ms  adam

After:
             Type  Time(%)      Time     Calls       Avg       Min       Max  Name
 GPU activities:    7.95%  4.59139s     42324  108.48us  29.184us  2.5801ms  turing_fp16_s1688gemm_fp16_128x128_ldg8_f2f_tn
                    6.56%  3.78955s    202426  18.720us  1.6320us  1.1526ms  _ZN2at6native27unrolled_elementwise_kernelIZZZNS0_21copy_device_to_deviceERNS_14TensorIteratorEbENKUlvE0_clEvENKUlvE18_clEvEUlN3c104HalfEE_NS_6detail5ArrayIPcLi2EEE23TrivialOffsetCalculatorILi1EjESE_NS0_6memory12LoadWithCastILi1EEENSF_13StoreWithCastEEEviT_T0_T1_T2_T3_T4_
                    6.53%  3.77162s     31488  119.78us  4.5760us  354.98us  turing_fp16_s1688gemm_fp16_128x128_ldg8_f2f_stages_32x1_nt
                    6.25%  3.61247s     78410  46.071us  1.6640us  2.8498ms  adam
                    6.21%  3.58839s    186237  19.267us     800ns  1.1974ms  void at::native::vectorized_elementwise_kernel<int=4, at::native::AddFunctor<float>, at::detail::Array<char*, int=3>>(int, float, at::native::AddFunctor<float>)",True,{'ROCKET': ['https://github.com/svlandeg']}
explosion/thinc,https://github.com/explosion/thinc,532,2021-09-06T13:37:50Z,2021-09-13T09:22:45Z,2021-09-13T09:22:45Z,MERGED,True,8,1,1,https://github.com/danieldk,Speed up clip_gradient with GPU training,1,"['enhancement', 'performance']",https://github.com/explosion/thinc/pull/532,https://github.com/honnibal,2,https://github.com/explosion/thinc/pull/532#issuecomment-913762546,"Roughly 25% of GPU time when training a transformer tagger with mixed-precision was spent on computing Frobenius norms for gradient clipping. The reason turns out to be that CuPy only uses a single block for scalar reductions (unless another backend such as CUB is used), resulting in a slow sum reduction when computing a norm.
This change replaces cupy.linalg.norm by cupy.cublas.nrm2, the optimized cuBLAS implementation for computing l2 norms. This reduces training time about by ~24%.
Top-5 cost before this change:
             Type  Time(%)      Time     Calls       Avg       Min       Max  Name
 GPU activities:   28.49%  23.0423s    185554  124.18us  1.0550us  19.842ms  cupy_sum
                    5.68%  4.59333s     42324  108.53us  29.151us  2.6026ms  turing_fp16_s1688gemm_fp16_128x128_ldg8_f2f_tn
                    4.69%  3.78947s    202426  18.720us  1.6320us  1.1343ms  _ZN2at6native27unrolled_elementwise_kernelIZZZNS0_21copy_device_to_deviceERNS_14TensorIteratorEbENKUlvE0_clEvENKUlvE18_clEvEUlN3c104HalfEE_NS_6detail5ArrayIPcLi2EEE23TrivialOffsetCalculatorILi1EjESE_NS0_6memory12LoadWithCastILi1EEENSF_13StoreWithCastEEEviT_T0_T1_T2_T3_T4_
                    4.66%  3.77189s     31488  119.79us  4.5760us  354.82us  turing_fp16_s1688gemm_fp16_128x128_ldg8_f2f_stages_32x1_nt
                    4.51%  3.64793s     79206  46.056us  1.6000us  2.8409ms  adam

After:
             Type  Time(%)      Time     Calls       Avg       Min       Max  Name
 GPU activities:    7.95%  4.59139s     42324  108.48us  29.184us  2.5801ms  turing_fp16_s1688gemm_fp16_128x128_ldg8_f2f_tn
                    6.56%  3.78955s    202426  18.720us  1.6320us  1.1526ms  _ZN2at6native27unrolled_elementwise_kernelIZZZNS0_21copy_device_to_deviceERNS_14TensorIteratorEbENKUlvE0_clEvENKUlvE18_clEvEUlN3c104HalfEE_NS_6detail5ArrayIPcLi2EEE23TrivialOffsetCalculatorILi1EjESE_NS0_6memory12LoadWithCastILi1EEENSF_13StoreWithCastEEEviT_T0_T1_T2_T3_T4_
                    6.53%  3.77162s     31488  119.78us  4.5760us  354.98us  turing_fp16_s1688gemm_fp16_128x128_ldg8_f2f_stages_32x1_nt
                    6.25%  3.61247s     78410  46.071us  1.6640us  2.8498ms  adam
                    6.21%  3.58839s    186237  19.267us     800ns  1.1974ms  void at::native::vectorized_elementwise_kernel<int=4, at::native::AddFunctor<float>, at::detail::Array<char*, int=3>>(int, float, at::native::AddFunctor<float>)","Wow, so great to have this resolved!
I had also noticed that the gradient clipping was surprisingly expensive, but I was never able to figure out how to make it better.",True,{}
explosion/thinc,https://github.com/explosion/thinc,533,2021-09-07T07:08:55Z,2021-09-07T13:19:08Z,2021-09-07T13:19:08Z,MERGED,True,3,2,1,https://github.com/adrianeboyd,Fix default ops for get_array_ops,1,"['bug', 'feat / ops']",https://github.com/explosion/thinc/pull/533,https://github.com/adrianeboyd,1,https://github.com/explosion/thinc/pull/533,get_array_ops should default to NumpyOps rather than the current ops so that numpy arrays are handled correctly.,get_array_ops should default to NumpyOps rather than the current ops so that numpy arrays are handled correctly.,True,{}
explosion/thinc,https://github.com/explosion/thinc,533,2021-09-07T07:08:55Z,2021-09-07T13:19:08Z,2021-09-07T13:19:08Z,MERGED,True,3,2,1,https://github.com/adrianeboyd,Fix default ops for get_array_ops,1,"['bug', 'feat / ops']",https://github.com/explosion/thinc/pull/533,https://github.com/adrianeboyd,2,https://github.com/explosion/thinc/pull/533#issuecomment-914050327,get_array_ops should default to NumpyOps rather than the current ops so that numpy arrays are handled correctly.,"I was worried that defaulting to NumpyOps would cause the ops to switch from AppleOps to NumpyOps unintentionally, but this is clearly incorrect for numpy arrays and current CupyOps.
It looks like get_array_ops is only used in a fairly limited context, but we still may have to keep an eye out in the future for interactions with AppleOps.",True,{}
explosion/thinc,https://github.com/explosion/thinc,534,2021-09-07T13:20:59Z,2021-09-07T13:50:27Z,2021-09-07T13:50:27Z,MERGED,True,1,1,1,https://github.com/adrianeboyd,Set version to v8.0.10,1,[],https://github.com/explosion/thinc/pull/534,https://github.com/adrianeboyd,1,https://github.com/explosion/thinc/pull/534,,,True,{}
explosion/thinc,https://github.com/explosion/thinc,535,2021-09-09T13:58:07Z,2021-09-16T17:20:49Z,2021-09-16T17:20:52Z,MERGED,True,88,23,4,https://github.com/danieldk,"Improve wrapping in with_{debug,nvtx_range}",2,"['enhancement', 'feat / layers']",https://github.com/explosion/thinc/pull/535,https://github.com/danieldk,1,https://github.com/explosion/thinc/pull/535,"This PR combines two changes:

In wrap_model_recursive, also make sure that node references are pointing to the wrapped nodes.
Pass through dims, refs, attrs, and ops, so that these can be queried when a layer is wrapped.","This PR combines two changes:

In wrap_model_recursive, also make sure that node references are pointing to the wrapped nodes.
Pass through dims, refs, attrs, and ops, so that these can be queried when a layer is wrapped.",True,{'THUMBS_UP': ['https://github.com/svlandeg']}
explosion/thinc,https://github.com/explosion/thinc,535,2021-09-09T13:58:07Z,2021-09-16T17:20:49Z,2021-09-16T17:20:52Z,MERGED,True,88,23,4,https://github.com/danieldk,"Improve wrapping in with_{debug,nvtx_range}",2,"['enhancement', 'feat / layers']",https://github.com/explosion/thinc/pull/535,https://github.com/honnibal,2,https://github.com/explosion/thinc/pull/535#issuecomment-916670185,"This PR combines two changes:

In wrap_model_recursive, also make sure that node references are pointing to the wrapped nodes.
Pass through dims, refs, attrs, and ops, so that these can be queried when a layer is wrapped.","Looking at this now, do you think the method would be more useful if it replaced the layer anywhere in the tree?
The issue is that references can be higher up than the point at which we're making the replacement. So to do the wrapping correctly, the user has to be quite careful. They'll have to use node.walk() from the top of their model to do it reliably. So maybe we should just do this for them. This would also match the behaviour in remove_node better.
So we'd have something like this:
class Model:
    def replace_node(self, old: Model, new: Model) -> bool:
        """"""Replace a node anywhere it occurs within the model. Returns a boolean indicating
        whether the replacement was made.""""""
        seen = False
        for node in list(self.walk()):
            if node is old:
                seen = True
            else:
                node._layers = [new if child is old else child for child in node._layers]
                for name in node.ref_names:
                    if node.get_ref(name) is old:
                        node.set_ref(name, new)
        return seen",True,{}
explosion/thinc,https://github.com/explosion/thinc,535,2021-09-09T13:58:07Z,2021-09-16T17:20:49Z,2021-09-16T17:20:52Z,MERGED,True,88,23,4,https://github.com/danieldk,"Improve wrapping in with_{debug,nvtx_range}",2,"['enhancement', 'feat / layers']",https://github.com/explosion/thinc/pull/535,https://github.com/danieldk,3,https://github.com/explosion/thinc/pull/535#issuecomment-916686359,"This PR combines two changes:

In wrap_model_recursive, also make sure that node references are pointing to the wrapped nodes.
Pass through dims, refs, attrs, and ops, so that these can be queried when a layer is wrapped.","So maybe we should just do this for them. This would also match the behaviour in remove_node better.

Yeah, I agree, this would be much nicer and more consistent with the behavior of remove_node.",True,{}
explosion/thinc,https://github.com/explosion/thinc,535,2021-09-09T13:58:07Z,2021-09-16T17:20:49Z,2021-09-16T17:20:52Z,MERGED,True,88,23,4,https://github.com/danieldk,"Improve wrapping in with_{debug,nvtx_range}",2,"['enhancement', 'feat / layers']",https://github.com/explosion/thinc/pull/535,https://github.com/danieldk,4,https://github.com/explosion/thinc/pull/535#issuecomment-916702286,"This PR combines two changes:

In wrap_model_recursive, also make sure that node references are pointing to the wrapped nodes.
Pass through dims, refs, attrs, and ops, so that these can be queried when a layer is wrapped.","Updated the PR with the updated replace_node. This also simplified wrap_model_recursive a lot, which is a good sign.",True,{}
explosion/thinc,https://github.com/explosion/thinc,535,2021-09-09T13:58:07Z,2021-09-16T17:20:49Z,2021-09-16T17:20:52Z,MERGED,True,88,23,4,https://github.com/danieldk,"Improve wrapping in with_{debug,nvtx_range}",2,"['enhancement', 'feat / layers']",https://github.com/explosion/thinc/pull/535,https://github.com/honnibal,5,https://github.com/explosion/thinc/pull/535#issuecomment-921029284,"This PR combines two changes:

In wrap_model_recursive, also make sure that node references are pointing to the wrapped nodes.
Pass through dims, refs, attrs, and ops, so that these can be queried when a layer is wrapped.",Sorry I didn't see this sooner. Looks good!,True,{}
explosion/thinc,https://github.com/explosion/thinc,536,2021-09-16T08:44:40Z,2021-09-22T12:22:23Z,2021-09-22T12:34:39Z,MERGED,True,100,24,2,https://github.com/danieldk,CategoricalCrossEntropy: support missing values for int arrays,3,"['enhancement', 'feat / ops']",https://github.com/explosion/thinc/pull/536,https://github.com/danieldk,1,https://github.com/explosion/thinc/pull/536,"CategoricalCrossEntropy provides a missing_value option that can be used
to mask instances that have a special label. However, this functionality
only works with non-integer categorical labels.
This change adds support for masking missing values for int lists and xp
arrays.","CategoricalCrossEntropy provides a missing_value option that can be used
to mask instances that have a special label. However, this functionality
only works with non-integer categorical labels.
This change adds support for masking missing values for int lists and xp
arrays.",True,{'THUMBS_UP': ['https://github.com/svlandeg']}
explosion/thinc,https://github.com/explosion/thinc,537,2021-09-21T07:38:59Z,2021-09-27T11:36:11Z,2021-09-27T11:36:11Z,MERGED,True,103,3,2,https://github.com/danieldk,Fix replace_node on nodes with indirect node refs,3,['bug'],https://github.com/explosion/thinc/pull/537,https://github.com/danieldk,1,https://github.com/explosion/thinc/pull/537,"Model.replace_node failed if a node n contained a reference to another node m, when m only occurs in one of the (grand)children of n.  Since replace_node used the breadth-first Model.walk method, the reference to m would be replaced before m is replaced in the (grand)children of n. However, this fails because set_ref verifies that the a new node reference occurs in the graph.
This change resolves this issue by replacing nodes using a depth-first search.","Model.replace_node failed if a node n contained a reference to another node m, when m only occurs in one of the (grand)children of n.  Since replace_node used the breadth-first Model.walk method, the reference to m would be replaced before m is replaced in the (grand)children of n. However, this fails because set_ref verifies that the a new node reference occurs in the graph.
This change resolves this issue by replacing nodes using a depth-first search.",True,{'THUMBS_UP': ['https://github.com/svlandeg']}
explosion/thinc,https://github.com/explosion/thinc,538,2021-09-28T11:19:22Z,2021-09-30T12:11:55Z,2021-09-30T12:13:26Z,CLOSED,False,1,1,1,https://github.com/danieldk,Fix name of the layer created with with_nvtx_range,1,[],https://github.com/explosion/thinc/pull/538,https://github.com/danieldk,1,https://github.com/explosion/thinc/pull/538,"The layer was named debug(...), should be nvtx_range(...). This is
only a cosmetic change, but looks nicer when debugging models.","The layer was named debug(...), should be nvtx_range(...). This is
only a cosmetic change, but looks nicer when debugging models.",True,{}
explosion/thinc,https://github.com/explosion/thinc,538,2021-09-28T11:19:22Z,2021-09-30T12:11:55Z,2021-09-30T12:13:26Z,CLOSED,False,1,1,1,https://github.com/danieldk,Fix name of the layer created with with_nvtx_range,1,[],https://github.com/explosion/thinc/pull/538,https://github.com/danieldk,2,https://github.com/explosion/thinc/pull/538#issuecomment-931264640,"The layer was named debug(...), should be nvtx_range(...). This is
only a cosmetic change, but looks nicer when debugging models.","I will close this, since the next NVTX PR will make this one unnecessary.",True,{}
explosion/thinc,https://github.com/explosion/thinc,539,2021-09-30T12:29:49Z,2021-10-06T09:26:27Z,2021-10-06T09:28:59Z,MERGED,True,100,60,4,https://github.com/danieldk,"Wrap layer callbacks in with_{debug,nvtx_range}",2,"['enhancement', 'feat / layers']",https://github.com/explosion/thinc/pull/539,https://github.com/danieldk,1,https://github.com/explosion/thinc/pull/539,"Before this change, with_{debug,nvtx_range} instrumented a Model's init/forward/backprop by wrapping it in another Model. However, this turns out to be tricky, since in some cases we have to propagate information from the wrapped nodes (e.g. references) while in other cases we shouldn't.
With this change, we instead add instumentation by wrapping a Model's init and forward callbacks. This makes the nodes in the graph appear as they were before wrapping, but adds behavior in the wrapped callbacks.
The with_{debug,nvtx_range} still returns Model to maintain compatibility with the existing API. Though the returned Model is the Model that is wrapped.","Before this change, with_{debug,nvtx_range} instrumented a Model's init/forward/backprop by wrapping it in another Model. However, this turns out to be tricky, since in some cases we have to propagate information from the wrapped nodes (e.g. references) while in other cases we shouldn't.
With this change, we instead add instumentation by wrapping a Model's init and forward callbacks. This makes the nodes in the graph appear as they were before wrapping, but adds behavior in the wrapped callbacks.
The with_{debug,nvtx_range} still returns Model to maintain compatibility with the existing API. Though the returned Model is the Model that is wrapped.",True,{}
explosion/thinc,https://github.com/explosion/thinc,539,2021-09-30T12:29:49Z,2021-10-06T09:26:27Z,2021-10-06T09:28:59Z,MERGED,True,100,60,4,https://github.com/danieldk,"Wrap layer callbacks in with_{debug,nvtx_range}",2,"['enhancement', 'feat / layers']",https://github.com/explosion/thinc/pull/539,https://github.com/honnibal,2,https://github.com/explosion/thinc/pull/539#issuecomment-935830558,"Before this change, with_{debug,nvtx_range} instrumented a Model's init/forward/backprop by wrapping it in another Model. However, this turns out to be tricky, since in some cases we have to propagate information from the wrapped nodes (e.g. references) while in other cases we shouldn't.
With this change, we instead add instumentation by wrapping a Model's init and forward callbacks. This makes the nodes in the graph appear as they were before wrapping, but adds behavior in the wrapped callbacks.
The with_{debug,nvtx_range} still returns Model to maintain compatibility with the existing API. Though the returned Model is the Model that is wrapped.","We tried to avoid modifying the internal _func here, but this way really is better. I'll consider adding a replace_function method later -- but for now, let's get this merged :)",True,{}
explosion/thinc,https://github.com/explosion/thinc,540,2021-10-05T09:45:58Z,2021-10-13T10:27:22Z,2021-10-13T10:27:37Z,MERGED,True,44,13,4,https://github.com/danieldk,get_ops: support returning the best available CPU backend,7,"['enhancement', 'feat / ops']",https://github.com/explosion/thinc/pull/540,https://github.com/danieldk,1,https://github.com/explosion/thinc/pull/540,"When using {get,use}_ops, we often do no want a particular CPU backend, but the best CPU backend that is available (e.g. Accelerate-based thinc ops on a Mac).
This change adds a special op value cpu that returns the best available CPU backend. E.g.:
with use_ops(""cpu""):
   # ...","When using {get,use}_ops, we often do no want a particular CPU backend, but the best CPU backend that is available (e.g. Accelerate-based thinc ops on a Mac).
This change adds a special op value cpu that returns the best available CPU backend. E.g.:
with use_ops(""cpu""):
   # ...",True,{'THUMBS_UP': ['https://github.com/honnibal']}
explosion/thinc,https://github.com/explosion/thinc,540,2021-10-05T09:45:58Z,2021-10-13T10:27:22Z,2021-10-13T10:27:37Z,MERGED,True,44,13,4,https://github.com/danieldk,get_ops: support returning the best available CPU backend,7,"['enhancement', 'feat / ops']",https://github.com/explosion/thinc/pull/540,https://github.com/adrianeboyd,2,https://github.com/explosion/thinc/pull/540#issuecomment-942003567,"When using {get,use}_ops, we often do no want a particular CPU backend, but the best CPU backend that is available (e.g. Accelerate-based thinc ops on a Mac).
This change adds a special op value cpu that returns the best available CPU backend. E.g.:
with use_ops(""cpu""):
   # ...",Let's add some CI tests for this...,True,{}
explosion/thinc,https://github.com/explosion/thinc,541,2021-10-07T05:26:46Z,2021-11-08T07:50:00Z,2021-11-08T07:50:00Z,MERGED,True,19,0,1,https://github.com/polm,Add citation file,3,['docs'],https://github.com/explosion/thinc/pull/541,https://github.com/polm,1,https://github.com/explosion/thinc/pull/541,"This adds a new-style citation file for the repo so people have something to cite, as requested in #455.
For the time being I just copied the author list from spaCy, not sure if that should be changed.","This adds a new-style citation file for the repo so people have something to cite, as requested in #455.
For the time being I just copied the author list from spaCy, not sure if that should be changed.",True,{}
explosion/thinc,https://github.com/explosion/thinc,541,2021-10-07T05:26:46Z,2021-11-08T07:50:00Z,2021-11-08T07:50:00Z,MERGED,True,19,0,1,https://github.com/polm,Add citation file,3,['docs'],https://github.com/explosion/thinc/pull/541,https://github.com/polm,2,https://github.com/explosion/thinc/pull/541#issuecomment-960475907,"This adds a new-style citation file for the repo so people have something to cite, as requested in #455.
For the time being I just copied the author list from spaCy, not sure if that should be changed.","Looks like tests are failing, though I think that's just due to the state at the time I started the branch. (I force-pushed because some other commits got pulled in somehow.)",True,{}
explosion/thinc,https://github.com/explosion/thinc,541,2021-10-07T05:26:46Z,2021-11-08T07:50:00Z,2021-11-08T07:50:00Z,MERGED,True,19,0,1,https://github.com/polm,Add citation file,3,['docs'],https://github.com/explosion/thinc/pull/541,https://github.com/svlandeg,3,https://github.com/explosion/thinc/pull/541#issuecomment-961217540,"This adds a new-style citation file for the repo so people have something to cite, as requested in #455.
For the time being I just copied the author list from spaCy, not sure if that should be changed.","Could you merge in upstream/master and push again? I'm hoping #560 will have solved the errors. It's obviously not this PR, but I really don't like merging when the CI isn't green ;-)",True,{'THUMBS_UP': ['https://github.com/polm']}
explosion/thinc,https://github.com/explosion/thinc,542,2021-10-11T13:39:37Z,2021-10-11T20:28:33Z,2021-10-11T20:28:35Z,MERGED,True,2,1,2,https://github.com/svlandeg,Fix failing cov CI tests,2,"['tests', 'third-party']",https://github.com/explosion/thinc/pull/542,https://github.com/svlandeg,1,https://github.com/explosion/thinc/pull/542,"Limiting coverage to <6.0.0 as a recent release broke the test suite for Windows:
INTERNALERROR> Traceback (most recent call last):
INTERNALERROR>   File ""...\lib\site-packages\_pytest\main.py"", line 269, in wrap_session
INTERNALERROR>     session.exitstatus = doit(config, session) or 0
INTERNALERROR>   File ""...\lib\site-packages\_pytest\main.py"", line 323, in _main
INTERNALERROR>     config.hook.pytest_runtestloop(session=session)
INTERNALERROR>   File ""...\lib\site-packages\pluggy\_hooks.py"", line 265, in __call__
INTERNALERROR>     return self._hookexec(self.name, self.get_hookimpls(), kwargs, firstresult)
INTERNALERROR>   File ""...\lib\site-packages\pluggy\_manager.py"", line 80, in _hookexec
INTERNALERROR>     return self._inner_hookexec(hook_name, methods, kwargs, firstresult)
INTERNALERROR>   File ""...\lib\site-packages\pluggy\_callers.py"", line 55, in _multicall
INTERNALERROR>     gen.send(outcome)
INTERNALERROR>   File ""...\lib\site-packages\pytest_cov\plugin.py"", line 229, in pytest_runtestloop
INTERNALERROR>     self.cov_controller.finish()
INTERNALERROR>   File ""...\lib\site-packages\pytest_cov\engine.py"", line 171, in finish
INTERNALERROR>     self.cov.stop()
INTERNALERROR>   File ""...\lib\site-packages\coverage\control.py"", line 721, in combine
INTERNALERROR>     message=self._message,
INTERNALERROR>   File ""...\lib\site-packages\coverage\data.py"", line 123, in combine_parallel_data
INTERNALERROR>     message(f""Combined data file {os.path.relpath(f)}"")
INTERNALERROR>   File ""...\lib\ntpath.py"", line 584, in relpath
INTERNALERROR>     path_drive, start_drive))
INTERNALERROR> ValueError: path is on mount 'D:', start on mount 'C:'","Limiting coverage to <6.0.0 as a recent release broke the test suite for Windows:
INTERNALERROR> Traceback (most recent call last):
INTERNALERROR>   File ""...\lib\site-packages\_pytest\main.py"", line 269, in wrap_session
INTERNALERROR>     session.exitstatus = doit(config, session) or 0
INTERNALERROR>   File ""...\lib\site-packages\_pytest\main.py"", line 323, in _main
INTERNALERROR>     config.hook.pytest_runtestloop(session=session)
INTERNALERROR>   File ""...\lib\site-packages\pluggy\_hooks.py"", line 265, in __call__
INTERNALERROR>     return self._hookexec(self.name, self.get_hookimpls(), kwargs, firstresult)
INTERNALERROR>   File ""...\lib\site-packages\pluggy\_manager.py"", line 80, in _hookexec
INTERNALERROR>     return self._inner_hookexec(hook_name, methods, kwargs, firstresult)
INTERNALERROR>   File ""...\lib\site-packages\pluggy\_callers.py"", line 55, in _multicall
INTERNALERROR>     gen.send(outcome)
INTERNALERROR>   File ""...\lib\site-packages\pytest_cov\plugin.py"", line 229, in pytest_runtestloop
INTERNALERROR>     self.cov_controller.finish()
INTERNALERROR>   File ""...\lib\site-packages\pytest_cov\engine.py"", line 171, in finish
INTERNALERROR>     self.cov.stop()
INTERNALERROR>   File ""...\lib\site-packages\coverage\control.py"", line 721, in combine
INTERNALERROR>     message=self._message,
INTERNALERROR>   File ""...\lib\site-packages\coverage\data.py"", line 123, in combine_parallel_data
INTERNALERROR>     message(f""Combined data file {os.path.relpath(f)}"")
INTERNALERROR>   File ""...\lib\ntpath.py"", line 584, in relpath
INTERNALERROR>     path_drive, start_drive))
INTERNALERROR> ValueError: path is on mount 'D:', start on mount 'C:'",True,{}
explosion/thinc,https://github.com/explosion/thinc,544,2021-10-18T14:35:18Z,2021-10-18T15:11:53Z,2021-10-18T15:11:56Z,MERGED,True,1,1,1,https://github.com/svlandeg,bump version to 8.0.11,1,[],https://github.com/explosion/thinc/pull/544,https://github.com/svlandeg,1,https://github.com/explosion/thinc/pull/544,,,True,{}
explosion/thinc,https://github.com/explosion/thinc,545,2021-10-19T08:47:49Z,2021-10-19T09:59:28Z,2021-10-19T09:59:28Z,MERGED,True,263,4,3,https://github.com/danieldk,Describe `Model.walk` iteration orders,3,['docs'],https://github.com/explosion/thinc/pull/545,https://github.com/danieldk,1,https://github.com/explosion/thinc/pull/545,,,True,{'HOORAY': ['https://github.com/svlandeg']}
explosion/thinc,https://github.com/explosion/thinc,546,2021-10-19T10:03:58Z,2021-10-19T21:42:56Z,2021-10-19T21:42:56Z,MERGED,True,480,118,16,https://github.com/svlandeg,Build docs for 8.0.11,8,['docs'],https://github.com/explosion/thinc/pull/546,https://github.com/svlandeg,1,https://github.com/explosion/thinc/pull/546,updating the docs branch for the upcoming release,updating the docs branch for the upcoming release,True,{}
explosion/thinc,https://github.com/explosion/thinc,546,2021-10-19T10:03:58Z,2021-10-19T21:42:56Z,2021-10-19T21:42:56Z,MERGED,True,480,118,16,https://github.com/svlandeg,Build docs for 8.0.11,8,['docs'],https://github.com/explosion/thinc/pull/546,https://github.com/svlandeg,2,https://github.com/explosion/thinc/pull/546#issuecomment-947102739,updating the docs branch for the upcoming release,@explosion-bot please test_gpu,True,"{'EYES': ['https://github.com/explosion-bot'], 'ROCKET': ['https://github.com/explosion-bot']}"
explosion/thinc,https://github.com/explosion/thinc,546,2021-10-19T10:03:58Z,2021-10-19T21:42:56Z,2021-10-19T21:42:56Z,MERGED,True,480,118,16,https://github.com/svlandeg,Build docs for 8.0.11,8,['docs'],https://github.com/explosion/thinc/pull/546,https://github.com/explosion-bot,3,https://github.com/explosion/thinc/pull/546#issuecomment-947103136,updating the docs branch for the upcoming release," Successfully triggered build on Buildkite

URL: https://buildkite.com/explosion-ai/thinc-gpu-test-suite/builds/9",True,"{'EYES': ['https://github.com/svlandeg'], 'HOORAY': ['https://github.com/svlandeg']}"
explosion/thinc,https://github.com/explosion/thinc,547,2021-10-19T11:28:01Z,2021-10-19T14:11:40Z,2021-10-19T14:11:40Z,CLOSED,False,43,39,2,https://github.com/danieldk,Fix two GPU tests,3,['bug'],https://github.com/explosion/thinc/pull/547,https://github.com/danieldk,1,https://github.com/explosion/thinc/pull/547,"The test_model_gpu function used prefer_gpu. However, this changed default ops to CupyOps beyond the scope of this function. This change uses use_ops instead to scope use of CupyOps.
MXNet models require initialization using a forward pass before parameters can be accessed.","The test_model_gpu function used prefer_gpu. However, this changed default ops to CupyOps beyond the scope of this function. This change uses use_ops instead to scope use of CupyOps.
MXNet models require initialization using a forward pass before parameters can be accessed.",True,{'THUMBS_UP': ['https://github.com/svlandeg']}
explosion/thinc,https://github.com/explosion/thinc,548,2021-10-19T14:14:45Z,2021-10-19T14:54:14Z,2021-10-19T16:47:09Z,MERGED,True,43,39,2,https://github.com/danieldk,Fix two GPU tests,3,[],https://github.com/explosion/thinc/pull/548,https://github.com/danieldk,1,https://github.com/explosion/thinc/pull/548,"The test_model_gpu function used prefer_gpu. However, this changed default ops to CupyOps beyond the scope of this function. This change uses use_ops instead to scope use of CupyOps.
MXNet models require initialization using a forward pass before parameters can be accessed.

(New PR to try to trigger GPU CI.)","The test_model_gpu function used prefer_gpu. However, this changed default ops to CupyOps beyond the scope of this function. This change uses use_ops instead to scope use of CupyOps.
MXNet models require initialization using a forward pass before parameters can be accessed.

(New PR to try to trigger GPU CI.)",True,{'THUMBS_UP': ['https://github.com/svlandeg']}
explosion/thinc,https://github.com/explosion/thinc,548,2021-10-19T14:14:45Z,2021-10-19T14:54:14Z,2021-10-19T16:47:09Z,MERGED,True,43,39,2,https://github.com/danieldk,Fix two GPU tests,3,[],https://github.com/explosion/thinc/pull/548,https://github.com/svlandeg,2,https://github.com/explosion/thinc/pull/548#issuecomment-946806838,"The test_model_gpu function used prefer_gpu. However, this changed default ops to CupyOps beyond the scope of this function. This change uses use_ops instead to scope use of CupyOps.
MXNet models require initialization using a forward pass before parameters can be accessed.

(New PR to try to trigger GPU CI.)","I'm going to merge this in and cherry-pick to the open docs PR, in the hope that that reruns the GPU tests there.",True,{}
explosion/thinc,https://github.com/explosion/thinc,548,2021-10-19T14:14:45Z,2021-10-19T14:54:14Z,2021-10-19T16:47:09Z,MERGED,True,43,39,2,https://github.com/danieldk,Fix two GPU tests,3,[],https://github.com/explosion/thinc/pull/548,https://github.com/ryndaniels,3,https://github.com/explosion/thinc/pull/548#issuecomment-946909318,"The test_model_gpu function used prefer_gpu. However, this changed default ops to CupyOps beyond the scope of this function. This change uses use_ops instead to scope use of CupyOps.
MXNet models require initialization using a forward pass before parameters can be accessed.

(New PR to try to trigger GPU CI.)",@explosion-bot please test_gpu,True,"{'EYES': ['https://github.com/explosion-bot'], 'ROCKET': ['https://github.com/explosion-bot']}"
explosion/thinc,https://github.com/explosion/thinc,548,2021-10-19T14:14:45Z,2021-10-19T14:54:14Z,2021-10-19T16:47:09Z,MERGED,True,43,39,2,https://github.com/danieldk,Fix two GPU tests,3,[],https://github.com/explosion/thinc/pull/548,https://github.com/explosion-bot,4,https://github.com/explosion/thinc/pull/548#issuecomment-946909769,"The test_model_gpu function used prefer_gpu. However, this changed default ops to CupyOps beyond the scope of this function. This change uses use_ops instead to scope use of CupyOps.
MXNet models require initialization using a forward pass before parameters can be accessed.

(New PR to try to trigger GPU CI.)"," Successfully triggered build on Buildkite

URL: https://buildkite.com/explosion-ai/thinc-gpu-test-suite/builds/8",True,"{'HOORAY': ['https://github.com/svlandeg', 'https://github.com/danieldk']}"
explosion/thinc,https://github.com/explosion/thinc,549,2021-10-19T14:18:05Z,2021-10-19T14:52:56Z,2021-10-19T14:53:01Z,CLOSED,False,1,1,1,https://github.com/svlandeg,test commit,1,[],https://github.com/explosion/thinc/pull/549,https://github.com/svlandeg,1,https://github.com/explosion/thinc/pull/549,Creating test PR to see what the CI will do...,Creating test PR to see what the CI will do...,True,{}
explosion/thinc,https://github.com/explosion/thinc,550,2021-10-19T16:09:32Z,2021-10-19T16:44:37Z,2021-10-19T21:02:38Z,MERGED,True,27,0,1,https://github.com/ryndaniels,Install the explosion-bot with the test_gpu command,1,[],https://github.com/explosion/thinc/pull/550,https://github.com/ryndaniels,1,https://github.com/explosion/thinc/pull/550,,,True,"{'ROCKET': ['https://github.com/svlandeg', 'https://github.com/danieldk'], 'HOORAY': ['https://github.com/svlandeg', 'https://github.com/danieldk']}"
explosion/thinc,https://github.com/explosion/thinc,551,2021-10-22T09:02:46Z,2021-10-26T09:35:27Z,2021-10-26T09:42:44Z,MERGED,True,22,6,5,https://github.com/adrianeboyd,"Set version to v8.0.12, extend to python 3.10",10,[],https://github.com/explosion/thinc/pull/551,https://github.com/adrianeboyd,1,https://github.com/explosion/thinc/pull/551,,,True,{}
explosion/thinc,https://github.com/explosion/thinc,551,2021-10-22T09:02:46Z,2021-10-26T09:35:27Z,2021-10-26T09:42:44Z,MERGED,True,22,6,5,https://github.com/adrianeboyd,"Set version to v8.0.12, extend to python 3.10",10,[],https://github.com/explosion/thinc/pull/551,https://github.com/adrianeboyd,2,https://github.com/explosion/thinc/pull/551#issuecomment-951765440,,"(As a note, the reason to exclude mypy with an environment marker is so that it's skipped during tests in wheelwright because the pure python version is too slow.)",True,{}
explosion/thinc,https://github.com/explosion/thinc,552,2021-10-25T14:13:35Z,2021-11-05T11:14:22Z,2021-11-08T09:07:55Z,MERGED,True,227,69,8,https://github.com/danieldk,mixed-precision: do not backprop Inf/NaN out of PyTorch layers,5,['enhancement'],https://github.com/explosion/thinc/pull/552,https://github.com/danieldk,1,https://github.com/explosion/thinc/pull/552,"With mixed-precision enabled in a PyTorchWrapper, an update is skipped and the gradient scale is adjusted when one or more of the gradients is Inf/NaN. However, Inf/NaN gradients were still backpropagated into preceding Thinc layers.
This change fixes this issue by clearing gradients of inputs to PyTorchWrapper if Inf/NaN occurs in any of the gradients.
Also:

Change PyTorchGradScaler.scale to accept Tensor besides Sequence[Tensor].
Expose PyTorchWrapper_v2 in the API
Add a unit test for mixed-precision training.


Set to draft because:

 Still needs testing on a transformer model. Similar accuracies in a tagger -> morphologizer pipeline 
 Needs to be rebased after #553  and #554. ","With mixed-precision enabled in a PyTorchWrapper, an update is skipped and the gradient scale is adjusted when one or more of the gradients is Inf/NaN. However, Inf/NaN gradients were still backpropagated into preceding Thinc layers.
This change fixes this issue by clearing gradients of inputs to PyTorchWrapper if Inf/NaN occurs in any of the gradients.
Also:

Change PyTorchGradScaler.scale to accept Tensor besides Sequence[Tensor].
Expose PyTorchWrapper_v2 in the API
Add a unit test for mixed-precision training.


Set to draft because:

 Still needs testing on a transformer model. Similar accuracies in a tagger -> morphologizer pipeline 
 Needs to be rebased after #553  and #554. ",True,{}
explosion/thinc,https://github.com/explosion/thinc,552,2021-10-25T14:13:35Z,2021-11-05T11:14:22Z,2021-11-08T09:07:55Z,MERGED,True,227,69,8,https://github.com/danieldk,mixed-precision: do not backprop Inf/NaN out of PyTorch layers,5,['enhancement'],https://github.com/explosion/thinc/pull/552,https://github.com/danieldk,2,https://github.com/explosion/thinc/pull/552#issuecomment-954547645,"With mixed-precision enabled in a PyTorchWrapper, an update is skipped and the gradient scale is adjusted when one or more of the gradients is Inf/NaN. However, Inf/NaN gradients were still backpropagated into preceding Thinc layers.
This change fixes this issue by clearing gradients of inputs to PyTorchWrapper if Inf/NaN occurs in any of the gradients.
Also:

Change PyTorchGradScaler.scale to accept Tensor besides Sequence[Tensor].
Expose PyTorchWrapper_v2 in the API
Add a unit test for mixed-precision training.


Set to draft because:

 Still needs testing on a transformer model. Similar accuracies in a tagger -> morphologizer pipeline 
 Needs to be rebased after #553  and #554. ",@explosion-bot please test_gpu,True,"{'EYES': ['https://github.com/explosion-bot'], 'ROCKET': ['https://github.com/explosion-bot']}"
explosion/thinc,https://github.com/explosion/thinc,552,2021-10-25T14:13:35Z,2021-11-05T11:14:22Z,2021-11-08T09:07:55Z,MERGED,True,227,69,8,https://github.com/danieldk,mixed-precision: do not backprop Inf/NaN out of PyTorch layers,5,['enhancement'],https://github.com/explosion/thinc/pull/552,https://github.com/explosion-bot,3,https://github.com/explosion/thinc/pull/552#issuecomment-954547956,"With mixed-precision enabled in a PyTorchWrapper, an update is skipped and the gradient scale is adjusted when one or more of the gradients is Inf/NaN. However, Inf/NaN gradients were still backpropagated into preceding Thinc layers.
This change fixes this issue by clearing gradients of inputs to PyTorchWrapper if Inf/NaN occurs in any of the gradients.
Also:

Change PyTorchGradScaler.scale to accept Tensor besides Sequence[Tensor].
Expose PyTorchWrapper_v2 in the API
Add a unit test for mixed-precision training.


Set to draft because:

 Still needs testing on a transformer model. Similar accuracies in a tagger -> morphologizer pipeline 
 Needs to be rebased after #553  and #554. "," Successfully triggered build on Buildkite

URL: https://buildkite.com/explosion-ai/thinc-gpu-test-suite/builds/12",True,{'HOORAY': ['https://github.com/svlandeg']}
explosion/thinc,https://github.com/explosion/thinc,552,2021-10-25T14:13:35Z,2021-11-05T11:14:22Z,2021-11-08T09:07:55Z,MERGED,True,227,69,8,https://github.com/danieldk,mixed-precision: do not backprop Inf/NaN out of PyTorch layers,5,['enhancement'],https://github.com/explosion/thinc/pull/552,https://github.com/honnibal,4,https://github.com/explosion/thinc/pull/552#issuecomment-958059181,"With mixed-precision enabled in a PyTorchWrapper, an update is skipped and the gradient scale is adjusted when one or more of the gradients is Inf/NaN. However, Inf/NaN gradients were still backpropagated into preceding Thinc layers.
This change fixes this issue by clearing gradients of inputs to PyTorchWrapper if Inf/NaN occurs in any of the gradients.
Also:

Change PyTorchGradScaler.scale to accept Tensor besides Sequence[Tensor].
Expose PyTorchWrapper_v2 in the API
Add a unit test for mixed-precision training.


Set to draft because:

 Still needs testing on a transformer model. Similar accuracies in a tagger -> morphologizer pipeline 
 Needs to be rebased after #553  and #554. ","Nice! Looks good.
Btw you could use Hypothesis to test a range of input tensors instead of one arbitrary one if you wanted. I think the test looks fine as it is though.",True,{}
explosion/thinc,https://github.com/explosion/thinc,552,2021-10-25T14:13:35Z,2021-11-05T11:14:22Z,2021-11-08T09:07:55Z,MERGED,True,227,69,8,https://github.com/danieldk,mixed-precision: do not backprop Inf/NaN out of PyTorch layers,5,['enhancement'],https://github.com/explosion/thinc/pull/552,https://github.com/danieldk,5,https://github.com/explosion/thinc/pull/552#issuecomment-958099971,"With mixed-precision enabled in a PyTorchWrapper, an update is skipped and the gradient scale is adjusted when one or more of the gradients is Inf/NaN. However, Inf/NaN gradients were still backpropagated into preceding Thinc layers.
This change fixes this issue by clearing gradients of inputs to PyTorchWrapper if Inf/NaN occurs in any of the gradients.
Also:

Change PyTorchGradScaler.scale to accept Tensor besides Sequence[Tensor].
Expose PyTorchWrapper_v2 in the API
Add a unit test for mixed-precision training.


Set to draft because:

 Still needs testing on a transformer model. Similar accuracies in a tagger -> morphologizer pipeline 
 Needs to be rebased after #553  and #554. ","Btw you could use Hypothesis to test a range of input tensors instead of one arbitrary one if you wanted. I think the test looks fine as it is though.

That's a good idea, I'll adjust the test to do that.",True,{}
explosion/thinc,https://github.com/explosion/thinc,552,2021-10-25T14:13:35Z,2021-11-05T11:14:22Z,2021-11-08T09:07:55Z,MERGED,True,227,69,8,https://github.com/danieldk,mixed-precision: do not backprop Inf/NaN out of PyTorch layers,5,['enhancement'],https://github.com/explosion/thinc/pull/552,https://github.com/danieldk,6,https://github.com/explosion/thinc/pull/552#issuecomment-961244663,"With mixed-precision enabled in a PyTorchWrapper, an update is skipped and the gradient scale is adjusted when one or more of the gradients is Inf/NaN. However, Inf/NaN gradients were still backpropagated into preceding Thinc layers.
This change fixes this issue by clearing gradients of inputs to PyTorchWrapper if Inf/NaN occurs in any of the gradients.
Also:

Change PyTorchGradScaler.scale to accept Tensor besides Sequence[Tensor].
Expose PyTorchWrapper_v2 in the API
Add a unit test for mixed-precision training.


Set to draft because:

 Still needs testing on a transformer model. Similar accuracies in a tagger -> morphologizer pipeline 
 Needs to be rebased after #553  and #554. ",@explosion-bot please test_gpu,True,"{'EYES': ['https://github.com/explosion-bot'], 'ROCKET': ['https://github.com/explosion-bot']}"
explosion/thinc,https://github.com/explosion/thinc,552,2021-10-25T14:13:35Z,2021-11-05T11:14:22Z,2021-11-08T09:07:55Z,MERGED,True,227,69,8,https://github.com/danieldk,mixed-precision: do not backprop Inf/NaN out of PyTorch layers,5,['enhancement'],https://github.com/explosion/thinc/pull/552,https://github.com/explosion-bot,7,https://github.com/explosion/thinc/pull/552#issuecomment-961245037,"With mixed-precision enabled in a PyTorchWrapper, an update is skipped and the gradient scale is adjusted when one or more of the gradients is Inf/NaN. However, Inf/NaN gradients were still backpropagated into preceding Thinc layers.
This change fixes this issue by clearing gradients of inputs to PyTorchWrapper if Inf/NaN occurs in any of the gradients.
Also:

Change PyTorchGradScaler.scale to accept Tensor besides Sequence[Tensor].
Expose PyTorchWrapper_v2 in the API
Add a unit test for mixed-precision training.


Set to draft because:

 Still needs testing on a transformer model. Similar accuracies in a tagger -> morphologizer pipeline 
 Needs to be rebased after #553  and #554. "," Successfully triggered build on Buildkite

URL: https://buildkite.com/explosion-ai/thinc-gpu-test-suite/builds/14",True,{}
explosion/thinc,https://github.com/explosion/thinc,553,2021-10-26T08:56:41Z,2021-10-28T08:18:10Z,2021-10-29T08:22:27Z,MERGED,True,22,9,2,https://github.com/danieldk,"{set,use}_ops: switch PyTorch Tensor type",1,[],https://github.com/explosion/thinc/pull/553,https://github.com/danieldk,1,https://github.com/explosion/thinc/pull/553,"{set,use}_ops did not change the default PyTorch Tensor type, whereas require_{cpu, gpu} do. As a result, these set/use functions require more work to use correctly with PyTorch
This change rectifies this by also changing the default PyTorch Tensor type when {set,use}_ops are used.","{set,use}_ops did not change the default PyTorch Tensor type, whereas require_{cpu, gpu} do. As a result, these set/use functions require more work to use correctly with PyTorch
This change rectifies this by also changing the default PyTorch Tensor type when {set,use}_ops are used.",True,{}
explosion/thinc,https://github.com/explosion/thinc,553,2021-10-26T08:56:41Z,2021-10-28T08:18:10Z,2021-10-29T08:22:27Z,MERGED,True,22,9,2,https://github.com/danieldk,"{set,use}_ops: switch PyTorch Tensor type",1,[],https://github.com/explosion/thinc/pull/553,https://github.com/danieldk,2,https://github.com/explosion/thinc/pull/553#issuecomment-951767487,"{set,use}_ops did not change the default PyTorch Tensor type, whereas require_{cpu, gpu} do. As a result, these set/use functions require more work to use correctly with PyTorch
This change rectifies this by also changing the default PyTorch Tensor type when {set,use}_ops are used.","I am not sure if we want this, since set_default_tensor_type changes the global tensor type, not the type per thread. On the other hand, I found that using use_ops(""cupy"") with PyTorch does not work correctly, without this change. Thinc tensors get allocated on GPU and PyTorch tensors on CPU.",True,{}
explosion/thinc,https://github.com/explosion/thinc,553,2021-10-26T08:56:41Z,2021-10-28T08:18:10Z,2021-10-29T08:22:27Z,MERGED,True,22,9,2,https://github.com/danieldk,"{set,use}_ops: switch PyTorch Tensor type",1,[],https://github.com/explosion/thinc/pull/553,https://github.com/adrianeboyd,3,https://github.com/explosion/thinc/pull/553#issuecomment-951809644,"{set,use}_ops did not change the default PyTorch Tensor type, whereas require_{cpu, gpu} do. As a result, these set/use functions require more work to use correctly with PyTorch
This change rectifies this by also changing the default PyTorch Tensor type when {set,use}_ops are used.",@explosion-bot please test_gpu,True,"{'EYES': ['https://github.com/explosion-bot'], 'ROCKET': ['https://github.com/explosion-bot']}"
explosion/thinc,https://github.com/explosion/thinc,553,2021-10-26T08:56:41Z,2021-10-28T08:18:10Z,2021-10-29T08:22:27Z,MERGED,True,22,9,2,https://github.com/danieldk,"{set,use}_ops: switch PyTorch Tensor type",1,[],https://github.com/explosion/thinc/pull/553,https://github.com/explosion-bot,4,https://github.com/explosion/thinc/pull/553#issuecomment-951809985,"{set,use}_ops did not change the default PyTorch Tensor type, whereas require_{cpu, gpu} do. As a result, these set/use functions require more work to use correctly with PyTorch
This change rectifies this by also changing the default PyTorch Tensor type when {set,use}_ops are used."," Successfully triggered build on Buildkite

URL: https://buildkite.com/explosion-ai/thinc-gpu-test-suite/builds/10",True,{}
explosion/thinc,https://github.com/explosion/thinc,554,2021-10-26T10:43:43Z,2021-10-26T14:55:41Z,2021-10-26T14:55:42Z,MERGED,True,4,2,1,https://github.com/danieldk,use_ops: always restore the original ops,1,[],https://github.com/explosion/thinc/pull/554,https://github.com/danieldk,1,https://github.com/explosion/thinc/pull/554,"If an exception was raised in a context, the original ops were not
restored. This is particularly annoying in unit tests, where an
exception in a single test would lead to failures in many other tests
because the default NumPy ops are not restored.","If an exception was raised in a context, the original ops were not
restored. This is particularly annoying in unit tests, where an
exception in a single test would lead to failures in many other tests
because the default NumPy ops are not restored.",True,{}
explosion/thinc,https://github.com/explosion/thinc,554,2021-10-26T10:43:43Z,2021-10-26T14:55:41Z,2021-10-26T14:55:42Z,MERGED,True,4,2,1,https://github.com/danieldk,use_ops: always restore the original ops,1,[],https://github.com/explosion/thinc/pull/554,https://github.com/danieldk,2,https://github.com/explosion/thinc/pull/554#issuecomment-951811429,"If an exception was raised in a context, the original ops were not
restored. This is particularly annoying in unit tests, where an
exception in a single test would lead to failures in many other tests
because the default NumPy ops are not restored.",@explosion-bot please test_gpu,True,"{'EYES': ['https://github.com/explosion-bot'], 'ROCKET': ['https://github.com/explosion-bot']}"
explosion/thinc,https://github.com/explosion/thinc,554,2021-10-26T10:43:43Z,2021-10-26T14:55:41Z,2021-10-26T14:55:42Z,MERGED,True,4,2,1,https://github.com/danieldk,use_ops: always restore the original ops,1,[],https://github.com/explosion/thinc/pull/554,https://github.com/explosion-bot,3,https://github.com/explosion/thinc/pull/554#issuecomment-951811712,"If an exception was raised in a context, the original ops were not
restored. This is particularly annoying in unit tests, where an
exception in a single test would lead to failures in many other tests
because the default NumPy ops are not restored."," Successfully triggered build on Buildkite

URL: https://buildkite.com/explosion-ai/thinc-gpu-test-suite/builds/11",True,{}
explosion/thinc,https://github.com/explosion/thinc,555,2021-10-27T06:18:06Z,2021-10-27T08:38:27Z,2021-10-27T08:38:27Z,MERGED,True,8,1,2,https://github.com/adrianeboyd,Update setup,3,[],https://github.com/explosion/thinc/pull/555,https://github.com/adrianeboyd,1,https://github.com/explosion/thinc/pull/555,"Add extras for cupy cuda 11.2-11.4
Add build constraints for python 3.10","Add extras for cupy cuda 11.2-11.4
Add build constraints for python 3.10",True,{}
explosion/thinc,https://github.com/explosion/thinc,557,2021-10-28T14:42:54Z,2021-10-29T06:54:01Z,2021-10-29T06:54:01Z,CLOSED,False,16,1,3,https://github.com/danieldk,Two tuplify improvements,2,[],https://github.com/explosion/thinc/pull/557,https://github.com/danieldk,1,https://github.com/explosion/thinc/pull/557,"This PR contains to improvements for using tuplify:


chain: also pass through Y when layer without nO in initialization
chain only passed through Y when a layer has nO set to None (nO is not
yet inferred). However, some layers do not have a nO dimension (e.g.
tuplify), but may have inner layers that still need Y to be properly
initialized.
This change changes chain initialization, so that it passes through Y
when: 1. nO is not inferred or nO is not a dimension of the layer.


tuplify: fix error when deducing nI
If nI can be deduced from the first layer, tuplify's initialization
tries to set nI. However, this failed because the dimension nI is not
known.
This change adds nI to tuplify as a known dimention and adds a unit test
to check that tuplify can be initialized correctly.","This PR contains to improvements for using tuplify:


chain: also pass through Y when layer without nO in initialization
chain only passed through Y when a layer has nO set to None (nO is not
yet inferred). However, some layers do not have a nO dimension (e.g.
tuplify), but may have inner layers that still need Y to be properly
initialized.
This change changes chain initialization, so that it passes through Y
when: 1. nO is not inferred or nO is not a dimension of the layer.


tuplify: fix error when deducing nI
If nI can be deduced from the first layer, tuplify's initialization
tries to set nI. However, this failed because the dimension nI is not
known.
This change adds nI to tuplify as a known dimention and adds a unit test
to check that tuplify can be initialized correctly.",True,{'EYES': ['https://github.com/svlandeg']}
explosion/thinc,https://github.com/explosion/thinc,557,2021-10-28T14:42:54Z,2021-10-29T06:54:01Z,2021-10-29T06:54:01Z,CLOSED,False,16,1,3,https://github.com/danieldk,Two tuplify improvements,2,[],https://github.com/explosion/thinc/pull/557,https://github.com/danieldk,2,https://github.com/explosion/thinc/pull/557#issuecomment-954476907,"This PR contains to improvements for using tuplify:


chain: also pass through Y when layer without nO in initialization
chain only passed through Y when a layer has nO set to None (nO is not
yet inferred). However, some layers do not have a nO dimension (e.g.
tuplify), but may have inner layers that still need Y to be properly
initialized.
This change changes chain initialization, so that it passes through Y
when: 1. nO is not inferred or nO is not a dimension of the layer.


tuplify: fix error when deducing nI
If nI can be deduced from the first layer, tuplify's initialization
tries to set nI. However, this failed because the dimension nI is not
known.
This change adds nI to tuplify as a known dimention and adds a unit test
to check that tuplify can be initialized correctly.",Closing to split up in two PRs.,True,{}
explosion/thinc,https://github.com/explosion/thinc,558,2021-10-29T06:59:27Z,2021-10-29T09:07:39Z,2021-10-29T09:07:39Z,MERGED,True,8,0,2,https://github.com/danieldk,tuplify: fix error when deducing nI,1,"['enhancement', 'feat / layers']",https://github.com/explosion/thinc/pull/558,https://github.com/danieldk,1,https://github.com/explosion/thinc/pull/558,"If nI can be deduced from the first layer, tuplify's initialization
tries to set nI. However, this failed because the dimension nI is not
known.
This change adds nI to tuplify as a known dimension and adds a unit test
to check that tuplify can be initialized correctly.","If nI can be deduced from the first layer, tuplify's initialization
tries to set nI. However, this failed because the dimension nI is not
known.
This change adds nI to tuplify as a known dimension and adds a unit test
to check that tuplify can be initialized correctly.",True,{'THUMBS_UP': ['https://github.com/svlandeg']}
explosion/thinc,https://github.com/explosion/thinc,559,2021-11-03T19:58:02Z,2021-12-21T12:40:42Z,2021-12-21T12:41:01Z,MERGED,True,15,2,2,https://github.com/andrewsi-z,Support big endian platform by providing new ops implementation,12,"['enhancement', 'feat / ops']",https://github.com/explosion/thinc/pull/559,https://github.com/andrewsi-z,1,https://github.com/explosion/thinc/pull/559,"Based on discussion in spaCy issue 9428, there are a set of modules related to the load and use of pre-trained language pipelines and/or models on a platform with a different byteorder.   This leads to incorrect results produced by the use of these pretrained models on platforms such as s390x.
As discussed in the spaCy issue referenced above, this PR adds support for thinc-bigendian-ops , which is created and located here: https://github.com/andrewsi-z/thinc-bigendian-ops .
thinc-bigendian-ops follows the precedent and use of thinc-apple-ops. When imported, it provides a custom ops class that implements the following:

numpy_ops.pyx hash has logic that maps a unsigned char array to the input ids array (uint64). The approach used maps the underlying storage and ""views"" as char array, and this implementation will result in different values depending on system byteorder. BigEndianOps modifies this algorithm and provides an alternative based on bit-shift operations, which will return the same value regardless of system byte order.
A new asarray method is implemented, which checks the byteorder of the output before returning it to caller. In cases where the byteorder is littleendian, it is swapped.

The logic in thinc proper is meant to mirror the logic added for thinc_apple_ops in a minimally intrusive way.
Notes:

ran test suite with thinc-bigendian-ops on s390x (big endian)
ran spaCy _sm model pipelines to validate original use cae.","Based on discussion in spaCy issue 9428, there are a set of modules related to the load and use of pre-trained language pipelines and/or models on a platform with a different byteorder.   This leads to incorrect results produced by the use of these pretrained models on platforms such as s390x.
As discussed in the spaCy issue referenced above, this PR adds support for thinc-bigendian-ops , which is created and located here: https://github.com/andrewsi-z/thinc-bigendian-ops .
thinc-bigendian-ops follows the precedent and use of thinc-apple-ops. When imported, it provides a custom ops class that implements the following:

numpy_ops.pyx hash has logic that maps a unsigned char array to the input ids array (uint64). The approach used maps the underlying storage and ""views"" as char array, and this implementation will result in different values depending on system byteorder. BigEndianOps modifies this algorithm and provides an alternative based on bit-shift operations, which will return the same value regardless of system byte order.
A new asarray method is implemented, which checks the byteorder of the output before returning it to caller. In cases where the byteorder is littleendian, it is swapped.

The logic in thinc proper is meant to mirror the logic added for thinc_apple_ops in a minimally intrusive way.
Notes:

ran test suite with thinc-bigendian-ops on s390x (big endian)
ran spaCy _sm model pipelines to validate original use cae.",True,{'THUMBS_UP': ['https://github.com/svlandeg']}
explosion/thinc,https://github.com/explosion/thinc,559,2021-11-03T19:58:02Z,2021-12-21T12:40:42Z,2021-12-21T12:41:01Z,MERGED,True,15,2,2,https://github.com/andrewsi-z,Support big endian platform by providing new ops implementation,12,"['enhancement', 'feat / ops']",https://github.com/explosion/thinc/pull/559,https://github.com/honnibal,2,https://github.com/explosion/thinc/pull/559#issuecomment-970466925,"Based on discussion in spaCy issue 9428, there are a set of modules related to the load and use of pre-trained language pipelines and/or models on a platform with a different byteorder.   This leads to incorrect results produced by the use of these pretrained models on platforms such as s390x.
As discussed in the spaCy issue referenced above, this PR adds support for thinc-bigendian-ops , which is created and located here: https://github.com/andrewsi-z/thinc-bigendian-ops .
thinc-bigendian-ops follows the precedent and use of thinc-apple-ops. When imported, it provides a custom ops class that implements the following:

numpy_ops.pyx hash has logic that maps a unsigned char array to the input ids array (uint64). The approach used maps the underlying storage and ""views"" as char array, and this implementation will result in different values depending on system byteorder. BigEndianOps modifies this algorithm and provides an alternative based on bit-shift operations, which will return the same value regardless of system byte order.
A new asarray method is implemented, which checks the byteorder of the output before returning it to caller. In cases where the byteorder is littleendian, it is swapped.

The logic in thinc proper is meant to mirror the logic added for thinc_apple_ops in a minimally intrusive way.
Notes:

ran test suite with thinc-bigendian-ops on s390x (big endian)
ran spaCy _sm model pipelines to validate original use cae.","I think this looks like a good match for how we've been doing things. @adrianeboyd do you have any comments? It looks okay to me. If it all works, I'm pleased to resolve this long-standing platform support issue!
I do think the mechanism we've designed for the ops selection could be better though. It's quite unsightly the way we have to reference the subclasses in Thinc, and it leads to the circular import problems.
Here's a suggestion we could try to improve this:

Ops classes implement a classmethod get_hardware_suitability(name, **kwargs) that returns an integer. A negative integer indicates incompatibility. A higher integer indicates that the ops class claims it should be used. kwargs can be passed in from above to influence the match.
In get_ops, we sort the registered ops by their hardware suitability score, and select the highest available. If no compatible ops are found, we raise an error.

This will prevent Thinc from having to know all of the ops classes in order to resolve which one should be used. Generally ops classes should return -1 for incompatible, 0 for compatible but not optimised, and 1 for optimised. If another plugin wants to overrule existing ones in specific circumstances it can return a higher match score under those conditions (including by supporting arbitrary keyword arguments).
I think we could merge the current PR as-is, as it doesn't seem to make things worse (that I can see). But I think the patch does indicate we could improve our system here.",True,{'ROCKET': ['https://github.com/andrewsi-z']}
explosion/thinc,https://github.com/explosion/thinc,559,2021-11-03T19:58:02Z,2021-12-21T12:40:42Z,2021-12-21T12:41:01Z,MERGED,True,15,2,2,https://github.com/andrewsi-z,Support big endian platform by providing new ops implementation,12,"['enhancement', 'feat / ops']",https://github.com/explosion/thinc/pull/559,https://github.com/andrewsi-z,3,https://github.com/explosion/thinc/pull/559#issuecomment-970551277,"Based on discussion in spaCy issue 9428, there are a set of modules related to the load and use of pre-trained language pipelines and/or models on a platform with a different byteorder.   This leads to incorrect results produced by the use of these pretrained models on platforms such as s390x.
As discussed in the spaCy issue referenced above, this PR adds support for thinc-bigendian-ops , which is created and located here: https://github.com/andrewsi-z/thinc-bigendian-ops .
thinc-bigendian-ops follows the precedent and use of thinc-apple-ops. When imported, it provides a custom ops class that implements the following:

numpy_ops.pyx hash has logic that maps a unsigned char array to the input ids array (uint64). The approach used maps the underlying storage and ""views"" as char array, and this implementation will result in different values depending on system byteorder. BigEndianOps modifies this algorithm and provides an alternative based on bit-shift operations, which will return the same value regardless of system byte order.
A new asarray method is implemented, which checks the byteorder of the output before returning it to caller. In cases where the byteorder is littleendian, it is swapped.

The logic in thinc proper is meant to mirror the logic added for thinc_apple_ops in a minimally intrusive way.
Notes:

ran test suite with thinc-bigendian-ops on s390x (big endian)
ran spaCy _sm model pipelines to validate original use cae.","For reference on associated issues, I should mention I have the needed murmurhash changes here:  https://github.com/andrewsi-z/murmurhash
This uses the murmurhash3 endian agnostic design introduced for (I believe) node use of it.
I followed this same design and applied it to murmurhash2.cpp hash64a, which caused errors. I will open up an issue against explosion/murmurhash to discuss whether you'd like these fixes or prefer to keep a separate murmurhash library for big endian (which also functionally works fine).",True,{}
explosion/thinc,https://github.com/explosion/thinc,559,2021-11-03T19:58:02Z,2021-12-21T12:40:42Z,2021-12-21T12:41:01Z,MERGED,True,15,2,2,https://github.com/andrewsi-z,Support big endian platform by providing new ops implementation,12,"['enhancement', 'feat / ops']",https://github.com/explosion/thinc/pull/559,https://github.com/adrianeboyd,4,https://github.com/explosion/thinc/pull/559#issuecomment-973996947,"Based on discussion in spaCy issue 9428, there are a set of modules related to the load and use of pre-trained language pipelines and/or models on a platform with a different byteorder.   This leads to incorrect results produced by the use of these pretrained models on platforms such as s390x.
As discussed in the spaCy issue referenced above, this PR adds support for thinc-bigendian-ops , which is created and located here: https://github.com/andrewsi-z/thinc-bigendian-ops .
thinc-bigendian-ops follows the precedent and use of thinc-apple-ops. When imported, it provides a custom ops class that implements the following:

numpy_ops.pyx hash has logic that maps a unsigned char array to the input ids array (uint64). The approach used maps the underlying storage and ""views"" as char array, and this implementation will result in different values depending on system byteorder. BigEndianOps modifies this algorithm and provides an alternative based on bit-shift operations, which will return the same value regardless of system byte order.
A new asarray method is implemented, which checks the byteorder of the output before returning it to caller. In cases where the byteorder is littleendian, it is swapped.

The logic in thinc proper is meant to mirror the logic added for thinc_apple_ops in a minimally intrusive way.
Notes:

ran test suite with thinc-bigendian-ops on s390x (big endian)
ran spaCy _sm model pipelines to validate original use cae.","I think I'd reverse/massage the logic in get_ops in bit, but given the current state of thinc, the basics for this PR seem fine.
Adding AppleOps is simpler in a number of ways than BigEndianOps:

you can't install it on systems where it wouldn't run correctly
backing off to numpy accidentally is slower, not broken

So we need to be more careful around BigEndianOps. I also hesitate a bit to add this without any CI testing on our end, but I'm not sure that we have any feasible options.",True,{}
explosion/thinc,https://github.com/explosion/thinc,559,2021-11-03T19:58:02Z,2021-12-21T12:40:42Z,2021-12-21T12:41:01Z,MERGED,True,15,2,2,https://github.com/andrewsi-z,Support big endian platform by providing new ops implementation,12,"['enhancement', 'feat / ops']",https://github.com/explosion/thinc/pull/559,https://github.com/andrewsi-z,5,https://github.com/explosion/thinc/pull/559#issuecomment-974056504,"Based on discussion in spaCy issue 9428, there are a set of modules related to the load and use of pre-trained language pipelines and/or models on a platform with a different byteorder.   This leads to incorrect results produced by the use of these pretrained models on platforms such as s390x.
As discussed in the spaCy issue referenced above, this PR adds support for thinc-bigendian-ops , which is created and located here: https://github.com/andrewsi-z/thinc-bigendian-ops .
thinc-bigendian-ops follows the precedent and use of thinc-apple-ops. When imported, it provides a custom ops class that implements the following:

numpy_ops.pyx hash has logic that maps a unsigned char array to the input ids array (uint64). The approach used maps the underlying storage and ""views"" as char array, and this implementation will result in different values depending on system byteorder. BigEndianOps modifies this algorithm and provides an alternative based on bit-shift operations, which will return the same value regardless of system byte order.
A new asarray method is implemented, which checks the byteorder of the output before returning it to caller. In cases where the byteorder is littleendian, it is swapped.

The logic in thinc proper is meant to mirror the logic added for thinc_apple_ops in a minimally intrusive way.
Notes:

ran test suite with thinc-bigendian-ops on s390x (big endian)
ran spaCy _sm model pipelines to validate original use cae.","Hi @adrianeboyd, thank you for your comments and I'll start reviewing them in more detail.
You brought up CI... I know github actions doesn't support s390x yet, but there are a number of supporting options that can potentially (?) be triggered by a github action. These would be community available no cost resources .
I can explore this as a follow-on action and present some options for discussion (and help get it going if you and the explosion team believe it is worthwhile to purse).",True,{}
explosion/thinc,https://github.com/explosion/thinc,559,2021-11-03T19:58:02Z,2021-12-21T12:40:42Z,2021-12-21T12:41:01Z,MERGED,True,15,2,2,https://github.com/andrewsi-z,Support big endian platform by providing new ops implementation,12,"['enhancement', 'feat / ops']",https://github.com/explosion/thinc/pull/559,https://github.com/svlandeg,6,https://github.com/explosion/thinc/pull/559#issuecomment-998746395,"Based on discussion in spaCy issue 9428, there are a set of modules related to the load and use of pre-trained language pipelines and/or models on a platform with a different byteorder.   This leads to incorrect results produced by the use of these pretrained models on platforms such as s390x.
As discussed in the spaCy issue referenced above, this PR adds support for thinc-bigendian-ops , which is created and located here: https://github.com/andrewsi-z/thinc-bigendian-ops .
thinc-bigendian-ops follows the precedent and use of thinc-apple-ops. When imported, it provides a custom ops class that implements the following:

numpy_ops.pyx hash has logic that maps a unsigned char array to the input ids array (uint64). The approach used maps the underlying storage and ""views"" as char array, and this implementation will result in different values depending on system byteorder. BigEndianOps modifies this algorithm and provides an alternative based on bit-shift operations, which will return the same value regardless of system byte order.
A new asarray method is implemented, which checks the byteorder of the output before returning it to caller. In cases where the byteorder is littleendian, it is swapped.

The logic in thinc proper is meant to mirror the logic added for thinc_apple_ops in a minimally intrusive way.
Notes:

ran test suite with thinc-bigendian-ops on s390x (big endian)
ran spaCy _sm model pipelines to validate original use cae.","While we probably want to think about more generic solutions in the future, this should be fine to merge as is for now, and will become available in Thinc's next release 8.0.14.",True,{}
explosion/thinc,https://github.com/explosion/thinc,560,2021-11-04T14:26:09Z,2021-11-04T15:44:04Z,2021-11-08T08:11:02Z,MERGED,True,2,2,2,https://github.com/danieldk,Fix CI by pinning Tensorflow,2,['tests'],https://github.com/explosion/thinc/pull/560,https://github.com/danieldk,1,https://github.com/explosion/thinc/pull/560,,,True,{'THUMBS_UP': ['https://github.com/svlandeg']}
explosion/thinc,https://github.com/explosion/thinc,560,2021-11-04T14:26:09Z,2021-11-04T15:44:04Z,2021-11-08T08:11:02Z,MERGED,True,2,2,2,https://github.com/danieldk,Fix CI by pinning Tensorflow,2,['tests'],https://github.com/explosion/thinc/pull/560,https://github.com/danieldk,2,https://github.com/explosion/thinc/pull/560#issuecomment-961158704,,@explosion-bot please test_gpu,True,"{'EYES': ['https://github.com/explosion-bot'], 'ROCKET': ['https://github.com/explosion-bot']}"
explosion/thinc,https://github.com/explosion/thinc,560,2021-11-04T14:26:09Z,2021-11-04T15:44:04Z,2021-11-08T08:11:02Z,MERGED,True,2,2,2,https://github.com/danieldk,Fix CI by pinning Tensorflow,2,['tests'],https://github.com/explosion/thinc/pull/560,https://github.com/explosion-bot,3,https://github.com/explosion/thinc/pull/560#issuecomment-961159066,," Successfully triggered build on Buildkite

URL: https://buildkite.com/explosion-ai/thinc-gpu-test-suite/builds/13",True,{}
explosion/thinc,https://github.com/explosion/thinc,561,2021-11-04T14:46:45Z,2021-11-05T09:27:00Z,2021-11-05T09:27:00Z,MERGED,True,12,10,3,https://github.com/adrianeboyd,Fix circular import related to ops registry,4,[],https://github.com/explosion/thinc/pull/561,https://github.com/adrianeboyd,1,https://github.com/explosion/thinc/pull/561,"It's currently not possible to import thinc_apple_ops because of a circular import related to a change from #540.
This works:
import thinc
import thinc_apple_ops
This does not:
import thinc_apple_ops","It's currently not possible to import thinc_apple_ops because of a circular import related to a change from #540.
This works:
import thinc
import thinc_apple_ops
This does not:
import thinc_apple_ops",True,{}
explosion/thinc,https://github.com/explosion/thinc,562,2021-11-05T09:28:51Z,2021-11-05T10:42:46Z,2021-11-05T10:42:46Z,MERGED,True,1,1,1,https://github.com/adrianeboyd,Set version to v8.0.13,1,[],https://github.com/explosion/thinc/pull/562,https://github.com/adrianeboyd,1,https://github.com/explosion/thinc/pull/562,,,True,{}
explosion/thinc,https://github.com/explosion/thinc,563,2021-11-10T10:32:28Z,,2021-12-21T12:07:10Z,OPEN,False,78,28,8,https://github.com/svlandeg,WIP: homomorphic setting,5,"['enhancement', 'feat / layers']",https://github.com/explosion/thinc/pull/563,https://github.com/svlandeg,1,https://github.com/explosion/thinc/pull/563,"The current implementation of chain's initialization does shape inference that guesses too much. For instance, relu >> relu >> relu, initialized with X and Y, would define the whole network and set all nO's to Y's shape, which is not necessarily what the user wants. You could argue that in this case, the network is simply underspecified and should be specified better upon declaration.
Guessing hidden widths also meant we made mistakes. With an ensemble textcat with inline transformer, two embedding layers are concatenated. But if the embedding width of the transformer isn't known upon initialization, the second embedding layer would just receive its nO from Y, which is clearly wrong.
This PR restricts transferring Y across the network, but introduces a ""homomorphic"" setting instead that should at least allow us to do better inference for layers where nI==nO.
TODO

 Think about versioning of chain as this change is breaking across spacy and spacy-transformers, but those libraries don't actually use chain.v1, they just import thinc.api.chain :|
 Look at unit test in notebooks: example 01
 Look more into the issue of inline transformer+textcat, as this might represent a use-case that requires a more complex solution to this problem
 Find a way to specify relations between dimensions in a network architecture, so that all related dimensions are set when one of them is specified?
 Test spaCy & spacy-transformers thoroughly against this branch","The current implementation of chain's initialization does shape inference that guesses too much. For instance, relu >> relu >> relu, initialized with X and Y, would define the whole network and set all nO's to Y's shape, which is not necessarily what the user wants. You could argue that in this case, the network is simply underspecified and should be specified better upon declaration.
Guessing hidden widths also meant we made mistakes. With an ensemble textcat with inline transformer, two embedding layers are concatenated. But if the embedding width of the transformer isn't known upon initialization, the second embedding layer would just receive its nO from Y, which is clearly wrong.
This PR restricts transferring Y across the network, but introduces a ""homomorphic"" setting instead that should at least allow us to do better inference for layers where nI==nO.
TODO

 Think about versioning of chain as this change is breaking across spacy and spacy-transformers, but those libraries don't actually use chain.v1, they just import thinc.api.chain :|
 Look at unit test in notebooks: example 01
 Look more into the issue of inline transformer+textcat, as this might represent a use-case that requires a more complex solution to this problem
 Find a way to specify relations between dimensions in a network architecture, so that all related dimensions are set when one of them is specified?
 Test spaCy & spacy-transformers thoroughly against this branch",True,{}
explosion/thinc,https://github.com/explosion/thinc,563,2021-11-10T10:32:28Z,,2021-12-21T12:07:10Z,OPEN,False,78,28,8,https://github.com/svlandeg,WIP: homomorphic setting,5,"['enhancement', 'feat / layers']",https://github.com/explosion/thinc/pull/563,https://github.com/svlandeg,2,https://github.com/explosion/thinc/pull/563#issuecomment-967078319,"The current implementation of chain's initialization does shape inference that guesses too much. For instance, relu >> relu >> relu, initialized with X and Y, would define the whole network and set all nO's to Y's shape, which is not necessarily what the user wants. You could argue that in this case, the network is simply underspecified and should be specified better upon declaration.
Guessing hidden widths also meant we made mistakes. With an ensemble textcat with inline transformer, two embedding layers are concatenated. But if the embedding width of the transformer isn't known upon initialization, the second embedding layer would just receive its nO from Y, which is clearly wrong.
This PR restricts transferring Y across the network, but introduces a ""homomorphic"" setting instead that should at least allow us to do better inference for layers where nI==nO.
TODO

 Think about versioning of chain as this change is breaking across spacy and spacy-transformers, but those libraries don't actually use chain.v1, they just import thinc.api.chain :|
 Look at unit test in notebooks: example 01
 Look more into the issue of inline transformer+textcat, as this might represent a use-case that requires a more complex solution to this problem
 Find a way to specify relations between dimensions in a network architecture, so that all related dimensions are set when one of them is specified?
 Test spaCy & spacy-transformers thoroughly against this branch","The failing test is about the Jupyter notebook example 01 which states:

Some combinators work on a layer and a numeric argument. For instance, the clone combinator creates a number of copies of a layer, and chains them together into a deep feed-forward network. The shape inference is especially handy here: we want the first and last layers to have different shapes, so we can avoid providing any dimensions into the layer we clone. We then just have to specify the first layer's output size, and we can let the rest of the dimensions be inferred from the data.

and runs
model = clone(Linear(), 5)
model.layers[0].set_dim(""nO"", n_hidden)
model.initialize(X=X, Y=Y)

This network is underspecified on purpose. So we need to decide whether we still want to support this (as we used to) or not (as this PR proposes). IMO, after reflecting upon this, I think the original code is not good practice. Y's output dimension will be propagated to all the intermediate Linear layers, which results in a pretty weird/atypical/possibly unuseful architecture overall?",True,{}
explosion/thinc,https://github.com/explosion/thinc,565,2021-11-23T12:33:16Z,2021-12-12T15:50:30Z,2021-12-12T15:50:30Z,MERGED,True,75,18,2,https://github.com/adrianeboyd,Make murmurhash endian-neutral and optimize,1,[],https://github.com/explosion/thinc/pull/565,https://github.com/adrianeboyd,1,https://github.com/explosion/thinc/pull/565,For NumpyOps.hash and SparseLinear replace functions imported from murmurhash with inline endian-neutral completely-unrolled versions for uint64 input.,For NumpyOps.hash and SparseLinear replace functions imported from murmurhash with inline endian-neutral completely-unrolled versions for uint64 input.,True,{}
explosion/thinc,https://github.com/explosion/thinc,565,2021-11-23T12:33:16Z,2021-12-12T15:50:30Z,2021-12-12T15:50:30Z,MERGED,True,75,18,2,https://github.com/adrianeboyd,Make murmurhash endian-neutral and optimize,1,[],https://github.com/explosion/thinc/pull/565,https://github.com/adrianeboyd,2,https://github.com/explosion/thinc/pull/565#issuecomment-976468413,For NumpyOps.hash and SparseLinear replace functions imported from murmurhash with inline endian-neutral completely-unrolled versions for uint64 input.,"This PR along with these PRs for murmurhash are intended to make all the hashing operations in murmurhash+thinc endian-neutral:

explosion/murmurhash#27
explosion/murmurhash#28",True,{}
explosion/thinc,https://github.com/explosion/thinc,566,2021-12-12T15:18:51Z,2022-05-17T15:29:16Z,2022-05-17T15:29:16Z,CLOSED,False,16,16,2,https://github.com/honnibal,Try out making Ragged generic,2,"['feat / types', ' wip']",https://github.com/explosion/thinc/pull/566,https://github.com/honnibal,1,https://github.com/explosion/thinc/pull/566,"The Ragged type is troublingly vague at the moment, because it doesn't tell you what type it holds. Try making it generic. Could require lots of adjustments, I'm not sure?","The Ragged type is troublingly vague at the moment, because it doesn't tell you what type it holds. Try making it generic. Could require lots of adjustments, I'm not sure?",True,{}
explosion/thinc,https://github.com/explosion/thinc,566,2021-12-12T15:18:51Z,2022-05-17T15:29:16Z,2022-05-17T15:29:16Z,CLOSED,False,16,16,2,https://github.com/honnibal,Try out making Ragged generic,2,"['feat / types', ' wip']",https://github.com/explosion/thinc/pull/566,https://github.com/richardpaulhudson,2,https://github.com/explosion/thinc/pull/566#issuecomment-1067681682,"The Ragged type is troublingly vague at the moment, because it doesn't tell you what type it holds. Try making it generic. Could require lots of adjustments, I'm not sure?","I hadn't seen this when I started on #599, but seem to have implemented everything that is here in the course of implementing #599.",True,{}
explosion/thinc,https://github.com/explosion/thinc,567,2021-12-13T14:29:52Z,2021-12-13T15:53:03Z,2021-12-13T15:53:03Z,MERGED,True,11,11,1,https://github.com/adrianeboyd,Switch to latest CI images,1,[],https://github.com/explosion/thinc/pull/567,https://github.com/adrianeboyd,1,https://github.com/explosion/thinc/pull/567,,,True,{}
explosion/thinc,https://github.com/explosion/thinc,568,2021-12-14T08:54:10Z,2022-01-13T15:37:16Z,2022-01-13T15:37:16Z,MERGED,True,861,9,11,https://github.com/kadarakos,New hard activations,55,"['enhancement', 'feat / layers', 'feat / ops']",https://github.com/explosion/thinc/pull/568,https://github.com/kadarakos,1,https://github.com/explosion/thinc/pull/568,"This is the first PR in a series of PRs adding new activation functions to thinc.
The general goal is to add activations to be able to reproduce different models and have more options. Specifically in this PR other than Swish the remaining activation functions are designed to be fast and to work well in low precision.
The new function in this PR are:

HardSigmoid https://paperswithcode.com/method/hard-sigmoid
HardTanh
ClippedLinear: generic linear nonlinearity with parametrizeable slope, offset, min and max. Its used to implement the previous three.
Swish https://blog.paperspace.com/swish-activation-function/
HardSwish  https://www.scitepress.org/Papers/2019/74696/74696.pdf
HardSwishMobilenet https://arxiv.org/abs/1905.02244v5
GELU: ""almost exact"" GELU with approximate erf
Approximate GELU: the approximation from the original paper https://paperswithcode.com/method/gelu

There is also a relu_n function in thinc/backends/ops.py, because I wasn't clear yet how to implement the ReLU with a threshold above (useful for low precision). At the moment its internally used by HardSwishMobilenet, but in the future it will be somehow merged with relu.
The forward and backward ops are implemented in thinc/backends/ops.py, but the optimized Cython and CUDA versions are deferred to a next PR.
For each op there is a separate dense layer implemented in thinc/layers/, however, the next PR will attempt to refactor this into a single Dense() API that takes the activation function as an argument.","This is the first PR in a series of PRs adding new activation functions to thinc.
The general goal is to add activations to be able to reproduce different models and have more options. Specifically in this PR other than Swish the remaining activation functions are designed to be fast and to work well in low precision.
The new function in this PR are:

HardSigmoid https://paperswithcode.com/method/hard-sigmoid
HardTanh
ClippedLinear: generic linear nonlinearity with parametrizeable slope, offset, min and max. Its used to implement the previous three.
Swish https://blog.paperspace.com/swish-activation-function/
HardSwish  https://www.scitepress.org/Papers/2019/74696/74696.pdf
HardSwishMobilenet https://arxiv.org/abs/1905.02244v5
GELU: ""almost exact"" GELU with approximate erf
Approximate GELU: the approximation from the original paper https://paperswithcode.com/method/gelu

There is also a relu_n function in thinc/backends/ops.py, because I wasn't clear yet how to implement the ReLU with a threshold above (useful for low precision). At the moment its internally used by HardSwishMobilenet, but in the future it will be somehow merged with relu.
The forward and backward ops are implemented in thinc/backends/ops.py, but the optimized Cython and CUDA versions are deferred to a next PR.
For each op there is a separate dense layer implemented in thinc/layers/, however, the next PR will attempt to refactor this into a single Dense() API that takes the activation function as an argument.",True,"{'ROCKET': ['https://github.com/svlandeg', 'https://github.com/ines', 'https://github.com/danieldk']}"
explosion/thinc,https://github.com/explosion/thinc,568,2021-12-14T08:54:10Z,2022-01-13T15:37:16Z,2022-01-13T15:37:16Z,MERGED,True,861,9,11,https://github.com/kadarakos,New hard activations,55,"['enhancement', 'feat / layers', 'feat / ops']",https://github.com/explosion/thinc/pull/568,https://github.com/svlandeg,2,https://github.com/explosion/thinc/pull/568#issuecomment-995558747,"This is the first PR in a series of PRs adding new activation functions to thinc.
The general goal is to add activations to be able to reproduce different models and have more options. Specifically in this PR other than Swish the remaining activation functions are designed to be fast and to work well in low precision.
The new function in this PR are:

HardSigmoid https://paperswithcode.com/method/hard-sigmoid
HardTanh
ClippedLinear: generic linear nonlinearity with parametrizeable slope, offset, min and max. Its used to implement the previous three.
Swish https://blog.paperspace.com/swish-activation-function/
HardSwish  https://www.scitepress.org/Papers/2019/74696/74696.pdf
HardSwishMobilenet https://arxiv.org/abs/1905.02244v5
GELU: ""almost exact"" GELU with approximate erf
Approximate GELU: the approximation from the original paper https://paperswithcode.com/method/gelu

There is also a relu_n function in thinc/backends/ops.py, because I wasn't clear yet how to implement the ReLU with a threshold above (useful for low precision). At the moment its internally used by HardSwishMobilenet, but in the future it will be somehow merged with relu.
The forward and backward ops are implemented in thinc/backends/ops.py, but the optimized Cython and CUDA versions are deferred to a next PR.
For each op there is a separate dense layer implemented in thinc/layers/, however, the next PR will attempt to refactor this into a single Dense() API that takes the activation function as an argument.",FYI: the new mypy failure should hopefully be fixed by #569,True,{}
explosion/thinc,https://github.com/explosion/thinc,568,2021-12-14T08:54:10Z,2022-01-13T15:37:16Z,2022-01-13T15:37:16Z,MERGED,True,861,9,11,https://github.com/kadarakos,New hard activations,55,"['enhancement', 'feat / layers', 'feat / ops']",https://github.com/explosion/thinc/pull/568,https://github.com/kadarakos,3,https://github.com/explosion/thinc/pull/568#issuecomment-995567554,"This is the first PR in a series of PRs adding new activation functions to thinc.
The general goal is to add activations to be able to reproduce different models and have more options. Specifically in this PR other than Swish the remaining activation functions are designed to be fast and to work well in low precision.
The new function in this PR are:

HardSigmoid https://paperswithcode.com/method/hard-sigmoid
HardTanh
ClippedLinear: generic linear nonlinearity with parametrizeable slope, offset, min and max. Its used to implement the previous three.
Swish https://blog.paperspace.com/swish-activation-function/
HardSwish  https://www.scitepress.org/Papers/2019/74696/74696.pdf
HardSwishMobilenet https://arxiv.org/abs/1905.02244v5
GELU: ""almost exact"" GELU with approximate erf
Approximate GELU: the approximation from the original paper https://paperswithcode.com/method/gelu

There is also a relu_n function in thinc/backends/ops.py, because I wasn't clear yet how to implement the ReLU with a threshold above (useful for low precision). At the moment its internally used by HardSwishMobilenet, but in the future it will be somehow merged with relu.
The forward and backward ops are implemented in thinc/backends/ops.py, but the optimized Cython and CUDA versions are deferred to a next PR.
For each op there is a separate dense layer implemented in thinc/layers/, however, the next PR will attempt to refactor this into a single Dense() API that takes the activation function as an argument.","FYI: the new mypy failure should hopefully be fixed by #569

phew that's good to know!",True,{}
explosion/thinc,https://github.com/explosion/thinc,568,2021-12-14T08:54:10Z,2022-01-13T15:37:16Z,2022-01-13T15:37:16Z,MERGED,True,861,9,11,https://github.com/kadarakos,New hard activations,55,"['enhancement', 'feat / layers', 'feat / ops']",https://github.com/explosion/thinc/pull/568,https://github.com/svlandeg,4,https://github.com/explosion/thinc/pull/568#issuecomment-995751658,"This is the first PR in a series of PRs adding new activation functions to thinc.
The general goal is to add activations to be able to reproduce different models and have more options. Specifically in this PR other than Swish the remaining activation functions are designed to be fast and to work well in low precision.
The new function in this PR are:

HardSigmoid https://paperswithcode.com/method/hard-sigmoid
HardTanh
ClippedLinear: generic linear nonlinearity with parametrizeable slope, offset, min and max. Its used to implement the previous three.
Swish https://blog.paperspace.com/swish-activation-function/
HardSwish  https://www.scitepress.org/Papers/2019/74696/74696.pdf
HardSwishMobilenet https://arxiv.org/abs/1905.02244v5
GELU: ""almost exact"" GELU with approximate erf
Approximate GELU: the approximation from the original paper https://paperswithcode.com/method/gelu

There is also a relu_n function in thinc/backends/ops.py, because I wasn't clear yet how to implement the ReLU with a threshold above (useful for low precision). At the moment its internally used by HardSwishMobilenet, but in the future it will be somehow merged with relu.
The forward and backward ops are implemented in thinc/backends/ops.py, but the optimized Cython and CUDA versions are deferred to a next PR.
For each op there is a separate dense layer implemented in thinc/layers/, however, the next PR will attempt to refactor this into a single Dense() API that takes the activation function as an argument.","@kadarakos : you can try pulling and merging upstream/master and push to this PR again, and the CI will hopefully behave :-)",True,{}
explosion/thinc,https://github.com/explosion/thinc,568,2021-12-14T08:54:10Z,2022-01-13T15:37:16Z,2022-01-13T15:37:16Z,MERGED,True,861,9,11,https://github.com/kadarakos,New hard activations,55,"['enhancement', 'feat / layers', 'feat / ops']",https://github.com/explosion/thinc/pull/568,https://github.com/kadarakos,5,https://github.com/explosion/thinc/pull/568#issuecomment-996049842,"This is the first PR in a series of PRs adding new activation functions to thinc.
The general goal is to add activations to be able to reproduce different models and have more options. Specifically in this PR other than Swish the remaining activation functions are designed to be fast and to work well in low precision.
The new function in this PR are:

HardSigmoid https://paperswithcode.com/method/hard-sigmoid
HardTanh
ClippedLinear: generic linear nonlinearity with parametrizeable slope, offset, min and max. Its used to implement the previous three.
Swish https://blog.paperspace.com/swish-activation-function/
HardSwish  https://www.scitepress.org/Papers/2019/74696/74696.pdf
HardSwishMobilenet https://arxiv.org/abs/1905.02244v5
GELU: ""almost exact"" GELU with approximate erf
Approximate GELU: the approximation from the original paper https://paperswithcode.com/method/gelu

There is also a relu_n function in thinc/backends/ops.py, because I wasn't clear yet how to implement the ReLU with a threshold above (useful for low precision). At the moment its internally used by HardSwishMobilenet, but in the future it will be somehow merged with relu.
The forward and backward ops are implemented in thinc/backends/ops.py, but the optimized Cython and CUDA versions are deferred to a next PR.
For each op there is a separate dense layer implemented in thinc/layers/, however, the next PR will attempt to refactor this into a single Dense() API that takes the activation function as an argument.","@kadarakos : you can try pulling and merging upstream/master and push to this PR again, and the CI will hopefully behave :-)

Thanks! Enjoying watching the green check marks again.",True,{'HOORAY': ['https://github.com/svlandeg']}
explosion/thinc,https://github.com/explosion/thinc,568,2021-12-14T08:54:10Z,2022-01-13T15:37:16Z,2022-01-13T15:37:16Z,MERGED,True,861,9,11,https://github.com/kadarakos,New hard activations,55,"['enhancement', 'feat / layers', 'feat / ops']",https://github.com/explosion/thinc/pull/568,https://github.com/svlandeg,6,https://github.com/explosion/thinc/pull/568#issuecomment-998721195,"This is the first PR in a series of PRs adding new activation functions to thinc.
The general goal is to add activations to be able to reproduce different models and have more options. Specifically in this PR other than Swish the remaining activation functions are designed to be fast and to work well in low precision.
The new function in this PR are:

HardSigmoid https://paperswithcode.com/method/hard-sigmoid
HardTanh
ClippedLinear: generic linear nonlinearity with parametrizeable slope, offset, min and max. Its used to implement the previous three.
Swish https://blog.paperspace.com/swish-activation-function/
HardSwish  https://www.scitepress.org/Papers/2019/74696/74696.pdf
HardSwishMobilenet https://arxiv.org/abs/1905.02244v5
GELU: ""almost exact"" GELU with approximate erf
Approximate GELU: the approximation from the original paper https://paperswithcode.com/method/gelu

There is also a relu_n function in thinc/backends/ops.py, because I wasn't clear yet how to implement the ReLU with a threshold above (useful for low precision). At the moment its internally used by HardSwishMobilenet, but in the future it will be somehow merged with relu.
The forward and backward ops are implemented in thinc/backends/ops.py, but the optimized Cython and CUDA versions are deferred to a next PR.
For each op there is a separate dense layer implemented in thinc/layers/, however, the next PR will attempt to refactor this into a single Dense() API that takes the activation function as an argument.","Maintenance note: there's a few open questions/comments here that might not be addressed until after the holidays, so I'll put this ""in draft"" for the time being :-)",True,{}
explosion/thinc,https://github.com/explosion/thinc,568,2021-12-14T08:54:10Z,2022-01-13T15:37:16Z,2022-01-13T15:37:16Z,MERGED,True,861,9,11,https://github.com/kadarakos,New hard activations,55,"['enhancement', 'feat / layers', 'feat / ops']",https://github.com/explosion/thinc/pull/568,https://github.com/honnibal,7,https://github.com/explosion/thinc/pull/568#issuecomment-1004041073,"This is the first PR in a series of PRs adding new activation functions to thinc.
The general goal is to add activations to be able to reproduce different models and have more options. Specifically in this PR other than Swish the remaining activation functions are designed to be fast and to work well in low precision.
The new function in this PR are:

HardSigmoid https://paperswithcode.com/method/hard-sigmoid
HardTanh
ClippedLinear: generic linear nonlinearity with parametrizeable slope, offset, min and max. Its used to implement the previous three.
Swish https://blog.paperspace.com/swish-activation-function/
HardSwish  https://www.scitepress.org/Papers/2019/74696/74696.pdf
HardSwishMobilenet https://arxiv.org/abs/1905.02244v5
GELU: ""almost exact"" GELU with approximate erf
Approximate GELU: the approximation from the original paper https://paperswithcode.com/method/gelu

There is also a relu_n function in thinc/backends/ops.py, because I wasn't clear yet how to implement the ReLU with a threshold above (useful for low precision). At the moment its internally used by HardSwishMobilenet, but in the future it will be somehow merged with relu.
The forward and backward ops are implemented in thinc/backends/ops.py, but the optimized Cython and CUDA versions are deferred to a next PR.
For each op there is a separate dense layer implemented in thinc/layers/, however, the next PR will attempt to refactor this into a single Dense() API that takes the activation function as an argument.","Really nice work!
I don't really have any corrections to add, except to note that we may want activation-only layers for these functions as well possibly?
Regarding the tests, the PyTorch comparison is a nice utility. Are the tests that use it marked as needing pytorch? I'm pretty sure we have a marker for that.",True,{}
explosion/thinc,https://github.com/explosion/thinc,568,2021-12-14T08:54:10Z,2022-01-13T15:37:16Z,2022-01-13T15:37:16Z,MERGED,True,861,9,11,https://github.com/kadarakos,New hard activations,55,"['enhancement', 'feat / layers', 'feat / ops']",https://github.com/explosion/thinc/pull/568,https://github.com/kadarakos,8,https://github.com/explosion/thinc/pull/568#issuecomment-1011097824,"This is the first PR in a series of PRs adding new activation functions to thinc.
The general goal is to add activations to be able to reproduce different models and have more options. Specifically in this PR other than Swish the remaining activation functions are designed to be fast and to work well in low precision.
The new function in this PR are:

HardSigmoid https://paperswithcode.com/method/hard-sigmoid
HardTanh
ClippedLinear: generic linear nonlinearity with parametrizeable slope, offset, min and max. Its used to implement the previous three.
Swish https://blog.paperspace.com/swish-activation-function/
HardSwish  https://www.scitepress.org/Papers/2019/74696/74696.pdf
HardSwishMobilenet https://arxiv.org/abs/1905.02244v5
GELU: ""almost exact"" GELU with approximate erf
Approximate GELU: the approximation from the original paper https://paperswithcode.com/method/gelu

There is also a relu_n function in thinc/backends/ops.py, because I wasn't clear yet how to implement the ReLU with a threshold above (useful for low precision). At the moment its internally used by HardSwishMobilenet, but in the future it will be somehow merged with relu.
The forward and backward ops are implemented in thinc/backends/ops.py, but the optimized Cython and CUDA versions are deferred to a next PR.
For each op there is a separate dense layer implemented in thinc/layers/, however, the next PR will attempt to refactor this into a single Dense() API that takes the activation function as an argument.","Really nice work!
I don't really have any corrections to add, except to note that we may want activation-only layers for these functions as well possibly?
Regarding the tests, the PyTorch comparison is a nice utility. Are the tests that use it marked as needing pytorch? I'm pretty sure we have a marker for that.

Sorry for replying late, yes the test has the has_torch",True,{}
explosion/thinc,https://github.com/explosion/thinc,569,2021-12-16T08:43:04Z,2021-12-16T12:02:54Z,2021-12-16T12:02:57Z,MERGED,True,2,2,2,https://github.com/svlandeg,upper pin to mypy,2,[],https://github.com/explosion/thinc/pull/569,https://github.com/svlandeg,1,https://github.com/explosion/thinc/pull/569,cf explosion/spaCy#9873,cf explosion/spaCy#9873,True,{}
explosion/thinc,https://github.com/explosion/thinc,570,2021-12-17T08:22:23Z,2021-12-17T09:30:42Z,2021-12-17T09:30:42Z,MERGED,True,1,1,1,https://github.com/adrianeboyd,Set version to v8.0.14.dev0,1,[],https://github.com/explosion/thinc/pull/570,https://github.com/adrianeboyd,1,https://github.com/explosion/thinc/pull/570,,,True,{}
explosion/thinc,https://github.com/explosion/thinc,571,2021-12-20T13:57:25Z,2021-12-20T14:48:42Z,2021-12-20T14:48:42Z,MERGED,True,2,0,1,https://github.com/adrianeboyd,Add extra for cupy-cuda115,1,[],https://github.com/explosion/thinc/pull/571,https://github.com/adrianeboyd,1,https://github.com/explosion/thinc/pull/571,,,True,{}
explosion/thinc,https://github.com/explosion/thinc,572,2022-01-16T11:44:01Z,2022-03-03T09:21:24Z,2022-03-03T09:21:24Z,MERGED,True,165,50,10,https://github.com/polm,Add assert that size hasn't changed for reduce mean backprop,18,"['feat / layers', 'feat / ux']",https://github.com/explosion/thinc/pull/572,https://github.com/polm,1,https://github.com/explosion/thinc/pull/572,"Behavior of this with numpyops seems wrong, as instead of giving an
error it produces nans, as though it's accessing out of bounds memory or
something. See explosion/spaCy#9669.
Some more general issues / questions around this:

Do we want to add this kind of check for all cases where appropriate?
Is there a better place (like in numpyops) for this check or an equivalent check?
Other asserts in Thinc seem to not use messages, should the message here be removed?","Behavior of this with numpyops seems wrong, as instead of giving an
error it produces nans, as though it's accessing out of bounds memory or
something. See explosion/spaCy#9669.
Some more general issues / questions around this:

Do we want to add this kind of check for all cases where appropriate?
Is there a better place (like in numpyops) for this check or an equivalent check?
Other asserts in Thinc seem to not use messages, should the message here be removed?",True,{}
explosion/thinc,https://github.com/explosion/thinc,572,2022-01-16T11:44:01Z,2022-03-03T09:21:24Z,2022-03-03T09:21:24Z,MERGED,True,165,50,10,https://github.com/polm,Add assert that size hasn't changed for reduce mean backprop,18,"['feat / layers', 'feat / ux']",https://github.com/explosion/thinc/pull/572,https://github.com/svlandeg,2,https://github.com/explosion/thinc/pull/572#issuecomment-1014800111,"Behavior of this with numpyops seems wrong, as instead of giving an
error it produces nans, as though it's accessing out of bounds memory or
something. See explosion/spaCy#9669.
Some more general issues / questions around this:

Do we want to add this kind of check for all cases where appropriate?
Is there a better place (like in numpyops) for this check or an equivalent check?
Other asserts in Thinc seem to not use messages, should the message here be removed?","Ah, good catch on the spaCy PR!
I would personally only use assert statements to check for logic that is entirely governed by the internal code, i.e. not checking for user input. In the context of this function being defined in Thinc, I'm not sure that holds up? I think this becomes apparent from the fact that you want to throw a user-facing error message - i.e. maybe this should be an actual check + raising an error.
I definitely think it would be good to check for other places where such checks would be appropriate, as it'll definitely help with user experience to catch these kind of mismatches earlier.",True,{'THUMBS_UP': ['https://github.com/polm']}
explosion/thinc,https://github.com/explosion/thinc,572,2022-01-16T11:44:01Z,2022-03-03T09:21:24Z,2022-03-03T09:21:24Z,MERGED,True,165,50,10,https://github.com/polm,Add assert that size hasn't changed for reduce mean backprop,18,"['feat / layers', 'feat / ux']",https://github.com/explosion/thinc/pull/572,https://github.com/polm,3,https://github.com/explosion/thinc/pull/572#issuecomment-1032407447,"Behavior of this with numpyops seems wrong, as instead of giving an
error it produces nans, as though it's accessing out of bounds memory or
something. See explosion/spaCy#9669.
Some more general issues / questions around this:

Do we want to add this kind of check for all cases where appropriate?
Is there a better place (like in numpyops) for this check or an equivalent check?
Other asserts in Thinc seem to not use messages, should the message here be removed?","OK, I have moved the logic into the actual layer implementation. Currently this covers all the reduce_* layers and, somewhat arbitrarily, softmax.
I think there are more things that could be covered by this, but it might make sense to use a decorator for the backprop function. Something like this:
def forward(...):
    Y = ...

    @consistent_backprop(Y.size)
    def backprop(dY: ..., ...)
        ....

Inspecting arguments in a decorator can get complicated, but it looks like all our backprop functions are unary, so that should actually be pretty simple to do.",True,{}
explosion/thinc,https://github.com/explosion/thinc,572,2022-01-16T11:44:01Z,2022-03-03T09:21:24Z,2022-03-03T09:21:24Z,MERGED,True,165,50,10,https://github.com/polm,Add assert that size hasn't changed for reduce mean backprop,18,"['feat / layers', 'feat / ux']",https://github.com/explosion/thinc/pull/572,https://github.com/polm,4,https://github.com/explosion/thinc/pull/572#issuecomment-1038669212,"Behavior of this with numpyops seems wrong, as instead of giving an
error it produces nans, as though it's accessing out of bounds memory or
something. See explosion/spaCy#9669.
Some more general issues / questions around this:

Do we want to add this kind of check for all cases where appropriate?
Is there a better place (like in numpyops) for this check or an equivalent check?
Other asserts in Thinc seem to not use messages, should the message here be removed?","This is OK as-is, but I left it in draft to think about whether there was a more general way to apply it, and whether it should be applied to other functions.
It would be helpful to check this for basically any layer that has an array as input and output; this mainly covers the reduce family because that's where I noticed the error. So there's also a question of which other layers exactly it should be applied to.",True,{'THUMBS_UP': ['https://github.com/svlandeg']}
explosion/thinc,https://github.com/explosion/thinc,572,2022-01-16T11:44:01Z,2022-03-03T09:21:24Z,2022-03-03T09:21:24Z,MERGED,True,165,50,10,https://github.com/polm,Add assert that size hasn't changed for reduce mean backprop,18,"['feat / layers', 'feat / ux']",https://github.com/explosion/thinc/pull/572,https://github.com/polm,5,https://github.com/explosion/thinc/pull/572#issuecomment-1041195360,"Behavior of this with numpyops seems wrong, as instead of giving an
error it produces nans, as though it's accessing out of bounds memory or
something. See explosion/spaCy#9669.
Some more general issues / questions around this:

Do we want to add this kind of check for all cases where appropriate?
Is there a better place (like in numpyops) for this check or an equivalent check?
Other asserts in Thinc seem to not use messages, should the message here be removed?","For now I think we don't want to expand this to other functions, so I'd just consider it ready as-is. I would like to consider adding more checks like this later, since I think when debugging it can help avoid cryptic errors. On the other hand it really is more important for some of these reduce functions which will silently and unpredictably produce NaNs with bad input.",True,{'THUMBS_UP': ['https://github.com/svlandeg']}
explosion/thinc,https://github.com/explosion/thinc,572,2022-01-16T11:44:01Z,2022-03-03T09:21:24Z,2022-03-03T09:21:24Z,MERGED,True,165,50,10,https://github.com/polm,Add assert that size hasn't changed for reduce mean backprop,18,"['feat / layers', 'feat / ux']",https://github.com/explosion/thinc/pull/572,https://github.com/polm,6,https://github.com/explosion/thinc/pull/572#issuecomment-1050568581,"Behavior of this with numpyops seems wrong, as instead of giving an
error it produces nans, as though it's accessing out of bounds memory or
something. See explosion/spaCy#9669.
Some more general issues / questions around this:

Do we want to add this kind of check for all cases where appropriate?
Is there a better place (like in numpyops) for this check or an equivalent check?
Other asserts in Thinc seem to not use messages, should the message here be removed?","After thinking about this a bit, this includes a non-decorator approach to this. The basic idea is to have a dataclass that contains information about the array necessary for consistency checking, and to call a method on the dataclass on the gradient that checks consistency. That looks a little like this:
def forward(...):
    ...
    ainfo = create_arrayinfo(Y)
    def backward(dY: OutT):
        ainfo.check_consistency(dY)

The consistency check throws an exception if something is wrong.
This is slightly more complicated than the decorator but not much. Error handling code is still in one place, and the factory function takes care of getting rid of any references that would hold up GC. You have to add two lines (besides an import) instead of the one decorator, but that's not bad. You do need to provide arguments, but again, no big deal.
The create_arrayinfo call is a little weird and there are several alternatives to consider

ArrayInfo.create(...): maybe more awkward?
override constructor ArrayInfo(...): Not sure if there are any downsides to doing that with a dataclass, probably not here?

If this looks OK I can replace the decorator with it wherever necessary.",True,{}
explosion/thinc,https://github.com/explosion/thinc,572,2022-01-16T11:44:01Z,2022-03-03T09:21:24Z,2022-03-03T09:21:24Z,MERGED,True,165,50,10,https://github.com/polm,Add assert that size hasn't changed for reduce mean backprop,18,"['feat / layers', 'feat / ux']",https://github.com/explosion/thinc/pull/572,https://github.com/svlandeg,7,https://github.com/explosion/thinc/pull/572#issuecomment-1050851492,"Behavior of this with numpyops seems wrong, as instead of giving an
error it produces nans, as though it's accessing out of bounds memory or
something. See explosion/spaCy#9669.
Some more general issues / questions around this:

Do we want to add this kind of check for all cases where appropriate?
Is there a better place (like in numpyops) for this check or an equivalent check?
Other asserts in Thinc seem to not use messages, should the message here be removed?","The create_arrayinfo call is a little weird and there are several alternatives to consider
1. `ArrayInfo.create(...)`: maybe more awkward?

2. override constructor `ArrayInfo(...)`: Not sure if there are any downsides to doing that with a dataclass, probably not here?


Maybe ArrayInfo.from_array()?",True,{}
explosion/thinc,https://github.com/explosion/thinc,572,2022-01-16T11:44:01Z,2022-03-03T09:21:24Z,2022-03-03T09:21:24Z,MERGED,True,165,50,10,https://github.com/polm,Add assert that size hasn't changed for reduce mean backprop,18,"['feat / layers', 'feat / ux']",https://github.com/explosion/thinc/pull/572,https://github.com/honnibal,8,https://github.com/explosion/thinc/pull/572#issuecomment-1050853270,"Behavior of this with numpyops seems wrong, as instead of giving an
error it produces nans, as though it's accessing out of bounds memory or
something. See explosion/spaCy#9669.
Some more general issues / questions around this:

Do we want to add this kind of check for all cases where appropriate?
Is there a better place (like in numpyops) for this check or an equivalent check?
Other asserts in Thinc seem to not use messages, should the message here be removed?","Maybe ArrayInfo.from_array()?

Yeah that's what I would call it, and we definitely prefer not to override the dataclass constructors, the classmethod is the way to go ",True,{}
explosion/thinc,https://github.com/explosion/thinc,572,2022-01-16T11:44:01Z,2022-03-03T09:21:24Z,2022-03-03T09:21:24Z,MERGED,True,165,50,10,https://github.com/polm,Add assert that size hasn't changed for reduce mean backprop,18,"['feat / layers', 'feat / ux']",https://github.com/explosion/thinc/pull/572,https://github.com/honnibal,9,https://github.com/explosion/thinc/pull/572#issuecomment-1050856139,"Behavior of this with numpyops seems wrong, as instead of giving an
error it produces nans, as though it's accessing out of bounds memory or
something. See explosion/spaCy#9669.
Some more general issues / questions around this:

Do we want to add this kind of check for all cases where appropriate?
Is there a better place (like in numpyops) for this check or an equivalent check?
Other asserts in Thinc seem to not use messages, should the message here be removed?",There's another question around whether to make the check a method or a function. I probably would've chosen a function but I don't really have a strong motive either way.,True,{}
explosion/thinc,https://github.com/explosion/thinc,572,2022-01-16T11:44:01Z,2022-03-03T09:21:24Z,2022-03-03T09:21:24Z,MERGED,True,165,50,10,https://github.com/polm,Add assert that size hasn't changed for reduce mean backprop,18,"['feat / layers', 'feat / ux']",https://github.com/explosion/thinc/pull/572,https://github.com/polm,10,https://github.com/explosion/thinc/pull/572#issuecomment-1053924714,"Behavior of this with numpyops seems wrong, as instead of giving an
error it produces nans, as though it's accessing out of bounds memory or
something. See explosion/spaCy#9669.
Some more general issues / questions around this:

Do we want to add this kind of check for all cases where appropriate?
Is there a better place (like in numpyops) for this check or an equivalent check?
Other asserts in Thinc seem to not use messages, should the message here be removed?","OK, switched to create with ArrayInfo.from_array() everywhere, so I think this is good now.

There's another question around whether to make the check a method or a function. I probably would've chosen a function but I don't really have a strong motive either way.

It's not a big deal but I think one advantage of using the method is reducing the number of names you have to import.",True,{}
explosion/thinc,https://github.com/explosion/thinc,572,2022-01-16T11:44:01Z,2022-03-03T09:21:24Z,2022-03-03T09:21:24Z,MERGED,True,165,50,10,https://github.com/polm,Add assert that size hasn't changed for reduce mean backprop,18,"['feat / layers', 'feat / ux']",https://github.com/explosion/thinc/pull/572,https://github.com/polm,11,https://github.com/explosion/thinc/pull/572#issuecomment-1053927308,"Behavior of this with numpyops seems wrong, as instead of giving an
error it produces nans, as though it's accessing out of bounds memory or
something. See explosion/spaCy#9669.
Some more general issues / questions around this:

Do we want to add this kind of check for all cases where appropriate?
Is there a better place (like in numpyops) for this check or an equivalent check?
Other asserts in Thinc seem to not use messages, should the message here be removed?",Note: this needed a merge because the recent softmax normalization changes caused a minor conflict.,True,{}
explosion/thinc,https://github.com/explosion/thinc,572,2022-01-16T11:44:01Z,2022-03-03T09:21:24Z,2022-03-03T09:21:24Z,MERGED,True,165,50,10,https://github.com/polm,Add assert that size hasn't changed for reduce mean backprop,18,"['feat / layers', 'feat / ux']",https://github.com/explosion/thinc/pull/572,https://github.com/svlandeg,12,https://github.com/explosion/thinc/pull/572#issuecomment-1054599777,"Behavior of this with numpyops seems wrong, as instead of giving an
error it produces nans, as though it's accessing out of bounds memory or
something. See explosion/spaCy#9669.
Some more general issues / questions around this:

Do we want to add this kind of check for all cases where appropriate?
Is there a better place (like in numpyops) for this check or an equivalent check?
Other asserts in Thinc seem to not use messages, should the message here be removed?",Looks good to me! There just seem to be a few issues with mypy that still need to get fixed.,True,{}
explosion/thinc,https://github.com/explosion/thinc,572,2022-01-16T11:44:01Z,2022-03-03T09:21:24Z,2022-03-03T09:21:24Z,MERGED,True,165,50,10,https://github.com/polm,Add assert that size hasn't changed for reduce mean backprop,18,"['feat / layers', 'feat / ux']",https://github.com/explosion/thinc/pull/572,https://github.com/polm,13,https://github.com/explosion/thinc/pull/572#issuecomment-1056303522,"Behavior of this with numpyops seems wrong, as instead of giving an
error it produces nans, as though it's accessing out of bounds memory or
something. See explosion/spaCy#9669.
Some more general issues / questions around this:

Do we want to add this kind of check for all cases where appropriate?
Is there a better place (like in numpyops) for this check or an equivalent check?
Other asserts in Thinc seem to not use messages, should the message here be removed?","OK, partly resolved the typing issues.
The remaining issue is that for ArrayXd types, the dtype is supposedly a string, which means it doesn't have a name member. But in practice that's definitely not the case - with numpy ops the arrays are just numpy arrays, where dtype is a particular kind of type object.
Should the ArrayXd type annotation here be fixed, and if fixed what would it look like? Or should I just put in a command to ignore typing?",True,{}
explosion/thinc,https://github.com/explosion/thinc,572,2022-01-16T11:44:01Z,2022-03-03T09:21:24Z,2022-03-03T09:21:24Z,MERGED,True,165,50,10,https://github.com/polm,Add assert that size hasn't changed for reduce mean backprop,18,"['feat / layers', 'feat / ux']",https://github.com/explosion/thinc/pull/572,https://github.com/svlandeg,14,https://github.com/explosion/thinc/pull/572#issuecomment-1056745241,"Behavior of this with numpyops seems wrong, as instead of giving an
error it produces nans, as though it's accessing out of bounds memory or
something. See explosion/spaCy#9669.
Some more general issues / questions around this:

Do we want to add this kind of check for all cases where appropriate?
Is there a better place (like in numpyops) for this check or an equivalent check?
Other asserts in Thinc seem to not use messages, should the message here be removed?","The remaining issue is that for ArrayXd types, the dtype is supposedly a string, which means it doesn't have a name member. But in practice that's definitely not the case - with numpy ops the arrays are just numpy arrays, where dtype is a particular kind of type object.

Can't we just remove the .name bit in all three places?",True,{}
explosion/thinc,https://github.com/explosion/thinc,572,2022-01-16T11:44:01Z,2022-03-03T09:21:24Z,2022-03-03T09:21:24Z,MERGED,True,165,50,10,https://github.com/polm,Add assert that size hasn't changed for reduce mean backprop,18,"['feat / layers', 'feat / ux']",https://github.com/explosion/thinc/pull/572,https://github.com/polm,15,https://github.com/explosion/thinc/pull/572#issuecomment-1056777037,"Behavior of this with numpyops seems wrong, as instead of giving an
error it produces nans, as though it's accessing out of bounds memory or
something. See explosion/spaCy#9669.
Some more general issues / questions around this:

Do we want to add this kind of check for all cases where appropriate?
Is there a better place (like in numpyops) for this check or an equivalent check?
Other asserts in Thinc seem to not use messages, should the message here be removed?",That works too!,True,{'HOORAY': ['https://github.com/svlandeg']}
explosion/thinc,https://github.com/explosion/thinc,573,2022-01-17T08:25:46Z,2022-03-30T13:46:44Z,2022-03-30T13:46:47Z,MERGED,True,10,11,1,https://github.com/danieldk,Do not use an explicit loop in unflatten,3,"['enhancement', 'performance', 'feat / ops']",https://github.com/explosion/thinc/pull/573,https://github.com/danieldk,1,https://github.com/explosion/thinc/pull/573,"...to avoid many small GPU <-> CPU transfers.
I still have to benchmark this change to see if this leads to a meaningful training time difference.","...to avoid many small GPU <-> CPU transfers.
I still have to benchmark this change to see if this leads to a meaningful training time difference.",True,{}
explosion/thinc,https://github.com/explosion/thinc,573,2022-01-17T08:25:46Z,2022-03-30T13:46:44Z,2022-03-30T13:46:47Z,MERGED,True,10,11,1,https://github.com/danieldk,Do not use an explicit loop in unflatten,3,"['enhancement', 'performance', 'feat / ops']",https://github.com/explosion/thinc/pull/573,https://github.com/danieldk,2,https://github.com/explosion/thinc/pull/573#issuecomment-1014320819,"...to avoid many small GPU <-> CPU transfers.
I still have to benchmark this change to see if this leads to a meaningful training time difference.",@explosion-bot please test_gpu,True,"{'EYES': ['https://github.com/explosion-bot'], 'ROCKET': ['https://github.com/explosion-bot']}"
explosion/thinc,https://github.com/explosion/thinc,573,2022-01-17T08:25:46Z,2022-03-30T13:46:44Z,2022-03-30T13:46:47Z,MERGED,True,10,11,1,https://github.com/danieldk,Do not use an explicit loop in unflatten,3,"['enhancement', 'performance', 'feat / ops']",https://github.com/explosion/thinc/pull/573,https://github.com/explosion-bot,3,https://github.com/explosion/thinc/pull/573#issuecomment-1014321217,"...to avoid many small GPU <-> CPU transfers.
I still have to benchmark this change to see if this leads to a meaningful training time difference."," Successfully triggered build on Buildkite

URL: https://buildkite.com/explosion-ai/thinc-gpu-test-suite/builds/15",True,{}
explosion/thinc,https://github.com/explosion/thinc,573,2022-01-17T08:25:46Z,2022-03-30T13:46:44Z,2022-03-30T13:46:47Z,MERGED,True,10,11,1,https://github.com/danieldk,Do not use an explicit loop in unflatten,3,"['enhancement', 'performance', 'feat / ops']",https://github.com/explosion/thinc/pull/573,https://github.com/svlandeg,4,https://github.com/explosion/thinc/pull/573#issuecomment-1017362855,"...to avoid many small GPU <-> CPU transfers.
I still have to benchmark this change to see if this leads to a meaningful training time difference.",@danieldk : looks good! Feel free to merge in once you're done with your benchmarks and everything seems OK :-),True,{}
explosion/thinc,https://github.com/explosion/thinc,573,2022-01-17T08:25:46Z,2022-03-30T13:46:44Z,2022-03-30T13:46:47Z,MERGED,True,10,11,1,https://github.com/danieldk,Do not use an explicit loop in unflatten,3,"['enhancement', 'performance', 'feat / ops']",https://github.com/explosion/thinc/pull/573,https://github.com/danieldk,5,https://github.com/explosion/thinc/pull/573#issuecomment-1022924544,"...to avoid many small GPU <-> CPU transfers.
I still have to benchmark this change to see if this leads to a meaningful training time difference.",Just a status update: I want to benchmark this a little more.,True,{'THUMBS_UP': ['https://github.com/svlandeg']}
explosion/thinc,https://github.com/explosion/thinc,573,2022-01-17T08:25:46Z,2022-03-30T13:46:44Z,2022-03-30T13:46:47Z,MERGED,True,10,11,1,https://github.com/danieldk,Do not use an explicit loop in unflatten,3,"['enhancement', 'performance', 'feat / ops']",https://github.com/explosion/thinc/pull/573,https://github.com/danieldk,6,https://github.com/explosion/thinc/pull/573#issuecomment-1083161593,"...to avoid many small GPU <-> CPU transfers.
I still have to benchmark this change to see if this leads to a meaningful training time difference.","I did more benchmarks and this change seems to make training slightly slower (2-3%) and inference slightly faster by the same amount. Since a declarative style is nicer in general and since this should scale better (the looping over the array will get more expensive with longer lengths arrays), I'll merge this.",True,{'THUMBS_UP': ['https://github.com/svlandeg']}
explosion/thinc,https://github.com/explosion/thinc,574,2022-01-17T09:18:36Z,2022-01-17T09:29:42Z,2022-01-17T09:29:42Z,CLOSED,False,2,2,1,https://github.com/varunsurapaneni,Update pydantic version and unpin mypy version,1,[],https://github.com/explosion/thinc/pull/574,https://github.com/varunsurapaneni,1,https://github.com/explosion/thinc/pull/574,"#569 pinned the mypy version because pydantic was not yet compatible with mypy versions > 0.910.
pydantic v1.9.0 was released and includes the following release note:

Extend pydantic's mypy plugin to support mypy versions 0.910, 0.920, 0.921 & 0.930

This updates requirements.txt to unpin the mypy version and update the pydantic version to allow for version 1.9.0.","#569 pinned the mypy version because pydantic was not yet compatible with mypy versions > 0.910.
pydantic v1.9.0 was released and includes the following release note:

Extend pydantic's mypy plugin to support mypy versions 0.910, 0.920, 0.921 & 0.930

This updates requirements.txt to unpin the mypy version and update the pydantic version to allow for version 1.9.0.",True,{}
explosion/thinc,https://github.com/explosion/thinc,575,2022-01-17T11:18:43Z,2022-01-18T11:06:08Z,2022-01-18T11:09:03Z,MERGED,True,1,1,1,https://github.com/danieldk,Set atol for Swish to 1e-6 when comparing output to PyTorch,1,[],https://github.com/explosion/thinc/pull/575,https://github.com/danieldk,1,https://github.com/explosion/thinc/pull/575,Otherwise the test fails with PyToch from the main branch.,Otherwise the test fails with PyToch from the main branch.,True,{}
explosion/thinc,https://github.com/explosion/thinc,576,2022-01-19T09:46:04Z,2022-01-21T05:54:54Z,2022-01-21T05:54:57Z,MERGED,True,502,89,7,https://github.com/danieldk,Add support for sequence lengths to seq2col,4,"['enhancement', 'feat / ops']",https://github.com/explosion/thinc/pull/576,https://github.com/danieldk,1,https://github.com/explosion/thinc/pull/576,"This change adds a lens option to seq2col and seq2col_backprop. When this option is set, the data is divided in sequences with the given lengths. Padding is added for parts of the window that are outside a sequence.
Checks

The accuracies for existing models does not change:

 CPU
 GPU


The accuracy for a newly-trained model does not change:

 CPU
 GPU


There are no performance regressions for predictions with lengths=None:

 CPU
 GPU


There are no performance regressions for training with lengths=None:

 CPU
 GPU","This change adds a lens option to seq2col and seq2col_backprop. When this option is set, the data is divided in sequences with the given lengths. Padding is added for parts of the window that are outside a sequence.
Checks

The accuracies for existing models does not change:

 CPU
 GPU


The accuracy for a newly-trained model does not change:

 CPU
 GPU


There are no performance regressions for predictions with lengths=None:

 CPU
 GPU


There are no performance regressions for training with lengths=None:

 CPU
 GPU",True,{'THUMBS_UP': ['https://github.com/svlandeg']}
explosion/thinc,https://github.com/explosion/thinc,577,2022-01-19T11:37:18Z,2022-02-21T12:25:37Z,2022-02-21T12:25:38Z,MERGED,True,378,6,3,https://github.com/danieldk,"Add kernels for GELU, Swish, and clipped_linear",9,"['enhancement', 'feat / ops']",https://github.com/explosion/thinc/pull/577,https://github.com/danieldk,1,https://github.com/explosion/thinc/pull/577,CUDA kernels for some of the newly-added activation functions.,CUDA kernels for some of the newly-added activation functions.,True,{'ROCKET': ['https://github.com/svlandeg']}
explosion/thinc,https://github.com/explosion/thinc,577,2022-01-19T11:37:18Z,2022-02-21T12:25:37Z,2022-02-21T12:25:38Z,MERGED,True,378,6,3,https://github.com/danieldk,"Add kernels for GELU, Swish, and clipped_linear",9,"['enhancement', 'feat / ops']",https://github.com/explosion/thinc/pull/577,https://github.com/danieldk,2,https://github.com/explosion/thinc/pull/577#issuecomment-1031193796,CUDA kernels for some of the newly-added activation functions.,@explosion-bot please test_gpu,True,"{'EYES': ['https://github.com/explosion-bot'], 'ROCKET': ['https://github.com/explosion-bot']}"
explosion/thinc,https://github.com/explosion/thinc,577,2022-01-19T11:37:18Z,2022-02-21T12:25:37Z,2022-02-21T12:25:38Z,MERGED,True,378,6,3,https://github.com/danieldk,"Add kernels for GELU, Swish, and clipped_linear",9,"['enhancement', 'feat / ops']",https://github.com/explosion/thinc/pull/577,https://github.com/explosion-bot,3,https://github.com/explosion/thinc/pull/577#issuecomment-1031194293,CUDA kernels for some of the newly-added activation functions.," Successfully triggered build on Buildkite

URL: https://buildkite.com/explosion-ai/thinc-gpu-test-suite/builds/20",True,{}
explosion/thinc,https://github.com/explosion/thinc,577,2022-01-19T11:37:18Z,2022-02-21T12:25:37Z,2022-02-21T12:25:38Z,MERGED,True,378,6,3,https://github.com/danieldk,"Add kernels for GELU, Swish, and clipped_linear",9,"['enhancement', 'feat / ops']",https://github.com/explosion/thinc/pull/577,https://github.com/danieldk,4,https://github.com/explosion/thinc/pull/577#issuecomment-1046763492,CUDA kernels for some of the newly-added activation functions.,"Discussed that this one was fine to merge, but do you know what the failing test is about; @danieldk ?

Looks similar to #580.",True,{}
explosion/thinc,https://github.com/explosion/thinc,577,2022-01-19T11:37:18Z,2022-02-21T12:25:37Z,2022-02-21T12:25:38Z,MERGED,True,378,6,3,https://github.com/danieldk,"Add kernels for GELU, Swish, and clipped_linear",9,"['enhancement', 'feat / ops']",https://github.com/explosion/thinc/pull/577,https://github.com/danieldk,5,https://github.com/explosion/thinc/pull/577#issuecomment-1046806320,CUDA kernels for some of the newly-added activation functions.,@explosion-bot please test_gpu,True,"{'EYES': ['https://github.com/explosion-bot'], 'ROCKET': ['https://github.com/explosion-bot'], 'THUMBS_UP': ['https://github.com/svlandeg']}"
explosion/thinc,https://github.com/explosion/thinc,577,2022-01-19T11:37:18Z,2022-02-21T12:25:37Z,2022-02-21T12:25:38Z,MERGED,True,378,6,3,https://github.com/danieldk,"Add kernels for GELU, Swish, and clipped_linear",9,"['enhancement', 'feat / ops']",https://github.com/explosion/thinc/pull/577,https://github.com/explosion-bot,6,https://github.com/explosion/thinc/pull/577#issuecomment-1046806779,CUDA kernels for some of the newly-added activation functions.," Successfully triggered build on Buildkite

URL: https://buildkite.com/explosion-ai/thinc-gpu-test-suite/builds/25",True,{}
explosion/thinc,https://github.com/explosion/thinc,578,2022-01-22T10:03:12Z,2022-02-18T14:24:41Z,2022-02-18T14:26:01Z,MERGED,True,62,48,6,https://github.com/danieldk,"Improve mish activation, test against PyTorch",5,"['enhancement', 'feat / ops']",https://github.com/explosion/thinc/pull/578,https://github.com/danieldk,1,https://github.com/explosion/thinc/pull/578,"Improve mish and backprop_mish

Fix incorrect casting of threshold in CUDA kernels.
Fix incorrect numerator of backprop_mish in Ops.
Make mish and mish_backprop invariant to the shape dimensionality.
Use inplace rather than out in Ops to work towards a more
uniform API and make it possible to test the Mish activations
against PyTorch.

Also add mish/backprop_mish to the PyTorch comparison test.","Improve mish and backprop_mish

Fix incorrect casting of threshold in CUDA kernels.
Fix incorrect numerator of backprop_mish in Ops.
Make mish and mish_backprop invariant to the shape dimensionality.
Use inplace rather than out in Ops to work towards a more
uniform API and make it possible to test the Mish activations
against PyTorch.

Also add mish/backprop_mish to the PyTorch comparison test.",True,{'HOORAY': ['https://github.com/svlandeg']}
explosion/thinc,https://github.com/explosion/thinc,578,2022-01-22T10:03:12Z,2022-02-18T14:24:41Z,2022-02-18T14:26:01Z,MERGED,True,62,48,6,https://github.com/danieldk,"Improve mish activation, test against PyTorch",5,"['enhancement', 'feat / ops']",https://github.com/explosion/thinc/pull/578,https://github.com/danieldk,2,https://github.com/explosion/thinc/pull/578#issuecomment-1031194916,"Improve mish and backprop_mish

Fix incorrect casting of threshold in CUDA kernels.
Fix incorrect numerator of backprop_mish in Ops.
Make mish and mish_backprop invariant to the shape dimensionality.
Use inplace rather than out in Ops to work towards a more
uniform API and make it possible to test the Mish activations
against PyTorch.

Also add mish/backprop_mish to the PyTorch comparison test.",@explosion-bot please test_gpu,True,"{'EYES': ['https://github.com/explosion-bot'], 'ROCKET': ['https://github.com/explosion-bot']}"
explosion/thinc,https://github.com/explosion/thinc,578,2022-01-22T10:03:12Z,2022-02-18T14:24:41Z,2022-02-18T14:26:01Z,MERGED,True,62,48,6,https://github.com/danieldk,"Improve mish activation, test against PyTorch",5,"['enhancement', 'feat / ops']",https://github.com/explosion/thinc/pull/578,https://github.com/explosion-bot,3,https://github.com/explosion/thinc/pull/578#issuecomment-1031195313,"Improve mish and backprop_mish

Fix incorrect casting of threshold in CUDA kernels.
Fix incorrect numerator of backprop_mish in Ops.
Make mish and mish_backprop invariant to the shape dimensionality.
Use inplace rather than out in Ops to work towards a more
uniform API and make it possible to test the Mish activations
against PyTorch.

Also add mish/backprop_mish to the PyTorch comparison test."," Successfully triggered build on Buildkite

URL: https://buildkite.com/explosion-ai/thinc-gpu-test-suite/builds/23",True,{}
explosion/thinc,https://github.com/explosion/thinc,579,2022-01-25T14:07:33Z,2022-02-04T12:16:47Z,2022-02-04T12:17:30Z,MERGED,True,20,17,2,https://github.com/danieldk,Speed up maxout by exploiting parallelism better,2,"['performance', 'feat / ops']",https://github.com/explosion/thinc/pull/579,https://github.com/danieldk,1,https://github.com/explosion/thinc/pull/579,"I was profiling the parser refactor and noticed that the maxout kernel is taking more time than I expected:

The maximum amount of parallelism by the kernel is determined by its batch size. However, this leaves the GPU underused. E.g. by default our amount of parallelism is nr_blocks: 128 * nr_threads_per_block: 128 = 16384, which is much larger than the typical batch size.
This PR changes the maxout kernel to parallelize at the output level (each thread computes one output). This makes the maxout kernel about 4.7 times faster with a batch size of 1024 on a RTX 2060 Super:

I haven't updated the backprop variant yet, since it barely appears in the profiles.","I was profiling the parser refactor and noticed that the maxout kernel is taking more time than I expected:

The maximum amount of parallelism by the kernel is determined by its batch size. However, this leaves the GPU underused. E.g. by default our amount of parallelism is nr_blocks: 128 * nr_threads_per_block: 128 = 16384, which is much larger than the typical batch size.
This PR changes the maxout kernel to parallelize at the output level (each thread computes one output). This makes the maxout kernel about 4.7 times faster with a batch size of 1024 on a RTX 2060 Super:

I haven't updated the backprop variant yet, since it barely appears in the profiles.",True,"{'HOORAY': ['https://github.com/honnibal'], 'ROCKET': ['https://github.com/svlandeg']}"
explosion/thinc,https://github.com/explosion/thinc,579,2022-01-25T14:07:33Z,2022-02-04T12:16:47Z,2022-02-04T12:17:30Z,MERGED,True,20,17,2,https://github.com/danieldk,Speed up maxout by exploiting parallelism better,2,"['performance', 'feat / ops']",https://github.com/explosion/thinc/pull/579,https://github.com/honnibal,2,https://github.com/explosion/thinc/pull/579#issuecomment-1021225432,"I was profiling the parser refactor and noticed that the maxout kernel is taking more time than I expected:

The maximum amount of parallelism by the kernel is determined by its batch size. However, this leaves the GPU underused. E.g. by default our amount of parallelism is nr_blocks: 128 * nr_threads_per_block: 128 = 16384, which is much larger than the typical batch size.
This PR changes the maxout kernel to parallelize at the output level (each thread computes one output). This makes the maxout kernel about 4.7 times faster with a batch size of 1024 on a RTX 2060 Super:

I haven't updated the backprop variant yet, since it barely appears in the profiles.",Awesome!,True,{}
explosion/thinc,https://github.com/explosion/thinc,579,2022-01-25T14:07:33Z,2022-02-04T12:16:47Z,2022-02-04T12:17:30Z,MERGED,True,20,17,2,https://github.com/danieldk,Speed up maxout by exploiting parallelism better,2,"['performance', 'feat / ops']",https://github.com/explosion/thinc/pull/579,https://github.com/danieldk,3,https://github.com/explosion/thinc/pull/579#issuecomment-1022924219,"I was profiling the parser refactor and noticed that the maxout kernel is taking more time than I expected:

The maximum amount of parallelism by the kernel is determined by its batch size. However, this leaves the GPU underused. E.g. by default our amount of parallelism is nr_blocks: 128 * nr_threads_per_block: 128 = 16384, which is much larger than the typical batch size.
This PR changes the maxout kernel to parallelize at the output level (each thread computes one output). This makes the maxout kernel about 4.7 times faster with a batch size of 1024 on a RTX 2060 Super:

I haven't updated the backprop variant yet, since it barely appears in the profiles.",Added a test which checks which output as well.,True,{}
explosion/thinc,https://github.com/explosion/thinc,579,2022-01-25T14:07:33Z,2022-02-04T12:16:47Z,2022-02-04T12:17:30Z,MERGED,True,20,17,2,https://github.com/danieldk,Speed up maxout by exploiting parallelism better,2,"['performance', 'feat / ops']",https://github.com/explosion/thinc/pull/579,https://github.com/danieldk,4,https://github.com/explosion/thinc/pull/579#issuecomment-1022934851,"I was profiling the parser refactor and noticed that the maxout kernel is taking more time than I expected:

The maximum amount of parallelism by the kernel is determined by its batch size. However, this leaves the GPU underused. E.g. by default our amount of parallelism is nr_blocks: 128 * nr_threads_per_block: 128 = 16384, which is much larger than the typical batch size.
This PR changes the maxout kernel to parallelize at the output level (each thread computes one output). This makes the maxout kernel about 4.7 times faster with a batch size of 1024 on a RTX 2060 Super:

I haven't updated the backprop variant yet, since it barely appears in the profiles.",@explosion-bot please test_gpu,True,"{'EYES': ['https://github.com/explosion-bot'], 'ROCKET': ['https://github.com/explosion-bot']}"
explosion/thinc,https://github.com/explosion/thinc,579,2022-01-25T14:07:33Z,2022-02-04T12:16:47Z,2022-02-04T12:17:30Z,MERGED,True,20,17,2,https://github.com/danieldk,Speed up maxout by exploiting parallelism better,2,"['performance', 'feat / ops']",https://github.com/explosion/thinc/pull/579,https://github.com/explosion-bot,5,https://github.com/explosion/thinc/pull/579#issuecomment-1022935189,"I was profiling the parser refactor and noticed that the maxout kernel is taking more time than I expected:

The maximum amount of parallelism by the kernel is determined by its batch size. However, this leaves the GPU underused. E.g. by default our amount of parallelism is nr_blocks: 128 * nr_threads_per_block: 128 = 16384, which is much larger than the typical batch size.
This PR changes the maxout kernel to parallelize at the output level (each thread computes one output). This makes the maxout kernel about 4.7 times faster with a batch size of 1024 on a RTX 2060 Super:

I haven't updated the backprop variant yet, since it barely appears in the profiles."," Successfully triggered build on Buildkite

URL: https://buildkite.com/explosion-ai/thinc-gpu-test-suite/builds/16",True,{}
explosion/thinc,https://github.com/explosion/thinc,579,2022-01-25T14:07:33Z,2022-02-04T12:16:47Z,2022-02-04T12:17:30Z,MERGED,True,20,17,2,https://github.com/danieldk,Speed up maxout by exploiting parallelism better,2,"['performance', 'feat / ops']",https://github.com/explosion/thinc/pull/579,https://github.com/danieldk,6,https://github.com/explosion/thinc/pull/579#issuecomment-1022947932,"I was profiling the parser refactor and noticed that the maxout kernel is taking more time than I expected:

The maximum amount of parallelism by the kernel is determined by its batch size. However, this leaves the GPU underused. E.g. by default our amount of parallelism is nr_blocks: 128 * nr_threads_per_block: 128 = 16384, which is much larger than the typical batch size.
This PR changes the maxout kernel to parallelize at the output level (each thread computes one output). This makes the maxout kernel about 4.7 times faster with a batch size of 1024 on a RTX 2060 Super:

I haven't updated the backprop variant yet, since it barely appears in the profiles.","The test failure is unrelated, but interesting (inconsistent results in the grad scaler test).",True,{}
explosion/thinc,https://github.com/explosion/thinc,579,2022-01-25T14:07:33Z,2022-02-04T12:16:47Z,2022-02-04T12:17:30Z,MERGED,True,20,17,2,https://github.com/danieldk,Speed up maxout by exploiting parallelism better,2,"['performance', 'feat / ops']",https://github.com/explosion/thinc/pull/579,https://github.com/danieldk,7,https://github.com/explosion/thinc/pull/579#issuecomment-1023968955,"I was profiling the parser refactor and noticed that the maxout kernel is taking more time than I expected:

The maximum amount of parallelism by the kernel is determined by its batch size. However, this leaves the GPU underused. E.g. by default our amount of parallelism is nr_blocks: 128 * nr_threads_per_block: 128 = 16384, which is much larger than the typical batch size.
This PR changes the maxout kernel to parallelize at the output level (each thread computes one output). This makes the maxout kernel about 4.7 times faster with a batch size of 1024 on a RTX 2060 Super:

I haven't updated the backprop variant yet, since it barely appears in the profiles.",@explosion-bot please test_gpu,True,"{'EYES': ['https://github.com/explosion-bot'], 'ROCKET': ['https://github.com/explosion-bot']}"
explosion/thinc,https://github.com/explosion/thinc,579,2022-01-25T14:07:33Z,2022-02-04T12:16:47Z,2022-02-04T12:17:30Z,MERGED,True,20,17,2,https://github.com/danieldk,Speed up maxout by exploiting parallelism better,2,"['performance', 'feat / ops']",https://github.com/explosion/thinc/pull/579,https://github.com/explosion-bot,8,https://github.com/explosion/thinc/pull/579#issuecomment-1023969345,"I was profiling the parser refactor and noticed that the maxout kernel is taking more time than I expected:

The maximum amount of parallelism by the kernel is determined by its batch size. However, this leaves the GPU underused. E.g. by default our amount of parallelism is nr_blocks: 128 * nr_threads_per_block: 128 = 16384, which is much larger than the typical batch size.
This PR changes the maxout kernel to parallelize at the output level (each thread computes one output). This makes the maxout kernel about 4.7 times faster with a batch size of 1024 on a RTX 2060 Super:

I haven't updated the backprop variant yet, since it barely appears in the profiles."," Successfully triggered build on Buildkite

URL: https://buildkite.com/explosion-ai/thinc-gpu-test-suite/builds/17",True,{}
explosion/thinc,https://github.com/explosion/thinc,580,2022-01-28T08:07:35Z,2022-02-02T16:17:50Z,2022-02-02T16:17:50Z,MERGED,True,2,1,1,https://github.com/danieldk,Remove deadline of test_scale_random_inputs,1,"['bug', 'tests']",https://github.com/explosion/thinc/pull/580,https://github.com/danieldk,1,https://github.com/explosion/thinc/pull/580,By default the hypothesis deadline is 200ms. The test_scale_random_inputs test would frequently fail because on the first run the deadline was exceeded because GPU initialization takes some time.,By default the hypothesis deadline is 200ms. The test_scale_random_inputs test would frequently fail because on the first run the deadline was exceeded because GPU initialization takes some time.,True,{'THUMBS_UP': ['https://github.com/svlandeg']}
explosion/thinc,https://github.com/explosion/thinc,580,2022-01-28T08:07:35Z,2022-02-02T16:17:50Z,2022-02-02T16:17:50Z,MERGED,True,2,1,1,https://github.com/danieldk,Remove deadline of test_scale_random_inputs,1,"['bug', 'tests']",https://github.com/explosion/thinc/pull/580,https://github.com/danieldk,2,https://github.com/explosion/thinc/pull/580#issuecomment-1023977957,By default the hypothesis deadline is 200ms. The test_scale_random_inputs test would frequently fail because on the first run the deadline was exceeded because GPU initialization takes some time.,@explosion-bot please test_gpu,True,"{'EYES': ['https://github.com/explosion-bot'], 'ROCKET': ['https://github.com/explosion-bot']}"
explosion/thinc,https://github.com/explosion/thinc,580,2022-01-28T08:07:35Z,2022-02-02T16:17:50Z,2022-02-02T16:17:50Z,MERGED,True,2,1,1,https://github.com/danieldk,Remove deadline of test_scale_random_inputs,1,"['bug', 'tests']",https://github.com/explosion/thinc/pull/580,https://github.com/explosion-bot,3,https://github.com/explosion/thinc/pull/580#issuecomment-1023978232,By default the hypothesis deadline is 200ms. The test_scale_random_inputs test would frequently fail because on the first run the deadline was exceeded because GPU initialization takes some time.," Successfully triggered build on Buildkite

URL: https://buildkite.com/explosion-ai/thinc-gpu-test-suite/builds/18",True,{}
explosion/thinc,https://github.com/explosion/thinc,581,2022-01-31T11:43:47Z,2022-01-31T16:09:30Z,2022-01-31T16:09:38Z,CLOSED,False,34,1,4,https://github.com/danieldk,Add expand_window.v2 with support for Ragged,1,"['enhancement', 'performance', 'feat / layers']",https://github.com/explosion/thinc/pull/581,https://github.com/danieldk,1,https://github.com/explosion/thinc/pull/581,"This version of expand_window uses the new seq2col lengths
functionality.","This version of expand_window uses the new seq2col lengths
functionality.",True,{}
explosion/thinc,https://github.com/explosion/thinc,581,2022-01-31T11:43:47Z,2022-01-31T16:09:30Z,2022-01-31T16:09:38Z,CLOSED,False,34,1,4,https://github.com/danieldk,Add expand_window.v2 with support for Ragged,1,"['enhancement', 'performance', 'feat / layers']",https://github.com/explosion/thinc/pull/581,https://github.com/honnibal,2,https://github.com/explosion/thinc/pull/581#issuecomment-1025667045,"This version of expand_window uses the new seq2col lengths
functionality.","Do we need the second version here? What if we changed expand_window to work over a Union[List[Floats2d], Ragged]`?",True,{}
explosion/thinc,https://github.com/explosion/thinc,581,2022-01-31T11:43:47Z,2022-01-31T16:09:30Z,2022-01-31T16:09:38Z,CLOSED,False,34,1,4,https://github.com/danieldk,Add expand_window.v2 with support for Ragged,1,"['enhancement', 'performance', 'feat / layers']",https://github.com/explosion/thinc/pull/581,https://github.com/danieldk,3,https://github.com/explosion/thinc/pull/581#issuecomment-1025669370,"This version of expand_window uses the new seq2col lengths
functionality.","Do we need the second version here? What if we changed expand_window to work over a Union[List[Floats2d], Ragged]`?

Works for me too, I wasn't to sure how much we want to overload the existing version. I'll make a new PR.",True,{}
explosion/thinc,https://github.com/explosion/thinc,581,2022-01-31T11:43:47Z,2022-01-31T16:09:30Z,2022-01-31T16:09:38Z,CLOSED,False,34,1,4,https://github.com/danieldk,Add expand_window.v2 with support for Ragged,1,"['enhancement', 'performance', 'feat / layers']",https://github.com/explosion/thinc/pull/581,https://github.com/danieldk,4,https://github.com/explosion/thinc/pull/581#issuecomment-1025945700,"This version of expand_window uses the new seq2col lengths
functionality.",Closed in favor of #582.,True,{}
explosion/thinc,https://github.com/explosion/thinc,582,2022-01-31T13:25:04Z,2022-02-11T12:40:27Z,2022-02-11T12:40:27Z,MERGED,True,35,7,2,https://github.com/danieldk,Add support for Ragged to expand_window.v1,3,"['enhancement', 'feat / layers']",https://github.com/explosion/thinc/pull/582,https://github.com/danieldk,1,https://github.com/explosion/thinc/pull/582,"Used type variables, so that the caller can be sure that when
they put in a type, they get it out as well.","Used type variables, so that the caller can be sure that when
they put in a type, they get it out as well.",True,{}
explosion/thinc,https://github.com/explosion/thinc,582,2022-01-31T13:25:04Z,2022-02-11T12:40:27Z,2022-02-11T12:40:27Z,MERGED,True,35,7,2,https://github.com/danieldk,Add support for Ragged to expand_window.v1,3,"['enhancement', 'feat / layers']",https://github.com/explosion/thinc/pull/582,https://github.com/svlandeg,2,https://github.com/explosion/thinc/pull/582#issuecomment-1029982583,"Used type variables, so that the caller can be sure that when
they put in a type, they get it out as well.","Now I wonder whether we should bump this version, too, because technically it did change / supports extended functionality...",True,{}
explosion/thinc,https://github.com/explosion/thinc,582,2022-01-31T13:25:04Z,2022-02-11T12:40:27Z,2022-02-11T12:40:27Z,MERGED,True,35,7,2,https://github.com/danieldk,Add support for Ragged to expand_window.v1,3,"['enhancement', 'feat / layers']",https://github.com/explosion/thinc/pull/582,https://github.com/danieldk,3,https://github.com/explosion/thinc/pull/582#issuecomment-1029996229,"Used type variables, so that the caller can be sure that when
they put in a type, they get it out as well.","Now I wonder whether we should bump this version, too, because technically it did change / supports extended functionality...

That is #581 :). We are just extending the API, so from the perspective of semantic versioning, it's fine to keep the version the same I think. But I also see the point that people might program against this newer version and then it doesn't work with older thinc versions.",True,{'EYES': ['https://github.com/svlandeg']}
explosion/thinc,https://github.com/explosion/thinc,582,2022-01-31T13:25:04Z,2022-02-11T12:40:27Z,2022-02-11T12:40:27Z,MERGED,True,35,7,2,https://github.com/danieldk,Add support for Ragged to expand_window.v1,3,"['enhancement', 'feat / layers']",https://github.com/explosion/thinc/pull/582,https://github.com/danieldk,4,https://github.com/explosion/thinc/pull/582#issuecomment-1031194681,"Used type variables, so that the caller can be sure that when
they put in a type, they get it out as well.",@explosion-bot please test_gpu,True,"{'EYES': ['https://github.com/explosion-bot'], 'ROCKET': ['https://github.com/explosion-bot']}"
explosion/thinc,https://github.com/explosion/thinc,582,2022-01-31T13:25:04Z,2022-02-11T12:40:27Z,2022-02-11T12:40:27Z,MERGED,True,35,7,2,https://github.com/danieldk,Add support for Ragged to expand_window.v1,3,"['enhancement', 'feat / layers']",https://github.com/explosion/thinc/pull/582,https://github.com/explosion-bot,5,https://github.com/explosion/thinc/pull/582#issuecomment-1031195127,"Used type variables, so that the caller can be sure that when
they put in a type, they get it out as well."," Successfully triggered build on Buildkite

URL: https://buildkite.com/explosion-ai/thinc-gpu-test-suite/builds/22",True,{}
explosion/thinc,https://github.com/explosion/thinc,583,2022-02-02T11:46:04Z,2022-02-11T12:29:44Z,2022-02-11T12:29:44Z,MERGED,True,127,34,6,https://github.com/danieldk,Add the `normalize` option to the Softmax layer,8,"['performance', 'feat / layers']",https://github.com/explosion/thinc/pull/583,https://github.com/danieldk,1,https://github.com/explosion/thinc/pull/583,"When this option is disabled, unnormalized probabilities (logits) are
returned during inference.
I was not too sure if adding a keyword argument requires making a
new version of the layer. The default behavior is unchanged.
Using unnormalized probabilities during inference can give nice
speedups. E.g. on my M1 Pro, a German tagging/parsing pipeline is
~27% faster with unnormalized probabilities due to not having to
use the expensive expf function.","When this option is disabled, unnormalized probabilities (logits) are
returned during inference.
I was not too sure if adding a keyword argument requires making a
new version of the layer. The default behavior is unchanged.
Using unnormalized probabilities during inference can give nice
speedups. E.g. on my M1 Pro, a German tagging/parsing pipeline is
~27% faster with unnormalized probabilities due to not having to
use the expensive expf function.",True,"{'THUMBS_UP': ['https://github.com/honnibal'], 'HOORAY': ['https://github.com/honnibal', 'https://github.com/svlandeg']}"
explosion/thinc,https://github.com/explosion/thinc,583,2022-02-02T11:46:04Z,2022-02-11T12:29:44Z,2022-02-11T12:29:44Z,MERGED,True,127,34,6,https://github.com/danieldk,Add the `normalize` option to the Softmax layer,8,"['performance', 'feat / layers']",https://github.com/explosion/thinc/pull/583,https://github.com/honnibal,2,https://github.com/explosion/thinc/pull/583#issuecomment-1027934488,"When this option is disabled, unnormalized probabilities (logits) are
returned during inference.
I was not too sure if adding a keyword argument requires making a
new version of the layer. The default behavior is unchanged.
Using unnormalized probabilities during inference can give nice
speedups. E.g. on my M1 Pro, a German tagging/parsing pipeline is
~27% faster with unnormalized probabilities due to not having to
use the expensive expf function.","This is great -- the attribute gives runtime control of this, which another layer wouldn't let you do. Nice!
Just make the attr name more specific though, so that it's easier to manipulate the attribute within model.walk() from the top-level model.",True,{}
explosion/thinc,https://github.com/explosion/thinc,583,2022-02-02T11:46:04Z,2022-02-11T12:29:44Z,2022-02-11T12:29:44Z,MERGED,True,127,34,6,https://github.com/danieldk,Add the `normalize` option to the Softmax layer,8,"['performance', 'feat / layers']",https://github.com/explosion/thinc/pull/583,https://github.com/danieldk,3,https://github.com/explosion/thinc/pull/583#issuecomment-1027938510,"When this option is disabled, unnormalized probabilities (logits) are
returned during inference.
I was not too sure if adding a keyword argument requires making a
new version of the layer. The default behavior is unchanged.
Using unnormalized probabilities during inference can give nice
speedups. E.g. on my M1 Pro, a German tagging/parsing pipeline is
~27% faster with unnormalized probabilities due to not having to
use the expensive expf function.","Just make the attr name more specific though, so that it's easier to manipulate the attribute within model.walk() from the top-level model.

Ah, yes, there may be name clashes. I have renamed it to normalize_softmax.",True,{'THUMBS_UP': ['https://github.com/honnibal']}
explosion/thinc,https://github.com/explosion/thinc,583,2022-02-02T11:46:04Z,2022-02-11T12:29:44Z,2022-02-11T12:29:44Z,MERGED,True,127,34,6,https://github.com/danieldk,Add the `normalize` option to the Softmax layer,8,"['performance', 'feat / layers']",https://github.com/explosion/thinc/pull/583,https://github.com/honnibal,4,https://github.com/explosion/thinc/pull/583#issuecomment-1027940295,"When this option is disabled, unnormalized probabilities (logits) are
returned during inference.
I was not too sure if adding a keyword argument requires making a
new version of the layer. The default behavior is unchanged.
Using unnormalized probabilities during inference can give nice
speedups. E.g. on my M1 Pro, a German tagging/parsing pipeline is
~27% faster with unnormalized probabilities due to not having to
use the expensive expf function.",Yeah that's great. The attribute names basically have a global namespace when you're doing model.walk() so it's better to be a bit more specific. But yeah great idea!,True,{}
explosion/thinc,https://github.com/explosion/thinc,583,2022-02-02T11:46:04Z,2022-02-11T12:29:44Z,2022-02-11T12:29:44Z,MERGED,True,127,34,6,https://github.com/danieldk,Add the `normalize` option to the Softmax layer,8,"['performance', 'feat / layers']",https://github.com/explosion/thinc/pull/583,https://github.com/svlandeg,5,https://github.com/explosion/thinc/pull/583#issuecomment-1036164610,"When this option is disabled, unnormalized probabilities (logits) are
returned during inference.
I was not too sure if adding a keyword argument requires making a
new version of the layer. The default behavior is unchanged.
Using unnormalized probabilities during inference can give nice
speedups. E.g. on my M1 Pro, a German tagging/parsing pipeline is
~27% faster with unnormalized probabilities due to not having to
use the expensive expf function.",Merging - we can continue the discussion over at #589,True,{}
explosion/thinc,https://github.com/explosion/thinc,584,2022-02-03T10:29:16Z,2022-02-21T14:07:53Z,2022-02-21T14:07:53Z,MERGED,True,48,8,5,https://github.com/adrianeboyd,Always serialize Model numpy data with little endian byte order,3,"['serialization', 'feat / ops']",https://github.com/explosion/thinc/pull/584,https://github.com/adrianeboyd,1,https://github.com/explosion/thinc/pull/584,Add byte_order kwarg to Ops.to_numpy and CupyOps.to_numpy,Add byte_order kwarg to Ops.to_numpy and CupyOps.to_numpy,True,"{'HOORAY': ['https://github.com/danieldk', 'https://github.com/svlandeg']}"
explosion/thinc,https://github.com/explosion/thinc,584,2022-02-03T10:29:16Z,2022-02-21T14:07:53Z,2022-02-21T14:07:53Z,MERGED,True,48,8,5,https://github.com/adrianeboyd,Always serialize Model numpy data with little endian byte order,3,"['serialization', 'feat / ops']",https://github.com/explosion/thinc/pull/584,https://github.com/adrianeboyd,2,https://github.com/explosion/thinc/pull/584#issuecomment-1028877428,Add byte_order kwarg to Ops.to_numpy and CupyOps.to_numpy,@explosion-bot please test_gpu,True,"{'EYES': ['https://github.com/explosion-bot'], 'ROCKET': ['https://github.com/explosion-bot']}"
explosion/thinc,https://github.com/explosion/thinc,584,2022-02-03T10:29:16Z,2022-02-21T14:07:53Z,2022-02-21T14:07:53Z,MERGED,True,48,8,5,https://github.com/adrianeboyd,Always serialize Model numpy data with little endian byte order,3,"['serialization', 'feat / ops']",https://github.com/explosion/thinc/pull/584,https://github.com/explosion-bot,3,https://github.com/explosion/thinc/pull/584#issuecomment-1028877780,Add byte_order kwarg to Ops.to_numpy and CupyOps.to_numpy," Successfully triggered build on Buildkite

URL: https://buildkite.com/explosion-ai/thinc-gpu-test-suite/builds/19",True,{}
explosion/thinc,https://github.com/explosion/thinc,584,2022-02-03T10:29:16Z,2022-02-21T14:07:53Z,2022-02-21T14:07:53Z,MERGED,True,48,8,5,https://github.com/adrianeboyd,Always serialize Model numpy data with little endian byte order,3,"['serialization', 'feat / ops']",https://github.com/explosion/thinc/pull/584,https://github.com/adrianeboyd,4,https://github.com/explosion/thinc/pull/584#issuecomment-1046804303,Add byte_order kwarg to Ops.to_numpy and CupyOps.to_numpy,@explosion-bot please test_gpu,True,"{'EYES': ['https://github.com/explosion-bot'], 'ROCKET': ['https://github.com/explosion-bot']}"
explosion/thinc,https://github.com/explosion/thinc,584,2022-02-03T10:29:16Z,2022-02-21T14:07:53Z,2022-02-21T14:07:53Z,MERGED,True,48,8,5,https://github.com/adrianeboyd,Always serialize Model numpy data with little endian byte order,3,"['serialization', 'feat / ops']",https://github.com/explosion/thinc/pull/584,https://github.com/explosion-bot,5,https://github.com/explosion/thinc/pull/584#issuecomment-1046804784,Add byte_order kwarg to Ops.to_numpy and CupyOps.to_numpy," Successfully triggered build on Buildkite

URL: https://buildkite.com/explosion-ai/thinc-gpu-test-suite/builds/24",True,{}
explosion/thinc,https://github.com/explosion/thinc,585,2022-02-04T10:46:07Z,2022-02-21T10:19:18Z,2022-02-21T10:19:18Z,MERGED,True,27,14,3,https://github.com/danieldk,Ragged: avoid concatenate when computing sequence spans,4,"['performance', 'feat / types']",https://github.com/explosion/thinc/pull/585,https://github.com/danieldk,1,https://github.com/explosion/thinc/pull/585,"When profiling the parser refactor, I found that quite a lot of time is spent on initializing the arc eager oracle. Turns out that this (indirectly) does a lot of indexing on Ragged.
In Ragged, concatenate is used for computing the end indices of sequences from their start indices. This creates a new copy of an array that is largely identical to the array of start indices.
This PR changes Ragged to compute a single array for start/end indices, avoiding concatenate during the array construction.
Before this change:

After this change:

Both the time spent in init_gold and Ragged.__getitem__ is reduced by quite a bit.","When profiling the parser refactor, I found that quite a lot of time is spent on initializing the arc eager oracle. Turns out that this (indirectly) does a lot of indexing on Ragged.
In Ragged, concatenate is used for computing the end indices of sequences from their start indices. This creates a new copy of an array that is largely identical to the array of start indices.
This PR changes Ragged to compute a single array for start/end indices, avoiding concatenate during the array construction.
Before this change:

After this change:

Both the time spent in init_gold and Ragged.__getitem__ is reduced by quite a bit.",True,{'ROCKET': ['https://github.com/svlandeg']}
explosion/thinc,https://github.com/explosion/thinc,585,2022-02-04T10:46:07Z,2022-02-21T10:19:18Z,2022-02-21T10:19:18Z,MERGED,True,27,14,3,https://github.com/danieldk,Ragged: avoid concatenate when computing sequence spans,4,"['performance', 'feat / types']",https://github.com/explosion/thinc/pull/585,https://github.com/danieldk,2,https://github.com/explosion/thinc/pull/585#issuecomment-1031194318,"When profiling the parser refactor, I found that quite a lot of time is spent on initializing the arc eager oracle. Turns out that this (indirectly) does a lot of indexing on Ragged.
In Ragged, concatenate is used for computing the end indices of sequences from their start indices. This creates a new copy of an array that is largely identical to the array of start indices.
This PR changes Ragged to compute a single array for start/end indices, avoiding concatenate during the array construction.
Before this change:

After this change:

Both the time spent in init_gold and Ragged.__getitem__ is reduced by quite a bit.",@explosion-bot please test_gpu,True,"{'EYES': ['https://github.com/explosion-bot'], 'ROCKET': ['https://github.com/explosion-bot']}"
explosion/thinc,https://github.com/explosion/thinc,585,2022-02-04T10:46:07Z,2022-02-21T10:19:18Z,2022-02-21T10:19:18Z,MERGED,True,27,14,3,https://github.com/danieldk,Ragged: avoid concatenate when computing sequence spans,4,"['performance', 'feat / types']",https://github.com/explosion/thinc/pull/585,https://github.com/explosion-bot,3,https://github.com/explosion/thinc/pull/585#issuecomment-1031194780,"When profiling the parser refactor, I found that quite a lot of time is spent on initializing the arc eager oracle. Turns out that this (indirectly) does a lot of indexing on Ragged.
In Ragged, concatenate is used for computing the end indices of sequences from their start indices. This creates a new copy of an array that is largely identical to the array of start indices.
This PR changes Ragged to compute a single array for start/end indices, avoiding concatenate during the array construction.
Before this change:

After this change:

Both the time spent in init_gold and Ragged.__getitem__ is reduced by quite a bit."," Successfully triggered build on Buildkite

URL: https://buildkite.com/explosion-ai/thinc-gpu-test-suite/builds/21",True,{}
explosion/thinc,https://github.com/explosion/thinc,586,2022-02-04T12:51:10Z,2022-02-04T13:41:52Z,2022-02-05T05:13:56Z,MERGED,True,10,10,10,https://github.com/kianmeng,Fix typos,1,['enhancement'],https://github.com/explosion/thinc/pull/586,https://github.com/kianmeng,1,https://github.com/explosion/thinc/pull/586,,,True,{'THUMBS_UP': ['https://github.com/svlandeg']}
explosion/thinc,https://github.com/explosion/thinc,586,2022-02-04T12:51:10Z,2022-02-04T13:41:52Z,2022-02-05T05:13:56Z,MERGED,True,10,10,10,https://github.com/kianmeng,Fix typos,1,['enhancement'],https://github.com/explosion/thinc/pull/586,https://github.com/svlandeg,2,https://github.com/explosion/thinc/pull/586#issuecomment-1029964922,,Thanks so much!,True,{'THUMBS_UP': ['https://github.com/kianmeng']}
explosion/thinc,https://github.com/explosion/thinc,586,2022-02-04T12:51:10Z,2022-02-04T13:41:52Z,2022-02-05T05:13:56Z,MERGED,True,10,10,10,https://github.com/kianmeng,Fix typos,1,['enhancement'],https://github.com/explosion/thinc/pull/586,https://github.com/kianmeng,3,https://github.com/explosion/thinc/pull/586#issuecomment-1030528966,,    ,True,{}
explosion/thinc,https://github.com/explosion/thinc,587,2022-02-05T10:49:31Z,2022-02-16T09:33:03Z,2022-02-16T09:33:03Z,MERGED,True,28,2,2,https://github.com/danieldk,CategoricalCrossEntropy.get_grad: fix invariants check,3,"['bug', 'feat / ops']",https://github.com/explosion/thinc/pull/587,https://github.com/danieldk,1,https://github.com/explosion/thinc/pull/587,"get_grad checked the guesses/target invariants as follows:
if guesses.any() > 1 or guesses.any() < 0:

However, guesses.any() only has two possible outputs, True or False,
which are reinterpreted as 1 or 0, so the condition is always false.
This change updates the conditions to actually check that guesses
and target are in [0, 1].","get_grad checked the guesses/target invariants as follows:
if guesses.any() > 1 or guesses.any() < 0:

However, guesses.any() only has two possible outputs, True or False,
which are reinterpreted as 1 or 0, so the condition is always false.
This change updates the conditions to actually check that guesses
and target are in [0, 1].",True,"{'THUMBS_UP': ['https://github.com/kadarakos', 'https://github.com/svlandeg', 'https://github.com/polm']}"
explosion/thinc,https://github.com/explosion/thinc,587,2022-02-05T10:49:31Z,2022-02-16T09:33:03Z,2022-02-16T09:33:03Z,MERGED,True,28,2,2,https://github.com/danieldk,CategoricalCrossEntropy.get_grad: fix invariants check,3,"['bug', 'feat / ops']",https://github.com/explosion/thinc/pull/587,https://github.com/danieldk,2,https://github.com/explosion/thinc/pull/587#issuecomment-1038384497,"get_grad checked the guesses/target invariants as follows:
if guesses.any() > 1 or guesses.any() < 0:

However, guesses.any() only has two possible outputs, True or False,
which are reinterpreted as 1 or 0, so the condition is always false.
This change updates the conditions to actually check that guesses
and target are in [0, 1].","Could you add a test that failed to fail before this PR?

Fixed!",True,{}
explosion/thinc,https://github.com/explosion/thinc,588,2022-02-07T10:59:37Z,2022-02-21T10:51:24Z,2022-02-21T10:51:25Z,MERGED,True,2,10,1,https://github.com/danieldk,Simplify to_categorical,1,['enhancement'],https://github.com/explosion/thinc/pull/588,https://github.com/danieldk,1,https://github.com/explosion/thinc/pull/588,"This change does not make this function observably faster (even though we are keeping everything on the same device), but I think it is easier to understand:

We make the one-hot encodings for all classes.
We select from these encodings based on the class indices.","This change does not make this function observably faster (even though we are keeping everything on the same device), but I think it is easier to understand:

We make the one-hot encodings for all classes.
We select from these encodings based on the class indices.",True,{}
explosion/thinc,https://github.com/explosion/thinc,589,2022-02-09T12:33:33Z,2022-02-21T10:21:33Z,2022-02-22T08:46:07Z,MERGED,True,152,13,5,https://github.com/danieldk,Softmax_v2: add support for softmax with temperature,6,"['enhancement', 'feat / layers']",https://github.com/explosion/thinc/pull/589,https://github.com/danieldk,1,https://github.com/explosion/thinc/pull/589,Temperature can be used to change the confidence/entropy of a model.,Temperature can be used to change the confidence/entropy of a model.,True,{'ROCKET': ['https://github.com/svlandeg']}
explosion/thinc,https://github.com/explosion/thinc,589,2022-02-09T12:33:33Z,2022-02-21T10:21:33Z,2022-02-22T08:46:07Z,MERGED,True,152,13,5,https://github.com/danieldk,Softmax_v2: add support for softmax with temperature,6,"['enhancement', 'feat / layers']",https://github.com/explosion/thinc/pull/589,https://github.com/danieldk,2,https://github.com/explosion/thinc/pull/589#issuecomment-1036318453,Temperature can be used to change the confidence/entropy of a model.,Rebased.,True,{}
explosion/thinc,https://github.com/explosion/thinc,590,2022-02-15T08:09:51Z,2022-02-16T14:35:51Z,2022-02-16T14:35:51Z,MERGED,True,71,853,10,https://github.com/Jette16,Update example-notebook,12,"['enhancement', 'examples']",https://github.com/explosion/thinc/pull/590,https://github.com/Jette16,1,https://github.com/explosion/thinc/pull/590,"Updated the example notebook with following changes:

Made indentation consistent in notebook 00
Changed install statements --> removed references to alpha and dev versions
Updated mathy to mathy_core in notebook 06
Deleted notebook parallel_training_ray as agreed in Slack
Checked if everything can be executed & cleared outputs (except notebook 03)
Checked if something should be updated for latest Thinc version (but here, I found nothing relevant; I checked it against git and website code)","Updated the example notebook with following changes:

Made indentation consistent in notebook 00
Changed install statements --> removed references to alpha and dev versions
Updated mathy to mathy_core in notebook 06
Deleted notebook parallel_training_ray as agreed in Slack
Checked if everything can be executed & cleared outputs (except notebook 03)
Checked if something should be updated for latest Thinc version (but here, I found nothing relevant; I checked it against git and website code)",True,"{'HOORAY': ['https://github.com/svlandeg'], 'HEART': ['https://github.com/svlandeg']}"
explosion/thinc,https://github.com/explosion/thinc,591,2022-02-16T18:36:32Z,2022-02-17T08:39:51Z,2022-02-17T08:39:51Z,MERGED,True,1,1,1,https://github.com/thatbudakguy,Fix typo in backends docs,1,['docs'],https://github.com/explosion/thinc/pull/591,https://github.com/thatbudakguy,1,https://github.com/explosion/thinc/pull/591,,,True,"{'THUMBS_UP': ['https://github.com/svlandeg', 'https://github.com/kadarakos']}"
explosion/thinc,https://github.com/explosion/thinc,591,2022-02-16T18:36:32Z,2022-02-17T08:39:51Z,2022-02-17T08:39:51Z,MERGED,True,1,1,1,https://github.com/thatbudakguy,Fix typo in backends docs,1,['docs'],https://github.com/explosion/thinc/pull/591,https://github.com/kadarakos,2,https://github.com/explosion/thinc/pull/591#issuecomment-1042699733,,Thank you so much for the fix! Let me just merge it quickly. Have a nice day!,True,{'HEART': ['https://github.com/thatbudakguy']}
explosion/thinc,https://github.com/explosion/thinc,593,2022-02-17T16:40:34Z,2022-02-17T19:01:47Z,2022-02-17T19:01:47Z,MERGED,True,4,4,3,https://github.com/adrianeboyd,Update to murmurhash>=1.0.2,1,[],https://github.com/explosion/thinc/pull/593,https://github.com/adrianeboyd,1,https://github.com/explosion/thinc/pull/593,,,True,{}
explosion/thinc,https://github.com/explosion/thinc,594,2022-02-18T17:07:17Z,2022-02-21T12:05:52Z,2022-02-21T12:05:53Z,MERGED,True,45,2,2,https://github.com/danieldk,Config: do not sort positional arguments,4,"['bug', 'feat / config']",https://github.com/explosion/thinc/pull/594,https://github.com/danieldk,1,https://github.com/explosion/thinc/pull/594,"In a configuration such as the following:
[model]

[model.chain]
@layers = ""chain.v1""

[model.chain.*.hashembed]
@layers = ""HashEmbed.v1""
nO = 8
nV = 8

[model.chain.*.expand_window]
@layers = ""expand_window.v1""
window_size = 1

The positional arguments in the chain were sorted, changing the chain order. This change masks the names of positional arguments. Since sorted is a stable sort, the order of positional arguments is retained.","In a configuration such as the following:
[model]

[model.chain]
@layers = ""chain.v1""

[model.chain.*.hashembed]
@layers = ""HashEmbed.v1""
nO = 8
nV = 8

[model.chain.*.expand_window]
@layers = ""expand_window.v1""
window_size = 1

The positional arguments in the chain were sorted, changing the chain order. This change masks the names of positional arguments. Since sorted is a stable sort, the order of positional arguments is retained.",True,{'HOORAY': ['https://github.com/svlandeg']}
explosion/thinc,https://github.com/explosion/thinc,595,2022-02-21T14:11:18Z,2022-03-11T12:14:02Z,2022-03-11T12:14:02Z,MERGED,True,132,23,5,https://github.com/kadarakos,Label-smoothing,35,"['enhancement', 'feat / loss']",https://github.com/explosion/thinc/pull/595,https://github.com/kadarakos,1,https://github.com/explosion/thinc/pull/595,"Suggestion for the implementation of label-smoothing: https://proceedings.neurips.cc/paper/2019/file/f1748d6b0fd9d439f71450117eba2725-Paper.pdf . In the CategoricalCrossentropy, whenever the truths are one-hot (generated by to_categorical) the smoothing is applied if the smoothing parameter alpha larger than 0.","Suggestion for the implementation of label-smoothing: https://proceedings.neurips.cc/paper/2019/file/f1748d6b0fd9d439f71450117eba2725-Paper.pdf . In the CategoricalCrossentropy, whenever the truths are one-hot (generated by to_categorical) the smoothing is applied if the smoothing parameter alpha larger than 0.",True,"{'THUMBS_UP': ['https://github.com/svlandeg'], 'ROCKET': ['https://github.com/svlandeg']}"
explosion/thinc,https://github.com/explosion/thinc,595,2022-02-21T14:11:18Z,2022-03-11T12:14:02Z,2022-03-11T12:14:02Z,MERGED,True,132,23,5,https://github.com/kadarakos,Label-smoothing,35,"['enhancement', 'feat / loss']",https://github.com/explosion/thinc/pull/595,https://github.com/danieldk,2,https://github.com/explosion/thinc/pull/595#issuecomment-1062662159,"Suggestion for the implementation of label-smoothing: https://proceedings.neurips.cc/paper/2019/file/f1748d6b0fd9d439f71450117eba2725-Paper.pdf . In the CategoricalCrossentropy, whenever the truths are one-hot (generated by to_categorical) the smoothing is applied if the smoothing parameter alpha larger than 0.",@explosion-bot please test_gpu,True,"{'EYES': ['https://github.com/explosion-bot'], 'ROCKET': ['https://github.com/explosion-bot']}"
explosion/thinc,https://github.com/explosion/thinc,595,2022-02-21T14:11:18Z,2022-03-11T12:14:02Z,2022-03-11T12:14:02Z,MERGED,True,132,23,5,https://github.com/kadarakos,Label-smoothing,35,"['enhancement', 'feat / loss']",https://github.com/explosion/thinc/pull/595,https://github.com/explosion-bot,3,https://github.com/explosion/thinc/pull/595#issuecomment-1062662509,"Suggestion for the implementation of label-smoothing: https://proceedings.neurips.cc/paper/2019/file/f1748d6b0fd9d439f71450117eba2725-Paper.pdf . In the CategoricalCrossentropy, whenever the truths are one-hot (generated by to_categorical) the smoothing is applied if the smoothing parameter alpha larger than 0."," Successfully triggered build on Buildkite

URL: https://buildkite.com/explosion-ai/thinc-gpu-test-suite/builds/29",True,{}
explosion/thinc,https://github.com/explosion/thinc,596,2022-02-22T08:07:19Z,2022-02-22T08:50:36Z,2022-02-22T19:43:49Z,MERGED,True,2,2,1,https://github.com/danieldk,test_softmax_temperature: use assert_allclose from ops,1,['tests'],https://github.com/explosion/thinc/pull/596,https://github.com/danieldk,1,https://github.com/explosion/thinc/pull/596,This avoids that the test fails with CupyOps.,This avoids that the test fails with CupyOps.,True,"{'THUMBS_UP': ['https://github.com/kadarakos', 'https://github.com/svlandeg']}"
explosion/thinc,https://github.com/explosion/thinc,596,2022-02-22T08:07:19Z,2022-02-22T08:50:36Z,2022-02-22T19:43:49Z,MERGED,True,2,2,1,https://github.com/danieldk,test_softmax_temperature: use assert_allclose from ops,1,['tests'],https://github.com/explosion/thinc/pull/596,https://github.com/danieldk,2,https://github.com/explosion/thinc/pull/596#issuecomment-1047525478,This avoids that the test fails with CupyOps.,@explosion-bot please test_gpu,True,"{'EYES': ['https://github.com/explosion-bot'], 'ROCKET': ['https://github.com/explosion-bot']}"
explosion/thinc,https://github.com/explosion/thinc,596,2022-02-22T08:07:19Z,2022-02-22T08:50:36Z,2022-02-22T19:43:49Z,MERGED,True,2,2,1,https://github.com/danieldk,test_softmax_temperature: use assert_allclose from ops,1,['tests'],https://github.com/explosion/thinc/pull/596,https://github.com/explosion-bot,3,https://github.com/explosion/thinc/pull/596#issuecomment-1047525846,This avoids that the test fails with CupyOps.," Successfully triggered build on Buildkite

URL: https://buildkite.com/explosion-ai/thinc-gpu-test-suite/builds/26",True,{}
explosion/thinc,https://github.com/explosion/thinc,597,2022-02-22T12:27:43Z,2022-03-07T12:06:17Z,2022-03-21T09:12:05Z,MERGED,True,210,81,2,https://github.com/danieldk,Improve bound and type checks in CUDA kernels,19,"['feat / types', 'feat / ops']",https://github.com/explosion/thinc/pull/597,https://github.com/danieldk,1,https://github.com/explosion/thinc/pull/597,"This PR combines some improvements that should remove most accidental out of bounds reads/writes:

Check that floating point kernel inputs have dtype float32.
Check that integer kernel inputs have dtype int32.
Check in elementwise backprop kernels that the shapes of X and Y match that of dY (where applicable).
Check that output arrays have the correct shape when they are provided.
Check that lengths are >= 0 and that lengths sum up to the number of instances.
Check that which used in backprop_maxout and backprop_reduce_max are in bounds.

The out option of mish is replaced by inplace, to give mish the same interface as other elementwise kernels. This should not break the API, since _custom_kernels is a private module.","This PR combines some improvements that should remove most accidental out of bounds reads/writes:

Check that floating point kernel inputs have dtype float32.
Check that integer kernel inputs have dtype int32.
Check in elementwise backprop kernels that the shapes of X and Y match that of dY (where applicable).
Check that output arrays have the correct shape when they are provided.
Check that lengths are >= 0 and that lengths sum up to the number of instances.
Check that which used in backprop_maxout and backprop_reduce_max are in bounds.

The out option of mish is replaced by inplace, to give mish the same interface as other elementwise kernels. This should not break the API, since _custom_kernels is a private module.",True,{'HEART': ['https://github.com/svlandeg']}
explosion/thinc,https://github.com/explosion/thinc,597,2022-02-22T12:27:43Z,2022-03-07T12:06:17Z,2022-03-21T09:12:05Z,MERGED,True,210,81,2,https://github.com/danieldk,Improve bound and type checks in CUDA kernels,19,"['feat / types', 'feat / ops']",https://github.com/explosion/thinc/pull/597,https://github.com/danieldk,2,https://github.com/explosion/thinc/pull/597#issuecomment-1048494590,"This PR combines some improvements that should remove most accidental out of bounds reads/writes:

Check that floating point kernel inputs have dtype float32.
Check that integer kernel inputs have dtype int32.
Check in elementwise backprop kernels that the shapes of X and Y match that of dY (where applicable).
Check that output arrays have the correct shape when they are provided.
Check that lengths are >= 0 and that lengths sum up to the number of instances.
Check that which used in backprop_maxout and backprop_reduce_max are in bounds.

The out option of mish is replaced by inplace, to give mish the same interface as other elementwise kernels. This should not break the API, since _custom_kernels is a private module.","About the use of assert versus an if+raise. I would not use assert if the wrong input can be caused by a user of Thinc. The fact that we're throwing user-facing error messages here implies to me that maybe assert is less applicable? To me, assert statements should only be used on internal code paths, to signal a bug within the library itself, not to check for user input.

I think there are several different cases:

Checking types should probably be assertions, because if the wrong type of array is passed, this is a bug in CupyOps, which should dispatch to a kernel when the type is float32 or to the generic Ops implementation otherwise.
Incorrect shapes should probably be exceptions, unless we start checking shapes in Ops.
Checking lengths/which should be exceptions. I don't think it makes sense to check these in Ops, since CuPy/NumPy implementations do bounds checks and otherwise we would be doing bounds checks twice.
Some kernels have an out type. However, since _custom_kernels is a private module and these out arguments are not used by CupyOps, maybe they should be removed altogether? I found at least one which was not functional, because it had a line best, which = None.",True,{'THUMBS_UP': ['https://github.com/svlandeg']}
explosion/thinc,https://github.com/explosion/thinc,597,2022-02-22T12:27:43Z,2022-03-07T12:06:17Z,2022-03-21T09:12:05Z,MERGED,True,210,81,2,https://github.com/danieldk,Improve bound and type checks in CUDA kernels,19,"['feat / types', 'feat / ops']",https://github.com/explosion/thinc/pull/597,https://github.com/danieldk,3,https://github.com/explosion/thinc/pull/597#issuecomment-1048512865,"This PR combines some improvements that should remove most accidental out of bounds reads/writes:

Check that floating point kernel inputs have dtype float32.
Check that integer kernel inputs have dtype int32.
Check in elementwise backprop kernels that the shapes of X and Y match that of dY (where applicable).
Check that output arrays have the correct shape when they are provided.
Check that lengths are >= 0 and that lengths sum up to the number of instances.
Check that which used in backprop_maxout and backprop_reduce_max are in bounds.

The out option of mish is replaced by inplace, to give mish the same interface as other elementwise kernels. This should not break the API, since _custom_kernels is a private module.","Incorrect shapes, which, lengths are now exceptions. I have also added missing dispatch to some CupyOps methods, for when our CUDA kernels cannot handle a data type.",True,{}
explosion/thinc,https://github.com/explosion/thinc,597,2022-02-22T12:27:43Z,2022-03-07T12:06:17Z,2022-03-21T09:12:05Z,MERGED,True,210,81,2,https://github.com/danieldk,Improve bound and type checks in CUDA kernels,19,"['feat / types', 'feat / ops']",https://github.com/explosion/thinc/pull/597,https://github.com/danieldk,4,https://github.com/explosion/thinc/pull/597#issuecomment-1048705312,"This PR combines some improvements that should remove most accidental out of bounds reads/writes:

Check that floating point kernel inputs have dtype float32.
Check that integer kernel inputs have dtype int32.
Check in elementwise backprop kernels that the shapes of X and Y match that of dY (where applicable).
Check that output arrays have the correct shape when they are provided.
Check that lengths are >= 0 and that lengths sum up to the number of instances.
Check that which used in backprop_maxout and backprop_reduce_max are in bounds.

The out option of mish is replaced by inplace, to give mish the same interface as other elementwise kernels. This should not break the API, since _custom_kernels is a private module.","Putting this in draft, the number of changes is quite large, so I think we should postpone this for after the upcoming release, so that it gets more testing.",True,{'THUMBS_UP': ['https://github.com/svlandeg']}
explosion/thinc,https://github.com/explosion/thinc,597,2022-02-22T12:27:43Z,2022-03-07T12:06:17Z,2022-03-21T09:12:05Z,MERGED,True,210,81,2,https://github.com/danieldk,Improve bound and type checks in CUDA kernels,19,"['feat / types', 'feat / ops']",https://github.com/explosion/thinc/pull/597,https://github.com/danieldk,5,https://github.com/explosion/thinc/pull/597#issuecomment-1057296342,"This PR combines some improvements that should remove most accidental out of bounds reads/writes:

Check that floating point kernel inputs have dtype float32.
Check that integer kernel inputs have dtype int32.
Check in elementwise backprop kernels that the shapes of X and Y match that of dY (where applicable).
Check that output arrays have the correct shape when they are provided.
Check that lengths are >= 0 and that lengths sum up to the number of instances.
Check that which used in backprop_maxout and backprop_reduce_max are in bounds.

The out option of mish is replaced by inplace, to give mish the same interface as other elementwise kernels. This should not break the API, since _custom_kernels is a private module.",I have retarget this to develop and removed the draft status.,True,{}
explosion/thinc,https://github.com/explosion/thinc,598,2022-02-22T13:54:27Z,2022-03-07T18:39:36Z,2022-03-07T18:39:36Z,MERGED,True,34,9,4,https://github.com/adrianeboyd,Set pytorch allocator in PyTorchShim on GPU,6,['feat / shims'],https://github.com/explosion/thinc/pull/598,https://github.com/adrianeboyd,1,https://github.com/explosion/thinc/pull/598,,,True,{}
explosion/thinc,https://github.com/explosion/thinc,598,2022-02-22T13:54:27Z,2022-03-07T18:39:36Z,2022-03-07T18:39:36Z,MERGED,True,34,9,4,https://github.com/adrianeboyd,Set pytorch allocator in PyTorchShim on GPU,6,['feat / shims'],https://github.com/explosion/thinc/pull/598,https://github.com/adrianeboyd,2,https://github.com/explosion/thinc/pull/598#issuecomment-1047820877,,The warning is difficult. Testing is also difficult because you can't really unset it.,True,{}
explosion/thinc,https://github.com/explosion/thinc,598,2022-02-22T13:54:27Z,2022-03-07T18:39:36Z,2022-03-07T18:39:36Z,MERGED,True,34,9,4,https://github.com/adrianeboyd,Set pytorch allocator in PyTorchShim on GPU,6,['feat / shims'],https://github.com/explosion/thinc/pull/598,https://github.com/adrianeboyd,3,https://github.com/explosion/thinc/pull/598#issuecomment-1047873213,,@explosion-bot please test_gpu,True,"{'EYES': ['https://github.com/explosion-bot'], 'ROCKET': ['https://github.com/explosion-bot']}"
explosion/thinc,https://github.com/explosion/thinc,598,2022-02-22T13:54:27Z,2022-03-07T18:39:36Z,2022-03-07T18:39:36Z,MERGED,True,34,9,4,https://github.com/adrianeboyd,Set pytorch allocator in PyTorchShim on GPU,6,['feat / shims'],https://github.com/explosion/thinc/pull/598,https://github.com/explosion-bot,4,https://github.com/explosion/thinc/pull/598#issuecomment-1047873640,," Successfully triggered build on Buildkite

URL: https://buildkite.com/explosion-ai/thinc-gpu-test-suite/builds/27",True,{}
explosion/thinc,https://github.com/explosion/thinc,598,2022-02-22T13:54:27Z,2022-03-07T18:39:36Z,2022-03-07T18:39:36Z,MERGED,True,34,9,4,https://github.com/adrianeboyd,Set pytorch allocator in PyTorchShim on GPU,6,['feat / shims'],https://github.com/explosion/thinc/pull/598,https://github.com/adrianeboyd,5,https://github.com/explosion/thinc/pull/598#issuecomment-1047874839,,"I strongly suspect people will be super-annoyed by this warning, and in most cases if would be better to do it automatically somehow.",True,{}
explosion/thinc,https://github.com/explosion/thinc,598,2022-02-22T13:54:27Z,2022-03-07T18:39:36Z,2022-03-07T18:39:36Z,MERGED,True,34,9,4,https://github.com/adrianeboyd,Set pytorch allocator in PyTorchShim on GPU,6,['feat / shims'],https://github.com/explosion/thinc/pull/598,https://github.com/adrianeboyd,6,https://github.com/explosion/thinc/pull/598#issuecomment-1049705867,,@explosion-bot please test_gpu,True,"{'EYES': ['https://github.com/explosion-bot'], 'ROCKET': ['https://github.com/explosion-bot']}"
explosion/thinc,https://github.com/explosion/thinc,598,2022-02-22T13:54:27Z,2022-03-07T18:39:36Z,2022-03-07T18:39:36Z,MERGED,True,34,9,4,https://github.com/adrianeboyd,Set pytorch allocator in PyTorchShim on GPU,6,['feat / shims'],https://github.com/explosion/thinc/pull/598,https://github.com/explosion-bot,7,https://github.com/explosion/thinc/pull/598#issuecomment-1049706266,," Successfully triggered build on Buildkite

URL: https://buildkite.com/explosion-ai/thinc-gpu-test-suite/builds/28",True,{}
explosion/thinc,https://github.com/explosion/thinc,599,2022-02-24T15:37:36Z,2022-05-09T09:47:28Z,2022-05-09T09:47:28Z,MERGED,True,581,444,38,https://github.com/richardpaulhudson,Add bound types throughout layers and Ops,79,"['enhancement', 'feat / types']",https://github.com/explosion/thinc/pull/599,https://github.com/richardpaulhudson,1,https://github.com/explosion/thinc/pull/599,"Make it possible to declare layer model types e.g. Model[A, K] where the model supports types Model[Union[A, B, C], Union[K, L, M]]. This relies on bound type variables as defined in PEP484.
This should hopefully make Thinc typing more developer-friendly and also increase the effectiveness of the Mypy Thinc plugin checking the validity of model chains, because it allows the specification of the concrete types being used in a given situation.
Note that most of the type the documented type remains the same: covariant types are defined and used in the context of individual modules.


Support Mypy version 0.942 and Pydantic version 1.9.0.


Correct the documented type for some functions where it did not reflect the functionality supported by the code.


Cross-check the documentation with the code with respect to typing.


Despite the size of this PR, it changes virtually no running code: almost all the changes are to type definitions.
Btw I have now noticed #566 and this PR seems to have covered everything that is changed there.","Make it possible to declare layer model types e.g. Model[A, K] where the model supports types Model[Union[A, B, C], Union[K, L, M]]. This relies on bound type variables as defined in PEP484.
This should hopefully make Thinc typing more developer-friendly and also increase the effectiveness of the Mypy Thinc plugin checking the validity of model chains, because it allows the specification of the concrete types being used in a given situation.
Note that most of the type the documented type remains the same: covariant types are defined and used in the context of individual modules.


Support Mypy version 0.942 and Pydantic version 1.9.0.


Correct the documented type for some functions where it did not reflect the functionality supported by the code.


Cross-check the documentation with the code with respect to typing.


Despite the size of this PR, it changes virtually no running code: almost all the changes are to type definitions.
Btw I have now noticed #566 and this PR seems to have covered everything that is changed there.",True,"{'HEART': ['https://github.com/svlandeg', 'https://github.com/tiangolo', 'https://github.com/andychisholm', 'https://github.com/danieldk', 'https://github.com/Talendar'], 'ROCKET': ['https://github.com/svlandeg', 'https://github.com/danieldk', 'https://github.com/Talendar']}"
explosion/thinc,https://github.com/explosion/thinc,599,2022-02-24T15:37:36Z,2022-05-09T09:47:28Z,2022-05-09T09:47:28Z,MERGED,True,581,444,38,https://github.com/richardpaulhudson,Add bound types throughout layers and Ops,79,"['enhancement', 'feat / types']",https://github.com/explosion/thinc/pull/599,https://github.com/svlandeg,2,https://github.com/explosion/thinc/pull/599#issuecomment-1111989060,"Make it possible to declare layer model types e.g. Model[A, K] where the model supports types Model[Union[A, B, C], Union[K, L, M]]. This relies on bound type variables as defined in PEP484.
This should hopefully make Thinc typing more developer-friendly and also increase the effectiveness of the Mypy Thinc plugin checking the validity of model chains, because it allows the specification of the concrete types being used in a given situation.
Note that most of the type the documented type remains the same: covariant types are defined and used in the context of individual modules.


Support Mypy version 0.942 and Pydantic version 1.9.0.


Correct the documented type for some functions where it did not reflect the functionality supported by the code.


Cross-check the documentation with the code with respect to typing.


Despite the size of this PR, it changes virtually no running code: almost all the changes are to type definitions.
Btw I have now noticed #566 and this PR seems to have covered everything that is changed there.","Before we'd merge this in, we'd also need to re-test spacy against the current version in this PR.",True,{}
explosion/thinc,https://github.com/explosion/thinc,600,2022-02-25T16:32:40Z,2022-03-29T15:48:44Z,2022-03-29T15:48:44Z,CLOSED,False,125,4,2,https://github.com/kadarakos,Couple of experimental optimizer features,4,['feat / optimizers'],https://github.com/explosion/thinc/pull/600,https://github.com/kadarakos,1,https://github.com/explosion/thinc/pull/600,"This PR contains a few recent optimizer features and is about exploring whether some of the new improvements could be helpful optimizing problems usually faced in spaCy. I'm taking inspiration from Ranger. My main moving target goal would probably something like replacing averaging with lookahead for convenience, weight-decay AdamW style, use adaptive-clipping maybe weight-norm.
Currently it has a sketch of the following features:

AdaBelief: A slight variation on Adam with the same API.
Lookahead: Rather than normal training and weight averaging after, during training keep a slow exponential moving average of the weights with k lookahead steps.
Gradient Centering: Subtracts the mean from the gradients to center.

So far I only ran a couple of sanity check experiments on MNIST, Fashion-MNIST and Kuzushiji-MNIST using https://github.com/explosion/ml-datasets and running the MNIST example https://github.com/explosion/thinc/blob/master/examples/mnist.py.
The AdaBelief implementation diverges when lr = 0.001. Its kind of fine on lr = 0.0001, but not as good as Adam. I feel like I might abandon AdaBelief and focus on other Adam tweaks like AdamW and
The gradient centering doesn't cause any issues, but I didn't observe benefits over vanilla clipping on these data sets. The lookahead seems somewhat promising in that it did seem to slightly improve over no lookahead with k=3 and alpha=0.8, but didn't compare it yet to averaging.
The good thing about the lookahead is that if it's as good as weight-averaging then serialization is the same as normal unlike with weight-averaging where one needs to store the moving average for in case of resume.","This PR contains a few recent optimizer features and is about exploring whether some of the new improvements could be helpful optimizing problems usually faced in spaCy. I'm taking inspiration from Ranger. My main moving target goal would probably something like replacing averaging with lookahead for convenience, weight-decay AdamW style, use adaptive-clipping maybe weight-norm.
Currently it has a sketch of the following features:

AdaBelief: A slight variation on Adam with the same API.
Lookahead: Rather than normal training and weight averaging after, during training keep a slow exponential moving average of the weights with k lookahead steps.
Gradient Centering: Subtracts the mean from the gradients to center.

So far I only ran a couple of sanity check experiments on MNIST, Fashion-MNIST and Kuzushiji-MNIST using https://github.com/explosion/ml-datasets and running the MNIST example https://github.com/explosion/thinc/blob/master/examples/mnist.py.
The AdaBelief implementation diverges when lr = 0.001. Its kind of fine on lr = 0.0001, but not as good as Adam. I feel like I might abandon AdaBelief and focus on other Adam tweaks like AdamW and
The gradient centering doesn't cause any issues, but I didn't observe benefits over vanilla clipping on these data sets. The lookahead seems somewhat promising in that it did seem to slightly improve over no lookahead with k=3 and alpha=0.8, but didn't compare it yet to averaging.
The good thing about the lookahead is that if it's as good as weight-averaging then serialization is the same as normal unlike with weight-averaging where one needs to store the moving average for in case of resume.",True,"{'HEART': ['https://github.com/bratao'], 'ROCKET': ['https://github.com/svlandeg']}"
explosion/thinc,https://github.com/explosion/thinc,600,2022-02-25T16:32:40Z,2022-03-29T15:48:44Z,2022-03-29T15:48:44Z,CLOSED,False,125,4,2,https://github.com/kadarakos,Couple of experimental optimizer features,4,['feat / optimizers'],https://github.com/explosion/thinc/pull/600,https://github.com/kadarakos,2,https://github.com/explosion/thinc/pull/600#issuecomment-1067039706,"This PR contains a few recent optimizer features and is about exploring whether some of the new improvements could be helpful optimizing problems usually faced in spaCy. I'm taking inspiration from Ranger. My main moving target goal would probably something like replacing averaging with lookahead for convenience, weight-decay AdamW style, use adaptive-clipping maybe weight-norm.
Currently it has a sketch of the following features:

AdaBelief: A slight variation on Adam with the same API.
Lookahead: Rather than normal training and weight averaging after, during training keep a slow exponential moving average of the weights with k lookahead steps.
Gradient Centering: Subtracts the mean from the gradients to center.

So far I only ran a couple of sanity check experiments on MNIST, Fashion-MNIST and Kuzushiji-MNIST using https://github.com/explosion/ml-datasets and running the MNIST example https://github.com/explosion/thinc/blob/master/examples/mnist.py.
The AdaBelief implementation diverges when lr = 0.001. Its kind of fine on lr = 0.0001, but not as good as Adam. I feel like I might abandon AdaBelief and focus on other Adam tweaks like AdamW and
The gradient centering doesn't cause any issues, but I didn't observe benefits over vanilla clipping on these data sets. The lookahead seems somewhat promising in that it did seem to slightly improve over no lookahead with k=3 and alpha=0.8, but didn't compare it yet to averaging.
The good thing about the lookahead is that if it's as good as weight-averaging then serialization is the same as normal unlike with weight-averaging where one needs to store the moving average for in case of resume.",I'm thinking about closing this one for now and come back to it. I kinda concluded that first I should flesh out the SWA and Lookahead properly and have a PR just on that first.,True,{'THUMBS_UP': ['https://github.com/svlandeg']}
explosion/thinc,https://github.com/explosion/thinc,601,2022-03-02T14:01:00Z,2022-03-02T18:29:47Z,2022-03-02T18:29:47Z,MERGED,True,22,1,2,https://github.com/danieldk,Fix Ops.backprop_reduce_max,1,"['bug', 'feat / ops']",https://github.com/explosion/thinc/pull/601,https://github.com/danieldk,1,https://github.com/explosion/thinc/pull/601,"Suppose that we have a sequence of length 2 and dimensionality 3:
1: a b c
2: d e f

The which array specifies for each dimension which sequence contained
the maximum value. For example, if which = [0, 1, 0], the maximum values
were:
a e c

However, the Ops implementation of this function was using which to
select columns. So,
a b a
d e d

would be selected. This change rectifies this, to select per column the
row indicated in which.","Suppose that we have a sequence of length 2 and dimensionality 3:
1: a b c
2: d e f

The which array specifies for each dimension which sequence contained
the maximum value. For example, if which = [0, 1, 0], the maximum values
were:
a e c

However, the Ops implementation of this function was using which to
select columns. So,
a b a
d e d

would be selected. This change rectifies this, to select per column the
row indicated in which.",True,{'THUMBS_UP': ['https://github.com/svlandeg']}
explosion/thinc,https://github.com/explosion/thinc,602,2022-03-02T15:15:15Z,2022-03-02T16:40:06Z,2022-03-02T16:40:09Z,MERGED,True,56261,4127,266,https://github.com/danieldk,Sync develop with master,455,[],https://github.com/explosion/thinc/pull/602,https://github.com/danieldk,1,https://github.com/explosion/thinc/pull/602,,,True,{}
explosion/thinc,https://github.com/explosion/thinc,603,2022-03-07T12:32:17Z,2022-03-16T23:10:54Z,2022-03-16T23:10:54Z,MERGED,True,638,326,5,https://github.com/danieldk,custom_kernels: make all CUDA kernels generic,7,"['enhancement', 'feat / ops']",https://github.com/explosion/thinc/pull/603,https://github.com/danieldk,1,https://github.com/explosion/thinc/pull/603,"This change makes all CUDA kernels generic using C++ templates, so that they can be used for float and double arrays.","This change makes all CUDA kernels generic using C++ templates, so that they can be used for float and double arrays.",True,{'ROCKET': ['https://github.com/svlandeg']}
explosion/thinc,https://github.com/explosion/thinc,603,2022-03-07T12:32:17Z,2022-03-16T23:10:54Z,2022-03-16T23:10:54Z,MERGED,True,638,326,5,https://github.com/danieldk,custom_kernels: make all CUDA kernels generic,7,"['enhancement', 'feat / ops']",https://github.com/explosion/thinc/pull/603,https://github.com/danieldk,2,https://github.com/explosion/thinc/pull/603#issuecomment-1063054926,"This change makes all CUDA kernels generic using C++ templates, so that they can be used for float and double arrays.","Should we also add some tests to test the functionality with double arrays?

I was looking into that. We already test the activation functions for float32 and float64. I think for most of the other functions we also need generic implementations in NumpyOps. I was already looking into that. What do you think, does it make more sense to add that to this PR or wait until I have done generic implementations of NumpyOps as well?",True,{}
explosion/thinc,https://github.com/explosion/thinc,603,2022-03-07T12:32:17Z,2022-03-16T23:10:54Z,2022-03-16T23:10:54Z,MERGED,True,638,326,5,https://github.com/danieldk,custom_kernels: make all CUDA kernels generic,7,"['enhancement', 'feat / ops']",https://github.com/explosion/thinc/pull/603,https://github.com/svlandeg,3,https://github.com/explosion/thinc/pull/603#issuecomment-1063110762,"This change makes all CUDA kernels generic using C++ templates, so that they can be used for float and double arrays.","Intuitively I would say it'd make sense to add the tests here, but I don't feel strongly. As long as it's on the TODO list, that's fine by me :-)


Should we also add some tests to test the functionality with double arrays?

I was looking into that. We already test the activation functions for float32 and float64. I think for most of the other functions we also need generic implementations in NumpyOps. I was already looking into that. What do you think, does it make more sense to add that to this PR or wait until I have done generic implementations of NumpyOps as well?",True,{}
explosion/thinc,https://github.com/explosion/thinc,603,2022-03-07T12:32:17Z,2022-03-16T23:10:54Z,2022-03-16T23:10:54Z,MERGED,True,638,326,5,https://github.com/danieldk,custom_kernels: make all CUDA kernels generic,7,"['enhancement', 'feat / ops']",https://github.com/explosion/thinc/pull/603,https://github.com/danieldk,4,https://github.com/explosion/thinc/pull/603#issuecomment-1064004215,"This change makes all CUDA kernels generic using C++ templates, so that they can be used for float and double arrays.","Intuitively I would say it'd make sense to add the tests here, but I don't feel strongly. As long as it's on the TODO list, that's fine by me :-)

I'll add them and disable them for ops that don't support float64 yet. I'll put this in draft so that it doesn't get merged accidentally.",True,{'THUMBS_UP': ['https://github.com/svlandeg']}
explosion/thinc,https://github.com/explosion/thinc,603,2022-03-07T12:32:17Z,2022-03-16T23:10:54Z,2022-03-16T23:10:54Z,MERGED,True,638,326,5,https://github.com/danieldk,custom_kernels: make all CUDA kernels generic,7,"['enhancement', 'feat / ops']",https://github.com/explosion/thinc/pull/603,https://github.com/danieldk,5,https://github.com/explosion/thinc/pull/603#issuecomment-1065137901,"This change makes all CUDA kernels generic using C++ templates, so that they can be used for float and double arrays.","Intuitively I would say it'd make sense to add the tests here, but I don't feel strongly. As long as it's on the TODO list, that's fine by me :-)

Done!
Added a small function ops_with_dtypes, which we can remove once NumpyOps properly supports float64 arrays as well.",True,{}
explosion/thinc,https://github.com/explosion/thinc,603,2022-03-07T12:32:17Z,2022-03-16T23:10:54Z,2022-03-16T23:10:54Z,MERGED,True,638,326,5,https://github.com/danieldk,custom_kernels: make all CUDA kernels generic,7,"['enhancement', 'feat / ops']",https://github.com/explosion/thinc/pull/603,https://github.com/danieldk,6,https://github.com/explosion/thinc/pull/603#issuecomment-1068907734,"This change makes all CUDA kernels generic using C++ templates, so that they can be used for float and double arrays.",@explosion-bot please test_gpu,True,"{'EYES': ['https://github.com/explosion-bot'], 'ROCKET': ['https://github.com/explosion-bot']}"
explosion/thinc,https://github.com/explosion/thinc,603,2022-03-07T12:32:17Z,2022-03-16T23:10:54Z,2022-03-16T23:10:54Z,MERGED,True,638,326,5,https://github.com/danieldk,custom_kernels: make all CUDA kernels generic,7,"['enhancement', 'feat / ops']",https://github.com/explosion/thinc/pull/603,https://github.com/explosion-bot,7,https://github.com/explosion/thinc/pull/603#issuecomment-1068908252,"This change makes all CUDA kernels generic using C++ templates, so that they can be used for float and double arrays."," Successfully triggered build on Buildkite

URL: https://buildkite.com/explosion-ai/thinc-gpu-test-suite/builds/33",True,{}
explosion/thinc,https://github.com/explosion/thinc,604,2022-03-10T11:24:38Z,2022-03-11T08:55:25Z,2022-03-11T08:55:30Z,MERGED,True,44,36,2,https://github.com/danieldk,Documentation updates,6,['docs'],https://github.com/explosion/thinc/pull/604,https://github.com/danieldk,1,https://github.com/explosion/thinc/pull/604,Documentation updates for changes that I made since the last release.,Documentation updates for changes that I made since the last release.,True,{'THUMBS_UP': ['https://github.com/svlandeg']}
explosion/thinc,https://github.com/explosion/thinc,604,2022-03-10T11:24:38Z,2022-03-11T08:55:25Z,2022-03-11T08:55:30Z,MERGED,True,44,36,2,https://github.com/danieldk,Documentation updates,6,['docs'],https://github.com/explosion/thinc/pull/604,https://github.com/danieldk,2,https://github.com/explosion/thinc/pull/604#issuecomment-1063992398,Documentation updates for changes that I made since the last release.,"If this is the final docs change before the release, we can merge it into the docs branch and let the website build, if we're otherwise good to go for the release :-)

@kadarakos is working on another PR with docs for the new activations.",True,{'THUMBS_UP': ['https://github.com/svlandeg']}
explosion/thinc,https://github.com/explosion/thinc,605,2022-03-10T14:36:32Z,2022-03-11T13:05:03Z,2022-03-11T13:05:03Z,MERGED,True,691,49,3,https://github.com/kadarakos,Docs for new activation functions and some of the missing initializers,39,['docs'],https://github.com/explosion/thinc/pull/605,https://github.com/kadarakos,1,https://github.com/explosion/thinc/pull/605,,,True,{'THUMBS_UP': ['https://github.com/svlandeg']}
explosion/thinc,https://github.com/explosion/thinc,606,2022-03-11T12:12:49Z,2022-03-11T12:55:42Z,2022-03-11T12:55:47Z,MERGED,True,265,96,17,https://github.com/danieldk,Merge master into develop,5,[],https://github.com/explosion/thinc/pull/606,https://github.com/danieldk,1,https://github.com/explosion/thinc/pull/606,,,True,{}
explosion/thinc,https://github.com/explosion/thinc,606,2022-03-11T12:12:49Z,2022-03-11T12:55:42Z,2022-03-11T12:55:47Z,MERGED,True,265,96,17,https://github.com/danieldk,Merge master into develop,5,[],https://github.com/explosion/thinc/pull/606,https://github.com/danieldk,2,https://github.com/explosion/thinc/pull/606#issuecomment-1065058972,,@explosion-bot please test_gpu,True,"{'EYES': ['https://github.com/explosion-bot'], 'ROCKET': ['https://github.com/explosion-bot']}"
explosion/thinc,https://github.com/explosion/thinc,606,2022-03-11T12:12:49Z,2022-03-11T12:55:42Z,2022-03-11T12:55:47Z,MERGED,True,265,96,17,https://github.com/danieldk,Merge master into develop,5,[],https://github.com/explosion/thinc/pull/606,https://github.com/explosion-bot,3,https://github.com/explosion/thinc/pull/606#issuecomment-1065059260,," Successfully triggered build on Buildkite

URL: https://buildkite.com/explosion-ai/thinc-gpu-test-suite/builds/30",True,{}
explosion/thinc,https://github.com/explosion/thinc,607,2022-03-11T15:14:23Z,2022-03-11T16:16:36Z,2022-03-11T16:16:36Z,MERGED,True,47,10,2,https://github.com/danieldk,to_categorical: permit having one class without label smoothing,1,[],https://github.com/explosion/thinc/pull/607,https://github.com/danieldk,1,https://github.com/explosion/thinc/pull/607,Also add tests to verify that the n_classes kwarg is correctly checked.,Also add tests to verify that the n_classes kwarg is correctly checked.,True,{'THUMBS_UP': ['https://github.com/svlandeg']}
explosion/thinc,https://github.com/explosion/thinc,608,2022-03-11T18:39:07Z,2022-03-11T19:24:26Z,2022-03-11T19:24:30Z,MERGED,True,1,1,1,https://github.com/danieldk,Set version to 8.0.14,1,[],https://github.com/explosion/thinc/pull/608,https://github.com/danieldk,1,https://github.com/explosion/thinc/pull/608,,,True,{}
explosion/thinc,https://github.com/explosion/thinc,608,2022-03-11T18:39:07Z,2022-03-11T19:24:26Z,2022-03-11T19:24:30Z,MERGED,True,1,1,1,https://github.com/danieldk,Set version to 8.0.14,1,[],https://github.com/explosion/thinc/pull/608,https://github.com/danieldk,2,https://github.com/explosion/thinc/pull/608#issuecomment-1065382704,,@explosion-bot please test_gpu,True,"{'EYES': ['https://github.com/explosion-bot'], 'ROCKET': ['https://github.com/explosion-bot']}"
explosion/thinc,https://github.com/explosion/thinc,608,2022-03-11T18:39:07Z,2022-03-11T19:24:26Z,2022-03-11T19:24:30Z,MERGED,True,1,1,1,https://github.com/danieldk,Set version to 8.0.14,1,[],https://github.com/explosion/thinc/pull/608,https://github.com/explosion-bot,3,https://github.com/explosion/thinc/pull/608#issuecomment-1065383289,," Successfully triggered build on Buildkite

URL: https://buildkite.com/explosion-ai/thinc-gpu-test-suite/builds/31",True,{}
explosion/thinc,https://github.com/explosion/thinc,609,2022-03-11T19:32:39Z,2022-03-14T07:53:50Z,2022-03-14T08:25:36Z,MERGED,True,3901,1400,71,https://github.com/danieldk,Update thinc.ai to Thinc 8.0.14,55,[],https://github.com/explosion/thinc/pull/609,https://github.com/danieldk,1,https://github.com/explosion/thinc/pull/609,,,True,{}
explosion/thinc,https://github.com/explosion/thinc,610,2022-03-14T11:26:06Z,2022-03-15T11:20:36Z,2022-03-15T11:49:30Z,MERGED,True,65,13,7,https://github.com/danieldk,Torch backwards compatibility fixes,8,['interop / pytorch'],https://github.com/explosion/thinc/pull/610,https://github.com/danieldk,1,https://github.com/explosion/thinc/pull/610,"Fix compatibility with Torch without torch.cuda.amp.common
Disable PyTorch-based activation tests pre-PyTorch 1.9.0","Fix compatibility with Torch without torch.cuda.amp.common
Disable PyTorch-based activation tests pre-PyTorch 1.9.0",True,{'THUMBS_UP': ['https://github.com/svlandeg']}
explosion/thinc,https://github.com/explosion/thinc,610,2022-03-14T11:26:06Z,2022-03-15T11:20:36Z,2022-03-15T11:49:30Z,MERGED,True,65,13,7,https://github.com/danieldk,Torch backwards compatibility fixes,8,['interop / pytorch'],https://github.com/explosion/thinc/pull/610,https://github.com/danieldk,2,https://github.com/explosion/thinc/pull/610#issuecomment-1066848811,"Fix compatibility with Torch without torch.cuda.amp.common
Disable PyTorch-based activation tests pre-PyTorch 1.9.0",@explosion-bot please test_gpu,True,"{'EYES': ['https://github.com/explosion-bot'], 'ROCKET': ['https://github.com/explosion-bot']}"
explosion/thinc,https://github.com/explosion/thinc,610,2022-03-14T11:26:06Z,2022-03-15T11:20:36Z,2022-03-15T11:49:30Z,MERGED,True,65,13,7,https://github.com/danieldk,Torch backwards compatibility fixes,8,['interop / pytorch'],https://github.com/explosion/thinc/pull/610,https://github.com/explosion-bot,3,https://github.com/explosion/thinc/pull/610#issuecomment-1066849453,"Fix compatibility with Torch without torch.cuda.amp.common
Disable PyTorch-based activation tests pre-PyTorch 1.9.0"," Successfully triggered build on Buildkite

URL: https://buildkite.com/explosion-ai/thinc-gpu-test-suite/builds/32",True,{}
explosion/thinc,https://github.com/explosion/thinc,611,2022-03-14T12:49:23Z,2022-05-18T07:30:51Z,2022-05-18T07:30:51Z,CLOSED,False,8,1,1,https://github.com/danieldk,CI: test lower-bound PyTorch version with Python 3.6,1,['tests'],https://github.com/explosion/thinc/pull/611,https://github.com/danieldk,1,https://github.com/explosion/thinc/pull/611,"Draft: fails without #610, plus I am not sure if the syntax is valid yet :).","Draft: fails without #610, plus I am not sure if the syntax is valid yet :).",True,{}
explosion/thinc,https://github.com/explosion/thinc,611,2022-03-14T12:49:23Z,2022-05-18T07:30:51Z,2022-05-18T07:30:51Z,CLOSED,False,8,1,1,https://github.com/danieldk,CI: test lower-bound PyTorch version with Python 3.6,1,['tests'],https://github.com/explosion/thinc/pull/611,https://github.com/adrianeboyd,2,https://github.com/explosion/thinc/pull/611#issuecomment-1067648053,"Draft: fails without #610, plus I am not sure if the syntax is valid yet :).","Let's do the hackier version of this for now, like this:
https://github.com/explosion/spacy-transformers/blob/bebb33cc6929a4b6b37c2488b16b82bf9cc4b082/azure-pipelines.yml#L60-L63",True,{}
explosion/thinc,https://github.com/explosion/thinc,611,2022-03-14T12:49:23Z,2022-05-18T07:30:51Z,2022-05-18T07:30:51Z,CLOSED,False,8,1,1,https://github.com/danieldk,CI: test lower-bound PyTorch version with Python 3.6,1,['tests'],https://github.com/explosion/thinc/pull/611,https://github.com/adrianeboyd,3,https://github.com/explosion/thinc/pull/611#issuecomment-1067722500,"Draft: fails without #610, plus I am not sure if the syntax is valid yet :).","This is passing without torch installed, the substitution isn't working as intended.",True,{}
explosion/thinc,https://github.com/explosion/thinc,611,2022-03-14T12:49:23Z,2022-05-18T07:30:51Z,2022-05-18T07:30:51Z,CLOSED,False,8,1,1,https://github.com/danieldk,CI: test lower-bound PyTorch version with Python 3.6,1,['tests'],https://github.com/explosion/thinc/pull/611,https://github.com/danieldk,4,https://github.com/explosion/thinc/pull/611#issuecomment-1129669874,"Draft: fails without #610, plus I am not sure if the syntax is valid yet :).","master has changed significantly since this PR was opened, so probably best to merge in the current branch & double check merge conflicts 

Since this is a tiny PR, I'll close it and open a new one.",True,{}
explosion/thinc,https://github.com/explosion/thinc,612,2022-03-14T13:09:38Z,2022-03-14T14:04:20Z,2022-03-14T14:04:24Z,MERGED,True,1,1,1,https://github.com/danieldk,"Limit pytest >=5.2.0,<7.1.0",1,['tests'],https://github.com/explosion/thinc/pull/612,https://github.com/danieldk,1,https://github.com/explosion/thinc/pull/612,CI fails with pytest 7.1.0.,CI fails with pytest 7.1.0.,True,{'THUMBS_UP': ['https://github.com/svlandeg']}
explosion/thinc,https://github.com/explosion/thinc,613,2022-03-14T15:13:48Z,2022-03-15T07:15:10Z,2022-03-15T07:15:10Z,MERGED,True,2,17,1,https://github.com/adrianeboyd,Reduce CI instances to one OS per pre-3.10 python version,1,[],https://github.com/explosion/thinc/pull/613,https://github.com/adrianeboyd,1,https://github.com/explosion/thinc/pull/613,,,True,"{'THUMBS_UP': ['https://github.com/danieldk', 'https://github.com/svlandeg']}"
explosion/thinc,https://github.com/explosion/thinc,614,2022-03-14T15:24:39Z,,2022-05-09T12:41:13Z,OPEN,False,93,14,4,https://github.com/kadarakos,class-weights for cross-entropy,18,"['enhancement', 'feat / loss']",https://github.com/explosion/thinc/pull/614,https://github.com/kadarakos,1,https://github.com/explosion/thinc/pull/614,It adds class-weights to the cross-entropy loss to potentially help with class imbalance.,It adds class-weights to the cross-entropy loss to potentially help with class imbalance.,True,"{'HOORAY': ['https://github.com/danieldk', 'https://github.com/svlandeg']}"
explosion/thinc,https://github.com/explosion/thinc,616,2022-03-15T11:38:23Z,2022-03-15T12:09:17Z,2022-03-15T12:18:00Z,MERGED,True,1,1,1,https://github.com/danieldk,Set version to 8.0.15,1,[],https://github.com/explosion/thinc/pull/616,https://github.com/danieldk,1,https://github.com/explosion/thinc/pull/616,,,True,{}
explosion/thinc,https://github.com/explosion/thinc,617,2022-03-16T08:16:49Z,2022-03-16T08:47:05Z,2022-03-16T08:47:08Z,MERGED,True,930,105,17,https://github.com/danieldk,Merge master into develop,9,[],https://github.com/explosion/thinc/pull/617,https://github.com/danieldk,1,https://github.com/explosion/thinc/pull/617,,,True,{'THUMBS_UP': ['https://github.com/svlandeg']}
explosion/thinc,https://github.com/explosion/thinc,618,2022-03-17T10:57:13Z,2022-03-29T15:26:43Z,2022-03-29T15:26:43Z,MERGED,True,246,47,4,https://github.com/danieldk,NumpyOps: add bound checks,5,"['enhancement', 'feat / ops']",https://github.com/explosion/thinc/pull/618,https://github.com/danieldk,1,https://github.com/explosion/thinc/pull/618,"This PR adds various bounds checks to NumpyOps to prevent out-of-bound reads and writes:

Check that which indices are in bounds for backprop_{maxout, reduce_max}.
Check that dY and X shapes match in backprop for elementwise functions.
Hook up ReLU to the PyTorch comparison tests.
Check that lengths are valid in backprop_reduce_{mean, max, sum}.","This PR adds various bounds checks to NumpyOps to prevent out-of-bound reads and writes:

Check that which indices are in bounds for backprop_{maxout, reduce_max}.
Check that dY and X shapes match in backprop for elementwise functions.
Hook up ReLU to the PyTorch comparison tests.
Check that lengths are valid in backprop_reduce_{mean, max, sum}.",True,"{'THUMBS_UP': ['https://github.com/polm', 'https://github.com/svlandeg']}"
explosion/thinc,https://github.com/explosion/thinc,618,2022-03-17T10:57:13Z,2022-03-29T15:26:43Z,2022-03-29T15:26:43Z,MERGED,True,246,47,4,https://github.com/danieldk,NumpyOps: add bound checks,5,"['enhancement', 'feat / ops']",https://github.com/explosion/thinc/pull/618,https://github.com/danieldk,2,https://github.com/explosion/thinc/pull/618#issuecomment-1072179654,"This PR adds various bounds checks to NumpyOps to prevent out-of-bound reads and writes:

Check that which indices are in bounds for backprop_{maxout, reduce_max}.
Check that dY and X shapes match in backprop for elementwise functions.
Hook up ReLU to the PyTorch comparison tests.
Check that lengths are valid in backprop_reduce_{mean, max, sum}.",@explosion-bot please test_gpu,True,"{'EYES': ['https://github.com/explosion-bot'], 'ROCKET': ['https://github.com/explosion-bot']}"
explosion/thinc,https://github.com/explosion/thinc,618,2022-03-17T10:57:13Z,2022-03-29T15:26:43Z,2022-03-29T15:26:43Z,MERGED,True,246,47,4,https://github.com/danieldk,NumpyOps: add bound checks,5,"['enhancement', 'feat / ops']",https://github.com/explosion/thinc/pull/618,https://github.com/explosion-bot,3,https://github.com/explosion/thinc/pull/618#issuecomment-1072180557,"This PR adds various bounds checks to NumpyOps to prevent out-of-bound reads and writes:

Check that which indices are in bounds for backprop_{maxout, reduce_max}.
Check that dY and X shapes match in backprop for elementwise functions.
Hook up ReLU to the PyTorch comparison tests.
Check that lengths are valid in backprop_reduce_{mean, max, sum}."," Successfully triggered build on Buildkite

URL: https://buildkite.com/explosion-ai/thinc-gpu-test-suite/builds/34",True,{}
explosion/thinc,https://github.com/explosion/thinc,618,2022-03-17T10:57:13Z,2022-03-29T15:26:43Z,2022-03-29T15:26:43Z,MERGED,True,246,47,4,https://github.com/danieldk,NumpyOps: add bound checks,5,"['enhancement', 'feat / ops']",https://github.com/explosion/thinc/pull/618,https://github.com/honnibal,4,https://github.com/explosion/thinc/pull/618#issuecomment-1081839373,"This PR adds various bounds checks to NumpyOps to prevent out-of-bound reads and writes:

Check that which indices are in bounds for backprop_{maxout, reduce_max}.
Check that dY and X shapes match in backprop for elementwise functions.
Hook up ReLU to the PyTorch comparison tests.
Check that lengths are valid in backprop_reduce_{mean, max, sum}.","Nice! Love the advanced syntax for the nogil except. That's either relatively new, or I just never knew about it  . Either way, neat.",True,{}
explosion/thinc,https://github.com/explosion/thinc,619,2022-03-18T12:44:52Z,2022-03-18T13:25:24Z,2022-03-21T09:09:51Z,MERGED,True,1,1,1,https://github.com/adrianeboyd,"Update pytest to forbid ==7.1.0, allow >=7.1.1",1,['tests'],https://github.com/explosion/thinc/pull/619,https://github.com/adrianeboyd,1,https://github.com/explosion/thinc/pull/619,,,True,{}
explosion/thinc,https://github.com/explosion/thinc,620,2022-03-19T12:28:17Z,2022-03-21T08:19:27Z,2022-03-21T08:19:33Z,MERGED,True,1,1,1,https://github.com/notplus,Update README.md,1,['docs'],https://github.com/explosion/thinc/pull/620,https://github.com/notplus,1,https://github.com/explosion/thinc/pull/620,,,True,{'THUMBS_UP': ['https://github.com/svlandeg']}
explosion/thinc,https://github.com/explosion/thinc,621,2022-03-21T09:11:28Z,2022-03-31T08:02:06Z,2022-03-31T08:02:06Z,MERGED,True,51,41,2,https://github.com/danieldk,_custom_kernels: make all arguments kwarg-only,3,['feat / ops'],https://github.com/explosion/thinc/pull/621,https://github.com/danieldk,1,https://github.com/explosion/thinc/pull/621,"Some kernels take multiple arguments, which are all arrays with the same shape. It's easy to accidentally mix up the order and introduce bugs in that way. This change makes all arguments kwarg-only, so that we always specify what we intend to pass at the call site.
Since _custom_kernels is an internal module, this does not result in any public API changes.","Some kernels take multiple arguments, which are all arrays with the same shape. It's easy to accidentally mix up the order and introduce bugs in that way. This change makes all arguments kwarg-only, so that we always specify what we intend to pass at the call site.
Since _custom_kernels is an internal module, this does not result in any public API changes.",True,{}
explosion/thinc,https://github.com/explosion/thinc,621,2022-03-21T09:11:28Z,2022-03-31T08:02:06Z,2022-03-31T08:02:06Z,MERGED,True,51,41,2,https://github.com/danieldk,_custom_kernels: make all arguments kwarg-only,3,['feat / ops'],https://github.com/explosion/thinc/pull/621,https://github.com/danieldk,2,https://github.com/explosion/thinc/pull/621#issuecomment-1073662971,"Some kernels take multiple arguments, which are all arrays with the same shape. It's easy to accidentally mix up the order and introduce bugs in that way. This change makes all arguments kwarg-only, so that we always specify what we intend to pass at the call site.
Since _custom_kernels is an internal module, this does not result in any public API changes.","I didn't specify kwargs for super() calls. Since they are currently not tested, we can't be sure that nothing breaks :(.",True,{}
explosion/thinc,https://github.com/explosion/thinc,623,2022-03-23T15:34:11Z,2022-03-24T08:54:06Z,2022-03-24T08:54:12Z,CLOSED,False,11,14,2,https://github.com/danieldk,GradScaler: Do not enable when training on CPU,1,[],https://github.com/explosion/thinc/pull/623,https://github.com/danieldk,1,https://github.com/explosion/thinc/pull/623,"Disable gradient scaling and emit a warning when but CPU training is
performed.
Disable gradient scaling and emit a warning when a PyTorch version
without gradient scaling is used (rather than raising an exception).

See explosion/spaCy#10527
Note: putting this in draft. I'd like to add a similar check and warning to the shim.","Disable gradient scaling and emit a warning when but CPU training is
performed.
Disable gradient scaling and emit a warning when a PyTorch version
without gradient scaling is used (rather than raising an exception).

See explosion/spaCy#10527
Note: putting this in draft. I'd like to add a similar check and warning to the shim.",True,{}
explosion/thinc,https://github.com/explosion/thinc,623,2022-03-23T15:34:11Z,2022-03-24T08:54:06Z,2022-03-24T08:54:12Z,CLOSED,False,11,14,2,https://github.com/danieldk,GradScaler: Do not enable when training on CPU,1,[],https://github.com/explosion/thinc/pull/623,https://github.com/danieldk,2,https://github.com/explosion/thinc/pull/623#issuecomment-1077384772,"Disable gradient scaling and emit a warning when but CPU training is
performed.
Disable gradient scaling and emit a warning when a PyTorch version
without gradient scaling is used (rather than raising an exception).

See explosion/spaCy#10527
Note: putting this in draft. I'd like to add a similar check and warning to the shim.","This turns out to be quite tricky, since Thinc may not be switched to GPU yet when PyTorchGradScaler is constructed. So, checking whether we are using a GPU in the constructor may disable gradient scaling unintentionally.
I will close this PR in favor of #624, which replaces the assertion by an exception and describes how this error can be avoided.",True,{'THUMBS_UP': ['https://github.com/svlandeg']}
explosion/thinc,https://github.com/explosion/thinc,624,2022-03-24T08:52:12Z,2022-03-29T15:09:17Z,2022-03-29T15:22:37Z,MERGED,True,29,27,4,https://github.com/danieldk,Mixed-precision training: improve checks,3,"['feat / shims', 'interop / pytorch']",https://github.com/explosion/thinc/pull/624,https://github.com/danieldk,1,https://github.com/explosion/thinc/pull/624,"Before this change, using gradient scaling on a non-CUDA tensor would trigger an assertion. However, it is possible to trigger this error outside Thinc by enabling mixed-precision training on a CPU. So this should be a proper exception rather than an AssertionError. Besides raising a ValueError, the error message is also extended to describe how the error can be avoided.


We checked that gradient scaling is supported when construction a PyTorchGradScaler. However, this gives issues when someone uses a model that was trained with gradient scaling. In such cases, it's safe to contruct a grad scaler, since it is not used. This change moves the check to the actual scaling.


Also remove the check that verifies that mixed-precision scaling is available (when enabled) from the constructor of PyTorchShim. PyTorch will at most give a warning when trying to autocast when there is no support.","Before this change, using gradient scaling on a non-CUDA tensor would trigger an assertion. However, it is possible to trigger this error outside Thinc by enabling mixed-precision training on a CPU. So this should be a proper exception rather than an AssertionError. Besides raising a ValueError, the error message is also extended to describe how the error can be avoided.


We checked that gradient scaling is supported when construction a PyTorchGradScaler. However, this gives issues when someone uses a model that was trained with gradient scaling. In such cases, it's safe to contruct a grad scaler, since it is not used. This change moves the check to the actual scaling.


Also remove the check that verifies that mixed-precision scaling is available (when enabled) from the constructor of PyTorchShim. PyTorch will at most give a warning when trying to autocast when there is no support.",True,{'THUMBS_UP': ['https://github.com/svlandeg']}
explosion/thinc,https://github.com/explosion/thinc,624,2022-03-24T08:52:12Z,2022-03-29T15:09:17Z,2022-03-29T15:22:37Z,MERGED,True,29,27,4,https://github.com/danieldk,Mixed-precision training: improve checks,3,"['feat / shims', 'interop / pytorch']",https://github.com/explosion/thinc/pull/624,https://github.com/danieldk,2,https://github.com/explosion/thinc/pull/624#issuecomment-1077632244,"Before this change, using gradient scaling on a non-CUDA tensor would trigger an assertion. However, it is possible to trigger this error outside Thinc by enabling mixed-precision training on a CPU. So this should be a proper exception rather than an AssertionError. Besides raising a ValueError, the error message is also extended to describe how the error can be avoided.


We checked that gradient scaling is supported when construction a PyTorchGradScaler. However, this gives issues when someone uses a model that was trained with gradient scaling. In such cases, it's safe to contruct a grad scaler, since it is not used. This change moves the check to the actual scaling.


Also remove the check that verifies that mixed-precision scaling is available (when enabled) from the constructor of PyTorchShim. PyTorch will at most give a warning when trying to autocast when there is no support.","Tested PyTorch versions:
CPU: 1.6.0 1.7.0 1.8.0 1.8.1 1.9.0 1.10.0 1.11.0
CUDA: 1.7.0+cu110 1.8.0+cu111 1.8.1+cu111 1.9.0+cu111 1.10.0+cu113 1.11.0+cu113
(All also with python -c ""import spacy; spacy.load(\""en_udv25_englishewt_trf\"")"" to verify that we can load a mixed-precision model on the GPU.)",True,{'ROCKET': ['https://github.com/svlandeg']}
explosion/thinc,https://github.com/explosion/thinc,624,2022-03-24T08:52:12Z,2022-03-29T15:09:17Z,2022-03-29T15:22:37Z,MERGED,True,29,27,4,https://github.com/danieldk,Mixed-precision training: improve checks,3,"['feat / shims', 'interop / pytorch']",https://github.com/explosion/thinc/pull/624,https://github.com/danieldk,3,https://github.com/explosion/thinc/pull/624#issuecomment-1077645329,"Before this change, using gradient scaling on a non-CUDA tensor would trigger an assertion. However, it is possible to trigger this error outside Thinc by enabling mixed-precision training on a CPU. So this should be a proper exception rather than an AssertionError. Besides raising a ValueError, the error message is also extended to describe how the error can be avoided.


We checked that gradient scaling is supported when construction a PyTorchGradScaler. However, this gives issues when someone uses a model that was trained with gradient scaling. In such cases, it's safe to contruct a grad scaler, since it is not used. This change moves the check to the actual scaling.


Also remove the check that verifies that mixed-precision scaling is available (when enabled) from the constructor of PyTorchShim. PyTorch will at most give a warning when trying to autocast when there is no support.",Related issues: explosion/spaCy#10527 and explosion/spaCy#10543,True,{}
explosion/thinc,https://github.com/explosion/thinc,625,2022-03-29T12:16:43Z,2022-03-31T14:39:31Z,2022-03-31T14:39:31Z,MERGED,True,13,13,1,https://github.com/kadarakos,Lstm benchmark example fix,6,"['bug', 'examples']",https://github.com/explosion/thinc/pull/625,https://github.com/kadarakos,1,https://github.com/explosion/thinc/pull/625,The example didn't work when I tried running it and this PR fixes it.,The example didn't work when I tried running it and this PR fixes it.,True,"{'THUMBS_UP': ['https://github.com/svlandeg'], 'ROCKET': ['https://github.com/svlandeg']}"
explosion/thinc,https://github.com/explosion/thinc,626,2022-03-29T16:04:31Z,2022-03-29T18:48:35Z,2022-03-29T18:57:46Z,MERGED,True,1,1,1,https://github.com/svlandeg,test end-of-life Python 3.6 on windows-2019 image,1,['tests'],https://github.com/explosion/thinc/pull/626,https://github.com/svlandeg,1,https://github.com/explosion/thinc/pull/626,"windows-latest recently switched to windows-2022, which doesn't support (end-of-life) Python 3.6 anymore. It doesn't hurt to keep testing this Python version on the CI when we still can, though, so editing this Azure step to use windows-2019 explicitely.","windows-latest recently switched to windows-2022, which doesn't support (end-of-life) Python 3.6 anymore. It doesn't hurt to keep testing this Python version on the CI when we still can, though, so editing this Azure step to use windows-2019 explicitely.",True,{}
explosion/thinc,https://github.com/explosion/thinc,627,2022-03-30T09:52:09Z,2022-04-07T14:41:20Z,2022-04-07T14:41:20Z,MERGED,True,651,430,6,https://github.com/danieldk,Make NumpyOps CPU kernels generic,7,"['enhancement', 'feat / ops']",https://github.com/explosion/thinc/pull/627,https://github.com/danieldk,1,https://github.com/explosion/thinc/pull/627,"This PR makes most CPU kernels generic, so that they can take both float32 and float64 arrays (and hopefully in the future float16). I experimented with kernels in Cython + fused types and kernels as C++ with templates, I found the C++ template route more promising:

More compact/ergonomic implementations with fewer compile-time conditionals.
Opens up the possibility to easily use SIMD intrinsics in the future.

To allow genericity in the NumpyOps method arguments, we use:

Fused types when we require a specific dimensionality;
np.ndarray otherwise.

Some of the kernels are not (yet) made generic:

cpu_scatter_add: needs tests to verify that the op still works
correctly.
cpu_position_encode: the position_encode op doesn't take float
array(s).
lstm kernels: I need to look more deeply into them.","This PR makes most CPU kernels generic, so that they can take both float32 and float64 arrays (and hopefully in the future float16). I experimented with kernels in Cython + fused types and kernels as C++ with templates, I found the C++ template route more promising:

More compact/ergonomic implementations with fewer compile-time conditionals.
Opens up the possibility to easily use SIMD intrinsics in the future.

To allow genericity in the NumpyOps method arguments, we use:

Fused types when we require a specific dimensionality;
np.ndarray otherwise.

Some of the kernels are not (yet) made generic:

cpu_scatter_add: needs tests to verify that the op still works
correctly.
cpu_position_encode: the position_encode op doesn't take float
array(s).
lstm kernels: I need to look more deeply into them.",True,{'THUMBS_UP': ['https://github.com/svlandeg']}
explosion/thinc,https://github.com/explosion/thinc,627,2022-03-30T09:52:09Z,2022-04-07T14:41:20Z,2022-04-07T14:41:20Z,MERGED,True,651,430,6,https://github.com/danieldk,Make NumpyOps CPU kernels generic,7,"['enhancement', 'feat / ops']",https://github.com/explosion/thinc/pull/627,https://github.com/svlandeg,2,https://github.com/explosion/thinc/pull/627#issuecomment-1090326073,"This PR makes most CPU kernels generic, so that they can take both float32 and float64 arrays (and hopefully in the future float16). I experimented with kernels in Cython + fused types and kernels as C++ with templates, I found the C++ template route more promising:

More compact/ergonomic implementations with fewer compile-time conditionals.
Opens up the possibility to easily use SIMD intrinsics in the future.

To allow genericity in the NumpyOps method arguments, we use:

Fused types when we require a specific dimensionality;
np.ndarray otherwise.

Some of the kernels are not (yet) made generic:

cpu_scatter_add: needs tests to verify that the op still works
correctly.
cpu_position_encode: the position_encode op doesn't take float
array(s).
lstm kernels: I need to look more deeply into them.",Ugh - do we have a flaky test?,True,{}
explosion/thinc,https://github.com/explosion/thinc,627,2022-03-30T09:52:09Z,2022-04-07T14:41:20Z,2022-04-07T14:41:20Z,MERGED,True,651,430,6,https://github.com/danieldk,Make NumpyOps CPU kernels generic,7,"['enhancement', 'feat / ops']",https://github.com/explosion/thinc/pull/627,https://github.com/danieldk,3,https://github.com/explosion/thinc/pull/627#issuecomment-1090343795,"This PR makes most CPU kernels generic, so that they can take both float32 and float64 arrays (and hopefully in the future float16). I experimented with kernels in Cython + fused types and kernels as C++ with templates, I found the C++ template route more promising:

More compact/ergonomic implementations with fewer compile-time conditionals.
Opens up the possibility to easily use SIMD intrinsics in the future.

To allow genericity in the NumpyOps method arguments, we use:

Fused types when we require a specific dimensionality;
np.ndarray otherwise.

Some of the kernels are not (yet) made generic:

cpu_scatter_add: needs tests to verify that the op still works
correctly.
cpu_position_encode: the position_encode op doesn't take float
array(s).
lstm kernels: I need to look more deeply into them.","Ugh - do we have a flaky test?

Oh no, again the dreadful Unreliable test timings! On an initial run, this test took XYZms, which exceeded the deadline of 200.00ms.",True,{}
explosion/thinc,https://github.com/explosion/thinc,627,2022-03-30T09:52:09Z,2022-04-07T14:41:20Z,2022-04-07T14:41:20Z,MERGED,True,651,430,6,https://github.com/danieldk,Make NumpyOps CPU kernels generic,7,"['enhancement', 'feat / ops']",https://github.com/explosion/thinc/pull/627,https://github.com/svlandeg,4,https://github.com/explosion/thinc/pull/627#issuecomment-1090346357,"This PR makes most CPU kernels generic, so that they can take both float32 and float64 arrays (and hopefully in the future float16). I experimented with kernels in Cython + fused types and kernels as C++ with templates, I found the C++ template route more promising:

More compact/ergonomic implementations with fewer compile-time conditionals.
Opens up the possibility to easily use SIMD intrinsics in the future.

To allow genericity in the NumpyOps method arguments, we use:

Fused types when we require a specific dimensionality;
np.ndarray otherwise.

Some of the kernels are not (yet) made generic:

cpu_scatter_add: needs tests to verify that the op still works
correctly.
cpu_position_encode: the position_encode op doesn't take float
array(s).
lstm kernels: I need to look more deeply into them.",Rerunning it ,True,{}
explosion/thinc,https://github.com/explosion/thinc,628,2022-03-31T10:46:08Z,2022-03-31T11:18:01Z,2022-03-31T11:18:08Z,MERGED,True,42,41,8,https://github.com/danieldk,Merge master into develop,14,[],https://github.com/explosion/thinc/pull/628,https://github.com/danieldk,1,https://github.com/explosion/thinc/pull/628,,,True,{}
explosion/thinc,https://github.com/explosion/thinc,628,2022-03-31T10:46:08Z,2022-03-31T11:18:01Z,2022-03-31T11:18:08Z,MERGED,True,42,41,8,https://github.com/danieldk,Merge master into develop,14,[],https://github.com/explosion/thinc/pull/628,https://github.com/danieldk,2,https://github.com/explosion/thinc/pull/628#issuecomment-1084423654,,@explosion-bot please test_gpu,True,"{'EYES': ['https://github.com/explosion-bot'], 'ROCKET': ['https://github.com/explosion-bot']}"
explosion/thinc,https://github.com/explosion/thinc,628,2022-03-31T10:46:08Z,2022-03-31T11:18:01Z,2022-03-31T11:18:08Z,MERGED,True,42,41,8,https://github.com/danieldk,Merge master into develop,14,[],https://github.com/explosion/thinc/pull/628,https://github.com/explosion-bot,3,https://github.com/explosion/thinc/pull/628#issuecomment-1084424103,," Successfully triggered build on Buildkite

URL: https://buildkite.com/explosion-ai/thinc-gpu-test-suite/builds/35",True,{}
explosion/thinc,https://github.com/explosion/thinc,630,2022-04-01T15:45:08Z,,2022-05-09T12:40:48Z,OPEN,False,197,6,2,https://github.com/kadarakos,SWA and Lookahead optimzer,6,"['enhancement', 'feat / optimizers']",https://github.com/explosion/thinc/pull/630,https://github.com/kadarakos,1,https://github.com/explosion/thinc/pull/630,"Optimizer wrappers
Currently the Optimizer is quite large class. It implements a version of weight averaging that starting from the first step it keeps an exponential moving average of the weights. This PR suggests to move this functionality out of the Optimizer into the SWA class.
The SWA is the Stochastic Weight Averaging as implemented in https://arxiv.org/abs/1803.05407. It has attributes:

start_step: the training-step where to start recording the moving averages from.
freq: the number of steps between updating the moving average.
lr: the new learning-rate for the SWA steps.

The PR contains also a Lookahead optimizer wrapper that implements a similar algorithm to SWA. It's from https://arxiv.org/abs/1907.08610. It keeps track of ""slow"" weights and ""fast"" weights. Every freq train-steps it updates the ""slow"" weights with an exponential moving average of the ""slow"" weights and replaces the ""fast"" with the ""slow"". Essentially it let's the optimizer run the optimization forward k iterates, but then it pulls it back with a factor pullback.
The example optimizer_wrappers.py shows how to run the Lookahead with Adam from a fixed number of epochs and then to swap it to SWA. This uses the SWA.start_swa() functionality that let's the caller immediately run SWA without setting start_step.","Optimizer wrappers
Currently the Optimizer is quite large class. It implements a version of weight averaging that starting from the first step it keeps an exponential moving average of the weights. This PR suggests to move this functionality out of the Optimizer into the SWA class.
The SWA is the Stochastic Weight Averaging as implemented in https://arxiv.org/abs/1803.05407. It has attributes:

start_step: the training-step where to start recording the moving averages from.
freq: the number of steps between updating the moving average.
lr: the new learning-rate for the SWA steps.

The PR contains also a Lookahead optimizer wrapper that implements a similar algorithm to SWA. It's from https://arxiv.org/abs/1907.08610. It keeps track of ""slow"" weights and ""fast"" weights. Every freq train-steps it updates the ""slow"" weights with an exponential moving average of the ""slow"" weights and replaces the ""fast"" with the ""slow"". Essentially it let's the optimizer run the optimization forward k iterates, but then it pulls it back with a factor pullback.
The example optimizer_wrappers.py shows how to run the Lookahead with Adam from a fixed number of epochs and then to swap it to SWA. This uses the SWA.start_swa() functionality that let's the caller immediately run SWA without setting start_step.",True,{'ROCKET': ['https://github.com/svlandeg']}
explosion/thinc,https://github.com/explosion/thinc,630,2022-04-01T15:45:08Z,,2022-05-09T12:40:48Z,OPEN,False,197,6,2,https://github.com/kadarakos,SWA and Lookahead optimzer,6,"['enhancement', 'feat / optimizers']",https://github.com/explosion/thinc/pull/630,https://github.com/danieldk,2,https://github.com/explosion/thinc/pull/630#issuecomment-1091129128,"Optimizer wrappers
Currently the Optimizer is quite large class. It implements a version of weight averaging that starting from the first step it keeps an exponential moving average of the weights. This PR suggests to move this functionality out of the Optimizer into the SWA class.
The SWA is the Stochastic Weight Averaging as implemented in https://arxiv.org/abs/1803.05407. It has attributes:

start_step: the training-step where to start recording the moving averages from.
freq: the number of steps between updating the moving average.
lr: the new learning-rate for the SWA steps.

The PR contains also a Lookahead optimizer wrapper that implements a similar algorithm to SWA. It's from https://arxiv.org/abs/1907.08610. It keeps track of ""slow"" weights and ""fast"" weights. Every freq train-steps it updates the ""slow"" weights with an exponential moving average of the ""slow"" weights and replaces the ""fast"" with the ""slow"". Essentially it let's the optimizer run the optimization forward k iterates, but then it pulls it back with a factor pullback.
The example optimizer_wrappers.py shows how to run the Lookahead with Adam from a fixed number of epochs and then to swap it to SWA. This uses the SWA.start_swa() functionality that let's the caller immediately run SWA without setting start_step.","Oh, one more thing about wrapping: there are various methods in Thinc and spaCy that take Optimizer arguments. Using these new classes would not typecheck, since they do not derive from Optimizer. I am not sure whether deriving from Optimizer does make sense, since that would also inherit a lot of internal state? I guess ideally Optimizer would be a base class.",True,{'THUMBS_UP': ['https://github.com/svlandeg']}
explosion/thinc,https://github.com/explosion/thinc,631,2022-04-06T13:29:52Z,2022-04-07T13:28:27Z,2022-04-07T13:28:27Z,MERGED,True,40,77,37,https://github.com/richardpaulhudson,Remove unnecessary init method return statements,1,"['enhancement', 'feat / layers']",https://github.com/explosion/thinc/pull/631,https://github.com/richardpaulhudson,1,https://github.com/explosion/thinc/pull/631,"Many layers return the initialized model from their init() methods, although the documentation of the superclass method does not include a return type and the returned model is not used anywhere when initialize() is called. Returning the models hence entails unnecessary code that in a couple of layers is also more involved than simply a return model at the end of the method, and is liable to lead to confusion about how model initialization works.
I've removed the return statements consistently across all layers.","Many layers return the initialized model from their init() methods, although the documentation of the superclass method does not include a return type and the returned model is not used anywhere when initialize() is called. Returning the models hence entails unnecessary code that in a couple of layers is also more involved than simply a return model at the end of the method, and is liable to lead to confusion about how model initialization works.
I've removed the return statements consistently across all layers.",True,"{'THUMBS_UP': ['https://github.com/svlandeg', 'https://github.com/kadarakos']}"
explosion/thinc,https://github.com/explosion/thinc,631,2022-04-06T13:29:52Z,2022-04-07T13:28:27Z,2022-04-07T13:28:27Z,MERGED,True,40,77,37,https://github.com/richardpaulhudson,Remove unnecessary init method return statements,1,"['enhancement', 'feat / layers']",https://github.com/explosion/thinc/pull/631,https://github.com/honnibal,2,https://github.com/explosion/thinc/pull/631#issuecomment-1091736066,"Many layers return the initialized model from their init() methods, although the documentation of the superclass method does not include a return type and the returned model is not used anywhere when initialize() is called. Returning the models hence entails unnecessary code that in a couple of layers is also more involved than simply a return model at the end of the method, and is liable to lead to confusion about how model initialization works.
I've removed the return statements consistently across all layers.",Looks good to me too!,True,{}
explosion/thinc,https://github.com/explosion/thinc,632,2022-04-11T13:45:50Z,2022-04-14T14:48:23Z,2022-04-14T14:48:28Z,MERGED,True,155,76,4,https://github.com/shadeMe,Reduce unnecessary zero-init'd allocations,10,"['enhancement', 'feat / ops']",https://github.com/explosion/thinc/pull/632,https://github.com/shadeMe,1,https://github.com/explosion/thinc/pull/632,I've added a new keyword arg uninitialized to the allocator methods in the NumpyOps and Ops classes. Also replaced some zero-init'd allocations where the elements in the destination array were immediately replaced.,I've added a new keyword arg uninitialized to the allocator methods in the NumpyOps and Ops classes. Also replaced some zero-init'd allocations where the elements in the destination array were immediately replaced.,True,{}
explosion/thinc,https://github.com/explosion/thinc,632,2022-04-11T13:45:50Z,2022-04-14T14:48:23Z,2022-04-14T14:48:28Z,MERGED,True,155,76,4,https://github.com/shadeMe,Reduce unnecessary zero-init'd allocations,10,"['enhancement', 'feat / ops']",https://github.com/explosion/thinc/pull/632,https://github.com/shadeMe,2,https://github.com/explosion/thinc/pull/632#issuecomment-1096919447,I've added a new keyword arg uninitialized to the allocator methods in the NumpyOps and Ops classes. Also replaced some zero-init'd allocations where the elements in the destination array were immediately replaced.,"Great idea!
Not sure about the naming - uninitialized is not the most straightforward to type, but maybe just empty is too cryptic?

Yeah, I thought empty was wasn't explicit enough, especially given that the incorrect usage of this flag will almost certainly lead to memory corruption. I also considered flipping the logic and calling the param zero_init but wasn't convinced that it was better.
I'm open to suggestions/switching to zero_init if you find that it's a better alternative.",True,{}
explosion/thinc,https://github.com/explosion/thinc,632,2022-04-11T13:45:50Z,2022-04-14T14:48:23Z,2022-04-14T14:48:28Z,MERGED,True,155,76,4,https://github.com/shadeMe,Reduce unnecessary zero-init'd allocations,10,"['enhancement', 'feat / ops']",https://github.com/explosion/thinc/pull/632,https://github.com/danieldk,3,https://github.com/explosion/thinc/pull/632#issuecomment-1097898733,I've added a new keyword arg uninitialized to the allocator methods in the NumpyOps and Ops classes. Also replaced some zero-init'd allocations where the elements in the destination array were immediately replaced.,There are some allocations in CupyOps/_custom_kernels that could be updated in a similar way. E.g. the maxout kernel wrapper unnecessarily zero-initializes an array.,True,{}
explosion/thinc,https://github.com/explosion/thinc,632,2022-04-11T13:45:50Z,2022-04-14T14:48:23Z,2022-04-14T14:48:28Z,MERGED,True,155,76,4,https://github.com/shadeMe,Reduce unnecessary zero-init'd allocations,10,"['enhancement', 'feat / ops']",https://github.com/explosion/thinc/pull/632,https://github.com/shadeMe,4,https://github.com/explosion/thinc/pull/632#issuecomment-1098083855,I've added a new keyword arg uninitialized to the allocator methods in the NumpyOps and Ops classes. Also replaced some zero-init'd allocations where the elements in the destination array were immediately replaced.,CupyOps had a fair amount of zero-allocs that could be replaced. These changes pass the tests on the GPU.,True,{'THUMBS_UP': ['https://github.com/svlandeg']}
explosion/thinc,https://github.com/explosion/thinc,632,2022-04-11T13:45:50Z,2022-04-14T14:48:23Z,2022-04-14T14:48:28Z,MERGED,True,155,76,4,https://github.com/shadeMe,Reduce unnecessary zero-init'd allocations,10,"['enhancement', 'feat / ops']",https://github.com/explosion/thinc/pull/632,https://github.com/danieldk,5,https://github.com/explosion/thinc/pull/632#issuecomment-1098093271,I've added a new keyword arg uninitialized to the allocator methods in the NumpyOps and Ops classes. Also replaced some zero-init'd allocations where the elements in the destination array were immediately replaced.,@explosion-bot please test_gpu,True,"{'EYES': ['https://github.com/explosion-bot'], 'ROCKET': ['https://github.com/explosion-bot']}"
explosion/thinc,https://github.com/explosion/thinc,632,2022-04-11T13:45:50Z,2022-04-14T14:48:23Z,2022-04-14T14:48:28Z,MERGED,True,155,76,4,https://github.com/shadeMe,Reduce unnecessary zero-init'd allocations,10,"['enhancement', 'feat / ops']",https://github.com/explosion/thinc/pull/632,https://github.com/explosion-bot,6,https://github.com/explosion/thinc/pull/632#issuecomment-1098093721,I've added a new keyword arg uninitialized to the allocator methods in the NumpyOps and Ops classes. Also replaced some zero-init'd allocations where the elements in the destination array were immediately replaced.," Successfully triggered build on Buildkite

URL: https://buildkite.com/explosion-ai/thinc-gpu-test-suite/builds/36",True,{}
explosion/thinc,https://github.com/explosion/thinc,632,2022-04-11T13:45:50Z,2022-04-14T14:48:23Z,2022-04-14T14:48:28Z,MERGED,True,155,76,4,https://github.com/shadeMe,Reduce unnecessary zero-init'd allocations,10,"['enhancement', 'feat / ops']",https://github.com/explosion/thinc/pull/632,https://github.com/danieldk,7,https://github.com/explosion/thinc/pull/632#issuecomment-1099075392,I've added a new keyword arg uninitialized to the allocator methods in the NumpyOps and Ops classes. Also replaced some zero-init'd allocations where the elements in the destination array were immediately replaced.,@explosion-bot please test_gpu,True,"{'EYES': ['https://github.com/explosion-bot'], 'ROCKET': ['https://github.com/explosion-bot']}"
explosion/thinc,https://github.com/explosion/thinc,632,2022-04-11T13:45:50Z,2022-04-14T14:48:23Z,2022-04-14T14:48:28Z,MERGED,True,155,76,4,https://github.com/shadeMe,Reduce unnecessary zero-init'd allocations,10,"['enhancement', 'feat / ops']",https://github.com/explosion/thinc/pull/632,https://github.com/explosion-bot,8,https://github.com/explosion/thinc/pull/632#issuecomment-1099075859,I've added a new keyword arg uninitialized to the allocator methods in the NumpyOps and Ops classes. Also replaced some zero-init'd allocations where the elements in the destination array were immediately replaced.," Successfully triggered build on Buildkite

URL: https://buildkite.com/explosion-ai/thinc-gpu-test-suite/builds/37",True,{}
explosion/thinc,https://github.com/explosion/thinc,632,2022-04-11T13:45:50Z,2022-04-14T14:48:23Z,2022-04-14T14:48:28Z,MERGED,True,155,76,4,https://github.com/shadeMe,Reduce unnecessary zero-init'd allocations,10,"['enhancement', 'feat / ops']",https://github.com/explosion/thinc/pull/632,https://github.com/shadeMe,9,https://github.com/explosion/thinc/pull/632#issuecomment-1099106525,I've added a new keyword arg uninitialized to the allocator methods in the NumpyOps and Ops classes. Also replaced some zero-init'd allocations where the elements in the destination array were immediately replaced.,"Whoops! Clicked on the wrong ""Re-request review"" button. Only mean to do so for @svlandeg.",True,{}
explosion/thinc,https://github.com/explosion/thinc,632,2022-04-11T13:45:50Z,2022-04-14T14:48:23Z,2022-04-14T14:48:28Z,MERGED,True,155,76,4,https://github.com/shadeMe,Reduce unnecessary zero-init'd allocations,10,"['enhancement', 'feat / ops']",https://github.com/explosion/thinc/pull/632,https://github.com/svlandeg,10,https://github.com/explosion/thinc/pull/632#issuecomment-1099264856,I've added a new keyword arg uninitialized to the allocator methods in the NumpyOps and Ops classes. Also replaced some zero-init'd allocations where the elements in the destination array were immediately replaced.,Merged!,True,{}
explosion/thinc,https://github.com/explosion/thinc,633,2022-04-12T08:56:26Z,2022-04-26T09:53:20Z,2022-04-26T09:53:20Z,MERGED,True,5,4,1,https://github.com/danieldk,search.Beam: Fix invalid indexing,1,"['bug', 'feat / beam']",https://github.com/explosion/thinc/pull/633,https://github.com/danieldk,1,https://github.com/explosion/thinc/pull/633,"If none of the states of the beam have valid moves, it was attempted to  index into the vector of scored valid moves anyway, which results in invalid memory access.
Fix this by first checking that there valid moves before attempting to  index into them.
@honnibal: I am not sure if this is the valid way to address this issue, but this is the source of access violations on Windows in explosion/spaCy#10633.","If none of the states of the beam have valid moves, it was attempted to  index into the vector of scored valid moves anyway, which results in invalid memory access.
Fix this by first checking that there valid moves before attempting to  index into them.
@honnibal: I am not sure if this is the valid way to address this issue, but this is the source of access violations on Windows in explosion/spaCy#10633.",True,{'THUMBS_UP': ['https://github.com/svlandeg']}
explosion/thinc,https://github.com/explosion/thinc,633,2022-04-12T08:56:26Z,2022-04-26T09:53:20Z,2022-04-26T09:53:20Z,MERGED,True,5,4,1,https://github.com/danieldk,search.Beam: Fix invalid indexing,1,"['bug', 'feat / beam']",https://github.com/explosion/thinc/pull/633,https://github.com/adrianeboyd,2,https://github.com/explosion/thinc/pull/633#issuecomment-1096636058,"If none of the states of the beam have valid moves, it was attempted to  index into the vector of scored valid moves anyway, which results in invalid memory access.
Fix this by first checking that there valid moves before attempting to  index into them.
@honnibal: I am not sure if this is the valid way to address this issue, but this is the source of access violations on Windows in explosion/spaCy#10633.","It is weird that the CI didn't run, I'll close and reopen to see what happens.",True,{}
explosion/thinc,https://github.com/explosion/thinc,633,2022-04-12T08:56:26Z,2022-04-26T09:53:20Z,2022-04-26T09:53:20Z,MERGED,True,5,4,1,https://github.com/danieldk,search.Beam: Fix invalid indexing,1,"['bug', 'feat / beam']",https://github.com/explosion/thinc/pull/633,https://github.com/honnibal,3,https://github.com/explosion/thinc/pull/633#issuecomment-1105087870,"If none of the states of the beam have valid moves, it was attempted to  index into the vector of scored valid moves anyway, which results in invalid memory access.
Fix this by first checking that there valid moves before attempting to  index into them.
@honnibal: I am not sure if this is the valid way to address this issue, but this is the source of access violations on Windows in explosion/spaCy#10633.",This is a fine guard but it should be impossible to have no valid continuations to a state. So is there a deeper problem here?,True,{}
explosion/thinc,https://github.com/explosion/thinc,633,2022-04-12T08:56:26Z,2022-04-26T09:53:20Z,2022-04-26T09:53:20Z,MERGED,True,5,4,1,https://github.com/danieldk,search.Beam: Fix invalid indexing,1,"['bug', 'feat / beam']",https://github.com/explosion/thinc/pull/633,https://github.com/danieldk,4,https://github.com/explosion/thinc/pull/633#issuecomment-1105490567,"If none of the states of the beam have valid moves, it was attempted to  index into the vector of scored valid moves anyway, which results in invalid memory access.
Fix this by first checking that there valid moves before attempting to  index into them.
@honnibal: I am not sure if this is the valid way to address this issue, but this is the source of access violations on Windows in explosion/spaCy#10633.","This is a fine guard but it should be impossible to have no valid continuations to a state. So is there a deeper problem here?

It is caused by this test:
https://github.com/explosion/spaCy/blob/29afbdb91e5fecf513125a85f1ac1d165f40bc93/spacy/tests/parser/test_ner.py#L183
I guess there are no continuations because the pipe is not trained, so all labels that were added are in unseen_classes and can not be predicted?",True,{}
explosion/thinc,https://github.com/explosion/thinc,633,2022-04-12T08:56:26Z,2022-04-26T09:53:20Z,2022-04-26T09:53:20Z,MERGED,True,5,4,1,https://github.com/danieldk,search.Beam: Fix invalid indexing,1,"['bug', 'feat / beam']",https://github.com/explosion/thinc/pull/633,https://github.com/svlandeg,5,https://github.com/explosion/thinc/pull/633#issuecomment-1106317195,"If none of the states of the beam have valid moves, it was attempted to  index into the vector of scored valid moves anyway, which results in invalid memory access.
Fix this by first checking that there valid moves before attempting to  index into them.
@honnibal: I am not sure if this is the valid way to address this issue, but this is the source of access violations on Windows in explosion/spaCy#10633.",So should we somehow try to catch this earlier and provide a more meaningful user warning/error?,True,{}
explosion/thinc,https://github.com/explosion/thinc,633,2022-04-12T08:56:26Z,2022-04-26T09:53:20Z,2022-04-26T09:53:20Z,MERGED,True,5,4,1,https://github.com/danieldk,search.Beam: Fix invalid indexing,1,"['bug', 'feat / beam']",https://github.com/explosion/thinc/pull/633,https://github.com/danieldk,6,https://github.com/explosion/thinc/pull/633#issuecomment-1106403636,"If none of the states of the beam have valid moves, it was attempted to  index into the vector of scored valid moves anyway, which results in invalid memory access.
Fix this by first checking that there valid moves before attempting to  index into them.
@honnibal: I am not sure if this is the valid way to address this issue, but this is the source of access violations on Windows in explosion/spaCy#10633.","So should we somehow try to catch this earlier and provide a more meaningful user warning/error?

Let me try and see if I can add a warning to spaCy. Edit: I guess spaCy is the best place, because in Thinc this is a utility class and there may be applications where this is an acceptable outcome?
There still seems to be a deeper issue, since this doesn't happen in the parser in spaCy master.",True,{}
explosion/thinc,https://github.com/explosion/thinc,633,2022-04-12T08:56:26Z,2022-04-26T09:53:20Z,2022-04-26T09:53:20Z,MERGED,True,5,4,1,https://github.com/danieldk,search.Beam: Fix invalid indexing,1,"['bug', 'feat / beam']",https://github.com/explosion/thinc/pull/633,https://github.com/danieldk,7,https://github.com/explosion/thinc/pull/633#issuecomment-1106507407,"If none of the states of the beam have valid moves, it was attempted to  index into the vector of scored valid moves anyway, which results in invalid memory access.
Fix this by first checking that there valid moves before attempting to  index into them.
@honnibal: I am not sure if this is the valid way to address this issue, but this is the source of access violations on Windows in explosion/spaCy#10633.",Found the issue: the Parser.beam_parse/greedy_parse methods in my branch didn't ensure that labels were added. I have fixed this in the PR that adds back beam parsing (explosion/spaCy#10633). It should now (hopefully) also pass on Windows without this PR.,True,"{'THUMBS_UP': ['https://github.com/svlandeg', 'https://github.com/shadeMe']}"
explosion/thinc,https://github.com/explosion/thinc,633,2022-04-12T08:56:26Z,2022-04-26T09:53:20Z,2022-04-26T09:53:20Z,MERGED,True,5,4,1,https://github.com/danieldk,search.Beam: Fix invalid indexing,1,"['bug', 'feat / beam']",https://github.com/explosion/thinc/pull/633,https://github.com/svlandeg,8,https://github.com/explosion/thinc/pull/633#issuecomment-1106551583,"If none of the states of the beam have valid moves, it was attempted to  index into the vector of scored valid moves anyway, which results in invalid memory access.
Fix this by first checking that there valid moves before attempting to  index into them.
@honnibal: I am not sure if this is the valid way to address this issue, but this is the source of access violations on Windows in explosion/spaCy#10633.",Great! We should still merge this as well though :-),True,{}
explosion/thinc,https://github.com/explosion/thinc,634,2022-04-14T13:29:51Z,2022-04-14T15:15:08Z,2022-04-14T15:15:08Z,MERGED,True,7,13,2,https://github.com/shadeMe,Doc fixes,3,['docs'],https://github.com/explosion/thinc/pull/634,https://github.com/shadeMe,1,https://github.com/explosion/thinc/pull/634,"Indicate support for DLPack when using TF models (already implemented).
Remove the Distributed Training section due to poor support for Ray and deleted Colab notebook.","Indicate support for DLPack when using TF models (already implemented).
Remove the Distributed Training section due to poor support for Ray and deleted Colab notebook.",True,{'THUMBS_UP': ['https://github.com/svlandeg']}
explosion/thinc,https://github.com/explosion/thinc,634,2022-04-14T13:29:51Z,2022-04-14T15:15:08Z,2022-04-14T15:15:08Z,MERGED,True,7,13,2,https://github.com/shadeMe,Doc fixes,3,['docs'],https://github.com/explosion/thinc/pull/634,https://github.com/svlandeg,2,https://github.com/explosion/thinc/pull/634#issuecomment-1099290152,"Indicate support for DLPack when using TF models (already implemented).
Remove the Distributed Training section due to poor support for Ray and deleted Colab notebook.",Thanks!,True,{}
explosion/thinc,https://github.com/explosion/thinc,635,2022-04-19T10:26:21Z,2022-04-19T11:03:02Z,2022-04-25T17:43:10Z,CLOSED,False,3,1,1,https://github.com/shadeMe,Revert uninit'd allocs in `Numpy.reduce_max` introduced in #632. ,14,"['bug', 'feat / ops']",https://github.com/explosion/thinc/pull/635,https://github.com/shadeMe,1,https://github.com/explosion/thinc/pull/635,This change broke spancat tests in the spaCy test suite.,This change broke spancat tests in the spaCy test suite.,True,{}
explosion/thinc,https://github.com/explosion/thinc,635,2022-04-19T10:26:21Z,2022-04-19T11:03:02Z,2022-04-25T17:43:10Z,CLOSED,False,3,1,1,https://github.com/shadeMe,Revert uninit'd allocs in `Numpy.reduce_max` introduced in #632. ,14,"['bug', 'feat / ops']",https://github.com/explosion/thinc/pull/635,https://github.com/danieldk,2,https://github.com/explosion/thinc/pull/635#issuecomment-1102480676,This change broke spancat tests in the spaCy test suite.,Why are there so many commits for a one-liner? Seems like some merges?,True,{}
explosion/thinc,https://github.com/explosion/thinc,635,2022-04-19T10:26:21Z,2022-04-19T11:03:02Z,2022-04-25T17:43:10Z,CLOSED,False,3,1,1,https://github.com/shadeMe,Revert uninit'd allocs in `Numpy.reduce_max` introduced in #632. ,14,"['bug', 'feat / ops']",https://github.com/explosion/thinc/pull/635,https://github.com/shadeMe,3,https://github.com/explosion/thinc/pull/635#issuecomment-1102483917,This change broke spancat tests in the spaCy test suite.,"Yeah, I was wondering that myself. I was expecting 3 - one for each merge and one for the change itself - but GitHub doesn't seem to have identified that the rest are already part of the commit history in the target branch ",True,{}
explosion/thinc,https://github.com/explosion/thinc,635,2022-04-19T10:26:21Z,2022-04-19T11:03:02Z,2022-04-25T17:43:10Z,CLOSED,False,3,1,1,https://github.com/shadeMe,Revert uninit'd allocs in `Numpy.reduce_max` introduced in #632. ,14,"['bug', 'feat / ops']",https://github.com/explosion/thinc/pull/635,https://github.com/svlandeg,4,https://github.com/explosion/thinc/pull/635#issuecomment-1102485999,This change broke spancat tests in the spaCy test suite.,Perhaps because the original PR was squashed? Would it make sense to just redo this PR from a clean copy & add the one commit?,True,{}
explosion/thinc,https://github.com/explosion/thinc,635,2022-04-19T10:26:21Z,2022-04-19T11:03:02Z,2022-04-25T17:43:10Z,CLOSED,False,3,1,1,https://github.com/shadeMe,Revert uninit'd allocs in `Numpy.reduce_max` introduced in #632. ,14,"['bug', 'feat / ops']",https://github.com/explosion/thinc/pull/635,https://github.com/svlandeg,5,https://github.com/explosion/thinc/pull/635#issuecomment-1102486228,This change broke spancat tests in the spaCy test suite.,"I don't care strongly, but it does look a bit weird ",True,{}
explosion/thinc,https://github.com/explosion/thinc,635,2022-04-19T10:26:21Z,2022-04-19T11:03:02Z,2022-04-25T17:43:10Z,CLOSED,False,3,1,1,https://github.com/shadeMe,Revert uninit'd allocs in `Numpy.reduce_max` introduced in #632. ,14,"['bug', 'feat / ops']",https://github.com/explosion/thinc/pull/635,https://github.com/shadeMe,6,https://github.com/explosion/thinc/pull/635#issuecomment-1102502085,This change broke spancat tests in the spaCy test suite.,"Yeah, I can do that.",True,{}
explosion/thinc,https://github.com/explosion/thinc,635,2022-04-19T10:26:21Z,2022-04-19T11:03:02Z,2022-04-25T17:43:10Z,CLOSED,False,3,1,1,https://github.com/shadeMe,Revert uninit'd allocs in `Numpy.reduce_max` introduced in #632. ,14,"['bug', 'feat / ops']",https://github.com/explosion/thinc/pull/635,https://github.com/shadeMe,7,https://github.com/explosion/thinc/pull/635#issuecomment-1102508022,This change broke spancat tests in the spaCy test suite.,New PR - #636,True,{}
explosion/thinc,https://github.com/explosion/thinc,636,2022-04-19T11:02:53Z,2022-04-21T10:35:13Z,2022-04-25T17:43:14Z,MERGED,True,8,1,2,https://github.com/shadeMe,Revert uninit'd allocs in `Numpy.reduce_max` introduced in #632. ,4,"['bug', 'feat / ops']",https://github.com/explosion/thinc/pull/636,https://github.com/shadeMe,1,https://github.com/explosion/thinc/pull/636,This change broke spancat tests in the spaCy test suite.,This change broke spancat tests in the spaCy test suite.,True,{'THUMBS_UP': ['https://github.com/svlandeg']}
explosion/thinc,https://github.com/explosion/thinc,637,2022-04-19T11:30:49Z,2022-05-05T17:54:21Z,2022-05-05T17:54:25Z,MERGED,True,82,34,7,https://github.com/danieldk,Fix reductions when applied to zero-length sequences,6,"['bug', 'feat / ops']",https://github.com/explosion/thinc/pull/637,https://github.com/danieldk,1,https://github.com/explosion/thinc/pull/637,"A bug was introduced in NumpyOps that caused the output pointer not to be incremented for zero-length sequences.
When using uninitialized arrays, the sum or mean for a zero-length array was not correctly set to zero in Ops.

We should also do something about zero-length sequences for reduce_max. However, it's unclear what the corresponding which should be set to (since 0 is not a valid index for a zero-sized sequence). Maybe we should throw an exception when trying to apply reduce_max to a zero-length sequence?","A bug was introduced in NumpyOps that caused the output pointer not to be incremented for zero-length sequences.
When using uninitialized arrays, the sum or mean for a zero-length array was not correctly set to zero in Ops.

We should also do something about zero-length sequences for reduce_max. However, it's unclear what the corresponding which should be set to (since 0 is not a valid index for a zero-sized sequence). Maybe we should throw an exception when trying to apply reduce_max to a zero-length sequence?",True,{}
explosion/thinc,https://github.com/explosion/thinc,637,2022-04-19T11:30:49Z,2022-05-05T17:54:21Z,2022-05-05T17:54:25Z,MERGED,True,82,34,7,https://github.com/danieldk,Fix reductions when applied to zero-length sequences,6,"['bug', 'feat / ops']",https://github.com/explosion/thinc/pull/637,https://github.com/shadeMe,2,https://github.com/explosion/thinc/pull/637#issuecomment-1102692321,"A bug was introduced in NumpyOps that caused the output pointer not to be incremented for zero-length sequences.
When using uninitialized arrays, the sum or mean for a zero-length array was not correctly set to zero in Ops.

We should also do something about zero-length sequences for reduce_max. However, it's unclear what the corresponding which should be set to (since 0 is not a valid index for a zero-sized sequence). Maybe we should throw an exception when trying to apply reduce_max to a zero-length sequence?","Are there any contexts where zero-length sequences can be considered valid input? Also, since the which array is signed, using -1 to signify zero-length input sequences might be one possible way to go.",True,{}
explosion/thinc,https://github.com/explosion/thinc,637,2022-04-19T11:30:49Z,2022-05-05T17:54:21Z,2022-05-05T17:54:25Z,MERGED,True,82,34,7,https://github.com/danieldk,Fix reductions when applied to zero-length sequences,6,"['bug', 'feat / ops']",https://github.com/explosion/thinc/pull/637,https://github.com/danieldk,3,https://github.com/explosion/thinc/pull/637#issuecomment-1102759688,"A bug was introduced in NumpyOps that caused the output pointer not to be incremented for zero-length sequences.
When using uninitialized arrays, the sum or mean for a zero-length array was not correctly set to zero in Ops.

We should also do something about zero-length sequences for reduce_max. However, it's unclear what the corresponding which should be set to (since 0 is not a valid index for a zero-sized sequence). Maybe we should throw an exception when trying to apply reduce_max to a zero-length sequence?","Are there any contexts where zero-length sequences can be considered valid input?

The floret support in spaCy relies on the mean of a zero-length sequence to be zero.

Also, since the which array is signed, using -1 to signify zero-length input sequences might be one possible way to go.

Maybe. I think the nice thing about which is that you can now use it with e.g. take_along_axis. However, -1 would silently take the last element along a given axis.",True,{}
explosion/thinc,https://github.com/explosion/thinc,637,2022-04-19T11:30:49Z,2022-05-05T17:54:21Z,2022-05-05T17:54:25Z,MERGED,True,82,34,7,https://github.com/danieldk,Fix reductions when applied to zero-length sequences,6,"['bug', 'feat / ops']",https://github.com/explosion/thinc/pull/637,https://github.com/danieldk,4,https://github.com/explosion/thinc/pull/637#issuecomment-1103593811,"A bug was introduced in NumpyOps that caused the output pointer not to be incremented for zero-length sequences.
When using uninitialized arrays, the sum or mean for a zero-length array was not correctly set to zero in Ops.

We should also do something about zero-length sequences for reduce_max. However, it's unclear what the corresponding which should be set to (since 0 is not a valid index for a zero-sized sequence). Maybe we should throw an exception when trying to apply reduce_max to a zero-length sequence?",@explosion-bot please test_gpu --spacy-branch master,True,"{'EYES': ['https://github.com/explosion-bot'], 'ROCKET': ['https://github.com/explosion-bot']}"
explosion/thinc,https://github.com/explosion/thinc,637,2022-04-19T11:30:49Z,2022-05-05T17:54:21Z,2022-05-05T17:54:25Z,MERGED,True,82,34,7,https://github.com/danieldk,Fix reductions when applied to zero-length sequences,6,"['bug', 'feat / ops']",https://github.com/explosion/thinc/pull/637,https://github.com/explosion-bot,5,https://github.com/explosion/thinc/pull/637#issuecomment-1103594264,"A bug was introduced in NumpyOps that caused the output pointer not to be incremented for zero-length sequences.
When using uninitialized arrays, the sum or mean for a zero-length array was not correctly set to zero in Ops.

We should also do something about zero-length sequences for reduce_max. However, it's unclear what the corresponding which should be set to (since 0 is not a valid index for a zero-sized sequence). Maybe we should throw an exception when trying to apply reduce_max to a zero-length sequence?"," Successfully triggered build on Buildkite

URL: https://buildkite.com/explosion-ai/thinc-gpu-test-suite/builds/39",True,{'THUMBS_UP': ['https://github.com/svlandeg']}
explosion/thinc,https://github.com/explosion/thinc,637,2022-04-19T11:30:49Z,2022-05-05T17:54:21Z,2022-05-05T17:54:25Z,MERGED,True,82,34,7,https://github.com/danieldk,Fix reductions when applied to zero-length sequences,6,"['bug', 'feat / ops']",https://github.com/explosion/thinc/pull/637,https://github.com/svlandeg,6,https://github.com/explosion/thinc/pull/637#issuecomment-1117544871,"A bug was introduced in NumpyOps that caused the output pointer not to be incremented for zero-length sequences.
When using uninitialized arrays, the sum or mean for a zero-length array was not correctly set to zero in Ops.

We should also do something about zero-length sequences for reduce_max. However, it's unclear what the corresponding which should be set to (since 0 is not a valid index for a zero-sized sequence). Maybe we should throw an exception when trying to apply reduce_max to a zero-length sequence?",I think kos is better positioned to review this one :-),True,{}
explosion/thinc,https://github.com/explosion/thinc,637,2022-04-19T11:30:49Z,2022-05-05T17:54:21Z,2022-05-05T17:54:25Z,MERGED,True,82,34,7,https://github.com/danieldk,Fix reductions when applied to zero-length sequences,6,"['bug', 'feat / ops']",https://github.com/explosion/thinc/pull/637,https://github.com/kadarakos,7,https://github.com/explosion/thinc/pull/637#issuecomment-1118543808,"A bug was introduced in NumpyOps that caused the output pointer not to be incremented for zero-length sequences.
When using uninitialized arrays, the sum or mean for a zero-length array was not correctly set to zero in Ops.

We should also do something about zero-length sequences for reduce_max. However, it's unclear what the corresponding which should be set to (since 0 is not a valid index for a zero-sized sequence). Maybe we should throw an exception when trying to apply reduce_max to a zero-length sequence?","I also have a final more general question about this PR. There are two extra reduction operations in thinc, which are the layers reduce_first and reduce_last.

https://github.com/explosion/thinc/blob/master/thinc/layers/reduce_first.py
https://github.com/explosion/thinc/blob/master/thinc/layers/reduce_last.py

These do not call into the ops so this PR hasn't dealt with them, but similarly to reduce_max I think its fair to handle 0 length sequences with a ValueError since first and last are selection type ops like max. Should this be part of this PR still or should this go to a next PR?",True,{}
explosion/thinc,https://github.com/explosion/thinc,637,2022-04-19T11:30:49Z,2022-05-05T17:54:21Z,2022-05-05T17:54:25Z,MERGED,True,82,34,7,https://github.com/danieldk,Fix reductions when applied to zero-length sequences,6,"['bug', 'feat / ops']",https://github.com/explosion/thinc/pull/637,https://github.com/danieldk,8,https://github.com/explosion/thinc/pull/637#issuecomment-1118722769,"A bug was introduced in NumpyOps that caused the output pointer not to be incremented for zero-length sequences.
When using uninitialized arrays, the sum or mean for a zero-length array was not correctly set to zero in Ops.

We should also do something about zero-length sequences for reduce_max. However, it's unclear what the corresponding which should be set to (since 0 is not a valid index for a zero-sized sequence). Maybe we should throw an exception when trying to apply reduce_max to a zero-length sequence?",@explosion-bot please test_gpu --spacy-branch master,True,"{'EYES': ['https://github.com/explosion-bot'], 'ROCKET': ['https://github.com/explosion-bot']}"
explosion/thinc,https://github.com/explosion/thinc,637,2022-04-19T11:30:49Z,2022-05-05T17:54:21Z,2022-05-05T17:54:25Z,MERGED,True,82,34,7,https://github.com/danieldk,Fix reductions when applied to zero-length sequences,6,"['bug', 'feat / ops']",https://github.com/explosion/thinc/pull/637,https://github.com/explosion-bot,9,https://github.com/explosion/thinc/pull/637#issuecomment-1118723223,"A bug was introduced in NumpyOps that caused the output pointer not to be incremented for zero-length sequences.
When using uninitialized arrays, the sum or mean for a zero-length array was not correctly set to zero in Ops.

We should also do something about zero-length sequences for reduce_max. However, it's unclear what the corresponding which should be set to (since 0 is not a valid index for a zero-sized sequence). Maybe we should throw an exception when trying to apply reduce_max to a zero-length sequence?"," Successfully triggered build on Buildkite

URL: https://buildkite.com/explosion-ai/thinc-gpu-test-suite/builds/60",True,{}
explosion/thinc,https://github.com/explosion/thinc,637,2022-04-19T11:30:49Z,2022-05-05T17:54:21Z,2022-05-05T17:54:25Z,MERGED,True,82,34,7,https://github.com/danieldk,Fix reductions when applied to zero-length sequences,6,"['bug', 'feat / ops']",https://github.com/explosion/thinc/pull/637,https://github.com/danieldk,10,https://github.com/explosion/thinc/pull/637#issuecomment-1118882321,"A bug was introduced in NumpyOps that caused the output pointer not to be incremented for zero-length sequences.
When using uninitialized arrays, the sum or mean for a zero-length array was not correctly set to zero in Ops.

We should also do something about zero-length sequences for reduce_max. However, it's unclear what the corresponding which should be set to (since 0 is not a valid index for a zero-sized sequence). Maybe we should throw an exception when trying to apply reduce_max to a zero-length sequence?","The tensorflow errors are unrelated, see #653.",True,{}
explosion/thinc,https://github.com/explosion/thinc,638,2022-04-19T16:07:11Z,2022-04-20T12:18:53Z,2022-04-25T17:43:15Z,MERGED,True,1,1,1,https://github.com/shadeMe,Fix broken link to `backends` in README.md,1,['docs'],https://github.com/explosion/thinc/pull/638,https://github.com/shadeMe,1,https://github.com/explosion/thinc/pull/638,,,True,{}
explosion/thinc,https://github.com/explosion/thinc,639,2022-04-19T17:53:46Z,2022-04-21T11:24:19Z,2022-04-25T17:43:18Z,MERGED,True,43,9,2,https://github.com/shadeMe,Fix PyTorch tensor handling in `CupyOps.asarray`,6,"['bug', 'feat / shims', 'feat / ops']",https://github.com/explosion/thinc/pull/639,https://github.com/shadeMe,1,https://github.com/explosion/thinc/pull/639,"This PR fixes the following and adds a regression test:

The original invocation of cupy.cuda.MemoryPointer was missing the mandatory offset argument. This should be 0.
cupy.cuda.MemoryPointer expects a wrapper to a cupy-owned CUDA memory and not a raw pointer to it. So, CUDA memory allocated by external libraries needs to be wrapped with UnownedMemory and then passed to the array c'tor [1].
We weren't testing that the incoming array-like obj was owned by a CUDA device.
We were incorrectly passing the array stride as its shape.

Should fix #564.","This PR fixes the following and adds a regression test:

The original invocation of cupy.cuda.MemoryPointer was missing the mandatory offset argument. This should be 0.
cupy.cuda.MemoryPointer expects a wrapper to a cupy-owned CUDA memory and not a raw pointer to it. So, CUDA memory allocated by external libraries needs to be wrapped with UnownedMemory and then passed to the array c'tor [1].
We weren't testing that the incoming array-like obj was owned by a CUDA device.
We were incorrectly passing the array stride as its shape.

Should fix #564.",True,"{'HOORAY': ['https://github.com/danieldk', 'https://github.com/svlandeg']}"
explosion/thinc,https://github.com/explosion/thinc,639,2022-04-19T17:53:46Z,2022-04-21T11:24:19Z,2022-04-25T17:43:18Z,MERGED,True,43,9,2,https://github.com/shadeMe,Fix PyTorch tensor handling in `CupyOps.asarray`,6,"['bug', 'feat / shims', 'feat / ops']",https://github.com/explosion/thinc/pull/639,https://github.com/shadeMe,2,https://github.com/explosion/thinc/pull/639#issuecomment-1102945543,"This PR fixes the following and adds a regression test:

The original invocation of cupy.cuda.MemoryPointer was missing the mandatory offset argument. This should be 0.
cupy.cuda.MemoryPointer expects a wrapper to a cupy-owned CUDA memory and not a raw pointer to it. So, CUDA memory allocated by external libraries needs to be wrapped with UnownedMemory and then passed to the array c'tor [1].
We weren't testing that the incoming array-like obj was owned by a CUDA device.
We were incorrectly passing the array stride as its shape.

Should fix #564.",@explosion-bot please test_gpu,True,"{'EYES': ['https://github.com/explosion-bot'], 'ROCKET': ['https://github.com/explosion-bot']}"
explosion/thinc,https://github.com/explosion/thinc,639,2022-04-19T17:53:46Z,2022-04-21T11:24:19Z,2022-04-25T17:43:18Z,MERGED,True,43,9,2,https://github.com/shadeMe,Fix PyTorch tensor handling in `CupyOps.asarray`,6,"['bug', 'feat / shims', 'feat / ops']",https://github.com/explosion/thinc/pull/639,https://github.com/explosion-bot,3,https://github.com/explosion/thinc/pull/639#issuecomment-1102945866,"This PR fixes the following and adds a regression test:

The original invocation of cupy.cuda.MemoryPointer was missing the mandatory offset argument. This should be 0.
cupy.cuda.MemoryPointer expects a wrapper to a cupy-owned CUDA memory and not a raw pointer to it. So, CUDA memory allocated by external libraries needs to be wrapped with UnownedMemory and then passed to the array c'tor [1].
We weren't testing that the incoming array-like obj was owned by a CUDA device.
We were incorrectly passing the array stride as its shape.

Should fix #564."," Successfully triggered build on Buildkite

URL: https://buildkite.com/explosion-ai/thinc-gpu-test-suite/builds/38",True,{}
explosion/thinc,https://github.com/explosion/thinc,639,2022-04-19T17:53:46Z,2022-04-21T11:24:19Z,2022-04-25T17:43:18Z,MERGED,True,43,9,2,https://github.com/shadeMe,Fix PyTorch tensor handling in `CupyOps.asarray`,6,"['bug', 'feat / shims', 'feat / ops']",https://github.com/explosion/thinc/pull/639,https://github.com/shadeMe,4,https://github.com/explosion/thinc/pull/639#issuecomment-1103771702,"This PR fixes the following and adds a regression test:

The original invocation of cupy.cuda.MemoryPointer was missing the mandatory offset argument. This should be 0.
cupy.cuda.MemoryPointer expects a wrapper to a cupy-owned CUDA memory and not a raw pointer to it. So, CUDA memory allocated by external libraries needs to be wrapped with UnownedMemory and then passed to the array c'tor [1].
We weren't testing that the incoming array-like obj was owned by a CUDA device.
We were incorrectly passing the array stride as its shape.

Should fix #564.","Added support for zero-alloc conversions for Torch, TF and MXNet arrays, handle type conversion and removed the error for CPU-allocated inputs - I think we'll want to just copy it to the GPU in such cases instead of raising a hard error.",True,{}
explosion/thinc,https://github.com/explosion/thinc,639,2022-04-19T17:53:46Z,2022-04-21T11:24:19Z,2022-04-25T17:43:18Z,MERGED,True,43,9,2,https://github.com/shadeMe,Fix PyTorch tensor handling in `CupyOps.asarray`,6,"['bug', 'feat / shims', 'feat / ops']",https://github.com/explosion/thinc/pull/639,https://github.com/shadeMe,5,https://github.com/explosion/thinc/pull/639#issuecomment-1103772029,"This PR fixes the following and adds a regression test:

The original invocation of cupy.cuda.MemoryPointer was missing the mandatory offset argument. This should be 0.
cupy.cuda.MemoryPointer expects a wrapper to a cupy-owned CUDA memory and not a raw pointer to it. So, CUDA memory allocated by external libraries needs to be wrapped with UnownedMemory and then passed to the array c'tor [1].
We weren't testing that the incoming array-like obj was owned by a CUDA device.
We were incorrectly passing the array stride as its shape.

Should fix #564.",@explosion-bot please test_gpu,True,"{'EYES': ['https://github.com/explosion-bot'], 'ROCKET': ['https://github.com/explosion-bot']}"
explosion/thinc,https://github.com/explosion/thinc,639,2022-04-19T17:53:46Z,2022-04-21T11:24:19Z,2022-04-25T17:43:18Z,MERGED,True,43,9,2,https://github.com/shadeMe,Fix PyTorch tensor handling in `CupyOps.asarray`,6,"['bug', 'feat / shims', 'feat / ops']",https://github.com/explosion/thinc/pull/639,https://github.com/explosion-bot,6,https://github.com/explosion/thinc/pull/639#issuecomment-1103772443,"This PR fixes the following and adds a regression test:

The original invocation of cupy.cuda.MemoryPointer was missing the mandatory offset argument. This should be 0.
cupy.cuda.MemoryPointer expects a wrapper to a cupy-owned CUDA memory and not a raw pointer to it. So, CUDA memory allocated by external libraries needs to be wrapped with UnownedMemory and then passed to the array c'tor [1].
We weren't testing that the incoming array-like obj was owned by a CUDA device.
We were incorrectly passing the array stride as its shape.

Should fix #564."," Successfully triggered build on Buildkite

URL: https://buildkite.com/explosion-ai/thinc-gpu-test-suite/builds/40",True,{}
explosion/thinc,https://github.com/explosion/thinc,640,2022-04-20T07:59:53Z,2022-04-21T07:52:20Z,2022-04-21T07:52:20Z,MERGED,True,727,0,1,https://github.com/koaning,Bloom Embeddings Notebook,5,[],https://github.com/explosion/thinc/pull/640,https://github.com/koaning,1,https://github.com/explosion/thinc/pull/640,Added a notebook to accompany explosion/explosion.ai#48.,Added a notebook to accompany explosion/explosion.ai#48.,True,{'HOORAY': ['https://github.com/svlandeg']}
explosion/thinc,https://github.com/explosion/thinc,640,2022-04-20T07:59:53Z,2022-04-21T07:52:20Z,2022-04-21T07:52:20Z,MERGED,True,727,0,1,https://github.com/koaning,Bloom Embeddings Notebook,5,[],https://github.com/explosion/thinc/pull/640,https://github.com/koaning,2,https://github.com/explosion/thinc/pull/640#issuecomment-1103738262,Added a notebook to accompany explosion/explosion.ai#48.,"Many of the comments here seem valid, but these are excerpts from the blog post written here. I wouldn't mind addressing the items that @kadarakos mentions here but I'm a bit hesitant. Any changes here should also be reflected in the blog post.
@adrianeboyd, looping you in because you're also working on the blog post.",True,"{'THUMBS_UP': ['https://github.com/kadarakos', 'https://github.com/svlandeg']}"
explosion/thinc,https://github.com/explosion/thinc,640,2022-04-20T07:59:53Z,2022-04-21T07:52:20Z,2022-04-21T07:52:20Z,MERGED,True,727,0,1,https://github.com/koaning,Bloom Embeddings Notebook,5,[],https://github.com/explosion/thinc/pull/640,https://github.com/adrianeboyd,3,https://github.com/explosion/thinc/pull/640#issuecomment-1104833394,Added a notebook to accompany explosion/explosion.ai#48.,Let me go ahead and merge this so we can test it in the blog post draft. We can always update details in the future...,True,{'THUMBS_UP': ['https://github.com/koaning']}
explosion/thinc,https://github.com/explosion/thinc,641,2022-04-21T08:03:25Z,2022-04-21T08:05:18Z,2022-04-21T08:05:18Z,MERGED,True,1,1,1,https://github.com/adrianeboyd,Rename bloom embeddings notebook,1,[],https://github.com/explosion/thinc/pull/641,https://github.com/adrianeboyd,1,https://github.com/explosion/thinc/pull/641,,,True,{}
explosion/thinc,https://github.com/explosion/thinc,642,2022-04-22T08:28:03Z,2022-04-22T10:01:10Z,2022-04-22T10:01:10Z,MERGED,True,48,18,3,https://github.com/adrianeboyd,Update install instructions,2,['docs'],https://github.com/explosion/thinc/pull/642,https://github.com/adrianeboyd,1,https://github.com/explosion/thinc/pull/642,"Remove --pre and add conda-forge back to the standard install instructions.
Add a section on compiling from source with build constraints.","Remove --pre and add conda-forge back to the standard install instructions.
Add a section on compiling from source with build constraints.",True,{'THUMBS_UP': ['https://github.com/svlandeg']}
explosion/thinc,https://github.com/explosion/thinc,643,2022-04-22T14:56:11Z,2022-05-10T18:59:34Z,2022-05-11T06:49:55Z,MERGED,True,94,4,9,https://github.com/danieldk,NumpyOps: Add a method to get a table of C BLAS functions,4,"['enhancement', 'feat / ops']",https://github.com/explosion/thinc/pull/643,https://github.com/danieldk,1,https://github.com/explosion/thinc/pull/643,"This table can be used for downstream cdef nogil functions that need to use a BLAS function from the BLAS implementation used by an Ops subclass.
In draft because this change requires explosion/cython-blis#70.","This table can be used for downstream cdef nogil functions that need to use a BLAS function from the BLAS implementation used by an Ops subclass.
In draft because this change requires explosion/cython-blis#70.",True,{}
explosion/thinc,https://github.com/explosion/thinc,643,2022-04-22T14:56:11Z,2022-05-10T18:59:34Z,2022-05-11T06:49:55Z,MERGED,True,94,4,9,https://github.com/danieldk,NumpyOps: Add a method to get a table of C BLAS functions,4,"['enhancement', 'feat / ops']",https://github.com/explosion/thinc/pull/643,https://github.com/danieldk,2,https://github.com/explosion/thinc/pull/643#issuecomment-1122425964,"This table can be used for downstream cdef nogil functions that need to use a BLAS function from the BLAS implementation used by an Ops subclass.
In draft because this change requires explosion/cython-blis#70.","We're retiring develop - can you target master instead? 

Updated to master. Now that blis 0.9.0 is released, I'll also put this out of draft.",True,{}
explosion/thinc,https://github.com/explosion/thinc,644,2022-04-22T15:00:02Z,2022-04-24T07:03:08Z,2022-04-24T07:03:11Z,MERGED,True,879,131,46,https://github.com/danieldk,Merge master into develop,9,[],https://github.com/explosion/thinc/pull/644,https://github.com/danieldk,1,https://github.com/explosion/thinc/pull/644,,,True,{}
explosion/thinc,https://github.com/explosion/thinc,644,2022-04-22T15:00:02Z,2022-04-24T07:03:08Z,2022-04-24T07:03:11Z,MERGED,True,879,131,46,https://github.com/danieldk,Merge master into develop,9,[],https://github.com/explosion/thinc/pull/644,https://github.com/danieldk,2,https://github.com/explosion/thinc/pull/644#issuecomment-1106601347,,@explosion-bot please test_gpu --spacy-branch master,True,"{'EYES': ['https://github.com/explosion-bot'], 'ROCKET': ['https://github.com/explosion-bot']}"
explosion/thinc,https://github.com/explosion/thinc,644,2022-04-22T15:00:02Z,2022-04-24T07:03:08Z,2022-04-24T07:03:11Z,MERGED,True,879,131,46,https://github.com/danieldk,Merge master into develop,9,[],https://github.com/explosion/thinc/pull/644,https://github.com/explosion-bot,3,https://github.com/explosion/thinc/pull/644#issuecomment-1106601760,," Successfully triggered build on Buildkite

URL: https://buildkite.com/explosion-ai/thinc-gpu-test-suite/builds/41",True,{'THUMBS_UP': ['https://github.com/svlandeg']}
explosion/thinc,https://github.com/explosion/thinc,645,2022-04-22T16:07:42Z,2022-04-25T17:48:12Z,2022-04-25T17:48:19Z,CLOSED,False,11,3,1,https://github.com/shadeMe,Handle `cupy.fromDlpack` deprecation for `cupy >= 10.0.0`,1,[],https://github.com/explosion/thinc/pull/645,https://github.com/shadeMe,1,https://github.com/explosion/thinc/pull/645,,,True,{}
explosion/thinc,https://github.com/explosion/thinc,645,2022-04-22T16:07:42Z,2022-04-25T17:48:12Z,2022-04-25T17:48:19Z,CLOSED,False,11,3,1,https://github.com/shadeMe,Handle `cupy.fromDlpack` deprecation for `cupy >= 10.0.0`,1,[],https://github.com/explosion/thinc/pull/645,https://github.com/shadeMe,2,https://github.com/explosion/thinc/pull/645#issuecomment-1106662954,,@explosion-bot please test_gpu,True,"{'EYES': ['https://github.com/explosion-bot'], 'ROCKET': ['https://github.com/explosion-bot']}"
explosion/thinc,https://github.com/explosion/thinc,645,2022-04-22T16:07:42Z,2022-04-25T17:48:12Z,2022-04-25T17:48:19Z,CLOSED,False,11,3,1,https://github.com/shadeMe,Handle `cupy.fromDlpack` deprecation for `cupy >= 10.0.0`,1,[],https://github.com/explosion/thinc/pull/645,https://github.com/explosion-bot,3,https://github.com/explosion/thinc/pull/645#issuecomment-1106663261,," Successfully triggered build on Buildkite

URL: https://buildkite.com/explosion-ai/thinc-gpu-test-suite/builds/42",True,{}
explosion/thinc,https://github.com/explosion/thinc,646,2022-04-25T17:48:05Z,2022-05-02T13:07:32Z,2022-05-02T13:09:50Z,MERGED,True,96,36,5,https://github.com/shadeMe,"Sanity checks for GPU tests, 3rd party framework tensor conversion changes",10,"['enhancement', 'feat / ops']",https://github.com/explosion/thinc/pull/646,https://github.com/shadeMe,1,https://github.com/explosion/thinc/pull/646,"Add sanity checks to handle cases of hidden GPU devices
Allow ...2xp utility methods to accept a target Ops object for conversions","Add sanity checks to handle cases of hidden GPU devices
Allow ...2xp utility methods to accept a target Ops object for conversions",True,{}
explosion/thinc,https://github.com/explosion/thinc,646,2022-04-25T17:48:05Z,2022-05-02T13:07:32Z,2022-05-02T13:09:50Z,MERGED,True,96,36,5,https://github.com/shadeMe,"Sanity checks for GPU tests, 3rd party framework tensor conversion changes",10,"['enhancement', 'feat / ops']",https://github.com/explosion/thinc/pull/646,https://github.com/svlandeg,2,https://github.com/explosion/thinc/pull/646#issuecomment-1109676768,"Add sanity checks to handle cases of hidden GPU devices
Allow ...2xp utility methods to accept a target Ops object for conversions","First of two stacked PRs (to be merged with a merge commit; squashing/rebasing will break the 2nd, in-progress PR).

I'm not sure I understand this? If we squash this one, can't you rebase the next PR or merge in develop? (not sure what you mean by ""will break"")",True,{}
explosion/thinc,https://github.com/explosion/thinc,646,2022-04-25T17:48:05Z,2022-05-02T13:07:32Z,2022-05-02T13:09:50Z,MERGED,True,96,36,5,https://github.com/shadeMe,"Sanity checks for GPU tests, 3rd party framework tensor conversion changes",10,"['enhancement', 'feat / ops']",https://github.com/explosion/thinc/pull/646,https://github.com/shadeMe,3,https://github.com/explosion/thinc/pull/646#issuecomment-1109708563,"Add sanity checks to handle cases of hidden GPU devices
Allow ...2xp utility methods to accept a target Ops object for conversions","Actually, please ignore that last bit - Looks like I misunderstood how stacked PRs work. The original intention was to work on a PR that was dependent on these changes whilst waiting for it to be merged. I was hoping to stack the 2nd PR on this in such a way that I wouldn't have to do any additional merges (i.e., from explosion:develop after this PR is merged), but it seems like it doesn't work when attempting to merge branches between forks.
So, this PR can be squashed/rebased into explosion:develop. I'll just merge that into my local repo and rebase by PR as you suggested.",True,{'THUMBS_UP': ['https://github.com/svlandeg']}
explosion/thinc,https://github.com/explosion/thinc,646,2022-04-25T17:48:05Z,2022-05-02T13:07:32Z,2022-05-02T13:09:50Z,MERGED,True,96,36,5,https://github.com/shadeMe,"Sanity checks for GPU tests, 3rd party framework tensor conversion changes",10,"['enhancement', 'feat / ops']",https://github.com/explosion/thinc/pull/646,https://github.com/shadeMe,4,https://github.com/explosion/thinc/pull/646#issuecomment-1110027610,"Add sanity checks to handle cases of hidden GPU devices
Allow ...2xp utility methods to accept a target Ops object for conversions","Re. ops - Yeah, it can be keyword-only, and I too doubt that we'll be adding many more arguments - if at all any - in the future. Will change that.
Re. merging - I'm fine with leaving this open until Danil gets a chance to take a look at it. The other PR doesn't include any urgent fixes, and we still need to fix the Buildkite GPU build environment to actually test the changes in CI. So, it can wait.",True,{}
explosion/thinc,https://github.com/explosion/thinc,646,2022-04-25T17:48:05Z,2022-05-02T13:07:32Z,2022-05-02T13:09:50Z,MERGED,True,96,36,5,https://github.com/shadeMe,"Sanity checks for GPU tests, 3rd party framework tensor conversion changes",10,"['enhancement', 'feat / ops']",https://github.com/explosion/thinc/pull/646,https://github.com/shadeMe,5,https://github.com/explosion/thinc/pull/646#issuecomment-1110761403,"Add sanity checks to handle cases of hidden GPU devices
Allow ...2xp utility methods to accept a target Ops object for conversions","Hmm, looks like the mypy checks fail when ops: ""Ops"" is used as the type annotation. Using a fully/partially qualified identifier (thinc.api.Ops or .api.Ops) doesn't fix it either. Any suggestions?
Nevermind.",True,"{'LAUGH': ['https://github.com/svlandeg'], 'THUMBS_UP': ['https://github.com/danieldk']}"
explosion/thinc,https://github.com/explosion/thinc,646,2022-04-25T17:48:05Z,2022-05-02T13:07:32Z,2022-05-02T13:09:50Z,MERGED,True,96,36,5,https://github.com/shadeMe,"Sanity checks for GPU tests, 3rd party framework tensor conversion changes",10,"['enhancement', 'feat / ops']",https://github.com/explosion/thinc/pull/646,https://github.com/danieldk,6,https://github.com/explosion/thinc/pull/646#issuecomment-1114833579,"Add sanity checks to handle cases of hidden GPU devices
Allow ...2xp utility methods to accept a target Ops object for conversions","Merging, since @svlandeg also approved, and this is to develop, so we can still fix any issues that might pop up.",True,{'THUMBS_UP': ['https://github.com/svlandeg']}
explosion/thinc,https://github.com/explosion/thinc,647,2022-04-28T16:21:03Z,,2022-06-06T09:57:19Z,OPEN,False,858,191,7,https://github.com/kadarakos,Cross entropy fix,38,"['bug', 'feat / loss']",https://github.com/explosion/thinc/pull/647,https://github.com/kadarakos,1,https://github.com/explosion/thinc/pull/647,This PR makes the CategoricalCrossentropy loss more strict only allowing guesses and truths that represent exclusive classes and it fixes the computation of the loss value. It also changes all the tests that involve the CategoricalCrossentropy and SequenceCategoricalCrossentropy.,This PR makes the CategoricalCrossentropy loss more strict only allowing guesses and truths that represent exclusive classes and it fixes the computation of the loss value. It also changes all the tests that involve the CategoricalCrossentropy and SequenceCategoricalCrossentropy.,True,{}
explosion/thinc,https://github.com/explosion/thinc,647,2022-04-28T16:21:03Z,,2022-06-06T09:57:19Z,OPEN,False,858,191,7,https://github.com/kadarakos,Cross entropy fix,38,"['bug', 'feat / loss']",https://github.com/explosion/thinc/pull/647,https://github.com/svlandeg,2,https://github.com/explosion/thinc/pull/647#issuecomment-1136140163,This PR makes the CategoricalCrossentropy loss more strict only allowing guesses and truths that represent exclusive classes and it fixes the computation of the loss value. It also changes all the tests that involve the CategoricalCrossentropy and SequenceCategoricalCrossentropy.,"This PR makes the CategoricalCrossentropy loss more strict only allowing guesses and truths that represent exclusive classes

Why can't / shouldn't it (also) support multi-label classification?",True,{}
explosion/thinc,https://github.com/explosion/thinc,647,2022-04-28T16:21:03Z,,2022-06-06T09:57:19Z,OPEN,False,858,191,7,https://github.com/kadarakos,Cross entropy fix,38,"['bug', 'feat / loss']",https://github.com/explosion/thinc/pull/647,https://github.com/kadarakos,3,https://github.com/explosion/thinc/pull/647#issuecomment-1136871323,This PR makes the CategoricalCrossentropy loss more strict only allowing guesses and truths that represent exclusive classes and it fixes the computation of the loss value. It also changes all the tests that involve the CategoricalCrossentropy and SequenceCategoricalCrossentropy.,"This PR makes the CategoricalCrossentropy loss more strict only allowing guesses and truths that represent exclusive classes

Why can't / shouldn't it (also) support multi-label classification?

I was thinking that I would have a separate loss for that for two main reasons.
The simple one was just that the loss computation itself is different. So if the rows add up to 1 then its the way its done now, otherwise it would have to compute the binary cross-entropy. The gradient is the same though that's true.
The other main reason was just the confusing usage pattern from my perspective. So it takes Ints1d, List[str], List[int] or Floats2d as truths and it also does label-smoothing. The first three types of input and label-smoothing doesn't make sense for the multi-label binary cross entropy case. The only way to make it do binary cross entropy is to give it Floats2d and label-smoothing = False and in all other configurations it does categorical.
I just thought its nicer to implement a binary cross-entropy separately that can take different kinds of truths that are more appropriate for multiple labels per sample and has its own docs plus it always computes that same loss. For example it could take a List[Dict[str, float]] (textcat-multilabel could directly pass down this one i think) or List[List[int]], List[List[str]].",True,{'THUMBS_UP': ['https://github.com/svlandeg']}
explosion/thinc,https://github.com/explosion/thinc,647,2022-04-28T16:21:03Z,,2022-06-06T09:57:19Z,OPEN,False,858,191,7,https://github.com/kadarakos,Cross entropy fix,38,"['bug', 'feat / loss']",https://github.com/explosion/thinc/pull/647,https://github.com/svlandeg,4,https://github.com/explosion/thinc/pull/647#issuecomment-1136901279,This PR makes the CategoricalCrossentropy loss more strict only allowing guesses and truths that represent exclusive classes and it fixes the computation of the loss value. It also changes all the tests that involve the CategoricalCrossentropy and SequenceCategoricalCrossentropy.,"Thanks for the additional explanation, and agreed!",True,{}
explosion/thinc,https://github.com/explosion/thinc,647,2022-04-28T16:21:03Z,,2022-06-06T09:57:19Z,OPEN,False,858,191,7,https://github.com/kadarakos,Cross entropy fix,38,"['bug', 'feat / loss']",https://github.com/explosion/thinc/pull/647,https://github.com/danieldk,5,https://github.com/explosion/thinc/pull/647#issuecomment-1136935509,This PR makes the CategoricalCrossentropy loss more strict only allowing guesses and truths that represent exclusive classes and it fixes the computation of the loss value. It also changes all the tests that involve the CategoricalCrossentropy and SequenceCategoricalCrossentropy.,"The simple one was just that the loss computation itself is different. So if the rows add up to 1 then its the way its done now, otherwise it would have to compute the binary cross-entropy. The gradient is the same though that's true.

But is CategoricalCrossEntropy currently used anywhere for multi-class classification? If so, this change would break those cases? I am not sure, I haven't really looked much yet at multi-class classification in Thinc/spaCy.",True,{}
explosion/thinc,https://github.com/explosion/thinc,647,2022-04-28T16:21:03Z,,2022-06-06T09:57:19Z,OPEN,False,858,191,7,https://github.com/kadarakos,Cross entropy fix,38,"['bug', 'feat / loss']",https://github.com/explosion/thinc/pull/647,https://github.com/svlandeg,6,https://github.com/explosion/thinc/pull/647#issuecomment-1137002973,This PR makes the CategoricalCrossentropy loss more strict only allowing guesses and truths that represent exclusive classes and it fixes the computation of the loss value. It also changes all the tests that involve the CategoricalCrossentropy and SequenceCategoricalCrossentropy.,"I don't think it's being used in spaCy right now, but it should. And when we do, I'm proposing we work via specific versions, e.g.
loss_function = registry.get(""losses"", ""CategoricalCrossentropy.v3"")

Instead of a direct import. This will make it easier to keep the same behaviour if people want it, but to allow others to upgrade if they chose so.
It might still be that other users of Thinc are using the current CategoricalCrossentropy and would get bitten by a change in numerical values when they're logging results across training runs, or get bitten because the functionality is now more strict as Danil mentions. Then we'd be able to present the above solution to them for making sure they have access to the same functionality.
The fact that we need to change the tests here, is a huge red flag that this can be very breaking - which is why I'm proposing we lean into the framework of versioning that we've already set up anyway. Then we can keep the tests largely the same for the old functionality but write new ones for the new functionality.",True,{}
explosion/thinc,https://github.com/explosion/thinc,647,2022-04-28T16:21:03Z,,2022-06-06T09:57:19Z,OPEN,False,858,191,7,https://github.com/kadarakos,Cross entropy fix,38,"['bug', 'feat / loss']",https://github.com/explosion/thinc/pull/647,https://github.com/kadarakos,7,https://github.com/explosion/thinc/pull/647#issuecomment-1137208005,This PR makes the CategoricalCrossentropy loss more strict only allowing guesses and truths that represent exclusive classes and it fixes the computation of the loss value. It also changes all the tests that involve the CategoricalCrossentropy and SequenceCategoricalCrossentropy.,"I don't think it's being used in spaCy right now, but it should. And when we do, I'm proposing we work via specific versions, e.g.
loss_function = registry.get(""losses"", ""CategoricalCrossentropy.v3"")

Instead of a direct import. This will make it easier to keep the same behaviour if people want it, but to allow others to upgrade if they chose so.
It might still be that other users of Thinc are using the current CategoricalCrossentropy and would get bitten by a change in numerical values when they're logging results across training runs, or get bitten because the functionality is now more strict as Danil mentions. Then we'd be able to present the above solution to them for making sure they have access to the same functionality.
The fact that we need to change the tests here, is a huge red flag that this can be very breaking - which is why I'm proposing we lean into the framework of versioning that we've already set up anyway. Then we can keep the tests largely the same for the old functionality but write new ones for the new functionality.

Added a suggestion for handling legacy stuff in general, but I didn't exactly follow your suggestion, but let me know if you'd like that better. Maybe I did follow your suggestion though?! I dunno.
The idea is that there is the thinc.legacy as you've said. So thinc.legacy at the moment only has loss.py meaning that the thinc.loss should import from thinc.legacy.loss if it would like to run older versions. The other thing that I'm suggesting here is the when there is a function called func or class called Class then the import should be from thinc.legacy import legacy_func, LegacyClass. Then the legacy factories return the legacy_ or Legacy versions of the functions or classes defined in the module.
This is what it does atm and I call this the ""(c)import your own legacy  "" solution :D.
For tests I've made a separate module thinc.tests.legacy, which again replicates the corresponding tests and imports from thinc.legacy. In this case its just a copy of the old test_loss.py removing the tests for which legacy versions do not exist.",True,{}
explosion/thinc,https://github.com/explosion/thinc,648,2022-04-28T17:14:44Z,2022-05-02T12:06:23Z,2022-05-02T12:06:23Z,CLOSED,False,11,4,2,https://github.com/shadeMe,Suppress benign overflow warning in `Ops.sigmoid`,6,"['enhancement', 'feat / ops']",https://github.com/explosion/thinc/pull/648,https://github.com/shadeMe,1,https://github.com/explosion/thinc/pull/648,"The exp(-X) term can saturate to +inf if X is negative and large enough, raising a RuntimeWarning about the overflow. The final result will nevertheless clamp to 0.0, so it can be ignored.
High-prio, as it otherwise breaks @koaning's pretty progress bars in the terminal ","The exp(-X) term can saturate to +inf if X is negative and large enough, raising a RuntimeWarning about the overflow. The final result will nevertheless clamp to 0.0, so it can be ignored.
High-prio, as it otherwise breaks @koaning's pretty progress bars in the terminal ",True,{}
explosion/thinc,https://github.com/explosion/thinc,648,2022-04-28T17:14:44Z,2022-05-02T12:06:23Z,2022-05-02T12:06:23Z,CLOSED,False,11,4,2,https://github.com/shadeMe,Suppress benign overflow warning in `Ops.sigmoid`,6,"['enhancement', 'feat / ops']",https://github.com/explosion/thinc/pull/648,https://github.com/koaning,2,https://github.com/explosion/thinc/pull/648#issuecomment-1112484223,"The exp(-X) term can saturate to +inf if X is negative and large enough, raising a RuntimeWarning about the overflow. The final result will nevertheless clamp to 0.0, so it can be ignored.
High-prio, as it otherwise breaks @koaning's pretty progress bars in the terminal ",I mean ... they don't break. They just don't look as good  .,True,{'LAUGH': ['https://github.com/shadeMe']}
explosion/thinc,https://github.com/explosion/thinc,648,2022-04-28T17:14:44Z,2022-05-02T12:06:23Z,2022-05-02T12:06:23Z,CLOSED,False,11,4,2,https://github.com/shadeMe,Suppress benign overflow warning in `Ops.sigmoid`,6,"['enhancement', 'feat / ops']",https://github.com/explosion/thinc/pull/648,https://github.com/svlandeg,3,https://github.com/explosion/thinc/pull/648#issuecomment-1112570610,"The exp(-X) term can saturate to +inf if X is negative and large enough, raising a RuntimeWarning about the overflow. The final result will nevertheless clamp to 0.0, so it can be ignored.
High-prio, as it otherwise breaks @koaning's pretty progress bars in the terminal ","If we want to prioritize getting this fix out, we should target master and plan for a next release. Ideally we'd need to look at current open PRs as well to see whether anything should be finalized to make the release cutoff as well :-)",True,{}
explosion/thinc,https://github.com/explosion/thinc,648,2022-04-28T17:14:44Z,2022-05-02T12:06:23Z,2022-05-02T12:06:23Z,CLOSED,False,11,4,2,https://github.com/shadeMe,Suppress benign overflow warning in `Ops.sigmoid`,6,"['enhancement', 'feat / ops']",https://github.com/explosion/thinc/pull/648,https://github.com/shadeMe,4,https://github.com/explosion/thinc/pull/648#issuecomment-1112598377,"The exp(-X) term can saturate to +inf if X is negative and large enough, raising a RuntimeWarning about the overflow. The final result will nevertheless clamp to 0.0, so it can be ignored.
High-prio, as it otherwise breaks @koaning's pretty progress bars in the terminal ","Oh, the comment about the PR being high-prio was made tongue-in-cheek, to be honest  But I'll defer to the folks affected by this issue on that topic  Re-targeting this to master shouldn't be a problem, in any event.",True,{}
explosion/thinc,https://github.com/explosion/thinc,648,2022-04-28T17:14:44Z,2022-05-02T12:06:23Z,2022-05-02T12:06:23Z,CLOSED,False,11,4,2,https://github.com/shadeMe,Suppress benign overflow warning in `Ops.sigmoid`,6,"['enhancement', 'feat / ops']",https://github.com/explosion/thinc/pull/648,https://github.com/koaning,5,https://github.com/explosion/thinc/pull/648#issuecomment-1112932341,"The exp(-X) term can saturate to +inf if X is negative and large enough, raising a RuntimeWarning about the overflow. The final result will nevertheless clamp to 0.0, so it can be ignored.
High-prio, as it otherwise breaks @koaning's pretty progress bars in the terminal ","Just to clarify the issue a bit, here's a place where this appears:
E    #       LOSS TEXTC...  CATS_SCORE  CATS_MICRO_P  CATS_MICRO_R  CATS_MICRO_F  SCORE 
---  ------  -------------  ----------  ------------  ------------  ------------  ------
  0       0           8.85       48.81          5.60         15.89          8.28    0.20
/home/vincent/Development/spacy-github-issues/venv/lib/python3.8/site-packages/thinc/backends/ops.py:590: RuntimeWarning: overflow encountered in exp
  return cast(FloatsType, 1.0 / (1.0 + self.xp.exp(-X)))
  0     100         149.65       54.58          0.00          0.00          0.00    0.14
  0     200         122.79       54.60         60.00          0.17          0.33    0.29
  0     300         113.31       56.19          0.00          0.00          0.00    0.14
  0     400         121.88       58.02          0.00          0.00          0.00    0.15
  0     500         118.56       57.97          0.00          0.00          0.00    0.14

It's not a huge problem. But it is an ""awh no0o0, my pretty table!""-moment.",True,{'THUMBS_UP': ['https://github.com/svlandeg']}
explosion/thinc,https://github.com/explosion/thinc,648,2022-04-28T17:14:44Z,2022-05-02T12:06:23Z,2022-05-02T12:06:23Z,CLOSED,False,11,4,2,https://github.com/shadeMe,Suppress benign overflow warning in `Ops.sigmoid`,6,"['enhancement', 'feat / ops']",https://github.com/explosion/thinc/pull/648,https://github.com/adrianeboyd,6,https://github.com/explosion/thinc/pull/648#issuecomment-1112935568,"The exp(-X) term can saturate to +inf if X is negative and large enough, raising a RuntimeWarning about the overflow. The final result will nevertheless clamp to 0.0, so it can be ignored.
High-prio, as it otherwise breaks @koaning's pretty progress bars in the terminal ","Users have a tendency to think this indicates an error or a bug, so removing it from the output in some way seems fine.
(For the next time: can you rebase the PR branch on the target branch so it doesn't show additional commits?)",True,{'THUMBS_UP': ['https://github.com/shadeMe']}
explosion/thinc,https://github.com/explosion/thinc,648,2022-04-28T17:14:44Z,2022-05-02T12:06:23Z,2022-05-02T12:06:23Z,CLOSED,False,11,4,2,https://github.com/shadeMe,Suppress benign overflow warning in `Ops.sigmoid`,6,"['enhancement', 'feat / ops']",https://github.com/explosion/thinc/pull/648,https://github.com/shadeMe,7,https://github.com/explosion/thinc/pull/648#issuecomment-1114673507,"The exp(-X) term can saturate to +inf if X is negative and large enough, raising a RuntimeWarning about the overflow. The final result will nevertheless clamp to 0.0, so it can be ignored.
High-prio, as it otherwise breaks @koaning's pretty progress bars in the terminal ",Sanity-tested with mnist; should be okay to merge.,True,{}
explosion/thinc,https://github.com/explosion/thinc,648,2022-04-28T17:14:44Z,2022-05-02T12:06:23Z,2022-05-02T12:06:23Z,CLOSED,False,11,4,2,https://github.com/shadeMe,Suppress benign overflow warning in `Ops.sigmoid`,6,"['enhancement', 'feat / ops']",https://github.com/explosion/thinc/pull/648,https://github.com/danieldk,8,https://github.com/explosion/thinc/pull/648#issuecomment-1114753631,"The exp(-X) term can saturate to +inf if X is negative and large enough, raising a RuntimeWarning about the overflow. The final result will nevertheless clamp to 0.0, so it can be ignored.
High-prio, as it otherwise breaks @koaning's pretty progress bars in the terminal ",Could you rebase the PR on master? Also feel free to click 'Resolve' for issues that have been fixed or conversations where consensus has been reached .,True,{}
explosion/thinc,https://github.com/explosion/thinc,648,2022-04-28T17:14:44Z,2022-05-02T12:06:23Z,2022-05-02T12:06:23Z,CLOSED,False,11,4,2,https://github.com/shadeMe,Suppress benign overflow warning in `Ops.sigmoid`,6,"['enhancement', 'feat / ops']",https://github.com/explosion/thinc/pull/648,https://github.com/shadeMe,9,https://github.com/explosion/thinc/pull/648#issuecomment-1114771690,"The exp(-X) term can saturate to +inf if X is negative and large enough, raising a RuntimeWarning about the overflow. The final result will nevertheless clamp to 0.0, so it can be ignored.
High-prio, as it otherwise breaks @koaning's pretty progress bars in the terminal ","@danieldk I've opened a cleaner PR here - Couldn't fast-forward the commits from master to produce a clean rebase on this branch.
Will close this one.",True,{}
explosion/thinc,https://github.com/explosion/thinc,649,2022-05-02T12:05:49Z,2022-05-02T13:01:10Z,2022-05-02T13:04:08Z,MERGED,True,6,0,1,https://github.com/shadeMe,Clamp inputs in `Ops.sigmoid` to prevent overflow,2,"['enhancement', 'feat / ops']",https://github.com/explosion/thinc/pull/649,https://github.com/shadeMe,1,https://github.com/explosion/thinc/pull/649,Changes from PR #648 rebased onto explosion:master.,Changes from PR #648 rebased onto explosion:master.,True,{'THUMBS_UP': ['https://github.com/svlandeg']}
explosion/thinc,https://github.com/explosion/thinc,650,2022-05-02T18:16:31Z,2022-05-05T06:06:35Z,2022-05-05T06:06:40Z,MERGED,True,36,6,3,https://github.com/danieldk,Check Ops.sigmoid against PyTorch,3,"['tests', 'feat / ops']",https://github.com/explosion/thinc/pull/650,https://github.com/danieldk,1,https://github.com/explosion/thinc/pull/650,A bit of a follow up to @shadeMe's sigmoid PR: let's check the output against Torch.,A bit of a follow up to @shadeMe's sigmoid PR: let's check the output against Torch.,True,{}
explosion/thinc,https://github.com/explosion/thinc,650,2022-05-02T18:16:31Z,2022-05-05T06:06:35Z,2022-05-05T06:06:40Z,MERGED,True,36,6,3,https://github.com/danieldk,Check Ops.sigmoid against PyTorch,3,"['tests', 'feat / ops']",https://github.com/explosion/thinc/pull/650,https://github.com/danieldk,2,https://github.com/explosion/thinc/pull/650#issuecomment-1117018441,A bit of a follow up to @shadeMe's sigmoid PR: let's check the output against Torch.,@explosion-bot please test_gpu,True,"{'EYES': ['https://github.com/explosion-bot'], 'ROCKET': ['https://github.com/explosion-bot']}"
explosion/thinc,https://github.com/explosion/thinc,650,2022-05-02T18:16:31Z,2022-05-05T06:06:35Z,2022-05-05T06:06:40Z,MERGED,True,36,6,3,https://github.com/danieldk,Check Ops.sigmoid against PyTorch,3,"['tests', 'feat / ops']",https://github.com/explosion/thinc/pull/650,https://github.com/explosion-bot,3,https://github.com/explosion/thinc/pull/650#issuecomment-1117018717,A bit of a follow up to @shadeMe's sigmoid PR: let's check the output against Torch.," Successfully triggered build on Buildkite

URL: https://buildkite.com/explosion-ai/thinc-gpu-test-suite/builds/57",True,{}
explosion/thinc,https://github.com/explosion/thinc,651,2022-05-03T12:51:12Z,2022-05-03T13:35:31Z,2022-05-03T13:35:31Z,MERGED,True,1,1,1,https://github.com/richardpaulhudson,Fix type safety issue with model ID assignment,1,['bug'],https://github.com/explosion/thinc/pull/651,https://github.com/richardpaulhudson,1,https://github.com/explosion/thinc/pull/651,"It was observed that the test checking typesafe assignment of model IDs failed very occasionally. The issue fixed here is likely to have been the cause of this, although unfortunately there is no way of verifying this.","It was observed that the test checking typesafe assignment of model IDs failed very occasionally. The issue fixed here is likely to have been the cause of this, although unfortunately there is no way of verifying this.",True,"{'THUMBS_UP': ['https://github.com/svlandeg', 'https://github.com/shadeMe']}"
explosion/thinc,https://github.com/explosion/thinc,651,2022-05-03T12:51:12Z,2022-05-03T13:35:31Z,2022-05-03T13:35:31Z,MERGED,True,1,1,1,https://github.com/richardpaulhudson,Fix type safety issue with model ID assignment,1,['bug'],https://github.com/explosion/thinc/pull/651,https://github.com/richardpaulhudson,2,https://github.com/explosion/thinc/pull/651#issuecomment-1116087710,"It was observed that the test checking typesafe assignment of model IDs failed very occasionally. The issue fixed here is likely to have been the cause of this, although unfortunately there is no way of verifying this.",@explosion-bot please test_gpu,True,"{'EYES': ['https://github.com/explosion-bot'], 'ROCKET': ['https://github.com/explosion-bot']}"
explosion/thinc,https://github.com/explosion/thinc,651,2022-05-03T12:51:12Z,2022-05-03T13:35:31Z,2022-05-03T13:35:31Z,MERGED,True,1,1,1,https://github.com/richardpaulhudson,Fix type safety issue with model ID assignment,1,['bug'],https://github.com/explosion/thinc/pull/651,https://github.com/explosion-bot,3,https://github.com/explosion/thinc/pull/651#issuecomment-1116088365,"It was observed that the test checking typesafe assignment of model IDs failed very occasionally. The issue fixed here is likely to have been the cause of this, although unfortunately there is no way of verifying this."," Successfully triggered build on Buildkite

URL: https://buildkite.com/explosion-ai/thinc-gpu-test-suite/builds/55",True,{}
explosion/thinc,https://github.com/explosion/thinc,652,2022-05-03T16:30:19Z,2022-05-10T18:28:05Z,2022-05-10T18:34:19Z,MERGED,True,141,193,24,https://github.com/shadeMe,Move compatiblity-related code into a separate `compat` module,9,['enhancement'],https://github.com/explosion/thinc/pull/652,https://github.com/shadeMe,1,https://github.com/explosion/thinc/pull/652,"There are multiple places in the codebase where we attempt to import libraries inside try...expect clauses to safely determine their availability. This PR is primarily meant to replace the individual imports with code that calls into a central compat module that maintains the imports for all optional libraries.
Additional changes:

Removal of the has_cupy exports from the cupy_ops, backends and api modules (these were not explicitly exposed to the public API but used internally).
Handle the deprecation of cupy.fromDlpack in cupy >= 10.0.0.
util.set_active_gpu returns None when cupy/GPU support is unavailable. Prior to this change, the function could potentially raise an unhandled exception if the cupy library was not installed. util.set_active_gpu now raises an exception if no GPU support is detected.","There are multiple places in the codebase where we attempt to import libraries inside try...expect clauses to safely determine their availability. This PR is primarily meant to replace the individual imports with code that calls into a central compat module that maintains the imports for all optional libraries.
Additional changes:

Removal of the has_cupy exports from the cupy_ops, backends and api modules (these were not explicitly exposed to the public API but used internally).
Handle the deprecation of cupy.fromDlpack in cupy >= 10.0.0.
util.set_active_gpu returns None when cupy/GPU support is unavailable. Prior to this change, the function could potentially raise an unhandled exception if the cupy library was not installed. util.set_active_gpu now raises an exception if no GPU support is detected.",True,"{'ROCKET': ['https://github.com/svlandeg'], 'THUMBS_UP': ['https://github.com/svlandeg']}"
explosion/thinc,https://github.com/explosion/thinc,652,2022-05-03T16:30:19Z,2022-05-10T18:28:05Z,2022-05-10T18:34:19Z,MERGED,True,141,193,24,https://github.com/shadeMe,Move compatiblity-related code into a separate `compat` module,9,['enhancement'],https://github.com/explosion/thinc/pull/652,https://github.com/shadeMe,2,https://github.com/explosion/thinc/pull/652#issuecomment-1116301372,"There are multiple places in the codebase where we attempt to import libraries inside try...expect clauses to safely determine their availability. This PR is primarily meant to replace the individual imports with code that calls into a central compat module that maintains the imports for all optional libraries.
Additional changes:

Removal of the has_cupy exports from the cupy_ops, backends and api modules (these were not explicitly exposed to the public API but used internally).
Handle the deprecation of cupy.fromDlpack in cupy >= 10.0.0.
util.set_active_gpu returns None when cupy/GPU support is unavailable. Prior to this change, the function could potentially raise an unhandled exception if the cupy library was not installed. util.set_active_gpu now raises an exception if no GPU support is detected.",@explosion-bot please test_gpu,True,"{'EYES': ['https://github.com/explosion-bot'], 'ROCKET': ['https://github.com/explosion-bot']}"
explosion/thinc,https://github.com/explosion/thinc,652,2022-05-03T16:30:19Z,2022-05-10T18:28:05Z,2022-05-10T18:34:19Z,MERGED,True,141,193,24,https://github.com/shadeMe,Move compatiblity-related code into a separate `compat` module,9,['enhancement'],https://github.com/explosion/thinc/pull/652,https://github.com/explosion-bot,3,https://github.com/explosion/thinc/pull/652#issuecomment-1116301804,"There are multiple places in the codebase where we attempt to import libraries inside try...expect clauses to safely determine their availability. This PR is primarily meant to replace the individual imports with code that calls into a central compat module that maintains the imports for all optional libraries.
Additional changes:

Removal of the has_cupy exports from the cupy_ops, backends and api modules (these were not explicitly exposed to the public API but used internally).
Handle the deprecation of cupy.fromDlpack in cupy >= 10.0.0.
util.set_active_gpu returns None when cupy/GPU support is unavailable. Prior to this change, the function could potentially raise an unhandled exception if the cupy library was not installed. util.set_active_gpu now raises an exception if no GPU support is detected."," Successfully triggered build on Buildkite

URL: https://buildkite.com/explosion-ai/thinc-gpu-test-suite/builds/56",True,{}
explosion/thinc,https://github.com/explosion/thinc,652,2022-05-03T16:30:19Z,2022-05-10T18:28:05Z,2022-05-10T18:34:19Z,MERGED,True,141,193,24,https://github.com/shadeMe,Move compatiblity-related code into a separate `compat` module,9,['enhancement'],https://github.com/explosion/thinc/pull/652,https://github.com/shadeMe,4,https://github.com/explosion/thinc/pull/652#issuecomment-1121071151,"There are multiple places in the codebase where we attempt to import libraries inside try...expect clauses to safely determine their availability. This PR is primarily meant to replace the individual imports with code that calls into a central compat module that maintains the imports for all optional libraries.
Additional changes:

Removal of the has_cupy exports from the cupy_ops, backends and api modules (these were not explicitly exposed to the public API but used internally).
Handle the deprecation of cupy.fromDlpack in cupy >= 10.0.0.
util.set_active_gpu returns None when cupy/GPU support is unavailable. Prior to this change, the function could potentially raise an unhandled exception if the cupy library was not installed. util.set_active_gpu now raises an exception if no GPU support is detected.",No conflicts! Such a good feeling ,True,{'HOORAY': ['https://github.com/svlandeg']}
explosion/thinc,https://github.com/explosion/thinc,652,2022-05-03T16:30:19Z,2022-05-10T18:28:05Z,2022-05-10T18:34:19Z,MERGED,True,141,193,24,https://github.com/shadeMe,Move compatiblity-related code into a separate `compat` module,9,['enhancement'],https://github.com/explosion/thinc/pull/652,https://github.com/svlandeg,5,https://github.com/explosion/thinc/pull/652#issuecomment-1122174316,"There are multiple places in the codebase where we attempt to import libraries inside try...expect clauses to safely determine their availability. This PR is primarily meant to replace the individual imports with code that calls into a central compat module that maintains the imports for all optional libraries.
Additional changes:

Removal of the has_cupy exports from the cupy_ops, backends and api modules (these were not explicitly exposed to the public API but used internally).
Handle the deprecation of cupy.fromDlpack in cupy >= 10.0.0.
util.set_active_gpu returns None when cupy/GPU support is unavailable. Prior to this change, the function could potentially raise an unhandled exception if the cupy library was not installed. util.set_active_gpu now raises an exception if no GPU support is detected.",Just a flaky test. Let's rerun it and see what happens ;-),True,{}
explosion/thinc,https://github.com/explosion/thinc,653,2022-05-05T17:14:11Z,2022-05-06T13:33:45Z,2022-05-06T13:45:36Z,MERGED,True,31,8,2,https://github.com/shadeMe,Tests: Correctly handle GPU-resident Tensorflow tensors,2,"['bug', 'tests', 'interop / tensorflow']",https://github.com/explosion/thinc/pull/653,https://github.com/shadeMe,1,https://github.com/explosion/thinc/pull/653,"Tensorflow automatically allocates on the GPU if possible, which is something the tests below didn't correctly account for. Since the CI host machine's CUDA libraries were not correctly configured, TF silently always used the CPU thereby hiding this bug.","Tensorflow automatically allocates on the GPU if possible, which is something the tests below didn't correctly account for. Since the CI host machine's CUDA libraries were not correctly configured, TF silently always used the CPU thereby hiding this bug.",True,{'HEART': ['https://github.com/svlandeg']}
explosion/thinc,https://github.com/explosion/thinc,653,2022-05-05T17:14:11Z,2022-05-06T13:33:45Z,2022-05-06T13:45:36Z,MERGED,True,31,8,2,https://github.com/shadeMe,Tests: Correctly handle GPU-resident Tensorflow tensors,2,"['bug', 'tests', 'interop / tensorflow']",https://github.com/explosion/thinc/pull/653,https://github.com/shadeMe,2,https://github.com/explosion/thinc/pull/653#issuecomment-1118849737,"Tensorflow automatically allocates on the GPU if possible, which is something the tests below didn't correctly account for. Since the CI host machine's CUDA libraries were not correctly configured, TF silently always used the CPU thereby hiding this bug.",@explosion-bot please test_gpu,True,"{'EYES': ['https://github.com/explosion-bot'], 'ROCKET': ['https://github.com/explosion-bot']}"
explosion/thinc,https://github.com/explosion/thinc,653,2022-05-05T17:14:11Z,2022-05-06T13:33:45Z,2022-05-06T13:45:36Z,MERGED,True,31,8,2,https://github.com/shadeMe,Tests: Correctly handle GPU-resident Tensorflow tensors,2,"['bug', 'tests', 'interop / tensorflow']",https://github.com/explosion/thinc/pull/653,https://github.com/explosion-bot,3,https://github.com/explosion/thinc/pull/653#issuecomment-1118850196,"Tensorflow automatically allocates on the GPU if possible, which is something the tests below didn't correctly account for. Since the CI host machine's CUDA libraries were not correctly configured, TF silently always used the CPU thereby hiding this bug."," Successfully triggered build on Buildkite

URL: https://buildkite.com/explosion-ai/thinc-gpu-test-suite/builds/61",True,{}
explosion/thinc,https://github.com/explosion/thinc,655,2022-05-06T14:32:16Z,2022-05-09T06:06:35Z,2022-05-09T06:30:21Z,MERGED,True,39,10,5,https://github.com/svlandeg,update develop with latest from master,6,[],https://github.com/explosion/thinc/pull/655,https://github.com/svlandeg,1,https://github.com/explosion/thinc/pull/655,"[DON'T SQUASH]
Updating develop with the latest from master.","[DON'T SQUASH]
Updating develop with the latest from master.",True,{}
explosion/thinc,https://github.com/explosion/thinc,656,2022-05-09T11:20:24Z,2022-05-17T09:54:24Z,2022-05-17T09:54:58Z,MERGED,True,10,9,1,https://github.com/shadeMe,NumpyOps: Better type-casting in `asarray`,6,"['feat / types', 'feat / ops']",https://github.com/explosion/thinc/pull/656,https://github.com/shadeMe,1,https://github.com/explosion/thinc/pull/656,Handle type-casting in all cases.,Handle type-casting in all cases.,True,{'THUMBS_UP': ['https://github.com/svlandeg']}
explosion/thinc,https://github.com/explosion/thinc,656,2022-05-09T11:20:24Z,2022-05-17T09:54:24Z,2022-05-17T09:54:58Z,MERGED,True,10,9,1,https://github.com/shadeMe,NumpyOps: Better type-casting in `asarray`,6,"['feat / types', 'feat / ops']",https://github.com/explosion/thinc/pull/656,https://github.com/adrianeboyd,2,https://github.com/explosion/thinc/pull/656#issuecomment-1123578615,Handle type-casting in all cases.,"Let's make the same change removing cast_array here, too?",True,{}
explosion/thinc,https://github.com/explosion/thinc,657,2022-05-09T12:06:15Z,2022-05-09T12:36:29Z,2022-05-09T12:36:44Z,MERGED,True,2481,1264,50,https://github.com/svlandeg,copy develop over to master,28,[],https://github.com/explosion/thinc/pull/657,https://github.com/svlandeg,1,https://github.com/explosion/thinc/pull/657,"[DON'T SQUASH]
Copying everything from develop to master after which we'll retire develop for now.","[DON'T SQUASH]
Copying everything from develop to master after which we'll retire develop for now.",True,{}
explosion/thinc,https://github.com/explosion/thinc,658,2022-05-10T12:37:48Z,2022-05-17T13:29:40Z,2022-05-17T13:29:41Z,CLOSED,False,4,4,3,https://github.com/adrianeboyd,Allow blis v0.9.x,1,[],https://github.com/explosion/thinc/pull/658,https://github.com/adrianeboyd,1,https://github.com/explosion/thinc/pull/658,,,True,{}
explosion/thinc,https://github.com/explosion/thinc,658,2022-05-10T12:37:48Z,2022-05-17T13:29:40Z,2022-05-17T13:29:41Z,CLOSED,False,4,4,3,https://github.com/adrianeboyd,Allow blis v0.9.x,1,[],https://github.com/explosion/thinc/pull/658,https://github.com/adrianeboyd,2,https://github.com/explosion/thinc/pull/658#issuecomment-1122336492,,@explosion-bot please test_gpu,True,"{'EYES': ['https://github.com/explosion-bot'], 'ROCKET': ['https://github.com/explosion-bot']}"
explosion/thinc,https://github.com/explosion/thinc,658,2022-05-10T12:37:48Z,2022-05-17T13:29:40Z,2022-05-17T13:29:41Z,CLOSED,False,4,4,3,https://github.com/adrianeboyd,Allow blis v0.9.x,1,[],https://github.com/explosion/thinc/pull/658,https://github.com/explosion-bot,3,https://github.com/explosion/thinc/pull/658#issuecomment-1122337087,," Successfully triggered build on Buildkite

URL: https://buildkite.com/explosion-ai/thinc-gpu-test-suite/builds/62",True,{}
explosion/thinc,https://github.com/explosion/thinc,658,2022-05-10T12:37:48Z,2022-05-17T13:29:40Z,2022-05-17T13:29:41Z,CLOSED,False,4,4,3,https://github.com/adrianeboyd,Allow blis v0.9.x,1,[],https://github.com/explosion/thinc/pull/658,https://github.com/svlandeg,4,https://github.com/explosion/thinc/pull/658#issuecomment-1128872674,,#643 got merged so closing this one :-),True,{}
explosion/thinc,https://github.com/explosion/thinc,659,2022-05-10T14:52:27Z,2022-05-18T17:22:45Z,2022-05-18T17:22:46Z,MERGED,True,68,4,2,https://github.com/richardpaulhudson,Fix model.copy() bug where layer used more than once,4,"['bug', 'serialization']",https://github.com/explosion/thinc/pull/659,https://github.com/richardpaulhudson,1,https://github.com/explosion/thinc/pull/659,"In the preexisting version of Thinc, model.copy() generated separate copies of layers and shims that were used at more than one place in the model structure. This led to inconsistencies e.g. between the outputs of to_dict() for the original and the copied models.
This PR creates model copies that preserve object identity between layers.","In the preexisting version of Thinc, model.copy() generated separate copies of layers and shims that were used at more than one place in the model structure. This led to inconsistencies e.g. between the outputs of to_dict() for the original and the copied models.
This PR creates model copies that preserve object identity between layers.",True,{}
explosion/thinc,https://github.com/explosion/thinc,659,2022-05-10T14:52:27Z,2022-05-18T17:22:45Z,2022-05-18T17:22:46Z,MERGED,True,68,4,2,https://github.com/richardpaulhudson,Fix model.copy() bug where layer used more than once,4,"['bug', 'serialization']",https://github.com/explosion/thinc/pull/659,https://github.com/richardpaulhudson,2,https://github.com/explosion/thinc/pull/659#issuecomment-1122504089,"In the preexisting version of Thinc, model.copy() generated separate copies of layers and shims that were used at more than one place in the model structure. This led to inconsistencies e.g. between the outputs of to_dict() for the original and the copied models.
This PR creates model copies that preserve object identity between layers.",@explosion-bot please test_gpu,True,"{'EYES': ['https://github.com/explosion-bot'], 'ROCKET': ['https://github.com/explosion-bot']}"
explosion/thinc,https://github.com/explosion/thinc,659,2022-05-10T14:52:27Z,2022-05-18T17:22:45Z,2022-05-18T17:22:46Z,MERGED,True,68,4,2,https://github.com/richardpaulhudson,Fix model.copy() bug where layer used more than once,4,"['bug', 'serialization']",https://github.com/explosion/thinc/pull/659,https://github.com/explosion-bot,3,https://github.com/explosion/thinc/pull/659#issuecomment-1122504708,"In the preexisting version of Thinc, model.copy() generated separate copies of layers and shims that were used at more than one place in the model structure. This led to inconsistencies e.g. between the outputs of to_dict() for the original and the copied models.
This PR creates model copies that preserve object identity between layers."," Successfully triggered build on Buildkite

URL: https://buildkite.com/explosion-ai/thinc-gpu-test-suite/builds/63",True,{}
explosion/thinc,https://github.com/explosion/thinc,659,2022-05-10T14:52:27Z,2022-05-18T17:22:45Z,2022-05-18T17:22:46Z,MERGED,True,68,4,2,https://github.com/richardpaulhudson,Fix model.copy() bug where layer used more than once,4,"['bug', 'serialization']",https://github.com/explosion/thinc/pull/659,https://github.com/richardpaulhudson,4,https://github.com/explosion/thinc/pull/659#issuecomment-1122550758,"In the preexisting version of Thinc, model.copy() generated separate copies of layers and shims that were used at more than one place in the model structure. This led to inconsistencies e.g. between the outputs of to_dict() for the original and the copied models.
This PR creates model copies that preserve object identity between layers.",@explosion-bot please test_gpu,True,"{'EYES': ['https://github.com/explosion-bot'], 'ROCKET': ['https://github.com/explosion-bot']}"
explosion/thinc,https://github.com/explosion/thinc,659,2022-05-10T14:52:27Z,2022-05-18T17:22:45Z,2022-05-18T17:22:46Z,MERGED,True,68,4,2,https://github.com/richardpaulhudson,Fix model.copy() bug where layer used more than once,4,"['bug', 'serialization']",https://github.com/explosion/thinc/pull/659,https://github.com/explosion-bot,5,https://github.com/explosion/thinc/pull/659#issuecomment-1122551515,"In the preexisting version of Thinc, model.copy() generated separate copies of layers and shims that were used at more than one place in the model structure. This led to inconsistencies e.g. between the outputs of to_dict() for the original and the copied models.
This PR creates model copies that preserve object identity between layers."," Successfully triggered build on Buildkite

URL: https://buildkite.com/explosion-ai/thinc-gpu-test-suite/builds/64",True,{}
explosion/thinc,https://github.com/explosion/thinc,659,2022-05-10T14:52:27Z,2022-05-18T17:22:45Z,2022-05-18T17:22:46Z,MERGED,True,68,4,2,https://github.com/richardpaulhudson,Fix model.copy() bug where layer used more than once,4,"['bug', 'serialization']",https://github.com/explosion/thinc/pull/659,https://github.com/danieldk,6,https://github.com/explosion/thinc/pull/659#issuecomment-1130285532,"In the preexisting version of Thinc, model.copy() generated separate copies of layers and shims that were used at more than one place in the model structure. This led to inconsistencies e.g. between the outputs of to_dict() for the original and the copied models.
This PR creates model copies that preserve object identity between layers.","Merging because this is a strict improvement over the current behavior. If we want to change behavior for shims, it can be an a later PR.",True,{}
explosion/thinc,https://github.com/explosion/thinc,660,2022-05-11T10:22:41Z,2022-05-11T16:29:21Z,2022-05-11T16:29:21Z,MERGED,True,1,0,1,https://github.com/danieldk,Fix to make 8.0.x pass tests,1,[],https://github.com/explosion/thinc/pull/660,https://github.com/danieldk,1,https://github.com/explosion/thinc/pull/660,Otherwise is_torch_array does not function correctly.,Otherwise is_torch_array does not function correctly.,True,{}
explosion/thinc,https://github.com/explosion/thinc,661,2022-05-11T10:46:31Z,2022-05-17T09:53:19Z,2022-05-17T09:55:05Z,MERGED,True,4,13,1,https://github.com/shadeMe,`CupyOps`: Simplify `asarray`,4,"['enhancement', 'feat / ops']",https://github.com/explosion/thinc/pull/661,https://github.com/shadeMe,1,https://github.com/explosion/thinc/pull/661,,,True,{'THUMBS_UP': ['https://github.com/svlandeg']}
explosion/thinc,https://github.com/explosion/thinc,661,2022-05-11T10:46:31Z,2022-05-17T09:53:19Z,2022-05-17T09:55:05Z,MERGED,True,4,13,1,https://github.com/shadeMe,`CupyOps`: Simplify `asarray`,4,"['enhancement', 'feat / ops']",https://github.com/explosion/thinc/pull/661,https://github.com/adrianeboyd,2,https://github.com/explosion/thinc/pull/661#issuecomment-1123568403,,"Looking at the numpy/cupy docs again, I think you can avoid cast_array entirely by using:
array = array.astype(dtype=dtype, copy=False)",True,{'THUMBS_UP': ['https://github.com/shadeMe']}
explosion/thinc,https://github.com/explosion/thinc,662,2022-05-11T11:24:27Z,2022-05-18T10:20:50Z,2022-05-18T10:26:19Z,MERGED,True,120,38,7,https://github.com/shadeMe,Backport fixes from `master` to `v8.0.x`,7,[],https://github.com/explosion/thinc/pull/662,https://github.com/shadeMe,1,https://github.com/explosion/thinc/pull/662,"Requires
The following PRs need to be merged into master and then cherry-picked into this branch.

#661
(Maybe?) #656","Requires
The following PRs need to be merged into master and then cherry-picked into this branch.

#661
(Maybe?) #656",True,"{'THUMBS_UP': ['https://github.com/svlandeg', 'https://github.com/danieldk']}"
explosion/thinc,https://github.com/explosion/thinc,662,2022-05-11T11:24:27Z,2022-05-18T10:20:50Z,2022-05-18T10:26:19Z,MERGED,True,120,38,7,https://github.com/shadeMe,Backport fixes from `master` to `v8.0.x`,7,[],https://github.com/explosion/thinc/pull/662,https://github.com/svlandeg,2,https://github.com/explosion/thinc/pull/662#issuecomment-1128680729,"Requires
The following PRs need to be merged into master and then cherry-picked into this branch.

#661
(Maybe?) #656",I suppose this can be taken out of WIP?,True,{}
explosion/thinc,https://github.com/explosion/thinc,662,2022-05-11T11:24:27Z,2022-05-18T10:20:50Z,2022-05-18T10:26:19Z,MERGED,True,120,38,7,https://github.com/shadeMe,Backport fixes from `master` to `v8.0.x`,7,[],https://github.com/explosion/thinc/pull/662,https://github.com/shadeMe,3,https://github.com/explosion/thinc/pull/662#issuecomment-1128758696,"Requires
The following PRs need to be merged into master and then cherry-picked into this branch.

#661
(Maybe?) #656",@explosion-bot please test_gpu,True,"{'EYES': ['https://github.com/explosion-bot'], 'ROCKET': ['https://github.com/explosion-bot']}"
explosion/thinc,https://github.com/explosion/thinc,662,2022-05-11T11:24:27Z,2022-05-18T10:20:50Z,2022-05-18T10:26:19Z,MERGED,True,120,38,7,https://github.com/shadeMe,Backport fixes from `master` to `v8.0.x`,7,[],https://github.com/explosion/thinc/pull/662,https://github.com/explosion-bot,4,https://github.com/explosion/thinc/pull/662#issuecomment-1128759161,"Requires
The following PRs need to be merged into master and then cherry-picked into this branch.

#661
(Maybe?) #656"," Successfully triggered build on Buildkite

URL: https://buildkite.com/explosion-ai/thinc-gpu-test-suite/builds/65",True,{}
explosion/thinc,https://github.com/explosion/thinc,662,2022-05-11T11:24:27Z,2022-05-18T10:20:50Z,2022-05-18T10:26:19Z,MERGED,True,120,38,7,https://github.com/shadeMe,Backport fixes from `master` to `v8.0.x`,7,[],https://github.com/explosion/thinc/pull/662,https://github.com/danieldk,5,https://github.com/explosion/thinc/pull/662#issuecomment-1129643931,"Requires
The following PRs need to be merged into master and then cherry-picked into this branch.

#661
(Maybe?) #656",@explosion-bot please test_gpu,True,"{'EYES': ['https://github.com/explosion-bot'], 'ROCKET': ['https://github.com/explosion-bot']}"
explosion/thinc,https://github.com/explosion/thinc,662,2022-05-11T11:24:27Z,2022-05-18T10:20:50Z,2022-05-18T10:26:19Z,MERGED,True,120,38,7,https://github.com/shadeMe,Backport fixes from `master` to `v8.0.x`,7,[],https://github.com/explosion/thinc/pull/662,https://github.com/explosion-bot,6,https://github.com/explosion/thinc/pull/662#issuecomment-1129644303,"Requires
The following PRs need to be merged into master and then cherry-picked into this branch.

#661
(Maybe?) #656"," Successfully triggered build on Buildkite

URL: https://buildkite.com/explosion-ai/thinc-gpu-test-suite/builds/66",True,{}
explosion/thinc,https://github.com/explosion/thinc,662,2022-05-11T11:24:27Z,2022-05-18T10:20:50Z,2022-05-18T10:26:19Z,MERGED,True,120,38,7,https://github.com/shadeMe,Backport fixes from `master` to `v8.0.x`,7,[],https://github.com/explosion/thinc/pull/662,https://github.com/danieldk,7,https://github.com/explosion/thinc/pull/662#issuecomment-1129644865,"Requires
The following PRs need to be merged into master and then cherry-picked into this branch.

#661
(Maybe?) #656",Added #664.,True,{}
explosion/thinc,https://github.com/explosion/thinc,663,2022-05-11T11:50:26Z,2022-05-12T07:36:46Z,2022-05-12T07:36:46Z,MERGED,True,2,1,1,https://github.com/danieldk,Fix a unit test in the PyTorch wrapper,3,[],https://github.com/explosion/thinc/pull/663,https://github.com/danieldk,1,https://github.com/explosion/thinc/pull/663,"This test checked whether the allocator was set to the PyTorch allocator when the PyTorch shim is used. However, this is not the case when PyTorch is installed, but CuPy isn't, so the test would fail. Since this test relies on CuPy, disable it when CuPy is not available.","This test checked whether the allocator was set to the PyTorch allocator when the PyTorch shim is used. However, this is not the case when PyTorch is installed, but CuPy isn't, so the test would fail. Since this test relies on CuPy, disable it when CuPy is not available.",True,{}
explosion/thinc,https://github.com/explosion/thinc,663,2022-05-11T11:50:26Z,2022-05-12T07:36:46Z,2022-05-12T07:36:46Z,MERGED,True,2,1,1,https://github.com/danieldk,Fix a unit test in the PyTorch wrapper,3,[],https://github.com/explosion/thinc/pull/663,https://github.com/danieldk,2,https://github.com/explosion/thinc/pull/663#issuecomment-1123649180,"This test checked whether the allocator was set to the PyTorch allocator when the PyTorch shim is used. However, this is not the case when PyTorch is installed, but CuPy isn't, so the test would fail. Since this test relies on CuPy, disable it when CuPy is not available.","@adrianeboyd I think you added the pool assertion, so you are probably the best reviewer for this PR.",True,{}
explosion/thinc,https://github.com/explosion/thinc,664,2022-05-13T09:20:23Z,2022-05-17T12:53:48Z,2022-05-17T12:53:48Z,MERGED,True,43,2,4,https://github.com/danieldk,Fix out-of-bounds writes in NumpyOps/CupyOps,3,"['enhancement', 'feat / ops']",https://github.com/explosion/thinc/pull/664,https://github.com/danieldk,1,https://github.com/explosion/thinc/pull/664,"Using {CupyOps,NumpyOps}.adam with incompatible shapes for weights,
gradients, or moments resulted in out-of-bound writes.
Using NumpyOps.adam with non-float32 arrays resulted filling arrays
with incorrect data.","Using {CupyOps,NumpyOps}.adam with incompatible shapes for weights,
gradients, or moments resulted in out-of-bound writes.
Using NumpyOps.adam with non-float32 arrays resulted filling arrays
with incorrect data.",True,{'EYES': ['https://github.com/svlandeg']}
explosion/thinc,https://github.com/explosion/thinc,664,2022-05-13T09:20:23Z,2022-05-17T12:53:48Z,2022-05-17T12:53:48Z,MERGED,True,43,2,4,https://github.com/danieldk,Fix out-of-bounds writes in NumpyOps/CupyOps,3,"['enhancement', 'feat / ops']",https://github.com/explosion/thinc/pull/664,https://github.com/danieldk,2,https://github.com/explosion/thinc/pull/664#issuecomment-1128800277,"Using {CupyOps,NumpyOps}.adam with incompatible shapes for weights,
gradients, or moments resulted in out-of-bound writes.
Using NumpyOps.adam with non-float32 arrays resulted filling arrays
with incorrect data.","This might be breaking upstream code, but then it probably should?

Yeah, we want it to break, since overwriting memory is worse. No issues in spaCy though.",True,{'THUMBS_UP': ['https://github.com/svlandeg']}
explosion/thinc,https://github.com/explosion/thinc,666,2022-05-17T16:46:57Z,2022-05-18T10:32:08Z,2022-05-18T10:32:11Z,MERGED,True,1,1,1,https://github.com/danieldk,Set version to v8.1.0.dev0,1,[],https://github.com/explosion/thinc/pull/666,https://github.com/danieldk,1,https://github.com/explosion/thinc/pull/666,"I thought it would be a good idea to update the version in master to
distinguish it from the v8.0.x branch.","I thought it would be a good idea to update the version in master to
distinguish it from the v8.0.x branch.",True,{'THUMBS_UP': ['https://github.com/svlandeg']}
explosion/thinc,https://github.com/explosion/thinc,667,2022-05-18T07:35:09Z,,2022-05-24T13:58:46Z,OPEN,False,17,1,1,https://github.com/danieldk,CI: test lower-bound PyTorch version with Python 3.6,3,"['tests', 'interop / pytorch']",https://github.com/explosion/thinc/pull/667,https://github.com/danieldk,1,https://github.com/explosion/thinc/pull/667,Also use PyTorch 1.11.0 for other PyTorch versions.,Also use PyTorch 1.11.0 for other PyTorch versions.,True,{}
explosion/thinc,https://github.com/explosion/thinc,668,2022-05-18T10:58:19Z,2022-05-18T11:53:34Z,2022-05-18T11:53:35Z,MERGED,True,2,1,1,https://github.com/danieldk,Backport #663 to v8.0.x,1,[],https://github.com/explosion/thinc/pull/668,https://github.com/danieldk,1,https://github.com/explosion/thinc/pull/668,"Backport #663 to v8.0.x:
This test checked whether the allocator was set to the PyTorch allocator
when the PyTorch shim is used. However, this is not the case when
PyTorch is installed, but CuPy isn't, so the test would fail. Since this
test relies on CuPy, disable it when CuPy is not available.","Backport #663 to v8.0.x:
This test checked whether the allocator was set to the PyTorch allocator
when the PyTorch shim is used. However, this is not the case when
PyTorch is installed, but CuPy isn't, so the test would fail. Since this
test relies on CuPy, disable it when CuPy is not available.",True,{}
explosion/thinc,https://github.com/explosion/thinc,668,2022-05-18T10:58:19Z,2022-05-18T11:53:34Z,2022-05-18T11:53:35Z,MERGED,True,2,1,1,https://github.com/danieldk,Backport #663 to v8.0.x,1,[],https://github.com/explosion/thinc/pull/668,https://github.com/danieldk,2,https://github.com/explosion/thinc/pull/668#issuecomment-1129866459,"Backport #663 to v8.0.x:
This test checked whether the allocator was set to the PyTorch allocator
when the PyTorch shim is used. However, this is not the case when
PyTorch is installed, but CuPy isn't, so the test would fail. Since this
test relies on CuPy, disable it when CuPy is not available.",@explosion-bot please test_gpu,True,"{'EYES': ['https://github.com/explosion-bot'], 'ROCKET': ['https://github.com/explosion-bot']}"
explosion/thinc,https://github.com/explosion/thinc,668,2022-05-18T10:58:19Z,2022-05-18T11:53:34Z,2022-05-18T11:53:35Z,MERGED,True,2,1,1,https://github.com/danieldk,Backport #663 to v8.0.x,1,[],https://github.com/explosion/thinc/pull/668,https://github.com/explosion-bot,3,https://github.com/explosion/thinc/pull/668#issuecomment-1129866842,"Backport #663 to v8.0.x:
This test checked whether the allocator was set to the PyTorch allocator
when the PyTorch shim is used. However, this is not the case when
PyTorch is installed, but CuPy isn't, so the test would fail. Since this
test relies on CuPy, disable it when CuPy is not available."," Successfully triggered build on Buildkite

URL: https://buildkite.com/explosion-ai/thinc-gpu-test-suite/builds/67",True,{}
explosion/thinc,https://github.com/explosion/thinc,669,2022-05-18T15:31:10Z,2022-05-19T05:28:56Z,2022-05-19T05:29:00Z,MERGED,True,1,1,1,https://github.com/danieldk,Set version to 8.0.16,1,[],https://github.com/explosion/thinc/pull/669,https://github.com/danieldk,1,https://github.com/explosion/thinc/pull/669,,,True,{}
explosion/thinc,https://github.com/explosion/thinc,670,2022-05-19T09:09:19Z,2022-05-19T09:49:53Z,2022-05-19T10:04:48Z,MERGED,True,6,1,1,https://github.com/shadeMe,Fix for `--slow` CI builds failing,1,[],https://github.com/explosion/thinc/pull/670,https://github.com/shadeMe,1,https://github.com/explosion/thinc/pull/670,Ported from spacy's conftest.py.,Ported from spacy's conftest.py.,True,{}
explosion/thinc,https://github.com/explosion/thinc,671,2022-05-20T09:17:00Z,2022-05-24T10:29:07Z,2022-05-24T10:29:10Z,MERGED,True,21,12,5,https://github.com/shadeMe,Fixes for slow tests,4,['tests'],https://github.com/explosion/thinc/pull/671,https://github.com/shadeMe,1,https://github.com/explosion/thinc/pull/671,"Will leave this in draft mode until explosionbot is updated to support the Buildkite thinc slow test suite.
Requirements

explosion/explosion-bot#3
#672","Will leave this in draft mode until explosionbot is updated to support the Buildkite thinc slow test suite.
Requirements

explosion/explosion-bot#3
#672",True,{'HOORAY': ['https://github.com/svlandeg']}
explosion/thinc,https://github.com/explosion/thinc,671,2022-05-20T09:17:00Z,2022-05-24T10:29:07Z,2022-05-24T10:29:10Z,MERGED,True,21,12,5,https://github.com/shadeMe,Fixes for slow tests,4,['tests'],https://github.com/explosion/thinc/pull/671,https://github.com/svlandeg,2,https://github.com/explosion/thinc/pull/671#issuecomment-1133048702,"Will leave this in draft mode until explosionbot is updated to support the Buildkite thinc slow test suite.
Requirements

explosion/explosion-bot#3
#672",Ready to test!,True,{'HOORAY': ['https://github.com/shadeMe']}
explosion/thinc,https://github.com/explosion/thinc,671,2022-05-20T09:17:00Z,2022-05-24T10:29:07Z,2022-05-24T10:29:10Z,MERGED,True,21,12,5,https://github.com/shadeMe,Fixes for slow tests,4,['tests'],https://github.com/explosion/thinc/pull/671,https://github.com/shadeMe,3,https://github.com/explosion/thinc/pull/671#issuecomment-1133086233,"Will leave this in draft mode until explosionbot is updated to support the Buildkite thinc slow test suite.
Requirements

explosion/explosion-bot#3
#672","Hmm, the Buildkite build is currently failing for some reason. I'll take a look.",True,{}
explosion/thinc,https://github.com/explosion/thinc,671,2022-05-20T09:17:00Z,2022-05-24T10:29:07Z,2022-05-24T10:29:10Z,MERGED,True,21,12,5,https://github.com/shadeMe,Fixes for slow tests,4,['tests'],https://github.com/explosion/thinc/pull/671,https://github.com/shadeMe,4,https://github.com/explosion/thinc/pull/671#issuecomment-1134765424,"Will leave this in draft mode until explosionbot is updated to support the Buildkite thinc slow test suite.
Requirements

explosion/explosion-bot#3
#672",@explosion-bot please test_slow,True,"{'EYES': ['https://github.com/explosion-bot'], 'ROCKET': ['https://github.com/explosion-bot']}"
explosion/thinc,https://github.com/explosion/thinc,671,2022-05-20T09:17:00Z,2022-05-24T10:29:07Z,2022-05-24T10:29:10Z,MERGED,True,21,12,5,https://github.com/shadeMe,Fixes for slow tests,4,['tests'],https://github.com/explosion/thinc/pull/671,https://github.com/shadeMe,5,https://github.com/explosion/thinc/pull/671#issuecomment-1134765588,"Will leave this in draft mode until explosionbot is updated to support the Buildkite thinc slow test suite.
Requirements

explosion/explosion-bot#3
#672",@explosion-bot please test_gpu,True,"{'EYES': ['https://github.com/explosion-bot'], 'ROCKET': ['https://github.com/explosion-bot']}"
explosion/thinc,https://github.com/explosion/thinc,671,2022-05-20T09:17:00Z,2022-05-24T10:29:07Z,2022-05-24T10:29:10Z,MERGED,True,21,12,5,https://github.com/shadeMe,Fixes for slow tests,4,['tests'],https://github.com/explosion/thinc/pull/671,https://github.com/explosion-bot,6,https://github.com/explosion/thinc/pull/671#issuecomment-1134765913,"Will leave this in draft mode until explosionbot is updated to support the Buildkite thinc slow test suite.
Requirements

explosion/explosion-bot#3
#672"," Successfully triggered build on Buildkite

URL: https://buildkite.com/explosion-ai/thinc-slow-tests/builds/21",True,{}
explosion/thinc,https://github.com/explosion/thinc,671,2022-05-20T09:17:00Z,2022-05-24T10:29:07Z,2022-05-24T10:29:10Z,MERGED,True,21,12,5,https://github.com/shadeMe,Fixes for slow tests,4,['tests'],https://github.com/explosion/thinc/pull/671,https://github.com/explosion-bot,7,https://github.com/explosion/thinc/pull/671#issuecomment-1134766133,"Will leave this in draft mode until explosionbot is updated to support the Buildkite thinc slow test suite.
Requirements

explosion/explosion-bot#3
#672"," Successfully triggered build on Buildkite

URL: https://buildkite.com/explosion-ai/thinc-gpu-test-suite/builds/69",True,{}
explosion/thinc,https://github.com/explosion/thinc,671,2022-05-20T09:17:00Z,2022-05-24T10:29:07Z,2022-05-24T10:29:10Z,MERGED,True,21,12,5,https://github.com/shadeMe,Fixes for slow tests,4,['tests'],https://github.com/explosion/thinc/pull/671,https://github.com/shadeMe,8,https://github.com/explosion/thinc/pull/671#issuecomment-1134786307,"Will leave this in draft mode until explosionbot is updated to support the Buildkite thinc slow test suite.
Requirements

explosion/explosion-bot#3
#672",Now passes both slow and GPU tests.,True,{'HOORAY': ['https://github.com/svlandeg']}
explosion/thinc,https://github.com/explosion/thinc,671,2022-05-20T09:17:00Z,2022-05-24T10:29:07Z,2022-05-24T10:29:10Z,MERGED,True,21,12,5,https://github.com/shadeMe,Fixes for slow tests,4,['tests'],https://github.com/explosion/thinc/pull/671,https://github.com/svlandeg,9,https://github.com/explosion/thinc/pull/671#issuecomment-1135000196,"Will leave this in draft mode until explosionbot is updated to support the Buildkite thinc slow test suite.
Requirements

explosion/explosion-bot#3
#672",What about when running the slow tests on GPU? (I think we want to add this command to both spacy and thinc),True,{'THUMBS_UP': ['https://github.com/shadeMe']}
explosion/thinc,https://github.com/explosion/thinc,671,2022-05-20T09:17:00Z,2022-05-24T10:29:07Z,2022-05-24T10:29:10Z,MERGED,True,21,12,5,https://github.com/shadeMe,Fixes for slow tests,4,['tests'],https://github.com/explosion/thinc/pull/671,https://github.com/shadeMe,10,https://github.com/explosion/thinc/pull/671#issuecomment-1135640882,"Will leave this in draft mode until explosionbot is updated to support the Buildkite thinc slow test suite.
Requirements

explosion/explosion-bot#3
#672","@svlandeg I'll open a new, separate PR for test_slow_gpu.",True,{'THUMBS_UP': ['https://github.com/svlandeg']}
explosion/thinc,https://github.com/explosion/thinc,672,2022-05-20T09:28:08Z,2022-05-20T15:39:12Z,2022-05-20T15:39:12Z,MERGED,True,1,1,1,https://github.com/shadeMe,Add support for bot-invoked slow tests,1,['tests'],https://github.com/explosion/thinc/pull/672,https://github.com/shadeMe,1,https://github.com/explosion/thinc/pull/672,,,True,{}
explosion/thinc,https://github.com/explosion/thinc,673,2022-05-20T11:30:34Z,2022-05-20T13:33:52Z,2022-05-20T13:33:53Z,MERGED,True,45,0,2,https://github.com/shadeMe,Auto-format code with `black` + Pin `black` requirement,3,[],https://github.com/explosion/thinc/pull/673,https://github.com/shadeMe,1,https://github.com/explosion/thinc/pull/673,Ported from spaCy. Also adds the black requirement from the former.,Ported from spaCy. Also adds the black requirement from the former.,True,"{'HOORAY': ['https://github.com/danieldk', 'https://github.com/svlandeg']}"
explosion/thinc,https://github.com/explosion/thinc,673,2022-05-20T11:30:34Z,2022-05-20T13:33:52Z,2022-05-20T13:33:53Z,MERGED,True,45,0,2,https://github.com/shadeMe,Auto-format code with `black` + Pin `black` requirement,3,[],https://github.com/explosion/thinc/pull/673,https://github.com/danieldk,2,https://github.com/explosion/thinc/pull/673#issuecomment-1132801213,Ported from spaCy. Also adds the black requirement from the former.,"There was also some discussion to add black version requirements, like in spaCy:
https://github.com/explosion/spaCy/blob/a82ec56aae830c1ef4b57af9c0faec1aa7170789/requirements.txt#L38
Maybe it's good to that in this PR as well, so that we are not only auto-formatting, but also all using the correct version?",True,"{'THUMBS_UP': ['https://github.com/shadeMe', 'https://github.com/svlandeg']}"
explosion/thinc,https://github.com/explosion/thinc,674,2022-05-20T15:41:17Z,2022-05-24T14:50:42Z,2022-05-24T14:50:47Z,MERGED,True,93,43,6,https://github.com/danieldk,Remove use of `torch.set_default_tensor_type`,4,"['enhancement', 'feat / shims', 'interop / pytorch']",https://github.com/explosion/thinc/pull/674,https://github.com/danieldk,1,https://github.com/explosion/thinc/pull/674,"Initial remark: this PR doesn't implement all the functionality that I am aiming at yet (e.g. support other than cpu and cuda) devices. I want to split this into multiple PRs to make sure that it is all reviewable. The spacy-transformers tests pass on the GPU and after this PR, training and evaluating a transformer works.

This PR removes use of torch.set_default_tensor_type. There are various reasons why we should probably move away from using this function:

Upstream will deprecate and remove it: pytorch/pytorch#53124
We cannot use this mechanism for other devices than CPU/CUDA, such as Metal Performance Shaders.
It offers little flexibility in allocating Torch models on different devices.

This PR makes PyTorchWrapper/PyTorchShim flexible in terms of the devices it can use. Both classes add a device argument to their constructors that takes a torch.device instance. The shim ensures that the model is on the given device. The wrapper ensures that input tensors are on the correct device, by calling xp2torch with the new device keyword argument.
Even though this approach offers more flexibility, as a default we want to use the cpu device when NumpyOps is used and cuda:N when CupyOps is used. In order to do so, this PR also adds a new function get_torch_default_device that returns the correct device for the currently active Ops. PyTorchWrapper/PyTorchShim/xp2torch use this function when None is given as the device to fall back on this default, mimicking the behavior from before this PR.","Initial remark: this PR doesn't implement all the functionality that I am aiming at yet (e.g. support other than cpu and cuda) devices. I want to split this into multiple PRs to make sure that it is all reviewable. The spacy-transformers tests pass on the GPU and after this PR, training and evaluating a transformer works.

This PR removes use of torch.set_default_tensor_type. There are various reasons why we should probably move away from using this function:

Upstream will deprecate and remove it: pytorch/pytorch#53124
We cannot use this mechanism for other devices than CPU/CUDA, such as Metal Performance Shaders.
It offers little flexibility in allocating Torch models on different devices.

This PR makes PyTorchWrapper/PyTorchShim flexible in terms of the devices it can use. Both classes add a device argument to their constructors that takes a torch.device instance. The shim ensures that the model is on the given device. The wrapper ensures that input tensors are on the correct device, by calling xp2torch with the new device keyword argument.
Even though this approach offers more flexibility, as a default we want to use the cpu device when NumpyOps is used and cuda:N when CupyOps is used. In order to do so, this PR also adds a new function get_torch_default_device that returns the correct device for the currently active Ops. PyTorchWrapper/PyTorchShim/xp2torch use this function when None is given as the device to fall back on this default, mimicking the behavior from before this PR.",True,"{'ROCKET': ['https://github.com/svlandeg'], 'THUMBS_UP': ['https://github.com/svlandeg']}"
explosion/thinc,https://github.com/explosion/thinc,674,2022-05-20T15:41:17Z,2022-05-24T14:50:42Z,2022-05-24T14:50:47Z,MERGED,True,93,43,6,https://github.com/danieldk,Remove use of `torch.set_default_tensor_type`,4,"['enhancement', 'feat / shims', 'interop / pytorch']",https://github.com/explosion/thinc/pull/674,https://github.com/svlandeg,2,https://github.com/explosion/thinc/pull/674#issuecomment-1135968699,"Initial remark: this PR doesn't implement all the functionality that I am aiming at yet (e.g. support other than cpu and cuda) devices. I want to split this into multiple PRs to make sure that it is all reviewable. The spacy-transformers tests pass on the GPU and after this PR, training and evaluating a transformer works.

This PR removes use of torch.set_default_tensor_type. There are various reasons why we should probably move away from using this function:

Upstream will deprecate and remove it: pytorch/pytorch#53124
We cannot use this mechanism for other devices than CPU/CUDA, such as Metal Performance Shaders.
It offers little flexibility in allocating Torch models on different devices.

This PR makes PyTorchWrapper/PyTorchShim flexible in terms of the devices it can use. Both classes add a device argument to their constructors that takes a torch.device instance. The shim ensures that the model is on the given device. The wrapper ensures that input tensors are on the correct device, by calling xp2torch with the new device keyword argument.
Even though this approach offers more flexibility, as a default we want to use the cpu device when NumpyOps is used and cuda:N when CupyOps is used. In order to do so, this PR also adds a new function get_torch_default_device that returns the correct device for the currently active Ops. PyTorchWrapper/PyTorchShim/xp2torch use this function when None is given as the device to fall back on this default, mimicking the behavior from before this PR.",Also might I add that I really appreciate these kind of detailed PR descriptions! :-),True,"{'ROCKET': ['https://github.com/danieldk'], 'THUMBS_UP': ['https://github.com/shadeMe']}"
explosion/thinc,https://github.com/explosion/thinc,674,2022-05-20T15:41:17Z,2022-05-24T14:50:42Z,2022-05-24T14:50:47Z,MERGED,True,93,43,6,https://github.com/danieldk,Remove use of `torch.set_default_tensor_type`,4,"['enhancement', 'feat / shims', 'interop / pytorch']",https://github.com/explosion/thinc/pull/674,https://github.com/danieldk,3,https://github.com/explosion/thinc/pull/674#issuecomment-1136030689,"Initial remark: this PR doesn't implement all the functionality that I am aiming at yet (e.g. support other than cpu and cuda) devices. I want to split this into multiple PRs to make sure that it is all reviewable. The spacy-transformers tests pass on the GPU and after this PR, training and evaluating a transformer works.

This PR removes use of torch.set_default_tensor_type. There are various reasons why we should probably move away from using this function:

Upstream will deprecate and remove it: pytorch/pytorch#53124
We cannot use this mechanism for other devices than CPU/CUDA, such as Metal Performance Shaders.
It offers little flexibility in allocating Torch models on different devices.

This PR makes PyTorchWrapper/PyTorchShim flexible in terms of the devices it can use. Both classes add a device argument to their constructors that takes a torch.device instance. The shim ensures that the model is on the given device. The wrapper ensures that input tensors are on the correct device, by calling xp2torch with the new device keyword argument.
Even though this approach offers more flexibility, as a default we want to use the cpu device when NumpyOps is used and cuda:N when CupyOps is used. In order to do so, this PR also adds a new function get_torch_default_device that returns the correct device for the currently active Ops. PyTorchWrapper/PyTorchShim/xp2torch use this function when None is given as the device to fall back on this default, mimicking the behavior from before this PR.","Cool, I'll merge it then :), it's on a branch anyway.",True,{}
explosion/thinc,https://github.com/explosion/thinc,675,2022-05-23T10:31:55Z,2022-05-24T07:55:10Z,2022-05-24T07:55:13Z,MERGED,True,16,23,4,https://github.com/danieldk,Replace use of gpu_is_available with has_cupy_gpu,4,['enhancement'],https://github.com/explosion/thinc/pull/675,https://github.com/danieldk,1,https://github.com/explosion/thinc/pull/675,"This PR is in preparation of better non-CUDA device support. Once we support non-CUDA GPUs, there may be GPUs available that are not 'CuPy GPUs'. In all places where we use gpu_is_available we actually mean: is 'CuPy available with a CUDA GPU'? So, this PR replaces uses of gpu_is_available by has_cupy_gpu. This allows us to use gpu_is_available in the future to check if any GPU is available.
In addition to that, some code had expressions like
has_cupy and gpu_is_available()

This PR simplify such conditions to has_cupy_gpu, since has_cupy_gpu implies has_cupy.","This PR is in preparation of better non-CUDA device support. Once we support non-CUDA GPUs, there may be GPUs available that are not 'CuPy GPUs'. In all places where we use gpu_is_available we actually mean: is 'CuPy available with a CUDA GPU'? So, this PR replaces uses of gpu_is_available by has_cupy_gpu. This allows us to use gpu_is_available in the future to check if any GPU is available.
In addition to that, some code had expressions like
has_cupy and gpu_is_available()

This PR simplify such conditions to has_cupy_gpu, since has_cupy_gpu implies has_cupy.",True,{}
explosion/thinc,https://github.com/explosion/thinc,676,2022-05-23T10:47:29Z,2022-05-23T17:18:04Z,2022-05-23T17:18:04Z,MERGED,True,3,1,2,https://github.com/danieldk,Fix two warnings,1,['feat / ops'],https://github.com/explosion/thinc/pull/676,https://github.com/danieldk,1,https://github.com/explosion/thinc/pull/676,"torch.nn.functional.sigmoid is deprecated in favor of torch.sigmoid.
Clip cosh input in sechsq to avoid overflow.","torch.nn.functional.sigmoid is deprecated in favor of torch.sigmoid.
Clip cosh input in sechsq to avoid overflow.",True,{'THUMBS_UP': ['https://github.com/svlandeg']}
explosion/thinc,https://github.com/explosion/thinc,677,2022-05-23T12:15:16Z,2022-05-23T12:57:24Z,2022-05-23T13:24:57Z,MERGED,True,2,1,1,https://github.com/shadeMe,`Shim`: Fix potential data race when allocated on different threads,1,['bug'],https://github.com/explosion/thinc/pull/677,https://github.com/shadeMe,1,https://github.com/explosion/thinc/pull/677,Moves both volatile reads of the global_id global variable into the critical section to prevent a write (from another thread) interleaving them and potentially resulting in multiple Shim instances receiving the same ID.,Moves both volatile reads of the global_id global variable into the critical section to prevent a write (from another thread) interleaving them and potentially resulting in multiple Shim instances receiving the same ID.,True,{'THUMBS_UP': ['https://github.com/svlandeg']}
explosion/thinc,https://github.com/explosion/thinc,677,2022-05-23T12:15:16Z,2022-05-23T12:57:24Z,2022-05-23T13:24:57Z,MERGED,True,2,1,1,https://github.com/shadeMe,`Shim`: Fix potential data race when allocated on different threads,1,['bug'],https://github.com/explosion/thinc/pull/677,https://github.com/danieldk,2,https://github.com/explosion/thinc/pull/677#issuecomment-1134600936,Moves both volatile reads of the global_id global variable into the critical section to prevent a write (from another thread) interleaving them and potentially resulting in multiple Shim instances receiving the same ID.,@explosion-bot please test_gpu,True,"{'EYES': ['https://github.com/explosion-bot'], 'ROCKET': ['https://github.com/explosion-bot']}"
explosion/thinc,https://github.com/explosion/thinc,677,2022-05-23T12:15:16Z,2022-05-23T12:57:24Z,2022-05-23T13:24:57Z,MERGED,True,2,1,1,https://github.com/shadeMe,`Shim`: Fix potential data race when allocated on different threads,1,['bug'],https://github.com/explosion/thinc/pull/677,https://github.com/explosion-bot,3,https://github.com/explosion/thinc/pull/677#issuecomment-1134601400,Moves both volatile reads of the global_id global variable into the critical section to prevent a write (from another thread) interleaving them and potentially resulting in multiple Shim instances receiving the same ID.," Successfully triggered build on Buildkite

URL: https://buildkite.com/explosion-ai/thinc-gpu-test-suite/builds/68",True,{}
explosion/thinc,https://github.com/explosion/thinc,678,2022-05-23T15:36:27Z,2022-05-24T13:44:43Z,2022-05-24T13:52:05Z,MERGED,True,2,1,1,https://github.com/shadeMe,Prevent test failures on Azure CI hosts running Windows,1,['tests'],https://github.com/explosion/thinc/pull/678,https://github.com/shadeMe,1,https://github.com/explosion/thinc/pull/678,"By default, pytest fails when it encounters unexpected variability in test execution times between consecutive runs (with different parameters, etc). This was leading to occasional build failures on Windows hosts.
The actual cause for this variability (specifically on Windows) is currently unknown, partly owing to the lack of options when it comes to debugging such non-deterministic issues on the remote machine.","By default, pytest fails when it encounters unexpected variability in test execution times between consecutive runs (with different parameters, etc). This was leading to occasional build failures on Windows hosts.
The actual cause for this variability (specifically on Windows) is currently unknown, partly owing to the lack of options when it comes to debugging such non-deterministic issues on the remote machine.",True,{}
explosion/thinc,https://github.com/explosion/thinc,678,2022-05-23T15:36:27Z,2022-05-24T13:44:43Z,2022-05-24T13:52:05Z,MERGED,True,2,1,1,https://github.com/shadeMe,Prevent test failures on Azure CI hosts running Windows,1,['tests'],https://github.com/explosion/thinc/pull/678,https://github.com/svlandeg,2,https://github.com/explosion/thinc/pull/678#issuecomment-1134998909,"By default, pytest fails when it encounters unexpected variability in test execution times between consecutive runs (with different parameters, etc). This was leading to occasional build failures on Windows hosts.
The actual cause for this variability (specifically on Windows) is currently unknown, partly owing to the lack of options when it comes to debugging such non-deterministic issues on the remote machine.",It looks like this test has some history :p,True,{'EYES': ['https://github.com/shadeMe']}
explosion/thinc,https://github.com/explosion/thinc,679,2022-05-24T08:23:47Z,,2022-05-30T07:28:01Z,OPEN,False,26,26,23,https://github.com/danieldk,Make the compat module private,2,[],https://github.com/explosion/thinc/pull/679,https://github.com/danieldk,1,https://github.com/explosion/thinc/pull/679,"As discussed, this module contains a lot of booleans that Thinc uses internally to keep track of what libraries and devices are available. However, we want to keep the freedom to rename or change the semantics of these booleans, so let's make the module private.","As discussed, this module contains a lot of booleans that Thinc uses internally to keep track of what libraries and devices are available. However, we want to keep the freedom to rename or change the semantics of these booleans, so let's make the module private.",True,{}
explosion/thinc,https://github.com/explosion/thinc,679,2022-05-24T08:23:47Z,,2022-05-30T07:28:01Z,OPEN,False,26,26,23,https://github.com/danieldk,Make the compat module private,2,[],https://github.com/explosion/thinc/pull/679,https://github.com/adrianeboyd,2,https://github.com/explosion/thinc/pull/679#issuecomment-1135799378,"As discussed, this module contains a lot of booleans that Thinc uses internally to keep track of what libraries and devices are available. However, we want to keep the freedom to rename or change the semantics of these booleans, so let's make the module private.",Can you change all the imports to import from util or api as much as possible rather than from _compat?,True,{}
explosion/thinc,https://github.com/explosion/thinc,679,2022-05-24T08:23:47Z,,2022-05-30T07:28:01Z,OPEN,False,26,26,23,https://github.com/danieldk,Make the compat module private,2,[],https://github.com/explosion/thinc/pull/679,https://github.com/danieldk,3,https://github.com/explosion/thinc/pull/679#issuecomment-1136310407,"As discussed, this module contains a lot of booleans that Thinc uses internally to keep track of what libraries and devices are available. However, we want to keep the freedom to rename or change the semantics of these booleans, so let's make the module private.","Can you change all the imports to import from util or api as much as possible rather than from _compat?

Do we really want this? To me has_.* are effectively private Thinc state variables that we use to keep track of what libraries/GPUs are available. Exposing them to the outside makes it harder to change/improve the semantics of these variables. E.g., currently we have has_torch_gpu which flags 'torch has access to a CUDA GPU', but should eventually mean 'torch has access to a GPU'. However, if we expose this officially through thinc.util we cannot give them the intended semantics, because someone may rely on has_torch_gpu to check whether there is a CUDA GPU.
The only use in spaCy of a has_ variable is has_cupy in the condition:
if has_cupy and gpu_is_available():

however, gpu_is_available() alone would suffice.",True,{}
explosion/thinc,https://github.com/explosion/thinc,679,2022-05-24T08:23:47Z,,2022-05-30T07:28:01Z,OPEN,False,26,26,23,https://github.com/danieldk,Make the compat module private,2,[],https://github.com/explosion/thinc/pull/679,https://github.com/adrianeboyd,4,https://github.com/explosion/thinc/pull/679#issuecomment-1137175459,"As discussed, this module contains a lot of booleans that Thinc uses internally to keep track of what libraries and devices are available. However, we want to keep the freedom to rename or change the semantics of these booleans, so let's make the module private.","It's too late to treat them as actually private, though. I think we should treat everything that was previously in thinc.util in a release as part of the public API.
Then only new/private things would be imported from thinc._compat to make this distinction clearer.
I don't think this entirely rules out this kind of change to has_torch_gpu (at a minor version), but it shouldn't be removed from thinc.util or treated like it's private in terms of when backwards-incompatible changes might be made.",True,{}
explosion/thinc,https://github.com/explosion/thinc,680,2022-05-25T13:15:51Z,2022-05-25T13:46:08Z,2022-05-25T14:40:28Z,MERGED,True,3,1,1,https://github.com/shadeMe,`test_to_categorical`: Ensure that `label_smoothing < 0.5`,2,[],https://github.com/explosion/thinc/pull/680,https://github.com/shadeMe,1,https://github.com/explosion/thinc/pull/680,to_categorical expects 0.0 <= label_smoothing < 0.5.,to_categorical expects 0.0 <= label_smoothing < 0.5.,True,{}
explosion/thinc,https://github.com/explosion/thinc,680,2022-05-25T13:15:51Z,2022-05-25T13:46:08Z,2022-05-25T14:40:28Z,MERGED,True,3,1,1,https://github.com/shadeMe,`test_to_categorical`: Ensure that `label_smoothing < 0.5`,2,[],https://github.com/explosion/thinc/pull/680,https://github.com/danieldk,2,https://github.com/explosion/thinc/pull/680#issuecomment-1137261783,to_categorical expects 0.0 <= label_smoothing < 0.5.,"The remaining failures are in relu, so let's merge this...",True,{}
explosion/thinc,https://github.com/explosion/thinc,681,2022-05-27T07:21:42Z,2022-05-27T09:00:05Z,2022-05-27T09:00:10Z,MERGED,True,3,6,1,https://github.com/danieldk,test_ops: do not lower precision in conversion to Torch tensor,3,"['bug', 'tests']",https://github.com/explosion/thinc/pull/681,https://github.com/danieldk,1,https://github.com/explosion/thinc/pull/681,"float64 test values close to zero were rounded by conversion to a float32 Torch tensor, resuling in mismatches between Thinc and Torch gradients. This change prevents the loss in precision.","float64 test values close to zero were rounded by conversion to a float32 Torch tensor, resuling in mismatches between Thinc and Torch gradients. This change prevents the loss in precision.",True,"{'THUMBS_DOWN': ['https://github.com/kadarakos'], 'HOORAY': ['https://github.com/kadarakos', 'https://github.com/svlandeg']}"
explosion/thinc,https://github.com/explosion/thinc,681,2022-05-27T07:21:42Z,2022-05-27T09:00:05Z,2022-05-27T09:00:10Z,MERGED,True,3,6,1,https://github.com/danieldk,test_ops: do not lower precision in conversion to Torch tensor,3,"['bug', 'tests']",https://github.com/explosion/thinc/pull/681,https://github.com/danieldk,2,https://github.com/explosion/thinc/pull/681#issuecomment-1139370414,"float64 test values close to zero were rounded by conversion to a float32 Torch tensor, resuling in mismatches between Thinc and Torch gradients. This change prevents the loss in precision.",@explosion-bot please test_gpu,True,"{'EYES': ['https://github.com/explosion-bot'], 'ROCKET': ['https://github.com/explosion-bot']}"
explosion/thinc,https://github.com/explosion/thinc,681,2022-05-27T07:21:42Z,2022-05-27T09:00:05Z,2022-05-27T09:00:10Z,MERGED,True,3,6,1,https://github.com/danieldk,test_ops: do not lower precision in conversion to Torch tensor,3,"['bug', 'tests']",https://github.com/explosion/thinc/pull/681,https://github.com/explosion-bot,3,https://github.com/explosion/thinc/pull/681#issuecomment-1139370884,"float64 test values close to zero were rounded by conversion to a float32 Torch tensor, resuling in mismatches between Thinc and Torch gradients. This change prevents the loss in precision."," Successfully triggered build on Buildkite

URL: https://buildkite.com/explosion-ai/thinc-gpu-test-suite/builds/72",True,{}
explosion/thinc,https://github.com/explosion/thinc,681,2022-05-27T07:21:42Z,2022-05-27T09:00:05Z,2022-05-27T09:00:10Z,MERGED,True,3,6,1,https://github.com/danieldk,test_ops: do not lower precision in conversion to Torch tensor,3,"['bug', 'tests']",https://github.com/explosion/thinc/pull/681,https://github.com/danieldk,4,https://github.com/explosion/thinc/pull/681#issuecomment-1139401213,"float64 test values close to zero were rounded by conversion to a float32 Torch tensor, resuling in mismatches between Thinc and Torch gradients. This change prevents the loss in precision.",@explosion-bot please test_gpu,True,"{'EYES': ['https://github.com/explosion-bot'], 'ROCKET': ['https://github.com/explosion-bot']}"
explosion/thinc,https://github.com/explosion/thinc,681,2022-05-27T07:21:42Z,2022-05-27T09:00:05Z,2022-05-27T09:00:10Z,MERGED,True,3,6,1,https://github.com/danieldk,test_ops: do not lower precision in conversion to Torch tensor,3,"['bug', 'tests']",https://github.com/explosion/thinc/pull/681,https://github.com/explosion-bot,5,https://github.com/explosion/thinc/pull/681#issuecomment-1139401510,"float64 test values close to zero were rounded by conversion to a float32 Torch tensor, resuling in mismatches between Thinc and Torch gradients. This change prevents the loss in precision."," Successfully triggered build on Buildkite

URL: https://buildkite.com/explosion-ai/thinc-gpu-test-suite/builds/73",True,{}
explosion/thinc,https://github.com/explosion/thinc,682,2022-05-27T08:07:10Z,2022-05-27T10:15:59Z,2022-05-27T10:15:59Z,MERGED,True,66,46,15,https://github.com/apps/github-actions,Auto-format code with black,1,['meta'],https://github.com/explosion/thinc/pull/682,https://github.com/apps/github-actions,1,https://github.com/explosion/thinc/pull/682,This PR is auto-generated.,This PR is auto-generated.,True,{}
explosion/thinc,https://github.com/explosion/thinc,683,2022-05-27T09:05:52Z,2022-05-27T13:30:03Z,2022-05-27T13:30:11Z,MERGED,True,51,46,14,https://github.com/danieldk,Merge `master` into `features/pytorch-device`,9,[],https://github.com/explosion/thinc/pull/683,https://github.com/danieldk,1,https://github.com/explosion/thinc/pull/683,,,True,{}
explosion/thinc,https://github.com/explosion/thinc,683,2022-05-27T09:05:52Z,2022-05-27T13:30:03Z,2022-05-27T13:30:11Z,MERGED,True,51,46,14,https://github.com/danieldk,Merge `master` into `features/pytorch-device`,9,[],https://github.com/explosion/thinc/pull/683,https://github.com/danieldk,2,https://github.com/explosion/thinc/pull/683#issuecomment-1139428027,,@explosion-bot please test_gpu,True,"{'EYES': ['https://github.com/explosion-bot'], 'ROCKET': ['https://github.com/explosion-bot']}"
explosion/thinc,https://github.com/explosion/thinc,683,2022-05-27T09:05:52Z,2022-05-27T13:30:03Z,2022-05-27T13:30:11Z,MERGED,True,51,46,14,https://github.com/danieldk,Merge `master` into `features/pytorch-device`,9,[],https://github.com/explosion/thinc/pull/683,https://github.com/explosion-bot,3,https://github.com/explosion/thinc/pull/683#issuecomment-1139428524,," Successfully triggered build on Buildkite

URL: https://buildkite.com/explosion-ai/thinc-gpu-test-suite/builds/74",True,{}
explosion/thinc,https://github.com/explosion/thinc,684,2022-05-27T09:25:52Z,2022-06-01T07:31:52Z,2022-06-01T07:37:54Z,MERGED,True,1,1,1,https://github.com/shadeMe,Add `test_slow_gpu` explosion-bot command,1,['tests'],https://github.com/explosion/thinc/pull/684,https://github.com/shadeMe,1,https://github.com/explosion/thinc/pull/684,Enables support for the test_slow_gpu explosion-bot command.,Enables support for the test_slow_gpu explosion-bot command.,True,{'HOORAY': ['https://github.com/svlandeg']}
explosion/thinc,https://github.com/explosion/thinc,685,2022-05-29T11:41:47Z,,2022-06-02T10:58:59Z,OPEN,False,105,33,9,https://github.com/danieldk,Add support for PyTorch Metal Performance Shaders,12,"['enhancement', 'performance', 'interop / pytorch']",https://github.com/explosion/thinc/pull/685,https://github.com/danieldk,1,https://github.com/explosion/thinc/pull/685,"Note: draft, because explosion/thinc-apple-ops#17 is required.
Nightly PyTorch versions add support for Metal Performance Shaders (MPS). Metal is a low-level graphics API for Apple platforms that also supports compute kernels (shaders). MPS is a framework of highly-optimized compute and graphics kernels, including kernels for neural networks. MPS is supported on both Apple Silicon, such as the M1 family of SoC, as well as a range of AMD GPUs used in Macs.
Since devices are handled in Thinc through a specific Ops implementation (e.g. CupyOps == CUDA GPUs), this change introduces the MPSOps class. This class is a subclass of NumpyOps or AppleOps (when available). MPSOps does not override any methods, but is used to signal to relevant code paths (e.g. xp2torch) that Torch tensors should be placed on the MPS device.
The mapping in the previously introduced get_torch_default_device function is updated to:

NumpyOps -> cpu
CupyOps -> cuda:N, where N is the selected CUDA device.
MPSOps -> mps

to ensure placement of Torch tensors on the mps device when MPSOps is active.
Finally, the following booleans have been added to or changed in compat:

has_torch_mps (new): PyTorch has MPS support
has_torch_mps_gpu (new): PyTorch has MPS support and an
MPS-capable GPU is available.
has_torch_cuda_gpu (new): PyTorch has CUDA support and a
CUDA-capable GPU is available.
has_torch_gpu (changed): PyTorch has a GPU available (CUDA
or MPS).","Note: draft, because explosion/thinc-apple-ops#17 is required.
Nightly PyTorch versions add support for Metal Performance Shaders (MPS). Metal is a low-level graphics API for Apple platforms that also supports compute kernels (shaders). MPS is a framework of highly-optimized compute and graphics kernels, including kernels for neural networks. MPS is supported on both Apple Silicon, such as the M1 family of SoC, as well as a range of AMD GPUs used in Macs.
Since devices are handled in Thinc through a specific Ops implementation (e.g. CupyOps == CUDA GPUs), this change introduces the MPSOps class. This class is a subclass of NumpyOps or AppleOps (when available). MPSOps does not override any methods, but is used to signal to relevant code paths (e.g. xp2torch) that Torch tensors should be placed on the MPS device.
The mapping in the previously introduced get_torch_default_device function is updated to:

NumpyOps -> cpu
CupyOps -> cuda:N, where N is the selected CUDA device.
MPSOps -> mps

to ensure placement of Torch tensors on the mps device when MPSOps is active.
Finally, the following booleans have been added to or changed in compat:

has_torch_mps (new): PyTorch has MPS support
has_torch_mps_gpu (new): PyTorch has MPS support and an
MPS-capable GPU is available.
has_torch_cuda_gpu (new): PyTorch has CUDA support and a
CUDA-capable GPU is available.
has_torch_gpu (changed): PyTorch has a GPU available (CUDA
or MPS).",True,{}
explosion/thinc,https://github.com/explosion/thinc,685,2022-05-29T11:41:47Z,,2022-06-02T10:58:59Z,OPEN,False,105,33,9,https://github.com/danieldk,Add support for PyTorch Metal Performance Shaders,12,"['enhancement', 'performance', 'interop / pytorch']",https://github.com/explosion/thinc/pull/685,https://github.com/danieldk,2,https://github.com/explosion/thinc/pull/685#issuecomment-1144679519,"Note: draft, because explosion/thinc-apple-ops#17 is required.
Nightly PyTorch versions add support for Metal Performance Shaders (MPS). Metal is a low-level graphics API for Apple platforms that also supports compute kernels (shaders). MPS is a framework of highly-optimized compute and graphics kernels, including kernels for neural networks. MPS is supported on both Apple Silicon, such as the M1 family of SoC, as well as a range of AMD GPUs used in Macs.
Since devices are handled in Thinc through a specific Ops implementation (e.g. CupyOps == CUDA GPUs), this change introduces the MPSOps class. This class is a subclass of NumpyOps or AppleOps (when available). MPSOps does not override any methods, but is used to signal to relevant code paths (e.g. xp2torch) that Torch tensors should be placed on the MPS device.
The mapping in the previously introduced get_torch_default_device function is updated to:

NumpyOps -> cpu
CupyOps -> cuda:N, where N is the selected CUDA device.
MPSOps -> mps

to ensure placement of Torch tensors on the mps device when MPSOps is active.
Finally, the following booleans have been added to or changed in compat:

has_torch_mps (new): PyTorch has MPS support
has_torch_mps_gpu (new): PyTorch has MPS support and an
MPS-capable GPU is available.
has_torch_cuda_gpu (new): PyTorch has CUDA support and a
CUDA-capable GPU is available.
has_torch_gpu (changed): PyTorch has a GPU available (CUDA
or MPS).",The diff should get less messy after #692 is merged.,True,{}
explosion/thinc,https://github.com/explosion/thinc,685,2022-05-29T11:41:47Z,,2022-06-02T10:58:59Z,OPEN,False,105,33,9,https://github.com/danieldk,Add support for PyTorch Metal Performance Shaders,12,"['enhancement', 'performance', 'interop / pytorch']",https://github.com/explosion/thinc/pull/685,https://github.com/danieldk,3,https://github.com/explosion/thinc/pull/685#issuecomment-1144728304,"Note: draft, because explosion/thinc-apple-ops#17 is required.
Nightly PyTorch versions add support for Metal Performance Shaders (MPS). Metal is a low-level graphics API for Apple platforms that also supports compute kernels (shaders). MPS is a framework of highly-optimized compute and graphics kernels, including kernels for neural networks. MPS is supported on both Apple Silicon, such as the M1 family of SoC, as well as a range of AMD GPUs used in Macs.
Since devices are handled in Thinc through a specific Ops implementation (e.g. CupyOps == CUDA GPUs), this change introduces the MPSOps class. This class is a subclass of NumpyOps or AppleOps (when available). MPSOps does not override any methods, but is used to signal to relevant code paths (e.g. xp2torch) that Torch tensors should be placed on the MPS device.
The mapping in the previously introduced get_torch_default_device function is updated to:

NumpyOps -> cpu
CupyOps -> cuda:N, where N is the selected CUDA device.
MPSOps -> mps

to ensure placement of Torch tensors on the mps device when MPSOps is active.
Finally, the following booleans have been added to or changed in compat:

has_torch_mps (new): PyTorch has MPS support
has_torch_mps_gpu (new): PyTorch has MPS support and an
MPS-capable GPU is available.
has_torch_cuda_gpu (new): PyTorch has CUDA support and a
CUDA-capable GPU is available.
has_torch_gpu (changed): PyTorch has a GPU available (CUDA
or MPS).","The diff should get less messy after #692 is merged.

The history is clean again :).",True,{}
explosion/thinc,https://github.com/explosion/thinc,686,2022-05-30T07:27:26Z,,2022-06-06T09:50:55Z,OPEN,False,4,0,1,https://github.com/danieldk,"xp2{tensorflow,torch}: convert NumPy arrays using dlpack",3,"['enhancement', 'interop / tensorflow', 'interop / pytorch']",https://github.com/explosion/thinc/pull/686,https://github.com/danieldk,1,https://github.com/explosion/thinc/pull/686,Newer versions of NumPy can expose arrays as dlpack capsules. Use this functionality (when supported) to speed up NumPy -> Torch/Tensorflow array conversion.,Newer versions of NumPy can expose arrays as dlpack capsules. Use this functionality (when supported) to speed up NumPy -> Torch/Tensorflow array conversion.,True,"{'THUMBS_UP': ['https://github.com/kadarakos', 'https://github.com/shadeMe']}"
explosion/thinc,https://github.com/explosion/thinc,686,2022-05-30T07:27:26Z,,2022-06-06T09:50:55Z,OPEN,False,4,0,1,https://github.com/danieldk,"xp2{tensorflow,torch}: convert NumPy arrays using dlpack",3,"['enhancement', 'interop / tensorflow', 'interop / pytorch']",https://github.com/explosion/thinc/pull/686,https://github.com/danieldk,2,https://github.com/explosion/thinc/pull/686#issuecomment-1140800699,Newer versions of NumPy can expose arrays as dlpack capsules. Use this functionality (when supported) to speed up NumPy -> Torch/Tensorflow array conversion.,@explosion-bot please test_gpu,True,"{'EYES': ['https://github.com/explosion-bot'], 'ROCKET': ['https://github.com/explosion-bot']}"
explosion/thinc,https://github.com/explosion/thinc,686,2022-05-30T07:27:26Z,,2022-06-06T09:50:55Z,OPEN,False,4,0,1,https://github.com/danieldk,"xp2{tensorflow,torch}: convert NumPy arrays using dlpack",3,"['enhancement', 'interop / tensorflow', 'interop / pytorch']",https://github.com/explosion/thinc/pull/686,https://github.com/explosion-bot,3,https://github.com/explosion/thinc/pull/686#issuecomment-1140801069,Newer versions of NumPy can expose arrays as dlpack capsules. Use this functionality (when supported) to speed up NumPy -> Torch/Tensorflow array conversion.," Successfully triggered build on Buildkite

URL: https://buildkite.com/explosion-ai/thinc-gpu-test-suite/builds/76",True,{}
explosion/thinc,https://github.com/explosion/thinc,687,2022-05-30T08:56:38Z,2022-05-31T09:13:13Z,2022-05-31T09:13:15Z,MERGED,True,1,1,1,https://github.com/danieldk,Azure: pin protobuf to fix Tensorflow,1,"['tests', 'interop / tensorflow']",https://github.com/explosion/thinc/pull/687,https://github.com/danieldk,1,https://github.com/explosion/thinc/pull/687,,,True,{}
explosion/thinc,https://github.com/explosion/thinc,687,2022-05-30T08:56:38Z,2022-05-31T09:13:13Z,2022-05-31T09:13:15Z,MERGED,True,1,1,1,https://github.com/danieldk,Azure: pin protobuf to fix Tensorflow,1,"['tests', 'interop / tensorflow']",https://github.com/explosion/thinc/pull/687,https://github.com/danieldk,2,https://github.com/explosion/thinc/pull/687#issuecomment-1140902614,,@explosion-bot please test_gpu,True,"{'EYES': ['https://github.com/explosion-bot'], 'ROCKET': ['https://github.com/explosion-bot']}"
explosion/thinc,https://github.com/explosion/thinc,687,2022-05-30T08:56:38Z,2022-05-31T09:13:13Z,2022-05-31T09:13:15Z,MERGED,True,1,1,1,https://github.com/danieldk,Azure: pin protobuf to fix Tensorflow,1,"['tests', 'interop / tensorflow']",https://github.com/explosion/thinc/pull/687,https://github.com/explosion-bot,3,https://github.com/explosion/thinc/pull/687#issuecomment-1140903009,," Successfully triggered build on Buildkite

URL: https://buildkite.com/explosion-ai/thinc-gpu-test-suite/builds/77",True,{}
explosion/thinc,https://github.com/explosion/thinc,688,2022-06-01T11:45:23Z,,2022-06-03T14:00:39Z,OPEN,False,21,5,3,https://github.com/shadeMe,"`test_model_gpu`: Use TF memory pool if available, feature-gate test",5,"['tests', 'interop / tensorflow']",https://github.com/explosion/thinc/pull/688,https://github.com/shadeMe,1,https://github.com/explosion/thinc/pull/688,This was breaking the slow GPU test suite.,This was breaking the slow GPU test suite.,True,{}
explosion/thinc,https://github.com/explosion/thinc,688,2022-06-01T11:45:23Z,,2022-06-03T14:00:39Z,OPEN,False,21,5,3,https://github.com/shadeMe,"`test_model_gpu`: Use TF memory pool if available, feature-gate test",5,"['tests', 'interop / tensorflow']",https://github.com/explosion/thinc/pull/688,https://github.com/shadeMe,2,https://github.com/explosion/thinc/pull/688#issuecomment-1143500954,This was breaking the slow GPU test suite.,@explosion-bot please test_slow_gpu,True,"{'EYES': ['https://github.com/explosion-bot'], 'ROCKET': ['https://github.com/explosion-bot']}"
explosion/thinc,https://github.com/explosion/thinc,688,2022-06-01T11:45:23Z,,2022-06-03T14:00:39Z,OPEN,False,21,5,3,https://github.com/shadeMe,"`test_model_gpu`: Use TF memory pool if available, feature-gate test",5,"['tests', 'interop / tensorflow']",https://github.com/explosion/thinc/pull/688,https://github.com/explosion-bot,3,https://github.com/explosion/thinc/pull/688#issuecomment-1143501341,This was breaking the slow GPU test suite.," Successfully triggered build on Buildkite

URL: https://buildkite.com/explosion-ai/thinc-slow-gpu-tests/builds/9",True,{}
explosion/thinc,https://github.com/explosion/thinc,688,2022-06-01T11:45:23Z,,2022-06-03T14:00:39Z,OPEN,False,21,5,3,https://github.com/shadeMe,"`test_model_gpu`: Use TF memory pool if available, feature-gate test",5,"['tests', 'interop / tensorflow']",https://github.com/explosion/thinc/pull/688,https://github.com/danieldk,4,https://github.com/explosion/thinc/pull/688#issuecomment-1145991767,This was breaking the slow GPU test suite.,"@explosion-bot please test_slow_gpu
Checking if it works ",True,{'EYES': ['https://github.com/explosion-bot']}
explosion/thinc,https://github.com/explosion/thinc,688,2022-06-01T11:45:23Z,,2022-06-03T14:00:39Z,OPEN,False,21,5,3,https://github.com/shadeMe,"`test_model_gpu`: Use TF memory pool if available, feature-gate test",5,"['tests', 'interop / tensorflow']",https://github.com/explosion/thinc/pull/688,https://github.com/explosion-bot,5,https://github.com/explosion/thinc/pull/688#issuecomment-1145992090,This was breaking the slow GPU test suite.," Errors
Usage: @explosion-bot please test_slow_gpu [OPTIONS]
Try '@explosion-bot please test_slow_gpu --help' for help.
Error: Got unexpected extra arguments (Checking if it works )",True,{}
explosion/thinc,https://github.com/explosion/thinc,688,2022-06-01T11:45:23Z,,2022-06-03T14:00:39Z,OPEN,False,21,5,3,https://github.com/shadeMe,"`test_model_gpu`: Use TF memory pool if available, feature-gate test",5,"['tests', 'interop / tensorflow']",https://github.com/explosion/thinc/pull/688,https://github.com/danieldk,6,https://github.com/explosion/thinc/pull/688#issuecomment-1145994530,This was breaking the slow GPU test suite.,@explosion-bot please test_slow_gpu,True,"{'EYES': ['https://github.com/explosion-bot'], 'ROCKET': ['https://github.com/explosion-bot']}"
explosion/thinc,https://github.com/explosion/thinc,688,2022-06-01T11:45:23Z,,2022-06-03T14:00:39Z,OPEN,False,21,5,3,https://github.com/shadeMe,"`test_model_gpu`: Use TF memory pool if available, feature-gate test",5,"['tests', 'interop / tensorflow']",https://github.com/explosion/thinc/pull/688,https://github.com/explosion-bot,7,https://github.com/explosion/thinc/pull/688#issuecomment-1145994916,This was breaking the slow GPU test suite.," Successfully triggered build on Buildkite

URL: https://buildkite.com/explosion-ai/thinc-slow-gpu-tests/builds/10",True,{}
explosion/thinc,https://github.com/explosion/thinc,689,2022-06-02T06:36:25Z,2022-06-02T07:10:46Z,2022-06-02T07:10:46Z,MERGED,True,3,3,2,https://github.com/adrianeboyd,Extend typing_extensions to <4.2.0,1,[],https://github.com/explosion/thinc/pull/689,https://github.com/adrianeboyd,1,https://github.com/explosion/thinc/pull/689,,,True,{}
explosion/thinc,https://github.com/explosion/thinc,690,2022-06-02T07:12:32Z,2022-06-02T12:06:50Z,2022-06-02T12:06:50Z,MERGED,True,2,2,2,https://github.com/adrianeboyd,Extend typing_extensions to <4.2.0 (#689),2,[],https://github.com/explosion/thinc/pull/690,https://github.com/adrianeboyd,1,https://github.com/explosion/thinc/pull/690,,,True,{}
explosion/thinc,https://github.com/explosion/thinc,691,2022-06-02T07:24:44Z,2022-06-02T08:50:31Z,2022-06-02T08:50:31Z,MERGED,True,13,10,3,https://github.com/adrianeboyd,Backport test fixes to v8.0.x,4,[],https://github.com/explosion/thinc/pull/691,https://github.com/adrianeboyd,1,https://github.com/explosion/thinc/pull/691,,,True,{}
explosion/thinc,https://github.com/explosion/thinc,691,2022-06-02T07:24:44Z,2022-06-02T08:50:31Z,2022-06-02T08:50:31Z,MERGED,True,13,10,3,https://github.com/adrianeboyd,Backport test fixes to v8.0.x,4,[],https://github.com/explosion/thinc/pull/691,https://github.com/danieldk,2,https://github.com/explosion/thinc/pull/691#issuecomment-1144551377,,I think you might also need 6472d0a.,True,{}
explosion/thinc,https://github.com/explosion/thinc,691,2022-06-02T07:24:44Z,2022-06-02T08:50:31Z,2022-06-02T08:50:31Z,MERGED,True,13,10,3,https://github.com/adrianeboyd,Backport test fixes to v8.0.x,4,[],https://github.com/explosion/thinc/pull/691,https://github.com/adrianeboyd,3,https://github.com/explosion/thinc/pull/691#issuecomment-1144582409,,I don't understand what's going on with float64 in this test. Is this a reasonable minimal backport?,True,{}
explosion/thinc,https://github.com/explosion/thinc,691,2022-06-02T07:24:44Z,2022-06-02T08:50:31Z,2022-06-02T08:50:31Z,MERGED,True,13,10,3,https://github.com/adrianeboyd,Backport test fixes to v8.0.x,4,[],https://github.com/explosion/thinc/pull/691,https://github.com/danieldk,4,https://github.com/explosion/thinc/pull/691#issuecomment-1144593582,,"I don't understand what's going on with float64 in this test. Is this a reasonable minimal backport?

Looks good to me. Some of the ops in 8.0.x don't support float64.",True,{}
explosion/thinc,https://github.com/explosion/thinc,692,2022-06-02T10:03:03Z,2022-06-02T10:56:41Z,2022-06-02T10:56:45Z,MERGED,True,71,51,19,https://github.com/danieldk,Merge `master` into `features/pytorch-device`,7,[],https://github.com/explosion/thinc/pull/692,https://github.com/danieldk,1,https://github.com/explosion/thinc/pull/692,,,True,{}
explosion/thinc,https://github.com/explosion/thinc,693,2022-06-02T12:08:03Z,2022-06-02T13:08:37Z,2022-06-02T13:08:37Z,MERGED,True,1,1,1,https://github.com/adrianeboyd,Set version to v8.0.17,1,[],https://github.com/explosion/thinc/pull/693,https://github.com/adrianeboyd,1,https://github.com/explosion/thinc/pull/693,,,True,{}
explosion/thinc,https://github.com/explosion/thinc,693,2022-06-02T12:08:03Z,2022-06-02T13:08:37Z,2022-06-02T13:08:37Z,MERGED,True,1,1,1,https://github.com/adrianeboyd,Set version to v8.0.17,1,[],https://github.com/explosion/thinc/pull/693,https://github.com/adrianeboyd,2,https://github.com/explosion/thinc/pull/693#issuecomment-1144788070,,@explosion-bot please test_gpu,True,"{'EYES': ['https://github.com/explosion-bot'], 'ROCKET': ['https://github.com/explosion-bot']}"
explosion/thinc,https://github.com/explosion/thinc,693,2022-06-02T12:08:03Z,2022-06-02T13:08:37Z,2022-06-02T13:08:37Z,MERGED,True,1,1,1,https://github.com/adrianeboyd,Set version to v8.0.17,1,[],https://github.com/explosion/thinc/pull/693,https://github.com/explosion-bot,3,https://github.com/explosion/thinc/pull/693#issuecomment-1144788404,," Successfully triggered build on Buildkite

URL: https://buildkite.com/explosion-ai/thinc-gpu-test-suite/builds/78",True,{}
