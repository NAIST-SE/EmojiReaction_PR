pmacct/pmacct,https://github.com/pmacct/pmacct,13,2016-03-04T09:40:26Z,2016-04-12T20:30:12Z,2016-04-12T20:30:12Z,MERGED,True,226,296,18,https://github.com/vincentbernat,uacctd.c: switch support from ULOG to NFLOG,1,[],https://github.com/pmacct/pmacct/pull/13,https://github.com/vincentbernat,1,https://github.com/pmacct/pmacct/pull/13,"ðŸ‘» Please, ignore the first commits. I am using #10 as a base since it makes my life easier. Once done, I can rebase the commit to whatever build system we have. Only the last commit is relevant.
NFLOG supports both IPv4 and IPv6. While ULOG is still supported in
recent kernels, NFLOG is supported since 2.6.14 and there is little
point to support both.
Each occurrence of ""ulog"" has been replaced by ""nflog"". However,
""uacctd"" is still called this way and configuration options remain the
same (but the semantics change a bit for some of them). This should
minimize the disruption for downstreams and users (but users will still
have to update the netfilter rules to use NFLOG instead of ULOG).
This is only lightly tested. I'll do more tests next week. I put it here for early comments.","ðŸ‘» Please, ignore the first commits. I am using #10 as a base since it makes my life easier. Once done, I can rebase the commit to whatever build system we have. Only the last commit is relevant.
NFLOG supports both IPv4 and IPv6. While ULOG is still supported in
recent kernels, NFLOG is supported since 2.6.14 and there is little
point to support both.
Each occurrence of ""ulog"" has been replaced by ""nflog"". However,
""uacctd"" is still called this way and configuration options remain the
same (but the semantics change a bit for some of them). This should
minimize the disruption for downstreams and users (but users will still
have to update the netfilter rules to use NFLOG instead of ULOG).
This is only lightly tested. I'll do more tests next week. I put it here for early comments.",True,{}
pmacct/pmacct,https://github.com/pmacct/pmacct,13,2016-03-04T09:40:26Z,2016-04-12T20:30:12Z,2016-04-12T20:30:12Z,MERGED,True,226,296,18,https://github.com/vincentbernat,uacctd.c: switch support from ULOG to NFLOG,1,[],https://github.com/pmacct/pmacct/pull/13,https://github.com/vincentbernat,2,https://github.com/pmacct/pmacct/pull/13#issuecomment-192227016,"ðŸ‘» Please, ignore the first commits. I am using #10 as a base since it makes my life easier. Once done, I can rebase the commit to whatever build system we have. Only the last commit is relevant.
NFLOG supports both IPv4 and IPv6. While ULOG is still supported in
recent kernels, NFLOG is supported since 2.6.14 and there is little
point to support both.
Each occurrence of ""ulog"" has been replaced by ""nflog"". However,
""uacctd"" is still called this way and configuration options remain the
same (but the semantics change a bit for some of them). This should
minimize the disruption for downstreams and users (but users will still
have to update the netfilter rules to use NFLOG instead of ULOG).
This is only lightly tested. I'll do more tests next week. I put it here for early comments.",I have asked Travis to whitelist libnetfilter-log-dev,True,{}
pmacct/pmacct,https://github.com/pmacct/pmacct,13,2016-03-04T09:40:26Z,2016-04-12T20:30:12Z,2016-04-12T20:30:12Z,MERGED,True,226,296,18,https://github.com/vincentbernat,uacctd.c: switch support from ULOG to NFLOG,1,[],https://github.com/pmacct/pmacct/pull/13,https://github.com/vincentbernat,3,https://github.com/pmacct/pmacct/pull/13#issuecomment-208304736,"ðŸ‘» Please, ignore the first commits. I am using #10 as a base since it makes my life easier. Once done, I can rebase the commit to whatever build system we have. Only the last commit is relevant.
NFLOG supports both IPv4 and IPv6. While ULOG is still supported in
recent kernels, NFLOG is supported since 2.6.14 and there is little
point to support both.
Each occurrence of ""ulog"" has been replaced by ""nflog"". However,
""uacctd"" is still called this way and configuration options remain the
same (but the semantics change a bit for some of them). This should
minimize the disruption for downstreams and users (but users will still
have to update the netfilter rules to use NFLOG instead of ULOG).
This is only lightly tested. I'll do more tests next week. I put it here for early comments.",I have updated the PR against the current master. And travis did whitelist the missing libnetfilter-log-dev package.,True,{}
pmacct/pmacct,https://github.com/pmacct/pmacct,17,2016-04-06T17:21:40Z,2016-04-09T16:28:57Z,2016-04-11T16:07:17Z,MERGED,True,1,1,1,https://github.com/jwestfall69,Make console logging goto stderr,1,[],https://github.com/pmacct/pmacct/pull/17,https://github.com/jwestfall69,1,https://github.com/pmacct/pmacct/pull/17,,,True,{}
pmacct/pmacct,https://github.com/pmacct/pmacct,18,2016-04-11T19:04:09Z,2016-04-12T11:40:02Z,2016-04-12T11:40:02Z,MERGED,True,1,0,1,https://github.com/jwestfall69,fflush stdout before releasing lock,1,[],https://github.com/pmacct/pmacct/pull/18,https://github.com/jwestfall69,1,https://github.com/pmacct/pmacct/pull/18,"Since stdout is buffered, we need to make sure to flush the buffer before releasing the lock.","Since stdout is buffered, we need to make sure to flush the buffer before releasing the lock.",True,{}
pmacct/pmacct,https://github.com/pmacct/pmacct,20,2016-04-14T05:28:02Z,2016-04-15T13:20:59Z,2016-04-15T13:20:59Z,MERGED,True,135,135,6,https://github.com/vincentbernat,json: promote integers to json_int_t before packing,1,[],https://github.com/pmacct/pmacct/pull/20,https://github.com/vincentbernat,1,https://github.com/pmacct/pmacct/pull/20,"Otherwise, promotion doesn't happen, like with printf, but without any warning. This needs to be tested on a big endian platform","Otherwise, promotion doesn't happen, like with printf, but without any warning. This needs to be tested on a big endian platform",True,{}
pmacct/pmacct,https://github.com/pmacct/pmacct,22,2016-05-19T07:53:36Z,,2021-03-23T23:48:00Z,OPEN,False,128,14,13,https://github.com/takehiakihiro,supported tcp retransmission,1,[],https://github.com/pmacct/pmacct/pull/22,https://github.com/takehiakihiro,1,https://github.com/pmacct/pmacct/pull/22,"When we used the flow, it allowed a retransmission of TCP to measure it.
And bucket size of flow(struct ip_flow_common) changed.","When we used the flow, it allowed a retransmission of TCP to measure it.
And bucket size of flow(struct ip_flow_common) changed.",True,{}
pmacct/pmacct,https://github.com/pmacct/pmacct,24,2016-06-02T06:21:21Z,2016-06-02T06:21:39Z,2016-06-02T06:21:39Z,CLOSED,False,78,1,10,https://github.com/joestein,Apache Avro Support,4,[],https://github.com/pmacct/pmacct/pull/24,https://github.com/joestein,1,https://github.com/pmacct/pmacct/pull/24,,,True,{}
pmacct/pmacct,https://github.com/pmacct/pmacct,26,2016-06-29T05:05:44Z,2016-06-30T23:07:09Z,2016-06-30T23:07:09Z,CLOSED,False,20,2,6,https://github.com/tlaberge,Port,3,[],https://github.com/pmacct/pmacct/pull/26,https://github.com/tlaberge,1,https://github.com/pmacct/pmacct/pull/26,"Add the capability to dump by port. This allows same box testing of many connections.
It may be generally useful to include the tcp port in the json wrapper, but will save this for a later commit.","Add the capability to dump by port. This allows same box testing of many connections.
It may be generally useful to include the tcp port in the json wrapper, but will save this for a later commit.",True,{}
pmacct/pmacct,https://github.com/pmacct/pmacct,27,2016-06-29T05:13:45Z,2016-06-29T07:09:46Z,2016-06-29T23:31:22Z,MERGED,True,2,2,1,https://github.com/tlaberge,Change minimum dump time to 10 seconds,1,[],https://github.com/pmacct/pmacct/pull/27,https://github.com/tlaberge,1,https://github.com/pmacct/pmacct/pull/27,Allow dumps to happen as often as once every ten seconds.,Allow dumps to happen as often as once every ten seconds.,True,{}
pmacct/pmacct,https://github.com/pmacct/pmacct,28,2016-06-29T23:53:55Z,2016-06-30T23:07:23Z,2016-06-30T23:07:23Z,CLOSED,False,1,1,1,https://github.com/tlaberge,Rename unused variable __unused to _unused to silence 100+ warnings.,1,[],https://github.com/pmacct/pmacct/pull/28,https://github.com/tlaberge,1,https://github.com/pmacct/pmacct/pull/28,"It turns out that __unused is a compiler directive for clang, so
a structure element named __unused is an empty declaration, which triggers
a warning:
./../network.h:233:7: warning: declaration does not declare anything [-Wmissing-declarations]
u_int16_t __unused;
^~~~~~~~~
Since network.h is included in many files, this warning is emitted 100+
times on a MacOS build.","It turns out that __unused is a compiler directive for clang, so
a structure element named __unused is an empty declaration, which triggers
a warning:
./../network.h:233:7: warning: declaration does not declare anything [-Wmissing-declarations]
u_int16_t __unused;
^~~~~~~~~
Since network.h is included in many files, this warning is emitted 100+
times on a MacOS build.",True,{}
pmacct/pmacct,https://github.com/pmacct/pmacct,29,2016-06-30T23:08:45Z,2016-07-01T22:39:45Z,2016-07-01T22:39:45Z,CLOSED,False,20,2,6,https://github.com/tlaberge,Port,3,[],https://github.com/pmacct/pmacct/pull/29,https://github.com/tlaberge,1,https://github.com/pmacct/pmacct/pull/29,Add a configuration knob to allow dumping telemetry data by port. This allows testing with multiple senders from the same host.,Add a configuration knob to allow dumping telemetry data by port. This allows testing with multiple senders from the same host.,True,{}
pmacct/pmacct,https://github.com/pmacct/pmacct,30,2016-06-30T23:09:58Z,2016-06-30T23:23:22Z,2016-06-30T23:23:22Z,MERGED,True,1,1,1,https://github.com/tlaberge,Rename unused variable __unused to _unused to silence 100+ warnings.,1,[],https://github.com/pmacct/pmacct/pull/30,https://github.com/tlaberge,1,https://github.com/pmacct/pmacct/pull/30,"It turns out that __unused is a compiler directive for clang, so
a structure element named __unused is an empty declaration, which triggers
a warning:
./../network.h:233:7: warning: declaration does not declare anything [-Wmissing-declarations]
      u_int16_t __unused;
      ^~~~~~~~~

Since network.h is included in many files, this warning is emitted 100+
times on a MacOS build.","It turns out that __unused is a compiler directive for clang, so
a structure element named __unused is an empty declaration, which triggers
a warning:
./../network.h:233:7: warning: declaration does not declare anything [-Wmissing-declarations]
      u_int16_t __unused;
      ^~~~~~~~~

Since network.h is included in many files, this warning is emitted 100+
times on a MacOS build.",True,{}
pmacct/pmacct,https://github.com/pmacct/pmacct,31,2016-07-01T23:09:44Z,2016-07-22T21:55:36Z,2016-07-22T21:58:41Z,CLOSED,False,4,0,1,https://github.com/tlaberge,Encode the source port in telemetry log messages.,1,[],https://github.com/pmacct/pmacct/pull/31,https://github.com/tlaberge,1,https://github.com/pmacct/pmacct/pull/31,"This allows post-processing tools to easily distinguish which messages
came from which stream when receiving many streams from the same host,
e.g., for testing purposes.","This allows post-processing tools to easily distinguish which messages
came from which stream when receiving many streams from the same host,
e.g., for testing purposes.",True,{}
pmacct/pmacct,https://github.com/pmacct/pmacct,32,2016-07-07T11:07:43Z,2016-07-07T13:04:21Z,2016-07-07T13:04:21Z,MERGED,True,1,1,1,https://github.com/JonasGroeger,s/tipically/typically/,1,[],https://github.com/pmacct/pmacct/pull/32,https://github.com/JonasGroeger,1,https://github.com/pmacct/pmacct/pull/32,,,True,{}
pmacct/pmacct,https://github.com/pmacct/pmacct,37,2016-07-22T21:59:55Z,2016-07-22T22:00:32Z,2016-07-22T22:00:32Z,CLOSED,False,24,2,7,https://github.com/tlaberge,Encode by port,8,[],https://github.com/pmacct/pmacct/pull/37,https://github.com/tlaberge,1,https://github.com/pmacct/pmacct/pull/37,"Small change to add the port number to the telemetry message. This allows for easy tracking of multiple independent connections from the same host for e.g., testing purposes.","Small change to add the port number to the telemetry message. This allows for easy tracking of multiple independent connections from the same host for e.g., testing purposes.",True,{}
pmacct/pmacct,https://github.com/pmacct/pmacct,38,2016-07-22T22:48:00Z,2016-07-23T15:26:53Z,2016-08-10T18:53:07Z,MERGED,True,4,0,1,https://github.com/tlaberge,pmtelemetryd: Add port number to telemetry data.,1,[],https://github.com/pmacct/pmacct/pull/38,https://github.com/tlaberge,1,https://github.com/pmacct/pmacct/pull/38,"Include the sender's port number along with their IP address.
This allows easy tracking of multiple streams per host, for
e.g., testing purposes.","Include the sender's port number along with their IP address.
This allows easy tracking of multiple streams per host, for
e.g., testing purposes.",True,{}
pmacct/pmacct,https://github.com/pmacct/pmacct,39,2016-07-26T05:56:21Z,2016-07-28T10:38:49Z,2016-08-10T18:53:11Z,MERGED,True,1,3,2,https://github.com/tlaberge,pmtelemtryd: Fix off-by-one-error,1,[],https://github.com/pmacct/pmacct/pull/39,https://github.com/tlaberge,1,https://github.com/pmacct/pmacct/pull/39,"There is an off-by-one-error in the string termination code that
reads raw json telemetry data.
Also, a local variable fd hides a local in an outer scope in the
socket IO loop.","There is an off-by-one-error in the string termination code that
reads raw json telemetry data.
Also, a local variable fd hides a local in an outer scope in the
socket IO loop.",True,{}
pmacct/pmacct,https://github.com/pmacct/pmacct,41,2016-08-01T10:17:18Z,2016-09-08T16:15:26Z,2016-09-08T16:15:26Z,CLOSED,False,1492,175,22,https://github.com/tacgomes,Add avro support,4,[],https://github.com/pmacct/pmacct/pull/41,https://github.com/tacgomes,1,https://github.com/pmacct/pmacct/pull/41,"This patch series lands support to the print, amqp and kafka plugins to output the data using the Apache Avro serialization system 1.
Other possible points of integration are adding Avro as an output option for the logs of the bgp, bmp and telemetry daemons; as an output option for streamed logging of sFlow counters; or as an output option for the print plugin. But let's concentrate in landing its support in the core places first.","This patch series lands support to the print, amqp and kafka plugins to output the data using the Apache Avro serialization system 1.
Other possible points of integration are adding Avro as an output option for the logs of the bgp, bmp and telemetry daemons; as an output option for streamed logging of sFlow counters; or as an output option for the print plugin. But let's concentrate in landing its support in the core places first.",True,"{'THUMBS_UP': ['https://github.com/jrossi'], 'HOORAY': ['https://github.com/jrossi']}"
pmacct/pmacct,https://github.com/pmacct/pmacct,42,2016-08-02T15:19:47Z,2016-08-02T17:44:42Z,2016-08-02T17:44:42Z,MERGED,True,2,1,1,https://github.com/tacgomes,Sfacctd fixes,2,[],https://github.com/pmacct/pmacct/pull/42,https://github.com/tacgomes,1,https://github.com/pmacct/pmacct/pull/42,This commit series fixes two bugs that I've spotted in the sfacctd daemon code.,This commit series fixes two bugs that I've spotted in the sfacctd daemon code.,True,{}
pmacct/pmacct,https://github.com/pmacct/pmacct,43,2016-08-08T15:50:45Z,2016-08-08T18:26:12Z,2016-08-10T18:53:12Z,MERGED,True,79,24,5,https://github.com/tlaberge,Counters,2,[],https://github.com/pmacct/pmacct/pull/43,https://github.com/tlaberge,1,https://github.com/pmacct/pmacct/pull/43,"Add a bgp_peer_stats structure and use it pmtelemetryd to capture statistics on telemetry.
Also, for compressed JSON, modify the code so it tolerates Z_STREAM_END.","Add a bgp_peer_stats structure and use it pmtelemetryd to capture statistics on telemetry.
Also, for compressed JSON, modify the code so it tolerates Z_STREAM_END.",True,{}
pmacct/pmacct,https://github.com/pmacct/pmacct,44,2016-08-10T06:17:48Z,2016-08-10T16:24:50Z,2016-08-10T18:53:13Z,MERGED,True,1,1,1,https://github.com/tlaberge,Fix build break on MacOS,1,[],https://github.com/pmacct/pmacct/pull/44,https://github.com/tlaberge,1,https://github.com/pmacct/pmacct/pull/44,,,True,{}
pmacct/pmacct,https://github.com/pmacct/pmacct,45,2016-08-10T10:21:08Z,2016-08-11T13:37:49Z,2016-08-11T13:37:49Z,MERGED,True,1,0,1,https://github.com/weyfonk,Make custom primitives names case-insensitive,1,[],https://github.com/pmacct/pmacct/pull/45,https://github.com/weyfonk,1,https://github.com/pmacct/pmacct/pull/45,"This commit prevents custom primitives which names contain uppercase
characters from being ignored.","This commit prevents custom primitives which names contain uppercase
characters from being ignored.",True,{}
pmacct/pmacct,https://github.com/pmacct/pmacct,46,2016-08-10T18:50:50Z,2016-08-11T13:38:26Z,2016-09-23T03:04:03Z,MERGED,True,1,1,1,https://github.com/tlaberge,Minor typo,1,[],https://github.com/pmacct/pmacct/pull/46,https://github.com/tlaberge,1,https://github.com/pmacct/pmacct/pull/46,,,True,{}
pmacct/pmacct,https://github.com/pmacct/pmacct,48,2016-08-19T14:39:36Z,2016-08-24T22:43:32Z,2016-08-24T22:43:32Z,MERGED,True,27,2,3,https://github.com/weyfonk,Add support for Apache Avro to the build system,1,[],https://github.com/pmacct/pmacct/pull/48,https://github.com/weyfonk,1,https://github.com/pmacct/pmacct/pull/48,"This is the first PR of a series aiming to introduce Avro support to pmacct, breaking up #41 into smaller, easier to merge changes.
This first PR focuses on changes to the build system.","This is the first PR of a series aiming to introduce Avro support to pmacct, breaking up #41 into smaller, easier to merge changes.
This first PR focuses on changes to the build system.",True,{}
pmacct/pmacct,https://github.com/pmacct/pmacct,49,2016-08-19T16:18:54Z,2016-08-22T08:50:54Z,2016-08-22T08:52:47Z,CLOSED,False,133,2,8,https://github.com/weyfonk,Add support for Apache Avro to the configuration system,2,[],https://github.com/pmacct/pmacct/pull/49,https://github.com/weyfonk,1,https://github.com/pmacct/pmacct/pull/49,"This is the second PR of a series aiming to introduce Avro support to pmacct, breaking up #41 into smaller, easier to merge changes.
This PR focuses on changes to the configuration system.","This is the second PR of a series aiming to introduce Avro support to pmacct, breaking up #41 into smaller, easier to merge changes.
This PR focuses on changes to the configuration system.",True,{}
pmacct/pmacct,https://github.com/pmacct/pmacct,49,2016-08-19T16:18:54Z,2016-08-22T08:50:54Z,2016-08-22T08:52:47Z,CLOSED,False,133,2,8,https://github.com/weyfonk,Add support for Apache Avro to the configuration system,2,[],https://github.com/pmacct/pmacct/pull/49,https://github.com/weyfonk,2,https://github.com/pmacct/pmacct/pull/49#issuecomment-241350840,"This is the second PR of a series aiming to introduce Avro support to pmacct, breaking up #41 into smaller, easier to merge changes.
This PR focuses on changes to the configuration system.","Closing this PR to make a new one, independent of #48. That should be easier to review and merge.",True,{}
pmacct/pmacct,https://github.com/pmacct/pmacct,50,2016-08-22T09:03:50Z,2016-08-26T17:56:04Z,2016-08-26T17:56:04Z,MERGED,True,106,0,5,https://github.com/weyfonk,Add support for Apache Avro to the configuration system,1,[],https://github.com/pmacct/pmacct/pull/50,https://github.com/weyfonk,1,https://github.com/pmacct/pmacct/pull/50,"This is the second PR of a series aiming to introduce Avro support to pmacct, breaking up #41 into smaller, easier to merge changes.
This PR focuses on changes to the configuration system. It should therefore be reviewed after #48.","This is the second PR of a series aiming to introduce Avro support to pmacct, breaking up #41 into smaller, easier to merge changes.
This PR focuses on changes to the configuration system. It should therefore be reviewed after #48.",True,{}
pmacct/pmacct,https://github.com/pmacct/pmacct,51,2016-08-22T09:07:34Z,2016-08-28T20:54:52Z,2016-08-28T20:54:52Z,MERGED,True,733,0,2,https://github.com/weyfonk, Add support for Apache Avro to the library code,1,[],https://github.com/pmacct/pmacct/pull/51,https://github.com/weyfonk,1,https://github.com/pmacct/pmacct/pull/51,"This is the third PR of a series aiming to introduce Avro support to pmacct, breaking up #41 into smaller, easier to merge changes.
This PR focuses on changes to the library code. It should therefore be reviewed after #50.","This is the third PR of a series aiming to introduce Avro support to pmacct, breaking up #41 into smaller, easier to merge changes.
This PR focuses on changes to the library code. It should therefore be reviewed after #50.",True,{}
pmacct/pmacct,https://github.com/pmacct/pmacct,52,2016-08-22T09:10:22Z,2016-08-22T09:10:39Z,2016-08-22T09:10:43Z,CLOSED,False,2,2,1,https://github.com/mehul-m-prajapati,Correcting Index Number,1,[],https://github.com/pmacct/pmacct/pull/52,https://github.com/mehul-m-prajapati,1,https://github.com/pmacct/pmacct/pull/52,,,True,{}
pmacct/pmacct,https://github.com/pmacct/pmacct,53,2016-08-22T09:11:36Z,2016-08-31T12:11:05Z,2016-08-31T12:11:05Z,MERGED,True,86,12,1,https://github.com/weyfonk,Add support for Apache Avro to the print plugin,2,[],https://github.com/pmacct/pmacct/pull/53,https://github.com/weyfonk,1,https://github.com/pmacct/pmacct/pull/53,"This is the fourth PR of a series aiming to introduce Avro support to pmacct, breaking up #41 into smaller, easier to merge changes.
This PR focuses on changes to the print plugin. It should therefore be reviewed after #51.","This is the fourth PR of a series aiming to introduce Avro support to pmacct, breaking up #41 into smaller, easier to merge changes.
This PR focuses on changes to the print plugin. It should therefore be reviewed after #51.",True,{}
pmacct/pmacct,https://github.com/pmacct/pmacct,54,2016-08-22T09:13:24Z,2016-08-22T10:58:02Z,2016-08-22T12:27:14Z,MERGED,True,2,2,1,https://github.com/mehul-m-prajapati,Update INTERNALS,1,[],https://github.com/pmacct/pmacct/pull/54,https://github.com/mehul-m-prajapati,1,https://github.com/pmacct/pmacct/pull/54,Correcting Index numbering,Correcting Index numbering,True,{}
pmacct/pmacct,https://github.com/pmacct/pmacct,56,2016-08-22T16:25:43Z,2016-09-04T10:09:37Z,2016-09-04T10:09:38Z,MERGED,True,192,69,1,https://github.com/weyfonk,Add support for Apache Avro to the AMQP plugin,2,[],https://github.com/pmacct/pmacct/pull/56,https://github.com/weyfonk,1,https://github.com/pmacct/pmacct/pull/56,"This is the fifth PR of a series aiming to introduce Avro support to pmacct, breaking up #41 into smaller, easier to merge changes.
This PR focuses on changes to the AMQP plugin. It should therefore be reviewed after #51.","This is the fifth PR of a series aiming to introduce Avro support to pmacct, breaking up #41 into smaller, easier to merge changes.
This PR focuses on changes to the AMQP plugin. It should therefore be reviewed after #51.",True,{}
pmacct/pmacct,https://github.com/pmacct/pmacct,57,2016-08-22T16:26:48Z,2016-09-02T17:11:01Z,2016-09-02T17:11:01Z,MERGED,True,179,57,1,https://github.com/weyfonk,Add support for Apache Avro to the Kafka plugin,2,[],https://github.com/pmacct/pmacct/pull/57,https://github.com/weyfonk,1,https://github.com/pmacct/pmacct/pull/57,"This is the sixth PR of a series aiming to introduce Avro support to pmacct, breaking up #41 into smaller, easier to merge changes.
This PR focuses on changes to the Kafka plugin. It should therefore be reviewed after #51.","This is the sixth PR of a series aiming to introduce Avro support to pmacct, breaking up #41 into smaller, easier to merge changes.
This PR focuses on changes to the Kafka plugin. It should therefore be reviewed after #51.",True,{}
pmacct/pmacct,https://github.com/pmacct/pmacct,58,2016-08-22T16:31:26Z,2016-09-08T16:12:22Z,2016-09-08T16:12:22Z,MERGED,True,92,27,7,https://github.com/weyfonk,Expose support for Apache Avro in the docs and UI,1,[],https://github.com/pmacct/pmacct/pull/58,https://github.com/weyfonk,1,https://github.com/pmacct/pmacct/pull/58,"This is the seventh PR of a series aiming to introduce Avro support to pmacct, breaking up #41 into smaller, easier to merge changes.
This PR focuses on exposing newly available features to the documentation and UI (ie. command-line parameters). It should therefore be reviewed after #53, #56 and #57.","This is the seventh PR of a series aiming to introduce Avro support to pmacct, breaking up #41 into smaller, easier to merge changes.
This PR focuses on exposing newly available features to the documentation and UI (ie. command-line parameters). It should therefore be reviewed after #53, #56 and #57.",True,{}
pmacct/pmacct,https://github.com/pmacct/pmacct,59,2016-08-22T16:35:12Z,2016-08-29T18:59:19Z,2016-08-29T18:59:19Z,MERGED,True,77,8,2,https://github.com/weyfonk,Expose support for Apache Avro in the AMQP and Kafka consumer examples,1,[],https://github.com/pmacct/pmacct/pull/59,https://github.com/weyfonk,1,https://github.com/pmacct/pmacct/pull/59,"This is the eighth and hopefully last PR of a series aiming to introduce Avro support to pmacct, breaking up #41 into smaller, easier to merge changes.
This PR focuses on exposing newly available features to the Python AMQP and Kafka consumer examples. It should therefore be reviewed after #58.","This is the eighth and hopefully last PR of a series aiming to introduce Avro support to pmacct, breaking up #41 into smaller, easier to merge changes.
This PR focuses on exposing newly available features to the Python AMQP and Kafka consumer examples. It should therefore be reviewed after #58.",True,{}
pmacct/pmacct,https://github.com/pmacct/pmacct,62,2016-09-16T00:18:43Z,2016-09-22T18:51:30Z,2016-09-23T03:03:22Z,MERGED,True,13,0,1,https://github.com/tlaberge,pmtelemetryd: Mask SIGCHLD during socket IO.,1,[],https://github.com/pmacct/pmacct/pull/62,https://github.com/tlaberge,1,https://github.com/pmacct/pmacct/pull/62,"If we happen to be blocked in recv() while a log dump happens,
recv() will fail with EINTR. This fix masks SIGCHLD during
socket IO and restores the original mask after the IO completes.","If we happen to be blocked in recv() while a log dump happens,
recv() will fail with EINTR. This fix masks SIGCHLD during
socket IO and restores the original mask after the IO completes.",True,{}
pmacct/pmacct,https://github.com/pmacct/pmacct,64,2016-10-11T03:44:19Z,2016-10-20T15:39:14Z,2017-05-01T00:04:17Z,MERGED,True,90,9,48,https://github.com/tlaberge,Compiler warnings: Fix up missing includes and prototypes,1,[],https://github.com/pmacct/pmacct/pull/64,https://github.com/tlaberge,1,https://github.com/pmacct/pmacct/pull/64,,,True,{}
pmacct/pmacct,https://github.com/pmacct/pmacct,66,2016-10-23T02:13:08Z,2020-08-02T14:11:55Z,2020-08-02T14:11:55Z,CLOSED,False,84,84,6,https://github.com/tlaberge,Fix all string format warnings on 64 bit Ubuntu,2,[],https://github.com/pmacct/pmacct/pull/66,https://github.com/tlaberge,1,https://github.com/pmacct/pmacct/pull/66,,,True,{}
pmacct/pmacct,https://github.com/pmacct/pmacct,67,2016-10-26T20:03:16Z,2016-10-27T09:35:27Z,2016-10-27T09:35:27Z,MERGED,True,1,1,1,https://github.com/pierky,New Early IANA Allocation,1,[],https://github.com/pmacct/pmacct/pull/67,https://github.com/pierky,1,https://github.com/pmacct/pmacct/pull/67,https://www.ietf.org/rfcdiff?url2=draft-ietf-idr-large-community-05,https://www.ietf.org/rfcdiff?url2=draft-ietf-idr-large-community-05,True,{}
pmacct/pmacct,https://github.com/pmacct/pmacct,70,2016-11-08T07:12:02Z,2016-11-14T03:24:42Z,2016-11-14T03:24:42Z,CLOSED,False,2,0,1,https://github.com/wcc526,need kafka version,1,[],https://github.com/pmacct/pmacct/pull/70,https://github.com/wcc526,1,https://github.com/pmacct/pmacct/pull/70,"https://github.com/edenhill/librdkafka/wiki/Broker-version-compatibility
kafka 0.8.2.1 need version compatibility","https://github.com/edenhill/librdkafka/wiki/Broker-version-compatibility
kafka 0.8.2.1 need version compatibility",True,{}
pmacct/pmacct,https://github.com/pmacct/pmacct,76,2016-12-06T18:05:45Z,2016-12-16T17:45:02Z,2016-12-16T17:45:02Z,MERGED,True,4,0,1,https://github.com/dfberger,fix linking when configured with --enable-avro,1,[],https://github.com/pmacct/pmacct/pull/76,https://github.com/dfberger,1,https://github.com/pmacct/pmacct/pull/76,,,True,{}
pmacct/pmacct,https://github.com/pmacct/pmacct,77,2016-12-06T18:06:36Z,2016-12-19T13:32:33Z,2016-12-19T13:32:33Z,MERGED,True,4,2,1,https://github.com/dfberger,don't read bgp_len past the end of the message buffer,2,[],https://github.com/pmacct/pmacct/pull/77,https://github.com/dfberger,1,https://github.com/pmacct/pmacct/pull/77,,,True,{}
pmacct/pmacct,https://github.com/pmacct/pmacct,78,2016-12-06T18:07:55Z,2017-02-01T17:35:03Z,2017-02-01T17:35:03Z,CLOSED,False,44,8,5,https://github.com/dfberger,a rather heavy handed fix to stop random memory scribbles,1,[],https://github.com/pmacct/pmacct/pull/78,https://github.com/dfberger,1,https://github.com/pmacct/pmacct/pull/78,"Adds mutexing around some of the data values in bgp_misc_db, which are being temporarily set on the main thread around line 128 of bgp_lookup.c and referenced in the worker thread in many places.
The impact of the bug was that if you happened to create a bgp_node while the main thread had set table_per_peer_buckets to 1 it had a significantly smaller info table than the rest of the code expected resulting in seg faults touching out of bounds memory (this could happen in the memset immediately following the malloc if you were unlucky)
I donâ€™t really understand why the main thread needs to twiddle these variables, so thereâ€™s very possibly a more elegant fix.","Adds mutexing around some of the data values in bgp_misc_db, which are being temporarily set on the main thread around line 128 of bgp_lookup.c and referenced in the worker thread in many places.
The impact of the bug was that if you happened to create a bgp_node while the main thread had set table_per_peer_buckets to 1 it had a significantly smaller info table than the rest of the code expected resulting in seg faults touching out of bounds memory (this could happen in the memset immediately following the malloc if you were unlucky)
I donâ€™t really understand why the main thread needs to twiddle these variables, so thereâ€™s very possibly a more elegant fix.",True,{}
pmacct/pmacct,https://github.com/pmacct/pmacct,85,2016-12-17T22:41:53Z,2016-12-18T15:04:13Z,2016-12-18T15:04:14Z,MERGED,True,3,3,1,https://github.com/vincentbernat,* uacctd: use current time when we don't have a timestamp from netlink,1,[],https://github.com/pmacct/pmacct/pull/85,https://github.com/vincentbernat,1,https://github.com/pmacct/pmacct/pull/85,"We only get a timestamp when there is a timestamp in the skb. Notably, locally generated packets don't get a timestamp. This would confuse other plugins.
Fix #83.","We only get a timestamp when there is a timestamp in the skb. Notably, locally generated packets don't get a timestamp. This would confuse other plugins.
Fix #83.",True,{}
pmacct/pmacct,https://github.com/pmacct/pmacct,85,2016-12-17T22:41:53Z,2016-12-18T15:04:13Z,2016-12-18T15:04:14Z,MERGED,True,3,3,1,https://github.com/vincentbernat,* uacctd: use current time when we don't have a timestamp from netlink,1,[],https://github.com/pmacct/pmacct/pull/85,https://github.com/vincentbernat,2,https://github.com/pmacct/pmacct/pull/85#issuecomment-267792051,"We only get a timestamp when there is a timestamp in the skb. Notably, locally generated packets don't get a timestamp. This would confuse other plugins.
Fix #83.","I am unsure if this is the right way to do that, performance-wise. Maybe there is a cached version?",True,{}
pmacct/pmacct,https://github.com/pmacct/pmacct,86,2016-12-18T14:34:29Z,2016-12-21T11:09:51Z,2016-12-21T11:09:51Z,MERGED,True,8,9,1,https://github.com/vincentbernat,* nfprobe: don't shortcut loop on poll() error,1,[],https://github.com/pmacct/pmacct/pull/86,https://github.com/vincentbernat,1,https://github.com/pmacct/pmacct/pull/86,"On exit, nfprobe plugin takes some time to shutdown because we have to
wait the default timeout (60s) before checking if we were asked to
gracefully exit. Instead, don't shortcut the loop on poll() error since
most of the code is correctly guarded a few lines after that.
This seems the simpler solution, but we could also make an exception for
graceful shutdown, or do the graceful shutdown in the signal
handler (like for SQL) or register a dedicated pipe to signal the
graceful shutdown to poll().","On exit, nfprobe plugin takes some time to shutdown because we have to
wait the default timeout (60s) before checking if we were asked to
gracefully exit. Instead, don't shortcut the loop on poll() error since
most of the code is correctly guarded a few lines after that.
This seems the simpler solution, but we could also make an exception for
graceful shutdown, or do the graceful shutdown in the signal
handler (like for SQL) or register a dedicated pipe to signal the
graceful shutdown to poll().",True,{}
pmacct/pmacct,https://github.com/pmacct/pmacct,86,2016-12-18T14:34:29Z,2016-12-21T11:09:51Z,2016-12-21T11:09:51Z,MERGED,True,8,9,1,https://github.com/vincentbernat,* nfprobe: don't shortcut loop on poll() error,1,[],https://github.com/pmacct/pmacct/pull/86,https://github.com/vincentbernat,2,https://github.com/pmacct/pmacct/pull/86#issuecomment-267991147,"On exit, nfprobe plugin takes some time to shutdown because we have to
wait the default timeout (60s) before checking if we were asked to
gracefully exit. Instead, don't shortcut the loop on poll() error since
most of the code is correctly guarded a few lines after that.
This seems the simpler solution, but we could also make an exception for
graceful shutdown, or do the graceful shutdown in the signal
handler (like for SQL) or register a dedicated pipe to signal the
graceful shutdown to poll().","With the current solution, the plugin won't bail out. It still needs the graceful_shutdown_request flag to be set, otherwise, the error will be treated as previously (except the Kafka/AMQP stuff will be executed).
Putting everything into a signal handler seems dangereous to me (usually, you have many stuff which are not safe  to do in a signal handler). Let me amend a bit the PR to have solution 2.",True,{}
pmacct/pmacct,https://github.com/pmacct/pmacct,87,2016-12-20T13:19:22Z,2017-04-02T15:37:01Z,2017-04-02T15:37:01Z,CLOSED,False,16,6,1,https://github.com/Thomas-Hn,Update uacctd.c / interface and direction,1,[],https://github.com/pmacct/pmacct/pull/87,https://github.com/Thomas-Hn,1,https://github.com/pmacct/pmacct/pull/87,"This is the first time I use GitHub and submit something. If this is wrong way, let me know ;)
If you have an router at a geographic site that has many interfaces, it will be hard to analyze traffic in tools like Netflowanalyzer because they are separated by interfaces. We are only interested in what traffic enters the whole site and leave the site through a satellite link. With this, the whole site appear as interface ""1"" This was the reason for this change. But this is probably not interesting for everyone.
This change sets the ifindex in netflow always to ""1""
the direction can be set dynamically via iptables rules. Just add --nflog-prefix <in|out> to the NFLOG target.","This is the first time I use GitHub and submit something. If this is wrong way, let me know ;)
If you have an router at a geographic site that has many interfaces, it will be hard to analyze traffic in tools like Netflowanalyzer because they are separated by interfaces. We are only interested in what traffic enters the whole site and leave the site through a satellite link. With this, the whole site appear as interface ""1"" This was the reason for this change. But this is probably not interesting for everyone.
This change sets the ifindex in netflow always to ""1""
the direction can be set dynamically via iptables rules. Just add --nflog-prefix <in|out> to the NFLOG target.",True,{}
pmacct/pmacct,https://github.com/pmacct/pmacct,96,2017-01-29T10:34:01Z,2017-01-29T10:34:31Z,2017-01-29T10:37:54Z,MERGED,True,2,1,1,https://github.com/paololucente,* BGP daemon: added support for draft-ietf-idr-shutdown-04,1,[],https://github.com/pmacct/pmacct/pull/96,https://github.com/paololucente,1,https://github.com/pmacct/pmacct/pull/96,,,True,{}
pmacct/pmacct,https://github.com/pmacct/pmacct,101,2017-02-21T11:46:30Z,2017-10-10T15:09:35Z,2017-10-21T13:27:28Z,MERGED,True,26,0,4,https://github.com/weyfonk,Support Kafka dynamic partitioning: config,1,[],https://github.com/pmacct/pmacct/pull/101,https://github.com/weyfonk,1,https://github.com/pmacct/pmacct/pull/101,"This commit includes changes to the configuration system. It is part of a
set of changes which purpose is to support dynamic Kafka partitioning.","This commit includes changes to the configuration system. It is part of a
set of changes which purpose is to support dynamic Kafka partitioning.",True,{'THUMBS_UP': ['https://github.com/jrossi']}
pmacct/pmacct,https://github.com/pmacct/pmacct,101,2017-02-21T11:46:30Z,2017-10-10T15:09:35Z,2017-10-21T13:27:28Z,MERGED,True,26,0,4,https://github.com/weyfonk,Support Kafka dynamic partitioning: config,1,[],https://github.com/pmacct/pmacct/pull/101,https://github.com/jrossi,2,https://github.com/pmacct/pmacct/pull/101#issuecomment-334530989,"This commit includes changes to the configuration system. It is part of a
set of changes which purpose is to support dynamic Kafka partitioning.","Any reason this is waiting ?  We need partition logic that this feature has to deal with the volume and I would like to not have to keep patching pmacct ;)
Once upstream it makes my life so much simpler.",True,{}
pmacct/pmacct,https://github.com/pmacct/pmacct,101,2017-02-21T11:46:30Z,2017-10-10T15:09:35Z,2017-10-21T13:27:28Z,MERGED,True,26,0,4,https://github.com/weyfonk,Support Kafka dynamic partitioning: config,1,[],https://github.com/pmacct/pmacct/pull/101,https://github.com/paololucente,3,https://github.com/pmacct/pmacct/pull/101#issuecomment-334945147,"This commit includes changes to the configuration system. It is part of a
set of changes which purpose is to support dynamic Kafka partitioning.","Hi @jrossi, thanks very much for your note. Let me make some progress on the merge next week. Paolo",True,{}
pmacct/pmacct,https://github.com/pmacct/pmacct,101,2017-02-21T11:46:30Z,2017-10-10T15:09:35Z,2017-10-21T13:27:28Z,MERGED,True,26,0,4,https://github.com/weyfonk,Support Kafka dynamic partitioning: config,1,[],https://github.com/pmacct/pmacct/pull/101,https://github.com/paololucente,4,https://github.com/pmacct/pmacct/pull/101#issuecomment-338394451,"This commit includes changes to the configuration system. It is part of a
set of changes which purpose is to support dynamic Kafka partitioning.","Hi @jrossi , the feature (PRs 101-104) was committed last week. With some reviews, refinements, scoping, etc. that did follow-up this week. With the commit of few mins ago i'm mostly done with this feature. I'd greatly appreciate if you can confirm it working for you. Any issues, feel free to reply here, open an issue or contact me via unicast email. Cheers, Paolo",True,{}
pmacct/pmacct,https://github.com/pmacct/pmacct,102,2017-02-21T11:48:52Z,2017-10-11T12:28:12Z,2017-10-11T12:28:12Z,MERGED,True,135,3,3,https://github.com/weyfonk,Support Kafka dynamic partitioning: libraries,1,[],https://github.com/pmacct/pmacct/pull/102,https://github.com/weyfonk,1,https://github.com/pmacct/pmacct/pull/102,"This pull request includes changes to the library code. It is part of a
set of changes which purpose is to support dynamic Kafka partitioning.
This PR should be reviewed after #101, since it requires changes in the configuration system.","This pull request includes changes to the library code. It is part of a
set of changes which purpose is to support dynamic Kafka partitioning.
This PR should be reviewed after #101, since it requires changes in the configuration system.",True,{'THUMBS_UP': ['https://github.com/jrossi']}
pmacct/pmacct,https://github.com/pmacct/pmacct,103,2017-02-21T11:50:21Z,2017-10-14T15:24:47Z,2017-10-14T15:24:47Z,MERGED,True,62,2,1,https://github.com/weyfonk,Support Kafka dynamic partitioning: plugin,1,[],https://github.com/pmacct/pmacct/pull/103,https://github.com/weyfonk,1,https://github.com/pmacct/pmacct/pull/103,"This pull request includes changes to the Kafka plugin. It is part of a
set of changes which purpose is to support dynamic Kafka partitioning.
This PR should be reviewed after #102 , since it requires changes in the library code.","This pull request includes changes to the Kafka plugin. It is part of a
set of changes which purpose is to support dynamic Kafka partitioning.
This PR should be reviewed after #102 , since it requires changes in the library code.",True,{'THUMBS_UP': ['https://github.com/jrossi']}
pmacct/pmacct,https://github.com/pmacct/pmacct,104,2017-02-21T11:51:10Z,2017-10-15T16:37:52Z,2017-10-15T16:37:52Z,MERGED,True,33,1,1,https://github.com/weyfonk,Support Kafka dynamic partitioning: docs,1,[],https://github.com/pmacct/pmacct/pull/104,https://github.com/weyfonk,1,https://github.com/pmacct/pmacct/pull/104,"This commit includes changes to the documentation. It is part of a
set of changes which purpose is to support dynamic Kafka partitioning.","This commit includes changes to the documentation. It is part of a
set of changes which purpose is to support dynamic Kafka partitioning.",True,{'THUMBS_UP': ['https://github.com/jrossi']}
pmacct/pmacct,https://github.com/pmacct/pmacct,105,2017-02-21T16:20:45Z,2017-04-11T15:35:54Z,2017-04-11T15:35:54Z,MERGED,True,22,0,4,https://github.com/weyfonk,Serialize nfacctd templates: config,1,[],https://github.com/pmacct/pmacct/pull/105,https://github.com/weyfonk,1,https://github.com/pmacct/pmacct/pull/105,"nfacctd templates can be cached to limit the amount of lost
Netflow/IPFIX packets due to unknown templates when nfacctd (re)starts.
This commit focuses on changes to the configuration system.","nfacctd templates can be cached to limit the amount of lost
Netflow/IPFIX packets due to unknown templates when nfacctd (re)starts.
This commit focuses on changes to the configuration system.",True,{}
pmacct/pmacct,https://github.com/pmacct/pmacct,106,2017-02-21T16:21:53Z,2017-04-10T23:54:38Z,2017-04-10T23:54:38Z,MERGED,True,45,0,2,https://github.com/weyfonk,Serialize nfacctd templates: libraries,1,[],https://github.com/pmacct/pmacct/pull/106,https://github.com/weyfonk,1,https://github.com/pmacct/pmacct/pull/106,"nfacctd templates can be cached to limit the amount of lost
Netflow/IPFIX packets due to unknown templates when nfacctd
(re)starts.
This commit focuses on changes to the library code.","nfacctd templates can be cached to limit the amount of lost
Netflow/IPFIX packets due to unknown templates when nfacctd
(re)starts.
This commit focuses on changes to the library code.",True,{}
pmacct/pmacct,https://github.com/pmacct/pmacct,107,2017-02-21T16:24:14Z,2017-04-13T16:42:57Z,2017-04-13T16:42:57Z,MERGED,True,744,6,3,https://github.com/weyfonk,Serialize nfacctd templates: nfacctd,1,[],https://github.com/pmacct/pmacct/pull/107,https://github.com/weyfonk,1,https://github.com/pmacct/pmacct/pull/107,"nfacctd templates can be cached to limit the amount of lost
Netflow/IPFIX packets due to unknown templates when nfacctd (re)starts.
This pull request focuses on changes to the nfacctd daemon.
Note: this pull request depends on library code changes and should therefore be reviewed after #106.","nfacctd templates can be cached to limit the amount of lost
Netflow/IPFIX packets due to unknown templates when nfacctd (re)starts.
This pull request focuses on changes to the nfacctd daemon.
Note: this pull request depends on library code changes and should therefore be reviewed after #106.",True,{}
pmacct/pmacct,https://github.com/pmacct/pmacct,108,2017-02-21T16:24:46Z,2017-04-17T18:36:11Z,2017-04-17T18:36:11Z,MERGED,True,10,0,1,https://github.com/weyfonk,Serialize nfacctd templates: docs,1,[],https://github.com/pmacct/pmacct/pull/108,https://github.com/weyfonk,1,https://github.com/pmacct/pmacct/pull/108,"nfacctd templates can be cached to limit the amount of lost
Netflow/IPFIX packets due to unknown templates when nfacctd (re)starts.
This commit focuses on changes to the documentation.","nfacctd templates can be cached to limit the amount of lost
Netflow/IPFIX packets due to unknown templates when nfacctd (re)starts.
This commit focuses on changes to the documentation.",True,{'THUMBS_UP': ['https://github.com/jrossi']}
pmacct/pmacct,https://github.com/pmacct/pmacct,110,2017-02-22T12:21:47Z,2017-02-25T11:52:40Z,2017-02-25T11:52:40Z,MERGED,True,4,4,1,https://github.com/acoul,src/isis: fix a musl-libc issue,1,[],https://github.com/pmacct/pmacct/pull/110,https://github.com/acoul,1,https://github.com/pmacct/pmacct/pull/110,according to this,according to this,True,{}
pmacct/pmacct,https://github.com/pmacct/pmacct,111,2017-02-23T09:44:00Z,,2021-03-23T23:48:00Z,OPEN,False,9,2,5,https://github.com/weyfonk,Add internal metrics support: build,1,[],https://github.com/pmacct/pmacct/pull/111,https://github.com/weyfonk,1,https://github.com/pmacct/pmacct/pull/111,"pmacct now supports periodic sending of internal use metrics to a StatsD instance.
This pull request focuses on build system changes.
Empty intstats.[ch] files have been added as placeholders for the
purposes of decomposition, to ensure the code still compiles with these
build systems changes. Actual versions of these files are submitted
in a later pull request.","pmacct now supports periodic sending of internal use metrics to a StatsD instance.
This pull request focuses on build system changes.
Empty intstats.[ch] files have been added as placeholders for the
purposes of decomposition, to ensure the code still compiles with these
build systems changes. Actual versions of these files are submitted
in a later pull request.",True,{}
pmacct/pmacct,https://github.com/pmacct/pmacct,112,2017-02-23T09:53:44Z,,2021-03-23T23:48:00Z,OPEN,False,206,0,5,https://github.com/weyfonk,Add internal metrics support: config,1,[],https://github.com/pmacct/pmacct/pull/112,https://github.com/weyfonk,1,https://github.com/pmacct/pmacct/pull/112,"pmacct now supports periodic sending of internal use metrics to a
StatsD instance. This pull request focuses on configuration changes.","pmacct now supports periodic sending of internal use metrics to a
StatsD instance. This pull request focuses on configuration changes.",True,{}
pmacct/pmacct,https://github.com/pmacct/pmacct,114,2017-02-23T10:00:00Z,,2021-03-23T23:48:00Z,OPEN,False,21,21,6,https://github.com/weyfonk,Add internal metrics support: libraries,1,[],https://github.com/pmacct/pmacct/pull/114,https://github.com/weyfonk,1,https://github.com/pmacct/pmacct/pull/114,"pmacct now supports periodic sending of internal use metrics to a
StatsD  instance. This pull request focuses on libraries changes.","pmacct now supports periodic sending of internal use metrics to a
StatsD  instance. This pull request focuses on libraries changes.",True,{}
pmacct/pmacct,https://github.com/pmacct/pmacct,115,2017-02-23T10:04:08Z,,2021-03-23T23:48:00Z,OPEN,False,600,0,2,https://github.com/weyfonk,Add internal metrics support: intstats,2,[],https://github.com/pmacct/pmacct/pull/115,https://github.com/weyfonk,1,https://github.com/pmacct/pmacct/pull/115,"pmacct now supports periodic sending of internal use metrics to a
StatsD instance. This pull request focuses on the intstats code, which can be called by daemons via a wrapper.","pmacct now supports periodic sending of internal use metrics to a
StatsD instance. This pull request focuses on the intstats code, which can be called by daemons via a wrapper.",True,{}
pmacct/pmacct,https://github.com/pmacct/pmacct,116,2017-02-23T10:08:53Z,,2021-03-23T23:48:00Z,OPEN,False,91,33,14,https://github.com/weyfonk,Add internal metrics support: plugins,1,[],https://github.com/pmacct/pmacct/pull/116,https://github.com/weyfonk,1,https://github.com/pmacct/pmacct/pull/116,"pmacct now supports periodic sending of internal use metrics to a
StatsD instance. This pull request focuses on changes to the plugins.
It depends on intstats.[ch], and should therefore be reviewed after #115.","pmacct now supports periodic sending of internal use metrics to a
StatsD instance. This pull request focuses on changes to the plugins.
It depends on intstats.[ch], and should therefore be reviewed after #115.",True,{}
pmacct/pmacct,https://github.com/pmacct/pmacct,117,2017-02-23T10:10:16Z,,2021-03-23T23:48:00Z,OPEN,False,184,0,3,https://github.com/weyfonk,Add internal metrics support: nfacctd,1,[],https://github.com/pmacct/pmacct/pull/117,https://github.com/weyfonk,1,https://github.com/pmacct/pmacct/pull/117,"pmacct now supports periodic sending of internal use metrics to a
StatsD instance. This pull request focuses on changes to nfacctd.
It depends on intstats.[ch], and should therefore be reviewed after #115.","pmacct now supports periodic sending of internal use metrics to a
StatsD instance. This pull request focuses on changes to nfacctd.
It depends on intstats.[ch], and should therefore be reviewed after #115.",True,{}
pmacct/pmacct,https://github.com/pmacct/pmacct,118,2017-02-23T10:11:09Z,,2021-03-23T23:48:00Z,OPEN,False,96,8,2,https://github.com/weyfonk,Add internal metrics support: docs,1,[],https://github.com/pmacct/pmacct/pull/118,https://github.com/weyfonk,1,https://github.com/pmacct/pmacct/pull/118,"pmacct now supports periodic sending of internal use metrics to a
StatsD instance. This pull request focuses on changes to the documentation.","pmacct now supports periodic sending of internal use metrics to a
StatsD instance. This pull request focuses on changes to the documentation.",True,{'THUMBS_UP': ['https://github.com/jrossi']}
pmacct/pmacct,https://github.com/pmacct/pmacct,120,2017-02-25T12:34:14Z,2017-02-26T10:54:40Z,2017-02-26T10:54:40Z,CLOSED,False,8,0,1,https://github.com/iddq,Add build example to README.pgsql,2,[],https://github.com/pmacct/pmacct/pull/120,https://github.com/iddq,1,https://github.com/pmacct/pmacct/pull/120,,,True,{}
pmacct/pmacct,https://github.com/pmacct/pmacct,122,2017-03-03T13:26:12Z,2017-03-03T21:15:48Z,2017-03-03T21:15:48Z,MERGED,True,3,3,1,https://github.com/pawmal,* IMT plugin: fix lru_elem_ptr heap corruption,1,[],https://github.com/pmacct/pmacct/pull/122,https://github.com/pawmal,1,https://github.com/pmacct/pmacct/pull/122,,,True,{}
pmacct/pmacct,https://github.com/pmacct/pmacct,123,2017-03-05T15:10:48Z,2017-03-05T15:29:03Z,2017-03-10T19:28:44Z,MERGED,True,58,49,6,https://github.com/fooelisa,enabling ipv6 support by default (use --disable-ipv6 instead),1,[],https://github.com/pmacct/pmacct/pull/123,https://github.com/fooelisa,1,https://github.com/pmacct/pmacct/pull/123,,,True,{}
pmacct/pmacct,https://github.com/pmacct/pmacct,123,2017-03-05T15:10:48Z,2017-03-05T15:29:03Z,2017-03-10T19:28:44Z,MERGED,True,58,49,6,https://github.com/fooelisa,enabling ipv6 support by default (use --disable-ipv6 instead),1,[],https://github.com/pmacct/pmacct/pull/123,https://github.com/fooelisa,2,https://github.com/pmacct/pmacct/pull/123#issuecomment-284234289,,"based on a discussion with paolo, this should be on by default these days",True,{}
pmacct/pmacct,https://github.com/pmacct/pmacct,123,2017-03-05T15:10:48Z,2017-03-05T15:29:03Z,2017-03-10T19:28:44Z,MERGED,True,58,49,6,https://github.com/fooelisa,enabling ipv6 support by default (use --disable-ipv6 instead),1,[],https://github.com/pmacct/pmacct/pull/123,https://github.com/jejenone,3,https://github.com/pmacct/pmacct/pull/123#issuecomment-285762164,,nice!,True,{}
pmacct/pmacct,https://github.com/pmacct/pmacct,127,2017-03-30T20:11:19Z,2017-03-31T10:56:12Z,2017-03-31T10:56:12Z,MERGED,True,5,0,1,https://github.com/kunschikov,fix of the memory leak during expired records export,1,[],https://github.com/pmacct/pmacct/pull/127,https://github.com/kunschikov,1,https://github.com/pmacct/pmacct/pull/127,Memleak can be reproduced by export to the unavailable collector. If there is no route to the collector host there will be no connection and all expired records should be freed.,Memleak can be reproduced by export to the unavailable collector. If there is no route to the collector host there will be no connection and all expired records should be freed.,True,{}
pmacct/pmacct,https://github.com/pmacct/pmacct,129,2017-04-06T09:47:41Z,2017-04-06T12:36:34Z,2017-04-06T12:36:34Z,MERGED,True,3,3,1,https://github.com/FvDxxx,Fix segfault with add-path enabled peers.,1,[],https://github.com/pmacct/pmacct/pull/129,https://github.com/FvDxxx,1,https://github.com/pmacct/pmacct/pull/129,Thanks to Paolo for help me offline to get here.,Thanks to Paolo for help me offline to get here.,True,{}
pmacct/pmacct,https://github.com/pmacct/pmacct,132,2017-05-11T17:03:12Z,2017-05-12T13:05:02Z,2017-05-12T13:05:02Z,MERGED,True,15,10,1,https://github.com/Matt-Texier,Update of quickstart document taking into account Kafka single node design,2,[],https://github.com/pmacct/pmacct/pull/132,https://github.com/Matt-Texier,1,https://github.com/pmacct/pmacct/pull/132,,,True,{}
pmacct/pmacct,https://github.com/pmacct/pmacct,133,2017-05-16T10:55:47Z,2017-05-16T21:10:44Z,2017-05-16T21:10:44Z,MERGED,True,1215,30,8,https://github.com/vittoriofoschi,add nDPI support,12,[],https://github.com/pmacct/pmacct/pull/133,https://github.com/vittoriofoschi,1,https://github.com/pmacct/pmacct/pull/133,,,True,{}
pmacct/pmacct,https://github.com/pmacct/pmacct,138,2017-06-29T14:59:45Z,2017-06-30T00:16:02Z,2017-06-30T00:16:02Z,MERGED,True,1,1,1,https://github.com/marado,Fixed Markdown,1,[],https://github.com/pmacct/pmacct/pull/138,https://github.com/marado,1,https://github.com/pmacct/pmacct/pull/138,,,True,{}
pmacct/pmacct,https://github.com/pmacct/pmacct,139,2017-07-07T11:25:14Z,2017-07-08T21:40:54Z,2017-07-08T21:40:54Z,MERGED,True,127,4,5,https://github.com/mrevilme,Adding support for forwardingstatus flag from netflow.,3,[],https://github.com/pmacct/pmacct/pull/139,https://github.com/mrevilme,1,https://github.com/pmacct/pmacct/pull/139,,,True,{}
pmacct/pmacct,https://github.com/pmacct/pmacct,146,2017-08-11T00:54:55Z,2017-08-12T09:54:21Z,2017-08-14T02:33:53Z,MERGED,True,5,0,1,https://github.com/mikejager,* fix whitespace on peer_src_ip/peer_dst_ip headers,1,[],https://github.com/pmacct/pmacct/pull/146,https://github.com/mikejager,1,https://github.com/pmacct/pmacct/pull/146,"Correct the width of the print plugin's PEER_SRC_IP and PEER_DST_IP stats
headers when print_output is 'formatted' and IPv6 support is compiled in.","Correct the width of the print plugin's PEER_SRC_IP and PEER_DST_IP stats
headers when print_output is 'formatted' and IPv6 support is compiled in.",True,{}
pmacct/pmacct,https://github.com/pmacct/pmacct,147,2017-08-11T02:03:03Z,,2021-03-23T23:48:00Z,OPEN,False,129,2,12,https://github.com/mikejager,* add src_vlan and dst_vlan primitives for sFlow,1,[],https://github.com/pmacct/pmacct/pull/147,https://github.com/mikejager,1,https://github.com/pmacct/pmacct/pull/147,"The existing 'vlan' primitive handler for sFlow selects the ingress VLAN ID,
unless it is not present, in which case the egress VLAN ID is used. Two new
primitives ('src_vlan' and 'dst_vlan') are added so that both ingress and
egress VLAN IDs are always available.","The existing 'vlan' primitive handler for sFlow selects the ingress VLAN ID,
unless it is not present, in which case the egress VLAN ID is used. Two new
primitives ('src_vlan' and 'dst_vlan') are added so that both ingress and
egress VLAN IDs are always available.",True,{}
pmacct/pmacct,https://github.com/pmacct/pmacct,147,2017-08-11T02:03:03Z,,2021-03-23T23:48:00Z,OPEN,False,129,2,12,https://github.com/mikejager,* add src_vlan and dst_vlan primitives for sFlow,1,[],https://github.com/pmacct/pmacct/pull/147,https://github.com/mikejager,2,https://github.com/pmacct/pmacct/pull/147#issuecomment-322089828,"The existing 'vlan' primitive handler for sFlow selects the ingress VLAN ID,
unless it is not present, in which case the egress VLAN ID is used. Two new
primitives ('src_vlan' and 'dst_vlan') are added so that both ingress and
egress VLAN IDs are always available.","pmacct/src/pkt_handlers.c
    
    
         Line 4661
      in
      3757ced
    
  
  
    

        
          
           if (!pdata->primitives.vlan_id) pdata->primitives.vlan_id = sample->out_vlan; 
        
    
  

 sets the 'vlan' primitive to the destination VLAN ID if the source VLAN ID is zero.
In the case where a frame enters a device untagged on port A, and exits the device tagged with VLAN 1234 on port B, wouldn't what you're suggesting result in the [src_]vlan primitive being incorrectly set to the 1234 rather than 0?",True,{}
pmacct/pmacct,https://github.com/pmacct/pmacct,147,2017-08-11T02:03:03Z,,2021-03-23T23:48:00Z,OPEN,False,129,2,12,https://github.com/mikejager,* add src_vlan and dst_vlan primitives for sFlow,1,[],https://github.com/pmacct/pmacct/pull/147,https://github.com/mikejager,3,https://github.com/pmacct/pmacct/pull/147#issuecomment-454871664,"The existing 'vlan' primitive handler for sFlow selects the ingress VLAN ID,
unless it is not present, in which case the egress VLAN ID is used. Two new
primitives ('src_vlan' and 'dst_vlan') are added so that both ingress and
egress VLAN IDs are always available.","Having src_vlan set COUNT_INT_VLAN may be confusing for users. They will have requested the src_vlan primitive, but this will be replaced with the vlan primitive on output.
If you want to avoid creating both a src_vlan and dst_vlan primitive, it may be better to drop the src_vlan primitive entirely; users would request vlan and dst_vlan instead. What do you think?",True,{}
pmacct/pmacct,https://github.com/pmacct/pmacct,147,2017-08-11T02:03:03Z,,2021-03-23T23:48:00Z,OPEN,False,129,2,12,https://github.com/mikejager,* add src_vlan and dst_vlan primitives for sFlow,1,[],https://github.com/pmacct/pmacct/pull/147,https://github.com/paololucente,4,https://github.com/pmacct/pmacct/pull/147#issuecomment-455281441,"The existing 'vlan' primitive handler for sFlow selects the ingress VLAN ID,
unless it is not present, in which case the egress VLAN ID is used. Two new
primitives ('src_vlan' and 'dst_vlan') are added so that both ingress and
egress VLAN IDs are always available.","Hi Mike,
Re-thinking this after some time: i propose to drop src_ and dst_ prefixes in favour of in_ and out_ ones: it aligns to interface primitives, ie. in_iface and out_iface, and overall it aligns to sFlow. Agree it looks more natural?
Then, second point, backward compatibility (we have been circling around this in the previous comments): we have the existing 'vlan' primitive which we should alias on either of in_vlan or out_vlan. You essentially propose to alias it on in_vlan and i am totally fine with it. I can complement this change with some documentation.
Paolo",True,{}
pmacct/pmacct,https://github.com/pmacct/pmacct,147,2017-08-11T02:03:03Z,,2021-03-23T23:48:00Z,OPEN,False,129,2,12,https://github.com/mikejager,* add src_vlan and dst_vlan primitives for sFlow,1,[],https://github.com/pmacct/pmacct/pull/147,https://github.com/mikejager,5,https://github.com/pmacct/pmacct/pull/147#issuecomment-455764678,"The existing 'vlan' primitive handler for sFlow selects the ingress VLAN ID,
unless it is not present, in which case the egress VLAN ID is used. Two new
primitives ('src_vlan' and 'dst_vlan') are added so that both ingress and
egress VLAN IDs are always available.","Hi @paololucente,
No problem replacing src_ and dst_ with in_ and out_.
How to make the change backward compatible is what I'm still unsure about. Assuming we alias vlan on to in_vlan (which seems to make sense given that the ""in"" VLAN is the first to be used for vlan), are you suggesting replacing vlan with in_vlan but keeping vlan for backwards compatibility (perhaps for sFlow only?).",True,{}
pmacct/pmacct,https://github.com/pmacct/pmacct,147,2017-08-11T02:03:03Z,,2021-03-23T23:48:00Z,OPEN,False,129,2,12,https://github.com/mikejager,* add src_vlan and dst_vlan primitives for sFlow,1,[],https://github.com/pmacct/pmacct/pull/147,https://github.com/paololucente,6,https://github.com/pmacct/pmacct/pull/147#issuecomment-455775556,"The existing 'vlan' primitive handler for sFlow selects the ingress VLAN ID,
unless it is not present, in which case the egress VLAN ID is used. Two new
primitives ('src_vlan' and 'dst_vlan') are added so that both ingress and
egress VLAN IDs are always available.","Hi Mike, perfect about in_ and out_ and entirely correct about the aliasing part.",True,{}
pmacct/pmacct,https://github.com/pmacct/pmacct,151,2017-08-24T05:39:32Z,2017-09-19T13:09:20Z,2017-09-22T00:14:08Z,CLOSED,False,83,10,7,https://github.com/gunkaaa,Permit use of INET6_ATON() for AF_INET using sql_num_hosts and sql_num_ipv4_inet6aton,1,[],https://github.com/pmacct/pmacct/pull/151,https://github.com/gunkaaa,1,https://github.com/pmacct/pmacct/pull/151,"INET6_ATON() allows representation of both AF_INET (as VARBINARY(4)) and AF_INET6 (as VARBINARY(16)), allowing either to be stored in VARBINARY(16). However, INET6_NTOA() cannot handle the INT(4) representation produced by INET_ATON(). https://dev.mysql.com/doc/refman/5.6/en/miscellaneous-functions.html#function_inet6-aton
This commit allows use of INET6_ATON() instead of INET_ATON() for numerical representation of IPv4 addresses.","INET6_ATON() allows representation of both AF_INET (as VARBINARY(4)) and AF_INET6 (as VARBINARY(16)), allowing either to be stored in VARBINARY(16). However, INET6_NTOA() cannot handle the INT(4) representation produced by INET_ATON(). https://dev.mysql.com/doc/refman/5.6/en/miscellaneous-functions.html#function_inet6-aton
This commit allows use of INET6_ATON() instead of INET_ATON() for numerical representation of IPv4 addresses.",True,{}
pmacct/pmacct,https://github.com/pmacct/pmacct,151,2017-08-24T05:39:32Z,2017-09-19T13:09:20Z,2017-09-22T00:14:08Z,CLOSED,False,83,10,7,https://github.com/gunkaaa,Permit use of INET6_ATON() for AF_INET using sql_num_hosts and sql_num_ipv4_inet6aton,1,[],https://github.com/pmacct/pmacct/pull/151,https://github.com/gunkaaa,2,https://github.com/pmacct/pmacct/pull/151#issuecomment-325259851,"INET6_ATON() allows representation of both AF_INET (as VARBINARY(4)) and AF_INET6 (as VARBINARY(16)), allowing either to be stored in VARBINARY(16). However, INET6_NTOA() cannot handle the INT(4) representation produced by INET_ATON(). https://dev.mysql.com/doc/refman/5.6/en/miscellaneous-functions.html#function_inet6-aton
This commit allows use of INET6_ATON() instead of INET_ATON() for numerical representation of IPv4 addresses.","Hi,
I'm happy to have sql_num_inetaton, with an inverse meaning to sql_num_ipv4_inet6aton.
In the context of MySQL/MariaDB, there are a few things which need to be done before this can be made default:

The introduction of several variable-sized fields (the only existing one being label) carries heavy performance penalties under both InnoDB (VARBINARY is stored off-table) and under MyISAM with ROW_FORMAT=(DYNAMIC|COMPRESSED). sql/README.mysql and sql/pmacct-create-db*.mysql need to be updated to recommend MyISAM with ROW_FORMAT=FIXED, as some quick testing shows performance roughly halving by default; the disk tradeoff should acceptable as VARBINARY(16) is only two bytes larger than CHAR(15) as used by IPv4 without sql_num_hosts.
#123 enabled IPv6 by default, but there was no change of default host type in sql/README.(mysql|sqlite3) or in the accounting table templates from CHAR(15) NOT NULL to CHAR(45) NOT NULL. With the changes in sql_mode to STRICT_TRANS_TABLES in MySQL 5.7.5 and MariaDB 10.2.4, this could cause inserts with sql_multi_values to fail when IPv6 is accounted for along with IPv4 (previously v6 records were just truncated).

Given the above, it seems that sql/README.mysql should be updated to:
* src_host => ip_src (CHAR(45) NOT NULL)
  - or (CHAR(15) NOT NULL, if --disable-ipv6)
  - or (VARBINARY(16) NOT NULL, if sql_num_hosts: true)
  - or (VARBINARY(4) NOT NULL, if sql_num_hosts: true and --disable-ipv6)
  - or (INT(4) UNSIGNED NOT NULL, if sql_num_hosts: true and --disable-ipv6 and sql_num_inetaton: false)

Let me know your thoughts and I'll update this PR.",True,{}
pmacct/pmacct,https://github.com/pmacct/pmacct,153,2017-08-30T14:42:59Z,2017-08-30T21:38:11Z,2017-12-28T08:17:08Z,MERGED,True,72,4,6,https://github.com/buytenh,* Add support for calling pcap_set_protocol() if supported by libpcap.,1,[],https://github.com/pmacct/pmacct/pull/153,https://github.com/buytenh,1,https://github.com/pmacct/pmacct/pull/153,,,True,{}
pmacct/pmacct,https://github.com/pmacct/pmacct,154,2017-09-01T10:49:12Z,2017-09-01T14:02:31Z,2017-09-01T14:43:36Z,MERGED,True,7,0,1,https://github.com/buytenh,* Add documentation for the pcap_protocol config key.,1,[],https://github.com/pmacct/pmacct/pull/154,https://github.com/buytenh,1,https://github.com/pmacct/pmacct/pull/154,,,True,{}
pmacct/pmacct,https://github.com/pmacct/pmacct,160,2017-09-21T08:37:28Z,2017-09-21T14:22:34Z,2017-09-21T14:22:34Z,MERGED,True,35,10,3,https://github.com/gunkaaa,Update docs for sql_num_hosts now using INET6_ATON(),2,[],https://github.com/pmacct/pmacct/pull/160,https://github.com/gunkaaa,1,https://github.com/pmacct/pmacct/pull/160,"Hi,
There are several changes to documentation required for the recent sql_num_hosts change.","Hi,
There are several changes to documentation required for the recent sql_num_hosts change.",True,{}
pmacct/pmacct,https://github.com/pmacct/pmacct,161,2017-10-11T13:29:43Z,2017-10-17T14:29:16Z,2017-10-17T14:29:16Z,MERGED,True,8,8,8,https://github.com/hvanderheide,Change parent alive check to work with Docker,2,[],https://github.com/pmacct/pmacct/pull/161,https://github.com/hvanderheide,1,https://github.com/pmacct/pmacct/pull/161,"The origional code checked if the parent PID was equal to 1 (init) to
detect if the parent process died. Docker containers only launch the
required processes which would make the pmacctd core process PID 1,
failing the check.
During initialization of the plugin the core PID is already saved. We
can use this to compare it with the current PID which should have the
same intended effect and still works with Docker.
If this solution is accepted it should be duplicated to the other
plugins.","The origional code checked if the parent PID was equal to 1 (init) to
detect if the parent process died. Docker containers only launch the
required processes which would make the pmacctd core process PID 1,
failing the check.
During initialization of the plugin the core PID is already saved. We
can use this to compare it with the current PID which should have the
same intended effect and still works with Docker.
If this solution is accepted it should be duplicated to the other
plugins.",True,{}
pmacct/pmacct,https://github.com/pmacct/pmacct,161,2017-10-11T13:29:43Z,2017-10-17T14:29:16Z,2017-10-17T14:29:16Z,MERGED,True,8,8,8,https://github.com/hvanderheide,Change parent alive check to work with Docker,2,[],https://github.com/pmacct/pmacct/pull/161,https://github.com/hvanderheide,2,https://github.com/pmacct/pmacct/pull/161#issuecomment-336424428,"The origional code checked if the parent PID was equal to 1 (init) to
detect if the parent process died. Docker containers only launch the
required processes which would make the pmacctd core process PID 1,
failing the check.
During initialization of the plugin the core PID is already saved. We
can use this to compare it with the current PID which should have the
same intended effect and still works with Docker.
If this solution is accepted it should be duplicated to the other
plugins.","Hi Paolo, thanks for reviewing so quickly. I've implemented the fix on the other plugins. Regards, - Hidde",True,{}
pmacct/pmacct,https://github.com/pmacct/pmacct,164,2017-10-24T10:24:09Z,2017-10-25T11:34:25Z,2017-10-25T11:34:25Z,MERGED,True,6,2,1,https://github.com/dsgwork,* pkt_handlers.c: Avoid underflow when sysUptime < first or last for NF5,1,[],https://github.com/pmacct/pmacct/pull/164,https://github.com/dsgwork,1,https://github.com/pmacct/pmacct/pull/164,Fix underflow in NF_time_msecs_handler. See #163 for details.,Fix underflow in NF_time_msecs_handler. See #163 for details.,True,{}
pmacct/pmacct,https://github.com/pmacct/pmacct,166,2017-11-07T15:19:11Z,2017-11-07T17:21:44Z,2017-11-07T17:21:44Z,MERGED,True,1,1,1,https://github.com/vincentbernat,* examples: fix out-of-tree build of examples/lg,1,[],https://github.com/pmacct/pmacct/pull/166,https://github.com/vincentbernat,1,https://github.com/pmacct/pmacct/pull/166,"Out-of-tree build is when you create a build/ directory, then use ../configure. Useful to easily clean a tree.","Out-of-tree build is when you create a build/ directory, then use ../configure. Useful to easily clean a tree.",True,{}
pmacct/pmacct,https://github.com/pmacct/pmacct,168,2017-11-08T18:48:16Z,2017-11-10T11:22:39Z,2017-11-10T11:22:39Z,MERGED,True,1,1,1,https://github.com/cozonac,send add-path capability to neighbor if neighbor supports it,1,[],https://github.com/pmacct/pmacct/pull/168,https://github.com/cozonac,1,https://github.com/pmacct/pmacct/pull/168,send add-path capability to neighbor if neighbor supports send or send/receive,send add-path capability to neighbor if neighbor supports send or send/receive,True,{}
pmacct/pmacct,https://github.com/pmacct/pmacct,172,2017-12-05T20:03:18Z,2017-12-07T13:46:46Z,2017-12-08T14:27:18Z,MERGED,True,2,2,1,https://github.com/ivanfmartinez,check for zero time before formatting,1,[],https://github.com/pmacct/pmacct/pull/172,https://github.com/ivanfmartinez,1,https://github.com/pmacct/pmacct/pull/172,"I'm using nfacctd with ipfix flows (from mikrotik) and print plugin, and the dynamic filename was created using 1970 date.
With this fix now it uses the current timestamp.","I'm using nfacctd with ipfix flows (from mikrotik) and print plugin, and the dynamic filename was created using 1970 date.
With this fix now it uses the current timestamp.",True,{}
pmacct/pmacct,https://github.com/pmacct/pmacct,175,2017-12-30T17:47:15Z,2017-12-30T19:27:57Z,2017-12-30T19:27:57Z,MERGED,True,31,1,6,https://github.com/vadimtk,Adding the support of a custom MySQL port,2,[],https://github.com/pmacct/pmacct/pull/175,https://github.com/vadimtk,1,https://github.com/pmacct/pmacct/pull/175,Useful when MySQL is listening not on default port,Useful when MySQL is listening not on default port,True,{}
pmacct/pmacct,https://github.com/pmacct/pmacct,178,2018-01-05T05:01:13Z,2018-01-06T11:20:43Z,2018-01-09T03:20:48Z,MERGED,True,1,1,1,https://github.com/mikiT,fix a bracket correspondence in addr.c.,1,[],https://github.com/pmacct/pmacct/pull/178,https://github.com/mikiT,1,https://github.com/pmacct/pmacct/pull/178,"I tried to build with --disable-ipv6, but I saw an error.
addr.c: In function 'sa_to_str':
addr.c:745:1: error: expected declaration or statement at end of input
 }
 ^
addr.c:745:1: warning: control reaches end of non-void function [-Wreturn-type]
 }
 ^

addr.c has a problem with bracket correspondence in sa_to_str().","I tried to build with --disable-ipv6, but I saw an error.
addr.c: In function 'sa_to_str':
addr.c:745:1: error: expected declaration or statement at end of input
 }
 ^
addr.c:745:1: warning: control reaches end of non-void function [-Wreturn-type]
 }
 ^

addr.c has a problem with bracket correspondence in sa_to_str().",True,{}
pmacct/pmacct,https://github.com/pmacct/pmacct,179,2018-01-10T10:22:54Z,2018-05-27T13:41:51Z,2018-05-27T13:41:51Z,CLOSED,False,2,2,1,https://github.com/hvanderheide,Fix for changed nDPI IPv6 header structs,1,[],https://github.com/pmacct/pmacct/pull/179,https://github.com/hvanderheide,1,https://github.com/pmacct/pmacct/pull/179,"See: ntop/nDPI@61bc528#diff-45a58cbbe3f1d81fb81861c46712803f
Although this fixes compilation with the latest development version of nDPI it will fail all previous versions of nDPI. I'm not sure how to support both versions of the hearder struct.","See: ntop/nDPI@61bc528#diff-45a58cbbe3f1d81fb81861c46712803f
Although this fixes compilation with the latest development version of nDPI it will fail all previous versions of nDPI. I'm not sure how to support both versions of the hearder struct.",True,{}
pmacct/pmacct,https://github.com/pmacct/pmacct,179,2018-01-10T10:22:54Z,2018-05-27T13:41:51Z,2018-05-27T13:41:51Z,CLOSED,False,2,2,1,https://github.com/hvanderheide,Fix for changed nDPI IPv6 header structs,1,[],https://github.com/pmacct/pmacct/pull/179,https://github.com/sebschrader,2,https://github.com/pmacct/pmacct/pull/179#issuecomment-356577864,"See: ntop/nDPI@61bc528#diff-45a58cbbe3f1d81fb81861c46712803f
Although this fixes compilation with the latest development version of nDPI it will fail all previous versions of nDPI. I'm not sure how to support both versions of the hearder struct.",You can use autoconf's AC_CHECK_MEMBER check to check for the presence of ip6_hdr.,True,{}
pmacct/pmacct,https://github.com/pmacct/pmacct,179,2018-01-10T10:22:54Z,2018-05-27T13:41:51Z,2018-05-27T13:41:51Z,CLOSED,False,2,2,1,https://github.com/hvanderheide,Fix for changed nDPI IPv6 header structs,1,[],https://github.com/pmacct/pmacct/pull/179,https://github.com/paololucente,3,https://github.com/pmacct/pmacct/pull/179#issuecomment-359591249,"See: ntop/nDPI@61bc528#diff-45a58cbbe3f1d81fb81861c46712803f
Although this fixes compilation with the latest development version of nDPI it will fail all previous versions of nDPI. I'm not sure how to support both versions of the hearder struct.","Hi @hvanderheide , do you need any help with the autoconf part for this? Or it is all under control? Or you gave up and prefer to close the PR? Paolo",True,{}
pmacct/pmacct,https://github.com/pmacct/pmacct,179,2018-01-10T10:22:54Z,2018-05-27T13:41:51Z,2018-05-27T13:41:51Z,CLOSED,False,2,2,1,https://github.com/hvanderheide,Fix for changed nDPI IPv6 header structs,1,[],https://github.com/pmacct/pmacct/pull/179,https://github.com/hvanderheide,4,https://github.com/pmacct/pmacct/pull/179#issuecomment-359712673,"See: ntop/nDPI@61bc528#diff-45a58cbbe3f1d81fb81861c46712803f
Although this fixes compilation with the latest development version of nDPI it will fail all previous versions of nDPI. I'm not sure how to support both versions of the hearder struct.","Hi @paololucente, I think I'll manage but havn't gotten around to implement yet. Hopefully later this week.",True,{}
pmacct/pmacct,https://github.com/pmacct/pmacct,179,2018-01-10T10:22:54Z,2018-05-27T13:41:51Z,2018-05-27T13:41:51Z,CLOSED,False,2,2,1,https://github.com/hvanderheide,Fix for changed nDPI IPv6 header structs,1,[],https://github.com/pmacct/pmacct/pull/179,https://github.com/paololucente,5,https://github.com/pmacct/pmacct/pull/179#issuecomment-392331933,"See: ntop/nDPI@61bc528#diff-45a58cbbe3f1d81fb81861c46712803f
Although this fixes compilation with the latest development version of nDPI it will fail all previous versions of nDPI. I'm not sure how to support both versions of the hearder struct.",Fixed code to compile against 2.2  9fdef7e and checking nDPI is >= 2.2 when building 9bfc08f,True,{}
pmacct/pmacct,https://github.com/pmacct/pmacct,181,2018-01-24T23:53:42Z,2018-01-26T06:27:21Z,2018-01-26T06:27:21Z,MERGED,True,2,6,2,https://github.com/higgsd,Fix trigger script execution,1,[],https://github.com/pmacct/pmacct/pull/181,https://github.com/higgsd,1,https://github.com/pmacct/pmacct/pull/181,"First argument to execv should be path to executable.  Otherwise this
causes execv(3) to fail and return EFAULT on OpenBSD.","First argument to execv should be path to executable.  Otherwise this
causes execv(3) to fail and return EFAULT on OpenBSD.",True,{}
pmacct/pmacct,https://github.com/pmacct/pmacct,196,2018-05-03T13:22:05Z,2018-05-04T11:28:57Z,2018-05-27T13:42:46Z,MERGED,True,4,5,2,https://github.com/pldubouilh,Fix pcap iterface-map lookup,1,[],https://github.com/pmacct/pmacct/pull/196,https://github.com/pldubouilh,1,https://github.com/pmacct/pmacct/pull/196,"So eth1 and eth10 are not mixed up anymore ðŸ‘
It was mostly an issue when the map was reloaded, as it made libpcap re-registering the same interface.","So eth1 and eth10 are not mixed up anymore ðŸ‘
It was mostly an issue when the map was reloaded, as it made libpcap re-registering the same interface.",True,{}
pmacct/pmacct,https://github.com/pmacct/pmacct,196,2018-05-03T13:22:05Z,2018-05-04T11:28:57Z,2018-05-27T13:42:46Z,MERGED,True,4,5,2,https://github.com/pldubouilh,Fix pcap iterface-map lookup,1,[],https://github.com/pmacct/pmacct/pull/196,https://github.com/paololucente,2,https://github.com/pmacct/pmacct/pull/196#issuecomment-386590556,"So eth1 and eth10 are not mixed up anymore ðŸ‘
It was mostly an issue when the map was reloaded, as it made libpcap re-registering the same interface.","Hi @pldubouilh , thanks very much for this. May i ask you the favour to to the same PR against the 1.7.1 branch? Otherwise i'll find a way to back port it. Paolo",True,{}
pmacct/pmacct,https://github.com/pmacct/pmacct,196,2018-05-03T13:22:05Z,2018-05-04T11:28:57Z,2018-05-27T13:42:46Z,MERGED,True,4,5,2,https://github.com/pldubouilh,Fix pcap iterface-map lookup,1,[],https://github.com/pmacct/pmacct/pull/196,https://github.com/pldubouilh,3,https://github.com/pmacct/pmacct/pull/196#issuecomment-386591184,"So eth1 and eth10 are not mixed up anymore ðŸ‘
It was mostly an issue when the map was reloaded, as it made libpcap re-registering the same interface.",Hey @paololucente - sure doing so now. Thanks !,True,{}
pmacct/pmacct,https://github.com/pmacct/pmacct,196,2018-05-03T13:22:05Z,2018-05-04T11:28:57Z,2018-05-27T13:42:46Z,MERGED,True,4,5,2,https://github.com/pldubouilh,Fix pcap iterface-map lookup,1,[],https://github.com/pmacct/pmacct/pull/196,https://github.com/paololucente,4,https://github.com/pmacct/pmacct/pull/196#issuecomment-392331985,"So eth1 and eth10 are not mixed up anymore ðŸ‘
It was mostly an issue when the map was reloaded, as it made libpcap re-registering the same interface.",A bit late but thanks for this @pldubouilh,True,{}
pmacct/pmacct,https://github.com/pmacct/pmacct,197,2018-05-04T12:51:11Z,2018-05-05T00:33:52Z,2018-05-05T00:33:52Z,MERGED,True,4,5,2,https://github.com/pldubouilh,Fix pcap-iface-map lookup #2,1,[],https://github.com/pmacct/pmacct/pull/197,https://github.com/pldubouilh,1,https://github.com/pmacct/pmacct/pull/197,#196,#196,True,{}
pmacct/pmacct,https://github.com/pmacct/pmacct,201,2018-05-17T14:53:19Z,2018-05-20T20:39:48Z,2018-05-20T20:39:48Z,MERGED,True,32,0,6,https://github.com/pldubouilh,Decode arista iface out of sflow trailer data,1,[],https://github.com/pmacct/pmacct/pull/201,https://github.com/pldubouilh,1,https://github.com/pmacct/pmacct/pull/201,New option to decode the output interface data that's present in the trailer of the packets on arista switches,New option to decode the output interface data that's present in the trailer of the packets on arista switches,True,{}
pmacct/pmacct,https://github.com/pmacct/pmacct,203,2018-05-28T11:45:37Z,2018-05-28T15:15:33Z,2018-05-28T15:15:33Z,MERGED,True,49,2,1,https://github.com/jccardonar,"Additional examples of Router configuration for Netflow, BGP",1,[],https://github.com/pmacct/pmacct/pull/203,https://github.com/jccardonar,1,https://github.com/pmacct/pmacct/pull/203,,,True,{}
pmacct/pmacct,https://github.com/pmacct/pmacct,204,2018-06-06T08:19:32Z,2018-06-07T16:26:09Z,2018-06-07T16:26:09Z,MERGED,True,341,7,16,https://github.com/slash31,added support for geoipv2 lat/lon primitives,4,[],https://github.com/pmacct/pmacct/pull/204,https://github.com/slash31,1,https://github.com/pmacct/pmacct/pull/204,"I tried to keep all changes in line with existing structure/format. So far the lat/long lookups seem to work well on a qa/testing cluster of collectors that receive ~200k messages per second at 1:10k sample rate. We're currently using the GeoLite2-City DB file for testing.
The benefit of having lat/lon for us is that our backend datastore (CrateDB) has a geo_point data type, which we can use to directly query against polygons in GeoJASON format.","I tried to keep all changes in line with existing structure/format. So far the lat/long lookups seem to work well on a qa/testing cluster of collectors that receive ~200k messages per second at 1:10k sample rate. We're currently using the GeoLite2-City DB file for testing.
The benefit of having lat/lon for us is that our backend datastore (CrateDB) has a geo_point data type, which we can use to directly query against polygons in GeoJASON format.",True,{}
pmacct/pmacct,https://github.com/pmacct/pmacct,204,2018-06-06T08:19:32Z,2018-06-07T16:26:09Z,2018-06-07T16:26:09Z,MERGED,True,341,7,16,https://github.com/slash31,added support for geoipv2 lat/lon primitives,4,[],https://github.com/pmacct/pmacct/pull/204,https://github.com/slash31,2,https://github.com/pmacct/pmacct/pull/204#issuecomment-395095146,"I tried to keep all changes in line with existing structure/format. So far the lat/long lookups seem to work well on a qa/testing cluster of collectors that receive ~200k messages per second at 1:10k sample rate. We're currently using the GeoLite2-City DB file for testing.
The benefit of having lat/lon for us is that our backend datastore (CrateDB) has a geo_point data type, which we can use to directly query against polygons in GeoJASON format.","Hi Paolo - no problem at all; do you have a preferred/suggested datatype for a combined lat/lon? I could make it a string in an accepted geo format (GeoJSON or WKT), or an array/object of two doubles.",True,{}
pmacct/pmacct,https://github.com/pmacct/pmacct,204,2018-06-06T08:19:32Z,2018-06-07T16:26:09Z,2018-06-07T16:26:09Z,MERGED,True,341,7,16,https://github.com/slash31,added support for geoipv2 lat/lon primitives,4,[],https://github.com/pmacct/pmacct/pull/204,https://github.com/paololucente,3,https://github.com/pmacct/pmacct/pull/204#issuecomment-395101492,"I tried to keep all changes in line with existing structure/format. So far the lat/long lookups seem to work well on a qa/testing cluster of collectors that receive ~200k messages per second at 1:10k sample rate. We're currently using the GeoLite2-City DB file for testing.
The benefit of having lat/lon for us is that our backend datastore (CrateDB) has a geo_point data type, which we can use to directly query against polygons in GeoJASON format.","Hi Aaron, i was only thinking to have one single configuration knob for both latitude and longitude. Then, i am totally with you that we need two 'double' variables - one to store latitude, one to store longitude. Taking as example the MongoDB plugin:
if (config.what_to_count_2 & COUNT_SRC_HOST_COORDS) {
  bson_append_double(bson_elem, ""lat_ip_src"", data->src_ip_lat);
  bson_append_double(bson_elem, ""lon_ip_src"", data->src_ip_lon);
}

if (config.what_to_count_2 & COUNT_DST_HOST_COORDS) {
  bson_append_double(bson_elem, ""lat_ip_dst"", data->dst_ip_lat);
  bson_append_double(bson_elem, ""lon_ip_dst"", data->dst_ip_lon);
}",True,{}
pmacct/pmacct,https://github.com/pmacct/pmacct,204,2018-06-06T08:19:32Z,2018-06-07T16:26:09Z,2018-06-07T16:26:09Z,MERGED,True,341,7,16,https://github.com/slash31,added support for geoipv2 lat/lon primitives,4,[],https://github.com/pmacct/pmacct/pull/204,https://github.com/slash31,4,https://github.com/pmacct/pmacct/pull/204#issuecomment-395147096,"I tried to keep all changes in line with existing structure/format. So far the lat/long lookups seem to work well on a qa/testing cluster of collectors that receive ~200k messages per second at 1:10k sample rate. We're currently using the GeoLite2-City DB file for testing.
The benefit of having lat/lon for us is that our backend datastore (CrateDB) has a geo_point data type, which we can use to directly query against polygons in GeoJASON format.","Ah, got it. No problem. :)",True,{}
pmacct/pmacct,https://github.com/pmacct/pmacct,204,2018-06-06T08:19:32Z,2018-06-07T16:26:09Z,2018-06-07T16:26:09Z,MERGED,True,341,7,16,https://github.com/slash31,added support for geoipv2 lat/lon primitives,4,[],https://github.com/pmacct/pmacct/pull/204,https://github.com/slash31,5,https://github.com/pmacct/pmacct/pull/204#issuecomment-395316141,"I tried to keep all changes in line with existing structure/format. So far the lat/long lookups seem to work well on a qa/testing cluster of collectors that receive ~200k messages per second at 1:10k sample rate. We're currently using the GeoLite2-City DB file for testing.
The benefit of having lat/lon for us is that our backend datastore (CrateDB) has a geo_point data type, which we can use to directly query against polygons in GeoJASON format.","Updated code committed, let me know if this is what you had in mind. I'm obviously new to the codebase, so please don't hold back from pointing out anything that's not correct.",True,{}
pmacct/pmacct,https://github.com/pmacct/pmacct,206,2018-06-09T16:23:16Z,2018-06-11T14:12:04Z,2018-06-11T14:12:05Z,MERGED,True,3,3,1,https://github.com/slash31,fixed dst lookup for geoip coords,1,[],https://github.com/pmacct/pmacct/pull/206,https://github.com/slash31,1,https://github.com/pmacct/pmacct/pull/206,"Hi Paolo - there was a typo in my update to pkt_handlers.c, it was using geoipv2_src for both directions. :)","Hi Paolo - there was a typo in my update to pkt_handlers.c, it was using geoipv2_src for both directions. :)",True,{}
pmacct/pmacct,https://github.com/pmacct/pmacct,217,2018-08-08T22:06:34Z,2018-08-09T21:04:46Z,2018-08-09T21:04:46Z,MERGED,True,6,6,2,https://github.com/raymondrussell,"Pad usecs, and use ""%ld"" since time fields are signed longs",1,[],https://github.com/pmacct/pmacct/pull/217,https://github.com/raymondrussell,1,https://github.com/pmacct/pmacct/pull/217,,,True,{}
pmacct/pmacct,https://github.com/pmacct/pmacct,219,2018-08-15T13:22:44Z,2018-08-21T12:37:47Z,2018-08-21T12:37:47Z,MERGED,True,31,17,1,https://github.com/FvDxxx,ADD-PATH capability exchange fix for MP-BGP OPEN,1,[],https://github.com/pmacct/pmacct/pull/219,https://github.com/FvDxxx,1,https://github.com/pmacct/pmacct/pull/219,"PMACCTs handled/handles replies to MP-BGP ADD-PATHs incorrectly. It checks only first AFI.SAFI and sets in the reply for last AFI.SAFI RECEIVE(1) if first included SEND(2) or SEND-RECEIVE(3).
Example: Peer with SEND capability for inet unicast (1.1) and inet6 unicast (2.1) sends:
Optional Parameter: Capability
Parameter Type: Capability (2)
Parameter Length: 10
Capability: Support for Additional Paths
Type: Support for Additional Paths (69)
Length: 8
AFI: IPv4 (1)
SAFI: Unicast (1)
Send/Receive: Send (2)
AFI: IPv6 (2)
SAFI: Unicast (1)
Send/Receive: Send (2)
Reply was/is from bgp_msg:
Optional Parameter: Capability
Parameter Type: Capability (2)
Parameter Length: 10
Capability: Support for Additional Paths
Type: Support for Additional Paths (69)
Length: 8
AFI: IPv4 (1)
SAFI: Unicast (1)
Send/Receive: Send (2)
AFI: IPv6 (2)
SAFI: Unicast (1)
Send/Receive: Receive (1)
This patch hopefully fixes it (not too much testing done ...). It checks each AFI.SAFI included in the capability signaled from the peer and only replies with RECEIVE for AFI.SAFI if peer said SEND or SEND-RECEIVE for it.
(Sorry, pasted into http:// from my source ... pull, push ... sigh).","PMACCTs handled/handles replies to MP-BGP ADD-PATHs incorrectly. It checks only first AFI.SAFI and sets in the reply for last AFI.SAFI RECEIVE(1) if first included SEND(2) or SEND-RECEIVE(3).
Example: Peer with SEND capability for inet unicast (1.1) and inet6 unicast (2.1) sends:
Optional Parameter: Capability
Parameter Type: Capability (2)
Parameter Length: 10
Capability: Support for Additional Paths
Type: Support for Additional Paths (69)
Length: 8
AFI: IPv4 (1)
SAFI: Unicast (1)
Send/Receive: Send (2)
AFI: IPv6 (2)
SAFI: Unicast (1)
Send/Receive: Send (2)
Reply was/is from bgp_msg:
Optional Parameter: Capability
Parameter Type: Capability (2)
Parameter Length: 10
Capability: Support for Additional Paths
Type: Support for Additional Paths (69)
Length: 8
AFI: IPv4 (1)
SAFI: Unicast (1)
Send/Receive: Send (2)
AFI: IPv6 (2)
SAFI: Unicast (1)
Send/Receive: Receive (1)
This patch hopefully fixes it (not too much testing done ...). It checks each AFI.SAFI included in the capability signaled from the peer and only replies with RECEIVE for AFI.SAFI if peer said SEND or SEND-RECEIVE for it.
(Sorry, pasted into http:// from my source ... pull, push ... sigh).",True,{}
pmacct/pmacct,https://github.com/pmacct/pmacct,219,2018-08-15T13:22:44Z,2018-08-21T12:37:47Z,2018-08-21T12:37:47Z,MERGED,True,31,17,1,https://github.com/FvDxxx,ADD-PATH capability exchange fix for MP-BGP OPEN,1,[],https://github.com/pmacct/pmacct/pull/219,https://github.com/FvDxxx,2,https://github.com/pmacct/pmacct/pull/219#issuecomment-414395537,"PMACCTs handled/handles replies to MP-BGP ADD-PATHs incorrectly. It checks only first AFI.SAFI and sets in the reply for last AFI.SAFI RECEIVE(1) if first included SEND(2) or SEND-RECEIVE(3).
Example: Peer with SEND capability for inet unicast (1.1) and inet6 unicast (2.1) sends:
Optional Parameter: Capability
Parameter Type: Capability (2)
Parameter Length: 10
Capability: Support for Additional Paths
Type: Support for Additional Paths (69)
Length: 8
AFI: IPv4 (1)
SAFI: Unicast (1)
Send/Receive: Send (2)
AFI: IPv6 (2)
SAFI: Unicast (1)
Send/Receive: Send (2)
Reply was/is from bgp_msg:
Optional Parameter: Capability
Parameter Type: Capability (2)
Parameter Length: 10
Capability: Support for Additional Paths
Type: Support for Additional Paths (69)
Length: 8
AFI: IPv4 (1)
SAFI: Unicast (1)
Send/Receive: Send (2)
AFI: IPv6 (2)
SAFI: Unicast (1)
Send/Receive: Receive (1)
This patch hopefully fixes it (not too much testing done ...). It checks each AFI.SAFI included in the capability signaled from the peer and only replies with RECEIVE for AFI.SAFI if peer said SEND or SEND-RECEIVE for it.
(Sorry, pasted into http:// from my source ... pull, push ... sigh).","Hi Paolo,
the comment ... ;-)
The simple approach would have been to answer for every AFI.SAFI in the ADD-PATH and not just to the AFI.SAFIs the peer could send multiple paths (if there are some at all). The cap_set wouldn't then be required (we send it in any case) and the overall coding is probably more straight forward.
If you think it's easier to understand and maintain then feel free to change it (simplicity vs. ... good question ... other than eventually saving a few bytes in the open and eventually preventing the other side some preparing add-path receive capability support for anyway never received prefixes, there's indeed not much benefit for the added complexity).
And you are always free to change any of my code into something which is more pmacct's style.
Cheers,
Markus",True,{}
pmacct/pmacct,https://github.com/pmacct/pmacct,219,2018-08-15T13:22:44Z,2018-08-21T12:37:47Z,2018-08-21T12:37:47Z,MERGED,True,31,17,1,https://github.com/FvDxxx,ADD-PATH capability exchange fix for MP-BGP OPEN,1,[],https://github.com/pmacct/pmacct/pull/219,https://github.com/paololucente,3,https://github.com/pmacct/pmacct/pull/219#issuecomment-414466507,"PMACCTs handled/handles replies to MP-BGP ADD-PATHs incorrectly. It checks only first AFI.SAFI and sets in the reply for last AFI.SAFI RECEIVE(1) if first included SEND(2) or SEND-RECEIVE(3).
Example: Peer with SEND capability for inet unicast (1.1) and inet6 unicast (2.1) sends:
Optional Parameter: Capability
Parameter Type: Capability (2)
Parameter Length: 10
Capability: Support for Additional Paths
Type: Support for Additional Paths (69)
Length: 8
AFI: IPv4 (1)
SAFI: Unicast (1)
Send/Receive: Send (2)
AFI: IPv6 (2)
SAFI: Unicast (1)
Send/Receive: Send (2)
Reply was/is from bgp_msg:
Optional Parameter: Capability
Parameter Type: Capability (2)
Parameter Length: 10
Capability: Support for Additional Paths
Type: Support for Additional Paths (69)
Length: 8
AFI: IPv4 (1)
SAFI: Unicast (1)
Send/Receive: Send (2)
AFI: IPv6 (2)
SAFI: Unicast (1)
Send/Receive: Receive (1)
This patch hopefully fixes it (not too much testing done ...). It checks each AFI.SAFI included in the capability signaled from the peer and only replies with RECEIVE for AFI.SAFI if peer said SEND or SEND-RECEIVE for it.
(Sorry, pasted into http:// from my source ... pull, push ... sigh).","Hi Markus,
No, no, i totally like the ""cap_set approach"". And with your comment above i think i have the elements to make the comment (in the code ;-)) a bit more clear - which was all my point about.
Thank you very much again, look forward seeing you at NLNOG in a couple of weeks.
Paolo",True,{}
pmacct/pmacct,https://github.com/pmacct/pmacct,221,2018-08-22T00:28:50Z,2018-08-22T13:31:09Z,2018-08-22T13:31:09Z,MERGED,True,1,1,1,https://github.com/mikiT,patch for issue #220,1,[],https://github.com/pmacct/pmacct/pull/221,https://github.com/mikiT,1,https://github.com/pmacct/pmacct/pull/221,a patch for #220,a patch for #220,True,{}
pmacct/pmacct,https://github.com/pmacct/pmacct,228,2018-09-21T14:29:04Z,,2021-03-23T23:48:00Z,OPEN,False,75,3,9,https://github.com/francoijs,pmacctd: add option 'gre_decap' for IPv4/GRE decapsulation of captured packets,1,[],https://github.com/pmacct/pmacct/pull/228,https://github.com/francoijs,1,https://github.com/pmacct/pmacct/pull/228,"Hi there,
I propose this option that enables accounting of packets inside a GRE tunnel:
If set, makes the daemon decapsulate GRE-tunneled traffic.
Packets that are not encapsulated in an IPv4/GRE tunnel are processed as usual.","Hi there,
I propose this option that enables accounting of packets inside a GRE tunnel:
If set, makes the daemon decapsulate GRE-tunneled traffic.
Packets that are not encapsulated in an IPv4/GRE tunnel are processed as usual.",True,{}
pmacct/pmacct,https://github.com/pmacct/pmacct,228,2018-09-21T14:29:04Z,,2021-03-23T23:48:00Z,OPEN,False,75,3,9,https://github.com/francoijs,pmacctd: add option 'gre_decap' for IPv4/GRE decapsulation of captured packets,1,[],https://github.com/pmacct/pmacct/pull/228,https://github.com/francoijs,2,https://github.com/pmacct/pmacct/pull/228#issuecomment-425893095,"Hi there,
I propose this option that enables accounting of packets inside a GRE tunnel:
If set, makes the daemon decapsulate GRE-tunneled traffic.
Packets that are not encapsulated in an IPv4/GRE tunnel are processed as usual.","Hi, thanks for the quick feedback!
No problem, I will have a look at the GTP code ASAP",True,{}
pmacct/pmacct,https://github.com/pmacct/pmacct,228,2018-09-21T14:29:04Z,,2021-03-23T23:48:00Z,OPEN,False,75,3,9,https://github.com/francoijs,pmacctd: add option 'gre_decap' for IPv4/GRE decapsulation of captured packets,1,[],https://github.com/pmacct/pmacct/pull/228,https://github.com/paololucente,3,https://github.com/pmacct/pmacct/pull/228#issuecomment-425981824,"Hi there,
I propose this option that enables accounting of packets inside a GRE tunnel:
If set, makes the daemon decapsulate GRE-tunneled traffic.
Packets that are not encapsulated in an IPv4/GRE tunnel are processed as usual.","@francoijs , thank you Sir. Paolo",True,{}
pmacct/pmacct,https://github.com/pmacct/pmacct,229,2018-09-25T15:50:03Z,2018-09-26T09:28:26Z,2018-09-26T09:28:26Z,MERGED,True,15,8,2,https://github.com/tbearma1,update doc and and picture,7,[],https://github.com/pmacct/pmacct/pull/229,https://github.com/tbearma1,1,https://github.com/pmacct/pmacct/pull/229,"I did with thomas a new discoussion. We created a new design how the server could work togeter.
Goals of the design:

GRPC-Python-collector has only to add the grpcPeer (comming from grpc-layer) to the received data and forward all this metrics to pmtelemetryd.
To hold it simple pmtelemetryd only has to get the avro-id based on the sensor-/encoding-path on the YSH-Mapping-File and forward the Metrics with this avro-id to a avro-topic.
With this pmtelemetry is also able to send as avro with schema-id for udp/tcp received ST-Data","I did with thomas a new discoussion. We created a new design how the server could work togeter.
Goals of the design:

GRPC-Python-collector has only to add the grpcPeer (comming from grpc-layer) to the received data and forward all this metrics to pmtelemetryd.
To hold it simple pmtelemetryd only has to get the avro-id based on the sensor-/encoding-path on the YSH-Mapping-File and forward the Metrics with this avro-id to a avro-topic.
With this pmtelemetry is also able to send as avro with schema-id for udp/tcp received ST-Data",True,{}
pmacct/pmacct,https://github.com/pmacct/pmacct,231,2018-10-01T10:05:46Z,2018-10-03T07:07:32Z,2018-10-03T07:07:32Z,CLOSED,False,23,16,5,https://github.com/rsolsn,1.7.1,4,[],https://github.com/pmacct/pmacct/pull/231,https://github.com/rsolsn,1,https://github.com/pmacct/pmacct/pull/231,,,True,{}
pmacct/pmacct,https://github.com/pmacct/pmacct,231,2018-10-01T10:05:46Z,2018-10-03T07:07:32Z,2018-10-03T07:07:32Z,CLOSED,False,23,16,5,https://github.com/rsolsn,1.7.1,4,[],https://github.com/pmacct/pmacct/pull/231,https://github.com/rsolsn,2,https://github.com/pmacct/pmacct/pull/231#issuecomment-426045829,,"Yes I know...sorry for the mistake. Do you have an howto pull a request for dummies ? :p

Inviato da iPhone
â€¦
 Il giorno 1 ott 2018, alle ore 19:35, Paolo Lucente ***@***.***> ha scritto:

 @paololucente requested changes on this pull request.

 Hi @rsolsn / Alessandro, i guess this was meant for following-up to #222 . Unfortunately, if you see the list of commits included in it, the pull request is invalid. Just commenting to have a way to communicate and in case you did not realise yet.

 â€”
 You are receiving this because you were mentioned.
 Reply to this email directly, view it on GitHub, or mute the thread.",True,{}
pmacct/pmacct,https://github.com/pmacct/pmacct,231,2018-10-01T10:05:46Z,2018-10-03T07:07:32Z,2018-10-03T07:07:32Z,CLOSED,False,23,16,5,https://github.com/rsolsn,1.7.1,4,[],https://github.com/pmacct/pmacct/pull/231,https://github.com/paololucente,3,https://github.com/pmacct/pmacct/pull/231#issuecomment-426060080,,"No worries. There is this: https://help.github.com/articles/creating-a-pull-request/ . Essentially, you: 1) fork the original pmacct repo, 2) clone it locally, 3) change the code, 4) commit/push to your own forked repo and 5) create a Pull Request towards the original pmacct repo (as described in the link).",True,{}
pmacct/pmacct,https://github.com/pmacct/pmacct,232,2018-10-02T08:37:30Z,2018-10-02T18:26:52Z,2018-10-02T18:26:52Z,CLOSED,False,303,0,2,https://github.com/tbearma1,"add pmgrpcd.py, replace design picture",3,[],https://github.com/pmacct/pmacct/pull/232,https://github.com/tbearma1,1,https://github.com/pmacct/pmacct/pull/232,,,True,{}
pmacct/pmacct,https://github.com/pmacct/pmacct,235,2018-10-02T20:03:57Z,2018-10-20T17:46:49Z,2018-10-20T17:46:49Z,MERGED,True,303,0,1,https://github.com/tbearma1,add the multivendor generic well named python grpc collector,1,[],https://github.com/pmacct/pmacct/pull/235,https://github.com/tbearma1,1,https://github.com/pmacct/pmacct/pull/235,i removed the fork of your pmacct and fork it again to my account. then i clone it to my home on daisy63 and addet the new file pmgrpcd.py (the one without teststatements). then i made a git push and now the pullrequest for only one file. :-) thanks.,i removed the fork of your pmacct and fork it again to my account. then i clone it to my home on daisy63 and addet the new file pmgrpcd.py (the one without teststatements). then i made a git push and now the pullrequest for only one file. :-) thanks.,True,{}
pmacct/pmacct,https://github.com/pmacct/pmacct,236,2018-10-03T07:07:02Z,2018-10-04T07:20:08Z,2018-10-04T07:20:08Z,MERGED,True,7,0,1,https://github.com/rsolsn,Update ndpi_util.c ,1,[],https://github.com/pmacct/pmacct/pull/236,https://github.com/rsolsn,1,https://github.com/pmacct/pmacct/pull/236,As from issue #222 this enables the detection of more classes.,As from issue #222 this enables the detection of more classes.,True,{}
pmacct/pmacct,https://github.com/pmacct/pmacct,240,2018-10-09T07:43:45Z,2018-10-11T13:27:50Z,2018-10-11T13:27:50Z,MERGED,True,52,52,26,https://github.com/Rico29,change ip_src/ip_dst field length for IPv6,2,[],https://github.com/pmacct/pmacct/pull/240,https://github.com/Rico29,1,https://github.com/pmacct/pmacct/pull/240,ipv6 max length is 45.,ipv6 max length is 45.,True,{}
pmacct/pmacct,https://github.com/pmacct/pmacct,240,2018-10-09T07:43:45Z,2018-10-11T13:27:50Z,2018-10-11T13:27:50Z,MERGED,True,52,52,26,https://github.com/Rico29,change ip_src/ip_dst field length for IPv6,2,[],https://github.com/pmacct/pmacct/pull/240,https://github.com/Rico29,2,https://github.com/pmacct/pmacct/pull/240#issuecomment-428448139,ipv6 max length is 45.,"Hi Paolo. Changes are done.
Regards",True,{}
pmacct/pmacct,https://github.com/pmacct/pmacct,244,2018-10-25T11:29:05Z,2018-10-26T06:08:20Z,2018-10-26T06:08:20Z,CLOSED,False,0,0,0,https://github.com/tbearma1,change picture reference_architecture.png,0,[],https://github.com/pmacct/pmacct/pull/244,https://github.com/tbearma1,1,https://github.com/pmacct/pmacct/pull/244,this new picture of the reference_architecture.png shows more details of the Mappingfiles.,this new picture of the reference_architecture.png shows more details of the Mappingfiles.,True,{}
pmacct/pmacct,https://github.com/pmacct/pmacct,245,2018-10-26T06:15:57Z,2018-10-26T12:11:17Z,2018-10-26T12:11:17Z,CLOSED,False,0,0,1,https://github.com/tbearma1,change picture reference_architecture.png to a new one with 896KB,2,[],https://github.com/pmacct/pmacct/pull/245,https://github.com/tbearma1,1,https://github.com/pmacct/pmacct/pull/245,picture is more correct with again more descriptions :-),picture is more correct with again more descriptions :-),True,{}
pmacct/pmacct,https://github.com/pmacct/pmacct,246,2018-10-26T13:18:43Z,2018-10-28T13:26:23Z,2018-10-28T13:26:23Z,CLOSED,False,0,0,1,https://github.com/tbearma1,remove reference_architecture.png 133kB,2,[],https://github.com/pmacct/pmacct/pull/246,https://github.com/tbearma1,1,https://github.com/pmacct/pmacct/pull/246,PR only for remove reference_architecture.png 133kB,PR only for remove reference_architecture.png 133kB,True,{}
pmacct/pmacct,https://github.com/pmacct/pmacct,247,2018-11-01T12:30:14Z,2018-11-04T12:44:23Z,2018-11-04T12:44:23Z,MERGED,True,15,1,1,https://github.com/vtsingaras,Make jumbo_container buffer's size dynamic and live on heap.,1,[],https://github.com/pmacct/pmacct/pull/247,https://github.com/vtsingaras,1,https://github.com/pmacct/pmacct/pull/247,"NFLOG can return packets with size larger than 10.000B (jumbo_container's previous size), in my case 32K.
This combined with the memcpy later in nflog_incoming would corrupt the stack (as jumbo_container previously was stack allocated and with a static size of 10.000B).
This fixes the resulting crashes in my case (IP4HdrSz and related vars would get overwritten with dummy values).
Is this fix acceptable?","NFLOG can return packets with size larger than 10.000B (jumbo_container's previous size), in my case 32K.
This combined with the memcpy later in nflog_incoming would corrupt the stack (as jumbo_container previously was stack allocated and with a static size of 10.000B).
This fixes the resulting crashes in my case (IP4HdrSz and related vars would get overwritten with dummy values).
Is this fix acceptable?",True,{}
pmacct/pmacct,https://github.com/pmacct/pmacct,256,2019-01-12T06:50:23Z,,2021-03-23T23:48:00Z,OPEN,False,566,0,11,https://github.com/hawkinsw,Initial JSON UDP Plugin Implementation,2,[],https://github.com/pmacct/pmacct/pull/256,https://github.com/hawkinsw,1,https://github.com/pmacct/pmacct/pull/256,"This is a simple plugin that will format captured
records as JSON objects and then deliver them
to a listening UDP server.","This is a simple plugin that will format captured
records as JSON objects and then deliver them
to a listening UDP server.",True,{}
pmacct/pmacct,https://github.com/pmacct/pmacct,256,2019-01-12T06:50:23Z,,2021-03-23T23:48:00Z,OPEN,False,566,0,11,https://github.com/hawkinsw,Initial JSON UDP Plugin Implementation,2,[],https://github.com/pmacct/pmacct/pull/256,https://github.com/hawkinsw,2,https://github.com/pmacct/pmacct/pull/256#issuecomment-453725071,"This is a simple plugin that will format captured
records as JSON objects and then deliver them
to a listening UDP server.","Hello! pmacct is such a great set of programs and a great tool. We are using it in our research project and had the requirement that we format netflow records as json-objects as part of a data processing pipeline. This little plugin does exactly what we need -- it formats each record as a json object and then sends it to a server over UDP.
I would love for this to be included if you think that it's useful. I know that there will be things that you want to fix -- please let me know how I can modify the patch so that it meets your high standards.
Thanks again for the great work maintaining and developing such a great suite of tools!
Will",True,{}
pmacct/pmacct,https://github.com/pmacct/pmacct,256,2019-01-12T06:50:23Z,,2021-03-23T23:48:00Z,OPEN,False,566,0,11,https://github.com/hawkinsw,Initial JSON UDP Plugin Implementation,2,[],https://github.com/pmacct/pmacct/pull/256,https://github.com/hawkinsw,3,https://github.com/pmacct/pmacct/pull/256#issuecomment-459164805,"This is a simple plugin that will format captured
records as JSON objects and then deliver them
to a listening UDP server.","Just FYI: As we emailed about last night, here is an updated pull request for the plugin that includes zeromq support! I hope that this matches your expectations!
As we also talked about in the email, I am waiting on the rebranding to hear your creative thoughts on the new name :-)
I look forward to working together to get this into shape so that it matches your high-quality standards!",True,{}
pmacct/pmacct,https://github.com/pmacct/pmacct,260,2019-01-24T09:04:09Z,2019-01-31T20:31:47Z,2019-01-31T21:34:21Z,MERGED,True,93,85,22,https://github.com/jbj,Fix type mismatches in format strings,8,[],https://github.com/pmacct/pmacct/pull/260,https://github.com/jbj,1,https://github.com/pmacct/pmacct/pull/260,"The first commit in this PR adds an annotation to the Log function to tell the compiler it's a printf-style function, allowing it to give warnings for argument mismatches. Subsequent commits address all the -Wformat errors I get on Apple Clang 10.0.0.
The problems fixed range from the benign, like printing signed as unsigned, to code that will certainly segfault when executed. I think they all deserve to be fixed so future serious problems don't drown in the noise of existing warnings. See details on individual commits.
These problems came to my attention when looking at the alerts for this project on https://lgtm.com/projects/g/pmacct/pmacct/alerts/?mode=tree (full disclosure: I work for the company behind lgtm.com).","The first commit in this PR adds an annotation to the Log function to tell the compiler it's a printf-style function, allowing it to give warnings for argument mismatches. Subsequent commits address all the -Wformat errors I get on Apple Clang 10.0.0.
The problems fixed range from the benign, like printing signed as unsigned, to code that will certainly segfault when executed. I think they all deserve to be fixed so future serious problems don't drown in the noise of existing warnings. See details on individual commits.
These problems came to my attention when looking at the alerts for this project on https://lgtm.com/projects/g/pmacct/pmacct/alerts/?mode=tree (full disclosure: I work for the company behind lgtm.com).",True,{}
pmacct/pmacct,https://github.com/pmacct/pmacct,260,2019-01-24T09:04:09Z,2019-01-31T20:31:47Z,2019-01-31T21:34:21Z,MERGED,True,93,85,22,https://github.com/jbj,Fix type mismatches in format strings,8,[],https://github.com/pmacct/pmacct/pull/260,https://github.com/paololucente,2,https://github.com/pmacct/pmacct/pull/260#issuecomment-458647027,"The first commit in this PR adds an annotation to the Log function to tell the compiler it's a printf-style function, allowing it to give warnings for argument mismatches. Subsequent commits address all the -Wformat errors I get on Apple Clang 10.0.0.
The problems fixed range from the benign, like printing signed as unsigned, to code that will certainly segfault when executed. I think they all deserve to be fixed so future serious problems don't drown in the noise of existing warnings. See details on individual commits.
These problems came to my attention when looking at the alerts for this project on https://lgtm.com/projects/g/pmacct/pmacct/alerts/?mode=tree (full disclosure: I work for the company behind lgtm.com).","On a different note: i did not know of lgtm.com, very nice what you guys do. In August i had a solid round of sorting out compiler warnings - it is remarkable how they went back up with new code being introduced. Definitely something to keep an eye on via lgtm.com. Thank you!",True,{}
pmacct/pmacct,https://github.com/pmacct/pmacct,260,2019-01-24T09:04:09Z,2019-01-31T20:31:47Z,2019-01-31T21:34:21Z,MERGED,True,93,85,22,https://github.com/jbj,Fix type mismatches in format strings,8,[],https://github.com/pmacct/pmacct/pull/260,https://github.com/jbj,3,https://github.com/pmacct/pmacct/pull/260#issuecomment-458991106,"The first commit in this PR adds an annotation to the Log function to tell the compiler it's a printf-style function, allowing it to give warnings for argument mismatches. Subsequent commits address all the -Wformat errors I get on Apple Clang 10.0.0.
The problems fixed range from the benign, like printing signed as unsigned, to code that will certainly segfault when executed. I think they all deserve to be fixed so future serious problems don't drown in the noise of existing warnings. See details on individual commits.
These problems came to my attention when looking at the alerts for this project on https://lgtm.com/projects/g/pmacct/pmacct/alerts/?mode=tree (full disclosure: I work for the company behind lgtm.com).",You're welcome. I've now rebased the branch and checked that it still compiles.,True,{}
pmacct/pmacct,https://github.com/pmacct/pmacct,260,2019-01-24T09:04:09Z,2019-01-31T20:31:47Z,2019-01-31T21:34:21Z,MERGED,True,93,85,22,https://github.com/jbj,Fix type mismatches in format strings,8,[],https://github.com/pmacct/pmacct/pull/260,https://github.com/jbj,4,https://github.com/pmacct/pmacct/pull/260#issuecomment-459514742,"The first commit in this PR adds an annotation to the Log function to tell the compiler it's a printf-style function, allowing it to give warnings for argument mismatches. Subsequent commits address all the -Wformat errors I get on Apple Clang 10.0.0.
The problems fixed range from the benign, like printing signed as unsigned, to code that will certainly segfault when executed. I think they all deserve to be fixed so future serious problems don't drown in the noise of existing warnings. See details on individual commits.
These problems came to my attention when looking at the alerts for this project on https://lgtm.com/projects/g/pmacct/pmacct/alerts/?mode=tree (full disclosure: I work for the company behind lgtm.com).",Iâ€™m glad to hear you found this helpful. To prevent errors from creeping back in you can go to https://lgtm.com/projects/g/pmacct/pmacct/ci/ to enable automated code review. Then weâ€™ll analyse every PR and post a comment on the PR if it introduces alerts.,True,{}
pmacct/pmacct,https://github.com/pmacct/pmacct,262,2019-02-04T10:45:30Z,2019-02-13T16:00:50Z,2019-02-13T16:00:50Z,MERGED,True,202,2,11,https://github.com/edge-intelligence,adding custom print plugin for nfacctd,2,[],https://github.com/pmacct/pmacct/pull/262,https://github.com/edge-intelligence,1,https://github.com/pmacct/pmacct/pull/262,Adding custom print plugin,Adding custom print plugin,True,{}
pmacct/pmacct,https://github.com/pmacct/pmacct,263,2019-02-07T16:32:04Z,2019-02-11T18:58:35Z,2019-02-11T19:02:59Z,MERGED,True,749,193,1,https://github.com/tbearma1,pmgrpcd is now able to avro serialize metrics,1,[],https://github.com/pmacct/pmacct/pull/263,https://github.com/tbearma1,1,https://github.com/pmacct/pmacct/pull/263,"I think i have to write a installation dokumentation and also a description of all the configparameters and also examples of the mappingfiles i have.
For me this is pmgrpcd version 1.0.
see pmgrpcd.py -v         :-)
S.D.G","I think i have to write a installation dokumentation and also a description of all the configparameters and also examples of the mappingfiles i have.
For me this is pmgrpcd version 1.0.
see pmgrpcd.py -v         :-)
S.D.G",True,{}
pmacct/pmacct,https://github.com/pmacct/pmacct,263,2019-02-07T16:32:04Z,2019-02-11T18:58:35Z,2019-02-11T19:02:59Z,MERGED,True,749,193,1,https://github.com/tbearma1,pmgrpcd is now able to avro serialize metrics,1,[],https://github.com/pmacct/pmacct/pull/263,https://github.com/paololucente,2,https://github.com/pmacct/pmacct/pull/263#issuecomment-462451987,"I think i have to write a installation dokumentation and also a description of all the configparameters and also examples of the mappingfiles i have.
For me this is pmgrpcd version 1.0.
see pmgrpcd.py -v         :-)
S.D.G","Hi Matthias, this is merged. Look forward to some docs :) Paolo",True,{}
pmacct/pmacct,https://github.com/pmacct/pmacct,264,2019-02-18T17:32:17Z,2019-02-19T06:32:20Z,2019-02-19T06:32:20Z,MERGED,True,177,2,6,https://github.com/edge-intelligence,adding example of using custom output feature,1,[],https://github.com/pmacct/pmacct/pull/264,https://github.com/edge-intelligence,1,https://github.com/pmacct/pmacct/pull/264,,,True,{}
pmacct/pmacct,https://github.com/pmacct/pmacct,265,2019-02-18T19:18:53Z,2019-02-20T22:10:27Z,2019-02-20T22:10:27Z,MERGED,True,307,4,2,https://github.com/tbearma1,optimalizations for the public use.  ...or i try to make improvements.,6,[],https://github.com/pmacct/pmacct/pull/265,https://github.com/tbearma1,1,https://github.com/pmacct/pmacct/pull/265,"I think it is better to hold the protofiles and the compiled pythonlibrarys separate from the code (cloned with git)
default files creations after the first launch of pmgrpcd.py are helpfull to use as default and to see whats possible to adapt.
not all people like to use port 10000 to listen for stremaing-telemetry data. Therefor it is now adaptible.","I think it is better to hold the protofiles and the compiled pythonlibrarys separate from the code (cloned with git)
default files creations after the first launch of pmgrpcd.py are helpfull to use as default and to see whats possible to adapt.
not all people like to use port 10000 to listen for stremaing-telemetry data. Therefor it is now adaptible.",True,{}
pmacct/pmacct,https://github.com/pmacct/pmacct,271,2019-03-03T20:44:14Z,2019-03-05T12:42:18Z,2019-03-05T12:42:18Z,MERGED,True,58,0,1,https://github.com/jccardonar,Adding Flow augmentation description doc,1,[],https://github.com/pmacct/pmacct/pull/271,https://github.com/jccardonar,1,https://github.com/pmacct/pmacct/pull/271,Adding document explaining the flow augmentation mechanisms  from BGP protocols.,Adding document explaining the flow augmentation mechanisms  from BGP protocols.,True,{}
pmacct/pmacct,https://github.com/pmacct/pmacct,273,2019-03-19T14:18:02Z,2019-03-20T09:45:41Z,2019-03-20T09:45:41Z,MERGED,True,7,6,1,https://github.com/jccardonar,Typos on README.telemetry,1,[],https://github.com/pmacct/pmacct/pull/273,https://github.com/jccardonar,1,https://github.com/pmacct/pmacct/pull/273,,,True,{}
pmacct/pmacct,https://github.com/pmacct/pmacct,276,2019-03-26T20:10:36Z,2019-03-27T11:03:33Z,2019-03-27T11:03:33Z,MERGED,True,20,19,2,https://github.com/jccardonar, Tuning telemetry documentation ,2,[],https://github.com/pmacct/pmacct/pull/276,https://github.com/jccardonar,1,https://github.com/pmacct/pmacct/pull/276,,,True,{}
pmacct/pmacct,https://github.com/pmacct/pmacct,279,2019-04-09T15:39:16Z,2019-04-23T12:21:58Z,2019-04-23T12:21:58Z,CLOSED,False,113,69,6,https://github.com/pldubouilh,implement multiple outputs to nfprobe_receiver,1,[],https://github.com/pmacct/pmacct/pull/279,https://github.com/pldubouilh,1,https://github.com/pmacct/pmacct/pull/279,"Hey @paololucente !
Here's a PR that allows to set multiple endpoints to nfprobe_receiver. Samples are then shipped to a pseudo-randomly selected endpoint.
The idea is to load balance the processing of samples. Hope it's of interest ! Keep me posted if you think something should be changed.
Cheers","Hey @paololucente !
Here's a PR that allows to set multiple endpoints to nfprobe_receiver. Samples are then shipped to a pseudo-randomly selected endpoint.
The idea is to load balance the processing of samples. Hope it's of interest ! Keep me posted if you think something should be changed.
Cheers",True,{}
pmacct/pmacct,https://github.com/pmacct/pmacct,279,2019-04-09T15:39:16Z,2019-04-23T12:21:58Z,2019-04-23T12:21:58Z,CLOSED,False,113,69,6,https://github.com/pldubouilh,implement multiple outputs to nfprobe_receiver,1,[],https://github.com/pmacct/pmacct/pull/279,https://github.com/pldubouilh,2,https://github.com/pmacct/pmacct/pull/279#issuecomment-485779412,"Hey @paololucente !
Here's a PR that allows to set multiple endpoints to nfprobe_receiver. Samples are then shipped to a pseudo-randomly selected endpoint.
The idea is to load balance the processing of samples. Hope it's of interest ! Keep me posted if you think something should be changed.
Cheers","thanks for the heads up, I'll investigate nfacctd/tee in addition to pmacct for this then ðŸ‘
cheers :)",True,{}
pmacct/pmacct,https://github.com/pmacct/pmacct,285,2019-05-20T04:07:30Z,2019-05-20T04:09:00Z,2019-05-20T04:09:00Z,CLOSED,False,34,27,2,https://github.com/vphatarp,Tweak signal handling behavior to enable blocking of signals during câ€¦,1,[],https://github.com/pmacct/pmacct/pull/285,https://github.com/vphatarp,1,https://github.com/pmacct/pmacct/pull/285,Tweaking signal handling behavior to enable blocking of signals during critical sections of nfacctd code.,Tweaking signal handling behavior to enable blocking of signals during critical sections of nfacctd code.,True,{}
pmacct/pmacct,https://github.com/pmacct/pmacct,286,2019-05-20T04:55:52Z,2019-06-06T19:54:50Z,2019-06-13T14:38:44Z,MERGED,True,39,27,2,https://github.com/vphatarp,Tweaking signal handling during critical sections of polling/netflow processing for nfacctd,4,[],https://github.com/pmacct/pmacct/pull/286,https://github.com/vphatarp,1,https://github.com/pmacct/pmacct/pull/286,There have been certain issues in the network regarding how signals are processed causing a block in zmq from sending a message to the print plugins. The added code tweaks the signal handling behavior to block certain signals(Currently blocking only USR1) during critical sections of polling/Netflow processing.,There have been certain issues in the network regarding how signals are processed causing a block in zmq from sending a message to the print plugins. The added code tweaks the signal handling behavior to block certain signals(Currently blocking only USR1) during critical sections of polling/Netflow processing.,True,{'THUMBS_UP': ['https://github.com/pldubouilh']}
pmacct/pmacct,https://github.com/pmacct/pmacct,286,2019-05-20T04:55:52Z,2019-06-06T19:54:50Z,2019-06-13T14:38:44Z,MERGED,True,39,27,2,https://github.com/vphatarp,Tweaking signal handling during critical sections of polling/netflow processing for nfacctd,4,[],https://github.com/pmacct/pmacct/pull/286,https://github.com/vphatarp,2,https://github.com/pmacct/pmacct/pull/286#issuecomment-494830881,There have been certain issues in the network regarding how signals are processed causing a block in zmq from sending a message to the print plugins. The added code tweaks the signal handling behavior to block certain signals(Currently blocking only USR1) during critical sections of polling/Netflow processing.,"Hi @paololucente , I agree. Ideally all the signals should be blocked. Please let me know if you want me to work on it. Thanks.",True,{}
pmacct/pmacct,https://github.com/pmacct/pmacct,286,2019-05-20T04:55:52Z,2019-06-06T19:54:50Z,2019-06-13T14:38:44Z,MERGED,True,39,27,2,https://github.com/vphatarp,Tweaking signal handling during critical sections of polling/netflow processing for nfacctd,4,[],https://github.com/pmacct/pmacct/pull/286,https://github.com/paololucente,3,https://github.com/pmacct/pmacct/pull/286#issuecomment-495178522,There have been certain issues in the network regarding how signals are processed causing a block in zmq from sending a message to the print plugins. The added code tweaks the signal handling behavior to block certain signals(Currently blocking only USR1) during critical sections of polling/Netflow processing.,"Hi @vphatarp , if you could carry out this little code add-on that would be indeed fantastic. Thanks very much for the offer. Paolo",True,{}
pmacct/pmacct,https://github.com/pmacct/pmacct,286,2019-05-20T04:55:52Z,2019-06-06T19:54:50Z,2019-06-13T14:38:44Z,MERGED,True,39,27,2,https://github.com/vphatarp,Tweaking signal handling during critical sections of polling/netflow processing for nfacctd,4,[],https://github.com/pmacct/pmacct/pull/286,https://github.com/vphatarp,4,https://github.com/pmacct/pmacct/pull/286#issuecomment-496607679,There have been certain issues in the network regarding how signals are processed causing a block in zmq from sending a message to the print plugins. The added code tweaks the signal handling behavior to block certain signals(Currently blocking only USR1) during critical sections of polling/Netflow processing.,"Hi @paololucente, I apologize for the delay in response. I have added lines to block all the handled signals during netflow processing. Please let me know if you see any errors with the changes. Thanks.",True,{}
pmacct/pmacct,https://github.com/pmacct/pmacct,286,2019-05-20T04:55:52Z,2019-06-06T19:54:50Z,2019-06-13T14:38:44Z,MERGED,True,39,27,2,https://github.com/vphatarp,Tweaking signal handling during critical sections of polling/netflow processing for nfacctd,4,[],https://github.com/pmacct/pmacct/pull/286,https://github.com/paololucente,5,https://github.com/pmacct/pmacct/pull/286#issuecomment-497309669,There have been certain issues in the network regarding how signals are processed causing a block in zmq from sending a message to the print plugins. The added code tweaks the signal handling behavior to block certain signals(Currently blocking only USR1) during critical sections of polling/Netflow processing.,"Hi @vphatarp , to confirm the last patch looks great. Will merge by/on the upcoming weekend. Many thanks again, Paolo",True,{}
pmacct/pmacct,https://github.com/pmacct/pmacct,286,2019-05-20T04:55:52Z,2019-06-06T19:54:50Z,2019-06-13T14:38:44Z,MERGED,True,39,27,2,https://github.com/vphatarp,Tweaking signal handling during critical sections of polling/netflow processing for nfacctd,4,[],https://github.com/pmacct/pmacct/pull/286,https://github.com/pldubouilh,6,https://github.com/pmacct/pmacct/pull/286#issuecomment-500221871,There have been certain issues in the network regarding how signals are processed causing a block in zmq from sending a message to the print plugins. The added code tweaks the signal handling behavior to block certain signals(Currently blocking only USR1) during critical sections of polling/Netflow processing.,"Hi @paololucente, it seems I'm facing a similar issue in pmacctd. I wrote a similar patch as the one here, I'll test this out in the next few weeks see if it fixes the issue, and I'll open a PR if it's relevant (and of interest!).
Thanks!",True,{}
pmacct/pmacct,https://github.com/pmacct/pmacct,286,2019-05-20T04:55:52Z,2019-06-06T19:54:50Z,2019-06-13T14:38:44Z,MERGED,True,39,27,2,https://github.com/vphatarp,Tweaking signal handling during critical sections of polling/netflow processing for nfacctd,4,[],https://github.com/pmacct/pmacct/pull/286,https://github.com/paololucente,7,https://github.com/pmacct/pmacct/pull/286#issuecomment-500561345,There have been certain issues in the network regarding how signals are processed causing a block in zmq from sending a message to the print plugins. The added code tweaks the signal handling behavior to block certain signals(Currently blocking only USR1) during critical sections of polling/Netflow processing.,"Hi Pierre ( @pldubouilh ), interesting. Consider i have ported @vphatarp work to all other daemons, including pmacctd. pmacctd is a strange beast because of the pcap_loop() that you can't really escape (if not restructuring code by using pcap_next()). This is of very interest so please any tests, review and patches are much welcome. @vphatarp work also may suggest a review of signal handling of the plugin themselves could be needed - this is still on my todo list. Paolo",True,{}
pmacct/pmacct,https://github.com/pmacct/pmacct,286,2019-05-20T04:55:52Z,2019-06-06T19:54:50Z,2019-06-13T14:38:44Z,MERGED,True,39,27,2,https://github.com/vphatarp,Tweaking signal handling during critical sections of polling/netflow processing for nfacctd,4,[],https://github.com/pmacct/pmacct/pull/286,https://github.com/pldubouilh,8,https://github.com/pmacct/pmacct/pull/286#issuecomment-501728679,There have been certain issues in the network regarding how signals are processed causing a block in zmq from sending a message to the print plugins. The added code tweaks the signal handling behavior to block certain signals(Currently blocking only USR1) during critical sections of polling/Netflow processing.,"@paololucente alright - didn't see you did a commit with a fix in 25e3807 ! I think that it was pretty much at the same time I wrote my patch :)
thanks a lot for this ! I'll test out in the next few weeks, I'll keep you posted how it fares ðŸ‘",True,{}
pmacct/pmacct,https://github.com/pmacct/pmacct,292,2019-06-11T20:06:33Z,2019-06-29T15:13:43Z,2019-06-29T15:19:17Z,MERGED,True,2,3,1,https://github.com/nimrody,* fix: correct handling of variable length IPFIX fields,1,[],https://github.com/pmacct/pmacct/pull/292,https://github.com/nimrody,1,https://github.com/pmacct/pmacct/pull/292,"nfacctd does not correctly handle variable length IPFIX packets.
The problem stems from the incorrect updating of remlen in resolve_vlen_template(). Since len keeps increasing and holds the current packet length, one cannot update remlen by remlen -= len. The currently remaining number of bytes in the flow is actually remlen - len (assuming remlen is not modified like in the suggested patch).","nfacctd does not correctly handle variable length IPFIX packets.
The problem stems from the incorrect updating of remlen in resolve_vlen_template(). Since len keeps increasing and holds the current packet length, one cannot update remlen by remlen -= len. The currently remaining number of bytes in the flow is actually remlen - len (assuming remlen is not modified like in the suggested patch).",True,{}
pmacct/pmacct,https://github.com/pmacct/pmacct,292,2019-06-11T20:06:33Z,2019-06-29T15:13:43Z,2019-06-29T15:19:17Z,MERGED,True,2,3,1,https://github.com/nimrody,* fix: correct handling of variable length IPFIX fields,1,[],https://github.com/pmacct/pmacct/pull/292,https://github.com/nimrody,2,https://github.com/pmacct/pmacct/pull/292#issuecomment-501574124,"nfacctd does not correctly handle variable length IPFIX packets.
The problem stems from the incorrect updating of remlen in resolve_vlen_template(). Since len keeps increasing and holds the current packet length, one cannot update remlen by remlen -= len. The currently remaining number of bytes in the flow is actually remlen - len (assuming remlen is not modified like in the suggested patch).",Hey @paololucente! I suspect there are more bugs in handling variable length IPFIX information elements. How do you test the code? I haven't seen any tests in the repository.,True,{}
pmacct/pmacct,https://github.com/pmacct/pmacct,292,2019-06-11T20:06:33Z,2019-06-29T15:13:43Z,2019-06-29T15:19:17Z,MERGED,True,2,3,1,https://github.com/nimrody,* fix: correct handling of variable length IPFIX fields,1,[],https://github.com/pmacct/pmacct/pull/292,https://github.com/paololucente,3,https://github.com/pmacct/pmacct/pull/292#issuecomment-501758827,"nfacctd does not correctly handle variable length IPFIX packets.
The problem stems from the incorrect updating of remlen in resolve_vlen_template(). Since len keeps increasing and holds the current packet length, one cannot update remlen by remlen -= len. The currently remaining number of bytes in the flow is actually remlen - len (assuming remlen is not modified like in the suggested patch).","hey @nimrody , thank you and any further code review would be greatly appreciated. And you are right: there are no public tests: i test against both live traffic (general testing, ie. does it crash?) and traces (for sepcific features) but unfortunately neither one nor the other can be made public. Paolo",True,{}
pmacct/pmacct,https://github.com/pmacct/pmacct,292,2019-06-11T20:06:33Z,2019-06-29T15:13:43Z,2019-06-29T15:19:17Z,MERGED,True,2,3,1,https://github.com/nimrody,* fix: correct handling of variable length IPFIX fields,1,[],https://github.com/pmacct/pmacct/pull/292,https://github.com/paololucente,4,https://github.com/pmacct/pmacct/pull/292#issuecomment-506964477,"nfacctd does not correctly handle variable length IPFIX packets.
The problem stems from the incorrect updating of remlen in resolve_vlen_template(). Since len keeps increasing and holds the current packet length, one cannot update remlen by remlen -= len. The currently remaining number of bytes in the flow is actually remlen - len (assuming remlen is not modified like in the suggested patch).","Hi @nimrody , this is merged. I was wondering whether you could get in touch via unicast email (i don't have your email address) to see what are your other suspicions of things not working well wrt IPFIX & variable length fields. Paolo",True,{}
pmacct/pmacct,https://github.com/pmacct/pmacct,294,2019-06-17T12:20:45Z,2019-06-17T14:26:57Z,2019-06-17T14:26:57Z,CLOSED,False,72,14,1,https://github.com/tbearma1,"simplification, correction and adding statistics with SIGUSR2",6,[],https://github.com/pmacct/pmacct/pull/294,https://github.com/tbearma1,1,https://github.com/pmacct/pmacct/pull/294,,,True,{}
pmacct/pmacct,https://github.com/pmacct/pmacct,295,2019-06-17T14:31:58Z,2019-06-22T07:29:19Z,2019-06-22T07:29:19Z,MERGED,True,72,14,1,https://github.com/tbearma1,Simplify,6,[],https://github.com/pmacct/pmacct/pull/295,https://github.com/tbearma1,1,https://github.com/pmacct/pmacct/pull/295,,,True,{}
pmacct/pmacct,https://github.com/pmacct/pmacct,296,2019-07-05T11:22:06Z,2019-07-06T16:52:38Z,2019-07-06T16:52:38Z,CLOSED,False,1,0,1,https://github.com/tbearma1,Subjectnamechange,2,[],https://github.com/pmacct/pmacct/pull/296,https://github.com/tbearma1,1,https://github.com/pmacct/pmacct/pull/296,"Based on the conventions on
https://docs.confluent.io/current/schema-registry/develop/api.html#subjects
we like to have a subjectname with the ending ""value"" (""-value"").
I addet this single strcat myself, compiled it and tested it.
it is generating: ""daisy.dev.flow-metric-raw-value"".","Based on the conventions on
https://docs.confluent.io/current/schema-registry/develop/api.html#subjects
we like to have a subjectname with the ending ""value"" (""-value"").
I addet this single strcat myself, compiled it and tested it.
it is generating: ""daisy.dev.flow-metric-raw-value"".",True,{}
pmacct/pmacct,https://github.com/pmacct/pmacct,301,2019-07-23T21:00:19Z,2019-07-27T00:55:15Z,2019-07-27T00:55:15Z,MERGED,True,275,342,59,https://github.com/msune,Enable -Wall -Werror with --enable-debug and fix all warnings. Add more targets to .travis.yml,24,[],https://github.com/pmacct/pmacct/pull/301,https://github.com/msune,1,https://github.com/pmacct/pmacct/pull/301,"What it does
This patchset:


Adds -Wall -Werror  in all builds


""Fixes"" all warnings generated by such a perilous act, including removing all dead code that gcc indicates. I didn't spend much time, as a lot of warnings come from old Quagga code, and that code is not actively being developed. It would be a good idea to fix some of them ""properly"", but at least now new code will be checked.


It modifies .travis.yml to add 4 compilation envs (targets):



ALL plugins + NDEBUG
ALL plugins + DEBUG
NO plugins + NDEBUG
NO plugins + DEBUG

It should be adding more compilation coverage.
Future work (for someone else with more time :P)


It would be a good idea to move al static arrays defined in the header as static inline, now marked as __attribute(unused)__ to be defined with extern and populated in a .c.


There is a certain degree of duplicity in the log dumpers etc... which could be refactored and unifed.


Peer-review
NOTE: this patchset  is a big (and rather stupid) set of changes, so it should be carefully peer-reviewed.","What it does
This patchset:


Adds -Wall -Werror  in all builds


""Fixes"" all warnings generated by such a perilous act, including removing all dead code that gcc indicates. I didn't spend much time, as a lot of warnings come from old Quagga code, and that code is not actively being developed. It would be a good idea to fix some of them ""properly"", but at least now new code will be checked.


It modifies .travis.yml to add 4 compilation envs (targets):



ALL plugins + NDEBUG
ALL plugins + DEBUG
NO plugins + NDEBUG
NO plugins + DEBUG

It should be adding more compilation coverage.
Future work (for someone else with more time :P)


It would be a good idea to move al static arrays defined in the header as static inline, now marked as __attribute(unused)__ to be defined with extern and populated in a .c.


There is a certain degree of duplicity in the log dumpers etc... which could be refactored and unifed.


Peer-review
NOTE: this patchset  is a big (and rather stupid) set of changes, so it should be carefully peer-reviewed.",True,{}
pmacct/pmacct,https://github.com/pmacct/pmacct,301,2019-07-23T21:00:19Z,2019-07-27T00:55:15Z,2019-07-27T00:55:15Z,MERGED,True,275,342,59,https://github.com/msune,Enable -Wall -Werror with --enable-debug and fix all warnings. Add more targets to .travis.yml,24,[],https://github.com/pmacct/pmacct/pull/301,https://github.com/msune,2,https://github.com/pmacct/pmacct/pull/301#issuecomment-514382181,"What it does
This patchset:


Adds -Wall -Werror  in all builds


""Fixes"" all warnings generated by such a perilous act, including removing all dead code that gcc indicates. I didn't spend much time, as a lot of warnings come from old Quagga code, and that code is not actively being developed. It would be a good idea to fix some of them ""properly"", but at least now new code will be checked.


It modifies .travis.yml to add 4 compilation envs (targets):



ALL plugins + NDEBUG
ALL plugins + DEBUG
NO plugins + NDEBUG
NO plugins + DEBUG

It should be adding more compilation coverage.
Future work (for someone else with more time :P)


It would be a good idea to move al static arrays defined in the header as static inline, now marked as __attribute(unused)__ to be defined with extern and populated in a .c.


There is a certain degree of duplicity in the log dumpers etc... which could be refactored and unifed.


Peer-review
NOTE: this patchset  is a big (and rather stupid) set of changes, so it should be carefully peer-reviewed.","I would enable -Wall -Werror for all compilations, but I didn't want to change default behaviour without asking.",True,{}
pmacct/pmacct,https://github.com/pmacct/pmacct,301,2019-07-23T21:00:19Z,2019-07-27T00:55:15Z,2019-07-27T00:55:15Z,MERGED,True,275,342,59,https://github.com/msune,Enable -Wall -Werror with --enable-debug and fix all warnings. Add more targets to .travis.yml,24,[],https://github.com/pmacct/pmacct/pull/301,https://github.com/msune,3,https://github.com/pmacct/pmacct/pull/301#issuecomment-514382813,"What it does
This patchset:


Adds -Wall -Werror  in all builds


""Fixes"" all warnings generated by such a perilous act, including removing all dead code that gcc indicates. I didn't spend much time, as a lot of warnings come from old Quagga code, and that code is not actively being developed. It would be a good idea to fix some of them ""properly"", but at least now new code will be checked.


It modifies .travis.yml to add 4 compilation envs (targets):



ALL plugins + NDEBUG
ALL plugins + DEBUG
NO plugins + NDEBUG
NO plugins + DEBUG

It should be adding more compilation coverage.
Future work (for someone else with more time :P)


It would be a good idea to move al static arrays defined in the header as static inline, now marked as __attribute(unused)__ to be defined with extern and populated in a .c.


There is a certain degree of duplicity in the log dumpers etc... which could be refactored and unifed.


Peer-review
NOTE: this patchset  is a big (and rather stupid) set of changes, so it should be carefully peer-reviewed.","Another note:
#include <inttypes.h> could be added to one of the main headers, avoiding to include it in every file individually.  I tried to follow the existing approach.",True,{}
pmacct/pmacct,https://github.com/pmacct/pmacct,301,2019-07-23T21:00:19Z,2019-07-27T00:55:15Z,2019-07-27T00:55:15Z,MERGED,True,275,342,59,https://github.com/msune,Enable -Wall -Werror with --enable-debug and fix all warnings. Add more targets to .travis.yml,24,[],https://github.com/pmacct/pmacct/pull/301,https://github.com/msune,4,https://github.com/pmacct/pmacct/pull/301#issuecomment-515586312,"What it does
This patchset:


Adds -Wall -Werror  in all builds


""Fixes"" all warnings generated by such a perilous act, including removing all dead code that gcc indicates. I didn't spend much time, as a lot of warnings come from old Quagga code, and that code is not actively being developed. It would be a good idea to fix some of them ""properly"", but at least now new code will be checked.


It modifies .travis.yml to add 4 compilation envs (targets):



ALL plugins + NDEBUG
ALL plugins + DEBUG
NO plugins + NDEBUG
NO plugins + DEBUG

It should be adding more compilation coverage.
Future work (for someone else with more time :P)


It would be a good idea to move al static arrays defined in the header as static inline, now marked as __attribute(unused)__ to be defined with extern and populated in a .c.


There is a certain degree of duplicity in the log dumpers etc... which could be refactored and unifed.


Peer-review
NOTE: this patchset  is a big (and rather stupid) set of changes, so it should be carefully peer-reviewed.","So,

Ok with inttypes.h being included in a main header
Can I enable -Wall Werror also for production binaries?

On the rebase; it's trivial. The only problem is that master (df20ad49b4167a3cd7b13679402b4a06fd1a4633) is broken for non __BGP_BLACKHOLE_C enabled builds
libtool: compile:  gcc -DPACKAGE_NAME=\""pmacct\"" -DPACKAGE_TARNAME=\""pmacct\"" -DPACKAGE_VERSION=\""1.7.4-git\"" ""-DPACKAGE_STRING=\""pmacct 1.7.4-git\"""" -DPACKAGE_BUGREPORT=\""paolo@pmacct.net\"" -DPACKAGE_URL=\""\"" -DPACKAGE=\""pmacct\"" -DVERSION=\""1.7.4-git\"" -DSTDC_HEADERS=1 -DHAVE_SYS_TYPES_H=1 -DHAVE_SYS_STAT_H=1 -DHAVE_STDLIB_H=1 -DHAVE_STRING_H=1 -DHAVE_MEMORY_H=1 -DHAVE_STRINGS_H=1 -DHAVE_INTTYPES_H=1 -DHAVE_STDINT_H=1 -DHAVE_UNISTD_H=1 -DHAVE_DLFCN_H=1 -DLT_OBJDIR=\"".libs/\"" -DLINUX=1 -DPROGNAME=1 -DIM_LITTLE_ENDIAN=1 -DHAVE_L2=1 -DHAVE_INET_PTON=1 -DHAVE_INET_NTOP=1 -DHAVE_PCAP_H=1 -DHAVE_LIBPCAP=1 -DPCAP_NOBPF=1 -DPCAP_TYPE_linux=1 -DHAVE_DLOPEN=1 -DSTDC_HEADERS=1 -DHAVE_SYS_WAIT_H=1 -DHAVE_GETOPT_H=1 -DHAVE_SYS_SELECT_H=1 -DHAVE_SYS_TIME_H=1 -DHAVE_ARPA_INET_H=1 -DHAVE_ASSERT_H=1 -DHAVE_CTYPE_H=1 -DHAVE_DIRENT_H=1 -DHAVE_DLFCN_H=1 -DHAVE_ERRNO_H=1 -DHAVE_FCNTL_H=1 -DHAVE_GRP_H=1 -DHAVE_INTTYPES_H=1 -DHAVE_LIMITS_H=1 -DHAVE_MALLOC_H=1 -DHAVE_NETDB_H=1 -DHAVE_NET_IF_H=1 -DHAVE_NETINET_IN_H=1 -DHAVE_NETINET_IN_SYSTM_H=1 -DHAVE_NETINET_IP_H=1 -DHAVE_NETINET_UDP_H=1 -DHAVE_PTHREAD_H=1 -DHAVE_PWD_H=1 -DHAVE_SIGNAL_H=1 -DHAVE_STRING_H=1 -DHAVE_SYS_ERRNO_H=1 -DHAVE_SYS_FILE_H=1 -DHAVE_SYS_IOCTL_H=1 -DHAVE_SYSLOG_H=1 -DHAVE_SYS_MMAN_H=1 -DHAVE_SYS_PARAM_H=1 -DHAVE_SYS_POLL_H=1 -DHAVE_SYS_RESOURCE_H=1 -DHAVE_SYS_SOCKET_H=1 -DHAVE_SYS_STAT_H=1 -DHAVE_SYS_TYPES_H=1 -DHAVE_SYS_UN_H=1 -DHAVE_SYS_UTSNAME_H=1 -DHAVE_TIME_H=1 -DHAVE_MATH_H=1 -DHAVE_U_INT64_T=1 -DHAVE_U_INT32_T=1 -DHAVE_U_INT16_T=1 -DHAVE_U_INT8_T=1 -DHAVE_UINT64_T=1 -DHAVE_UINT32_T=1 -DHAVE_UINT16_T=1 -DHAVE_UINT8_T=1 -DHAVE_64BIT_COUNTERS=1 -DHAVE_TRAFFIC_BINS=1 -DHAVE_BGP_BINS=1 -DHAVE_BMP_BINS=1 -DHAVE_ST_BINS=1 -DRETSIGTYPE=void -DHAVE_MALLOPT=1 -DHAVE_TDESTROY=1 -DHAVE_SO_REUSEPORT=1 ""-DCOMPILE_ARGS=\"" '--disable-silent-rules' '--enable-l2' '--enable-64bit' '--enable-traffic-bins' '--enable-bgp-bins' '--enable-bmp-bins' '--enable-st-bins'\"""" -I. -I../../../src/bgp -I../../../src/bgp/.. -O2 -g -O2 -MT libpmbgp_la-bgp_table.lo -MD -MP -MF .deps/libpmbgp_la-bgp_table.Tpo -c ../../../src/bgp/bgp_table.c -o libpmbgp_la-bgp_table.o >/dev/null 2>&1
libtool: compile:  gcc -DPACKAGE_NAME=\""pmacct\"" -DPACKAGE_TARNAME=\""pmacct\"" -DPACKAGE_VERSION=\""1.7.4-git\"" ""-DPACKAGE_STRING=\""pmacct 1.7.4-git\"""" -DPACKAGE_BUGREPORT=\""paolo@pmacct.net\"" -DPACKAGE_URL=\""\"" -DPACKAGE=\""pmacct\"" -DVERSION=\""1.7.4-git\"" -DSTDC_HEADERS=1 -DHAVE_SYS_TYPES_H=1 -DHAVE_SYS_STAT_H=1 -DHAVE_STDLIB_H=1 -DHAVE_STRING_H=1 -DHAVE_MEMORY_H=1 -DHAVE_STRINGS_H=1 -DHAVE_INTTYPES_H=1 -DHAVE_STDINT_H=1 -DHAVE_UNISTD_H=1 -DHAVE_DLFCN_H=1 -DLT_OBJDIR=\"".libs/\"" -DLINUX=1 -DPROGNAME=1 -DIM_LITTLE_ENDIAN=1 -DHAVE_L2=1 -DHAVE_INET_PTON=1 -DHAVE_INET_NTOP=1 -DHAVE_PCAP_H=1 -DHAVE_LIBPCAP=1 -DPCAP_NOBPF=1 -DPCAP_TYPE_linux=1 -DHAVE_DLOPEN=1 -DSTDC_HEADERS=1 -DHAVE_SYS_WAIT_H=1 -DHAVE_GETOPT_H=1 -DHAVE_SYS_SELECT_H=1 -DHAVE_SYS_TIME_H=1 -DHAVE_ARPA_INET_H=1 -DHAVE_ASSERT_H=1 -DHAVE_CTYPE_H=1 -DHAVE_DIRENT_H=1 -DHAVE_DLFCN_H=1 -DHAVE_ERRNO_H=1 -DHAVE_FCNTL_H=1 -DHAVE_GRP_H=1 -DHAVE_INTTYPES_H=1 -DHAVE_LIMITS_H=1 -DHAVE_MALLOC_H=1 -DHAVE_NETDB_H=1 -DHAVE_NET_IF_H=1 -DHAVE_NETINET_IN_H=1 -DHAVE_NETINET_IN_SYSTM_H=1 -DHAVE_NETINET_IP_H=1 -DHAVE_NETINET_UDP_H=1 -DHAVE_PTHREAD_H=1 -DHAVE_PWD_H=1 -DHAVE_SIGNAL_H=1 -DHAVE_STRING_H=1 -DHAVE_SYS_ERRNO_H=1 -DHAVE_SYS_FILE_H=1 -DHAVE_SYS_IOCTL_H=1 -DHAVE_SYSLOG_H=1 -DHAVE_SYS_MMAN_H=1 -DHAVE_SYS_PARAM_H=1 -DHAVE_SYS_POLL_H=1 -DHAVE_SYS_RESOURCE_H=1 -DHAVE_SYS_SOCKET_H=1 -DHAVE_SYS_STAT_H=1 -DHAVE_SYS_TYPES_H=1 -DHAVE_SYS_UN_H=1 -DHAVE_SYS_UTSNAME_H=1 -DHAVE_TIME_H=1 -DHAVE_MATH_H=1 -DHAVE_U_INT64_T=1 -DHAVE_U_INT32_T=1 -DHAVE_U_INT16_T=1 -DHAVE_U_INT8_T=1 -DHAVE_UINT64_T=1 -DHAVE_UINT32_T=1 -DHAVE_UINT16_T=1 -DHAVE_UINT8_T=1 -DHAVE_64BIT_COUNTERS=1 -DHAVE_TRAFFIC_BINS=1 -DHAVE_BGP_BINS=1 -DHAVE_BMP_BINS=1 -DHAVE_ST_BINS=1 -DRETSIGTYPE=void -DHAVE_MALLOPT=1 -DHAVE_TDESTROY=1 -DHAVE_SO_REUSEPORT=1 ""-DCOMPILE_ARGS=\"" '--disable-silent-rules' '--enable-l2' '--enable-64bit' '--enable-traffic-bins' '--enable-bgp-bins' '--enable-bmp-bins' '--enable-st-bins'\"""" -I. -I../../../src/bgp -I../../../src/bgp/.. -O2 -g -O2 -MT libpmbgp_la-bgp_msg.lo -MD -MP -MF .deps/libpmbgp_la-bgp_msg.Tpo -c ../../../src/bgp/bgp_msg.c  -fPIC -DPIC -o .libs/libpmbgp_la-bgp_msg.o
../../../src/bgp/bgp_msg.c: In function 'bgp_nlri_parse':
../../../src/bgp/bgp_msg.c:1378:1: error: expected declaration or statement at end of input
 }
 ^
../../../src/bgp/bgp_msg.c:1378:1: error: expected declaration or statement at end of input
Makefile:491: recipe for target 'libpmbgp_la-bgp_msg.lo' failed
make[2]: *** [libpmbgp_la-bgp_msg.lo] Error 1
make[2]: *** Waiting for unfinished jobs....

bgp_blackhole_evaluate_comms() is correctly ifdef in WITH_ZMQ but not for __BGP_BLACKHOLE_C  in bgp_msg.c, at least, but maybe somewhere else too.
If master is fix, i can finalise the rebase and push the update.
PS.  btw enabling -Wall -Werror will make sure this is not happening anymore",True,{}
pmacct/pmacct,https://github.com/pmacct/pmacct,301,2019-07-23T21:00:19Z,2019-07-27T00:55:15Z,2019-07-27T00:55:15Z,MERGED,True,275,342,59,https://github.com/msune,Enable -Wall -Werror with --enable-debug and fix all warnings. Add more targets to .travis.yml,24,[],https://github.com/pmacct/pmacct/pull/301,https://github.com/paololucente,5,https://github.com/pmacct/pmacct/pull/301#issuecomment-515614490,"What it does
This patchset:


Adds -Wall -Werror  in all builds


""Fixes"" all warnings generated by such a perilous act, including removing all dead code that gcc indicates. I didn't spend much time, as a lot of warnings come from old Quagga code, and that code is not actively being developed. It would be a good idea to fix some of them ""properly"", but at least now new code will be checked.


It modifies .travis.yml to add 4 compilation envs (targets):



ALL plugins + NDEBUG
ALL plugins + DEBUG
NO plugins + NDEBUG
NO plugins + DEBUG

It should be adding more compilation coverage.
Future work (for someone else with more time :P)


It would be a good idea to move al static arrays defined in the header as static inline, now marked as __attribute(unused)__ to be defined with extern and populated in a .c.


There is a certain degree of duplicity in the log dumpers etc... which could be refactored and unifed.


Peer-review
NOTE: this patchset  is a big (and rather stupid) set of changes, so it should be carefully peer-reviewed.",Fixed the WITH_ZMQ ifdefs in bgp_msg.c. Thank you Marc.,True,{}
pmacct/pmacct,https://github.com/pmacct/pmacct,302,2019-07-28T17:29:08Z,2019-07-29T20:21:50Z,2019-07-29T20:21:50Z,MERGED,True,8,4,4,https://github.com/msune,misc: fix warnings introduced 0e25ee3,1,[],https://github.com/pmacct/pmacct/pull/302,https://github.com/msune,1,https://github.com/pmacct/pmacct/pull/302,"Move int no=0 to #ifdef section, so that both Linux and non-Linux
compilations work without warnings.","Move int no=0 to #ifdef section, so that both Linux and non-Linux
compilations work without warnings.",True,{}
pmacct/pmacct,https://github.com/pmacct/pmacct,302,2019-07-28T17:29:08Z,2019-07-29T20:21:50Z,2019-07-29T20:21:50Z,MERGED,True,8,4,4,https://github.com/msune,misc: fix warnings introduced 0e25ee3,1,[],https://github.com/pmacct/pmacct/pull/302,https://github.com/msune,2,https://github.com/pmacct/pmacct/pull/302#issuecomment-515783204,"Move int no=0 to #ifdef section, so that both Linux and non-Linux
compilations work without warnings.","I don't have a non Linux device here, so please double check BSD* is not broken again.",True,{}
pmacct/pmacct,https://github.com/pmacct/pmacct,304,2019-07-30T21:23:37Z,2019-08-03T16:19:19Z,2019-08-03T16:19:31Z,CLOSED,False,3831,3851,157,https://github.com/msune,Fix more warnings detected by GCC8 and clang,38,[],https://github.com/pmacct/pmacct/pull/304,https://github.com/msune,1,https://github.com/pmacct/pmacct/pull/304,"Fix more warnings detectedby gcc8 and clang.
Please review thoroughly.","Fix more warnings detectedby gcc8 and clang.
Please review thoroughly.",True,{}
pmacct/pmacct,https://github.com/pmacct/pmacct,304,2019-07-30T21:23:37Z,2019-08-03T16:19:19Z,2019-08-03T16:19:31Z,CLOSED,False,3831,3851,157,https://github.com/msune,Fix more warnings detected by GCC8 and clang,38,[],https://github.com/pmacct/pmacct/pull/304,https://github.com/msune,2,https://github.com/pmacct/pmacct/pull/304#issuecomment-516991750,"Fix more warnings detectedby gcc8 and clang.
Please review thoroughly.",Please hold this PR,True,{}
pmacct/pmacct,https://github.com/pmacct/pmacct,304,2019-07-30T21:23:37Z,2019-08-03T16:19:19Z,2019-08-03T16:19:31Z,CLOSED,False,3831,3851,157,https://github.com/msune,Fix more warnings detected by GCC8 and clang,38,[],https://github.com/pmacct/pmacct/pull/304,https://github.com/msune,3,https://github.com/pmacct/pmacct/pull/304#issuecomment-517936524,"Fix more warnings detectedby gcc8 and clang.
Please review thoroughly.",Superseeded by #306,True,{}
pmacct/pmacct,https://github.com/pmacct/pmacct,306,2019-08-03T16:18:53Z,2019-08-05T23:20:41Z,2019-08-05T23:20:42Z,MERGED,True,4613,4852,231,https://github.com/msune,"Major refactor: add header guards, remove {EXT, _EXPORT, _C}, fix GCC/clang warnings",28,[],https://github.com/pmacct/pmacct/pull/306,https://github.com/msune,1,https://github.com/pmacct/pmacct/pull/306,"This PR is a long one and superseed #304. The patchset does:

Fixes configure args in .travis CI, so that all plugins are enabled in ""all-plugins"" env
Fixes a log of GCC8, and some clang warnings
Removes all #define EXT #undef, _EXPORT etc... This is a major change. To do that:

All headers now have standard guards ifndef X #define X
Most global variables declarations have been moved to .c files, and left fwd declarations as exter in headers
Fix conditional compilation of some plugins



This PR needs extensive review","This PR is a long one and superseed #304. The patchset does:

Fixes configure args in .travis CI, so that all plugins are enabled in ""all-plugins"" env
Fixes a log of GCC8, and some clang warnings
Removes all #define EXT #undef, _EXPORT etc... This is a major change. To do that:

All headers now have standard guards ifndef X #define X
Most global variables declarations have been moved to .c files, and left fwd declarations as exter in headers
Fix conditional compilation of some plugins



This PR needs extensive review",True,{}
pmacct/pmacct,https://github.com/pmacct/pmacct,306,2019-08-03T16:18:53Z,2019-08-05T23:20:41Z,2019-08-05T23:20:42Z,MERGED,True,4613,4852,231,https://github.com/msune,"Major refactor: add header guards, remove {EXT, _EXPORT, _C}, fix GCC/clang warnings",28,[],https://github.com/pmacct/pmacct/pull/306,https://github.com/msune,2,https://github.com/pmacct/pmacct/pull/306#issuecomment-518382397,"This PR is a long one and superseed #304. The patchset does:

Fixes configure args in .travis CI, so that all plugins are enabled in ""all-plugins"" env
Fixes a log of GCC8, and some clang warnings
Removes all #define EXT #undef, _EXPORT etc... This is a major change. To do that:

All headers now have standard guards ifndef X #define X
Most global variables declarations have been moved to .c files, and left fwd declarations as exter in headers
Fix conditional compilation of some plugins



This PR needs extensive review","Rebased.
Added -Wall -Werror for non-debug builds and fixed all remaining warnings in gcc4.9.",True,{}
pmacct/pmacct,https://github.com/pmacct/pmacct,311,2019-08-14T17:46:20Z,2019-08-15T16:27:31Z,2019-08-15T16:27:31Z,MERGED,True,16,7,1,https://github.com/aka-snh,Duplicate config object (don't re-use) for librdkafka,1,[],https://github.com/pmacct/pmacct/pull/311,https://github.com/aka-snh,1,https://github.com/pmacct/pmacct/pull/311,"According to the librdkafka docs, re-using the config object is an error:
""The conf object is freed by this function and must not be used or destroyed by the application sub-sequently.""
This fixes that error by creating a copy, which also fixes the crash on conf_destroy that was commented out elsewhere in the code.
Tested in daemon mode with BMP.
======

The code and any documentation or other material submitted therewith, (collectively, the â€œContributionâ€) is provided by Akamai Technologies, Inc. (â€œAkamaiâ€) â€œAS ISâ€ WITHOUT REPRESENTATIONS OR WARRANTIES OF ANY KIND, EITHER EXPRESS OR IMPLIED, INCLUDING WITHOUT LIMITATION ANY REPRESENTATIONS OR WARRANTIES OF TITLE, NON-INFRINGEMENT, MERCHANTABILITY, OR FITNESS FOR A PARTICULAR PURPOSES.
The contributor is authorized to submit only this Contribution, dated 2019/08/14, on behalf of Akamai, and the contributor is not authorized to submit additional contributions on behalf of Akamai without further authorization by Akamai.
Akamai shall not be expected to provide, and shall not provide, support for the Contribution.","According to the librdkafka docs, re-using the config object is an error:
""The conf object is freed by this function and must not be used or destroyed by the application sub-sequently.""
This fixes that error by creating a copy, which also fixes the crash on conf_destroy that was commented out elsewhere in the code.
Tested in daemon mode with BMP.
======

The code and any documentation or other material submitted therewith, (collectively, the â€œContributionâ€) is provided by Akamai Technologies, Inc. (â€œAkamaiâ€) â€œAS ISâ€ WITHOUT REPRESENTATIONS OR WARRANTIES OF ANY KIND, EITHER EXPRESS OR IMPLIED, INCLUDING WITHOUT LIMITATION ANY REPRESENTATIONS OR WARRANTIES OF TITLE, NON-INFRINGEMENT, MERCHANTABILITY, OR FITNESS FOR A PARTICULAR PURPOSES.
The contributor is authorized to submit only this Contribution, dated 2019/08/14, on behalf of Akamai, and the contributor is not authorized to submit additional contributions on behalf of Akamai without further authorization by Akamai.
Akamai shall not be expected to provide, and shall not provide, support for the Contribution.",True,{}
pmacct/pmacct,https://github.com/pmacct/pmacct,312,2019-08-18T18:17:33Z,2019-08-19T00:06:00Z,2019-08-19T00:06:00Z,CLOSED,False,748,660,30,https://github.com/rouba002,Missing v8 Postgres file,2,[],https://github.com/pmacct/pmacct/pull/312,https://github.com/rouba002,1,https://github.com/pmacct/pmacct/pull/312,Hopefully correctly created missing Postgres v8 table file.,Hopefully correctly created missing Postgres v8 table file.,True,{}
pmacct/pmacct,https://github.com/pmacct/pmacct,314,2019-08-20T15:42:24Z,2019-08-20T15:44:18Z,2019-08-20T15:44:18Z,MERGED,True,130,0,3,https://github.com/claudio-ortega,build inside a docker container,8,[],https://github.com/pmacct/pmacct/pull/314,https://github.com/claudio-ortega,1,https://github.com/pmacct/pmacct/pull/314,,,True,{}
pmacct/pmacct,https://github.com/pmacct/pmacct,315,2019-08-22T17:00:28Z,2019-08-22T21:39:36Z,2019-08-22T21:39:36Z,MERGED,True,28,28,3,https://github.com/claudio-ortega,possible now to make this procedure fully automated from jenkins,1,[],https://github.com/pmacct/pmacct/pull/315,https://github.com/claudio-ortega,1,https://github.com/pmacct/pmacct/pull/315,,,True,{}
pmacct/pmacct,https://github.com/pmacct/pmacct,316,2019-08-27T07:59:29Z,2019-09-03T16:25:34Z,2019-09-10T14:05:34Z,CLOSED,False,660,660,29,https://github.com/rouba002,FIX SQL schemas identation,4,[],https://github.com/pmacct/pmacct/pull/316,https://github.com/rouba002,1,https://github.com/pmacct/pmacct/pull/316,"This is a little messy history of this commit, but still I hope this will help.
Unified identation of SQL schema files to 4 spaces. Previously mix of TAB and SPACE.","This is a little messy history of this commit, but still I hope this will help.
Unified identation of SQL schema files to 4 spaces. Previously mix of TAB and SPACE.",True,{}
pmacct/pmacct,https://github.com/pmacct/pmacct,316,2019-08-27T07:59:29Z,2019-09-03T16:25:34Z,2019-09-10T14:05:34Z,CLOSED,False,660,660,29,https://github.com/rouba002,FIX SQL schemas identation,4,[],https://github.com/pmacct/pmacct/pull/316,https://github.com/rouba002,2,https://github.com/pmacct/pmacct/pull/316#issuecomment-529676498,"This is a little messy history of this commit, but still I hope this will help.
Unified identation of SQL schema files to 4 spaces. Previously mix of TAB and SPACE.","Hi @paololucente

The identation is incosistent, tabs, 4 spaces, sometimes just 2. So I tried to unify them, I am ok with any suggested option
Yes you are right, as I am not as skilled I tried to remove that particular commit from history, but obviously failed
If I fail again to remove that commit I will create a new fork and fix the identation on fresh.

rouba",True,{}
pmacct/pmacct,https://github.com/pmacct/pmacct,316,2019-08-27T07:59:29Z,2019-09-03T16:25:34Z,2019-09-10T14:05:34Z,CLOSED,False,660,660,29,https://github.com/rouba002,FIX SQL schemas identation,4,[],https://github.com/pmacct/pmacct/pull/316,https://github.com/paololucente,3,https://github.com/pmacct/pmacct/pull/316#issuecomment-529951289,"This is a little messy history of this commit, but still I hope this will help.
Unified identation of SQL schema files to 4 spaces. Previously mix of TAB and SPACE.","Hi @rouba002 , ack about point 1. Please start a new commit to only handle the indentation. Thanks, Paolo",True,{}
pmacct/pmacct,https://github.com/pmacct/pmacct,317,2019-09-10T20:10:20Z,2019-09-13T10:38:42Z,2019-09-13T10:38:42Z,MERGED,True,665,665,30,https://github.com/rouba002,Unified spacing to 4 spaces. Fixed v8 postgresql configuration section.,1,[],https://github.com/pmacct/pmacct/pull/317,https://github.com/rouba002,1,https://github.com/pmacct/pmacct/pull/317,"Hello @paololucente
Here is another attempt to provide the ""unified spacing"" for the config files.
I have chosen 4 spaces as a default and replaced tabs or different number of spaces to this default.
Beside that I have also fixed the PostgreSQL readme to reflect new name of the v8 file. Hopefully this will be now OK.","Hello @paololucente
Here is another attempt to provide the ""unified spacing"" for the config files.
I have chosen 4 spaces as a default and replaced tabs or different number of spaces to this default.
Beside that I have also fixed the PostgreSQL readme to reflect new name of the v8 file. Hopefully this will be now OK.",True,{}
pmacct/pmacct,https://github.com/pmacct/pmacct,320,2019-09-24T08:07:14Z,2019-09-29T10:06:05Z,2019-09-29T10:06:05Z,MERGED,True,282,0,1,None,Collecting BGP metrics with pmacct,1,[],https://github.com/pmacct/pmacct/pull/320,None,1,https://github.com/pmacct/pmacct/pull/320,Add BGP/BMP documentation,Add BGP/BMP documentation,True,{}
pmacct/pmacct,https://github.com/pmacct/pmacct,327,2019-10-02T10:12:12Z,2019-10-03T18:26:47Z,2019-10-03T18:26:47Z,MERGED,True,89504,0,70,https://github.com/jccardonar,Adding telemetry v3 decoder,1,[],https://github.com/pmacct/pmacct/pull/327,https://github.com/jccardonar,1,https://github.com/pmacct/pmacct/pull/327,,,True,{}
pmacct/pmacct,https://github.com/pmacct/pmacct,328,2019-10-04T22:37:53Z,2019-10-05T10:08:17Z,2019-10-05T10:08:17Z,MERGED,True,0,3,1,https://github.com/jccardonar,Removing unnecesary sys path calls,1,[],https://github.com/pmacct/pmacct/pull/328,https://github.com/jccardonar,1,https://github.com/pmacct/pmacct/pull/328,,,True,{}
pmacct/pmacct,https://github.com/pmacct/pmacct,330,2019-10-19T16:44:32Z,2019-10-20T12:27:33Z,2019-10-20T12:27:33Z,MERGED,True,3,3,1,https://github.com/nansenat16,Fixed pgsql plugin Purging cache Crash,1,[],https://github.com/pmacct/pmacct/pull/330,https://github.com/nansenat16,1,https://github.com/pmacct/pmacct/pull/330,"#0  0x00007ffff6d1ac4b in __memcpy_sse2 () from /lib64/libc.so.6
#1  0x0000000000495419 in PG_cache_purge (queue=0x7ffff7f97010, index=976, idata=0x7ffffffe23e0) at pgsql_plugin.c:475
#2  0x000000000049933a in sql_cache_handle_flush_event (idata=0x7ffffffe23e0, refresh_deadline=0x7ffffffe23d8, pt=0x7ffffffe25c0) at sql_common.c:466
#3  0x00000000004942fd in pgsql_plugin (pipe_fd=7, cfgptr=0x91bbf8, ptr=0x836f60 <channels_list>) at pgsql_plugin.c:169
#4  0x000000000043570b in load_plugins (req=0x7fffffffe380) at plugin_hooks.c:287
#5  0x000000000042a795 in main (argc=3, argv=0x7fffffffe578, envp=0x7fffffffe598) at pmacctd.c:918","#0  0x00007ffff6d1ac4b in __memcpy_sse2 () from /lib64/libc.so.6
#1  0x0000000000495419 in PG_cache_purge (queue=0x7ffff7f97010, index=976, idata=0x7ffffffe23e0) at pgsql_plugin.c:475
#2  0x000000000049933a in sql_cache_handle_flush_event (idata=0x7ffffffe23e0, refresh_deadline=0x7ffffffe23d8, pt=0x7ffffffe25c0) at sql_common.c:466
#3  0x00000000004942fd in pgsql_plugin (pipe_fd=7, cfgptr=0x91bbf8, ptr=0x836f60 <channels_list>) at pgsql_plugin.c:169
#4  0x000000000043570b in load_plugins (req=0x7fffffffe380) at plugin_hooks.c:287
#5  0x000000000042a795 in main (argc=3, argv=0x7fffffffe578, envp=0x7fffffffe598) at pmacctd.c:918",True,{}
pmacct/pmacct,https://github.com/pmacct/pmacct,331,2019-10-20T01:16:48Z,2019-10-21T11:09:59Z,2019-10-21T11:16:59Z,MERGED,True,12,12,2,https://github.com/mmlb,Replace ABSTIME with to_timestamp in PG sqls,1,[],https://github.com/pmacct/pmacct/pull/331,https://github.com/mmlb,1,https://github.com/pmacct/pmacct/pull/331,"ABSTIME was dropped in PG commit da6a8d01d391eab45c4b3e0043a1b2b31072f5f
and released in PG12. ABSTIME has been marked deprecated for a long
time, to_timestamp is the supported equivalent.
to_timestamp goes back to at least 8.4 which is the PG version shipped
in RHEL/CentOS 6 and the minimum version supported via configure.ac so
all is good.
Closes #329","ABSTIME was dropped in PG commit da6a8d01d391eab45c4b3e0043a1b2b31072f5f
and released in PG12. ABSTIME has been marked deprecated for a long
time, to_timestamp is the supported equivalent.
to_timestamp goes back to at least 8.4 which is the PG version shipped
in RHEL/CentOS 6 and the minimum version supported via configure.ac so
all is good.
Closes #329",True,{}
pmacct/pmacct,https://github.com/pmacct/pmacct,331,2019-10-20T01:16:48Z,2019-10-21T11:09:59Z,2019-10-21T11:16:59Z,MERGED,True,12,12,2,https://github.com/mmlb,Replace ABSTIME with to_timestamp in PG sqls,1,[],https://github.com/pmacct/pmacct/pull/331,https://github.com/mmlb,2,https://github.com/pmacct/pmacct/pull/331#issuecomment-544449359,"ABSTIME was dropped in PG commit da6a8d01d391eab45c4b3e0043a1b2b31072f5f
and released in PG12. ABSTIME has been marked deprecated for a long
time, to_timestamp is the supported equivalent.
to_timestamp goes back to at least 8.4 which is the PG version shipped
in RHEL/CentOS 6 and the minimum version supported via configure.ac so
all is good.
Closes #329",no problem ðŸ‘,True,{}
pmacct/pmacct,https://github.com/pmacct/pmacct,333,2019-10-26T18:25:44Z,2019-10-27T07:07:41Z,2019-10-27T07:07:42Z,MERGED,True,166,162,1,https://github.com/brusilov,migrating examples/lg/pmbgp.py to python3,4,[],https://github.com/pmacct/pmacct/pull/333,https://github.com/brusilov,1,https://github.com/pmacct/pmacct/pull/333,and improved indentation scheme in script.,and improved indentation scheme in script.,True,{}
pmacct/pmacct,https://github.com/pmacct/pmacct,334,2019-10-29T09:48:27Z,2019-11-07T14:36:10Z,2019-11-11T15:19:07Z,MERGED,True,16,15,4,https://github.com/rbarazzutti,"fix, store the string in the heap (instead of the stack)",7,[],https://github.com/pmacct/pmacct/pull/334,https://github.com/rbarazzutti,1,https://github.com/pmacct/pmacct/pull/334,"fix, using a string in the heap. previously a pointer to the string in the stack was used.","fix, using a string in the heap. previously a pointer to the string in the stack was used.",True,{}
pmacct/pmacct,https://github.com/pmacct/pmacct,335,2019-10-29T10:06:48Z,2019-10-31T00:55:55Z,2019-10-31T11:10:02Z,MERGED,True,2,0,1,https://github.com/rbarazzutti,Missing log attribute,2,[],https://github.com/pmacct/pmacct/pull/335,https://github.com/rbarazzutti,1,https://github.com/pmacct/pmacct/pull/335,Check the existence of peer->log before accessing attributes in order to avoid to have a segfault. Emit a log warning message when it occurs.,Check the existence of peer->log before accessing attributes in order to avoid to have a segfault. Emit a log warning message when it occurs.,True,{}
pmacct/pmacct,https://github.com/pmacct/pmacct,335,2019-10-29T10:06:48Z,2019-10-31T00:55:55Z,2019-10-31T11:10:02Z,MERGED,True,2,0,1,https://github.com/rbarazzutti,Missing log attribute,2,[],https://github.com/pmacct/pmacct/pull/335,https://github.com/rbarazzutti,2,https://github.com/pmacct/pmacct/pull/335#issuecomment-548320189,Check the existence of peer->log before accessing attributes in order to avoid to have a segfault. Emit a log warning message when it occurs.,Thanks!,True,{'THUMBS_UP': ['https://github.com/paololucente']}
pmacct/pmacct,https://github.com/pmacct/pmacct,337,2019-11-04T08:21:46Z,2019-11-08T08:36:50Z,2019-11-08T08:36:51Z,MERGED,True,35,4,5,https://github.com/setup74,add config option: print_allow_empty_file = true|false,2,[],https://github.com/pmacct/pmacct/pull/337,https://github.com/setup74,1,https://github.com/pmacct/pmacct/pull/337,"to create empty file when no cache entry to purge out
My system uses old pmacct's behavior -- to generate empty file even not data to purge exists --
to chech the proof-of-healthyness of sfacctd by the existence of data file (configured per-minute).
But,  new sfacctd omits data file without data. So option is added to behave as old...","to create empty file when no cache entry to purge out
My system uses old pmacct's behavior -- to generate empty file even not data to purge exists --
to chech the proof-of-healthyness of sfacctd by the existence of data file (configured per-minute).
But,  new sfacctd omits data file without data. So option is added to behave as old...",True,{}
pmacct/pmacct,https://github.com/pmacct/pmacct,337,2019-11-04T08:21:46Z,2019-11-08T08:36:50Z,2019-11-08T08:36:51Z,MERGED,True,35,4,5,https://github.com/setup74,add config option: print_allow_empty_file = true|false,2,[],https://github.com/pmacct/pmacct/pull/337,https://github.com/setup74,2,https://github.com/pmacct/pmacct/pull/337#issuecomment-551401924,"to create empty file when no cache entry to purge out
My system uses old pmacct's behavior -- to generate empty file even not data to purge exists --
to chech the proof-of-healthyness of sfacctd by the existence of data file (configured per-minute).
But,  new sfacctd omits data file without data. So option is added to behave as old...","Dear @paololucente,

1)  It sounds good. I've changed to to ""print_write_empty_file""

2a) I rechecked and index > 0, pqq_ptr > 0 is needless, so rollbacked.
      (just from my old experiences on memcpy()'s with size=0,
        but i've tested and checked memcpy() with size=0 is ok)

2b) the save_index > 0 test is protect the use of queue[0] at next lines,
       on the empty casse the use of queue[0]->* caused segfault.
       But i've not confirmed that queue[0] should be NULL or could be some garbage pointer,
       so I just tested save_index > 0 instead of queue[0].
       If queue[0] is always NULL on empty case, the test logic would be good to be queue[0] != NULL

I'v applied 1 and 2a and commited to github.

Thanks.
â€¦
 2019. 11. 8. ì˜¤ì „ 11:50, Paolo Lucente ***@***.***> ìž‘ì„±:

 @paololucente commented on this pull request.

 Dear @setup74 <https://github.com/setup74> ,

 This is a very good initiative worth merging. I have one comment and one question for you: 1) Comment: i may prefer ""print_write_empty_file"" to ""print_allow_empty_file"", is it something you may easily change? If not, i can do it for you upon merging no problem; 2) you introduced some checks, ie. ""index > 0"", ""pqq_ptr > 0"" and ""saved_index > 0"": did you run into some bug or mis-behaviour? If so, can you elaborate a bit? And also if these changes are not specifically related to the feature, it may be worth moving to a different PR. Look forward hearing from you.

 Thanks, Paolo

 â€”
 You are receiving this because you were mentioned.
 Reply to this email directly, view it on GitHub <#337?email_source=notifications&email_token=AHHYWAP7A4ETIKZL3LDUKVTQSTHYRA5CNFSM4JIQIRVKYY3PNVWWK3TUL52HS4DFWFIHK3DMKJSXC5LFON2FEZLWNFSXPKTDN5WW2ZLOORPWSZGOCKZZDKQ#pullrequestreview-313758122>, or unsubscribe <https://github.com/notifications/unsubscribe-auth/AHHYWAPQRL4X7TVPKXNEIXDQSTHYRANCNFSM4JIQIRVA>.",True,{}
pmacct/pmacct,https://github.com/pmacct/pmacct,337,2019-11-04T08:21:46Z,2019-11-08T08:36:50Z,2019-11-08T08:36:51Z,MERGED,True,35,4,5,https://github.com/setup74,add config option: print_allow_empty_file = true|false,2,[],https://github.com/pmacct/pmacct/pull/337,https://github.com/paololucente,3,https://github.com/pmacct/pmacct/pull/337#issuecomment-551436439,"to create empty file when no cache entry to purge out
My system uses old pmacct's behavior -- to generate empty file even not data to purge exists --
to chech the proof-of-healthyness of sfacctd by the existence of data file (configured per-minute).
But,  new sfacctd omits data file without data. So option is added to behave as old...","Wonderful, thank you @setup74 . Merging. Paolo",True,{}
pmacct/pmacct,https://github.com/pmacct/pmacct,344,2019-11-20T06:57:56Z,2019-11-28T17:18:50Z,2019-11-28T17:18:50Z,MERGED,True,2,0,1,https://github.com/jaredmauch,add non blocking i/o,4,[],https://github.com/pmacct/pmacct/pull/344,https://github.com/jaredmauch,1,https://github.com/pmacct/pmacct/pull/344,set non-blocking I/O on socket,set non-blocking I/O on socket,True,{}
pmacct/pmacct,https://github.com/pmacct/pmacct,344,2019-11-20T06:57:56Z,2019-11-28T17:18:50Z,2019-11-28T17:18:50Z,MERGED,True,2,0,1,https://github.com/jaredmauch,add non blocking i/o,4,[],https://github.com/pmacct/pmacct/pull/344,https://github.com/jaredmauch,2,https://github.com/pmacct/pmacct/pull/344#issuecomment-557012116,set non-blocking I/O on socket,"I do not expect any issues from this patch, but hope to provide a report from deployment of the code in our environment later this week",True,{}
pmacct/pmacct,https://github.com/pmacct/pmacct,345,2019-11-20T07:07:33Z,2019-11-20T08:59:56Z,2019-11-20T08:59:56Z,MERGED,True,25,0,1,https://github.com/graf3net,Huawei BMP example configuration,2,[],https://github.com/pmacct/pmacct/pull/345,https://github.com/graf3net,1,https://github.com/pmacct/pmacct/pull/345,,,True,{}
pmacct/pmacct,https://github.com/pmacct/pmacct,349,2019-12-02T15:38:07Z,2019-12-03T15:16:52Z,2019-12-03T15:16:53Z,MERGED,True,3,0,1,https://github.com/jaredmauch,#348 pmacct/pmacct SO_KEEPALIVE,1,[],https://github.com/pmacct/pmacct/pull/349,https://github.com/jaredmauch,1,https://github.com/pmacct/pmacct/pull/349,,,True,{}
pmacct/pmacct,https://github.com/pmacct/pmacct,349,2019-12-02T15:38:07Z,2019-12-03T15:16:52Z,2019-12-03T15:16:53Z,MERGED,True,3,0,1,https://github.com/jaredmauch,#348 pmacct/pmacct SO_KEEPALIVE,1,[],https://github.com/pmacct/pmacct/pull/349,https://github.com/jaredmauch,2,https://github.com/pmacct/pmacct/pull/349#issuecomment-560550205,,We have tested this in production and it seems to resolve what we were seeing.,True,{}
pmacct/pmacct,https://github.com/pmacct/pmacct,352,2019-12-05T21:20:22Z,2019-12-06T14:06:28Z,2019-12-06T14:06:28Z,MERGED,True,6,3,1,https://github.com/jaredmauch,setsockopt SO_KEEPALIVE fix,2,[],https://github.com/pmacct/pmacct/pull/352,https://github.com/jaredmauch,1,https://github.com/pmacct/pmacct/pull/352,"This solves a problem where it should have been on the fd variable but wasn't, and the first connected socket would not receive the SO_KEEPALIVE option.  Move this out of the loop to make it work better and all connections will inherit the SO_KEEPALIVE option","This solves a problem where it should have been on the fd variable but wasn't, and the first connected socket would not receive the SO_KEEPALIVE option.  Move this out of the loop to make it work better and all connections will inherit the SO_KEEPALIVE option",True,{}
pmacct/pmacct,https://github.com/pmacct/pmacct,355,2019-12-29T13:12:08Z,2020-01-01T17:20:47Z,2020-01-01T17:20:47Z,MERGED,True,6,1,2,https://github.com/jer-gentoo,Fix building against >=nDPI-3.0,1,[],https://github.com/pmacct/pmacct/pull/355,https://github.com/jer-gentoo,1,https://github.com/pmacct/pmacct/pull/355,"ndpi.c: In function 'pm_ndpi_packet_processing':
ndpi.c:377:33: error: too few arguments to function 'ndpi_detection_giveup'
  377 |       flow->detected_protocol = ndpi_detection_giveup(workflow->ndpi_struct, flow->ndpi_flow, workflow->prefs.protocol_guess);
      |                                 ^~~~~~~~~~~~~~~~~~~~~
In file included from /usr/include/ndpi/ndpi_main.h:31,
                 from ../pmacct.h:97,
                 from ndpi.c:30:
/usr/include/ndpi/ndpi_api.h:232:17: note: declared here

Signed-off-by: Jeroen Roovers jer@gentoo.org","ndpi.c: In function 'pm_ndpi_packet_processing':
ndpi.c:377:33: error: too few arguments to function 'ndpi_detection_giveup'
  377 |       flow->detected_protocol = ndpi_detection_giveup(workflow->ndpi_struct, flow->ndpi_flow, workflow->prefs.protocol_guess);
      |                                 ^~~~~~~~~~~~~~~~~~~~~
In file included from /usr/include/ndpi/ndpi_main.h:31,
                 from ../pmacct.h:97,
                 from ndpi.c:30:
/usr/include/ndpi/ndpi_api.h:232:17: note: declared here

Signed-off-by: Jeroen Roovers jer@gentoo.org",True,{}
pmacct/pmacct,https://github.com/pmacct/pmacct,355,2019-12-29T13:12:08Z,2020-01-01T17:20:47Z,2020-01-01T17:20:47Z,MERGED,True,6,1,2,https://github.com/jer-gentoo,Fix building against >=nDPI-3.0,1,[],https://github.com/pmacct/pmacct/pull/355,https://github.com/jer-gentoo,2,https://github.com/pmacct/pmacct/pull/355#issuecomment-569505161,"ndpi.c: In function 'pm_ndpi_packet_processing':
ndpi.c:377:33: error: too few arguments to function 'ndpi_detection_giveup'
  377 |       flow->detected_protocol = ndpi_detection_giveup(workflow->ndpi_struct, flow->ndpi_flow, workflow->prefs.protocol_guess);
      |                                 ^~~~~~~~~~~~~~~~~~~~~
In file included from /usr/include/ndpi/ndpi_main.h:31,
                 from ../pmacct.h:97,
                 from ndpi.c:30:
/usr/include/ndpi/ndpi_api.h:232:17: note: declared here

Signed-off-by: Jeroen Roovers jer@gentoo.org",Downstream: https://bugs.gentoo.org/704072,True,{}
pmacct/pmacct,https://github.com/pmacct/pmacct,358,2020-01-14T20:21:49Z,2020-01-14T23:56:46Z,2020-01-14T23:56:46Z,MERGED,True,1,1,1,https://github.com/msune,.travis.yml: fix CI (REDIS),1,[],https://github.com/pmacct/pmacct/pull/358,https://github.com/msune,1,https://github.com/pmacct/pmacct/pull/358,"Short description
Commit '06aaa44d' broke travis, as the working directory is
incorrect and REDIS is not installed on target.
Checklist

 compiled & tested this code","Short description
Commit '06aaa44d' broke travis, as the working directory is
incorrect and REDIS is not installed on target.
Checklist

 compiled & tested this code",True,{}
pmacct/pmacct,https://github.com/pmacct/pmacct,362,2020-01-24T18:46:44Z,2020-01-27T14:57:08Z,2020-01-27T14:57:42Z,MERGED,True,1,1,1,https://github.com/bcavns01,Fixing BMP v flag test to allow IPv6 prefixes/peer,1,[],https://github.com/pmacct/pmacct/pull/362,https://github.com/bcavns01,1,https://github.com/pmacct/pmacct/pull/362,"The test on line 856 was originally checking else if (version == 1) (*family) = AF_INET6; which it seems will never be true.
In the case of IPv6:
bph->flags & BMP_PEER_FLAGS_ARI_V == 11010000 & 0x80 == 10000000 (binary) ==  128 integer
Debug output:
INFO bmp_peer_hdr_get_v_flag flags == 11010000
INFO bmp_peer_hdr_get_v_flag version (bin) == 10000000
INFO bmp_peer_hdr_get_v_flag version (int) == 128

With that check never passing for v6, family was never being set to a non-zero value, and the if (bdata.family) { test on line 434 never passes, and so v6 peers were always skipped.
Short description

Checklist

I have:

 added the LICENSE template to new files
 compiled & tested this code
 included documentation (including possible behaviour changes)","The test on line 856 was originally checking else if (version == 1) (*family) = AF_INET6; which it seems will never be true.
In the case of IPv6:
bph->flags & BMP_PEER_FLAGS_ARI_V == 11010000 & 0x80 == 10000000 (binary) ==  128 integer
Debug output:
INFO bmp_peer_hdr_get_v_flag flags == 11010000
INFO bmp_peer_hdr_get_v_flag version (bin) == 10000000
INFO bmp_peer_hdr_get_v_flag version (int) == 128

With that check never passing for v6, family was never being set to a non-zero value, and the if (bdata.family) { test on line 434 never passes, and so v6 peers were always skipped.
Short description

Checklist

I have:

 added the LICENSE template to new files
 compiled & tested this code
 included documentation (including possible behaviour changes)",True,{}
pmacct/pmacct,https://github.com/pmacct/pmacct,365,2020-02-02T21:07:39Z,2020-02-02T21:20:05Z,2020-02-02T21:20:05Z,MERGED,True,1,1,1,https://github.com/msune,RELICENSE: consent msune,1,[],https://github.com/pmacct/pmacct/pull/365,https://github.com/msune,1,https://github.com/pmacct/pmacct/pull/365,"By this commit, I consent to relicense all my contributions I made
or will make to the pmacct project to BSD-style license.
Move msune to the ""agreed"" section.","By this commit, I consent to relicense all my contributions I made
or will make to the pmacct project to BSD-style license.
Move msune to the ""agreed"" section.",True,{}
pmacct/pmacct,https://github.com/pmacct/pmacct,366,2020-02-11T11:42:26Z,2020-03-05T20:26:54Z,2020-03-05T20:26:55Z,MERGED,True,403,271,5,https://github.com/rbarazzutti,Telemetry multiprocessing,20,[],https://github.com/pmacct/pmacct/pull/366,https://github.com/rbarazzutti,1,https://github.com/pmacct/pmacct/pull/366,"Use of multiple processes for the Kafka Avro exporter
The Kafka connector creates a pool of threads.
Python threads are running on the same physical core (discussion on Stackoverflow), to address this, a multi-process approach is required to leverage the potential of multi-core/processors architectures.
Checklist

 implemented multi-processes in the Kafka Avro export connector
 adapted the logging mechanism for multi-processing
 added config entry for processes pool (thanks @tbearma1)
 tested on real infrastructure with @tbearma1","Use of multiple processes for the Kafka Avro exporter
The Kafka connector creates a pool of threads.
Python threads are running on the same physical core (discussion on Stackoverflow), to address this, a multi-process approach is required to leverage the potential of multi-core/processors architectures.
Checklist

 implemented multi-processes in the Kafka Avro export connector
 adapted the logging mechanism for multi-processing
 added config entry for processes pool (thanks @tbearma1)
 tested on real infrastructure with @tbearma1",True,"{'THUMBS_UP': ['https://github.com/tbearma1'], 'HEART': ['https://github.com/tbearma1']}"
pmacct/pmacct,https://github.com/pmacct/pmacct,367,2020-02-11T13:45:31Z,2020-03-05T20:32:34Z,2020-03-05T20:32:34Z,MERGED,True,4,4,3,https://github.com/rbarazzutti,Small typos,1,[],https://github.com/pmacct/pmacct/pull/367,https://github.com/rbarazzutti,1,https://github.com/pmacct/pmacct/pull/367,Small typos,Small typos,True,{}
pmacct/pmacct,https://github.com/pmacct/pmacct,368,2020-02-11T13:46:55Z,2020-03-05T20:35:11Z,2020-03-05T20:35:12Z,MERGED,True,57,38,19,https://github.com/rbarazzutti,Copyright in telemetry,3,[],https://github.com/pmacct/pmacct/pull/368,https://github.com/rbarazzutti,1,https://github.com/pmacct/pmacct/pull/368,"Copyright in telemetry

 changed to 2020
 added myself ;-)","Copyright in telemetry

 changed to 2020
 added myself ;-)",True,{}
pmacct/pmacct,https://github.com/pmacct/pmacct,369,2020-02-12T17:29:25Z,2020-02-12T17:38:30Z,2020-02-12T17:38:31Z,MERGED,True,1,1,1,https://github.com/rbarazzutti,Travis: download Avro-C binding from Apache archives,1,[],https://github.com/pmacct/pmacct/pull/369,https://github.com/rbarazzutti,1,https://github.com/pmacct/pmacct/pull/369,"Short description
Travis: download Avro-C binding from the Apache archive website.","Short description
Travis: download Avro-C binding from the Apache archive website.",True,{}
pmacct/pmacct,https://github.com/pmacct/pmacct,374,2020-03-17T10:05:57Z,2020-03-17T14:28:19Z,2020-03-17T14:28:19Z,MERGED,True,9,2,2,https://github.com/rbarazzutti,Check schema presence,2,[],https://github.com/pmacct/pmacct/pull/374,https://github.com/rbarazzutti,1,https://github.com/pmacct/pmacct/pull/374,"Short description
Check if schema is not null before sending it to libserdes, if null (not set), do log this as an error.
Checklist
I have:

 compiled & tested this code","Short description
Check if schema is not null before sending it to libserdes, if null (not set), do log this as an error.
Checklist
I have:

 compiled & tested this code",True,{}
pmacct/pmacct,https://github.com/pmacct/pmacct,381,2020-04-08T15:34:19Z,2020-04-08T15:39:22Z,2020-04-08T15:39:22Z,MERGED,True,208,171,11,https://github.com/claudio-ortega,"building pmacct inside a docker container, all plugin variants, travis file",2,[],https://github.com/pmacct/pmacct/pull/381,https://github.com/claudio-ortega,1,https://github.com/pmacct/pmacct/pull/381,"building pmacct inside a docker container, all plugin variants, travis file","building pmacct inside a docker container, all plugin variants, travis file",True,{}
pmacct/pmacct,https://github.com/pmacct/pmacct,382,2020-04-12T08:35:13Z,2020-04-13T13:11:59Z,2020-04-15T08:35:15Z,MERGED,True,3,2,2,https://github.com/jer-gentoo,ndpi: Fix building against nDPI 3.2,1,[],https://github.com/pmacct/pmacct/pull/382,https://github.com/jer-gentoo,1,https://github.com/pmacct/pmacct/pull/382,,,True,{}
pmacct/pmacct,https://github.com/pmacct/pmacct,382,2020-04-12T08:35:13Z,2020-04-13T13:11:59Z,2020-04-15T08:35:15Z,MERGED,True,3,2,2,https://github.com/jer-gentoo,ndpi: Fix building against nDPI 3.2,1,[],https://github.com/pmacct/pmacct/pull/382,https://github.com/jer-gentoo,2,https://github.com/pmacct/pmacct/pull/382#issuecomment-612584435,,"The prefs initialisation still needs work:
ndpi_util.c: In function â€˜pm_ndpi_workflow_initâ€™:
ndpi_util.c:33:35: warning: initialization of â€˜ndpi_init_prefsâ€™ {aka â€˜unsigned intâ€™} from â€˜void *â€™ makes integer from pointer without a cast [-Wint-conversion]
   33 |   ndpi_init_prefs pm_ndpi_prefs = NULL;
      |                                   ^~~~
In file included from /usr/include/string.h:494,
                 from ../pmacct.h:38,
                 from ndpi_util.c:22:
In function â€˜strncpyâ€™,
    inlined from â€˜pm_ndpi_export_proto_to_classâ€™ at ndpi_util.c:104:7:
/usr/include/bits/string_fortified.h:106:10: warning: â€˜__builtin_strncpyâ€™ specified bound 32 equals destination size [-Wstringop-truncation]
  106 |   return __builtin___strncpy_chk (__dest, __src, __len, __bos (__dest));
      |          ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~",True,{}
pmacct/pmacct,https://github.com/pmacct/pmacct,382,2020-04-12T08:35:13Z,2020-04-13T13:11:59Z,2020-04-15T08:35:15Z,MERGED,True,3,2,2,https://github.com/jer-gentoo,ndpi: Fix building against nDPI 3.2,1,[],https://github.com/pmacct/pmacct/pull/382,https://github.com/jer-gentoo,3,https://github.com/pmacct/pmacct/pull/382#issuecomment-612588073,,"The prefs initialisation still needs work:
ndpi_util.c: In function â€˜pm_ndpi_workflow_initâ€™:
ndpi_util.c:33:35: warning: initialization of â€˜ndpi_init_prefsâ€™ {aka â€˜unsigned intâ€™} from â€˜void *â€™ makes integer from pointer without a cast [-Wint-conversion]
   33 |   ndpi_init_prefs pm_ndpi_prefs = NULL;
      |                                   ^~~~
In file included from /usr/include/string.h:494,
                 from ../pmacct.h:38,
                 from ndpi_util.c:22:
In function â€˜strncpyâ€™,
    inlined from â€˜pm_ndpi_export_proto_to_classâ€™ at ndpi_util.c:104:7:
/usr/include/bits/string_fortified.h:106:10: warning: â€˜__builtin_strncpyâ€™ specified bound 32 equals destination size [-Wstringop-truncation]
  106 |   return __builtin___strncpy_chk (__dest, __src, __len, __bos (__dest));
      |          ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~


That ought to be fixed now.",True,{}
pmacct/pmacct,https://github.com/pmacct/pmacct,383,2020-04-13T23:51:03Z,2020-04-14T13:10:20Z,2020-04-14T13:10:20Z,MERGED,True,111,40,7,https://github.com/claudio-ortega,building on both centos/ubuntu in paralell,16,[],https://github.com/pmacct/pmacct/pull/383,https://github.com/claudio-ortega,1,https://github.com/pmacct/pmacct/pull/383,"Short description

Checklist

I have:

 added the LICENSE template to new files
 compiled & tested this code
 included documentation (including possible behaviour changes)","Short description

Checklist

I have:

 added the LICENSE template to new files
 compiled & tested this code
 included documentation (including possible behaviour changes)",True,{}
pmacct/pmacct,https://github.com/pmacct/pmacct,384,2020-04-15T17:26:32Z,2020-04-15T19:27:58Z,2020-04-15T19:27:58Z,MERGED,True,5,4,2,https://github.com/claudio-ortega,changing config for 'single' variant ,3,[],https://github.com/pmacct/pmacct/pull/384,https://github.com/claudio-ortega,1,https://github.com/pmacct/pmacct/pull/384,"Short description

Checklist

I have:

 added the LICENSE template to new files
 compiled & tested this code
 included documentation (including possible behaviour changes)","Short description

Checklist

I have:

 added the LICENSE template to new files
 compiled & tested this code
 included documentation (including possible behaviour changes)",True,{}
pmacct/pmacct,https://github.com/pmacct/pmacct,385,2020-04-15T23:07:25Z,2020-04-16T12:37:03Z,2020-04-16T12:37:03Z,MERGED,True,0,2,1,https://github.com/cyclops1982,Fix compile on debian buster,1,[],https://github.com/pmacct/pmacct/pull/385,https://github.com/cyclops1982,1,https://github.com/pmacct/pmacct/pull/385,"Short description
I had to make this modification on a Debian 10.3 machine.
I received this error while running make before this patch:
make[2]: Entering directory '/root/pmacct/src'
CC       libdaemons_la-util.lo
In file included from mysql_plugin.h:25,
from util.c:29:
/usr/include/mariadb/mysql_version.h:3:2: error: #warning This file should not be included by clients, include only <mysql.h> [-Werror=cpp]
#warning This file should not be included by clients, include only <mysql.h>
^~~~~~~
cc1: all warnings being treated as errors
The patched fixed that.
Checklist
I have:

 added the LICENSE template to new files
 compiled & tested this code
 included documentation (including possible behaviour changes)","Short description
I had to make this modification on a Debian 10.3 machine.
I received this error while running make before this patch:
make[2]: Entering directory '/root/pmacct/src'
CC       libdaemons_la-util.lo
In file included from mysql_plugin.h:25,
from util.c:29:
/usr/include/mariadb/mysql_version.h:3:2: error: #warning This file should not be included by clients, include only <mysql.h> [-Werror=cpp]
#warning This file should not be included by clients, include only <mysql.h>
^~~~~~~
cc1: all warnings being treated as errors
The patched fixed that.
Checklist
I have:

 added the LICENSE template to new files
 compiled & tested this code
 included documentation (including possible behaviour changes)",True,{}
pmacct/pmacct,https://github.com/pmacct/pmacct,386,2020-04-17T15:43:55Z,2020-04-17T19:02:02Z,2020-04-17T19:37:56Z,MERGED,True,79,1,5,https://github.com/claudio-ortega,adding copyright notices to docker/*.sh files,2,[],https://github.com/pmacct/pmacct/pull/386,https://github.com/claudio-ortega,1,https://github.com/pmacct/pmacct/pull/386,"Short description

Checklist

I have:

 added the LICENSE template to new files
 compiled & tested this code
 included documentation (including possible behaviour changes)","Short description

Checklist

I have:

 added the LICENSE template to new files
 compiled & tested this code
 included documentation (including possible behaviour changes)",True,{}
pmacct/pmacct,https://github.com/pmacct/pmacct,386,2020-04-17T15:43:55Z,2020-04-17T19:02:02Z,2020-04-17T19:37:56Z,MERGED,True,79,1,5,https://github.com/claudio-ortega,adding copyright notices to docker/*.sh files,2,[],https://github.com/pmacct/pmacct/pull/386,https://github.com/job,2,https://github.com/pmacct/pmacct/pull/386#issuecomment-615320588,"Short description

Checklist

I have:

 added the LICENSE template to new files
 compiled & tested this code
 included documentation (including possible behaviour changes)","Dear @claudio-ortega
pmacct is attempting to relicense the project to a more permissive license. Could this be an opportunity to consider putting a different license on these files?
https://www.mail-archive.com/pmacct-discussion@pmacct.net/msg03881.html",True,{}
pmacct/pmacct,https://github.com/pmacct/pmacct,386,2020-04-17T15:43:55Z,2020-04-17T19:02:02Z,2020-04-17T19:37:56Z,MERGED,True,79,1,5,https://github.com/claudio-ortega,adding copyright notices to docker/*.sh files,2,[],https://github.com/pmacct/pmacct/pull/386,https://github.com/claudio-ortega,3,https://github.com/pmacct/pmacct/pull/386#issuecomment-615355781,"Short description

Checklist

I have:

 added the LICENSE template to new files
 compiled & tested this code
 included documentation (including possible behaviour changes)","Hey there,
Sure, please use any license you like the most.
Paolo provided the license verbose you read now as a starting point, but
please feel free to adapt as needed.
Claudio
â€¦
On Fri, Apr 17, 2020 at 8:48 AM Job Snijders ***@***.***> wrote:
 Dear @claudio-ortega <https://github.com/claudio-ortega>

 pmacct is attempting to relicense the project to a more permissive
 license. Could this be an opportunity to consider putting a different
 license on these files?

 ***@***.***/msg03881.html

 â€”
 You are receiving this because you were mentioned.
 Reply to this email directly, view it on GitHub
 <#386 (comment)>, or
 unsubscribe
 <https://github.com/notifications/unsubscribe-auth/ADF4SM3Q2CM5YUZQYM47IFTRNB24HANCNFSM4MK3BFGQ>
 .",True,{}
pmacct/pmacct,https://github.com/pmacct/pmacct,386,2020-04-17T15:43:55Z,2020-04-17T19:02:02Z,2020-04-17T19:37:56Z,MERGED,True,79,1,5,https://github.com/claudio-ortega,adding copyright notices to docker/*.sh files,2,[],https://github.com/pmacct/pmacct/pull/386,https://github.com/job,4,https://github.com/pmacct/pmacct/pull/386#issuecomment-615357168,"Short description

Checklist

I have:

 added the LICENSE template to new files
 compiled & tested this code
 included documentation (including possible behaviour changes)","You can maybe use the following:
Copyright (c) YYYY YOUR NAME HERE <user@your.dom.ain>

Permission to use, copy, modify, and distribute this software for any
purpose with or without fee is hereby granted, provided that the above
copyright notice and this permission notice appear in all copies.

THE SOFTWARE IS PROVIDED ""AS IS"" AND THE AUTHOR DISCLAIMS ALL WARRANTIES
WITH REGARD TO THIS SOFTWARE INCLUDING ALL IMPLIED WARRANTIES OF
MERCHANTABILITY AND FITNESS. IN NO EVENT SHALL THE AUTHOR BE LIABLE FOR
ANY SPECIAL, DIRECT, INDIRECT, OR CONSEQUENTIAL DAMAGES OR ANY DAMAGES
WHATSOEVER RESULTING FROM LOSS OF USE, DATA OR PROFITS, WHETHER IN AN
ACTION OF CONTRACT, NEGLIGENCE OR OTHER TORTIOUS ACTION, ARISING OUT OF
OR IN CONNECTION WITH THE USE OR PERFORMANCE OF THIS SOFTWARE.",True,{}
pmacct/pmacct,https://github.com/pmacct/pmacct,388,2020-04-22T13:58:36Z,2020-04-22T17:40:31Z,2020-04-22T17:40:31Z,MERGED,True,4,1,1,https://github.com/aleksandrgilfanov,uacctd: handle non-ethernet packets correctly,1,[],https://github.com/pmacct/pmacct/pull/388,https://github.com/aleksandrgilfanov,1,https://github.com/pmacct/pmacct/pull/388,"Use mac_len = 0 for non-ethernet packets. In this case zeroed ethernet header
is used.
Short description

Checklist

I have:

 added the LICENSE template to new files
 compiled & tested this code
 included documentation (including possible behaviour changes)","Use mac_len = 0 for non-ethernet packets. In this case zeroed ethernet header
is used.
Short description

Checklist

I have:

 added the LICENSE template to new files
 compiled & tested this code
 included documentation (including possible behaviour changes)",True,{}
pmacct/pmacct,https://github.com/pmacct/pmacct,395,2020-05-17T14:53:50Z,2020-05-17T19:45:53Z,2020-05-17T19:45:53Z,MERGED,True,4,0,1,https://github.com/utoni,"* fix, nDPI >= 3.2 requires to call ndpi_finalize_initalization",1,[],https://github.com/pmacct/pmacct/pull/395,https://github.com/utoni,1,https://github.com/pmacct/pmacct/pull/395,"Signed-off-by: Toni Uhlig matzeton@googlemail.com
Short description
Newer nDPI libraries require calling ndpi_finalize_initialization somewhere after the detection module init finished.
It was first introduced in nDPI-0558d641f and released with 3.2
Checklist

I have:

 added the LICENSE template to new files
 compiled & tested this code
 included documentation (including possible behaviour changes)","Signed-off-by: Toni Uhlig matzeton@googlemail.com
Short description
Newer nDPI libraries require calling ndpi_finalize_initialization somewhere after the detection module init finished.
It was first introduced in nDPI-0558d641f and released with 3.2
Checklist

I have:

 added the LICENSE template to new files
 compiled & tested this code
 included documentation (including possible behaviour changes)",True,{}
pmacct/pmacct,https://github.com/pmacct/pmacct,397,2020-05-24T13:55:21Z,2020-05-25T14:30:08Z,2020-05-25T14:30:08Z,MERGED,True,368,281,1,https://github.com/graf3net,Added new attributes and BMP message type 5 and 6,1,[],https://github.com/pmacct/pmacct/pull/397,https://github.com/graf3net,1,https://github.com/pmacct/pmacct/pull/397,"Short description

Checklist

I have:

 added the LICENSE template to new files
 compiled & tested this code
 included documentation (including possible behaviour changes)","Short description

Checklist

I have:

 added the LICENSE template to new files
 compiled & tested this code
 included documentation (including possible behaviour changes)",True,{}
pmacct/pmacct,https://github.com/pmacct/pmacct,397,2020-05-24T13:55:21Z,2020-05-25T14:30:08Z,2020-05-25T14:30:08Z,MERGED,True,368,281,1,https://github.com/graf3net,Added new attributes and BMP message type 5 and 6,1,[],https://github.com/pmacct/pmacct/pull/397,https://github.com/graf3net,2,https://github.com/pmacct/pmacct/pull/397#issuecomment-633234970,"Short description

Checklist

I have:

 added the LICENSE template to new files
 compiled & tested this code
 included documentation (including possible behaviour changes)","Hi Paolo, as requested, I updated the document not only with the added BGP ADD-Path, AIGP, Prefix-SID attributes but also with the updated from the BMP hackathon such as Peer RD, path marking and message type 5 and 6.",True,{}
pmacct/pmacct,https://github.com/pmacct/pmacct,399,2020-06-02T00:51:14Z,2020-06-03T14:40:40Z,2020-06-03T14:40:40Z,MERGED,True,185,238,9,https://github.com/msune,"ci: use dedicated jobs per {docker, config_flags}",2,[],https://github.com/pmacct/pmacct/pull/399,https://github.com/msune,1,https://github.com/pmacct/pmacct/pull/399,"Short description
Prior to this commit there were only 2 jobs being launched, in which
all possible combinations of pmacct flags were compiled.
This had two fundamental problems:

Jobs could not run in parallel so could not exploit parallel jobs
of travis CI (even in free tier)
If a combination was failing, the entire job was failing

In addition all the log was in the same very long job, making it
difficult to debug.
Unfortunately, parallel jobs is not improving pipeline times, since
docker generation - which takes > 70% of time - cannot be reused
across jobs.
This should be addressed in a later commit, in which two different stages
are defined.
This commit builds on top of:

Moves dependencies to Dockerfile
Moves all related files from docker/ to ci/. This is to avoid users
thinking that docker file is a pmacct docker => btw this might be
a good idea; someone thought of adding a pmacct public layer?
Refactors CI to use a job per tuple {docker, config_flags}. It has
a single stage and it prefixes all jobs with name:

Ubuntu Bionic
Centos 8.1


Tries to simplify the CI code
Adds minor trick to pmacct ./configure to show config.log in case
of failure

Checklist
I have:

 added the LICENSE template to new files
 compiled & tested this code
 included documentation (including possible behaviour changes)","Short description
Prior to this commit there were only 2 jobs being launched, in which
all possible combinations of pmacct flags were compiled.
This had two fundamental problems:

Jobs could not run in parallel so could not exploit parallel jobs
of travis CI (even in free tier)
If a combination was failing, the entire job was failing

In addition all the log was in the same very long job, making it
difficult to debug.
Unfortunately, parallel jobs is not improving pipeline times, since
docker generation - which takes > 70% of time - cannot be reused
across jobs.
This should be addressed in a later commit, in which two different stages
are defined.
This commit builds on top of:

Moves dependencies to Dockerfile
Moves all related files from docker/ to ci/. This is to avoid users
thinking that docker file is a pmacct docker => btw this might be
a good idea; someone thought of adding a pmacct public layer?
Refactors CI to use a job per tuple {docker, config_flags}. It has
a single stage and it prefixes all jobs with name:

Ubuntu Bionic
Centos 8.1


Tries to simplify the CI code
Adds minor trick to pmacct ./configure to show config.log in case
of failure

Checklist
I have:

 added the LICENSE template to new files
 compiled & tested this code
 included documentation (including possible behaviour changes)",True,{}
pmacct/pmacct,https://github.com/pmacct/pmacct,399,2020-06-02T00:51:14Z,2020-06-03T14:40:40Z,2020-06-03T14:40:40Z,MERGED,True,185,238,9,https://github.com/msune,"ci: use dedicated jobs per {docker, config_flags}",2,[],https://github.com/pmacct/pmacct/pull/399,https://github.com/msune,2,https://github.com/pmacct/pmacct/pull/399#issuecomment-637204541,"Short description
Prior to this commit there were only 2 jobs being launched, in which
all possible combinations of pmacct flags were compiled.
This had two fundamental problems:

Jobs could not run in parallel so could not exploit parallel jobs
of travis CI (even in free tier)
If a combination was failing, the entire job was failing

In addition all the log was in the same very long job, making it
difficult to debug.
Unfortunately, parallel jobs is not improving pipeline times, since
docker generation - which takes > 70% of time - cannot be reused
across jobs.
This should be addressed in a later commit, in which two different stages
are defined.
This commit builds on top of:

Moves dependencies to Dockerfile
Moves all related files from docker/ to ci/. This is to avoid users
thinking that docker file is a pmacct docker => btw this might be
a good idea; someone thought of adding a pmacct public layer?
Refactors CI to use a job per tuple {docker, config_flags}. It has
a single stage and it prefixes all jobs with name:

Ubuntu Bionic
Centos 8.1


Tries to simplify the CI code
Adds minor trick to pmacct ./configure to show config.log in case
of failure

Checklist
I have:

 added the LICENSE template to new files
 compiled & tested this code
 included documentation (including possible behaviour changes)","Several comments:
Take a look at the job structure, and let me know your thoughts: https://travis-ci.org/github/pmacct/pmacct/builds/693668890


Bonus: negative diff.


Multi-stage, with docker caching is here: msune@7695280.  How it would look like:


https://travis-ci.org/github/msune/pmacct/builds/693668419
But it seems an external ""storage"" is needed (and btw, care needs to not break pipelines in PRs from other repos)... this is annoying. Maybe workspaces?
https://docs.travis-ci.com/user/using-workspaces/

Did you consider having a Dockerfile with say, Debian + pmacct ""ready to go""?",True,{}
pmacct/pmacct,https://github.com/pmacct/pmacct,399,2020-06-02T00:51:14Z,2020-06-03T14:40:40Z,2020-06-03T14:40:40Z,MERGED,True,185,238,9,https://github.com/msune,"ci: use dedicated jobs per {docker, config_flags}",2,[],https://github.com/pmacct/pmacct/pull/399,https://github.com/msune,3,https://github.com/pmacct/pmacct/pull/399#issuecomment-637347436,"Short description
Prior to this commit there were only 2 jobs being launched, in which
all possible combinations of pmacct flags were compiled.
This had two fundamental problems:

Jobs could not run in parallel so could not exploit parallel jobs
of travis CI (even in free tier)
If a combination was failing, the entire job was failing

In addition all the log was in the same very long job, making it
difficult to debug.
Unfortunately, parallel jobs is not improving pipeline times, since
docker generation - which takes > 70% of time - cannot be reused
across jobs.
This should be addressed in a later commit, in which two different stages
are defined.
This commit builds on top of:

Moves dependencies to Dockerfile
Moves all related files from docker/ to ci/. This is to avoid users
thinking that docker file is a pmacct docker => btw this might be
a good idea; someone thought of adding a pmacct public layer?
Refactors CI to use a job per tuple {docker, config_flags}. It has
a single stage and it prefixes all jobs with name:

Ubuntu Bionic
Centos 8.1


Tries to simplify the CI code
Adds minor trick to pmacct ./configure to show config.log in case
of failure

Checklist
I have:

 added the LICENSE template to new files
 compiled & tested this code
 included documentation (including possible behaviour changes)",Added wget retrial backoff. Pipelines sometimes were failing retrieves packages using wget when preparing the container.,True,{}
pmacct/pmacct,https://github.com/pmacct/pmacct,400,2020-06-02T17:11:24Z,2020-06-03T23:11:50Z,2020-06-03T23:11:50Z,MERGED,True,219,238,9,https://github.com/msune,ci: improve build times with docker caching,3,[],https://github.com/pmacct/pmacct/pull/400,https://github.com/msune,1,https://github.com/pmacct/pmacct/pull/400,"Short description
!!This PR requires prior integration of #399!!
ci: reuse containers for all jobs

This commit adds support for sharing containers across all jobs.

It defines 2 stages:

* docker: base docker images are created
* build_test: pmacct build and testing process happens
  - Ubuntu Bionic: all confs
  - Centos 8.1: all confs

This reduces considerably pipeline times

Pipeline times go from ~1h to 37min (https://travis-ci.org/github/msune/pmacct/builds/693925326)
This commit makes use of the ""Beta"" feature: https://docs.travis-ci.com/user/using-workspaces/.

If there are changes on this beta feature, the CI file will have to be adapted.
If travis would eventually remove this feature (very unlikely), this commit should be reverted.

Checklist
I have:

 added the LICENSE template to new files
 compiled & tested this code
 included documentation (including possible behaviour changes)","Short description
!!This PR requires prior integration of #399!!
ci: reuse containers for all jobs

This commit adds support for sharing containers across all jobs.

It defines 2 stages:

* docker: base docker images are created
* build_test: pmacct build and testing process happens
  - Ubuntu Bionic: all confs
  - Centos 8.1: all confs

This reduces considerably pipeline times

Pipeline times go from ~1h to 37min (https://travis-ci.org/github/msune/pmacct/builds/693925326)
This commit makes use of the ""Beta"" feature: https://docs.travis-ci.com/user/using-workspaces/.

If there are changes on this beta feature, the CI file will have to be adapted.
If travis would eventually remove this feature (very unlikely), this commit should be reverted.

Checklist
I have:

 added the LICENSE template to new files
 compiled & tested this code
 included documentation (including possible behaviour changes)",True,{}
pmacct/pmacct,https://github.com/pmacct/pmacct,401,2020-06-03T23:55:02Z,2020-06-04T18:11:45Z,2020-06-04T18:11:45Z,MERGED,True,64,47,5,https://github.com/msune,ci: fix CONFIG_FLAGS passing to docker,1,[],https://github.com/pmacct/pmacct/pull/401,https://github.com/msune,1,https://github.com/pmacct/pmacct/pull/401,"Previous commit had a stupid mistake in which CONFIG_FLAGS was
not passed to the container (-e CONFIG_FLAGS), hence all jobs were
compiling the same pmacct variant.
This commit:

fixes CONFIG_FLAGS passing to docker
adds a work-around for libavro that is imported from the old
scripts - sigh.
Sorts packages in Dockerfiles
Disables tests for in CentOS that require libnetfilter_log, as
this package is not available (?)

This PR takes precedence over #400
Checklist
I have:

 added the LICENSE template to new files
 compiled & tested this code
 included documentation (including possible behaviour changes)","Previous commit had a stupid mistake in which CONFIG_FLAGS was
not passed to the container (-e CONFIG_FLAGS), hence all jobs were
compiling the same pmacct variant.
This commit:

fixes CONFIG_FLAGS passing to docker
adds a work-around for libavro that is imported from the old
scripts - sigh.
Sorts packages in Dockerfiles
Disables tests for in CentOS that require libnetfilter_log, as
this package is not available (?)

This PR takes precedence over #400
Checklist
I have:

 added the LICENSE template to new files
 compiled & tested this code
 included documentation (including possible behaviour changes)",True,{}
pmacct/pmacct,https://github.com/pmacct/pmacct,402,2020-06-04T22:56:11Z,2020-06-05T19:36:58Z,2020-06-05T19:36:58Z,MERGED,True,96,0,3,https://github.com/msune,Add `base` docker image and upload it automatically to docker hub,2,[],https://github.com/pmacct/pmacct/pull/402,https://github.com/msune,1,https://github.com/pmacct/pmacct/pull/402,"Short description
This patchset:

Adds a Dockerfile in folder docker/base to build a container with pmacct already instaled, along with few utilities utilities
Adds to .travis-ci.yml to upload, only when a release is tagged the container to {username}/base, with two tags:

latest: overriding whatever was latest before
vX.Y.Z: setting the version specific container, so that it can always be used


Adds section in the README to use the docker container

In order to have the Travis - dockerhub integration work, 3 simple steps:

Go to dockerhub website and create a public docker registry under pmacct username called base. It's the name chosen - subject to discussion
In your dockerhub user preferences, tab security, create a token to be used as ""password"" for the travis CI
Go to travis-ci, and add DOCKER_USERNAME (pmacct) and DOCKER_PASSWORD with the token previously generated. Should be ready to go (https://miro.medium.com/max/1400/0*Qneql3m2L0YM4DIt.png)

You can test it by creating a dummy tag vX.Y.Z and pushing it to the repo, once merged.
Next steps:

Have per daemon containers.

Checklist
I have:

 added the LICENSE template to new files
 compiled & tested this code
 included documentation (including possible behaviour changes)","Short description
This patchset:

Adds a Dockerfile in folder docker/base to build a container with pmacct already instaled, along with few utilities utilities
Adds to .travis-ci.yml to upload, only when a release is tagged the container to {username}/base, with two tags:

latest: overriding whatever was latest before
vX.Y.Z: setting the version specific container, so that it can always be used


Adds section in the README to use the docker container

In order to have the Travis - dockerhub integration work, 3 simple steps:

Go to dockerhub website and create a public docker registry under pmacct username called base. It's the name chosen - subject to discussion
In your dockerhub user preferences, tab security, create a token to be used as ""password"" for the travis CI
Go to travis-ci, and add DOCKER_USERNAME (pmacct) and DOCKER_PASSWORD with the token previously generated. Should be ready to go (https://miro.medium.com/max/1400/0*Qneql3m2L0YM4DIt.png)

You can test it by creating a dummy tag vX.Y.Z and pushing it to the repo, once merged.
Next steps:

Have per daemon containers.

Checklist
I have:

 added the LICENSE template to new files
 compiled & tested this code
 included documentation (including possible behaviour changes)",True,{}
pmacct/pmacct,https://github.com/pmacct/pmacct,402,2020-06-04T22:56:11Z,2020-06-05T19:36:58Z,2020-06-05T19:36:58Z,MERGED,True,96,0,3,https://github.com/msune,Add `base` docker image and upload it automatically to docker hub,2,[],https://github.com/pmacct/pmacct/pull/402,https://github.com/msune,2,https://github.com/pmacct/pmacct/pull/402#issuecomment-639160424,"Short description
This patchset:

Adds a Dockerfile in folder docker/base to build a container with pmacct already instaled, along with few utilities utilities
Adds to .travis-ci.yml to upload, only when a release is tagged the container to {username}/base, with two tags:

latest: overriding whatever was latest before
vX.Y.Z: setting the version specific container, so that it can always be used


Adds section in the README to use the docker container

In order to have the Travis - dockerhub integration work, 3 simple steps:

Go to dockerhub website and create a public docker registry under pmacct username called base. It's the name chosen - subject to discussion
In your dockerhub user preferences, tab security, create a token to be used as ""password"" for the travis CI
Go to travis-ci, and add DOCKER_USERNAME (pmacct) and DOCKER_PASSWORD with the token previously generated. Should be ready to go (https://miro.medium.com/max/1400/0*Qneql3m2L0YM4DIt.png)

You can test it by creating a dummy tag vX.Y.Z and pushing it to the repo, once merged.
Next steps:

Have per daemon containers.

Checklist
I have:

 added the LICENSE template to new files
 compiled & tested this code
 included documentation (including possible behaviour changes)","An example of a job that upload the container:
https://travis-ci.org/github/msune/pmacct/builds/694834083
And the container:
https://hub.docker.com/r/msune/base/tags",True,{}
pmacct/pmacct,https://github.com/pmacct/pmacct,404,2020-06-07T22:54:07Z,2020-06-11T18:57:53Z,2020-06-11T18:57:53Z,MERGED,True,124,80,4,https://github.com/msune,"Docker improvements: reduced container size, {bleeding edge, latest, vX.Y.Z}, pmacctd container",4,[],https://github.com/pmacct/pmacct/pull/404,https://github.com/msune,1,https://github.com/pmacct/pmacct/pull/404,"Short description
This patchset:

Reduces the base container size roughly 70%
Uploads base docker image with three different tags:

latest: always the latest stable release. It is updated on every new release
vX.Y.Z: always presenet, once published. Fixed tag for the release vX.Y.Z
bleeding-edge: only for the brave ones. Snapshot of the current master HEAD


Creates pmacctd container based on base. This container, in contrast to base has the daemon as an entry point and expects /etc/pmacct/pmacctd.conf to exist (being mounted as a volume). Same tags as base are pushed.

After the integration of this PR, bleeding-edge containers will start to be generated.
Checklist
I have:

 added the LICENSE template to new files
 compiled & tested this code
 included documentation (including possible behaviour changes)","Short description
This patchset:

Reduces the base container size roughly 70%
Uploads base docker image with three different tags:

latest: always the latest stable release. It is updated on every new release
vX.Y.Z: always presenet, once published. Fixed tag for the release vX.Y.Z
bleeding-edge: only for the brave ones. Snapshot of the current master HEAD


Creates pmacctd container based on base. This container, in contrast to base has the daemon as an entry point and expects /etc/pmacct/pmacctd.conf to exist (being mounted as a volume). Same tags as base are pushed.

After the integration of this PR, bleeding-edge containers will start to be generated.
Checklist
I have:

 added the LICENSE template to new files
 compiled & tested this code
 included documentation (including possible behaviour changes)",True,{}
pmacct/pmacct,https://github.com/pmacct/pmacct,404,2020-06-07T22:54:07Z,2020-06-11T18:57:53Z,2020-06-11T18:57:53Z,MERGED,True,124,80,4,https://github.com/msune,"Docker improvements: reduced container size, {bleeding edge, latest, vX.Y.Z}, pmacctd container",4,[],https://github.com/pmacct/pmacct/pull/404,https://github.com/msune,2,https://github.com/pmacct/pmacct/pull/404#issuecomment-640292336,"Short description
This patchset:

Reduces the base container size roughly 70%
Uploads base docker image with three different tags:

latest: always the latest stable release. It is updated on every new release
vX.Y.Z: always presenet, once published. Fixed tag for the release vX.Y.Z
bleeding-edge: only for the brave ones. Snapshot of the current master HEAD


Creates pmacctd container based on base. This container, in contrast to base has the daemon as an entry point and expects /etc/pmacct/pmacctd.conf to exist (being mounted as a volume). Same tags as base are pushed.

After the integration of this PR, bleeding-edge containers will start to be generated.
Checklist
I have:

 added the LICENSE template to new files
 compiled & tested this code
 included documentation (including possible behaviour changes)","For daemons that listen, either:

EXPOSE directive in Dockerfile shall be added (and list all possible ports - this is particularly important for base)
docker needs to run with host network.",True,{}
pmacct/pmacct,https://github.com/pmacct/pmacct,406,2020-06-11T22:53:27Z,2020-06-19T14:06:14Z,2020-06-19T14:07:26Z,MERGED,True,60,0,1,https://github.com/tbearma1,Systemddoku,2,[],https://github.com/pmacct/pmacct/pull/406,https://github.com/tbearma1,1,https://github.com/pmacct/pmacct/pull/406,"Short description

Checklist

I have:

 added the LICENSE template to new files
 compiled & tested this code
 included documentation (including possible behaviour changes)","Short description

Checklist

I have:

 added the LICENSE template to new files
 compiled & tested this code
 included documentation (including possible behaviour changes)",True,{}
pmacct/pmacct,https://github.com/pmacct/pmacct,406,2020-06-11T22:53:27Z,2020-06-19T14:06:14Z,2020-06-19T14:07:26Z,MERGED,True,60,0,1,https://github.com/tbearma1,Systemddoku,2,[],https://github.com/pmacct/pmacct/pull/406,https://github.com/tbearma1,2,https://github.com/pmacct/pmacct/pull/406#issuecomment-642974053,"Short description

Checklist

I have:

 added the LICENSE template to new files
 compiled & tested this code
 included documentation (including possible behaviour changes)","This is my pullrequest to merge my branch systemddoku to your project.
This will add a directory called systemd to the directory examples.
This will add a readme-file to describe how to use systemd service file with pmacct daemons.",True,{}
pmacct/pmacct,https://github.com/pmacct/pmacct,407,2020-06-13T13:25:02Z,2020-06-13T16:00:55Z,2020-06-13T16:00:55Z,MERGED,True,8,8,1,https://github.com/graf3net,Updated BGP time stamps in example metrics and BMP message type 6 to TBD,1,[],https://github.com/pmacct/pmacct/pull/407,https://github.com/graf3net,1,https://github.com/pmacct/pmacct/pull/407,"Hi Paolo,
Thanks for the feedback. Updated accordingly. Apologize for late reply.
Best Wishes
Thomas","Hi Paolo,
Thanks for the feedback. Updated accordingly. Apologize for late reply.
Best Wishes
Thomas",True,{}
pmacct/pmacct,https://github.com/pmacct/pmacct,408,2020-06-15T16:45:30Z,2020-06-15T17:24:03Z,2020-06-15T17:24:03Z,MERGED,True,1,0,1,https://github.com/msune,.travis.yml: do not run hub_docker on PR,1,[],https://github.com/pmacct/pmacct/pull/408,https://github.com/msune,1,https://github.com/pmacct/pmacct/pull/408,"Short description
For security reasons, stage hub_docker should never be attempted
to run on a PR. Fortunately, travis is already wise enough not
to load the credentials to not screw things badly, but it makes
the pipeline fail - always - for PRs.
Make sure this stage is only run on legit commits.
Checklist
I have:

 added the LICENSE template to new files
 compiled & tested this code
 included documentation (including possible behaviour changes)","Short description
For security reasons, stage hub_docker should never be attempted
to run on a PR. Fortunately, travis is already wise enough not
to load the credentials to not screw things badly, but it makes
the pipeline fail - always - for PRs.
Make sure this stage is only run on legit commits.
Checklist
I have:

 added the LICENSE template to new files
 compiled & tested this code
 included documentation (including possible behaviour changes)",True,{}
pmacct/pmacct,https://github.com/pmacct/pmacct,409,2020-06-20T17:39:57Z,2020-06-21T13:41:10Z,2020-06-21T13:41:10Z,MERGED,True,44,19,1,https://github.com/msune,Docker readme,2,[],https://github.com/pmacct/pmacct/pull/409,https://github.com/msune,1,https://github.com/pmacct/pmacct/pull/409,"Short description
This patchset:

Update docker section of the README.md with the new docker daemons, explanations of the tags etc..
Minor modification to the build section using a block code.

Checklist
I have:

 added the LICENSE template to new files
 compiled & tested this code
 included documentation (including possible behaviour changes)","Short description
This patchset:

Update docker section of the README.md with the new docker daemons, explanations of the tags etc..
Minor modification to the build section using a block code.

Checklist
I have:

 added the LICENSE template to new files
 compiled & tested this code
 included documentation (including possible behaviour changes)",True,{}
pmacct/pmacct,https://github.com/pmacct/pmacct,416,2020-07-09T18:17:11Z,2020-07-12T18:52:56Z,2020-07-13T17:27:00Z,MERGED,True,162,0,9,https://github.com/pothier-peter,Proxy protocol support,2,[],https://github.com/pmacct/pmacct/pull/416,https://github.com/pothier-peter,1,https://github.com/pmacct/pmacct/pull/416,"Contributions

The code and any documentation or other material submitted therewith, (collectively, the â€œContributionâ€) is provided by Akamai Technologies, Inc. (â€œAkamaiâ€) â€œAS ISâ€ WITHOUT REPRESENTATIONS OR WARRANTIES OF ANY KIND, EITHER EXPRESS OR IMPLIED, INCLUDING WITHOUT LIMITATION ANY REPRESENTATIONS OR WARRANTIES OF TITLE, NON-INFRINGEMENT, MERCHANTABILITY, OR FITNESS FOR A PARTICULAR PURPOSES.
The contributor is authorized to submit only this Contribution, dated [July 9, 2020], on behalf of Akamai, and the contributor is not authorized to submit additional contributions on behalf of Akamai without further authorization by Akamai.
Akamai shall not be expected to provide, and shall not provide, support for the Contribution.

Short description
Add support for BMP parsing of a Proxy Protocol header from the first packet of a new connection, replacing the peer's IP address and TCP port determined from the socket with the client's source IP address and TCP port found within the Proxy Protocol header. This functionality is enabled or disabled via a new configuration parameter. Code has been tested only for IPv4 (future TODO for IPv6).
Checklist
I have:

 added the LICENSE template to new files
[y ] compiled & tested this code
[y ] included documentation (including possible behaviour changes)","Contributions

The code and any documentation or other material submitted therewith, (collectively, the â€œContributionâ€) is provided by Akamai Technologies, Inc. (â€œAkamaiâ€) â€œAS ISâ€ WITHOUT REPRESENTATIONS OR WARRANTIES OF ANY KIND, EITHER EXPRESS OR IMPLIED, INCLUDING WITHOUT LIMITATION ANY REPRESENTATIONS OR WARRANTIES OF TITLE, NON-INFRINGEMENT, MERCHANTABILITY, OR FITNESS FOR A PARTICULAR PURPOSES.
The contributor is authorized to submit only this Contribution, dated [July 9, 2020], on behalf of Akamai, and the contributor is not authorized to submit additional contributions on behalf of Akamai without further authorization by Akamai.
Akamai shall not be expected to provide, and shall not provide, support for the Contribution.

Short description
Add support for BMP parsing of a Proxy Protocol header from the first packet of a new connection, replacing the peer's IP address and TCP port determined from the socket with the client's source IP address and TCP port found within the Proxy Protocol header. This functionality is enabled or disabled via a new configuration parameter. Code has been tested only for IPv4 (future TODO for IPv6).
Checklist
I have:

 added the LICENSE template to new files
[y ] compiled & tested this code
[y ] included documentation (including possible behaviour changes)",True,{}
pmacct/pmacct,https://github.com/pmacct/pmacct,417,2020-07-11T20:45:32Z,,2021-03-23T23:48:01Z,OPEN,False,109,0,5,https://github.com/vphatarp,Debug messages related to Netflow parsing,8,[],https://github.com/pmacct/pmacct/pull/417,https://github.com/vphatarp,1,https://github.com/pmacct/pmacct/pull/417,"Because of BGP policy changes I was seeing issues on the network which required me to filter out which Flow Destination IP addresses were not showing up any match on BGP tables learned by the BGP daemon. Earlier I used to debug this issue by dumping entire BGP tables on Nfacctd Collector and generating the flow matrix file with Ip Src/Dst listed and then doing a correlation between the 2.
I believe some debug commands related to Netflow parsing would ease debugging efforts needed and provide easier drill downs. It would also help to push the logs to ELK or similar Log Analyzers in order to debug the issue faster. I have added debug commands to the code which lists the following details:

IP Address of the device that sourced the Netflow packet.
BGP PeerIP which would be used to perform the lookups.(Helps in scenarios where RRs are used)
BgpNextHop for the flow as seen in the Netflow packet.
Source IP address of the Flow.
Destination IP address of the Flow.
Whether a lookup Match was found or not.

I have added a new cfg command for a conditional trigger of the debug commands. Only when debug and debug_netflow_parsing are both set to true, messages will be pushed to the log files. Currently debug messages have been added only to IPv4 and IPv6 flows.
Please let me know if any changes are needed from my end.
Regards,
vphatarp
Checklist

I have:

 added the LICENSE template to new files
 compiled & tested this code
 included documentation (including possible behaviour changes)","Because of BGP policy changes I was seeing issues on the network which required me to filter out which Flow Destination IP addresses were not showing up any match on BGP tables learned by the BGP daemon. Earlier I used to debug this issue by dumping entire BGP tables on Nfacctd Collector and generating the flow matrix file with Ip Src/Dst listed and then doing a correlation between the 2.
I believe some debug commands related to Netflow parsing would ease debugging efforts needed and provide easier drill downs. It would also help to push the logs to ELK or similar Log Analyzers in order to debug the issue faster. I have added debug commands to the code which lists the following details:

IP Address of the device that sourced the Netflow packet.
BGP PeerIP which would be used to perform the lookups.(Helps in scenarios where RRs are used)
BgpNextHop for the flow as seen in the Netflow packet.
Source IP address of the Flow.
Destination IP address of the Flow.
Whether a lookup Match was found or not.

I have added a new cfg command for a conditional trigger of the debug commands. Only when debug and debug_netflow_parsing are both set to true, messages will be pushed to the log files. Currently debug messages have been added only to IPv4 and IPv6 flows.
Please let me know if any changes are needed from my end.
Regards,
vphatarp
Checklist

I have:

 added the LICENSE template to new files
 compiled & tested this code
 included documentation (including possible behaviour changes)",True,{}
pmacct/pmacct,https://github.com/pmacct/pmacct,417,2020-07-11T20:45:32Z,,2021-03-23T23:48:01Z,OPEN,False,109,0,5,https://github.com/vphatarp,Debug messages related to Netflow parsing,8,[],https://github.com/pmacct/pmacct/pull/417,https://github.com/patrickjahns,2,https://github.com/pmacct/pmacct/pull/417#issuecomment-709158284,"Because of BGP policy changes I was seeing issues on the network which required me to filter out which Flow Destination IP addresses were not showing up any match on BGP tables learned by the BGP daemon. Earlier I used to debug this issue by dumping entire BGP tables on Nfacctd Collector and generating the flow matrix file with Ip Src/Dst listed and then doing a correlation between the 2.
I believe some debug commands related to Netflow parsing would ease debugging efforts needed and provide easier drill downs. It would also help to push the logs to ELK or similar Log Analyzers in order to debug the issue faster. I have added debug commands to the code which lists the following details:

IP Address of the device that sourced the Netflow packet.
BGP PeerIP which would be used to perform the lookups.(Helps in scenarios where RRs are used)
BgpNextHop for the flow as seen in the Netflow packet.
Source IP address of the Flow.
Destination IP address of the Flow.
Whether a lookup Match was found or not.

I have added a new cfg command for a conditional trigger of the debug commands. Only when debug and debug_netflow_parsing are both set to true, messages will be pushed to the log files. Currently debug messages have been added only to IPv4 and IPv6 flows.
Please let me know if any changes are needed from my end.
Regards,
vphatarp
Checklist

I have:

 added the LICENSE template to new files
 compiled & tested this code
 included documentation (including possible behaviour changes)","@paololucente
Any update on the process of merging the PR ?",True,{'THUMBS_UP': ['https://github.com/renanqts']}
pmacct/pmacct,https://github.com/pmacct/pmacct,418,2020-07-13T17:41:33Z,2020-07-13T18:55:17Z,2020-07-13T18:55:17Z,MERGED,True,11,0,1,https://github.com/pothier-peter,    Add simple HAProxy example configuration,1,[],https://github.com/pmacct/pmacct/pull/418,https://github.com/pothier-peter,1,https://github.com/pmacct/pmacct/pull/418,"The code and any documentation or other material submitted therewith, (collectively, the â€œContributionâ€) is provided by Akamai Technologies, Inc. (â€œAkamaiâ€) â€œAS ISâ€ WITHOUT REPRESENTATIONS OR WARRANTIES OF ANY KIND, EITHER EXPRESS OR IMPLIED, INCLUDING WITHOUT LIMITATION ANY REPRESENTATIONS OR WARRANTIES OF TITLE, NON-INFRINGEMENT, MERCHANTABILITY, OR FITNESS FOR A PARTICULAR PURPOSES.
The contributor is authorized to submit only this Contribution, dated [July 13, 2020], on behalf of Akamai, and the contributor is not authorized to submit additional contributions on behalf of Akamai without further authorization by Akamai.
Akamai shall not be expected to provide, and shall not provide, support for the Contribution.

Short description
Add simple HAProxy configuration (follow up to issue #412)
Checklist
I have:

 added the LICENSE template to new files
 compiled & tested this code
[ y] included documentation (including possible behaviour changes)","The code and any documentation or other material submitted therewith, (collectively, the â€œContributionâ€) is provided by Akamai Technologies, Inc. (â€œAkamaiâ€) â€œAS ISâ€ WITHOUT REPRESENTATIONS OR WARRANTIES OF ANY KIND, EITHER EXPRESS OR IMPLIED, INCLUDING WITHOUT LIMITATION ANY REPRESENTATIONS OR WARRANTIES OF TITLE, NON-INFRINGEMENT, MERCHANTABILITY, OR FITNESS FOR A PARTICULAR PURPOSES.
The contributor is authorized to submit only this Contribution, dated [July 13, 2020], on behalf of Akamai, and the contributor is not authorized to submit additional contributions on behalf of Akamai without further authorization by Akamai.
Akamai shall not be expected to provide, and shall not provide, support for the Contribution.

Short description
Add simple HAProxy configuration (follow up to issue #412)
Checklist
I have:

 added the LICENSE template to new files
 compiled & tested this code
[ y] included documentation (including possible behaviour changes)",True,{}
pmacct/pmacct,https://github.com/pmacct/pmacct,420,2020-07-18T22:37:44Z,2020-07-25T21:11:54Z,2020-07-25T21:11:54Z,MERGED,True,26,3,5,https://github.com/msune,Autogenerate pmacct-build.h,2,[],https://github.com/pmacct/pmacct/pull/420,https://github.com/msune,1,https://github.com/pmacct/pmacct/pull/420,"Maintaining, by hand, pmacct-build.h is a waste of Paolo's
precious time. This commit autogenerates - in the same format - the pmacct-build.h build string.
It's highly unlikely this change will make it in. It has two inherent
problems, pretty much unavoidable (if someone has ideas, please share):

If .git/ is not present, the string cannot be deduced - obviously
If ./configure is not repeated when new commits are pulled,
the build of the last ./configure would appear. Even more, if
old objects are present, they will not be recompiled.

Anyhow, at least Paolo can extract the shell commands to autogen
the file :D
Checklist
I have:

 added the LICENSE template to new files
 compiled & tested this code
 included documentation (including possible behaviour changes)","Maintaining, by hand, pmacct-build.h is a waste of Paolo's
precious time. This commit autogenerates - in the same format - the pmacct-build.h build string.
It's highly unlikely this change will make it in. It has two inherent
problems, pretty much unavoidable (if someone has ideas, please share):

If .git/ is not present, the string cannot be deduced - obviously
If ./configure is not repeated when new commits are pulled,
the build of the last ./configure would appear. Even more, if
old objects are present, they will not be recompiled.

Anyhow, at least Paolo can extract the shell commands to autogen
the file :D
Checklist
I have:

 added the LICENSE template to new files
 compiled & tested this code
 included documentation (including possible behaviour changes)",True,{}
pmacct/pmacct,https://github.com/pmacct/pmacct,420,2020-07-18T22:37:44Z,2020-07-25T21:11:54Z,2020-07-25T21:11:54Z,MERGED,True,26,3,5,https://github.com/msune,Autogenerate pmacct-build.h,2,[],https://github.com/pmacct/pmacct/pull/420,https://github.com/msune,2,https://github.com/pmacct/pmacct/pull/420#issuecomment-660720710,"Maintaining, by hand, pmacct-build.h is a waste of Paolo's
precious time. This commit autogenerates - in the same format - the pmacct-build.h build string.
It's highly unlikely this change will make it in. It has two inherent
problems, pretty much unavoidable (if someone has ideas, please share):

If .git/ is not present, the string cannot be deduced - obviously
If ./configure is not repeated when new commits are pulled,
the build of the last ./configure would appear. Even more, if
old objects are present, they will not be recompiled.

Anyhow, at least Paolo can extract the shell commands to autogen
the file :D
Checklist
I have:

 added the LICENSE template to new files
 compiled & tested this code
 included documentation (including possible behaviour changes)","wrt to your comments, first I have to say the PR is half a joke and half serious. I just wanted to give you the commands.
Second, I realised the commit message is indeed misleading. The initial intention was to autogen pmacct-build.h from an pmacct-build.h.in, but then I went the easy path of AC_DEFINE as propsective.
To your questions:

Guess a commit involves a certain file and someone does a 'git pull' after that; the configure script is not guaranteed to be called. Actually there is a risk that the build remains stuck to the first time you did a 'git clone'. Agree? Disagree? If agree, would you see a way around it? I think this is the key point.

Yes, sort of (autoreconf would exec if some Makefile.am would change) but the problem for compiled objs would stand still:


If ./configure is not repeated when new commits are pulled, the build of the last ./configure would appear. Even more, if old objects are present, they will not be recompiled.


On the serious side of things...
Why do you need a PMACCT_BUILD at all? I understand the need for this variable before git. After git, the commit hash is unequivocally identifying the precise build/commit. Most projects use that as a build number, and that has the advantage that git log <hash> gives immediately the log of the commit, without a translation in between.
For releases, this is actually not a problem - IMHO - as the git tag has a 1:1 relation to the commit, hence again, you can git log <tag>. So that would solve the problem of the tarballs for releases, as if .git/ is not find, the BUILD string could be RELEASE or s/t along those lines.
Regardless of what actually identifies the build, current format or git hash, the problem of already cloned (and built) repos, would be there.
I am pretty sure there must be a way to cache locally the commit ,and in a Makfefile.am, if a new commit now exists, force an autoreconf or alike... For old compiled objects, then pmacct-build.h.in shall be used to solve it.
If you want me to truly look into it, just let me know. You can also close the PR if you don't see the need to bother... Really, no strings attached :D",True,{}
pmacct/pmacct,https://github.com/pmacct/pmacct,420,2020-07-18T22:37:44Z,2020-07-25T21:11:54Z,2020-07-25T21:11:54Z,MERGED,True,26,3,5,https://github.com/msune,Autogenerate pmacct-build.h,2,[],https://github.com/pmacct/pmacct/pull/420,https://github.com/paololucente,3,https://github.com/pmacct/pmacct/pull/420#issuecomment-661018545,"Maintaining, by hand, pmacct-build.h is a waste of Paolo's
precious time. This commit autogenerates - in the same format - the pmacct-build.h build string.
It's highly unlikely this change will make it in. It has two inherent
problems, pretty much unavoidable (if someone has ideas, please share):

If .git/ is not present, the string cannot be deduced - obviously
If ./configure is not repeated when new commits are pulled,
the build of the last ./configure would appear. Even more, if
old objects are present, they will not be recompiled.

Anyhow, at least Paolo can extract the shell commands to autogen
the file :D
Checklist
I have:

 added the LICENSE template to new files
 compiled & tested this code
 included documentation (including possible behaviour changes)","Hey Marc,

wrt to your comments, first I have to say the PR is half a joke and half serious. I just wanted to give you the commands.

Yes, yes i got that â¤ï¸

Why do you need a PMACCT_BUILD at all? I understand the need for this variable before git. After git, the commit hash is unequivocally identifying the precise build/commit. Most projects use that as a build number, and that has the advantage that git log <hash> gives immediately the log of the commit, without a translation in between.
For releases, this is actually not a problem - IMHO - as the git tag has a 1:1 relation to the commit, hence again, you can git log <tag>. So that would solve the problem of the tarballs for releases, as if .git/ is not find, the BUILD string could be RELEASE or s/t along those lines.

The idea was to make it super simple for people, ie. when opening an issue or reporting a bug a 'nfacctd -V' would capture all needed info to start a meaningful troubleshooting / debugging (version, commit, compiling options, compiler info and system info). Reason being: not everybody cloning from GitHub can do much beyond 'git clone'. This is why referencing the hash (or the beginning of the hash better) instead of using the current jargon would work too.

If you want me to truly look into it, just let me know. You can also close the PR if you don't see the need to bother... Really, no strings attached :D

I really was looking at this in the past myself but stumbled pretty much into what you did stumble. I would say this is a nice to have thing (as opposed to a necessary one) and i always said to myself there is better use of time. But you have voodoo powers, i know; so i would say: if it looks like something that can be solved quickly, great. Otherwise if research and thinking are needed then perhaps, yeah, we can defer to the next pandemic boredom.
Paolo",True,{}
pmacct/pmacct,https://github.com/pmacct/pmacct,420,2020-07-18T22:37:44Z,2020-07-25T21:11:54Z,2020-07-25T21:11:54Z,MERGED,True,26,3,5,https://github.com/msune,Autogenerate pmacct-build.h,2,[],https://github.com/pmacct/pmacct/pull/420,https://github.com/msune,4,https://github.com/pmacct/pmacct/pull/420#issuecomment-663431487,"Maintaining, by hand, pmacct-build.h is a waste of Paolo's
precious time. This commit autogenerates - in the same format - the pmacct-build.h build string.
It's highly unlikely this change will make it in. It has two inherent
problems, pretty much unavoidable (if someone has ideas, please share):

If .git/ is not present, the string cannot be deduced - obviously
If ./configure is not repeated when new commits are pulled,
the build of the last ./configure would appear. Even more, if
old objects are present, they will not be recompiled.

Anyhow, at least Paolo can extract the shell commands to autogen
the file :D
Checklist
I have:

 added the LICENSE template to new files
 compiled & tested this code
 included documentation (including possible behaviour changes)","Solved,
Take a look at the commit message itself:
b99d242
They key is:
b99d242#diff-480477e89f9b6ddafb30c4383dcdd705R20
Regards",True,{}
pmacct/pmacct,https://github.com/pmacct/pmacct,420,2020-07-18T22:37:44Z,2020-07-25T21:11:54Z,2020-07-25T21:11:54Z,MERGED,True,26,3,5,https://github.com/msune,Autogenerate pmacct-build.h,2,[],https://github.com/pmacct/pmacct/pull/420,https://github.com/msune,5,https://github.com/pmacct/pmacct/pull/420#issuecomment-663878266,"Maintaining, by hand, pmacct-build.h is a waste of Paolo's
precious time. This commit autogenerates - in the same format - the pmacct-build.h build string.
It's highly unlikely this change will make it in. It has two inherent
problems, pretty much unavoidable (if someone has ideas, please share):

If .git/ is not present, the string cannot be deduced - obviously
If ./configure is not repeated when new commits are pulled,
the build of the last ./configure would appear. Even more, if
old objects are present, they will not be recompiled.

Anyhow, at least Paolo can extract the shell commands to autogen
the file :D
Checklist
I have:

 added the LICENSE template to new files
 compiled & tested this code
 included documentation (including possible behaviour changes)","Added the git hash (short form):
marc@Mriya:~/personal/pmacct$ src/nfacctd -v
src/nfacctd: invalid option -- 'v'
NetFlow Accounting Daemon, nfacctd 1.7.6-git (20200725-1 [a10602f0])",True,{}
pmacct/pmacct,https://github.com/pmacct/pmacct,429,2020-09-30T10:15:46Z,2020-10-01T13:54:50Z,2020-10-01T13:54:50Z,MERGED,True,10,3,3,https://github.com/rbarazzutti,BMP leak fix,4,[],https://github.com/pmacct/pmacct/pull/429,https://github.com/rbarazzutti,1,https://github.com/pmacct/pmacct/pull/429,"Short description

After some investigations, I was able to spot some leaks that were causing pmbmpd memory usage to consume a bit too much memory.
Checklist

I have:

 added the LICENSE template to new files
 compiled & tested this code
 included documentation (including possible behaviour changes)","Short description

After some investigations, I was able to spot some leaks that were causing pmbmpd memory usage to consume a bit too much memory.
Checklist

I have:

 added the LICENSE template to new files
 compiled & tested this code
 included documentation (including possible behaviour changes)",True,{}
pmacct/pmacct,https://github.com/pmacct/pmacct,430,2020-10-02T17:04:05Z,2020-10-05T13:25:23Z,2020-10-05T13:25:23Z,MERGED,True,1,1,1,https://github.com/elindsey,fix a small typo in the quickstart,1,[],https://github.com/pmacct/pmacct/pull/430,https://github.com/elindsey,1,https://github.com/pmacct/pmacct/pull/430,,,True,{}
pmacct/pmacct,https://github.com/pmacct/pmacct,434,2020-10-07T16:07:49Z,2020-10-09T16:31:23Z,2020-10-10T15:59:26Z,MERGED,True,28,1,4,https://github.com/edge-intelligence,adding crc32 as another method of hashing for tee plugin,1,[],https://github.com/pmacct/pmacct/pull/434,https://github.com/edge-intelligence,1,https://github.com/pmacct/pmacct/pull/434,"I have added crc32 hash algorithm as an option for tee_plugin.
It's aim is to get a better balancing to receiver nodes.
example of usage receivers.lst:
id=1    ip=127.0.0.1:2001,127.0.0.1:2002,127.0.0.1:2003 balance-alg=hash-crc32","I have added crc32 hash algorithm as an option for tee_plugin.
It's aim is to get a better balancing to receiver nodes.
example of usage receivers.lst:
id=1    ip=127.0.0.1:2001,127.0.0.1:2002,127.0.0.1:2003 balance-alg=hash-crc32",True,{}
pmacct/pmacct,https://github.com/pmacct/pmacct,434,2020-10-07T16:07:49Z,2020-10-09T16:31:23Z,2020-10-10T15:59:26Z,MERGED,True,28,1,4,https://github.com/edge-intelligence,adding crc32 as another method of hashing for tee plugin,1,[],https://github.com/pmacct/pmacct/pull/434,https://github.com/paololucente,2,https://github.com/pmacct/pmacct/pull/434#issuecomment-706570538,"I have added crc32 hash algorithm as an option for tee_plugin.
It's aim is to get a better balancing to receiver nodes.
example of usage receivers.lst:
id=1    ip=127.0.0.1:2001,127.0.0.1:2002,127.0.0.1:2003 balance-alg=hash-crc32","Hi @edge-intelligence ,
I did work out the IPv6 case and for the task i adapted an existing addr_hash() function to work against sockaddr structures, sa_hash(). See here the commit: 78970c5
In theory this can be used seamlessly for IPv4 too but i did not want to touch that part without some testing first: if you think this is useful and have a way to test its robustness, please let me know your thoughts and maybe open a new PR with the updated code? Should find you positive probably at that point the feature should be renamed to something else than crc32, dunno, 'hash_exporter_ip' or something like that.
Paolo",True,{}
pmacct/pmacct,https://github.com/pmacct/pmacct,437,2020-10-28T15:22:33Z,2020-10-31T17:27:23Z,2020-10-31T17:27:23Z,MERGED,True,4,1,1,https://github.com/pothier-peter,    ISSUE # 436 Valgrind detected a memory leak in bgp_aspath.c,1,[],https://github.com/pmacct/pmacct/pull/437,https://github.com/pothier-peter,1,https://github.com/pmacct/pmacct/pull/437,"The code and any documentation or other material submitted therewith, (collectively, the â€œContributionâ€) is provided by Akamai Technologies, Inc. (â€œAkamaiâ€) â€œAS ISâ€ WITHOUT REPRESENTATIONS OR WARRANTIES OF ANY KIND, EITHER EXPRESS OR IMPLIED, INCLUDING WITHOUT LIMITATION ANY REPRESENTATIONS OR WARRANTIES OF TITLE, NON-INFRINGEMENT, MERCHANTABILITY, OR FITNESS FOR A PARTICULAR PURPOSES.


The contributor is authorized to submit only this Contribution, dated [October 28, 2020], on behalf of Akamai, and the contributor is not authorized to submit additional contributions on behalf of Akamai without further authorization by Akamai.


Akamai shall not be expected to provide, and shall not provide, support for the Contribution.
add return after generating str_path via aspath_make_empty


Short description
The memory allocated by aspath_make_empty() into str_buf was leaked as str_buf was subsequently reassigned.
The code has been changed to return str_buf instead.
Checklist
I have:

 added the LICENSE template to new files
[ y] compiled & tested this code
 included documentation (including possible behaviour changes)","The code and any documentation or other material submitted therewith, (collectively, the â€œContributionâ€) is provided by Akamai Technologies, Inc. (â€œAkamaiâ€) â€œAS ISâ€ WITHOUT REPRESENTATIONS OR WARRANTIES OF ANY KIND, EITHER EXPRESS OR IMPLIED, INCLUDING WITHOUT LIMITATION ANY REPRESENTATIONS OR WARRANTIES OF TITLE, NON-INFRINGEMENT, MERCHANTABILITY, OR FITNESS FOR A PARTICULAR PURPOSES.


The contributor is authorized to submit only this Contribution, dated [October 28, 2020], on behalf of Akamai, and the contributor is not authorized to submit additional contributions on behalf of Akamai without further authorization by Akamai.


Akamai shall not be expected to provide, and shall not provide, support for the Contribution.
add return after generating str_path via aspath_make_empty


Short description
The memory allocated by aspath_make_empty() into str_buf was leaked as str_buf was subsequently reassigned.
The code has been changed to return str_buf instead.
Checklist
I have:

 added the LICENSE template to new files
[ y] compiled & tested this code
 included documentation (including possible behaviour changes)",True,{}
pmacct/pmacct,https://github.com/pmacct/pmacct,440,2020-11-04T19:12:41Z,2020-11-07T16:43:29Z,2020-11-07T16:43:29Z,MERGED,True,107,2,6,https://github.com/msune,Add git submodule dependency builds. Add -lcdada as dependency ,3,[],https://github.com/pmacct/pmacct/pull/440,https://github.com/msune,1,https://github.com/pmacct/pmacct/pull/440,"Short description
Dependencies are always a bit of PITA. This patchset extends the build system to support building external libraries and statically link them to pmacct for the ease of users.
The fundamental design principle has been to keep the ""user interface"" exactly the same:
git clone
cd pmacct
./autogen.sh
./configure
sudo make install

As per discussed with @paololucente, this commit:

Introduces in src/external_builds directory. Under this directory:

git submodules can be added for each dependency
Makefile.am contains a single target submodule_deps that list the submodule recipes (make targets) to build. Make sure to add also cleanup functions.
Dependencies are only compiled when needed, including when the submodule commit changes (even locally without committing)


Adds libcdada as a dependency of pmacct (all) (currently pointing to v0.3.2) and a show-case of the submodule concept. This also adds the dependency of libstdc++ (dynamic)

Use-cases

Regular users using git: build system works exactly the same as before.
Tarballs: build system behaves the same as before. Users need to compile libcdada and install it in the system
OS package maintainers: can use 1) or opt to use --without-external-deps to skip completely building the submodules.

For 2, 3, -lcdada is checked as a dependency in configure.ac. Intree and OutTree compilation is tested (CI) and checked. make clean and make maintainer clean manually checked in both cases.
Other comments

I would still move src/external_libs/ -> external_libs/. To me it looks cleaner, and will still work.
I would suggest to explore the same approach for some of the hard to compile dependencies of pmacct, those not available in the OS pkg managers. That might save some time to users.
The Makefile.am should be self-explanatory on how to add more deps (just add more targets), but let me know if you want extra documentation.

Checklist
I have:

 added the LICENSE template to new files
 compiled & tested this code
 included documentation (including possible behaviour changes)","Short description
Dependencies are always a bit of PITA. This patchset extends the build system to support building external libraries and statically link them to pmacct for the ease of users.
The fundamental design principle has been to keep the ""user interface"" exactly the same:
git clone
cd pmacct
./autogen.sh
./configure
sudo make install

As per discussed with @paololucente, this commit:

Introduces in src/external_builds directory. Under this directory:

git submodules can be added for each dependency
Makefile.am contains a single target submodule_deps that list the submodule recipes (make targets) to build. Make sure to add also cleanup functions.
Dependencies are only compiled when needed, including when the submodule commit changes (even locally without committing)


Adds libcdada as a dependency of pmacct (all) (currently pointing to v0.3.2) and a show-case of the submodule concept. This also adds the dependency of libstdc++ (dynamic)

Use-cases

Regular users using git: build system works exactly the same as before.
Tarballs: build system behaves the same as before. Users need to compile libcdada and install it in the system
OS package maintainers: can use 1) or opt to use --without-external-deps to skip completely building the submodules.

For 2, 3, -lcdada is checked as a dependency in configure.ac. Intree and OutTree compilation is tested (CI) and checked. make clean and make maintainer clean manually checked in both cases.
Other comments

I would still move src/external_libs/ -> external_libs/. To me it looks cleaner, and will still work.
I would suggest to explore the same approach for some of the hard to compile dependencies of pmacct, those not available in the OS pkg managers. That might save some time to users.
The Makefile.am should be self-explanatory on how to add more deps (just add more targets), but let me know if you want extra documentation.

Checklist
I have:

 added the LICENSE template to new files
 compiled & tested this code
 included documentation (including possible behaviour changes)",True,{}
pmacct/pmacct,https://github.com/pmacct/pmacct,440,2020-11-04T19:12:41Z,2020-11-07T16:43:29Z,2020-11-07T16:43:29Z,MERGED,True,107,2,6,https://github.com/msune,Add git submodule dependency builds. Add -lcdada as dependency ,3,[],https://github.com/pmacct/pmacct/pull/440,https://github.com/msune,2,https://github.com/pmacct/pmacct/pull/440#issuecomment-721921796,"Short description
Dependencies are always a bit of PITA. This patchset extends the build system to support building external libraries and statically link them to pmacct for the ease of users.
The fundamental design principle has been to keep the ""user interface"" exactly the same:
git clone
cd pmacct
./autogen.sh
./configure
sudo make install

As per discussed with @paololucente, this commit:

Introduces in src/external_builds directory. Under this directory:

git submodules can be added for each dependency
Makefile.am contains a single target submodule_deps that list the submodule recipes (make targets) to build. Make sure to add also cleanup functions.
Dependencies are only compiled when needed, including when the submodule commit changes (even locally without committing)


Adds libcdada as a dependency of pmacct (all) (currently pointing to v0.3.2) and a show-case of the submodule concept. This also adds the dependency of libstdc++ (dynamic)

Use-cases

Regular users using git: build system works exactly the same as before.
Tarballs: build system behaves the same as before. Users need to compile libcdada and install it in the system
OS package maintainers: can use 1) or opt to use --without-external-deps to skip completely building the submodules.

For 2, 3, -lcdada is checked as a dependency in configure.ac. Intree and OutTree compilation is tested (CI) and checked. make clean and make maintainer clean manually checked in both cases.
Other comments

I would still move src/external_libs/ -> external_libs/. To me it looks cleaner, and will still work.
I would suggest to explore the same approach for some of the hard to compile dependencies of pmacct, those not available in the OS pkg managers. That might save some time to users.
The Makefile.am should be self-explanatory on how to add more deps (just add more targets), but let me know if you want extra documentation.

Checklist
I have:

 added the LICENSE template to new files
 compiled & tested this code
 included documentation (including possible behaviour changes)","Btw, I would suggest to create a mirror of libcdada and point .gitmodules there, to have it under your control",True,{}
pmacct/pmacct,https://github.com/pmacct/pmacct,441,2020-11-08T13:58:15Z,2020-11-08T14:03:07Z,2020-11-08T14:03:07Z,MERGED,True,5,1,2,https://github.com/msune,submodules: fix cross-compilation and pass enable/disable-silent-rules,2,[],https://github.com/pmacct/pmacct/pull/441,https://github.com/msune,1,https://github.com/pmacct/pmacct/pull/441,"Short description
This commit:

Fixes cross-compilation of git submodule libraries by correctly setting --host= using $host_alias result of the parent pmacct's configure.ac
Passes by the value of --enable/--disable-silent-rules to the git submodule build system, by defining a new AC_SUBST (@configure_silent_rules_val@). Default remains --enable-silent-rules, as pmacct's package.

Checklist

I have:

 added the LICENSE template to new files
 compiled & tested this code
 included documentation (including possible behaviour changes)","Short description
This commit:

Fixes cross-compilation of git submodule libraries by correctly setting --host= using $host_alias result of the parent pmacct's configure.ac
Passes by the value of --enable/--disable-silent-rules to the git submodule build system, by defining a new AC_SUBST (@configure_silent_rules_val@). Default remains --enable-silent-rules, as pmacct's package.

Checklist

I have:

 added the LICENSE template to new files
 compiled & tested this code
 included documentation (including possible behaviour changes)",True,{}
pmacct/pmacct,https://github.com/pmacct/pmacct,442,2020-11-09T23:24:00Z,2020-11-10T11:53:44Z,2020-11-10T11:53:44Z,MERGED,True,6,1,1,https://github.com/msune,external_libs: fix submodule init for vintage gits,1,[],https://github.com/pmacct/pmacct/pull/442,https://github.com/msune,1,https://github.com/pmacct/pmacct/pull/442,"Short description
Some pmacct are confessed vintage SW aficionados. In particular,
Centos7 ships with git v1.8.3.1. Old versions of git can't initialize
a submodule if not done from the root of the supermodule.
Therefore, workaround it by temporally moving to abs_top_srcdir, and
then go back to the builddir.

Checklist

I have:

 added the LICENSE template to new files
 compiled & tested this code
 included documentation (including possible behaviour changes)","Short description
Some pmacct are confessed vintage SW aficionados. In particular,
Centos7 ships with git v1.8.3.1. Old versions of git can't initialize
a submodule if not done from the root of the supermodule.
Therefore, workaround it by temporally moving to abs_top_srcdir, and
then go back to the builddir.

Checklist

I have:

 added the LICENSE template to new files
 compiled & tested this code
 included documentation (including possible behaviour changes)",True,{}
pmacct/pmacct,https://github.com/pmacct/pmacct,443,2020-11-10T13:54:26Z,2020-11-11T20:59:33Z,2020-11-11T20:59:33Z,MERGED,True,5,0,1,https://github.com/msune,configure.ac: check for C++ compiler,1,[],https://github.com/pmacct/pmacct/pull/443,https://github.com/msune,1,https://github.com/pmacct/pmacct/pull/443,"Short description
Prior to this commit, if libcdada (C++ dependent) was compiled, and
no C++ compiler was found, the build was failing at the compilation
stage.
This patch makes sure this check is happening during configure time,
and a clear warning is printed to the user

Checklist

I have:

 added the LICENSE template to new files
 compiled & tested this code
 included documentation (including possible behaviour changes)","Short description
Prior to this commit, if libcdada (C++ dependent) was compiled, and
no C++ compiler was found, the build was failing at the compilation
stage.
This patch makes sure this check is happening during configure time,
and a clear warning is printed to the user

Checklist

I have:

 added the LICENSE template to new files
 compiled & tested this code
 included documentation (including possible behaviour changes)",True,{}
pmacct/pmacct,https://github.com/pmacct/pmacct,454,2020-12-17T11:45:44Z,2020-12-18T19:33:42Z,2020-12-18T19:33:43Z,MERGED,True,87,43,2,https://github.com/graf3net,BMP and flow_to_rd,2,[],https://github.com/pmacct/pmacct/pull/454,https://github.com/graf3net,1,https://github.com/pmacct/pmacct/pull/454,BMP/BGP metrics documentation and flow_to_rd.map examples updated.,BMP/BGP metrics documentation and flow_to_rd.map examples updated.,True,{}
pmacct/pmacct,https://github.com/pmacct/pmacct,462,2021-01-15T00:52:59Z,2021-01-16T19:02:08Z,2021-01-16T19:02:34Z,MERGED,True,27,12,4,https://github.com/msune,"Fix build for *BSD systems, gcc-4.2 and submodule updates",2,[],https://github.com/pmacct/pmacct/pull/462,https://github.com/msune,1,https://github.com/pmacct/pmacct/pull/462,"Short description
This MR addresses some portability issues found while compiling in openBSD, with a rather old gcc-4.2. It fixes:

Substitutes $(shell <cmd>) by `` in all Makefiles, fixing pmacct-build.h generation and submodule compilation.
Bumps libcdada dependency to v0.3.4, which fixes compilation warnings in ancient gcc-4.2
Fixes compilation in *BSD due to missing pthread.h in bgp.h
Fixes bug in which, when getting new pmacct commits (git pull) on an already cloned repository, and when those commits where updating submodules, the deduction on whether the submodule deps had to be compiled or not was incorrect.

This MR has been tested in:

Linux (debian9)
OpenBSD 6.8

Checklist
I have:

 added the LICENSE template to new files
 compiled & tested this code
 included documentation (including possible behaviour changes)","Short description
This MR addresses some portability issues found while compiling in openBSD, with a rather old gcc-4.2. It fixes:

Substitutes $(shell <cmd>) by `` in all Makefiles, fixing pmacct-build.h generation and submodule compilation.
Bumps libcdada dependency to v0.3.4, which fixes compilation warnings in ancient gcc-4.2
Fixes compilation in *BSD due to missing pthread.h in bgp.h
Fixes bug in which, when getting new pmacct commits (git pull) on an already cloned repository, and when those commits where updating submodules, the deduction on whether the submodule deps had to be compiled or not was incorrect.

This MR has been tested in:

Linux (debian9)
OpenBSD 6.8

Checklist
I have:

 added the LICENSE template to new files
 compiled & tested this code
 included documentation (including possible behaviour changes)",True,{}
pmacct/pmacct,https://github.com/pmacct/pmacct,462,2021-01-15T00:52:59Z,2021-01-16T19:02:08Z,2021-01-16T19:02:34Z,MERGED,True,27,12,4,https://github.com/msune,"Fix build for *BSD systems, gcc-4.2 and submodule updates",2,[],https://github.com/pmacct/pmacct/pull/462,https://github.com/msune,2,https://github.com/pmacct/pmacct/pull/462#issuecomment-760572853,"Short description
This MR addresses some portability issues found while compiling in openBSD, with a rather old gcc-4.2. It fixes:

Substitutes $(shell <cmd>) by `` in all Makefiles, fixing pmacct-build.h generation and submodule compilation.
Bumps libcdada dependency to v0.3.4, which fixes compilation warnings in ancient gcc-4.2
Fixes compilation in *BSD due to missing pthread.h in bgp.h
Fixes bug in which, when getting new pmacct commits (git pull) on an already cloned repository, and when those commits where updating submodules, the deduction on whether the submodule deps had to be compiled or not was incorrect.

This MR has been tested in:

Linux (debian9)
OpenBSD 6.8

Checklist
I have:

 added the LICENSE template to new files
 compiled & tested this code
 included documentation (including possible behaviour changes)","MInd (developers only), that:

To prevent make to sync the commit - only for developers define:
export PMACCT_EXT_LIBS_DONT_SYNC=1",True,{}
pmacct/pmacct,https://github.com/pmacct/pmacct,464,2021-01-21T14:40:06Z,2021-01-24T20:06:41Z,2021-01-24T20:06:41Z,MERGED,True,1,0,1,https://github.com/pothier-peter,    ISSUE 463,1,[],https://github.com/pmacct/pmacct/pull/464,https://github.com/pothier-peter,1,https://github.com/pmacct/pmacct/pull/464,"Fix Memory Leak bgp_peer_free

Short description
This Pull Request fixes issue #463 by invoking free() on the key allocated by pm_tsearch() and freed by pm_tdestroy()

The code and any documentation or other material submitted therewith, (collectively, the â€œContributionâ€) is provided by Akamai Technologies, Inc. (â€œAkamaiâ€) â€œAS ISâ€ WITHOUT REPRESENTATIONS OR WARRANTIES OF ANY KIND, EITHER EXPRESS OR IMPLIED, INCLUDING WITHOUT LIMITATION ANY REPRESENTATIONS OR WARRANTIES OF TITLE, NON-INFRINGEMENT, MERCHANTABILITY, OR FITNESS FOR A PARTICULAR PURPOSES.Â 2. The contributor is authorized to submit onlyÂ thisÂ Contribution, dated [January 21, 2021], on behalf of Akamai, and the contributor is not authorized to submit additional contributions on behalf of Akamai without further authorization by Akamai.Â 3. Akamai shall not be expected to provide, and shall not provide, supportÂ forÂ the Contribution.
--

Checklist
I have:

 added the LICENSE template to new files
[ y] compiled & tested this code
 included documentation (including possible behaviour changes)","Fix Memory Leak bgp_peer_free

Short description
This Pull Request fixes issue #463 by invoking free() on the key allocated by pm_tsearch() and freed by pm_tdestroy()

The code and any documentation or other material submitted therewith, (collectively, the â€œContributionâ€) is provided by Akamai Technologies, Inc. (â€œAkamaiâ€) â€œAS ISâ€ WITHOUT REPRESENTATIONS OR WARRANTIES OF ANY KIND, EITHER EXPRESS OR IMPLIED, INCLUDING WITHOUT LIMITATION ANY REPRESENTATIONS OR WARRANTIES OF TITLE, NON-INFRINGEMENT, MERCHANTABILITY, OR FITNESS FOR A PARTICULAR PURPOSES.Â 2. The contributor is authorized to submit onlyÂ thisÂ Contribution, dated [January 21, 2021], on behalf of Akamai, and the contributor is not authorized to submit additional contributions on behalf of Akamai without further authorization by Akamai.Â 3. Akamai shall not be expected to provide, and shall not provide, supportÂ forÂ the Contribution.
--

Checklist
I have:

 added the LICENSE template to new files
[ y] compiled & tested this code
 included documentation (including possible behaviour changes)",True,{}
pmacct/pmacct,https://github.com/pmacct/pmacct,468,2021-02-01T22:21:18Z,2021-02-05T01:05:17Z,2021-02-05T01:05:17Z,MERGED,True,16,10,3,https://github.com/paololucente,Build system improvements for release process,5,[],https://github.com/pmacct/pmacct/pull/468,https://github.com/paololucente,1,https://github.com/pmacct/pmacct/pull/468,"Short description
PR for kind review of the inter-galactic super-hero of the build system (among the other things) Marc ( @msune )
Checklist
I have:

[ X] compiled & tested this code","Short description
PR for kind review of the inter-galactic super-hero of the build system (among the other things) Marc ( @msune )
Checklist
I have:

[ X] compiled & tested this code",True,{}
pmacct/pmacct,https://github.com/pmacct/pmacct,469,2021-02-14T00:38:35Z,2021-02-16T20:24:14Z,2021-02-16T20:24:14Z,MERGED,True,227,293,2,https://github.com/msune,ci: move to Github Actions,1,[],https://github.com/pmacct/pmacct/pull/469,https://github.com/msune,1,https://github.com/pmacct/pmacct/pull/469,"travis-ci.org is dead, and travis.com is also dead, as they are
putting all the possible problems for OSS - even though they claim
they still fully support OSS. Sigh...
We urgently need to change CI as all pipelines in travis-ci.org are failing miserably, which prevents 1.7.6 dockers to be uploaded.
This commit replicates the same exact CI
structure that we had in travis-ci.org into Github Actions (GHA).
VERY IMPORTANT
Steps to integrate:

Peer review. Don't merge!
Add variables  DOCKER_USERNAME DOCKER_PASSWORD to github secrets. The password is the same token used in Travis.
Current tags are not annotated on the base of 1.7.6 branch, so git describe fails (``). Therefore tag commit 300603452739bb80a271ecfd2c64093ee9799953 (the base of both `master` and `1.7.6.` branches) with say `1.7.6-rc1` or something along those lines.
Merge this MR
Tag v1.7.6. Make sure it's annotated. Tags shall always be annoted for the scripts to work. E.g.:

git tag v1.7.6 -m ""Adding v1.7.6""

The easiest way to check if the tag is ok is using git describe, without any arguments (specially NOT --tags or --all)

Push it. Make sure docker image is uploaded to dockerhub before moving to the next step (to double check github secrets are ok).
Merge 1.7.6 branch into master. This will import the CI code, and the base for master. From that point on you can develop regularly.
Have a cocktail on my behalf!

Details
travis-ci.org is dead, and travis.com is also dead, as they are
putting all the possible problems for OSS - even though they claim
they still fully support OSS. Sigh...
Time to move on. This commit replicates the same exact CI
structure that we had in travis-ci.org into Github Actions (GHA).
It also removes all travis related files.
For this PR to work, the following ""secrets"" have to be defined in
Github settings of the project:

DOCKER_USERNAME
DOCKER_PASSWORD

Which are the same ones already defined in travis-ci.org and are the
username (pmacct) and the token for dockerhub uploads.
As a final remark, this commit adds nightly builds at 12am, everyday
in the default branch.
Checklist
I have:

 added the LICENSE template to new files
 compiled & tested this code
 included documentation (including possible behaviour changes)","travis-ci.org is dead, and travis.com is also dead, as they are
putting all the possible problems for OSS - even though they claim
they still fully support OSS. Sigh...
We urgently need to change CI as all pipelines in travis-ci.org are failing miserably, which prevents 1.7.6 dockers to be uploaded.
This commit replicates the same exact CI
structure that we had in travis-ci.org into Github Actions (GHA).
VERY IMPORTANT
Steps to integrate:

Peer review. Don't merge!
Add variables  DOCKER_USERNAME DOCKER_PASSWORD to github secrets. The password is the same token used in Travis.
Current tags are not annotated on the base of 1.7.6 branch, so git describe fails (``). Therefore tag commit 300603452739bb80a271ecfd2c64093ee9799953 (the base of both `master` and `1.7.6.` branches) with say `1.7.6-rc1` or something along those lines.
Merge this MR
Tag v1.7.6. Make sure it's annotated. Tags shall always be annoted for the scripts to work. E.g.:

git tag v1.7.6 -m ""Adding v1.7.6""

The easiest way to check if the tag is ok is using git describe, without any arguments (specially NOT --tags or --all)

Push it. Make sure docker image is uploaded to dockerhub before moving to the next step (to double check github secrets are ok).
Merge 1.7.6 branch into master. This will import the CI code, and the base for master. From that point on you can develop regularly.
Have a cocktail on my behalf!

Details
travis-ci.org is dead, and travis.com is also dead, as they are
putting all the possible problems for OSS - even though they claim
they still fully support OSS. Sigh...
Time to move on. This commit replicates the same exact CI
structure that we had in travis-ci.org into Github Actions (GHA).
It also removes all travis related files.
For this PR to work, the following ""secrets"" have to be defined in
Github settings of the project:

DOCKER_USERNAME
DOCKER_PASSWORD

Which are the same ones already defined in travis-ci.org and are the
username (pmacct) and the token for dockerhub uploads.
As a final remark, this commit adds nightly builds at 12am, everyday
in the default branch.
Checklist
I have:

 added the LICENSE template to new files
 compiled & tested this code
 included documentation (including possible behaviour changes)",True,{}
pmacct/pmacct,https://github.com/pmacct/pmacct,469,2021-02-14T00:38:35Z,2021-02-16T20:24:14Z,2021-02-16T20:24:14Z,MERGED,True,227,293,2,https://github.com/msune,ci: move to Github Actions,1,[],https://github.com/pmacct/pmacct/pull/469,https://github.com/msune,2,https://github.com/pmacct/pmacct/pull/469#issuecomment-778700668,"travis-ci.org is dead, and travis.com is also dead, as they are
putting all the possible problems for OSS - even though they claim
they still fully support OSS. Sigh...
We urgently need to change CI as all pipelines in travis-ci.org are failing miserably, which prevents 1.7.6 dockers to be uploaded.
This commit replicates the same exact CI
structure that we had in travis-ci.org into Github Actions (GHA).
VERY IMPORTANT
Steps to integrate:

Peer review. Don't merge!
Add variables  DOCKER_USERNAME DOCKER_PASSWORD to github secrets. The password is the same token used in Travis.
Current tags are not annotated on the base of 1.7.6 branch, so git describe fails (``). Therefore tag commit 300603452739bb80a271ecfd2c64093ee9799953 (the base of both `master` and `1.7.6.` branches) with say `1.7.6-rc1` or something along those lines.
Merge this MR
Tag v1.7.6. Make sure it's annotated. Tags shall always be annoted for the scripts to work. E.g.:

git tag v1.7.6 -m ""Adding v1.7.6""

The easiest way to check if the tag is ok is using git describe, without any arguments (specially NOT --tags or --all)

Push it. Make sure docker image is uploaded to dockerhub before moving to the next step (to double check github secrets are ok).
Merge 1.7.6 branch into master. This will import the CI code, and the base for master. From that point on you can develop regularly.
Have a cocktail on my behalf!

Details
travis-ci.org is dead, and travis.com is also dead, as they are
putting all the possible problems for OSS - even though they claim
they still fully support OSS. Sigh...
Time to move on. This commit replicates the same exact CI
structure that we had in travis-ci.org into Github Actions (GHA).
It also removes all travis related files.
For this PR to work, the following ""secrets"" have to be defined in
Github settings of the project:

DOCKER_USERNAME
DOCKER_PASSWORD

Which are the same ones already defined in travis-ci.org and are the
username (pmacct) and the token for dockerhub uploads.
As a final remark, this commit adds nightly builds at 12am, everyday
in the default branch.
Checklist
I have:

 added the LICENSE template to new files
 compiled & tested this code
 included documentation (including possible behaviour changes)","Also, you need to enable Actions in the settings page of the project.",True,{}
pmacct/pmacct,https://github.com/pmacct/pmacct,469,2021-02-14T00:38:35Z,2021-02-16T20:24:14Z,2021-02-16T20:24:14Z,MERGED,True,227,293,2,https://github.com/msune,ci: move to Github Actions,1,[],https://github.com/pmacct/pmacct/pull/469,https://github.com/msune,3,https://github.com/pmacct/pmacct/pull/469#issuecomment-779482257,"travis-ci.org is dead, and travis.com is also dead, as they are
putting all the possible problems for OSS - even though they claim
they still fully support OSS. Sigh...
We urgently need to change CI as all pipelines in travis-ci.org are failing miserably, which prevents 1.7.6 dockers to be uploaded.
This commit replicates the same exact CI
structure that we had in travis-ci.org into Github Actions (GHA).
VERY IMPORTANT
Steps to integrate:

Peer review. Don't merge!
Add variables  DOCKER_USERNAME DOCKER_PASSWORD to github secrets. The password is the same token used in Travis.
Current tags are not annotated on the base of 1.7.6 branch, so git describe fails (``). Therefore tag commit 300603452739bb80a271ecfd2c64093ee9799953 (the base of both `master` and `1.7.6.` branches) with say `1.7.6-rc1` or something along those lines.
Merge this MR
Tag v1.7.6. Make sure it's annotated. Tags shall always be annoted for the scripts to work. E.g.:

git tag v1.7.6 -m ""Adding v1.7.6""

The easiest way to check if the tag is ok is using git describe, without any arguments (specially NOT --tags or --all)

Push it. Make sure docker image is uploaded to dockerhub before moving to the next step (to double check github secrets are ok).
Merge 1.7.6 branch into master. This will import the CI code, and the base for master. From that point on you can develop regularly.
Have a cocktail on my behalf!

Details
travis-ci.org is dead, and travis.com is also dead, as they are
putting all the possible problems for OSS - even though they claim
they still fully support OSS. Sigh...
Time to move on. This commit replicates the same exact CI
structure that we had in travis-ci.org into Github Actions (GHA).
It also removes all travis related files.
For this PR to work, the following ""secrets"" have to be defined in
Github settings of the project:

DOCKER_USERNAME
DOCKER_PASSWORD

Which are the same ones already defined in travis-ci.org and are the
username (pmacct) and the token for dockerhub uploads.
As a final remark, this commit adds nightly builds at 12am, everyday
in the default branch.
Checklist
I have:

 added the LICENSE template to new files
 compiled & tested this code
 included documentation (including possible behaviour changes)",Let's touch base before merge,True,{}
pmacct/pmacct,https://github.com/pmacct/pmacct,471,2021-02-16T23:50:08Z,2021-02-16T23:52:05Z,2021-02-16T23:52:05Z,MERGED,True,1,1,1,https://github.com/msune,README.md: replace badge travis -> gha,1,[],https://github.com/pmacct/pmacct/pull/471,https://github.com/msune,1,https://github.com/pmacct/pmacct/pull/471,"Short description
Remove the last bit of travis in the repo by adding the GHA CI badge in README.md
Checklist
I have:

 added the LICENSE template to new files
 compiled & tested this code
 included documentation (including possible behaviour changes)","Short description
Remove the last bit of travis in the repo by adding the GHA CI badge in README.md
Checklist
I have:

 added the LICENSE template to new files
 compiled & tested this code
 included documentation (including possible behaviour changes)",True,{}
pmacct/pmacct,https://github.com/pmacct/pmacct,472,2021-02-23T13:19:52Z,2021-02-24T15:31:33Z,2021-02-24T15:31:34Z,MERGED,True,1,1,1,https://github.com/SeeMyPing,Update ndpi_util.c,1,[],https://github.com/pmacct/pmacct/pull/472,https://github.com/SeeMyPing,1,https://github.com/pmacct/pmacct/pull/472,"Correction : ndpi_finalize_initialization name.
Short description
Correction of ndpi_finalize_initialization call line 82.
Checklist

I have:

 added the LICENSE template to new files
 compiled & tested this code
 included documentation (including possible behaviour changes)","Correction : ndpi_finalize_initialization name.
Short description
Correction of ndpi_finalize_initialization call line 82.
Checklist

I have:

 added the LICENSE template to new files
 compiled & tested this code
 included documentation (including possible behaviour changes)",True,{}
pmacct/pmacct,https://github.com/pmacct/pmacct,475,2021-02-27T11:17:20Z,2021-03-07T15:46:30Z,2021-03-07T15:46:30Z,MERGED,True,3402,0,1,https://github.com/stucchimax,Add a markdown-ified version of the quickstart guide.,1,[],https://github.com/pmacct/pmacct/pull/475,https://github.com/stucchimax,1,https://github.com/pmacct/pmacct/pull/475,"I have converted the actual text into Markdown for more readability.
The work is not completed, as there are parts that still need some
refactoring in Markdown, but it's better to have the file in the repo as
soon as possible so new changes might get into this one rather than on
the text file (which I kept separate for preserving history).
Short description

Checklist

I have:

 added the LICENSE template to new files
 compiled & tested this code
 included documentation (including possible behaviour changes)","I have converted the actual text into Markdown for more readability.
The work is not completed, as there are parts that still need some
refactoring in Markdown, but it's better to have the file in the repo as
soon as possible so new changes might get into this one rather than on
the text file (which I kept separate for preserving history).
Short description

Checklist

I have:

 added the LICENSE template to new files
 compiled & tested this code
 included documentation (including possible behaviour changes)",True,{}
pmacct/pmacct,https://github.com/pmacct/pmacct,476,2021-02-28T18:27:01Z,2021-02-28T23:08:07Z,2021-03-01T21:24:28Z,MERGED,True,1,1,1,https://github.com/msune,ci: fix don't run dockerhub stage for PRs,1,[],https://github.com/pmacct/pmacct/pull/476,https://github.com/msune,1,https://github.com/pmacct/pmacct/pull/476,"This commit fixes the condition so that dockerhub stage is never
run on PRs, which avoids red lights on external PRs.
Short description

Checklist

I have:

 added the LICENSE template to new files
 compiled & tested this code
 included documentation (including possible behaviour changes)","This commit fixes the condition so that dockerhub stage is never
run on PRs, which avoids red lights on external PRs.
Short description

Checklist

I have:

 added the LICENSE template to new files
 compiled & tested this code
 included documentation (including possible behaviour changes)",True,{}
pmacct/pmacct,https://github.com/pmacct/pmacct,477,2021-02-28T23:13:26Z,2021-02-28T23:42:37Z,2021-02-28T23:42:37Z,MERGED,True,1,1,1,https://github.com/msune,configure.ac: fix external-deps help format,1,[],https://github.com/pmacct/pmacct/pull/477,https://github.com/msune,1,https://github.com/pmacct/pmacct/pull/477,"Fix formatting on --without-external-deps help string.
(very minor thing)
Short description
Checklist
I have:

 added the LICENSE template to new files
 compiled & tested this code
 included documentation (including possible behaviour changes)","Fix formatting on --without-external-deps help string.
(very minor thing)
Short description
Checklist
I have:

 added the LICENSE template to new files
 compiled & tested this code
 included documentation (including possible behaviour changes)",True,{}
pmacct/pmacct,https://github.com/pmacct/pmacct,481,2021-03-12T09:17:35Z,2021-03-12T15:27:54Z,2021-03-12T15:28:29Z,CLOSED,False,9,9,1,https://github.com/yuriskinfo,Update QUICKSTART.md,1,[],https://github.com/pmacct/pmacct/pull/481,https://github.com/yuriskinfo,1,https://github.com/pmacct/pmacct/pull/481,"Fixed links according to the  Markdown syntax (link itself goes in () after the link's name in []).
Short description
As said above - broken links fixing.
Checklist

I have:

 added the LICENSE template to new files
 compiled & tested this code
 included documentation (including possible behaviour changes)","Fixed links according to the  Markdown syntax (link itself goes in () after the link's name in []).
Short description
As said above - broken links fixing.
Checklist

I have:

 added the LICENSE template to new files
 compiled & tested this code
 included documentation (including possible behaviour changes)",True,{}
pmacct/pmacct,https://github.com/pmacct/pmacct,481,2021-03-12T09:17:35Z,2021-03-12T15:27:54Z,2021-03-12T15:28:29Z,CLOSED,False,9,9,1,https://github.com/yuriskinfo,Update QUICKSTART.md,1,[],https://github.com/pmacct/pmacct/pull/481,https://github.com/yuriskinfo,2,https://github.com/pmacct/pmacct/pull/481#issuecomment-797562023,"Fixed links according to the  Markdown syntax (link itself goes in () after the link's name in []).
Short description
As said above - broken links fixing.
Checklist

I have:

 added the LICENSE template to new files
 compiled & tested this code
 included documentation (including possible behaviour changes)","I see, yes, in that case fixing something later to be removed sounds pointless, so I go ahead and close this.
You're correct, Github Markdown flavor does not allow linking inside the page except to the headers, which is not what you are looking for. So, yes, using line numbers inside text file will be better suited.",True,{}
pmacct/pmacct,https://github.com/pmacct/pmacct,489,2021-04-02T16:37:18Z,2021-04-04T15:45:28Z,2021-04-04T16:42:23Z,MERGED,True,27,3,6,https://github.com/floatingstatic,Add configurable start offset for 'decode_arista_trailer' option,1,[],https://github.com/pmacct/pmacct/pull/489,https://github.com/floatingstatic,1,https://github.com/pmacct/pmacct/pull/489,"Short description
Arista has changed the structure of the trailer format in recent releases of EOS by appending additional fields such that the byte offset used to decode the output interface from the Arista trailer (#201) is now offset by 27 bytes (EOS 4.25) instead of 8 bytes (EOS 4.21 and earlier) from the end of the packet. To make this backwards compatible we should add a configurable byte offset to mark the start of the trailer from the end of a packet with a default value of 8 to maintain backwards compatibility. Future releases of EOS will likely append additional fields and arista_trailer_offset will allow one to compensate for any changes to the trailer structure.
Checklist
I have:

[x ] compiled & tested this code
 included documentation (including possible behaviour changes)","Short description
Arista has changed the structure of the trailer format in recent releases of EOS by appending additional fields such that the byte offset used to decode the output interface from the Arista trailer (#201) is now offset by 27 bytes (EOS 4.25) instead of 8 bytes (EOS 4.21 and earlier) from the end of the packet. To make this backwards compatible we should add a configurable byte offset to mark the start of the trailer from the end of a packet with a default value of 8 to maintain backwards compatibility. Future releases of EOS will likely append additional fields and arista_trailer_offset will allow one to compensate for any changes to the trailer structure.
Checklist
I have:

[x ] compiled & tested this code
 included documentation (including possible behaviour changes)",True,{}
pmacct/pmacct,https://github.com/pmacct/pmacct,489,2021-04-02T16:37:18Z,2021-04-04T15:45:28Z,2021-04-04T16:42:23Z,MERGED,True,27,3,6,https://github.com/floatingstatic,Add configurable start offset for 'decode_arista_trailer' option,1,[],https://github.com/pmacct/pmacct/pull/489,https://github.com/floatingstatic,2,https://github.com/pmacct/pmacct/pull/489#issuecomment-812608712,"Short description
Arista has changed the structure of the trailer format in recent releases of EOS by appending additional fields such that the byte offset used to decode the output interface from the Arista trailer (#201) is now offset by 27 bytes (EOS 4.25) instead of 8 bytes (EOS 4.21 and earlier) from the end of the packet. To make this backwards compatible we should add a configurable byte offset to mark the start of the trailer from the end of a packet with a default value of 8 to maintain backwards compatibility. Future releases of EOS will likely append additional fields and arista_trailer_offset will allow one to compensate for any changes to the trailer structure.
Checklist
I have:

[x ] compiled & tested this code
 included documentation (including possible behaviour changes)","I think we should add a check for config.arista_trailer_offset as well here, such as:
if (config.decode_arista_trailer && config.arista_trailer_offset) {

Because we have a default value I think we can omit this, but I do not think it would hurt to add the extra check. Let me add this.",True,{}
pmacct/pmacct,https://github.com/pmacct/pmacct,489,2021-04-02T16:37:18Z,2021-04-04T15:45:28Z,2021-04-04T16:42:23Z,MERGED,True,27,3,6,https://github.com/floatingstatic,Add configurable start offset for 'decode_arista_trailer' option,1,[],https://github.com/pmacct/pmacct/pull/489,https://github.com/floatingstatic,3,https://github.com/pmacct/pmacct/pull/489#issuecomment-812917324,"Short description
Arista has changed the structure of the trailer format in recent releases of EOS by appending additional fields such that the byte offset used to decode the output interface from the Arista trailer (#201) is now offset by 27 bytes (EOS 4.25) instead of 8 bytes (EOS 4.21 and earlier) from the end of the packet. To make this backwards compatible we should add a configurable byte offset to mark the start of the trailer from the end of a packet with a default value of 8 to maintain backwards compatibility. Future releases of EOS will likely append additional fields and arista_trailer_offset will allow one to compensate for any changes to the trailer structure.
Checklist
I have:

[x ] compiled & tested this code
 included documentation (including possible behaviour changes)","Hi @paololucente , thank you for the feedback. I'm glad you raised this. I'm honestly wondering if anyone outside of my employer (Fastly) is actually using decode_arista_trailer. Given the decode_arista_trailer is broken for recent EOS releases and no one has complained to you I suspect we are the only ones ðŸ˜„
What I would propose, if you are in agreement, is that we remove decode_arista_trailer (boolean) completely from the source. In its place we use the new arista_trailer_offset (unsigned integer) config option I am adding here, which by default will be set to 0 instead of 8. We will use a conditional to match config.arista_trailer_offset being a non-zero value to get into the relevant block of code that decodes the trailer to get the output interface as I have implemented in the PR. Does that work for you?",True,{}
pmacct/pmacct,https://github.com/pmacct/pmacct,489,2021-04-02T16:37:18Z,2021-04-04T15:45:28Z,2021-04-04T16:42:23Z,MERGED,True,27,3,6,https://github.com/floatingstatic,Add configurable start offset for 'decode_arista_trailer' option,1,[],https://github.com/pmacct/pmacct/pull/489,https://github.com/paololucente,4,https://github.com/pmacct/pmacct/pull/489#issuecomment-812923448,"Short description
Arista has changed the structure of the trailer format in recent releases of EOS by appending additional fields such that the byte offset used to decode the output interface from the Arista trailer (#201) is now offset by 27 bytes (EOS 4.25) instead of 8 bytes (EOS 4.21 and earlier) from the end of the packet. To make this backwards compatible we should add a configurable byte offset to mark the start of the trailer from the end of a packet with a default value of 8 to maintain backwards compatibility. Future releases of EOS will likely append additional fields and arista_trailer_offset will allow one to compensate for any changes to the trailer structure.
Checklist
I have:

[x ] compiled & tested this code
 included documentation (including possible behaviour changes)","Yes, that does work perfectly, we are aligned - you did also fill some blanks that i did not want to fill publicly :-D Last bit, do you want to take up the commit or shall i? What does work for you, works for me.",True,{}
pmacct/pmacct,https://github.com/pmacct/pmacct,489,2021-04-02T16:37:18Z,2021-04-04T15:45:28Z,2021-04-04T16:42:23Z,MERGED,True,27,3,6,https://github.com/floatingstatic,Add configurable start offset for 'decode_arista_trailer' option,1,[],https://github.com/pmacct/pmacct/pull/489,https://github.com/floatingstatic,5,https://github.com/pmacct/pmacct/pull/489#issuecomment-813062786,"Short description
Arista has changed the structure of the trailer format in recent releases of EOS by appending additional fields such that the byte offset used to decode the output interface from the Arista trailer (#201) is now offset by 27 bytes (EOS 4.25) instead of 8 bytes (EOS 4.21 and earlier) from the end of the packet. To make this backwards compatible we should add a configurable byte offset to mark the start of the trailer from the end of a packet with a default value of 8 to maintain backwards compatibility. Future releases of EOS will likely append additional fields and arista_trailer_offset will allow one to compensate for any changes to the trailer structure.
Checklist
I have:

[x ] compiled & tested this code
 included documentation (including possible behaviour changes)","Cool, looks like you made some edits. LGTM. Thanks again!",True,{}
pmacct/pmacct,https://github.com/pmacct/pmacct,491,2021-04-08T17:32:35Z,2021-04-22T09:37:40Z,2021-05-28T16:42:41Z,CLOSED,False,1,1,1,https://github.com/shaitan,bmp: fix using unconfirmed BGP ADD-PATH capability,1,[],https://github.com/pmacct/pmacct/pull/491,https://github.com/shaitan,1,https://github.com/pmacct/pmacct/pull/491,"Short description

pmbmpd runs bgp in ""offline"" mode. Therefore for OPEN message it doesn't confirm ADD-PATH capabilities to peers. As a result bgp peers don't send path identifiers in UPDATE messages.
According to RFC 7911 this confirmation is required:

A BGP speaker MUST follow the procedures defined in [RFC4271] when
generating an UPDATE message for a particular <AFI, SAFI> to a peer
unless the BGP speaker advertises the ADD-PATH Capability to the peer
indicating its ability to send multiple paths for the <AFI, SAFI>,
and also receives the ADD-PATH Capability from the peer indicating
its ability to receive multiple paths for the <AFI, SAFI>, in which
case the speaker MUST generate a route update for the <AFI, SAFI>
based on the combination of the address prefix and the Path
Identifier, and use the extended NLRI encodings specified in this
document.  The peer SHALL act accordingly in processing an UPDATE
message related to a particular <AFI, SAFI>.

When pmbmpd parses UPDATE messages, it incorrectly tries to get path identifiers from them.
This PR disables this capability for ""offline"" mode.
Checklist

I have:

 added the LICENSE template to new files
 compiled & tested this code
 included documentation (including possible behaviour changes)","Short description

pmbmpd runs bgp in ""offline"" mode. Therefore for OPEN message it doesn't confirm ADD-PATH capabilities to peers. As a result bgp peers don't send path identifiers in UPDATE messages.
According to RFC 7911 this confirmation is required:

A BGP speaker MUST follow the procedures defined in [RFC4271] when
generating an UPDATE message for a particular <AFI, SAFI> to a peer
unless the BGP speaker advertises the ADD-PATH Capability to the peer
indicating its ability to send multiple paths for the <AFI, SAFI>,
and also receives the ADD-PATH Capability from the peer indicating
its ability to receive multiple paths for the <AFI, SAFI>, in which
case the speaker MUST generate a route update for the <AFI, SAFI>
based on the combination of the address prefix and the Path
Identifier, and use the extended NLRI encodings specified in this
document.  The peer SHALL act accordingly in processing an UPDATE
message related to a particular <AFI, SAFI>.

When pmbmpd parses UPDATE messages, it incorrectly tries to get path identifiers from them.
This PR disables this capability for ""offline"" mode.
Checklist

I have:

 added the LICENSE template to new files
 compiled & tested this code
 included documentation (including possible behaviour changes)",True,{}
pmacct/pmacct,https://github.com/pmacct/pmacct,491,2021-04-08T17:32:35Z,2021-04-22T09:37:40Z,2021-05-28T16:42:41Z,CLOSED,False,1,1,1,https://github.com/shaitan,bmp: fix using unconfirmed BGP ADD-PATH capability,1,[],https://github.com/pmacct/pmacct/pull/491,https://github.com/shaitan,2,https://github.com/pmacct/pmacct/pull/491#issuecomment-819589168,"Short description

pmbmpd runs bgp in ""offline"" mode. Therefore for OPEN message it doesn't confirm ADD-PATH capabilities to peers. As a result bgp peers don't send path identifiers in UPDATE messages.
According to RFC 7911 this confirmation is required:

A BGP speaker MUST follow the procedures defined in [RFC4271] when
generating an UPDATE message for a particular <AFI, SAFI> to a peer
unless the BGP speaker advertises the ADD-PATH Capability to the peer
indicating its ability to send multiple paths for the <AFI, SAFI>,
and also receives the ADD-PATH Capability from the peer indicating
its ability to receive multiple paths for the <AFI, SAFI>, in which
case the speaker MUST generate a route update for the <AFI, SAFI>
based on the combination of the address prefix and the Path
Identifier, and use the extended NLRI encodings specified in this
document.  The peer SHALL act accordingly in processing an UPDATE
message related to a particular <AFI, SAFI>.

When pmbmpd parses UPDATE messages, it incorrectly tries to get path identifiers from them.
This PR disables this capability for ""offline"" mode.
Checklist

I have:

 added the LICENSE template to new files
 compiled & tested this code
 included documentation (including possible behaviour changes)","Hi Paolo!

Are you saying that there are implications in how subsequent BGP Update packets carried by BMP Route Monitoring are parsed?

Exactly, these implications are made by the code. It checks whether the capability was enabled and interprets first 4 bytes of each prefix as a path identifier.",True,{}
pmacct/pmacct,https://github.com/pmacct/pmacct,491,2021-04-08T17:32:35Z,2021-04-22T09:37:40Z,2021-05-28T16:42:41Z,CLOSED,False,1,1,1,https://github.com/shaitan,bmp: fix using unconfirmed BGP ADD-PATH capability,1,[],https://github.com/pmacct/pmacct/pull/491,https://github.com/paololucente,3,https://github.com/pmacct/pmacct/pull/491#issuecomment-820584249,"Short description

pmbmpd runs bgp in ""offline"" mode. Therefore for OPEN message it doesn't confirm ADD-PATH capabilities to peers. As a result bgp peers don't send path identifiers in UPDATE messages.
According to RFC 7911 this confirmation is required:

A BGP speaker MUST follow the procedures defined in [RFC4271] when
generating an UPDATE message for a particular <AFI, SAFI> to a peer
unless the BGP speaker advertises the ADD-PATH Capability to the peer
indicating its ability to send multiple paths for the <AFI, SAFI>,
and also receives the ADD-PATH Capability from the peer indicating
its ability to receive multiple paths for the <AFI, SAFI>, in which
case the speaker MUST generate a route update for the <AFI, SAFI>
based on the combination of the address prefix and the Path
Identifier, and use the extended NLRI encodings specified in this
document.  The peer SHALL act accordingly in processing an UPDATE
message related to a particular <AFI, SAFI>.

When pmbmpd parses UPDATE messages, it incorrectly tries to get path identifiers from them.
This PR disables this capability for ""offline"" mode.
Checklist

I have:

 added the LICENSE template to new files
 compiled & tested this code
 included documentation (including possible behaviour changes)","Hi Kirill,
I see, indeed: the capability exchange influences how BGP Update messages are parsed, clear. So ADD-PATH is received in the BGP Open, feature is enabled when parsing BGP Update messages. What i am struggling with is the original description of the problem, i can't make sense of it Therefore for OPEN message it doesn't confirm ADD-PATH capabilities to peers. As a result bgp peers don't send path identifiers in UPDATE messages..
Let us take a different approach. See here: https://github.com/pmacct/pmacct/blob/master/QUICKSTART#L3034-#L3044 . Can you please produce me a brief BMP trace with the bmp_play tool so that i can reproduce the current issue in the code? I am unfortunately still unable to see it.
Paolo",True,{}
pmacct/pmacct,https://github.com/pmacct/pmacct,491,2021-04-08T17:32:35Z,2021-04-22T09:37:40Z,2021-05-28T16:42:41Z,CLOSED,False,1,1,1,https://github.com/shaitan,bmp: fix using unconfirmed BGP ADD-PATH capability,1,[],https://github.com/pmacct/pmacct/pull/491,https://github.com/shaitan,4,https://github.com/pmacct/pmacct/pull/491#issuecomment-843033603,"Short description

pmbmpd runs bgp in ""offline"" mode. Therefore for OPEN message it doesn't confirm ADD-PATH capabilities to peers. As a result bgp peers don't send path identifiers in UPDATE messages.
According to RFC 7911 this confirmation is required:

A BGP speaker MUST follow the procedures defined in [RFC4271] when
generating an UPDATE message for a particular <AFI, SAFI> to a peer
unless the BGP speaker advertises the ADD-PATH Capability to the peer
indicating its ability to send multiple paths for the <AFI, SAFI>,
and also receives the ADD-PATH Capability from the peer indicating
its ability to receive multiple paths for the <AFI, SAFI>, in which
case the speaker MUST generate a route update for the <AFI, SAFI>
based on the combination of the address prefix and the Path
Identifier, and use the extended NLRI encodings specified in this
document.  The peer SHALL act accordingly in processing an UPDATE
message related to a particular <AFI, SAFI>.

When pmbmpd parses UPDATE messages, it incorrectly tries to get path identifiers from them.
This PR disables this capability for ""offline"" mode.
Checklist

I have:

 added the LICENSE template to new files
 compiled & tested this code
 included documentation (including possible behaviour changes)","Sorry for the delay with answer.

What i am struggling with is the original description of the problem, i can't make sense of it Therefore for OPEN message it doesn't confirm ADD-PATH capabilities to peers. As a result bgp peers don't send path identifiers in UPDATE messages..

According to RFC 7911, this capability should be confirmed by the peer:

and also receives the ADD-PATH Capability from the peer indicating
its ability to receive multiple paths for the <AFI, SAFI>

Since BMP doesn't send a reply packet, the capability is considered as unconfirmed by BGP speaker and it doesn't send path identifier with Update messages.
More details in screenshots:
Open message that contains the ADD-PATH Capability:

Update message received from this BGP speaker that doesn't include path identifier (only prefixes)

And also raw bgp packages: bmp.zip
I tried to use bmp_play.py, but it creates big dump with all messages from all speakers.",True,{}
pmacct/pmacct,https://github.com/pmacct/pmacct,491,2021-04-08T17:32:35Z,2021-04-22T09:37:40Z,2021-05-28T16:42:41Z,CLOSED,False,1,1,1,https://github.com/shaitan,bmp: fix using unconfirmed BGP ADD-PATH capability,1,[],https://github.com/pmacct/pmacct/pull/491,https://github.com/paololucente,5,https://github.com/pmacct/pmacct/pull/491#issuecomment-847207431,"Short description

pmbmpd runs bgp in ""offline"" mode. Therefore for OPEN message it doesn't confirm ADD-PATH capabilities to peers. As a result bgp peers don't send path identifiers in UPDATE messages.
According to RFC 7911 this confirmation is required:

A BGP speaker MUST follow the procedures defined in [RFC4271] when
generating an UPDATE message for a particular <AFI, SAFI> to a peer
unless the BGP speaker advertises the ADD-PATH Capability to the peer
indicating its ability to send multiple paths for the <AFI, SAFI>,
and also receives the ADD-PATH Capability from the peer indicating
its ability to receive multiple paths for the <AFI, SAFI>, in which
case the speaker MUST generate a route update for the <AFI, SAFI>
based on the combination of the address prefix and the Path
Identifier, and use the extended NLRI encodings specified in this
document.  The peer SHALL act accordingly in processing an UPDATE
message related to a particular <AFI, SAFI>.

When pmbmpd parses UPDATE messages, it incorrectly tries to get path identifiers from them.
This PR disables this capability for ""offline"" mode.
Checklist

I have:

 added the LICENSE template to new files
 compiled & tested this code
 included documentation (including possible behaviour changes)","Hi Kirill,
Let's focus on the BMP Peer Up message, your first screenshot. If you see in there, there are two BGP OPEN messages: one captures the BGP OPEN that the remote peer sends to the BMP exporter, the other captures the BGP OPEN that the BMP exporter sends back to the remote peer. One of the two OPENs does include the ADD-PATH capability, i bet if you go explore the other one, it will not have it. But, you see, this pertains to the capability supported by the two peers and the way they are configured, in other words BMP has nothing to do with the negotiation and the subsequent BGP UPDATEs, BMP does only report what (roughly) happened in BGP.
Paolo",True,{}
pmacct/pmacct,https://github.com/pmacct/pmacct,491,2021-04-08T17:32:35Z,2021-04-22T09:37:40Z,2021-05-28T16:42:41Z,CLOSED,False,1,1,1,https://github.com/shaitan,bmp: fix using unconfirmed BGP ADD-PATH capability,1,[],https://github.com/pmacct/pmacct/pull/491,https://github.com/shaitan,6,https://github.com/pmacct/pmacct/pull/491#issuecomment-847675631,"Short description

pmbmpd runs bgp in ""offline"" mode. Therefore for OPEN message it doesn't confirm ADD-PATH capabilities to peers. As a result bgp peers don't send path identifiers in UPDATE messages.
According to RFC 7911 this confirmation is required:

A BGP speaker MUST follow the procedures defined in [RFC4271] when
generating an UPDATE message for a particular <AFI, SAFI> to a peer
unless the BGP speaker advertises the ADD-PATH Capability to the peer
indicating its ability to send multiple paths for the <AFI, SAFI>,
and also receives the ADD-PATH Capability from the peer indicating
its ability to receive multiple paths for the <AFI, SAFI>, in which
case the speaker MUST generate a route update for the <AFI, SAFI>
based on the combination of the address prefix and the Path
Identifier, and use the extended NLRI encodings specified in this
document.  The peer SHALL act accordingly in processing an UPDATE
message related to a particular <AFI, SAFI>.

When pmbmpd parses UPDATE messages, it incorrectly tries to get path identifiers from them.
This PR disables this capability for ""offline"" mode.
Checklist

I have:

 added the LICENSE template to new files
 compiled & tested this code
 included documentation (including possible behaviour changes)","Hi Paolo,
You are right! The only BGP OPEN does include the ADD-PATH capability and the other one doesn't include it. Thank you for bringing to light that BMP do nothing with the negotiation, now I see the point.
There is still a problem with parsing such BGP UPDATEs. Maybe you have a more elegant solution for it? I can suggest that BMP could analyze the negotiation and enables the ADD-PATH capability based on it.",True,{}
pmacct/pmacct,https://github.com/pmacct/pmacct,491,2021-04-08T17:32:35Z,2021-04-22T09:37:40Z,2021-05-28T16:42:41Z,CLOSED,False,1,1,1,https://github.com/shaitan,bmp: fix using unconfirmed BGP ADD-PATH capability,1,[],https://github.com/pmacct/pmacct/pull/491,https://github.com/paololucente,7,https://github.com/pmacct/pmacct/pull/491#issuecomment-850540959,"Short description

pmbmpd runs bgp in ""offline"" mode. Therefore for OPEN message it doesn't confirm ADD-PATH capabilities to peers. As a result bgp peers don't send path identifiers in UPDATE messages.
According to RFC 7911 this confirmation is required:

A BGP speaker MUST follow the procedures defined in [RFC4271] when
generating an UPDATE message for a particular <AFI, SAFI> to a peer
unless the BGP speaker advertises the ADD-PATH Capability to the peer
indicating its ability to send multiple paths for the <AFI, SAFI>,
and also receives the ADD-PATH Capability from the peer indicating
its ability to receive multiple paths for the <AFI, SAFI>, in which
case the speaker MUST generate a route update for the <AFI, SAFI>
based on the combination of the address prefix and the Path
Identifier, and use the extended NLRI encodings specified in this
document.  The peer SHALL act accordingly in processing an UPDATE
message related to a particular <AFI, SAFI>.

When pmbmpd parses UPDATE messages, it incorrectly tries to get path identifiers from them.
This PR disables this capability for ""offline"" mode.
Checklist

I have:

 added the LICENSE template to new files
 compiled & tested this code
 included documentation (including possible behaviour changes)","Hi Kirill,
Fantastic, we are in sync on the role of BMP with regards to ADD-PATH negotiation. With regards to parsing the BGP Update, can you confirm me what version of pmacct you are running and tell me exactly what issue you are running into?
This is the logic implemented: when the Peer Up message is received, pmacct looks into both BGP Open messages and does a sort of reconciliation of the two a-la ""do i see a capability in both BGP Open messages? Then it must be true; if i did see it only in one of them then it may be not true so let's disable the capability"". Now, while reviewing the code for this issue i bumped in this: https://github.com/pmacct/pmacct/blob/master/src/bmp/bmp_util.c#L207-#L210 , essentially this check is disabled for ADD-PATH and i am sure there must be some further background than what is described (ie. buggy BMP exporter implementation): can you please de-comment line 210, essentially re-enable the check, re-compile and see if that does for you? If so, we'll take it from there; otherwise please explain me better which situation you are running into.
Paolo",True,{}
pmacct/pmacct,https://github.com/pmacct/pmacct,492,2021-04-08T20:46:21Z,2021-04-09T13:44:58Z,2021-04-09T13:44:58Z,MERGED,True,26,1,6,https://github.com/floatingstatic,Add configurable pcap_arista_trailer_flag_value option,3,[],https://github.com/pmacct/pmacct/pull/492,https://github.com/floatingstatic,1,https://github.com/pmacct/pmacct/pull/492,"Short description
In applying #489 to a broader set of hardware in my network I observed that certain chipsets set additional flags in the Arista trailer. 1 which is hardcoded in the source as the expected value to tell pmacct ""the output interface field is present in the trailer"" ends up not being correct the correct value and needs tuning along with the trailer start offset. For example T2 chipsets would utilize 1 but this should actually be 7 on Jericho. Given it is uncertain what this value should be and it may change over time we should make this configurable as well.
Checklist
I have:

[x ] compiled & tested this code
[x ] included documentation (including possible behaviour changes)","Short description
In applying #489 to a broader set of hardware in my network I observed that certain chipsets set additional flags in the Arista trailer. 1 which is hardcoded in the source as the expected value to tell pmacct ""the output interface field is present in the trailer"" ends up not being correct the correct value and needs tuning along with the trailer start offset. For example T2 chipsets would utilize 1 but this should actually be 7 on Jericho. Given it is uncertain what this value should be and it may change over time we should make this configurable as well.
Checklist
I have:

[x ] compiled & tested this code
[x ] included documentation (including possible behaviour changes)",True,{}
pmacct/pmacct,https://github.com/pmacct/pmacct,494,2021-04-10T18:15:36Z,2021-04-11T18:25:16Z,2021-04-11T18:25:16Z,MERGED,True,255,28,2,https://github.com/msune,docs: add DOCKER.md,1,[],https://github.com/pmacct/pmacct/pull/494,https://github.com/msune,1,https://github.com/pmacct/pmacct/pull/494,"Short description
This commit adds a dedicated DOCKER.md page, with an introduction to
the Docker images, how to use them and troubleshooting procedures.
It also adapts the README.md accordingly to avoid duplicity.
Checklist
I have:

 added the LICENSE template to new files
 compiled & tested this code
 included documentation (including possible behaviour changes)","Short description
This commit adds a dedicated DOCKER.md page, with an introduction to
the Docker images, how to use them and troubleshooting procedures.
It also adapts the README.md accordingly to avoid duplicity.
Checklist
I have:

 added the LICENSE template to new files
 compiled & tested this code
 included documentation (including possible behaviour changes)",True,{}
pmacct/pmacct,https://github.com/pmacct/pmacct,495,2021-04-11T12:35:30Z,2021-04-13T16:16:05Z,2021-04-13T16:16:05Z,CLOSED,False,8,3,1,https://github.com/floatingstatic,Fix tee_source_ip - issue #493,1,[],https://github.com/pmacct/pmacct/pull/495,https://github.com/floatingstatic,1,https://github.com/pmacct/pmacct/pull/495,"Short description
Fixes issue #493 by partially reverting a35e3c4
Before this patch using tee_source_ip: 203.0.113.2:
12:27:56.503088 Out 74:83:ef:1e:7f:99 ethertype IPv4 (0x0800), length 516: 172.24.12.3.38956 > 203.0.113.254.4739: UDP, length 472
12:27:56.503106 Out 74:83:ef:1e:7f:99 ethertype IPv4 (0x0800), length 516: 172.24.12.3.41767 > 203.0.113.254.4739: UDP, length 472
12:27:56.503129 Out 74:83:ef:1e:7f:99 ethertype IPv4 (0x0800), length 364: 172.24.12.3.49307 > 203.0.113.254.4739: UDP, length 320

After:
12:28:15.287022 Out 74:83:ef:1e:7f:99 ethertype IPv4 (0x0800), length 504: 203.0.113.2.51519 > 203.0.113.254.4739: UDP, length 460
12:28:15.287076 Out 74:83:ef:1e:7f:99 ethertype IPv4 (0x0800), length 516: 203.0.113.2.43431 > 203.0.113.254.4739: UDP, length 472
12:28:15.287118 Out 74:83:ef:1e:7f:99 ethertype IPv4 (0x0800), length 516: 203.0.113.2.51492 > 203.0.113.254.4739: UDP, length 472

Checklist

I have:

 added the LICENSE template to new files
[x ] compiled & tested this code
 included documentation (including possible behaviour changes)","Short description
Fixes issue #493 by partially reverting a35e3c4
Before this patch using tee_source_ip: 203.0.113.2:
12:27:56.503088 Out 74:83:ef:1e:7f:99 ethertype IPv4 (0x0800), length 516: 172.24.12.3.38956 > 203.0.113.254.4739: UDP, length 472
12:27:56.503106 Out 74:83:ef:1e:7f:99 ethertype IPv4 (0x0800), length 516: 172.24.12.3.41767 > 203.0.113.254.4739: UDP, length 472
12:27:56.503129 Out 74:83:ef:1e:7f:99 ethertype IPv4 (0x0800), length 364: 172.24.12.3.49307 > 203.0.113.254.4739: UDP, length 320

After:
12:28:15.287022 Out 74:83:ef:1e:7f:99 ethertype IPv4 (0x0800), length 504: 203.0.113.2.51519 > 203.0.113.254.4739: UDP, length 460
12:28:15.287076 Out 74:83:ef:1e:7f:99 ethertype IPv4 (0x0800), length 516: 203.0.113.2.43431 > 203.0.113.254.4739: UDP, length 472
12:28:15.287118 Out 74:83:ef:1e:7f:99 ethertype IPv4 (0x0800), length 516: 203.0.113.2.51492 > 203.0.113.254.4739: UDP, length 472

Checklist

I have:

 added the LICENSE template to new files
[x ] compiled & tested this code
 included documentation (including possible behaviour changes)",True,{}
pmacct/pmacct,https://github.com/pmacct/pmacct,503,2021-05-13T09:51:28Z,2021-05-20T00:12:14Z,2021-05-20T09:25:53Z,MERGED,True,92,14,5,https://github.com/dcaba,Minor docker build improvements,9,[],https://github.com/pmacct/pmacct/pull/503,https://github.com/dcaba,1,https://github.com/pmacct/pmacct/pull/503,"Short description
This PR just bring minor build improvements, mostly in the base Dockerfile:

In order to reduce disk usage a bit (suggesting to move to the Debian slim flavor, disabling man pages installation, cleaning apt dirs)
In order to improve build performance (compiling with -j CPU_CORES)
to avoid surprises (.dockerignore pointing to .gitignore, that ensures we don't add to the build context/resulting images things that we normally don't commit to git, as artifacts from builds in your workstation, and pulling software checking certificates... not doing that is a sec risk)
And, finally, minor formatting improvements (the MAINTAINER statement is deprecated)

A second PR will bring a multi-stage proposal as, despite the explicit package cleanup, that is partial and we are keeping lots of build requirements in the final runtime image.
Checklist
Note: no new files introduced, neither source code changed. If there are manual image validation steps, let me know... I've built things locally, but that's it. And if you want me to update some docs or CHANGELOG, just let me know!
I have:

 added the LICENSE template to new files
 compiled & tested this code
 included documentation (including possible behavior changes)","Short description
This PR just bring minor build improvements, mostly in the base Dockerfile:

In order to reduce disk usage a bit (suggesting to move to the Debian slim flavor, disabling man pages installation, cleaning apt dirs)
In order to improve build performance (compiling with -j CPU_CORES)
to avoid surprises (.dockerignore pointing to .gitignore, that ensures we don't add to the build context/resulting images things that we normally don't commit to git, as artifacts from builds in your workstation, and pulling software checking certificates... not doing that is a sec risk)
And, finally, minor formatting improvements (the MAINTAINER statement is deprecated)

A second PR will bring a multi-stage proposal as, despite the explicit package cleanup, that is partial and we are keeping lots of build requirements in the final runtime image.
Checklist
Note: no new files introduced, neither source code changed. If there are manual image validation steps, let me know... I've built things locally, but that's it. And if you want me to update some docs or CHANGELOG, just let me know!
I have:

 added the LICENSE template to new files
 compiled & tested this code
 included documentation (including possible behavior changes)",True,{}
pmacct/pmacct,https://github.com/pmacct/pmacct,503,2021-05-13T09:51:28Z,2021-05-20T00:12:14Z,2021-05-20T09:25:53Z,MERGED,True,92,14,5,https://github.com/dcaba,Minor docker build improvements,9,[],https://github.com/pmacct/pmacct/pull/503,https://github.com/msune,2,https://github.com/pmacct/pmacct/pull/503#issuecomment-841538811,"Short description
This PR just bring minor build improvements, mostly in the base Dockerfile:

In order to reduce disk usage a bit (suggesting to move to the Debian slim flavor, disabling man pages installation, cleaning apt dirs)
In order to improve build performance (compiling with -j CPU_CORES)
to avoid surprises (.dockerignore pointing to .gitignore, that ensures we don't add to the build context/resulting images things that we normally don't commit to git, as artifacts from builds in your workstation, and pulling software checking certificates... not doing that is a sec risk)
And, finally, minor formatting improvements (the MAINTAINER statement is deprecated)

A second PR will bring a multi-stage proposal as, despite the explicit package cleanup, that is partial and we are keeping lots of build requirements in the final runtime image.
Checklist
Note: no new files introduced, neither source code changed. If there are manual image validation steps, let me know... I've built things locally, but that's it. And if you want me to update some docs or CHANGELOG, just let me know!
I have:

 added the LICENSE template to new files
 compiled & tested this code
 included documentation (including possible behavior changes)","@dcaba First, thank you for this PR. Good improvements overall. All looks good to me except for the --no-check-certificate.
I don't know why these are there. They came from the existing CI build scripts already. @paololucente do you recall why is that? I agree on the rule of not disabling checking certificates in general, but I guess it's worth double checking.
Aside from this, we are missing the CI run:

1 workflow awaiting approval (First-time contributors need a maintainer to approve running workflows. Learn more.)

@paololucente could you do the honours?
If we get a green light for the CI and --no-check-certificate is clarified, then it's a ðŸ‘ from me. (edited, see below)",True,{}
pmacct/pmacct,https://github.com/pmacct/pmacct,503,2021-05-13T09:51:28Z,2021-05-20T00:12:14Z,2021-05-20T09:25:53Z,MERGED,True,92,14,5,https://github.com/dcaba,Minor docker build improvements,9,[],https://github.com/pmacct/pmacct/pull/503,https://github.com/paololucente,3,https://github.com/pmacct/pmacct/pull/503#issuecomment-841669704,"Short description
This PR just bring minor build improvements, mostly in the base Dockerfile:

In order to reduce disk usage a bit (suggesting to move to the Debian slim flavor, disabling man pages installation, cleaning apt dirs)
In order to improve build performance (compiling with -j CPU_CORES)
to avoid surprises (.dockerignore pointing to .gitignore, that ensures we don't add to the build context/resulting images things that we normally don't commit to git, as artifacts from builds in your workstation, and pulling software checking certificates... not doing that is a sec risk)
And, finally, minor formatting improvements (the MAINTAINER statement is deprecated)

A second PR will bring a multi-stage proposal as, despite the explicit package cleanup, that is partial and we are keeping lots of build requirements in the final runtime image.
Checklist
Note: no new files introduced, neither source code changed. If there are manual image validation steps, let me know... I've built things locally, but that's it. And if you want me to update some docs or CHANGELOG, just let me know!
I have:

 added the LICENSE template to new files
 compiled & tested this code
 included documentation (including possible behavior changes)","--no-check-certificate is clarified

@msune : i am supportive for that one: it happened a couple of times - only couple of times in, dunno, few years but still it did happen - that certificates at an https website randomly expire (and i clearly remember one of the apache.org websites did that!) and then the CI does not complete.",True,{}
pmacct/pmacct,https://github.com/pmacct/pmacct,503,2021-05-13T09:51:28Z,2021-05-20T00:12:14Z,2021-05-20T09:25:53Z,MERGED,True,92,14,5,https://github.com/dcaba,Minor docker build improvements,9,[],https://github.com/pmacct/pmacct/pull/503,https://github.com/msune,4,https://github.com/pmacct/pmacct/pull/503#issuecomment-841705693,"Short description
This PR just bring minor build improvements, mostly in the base Dockerfile:

In order to reduce disk usage a bit (suggesting to move to the Debian slim flavor, disabling man pages installation, cleaning apt dirs)
In order to improve build performance (compiling with -j CPU_CORES)
to avoid surprises (.dockerignore pointing to .gitignore, that ensures we don't add to the build context/resulting images things that we normally don't commit to git, as artifacts from builds in your workstation, and pulling software checking certificates... not doing that is a sec risk)
And, finally, minor formatting improvements (the MAINTAINER statement is deprecated)

A second PR will bring a multi-stage proposal as, despite the explicit package cleanup, that is partial and we are keeping lots of build requirements in the final runtime image.
Checklist
Note: no new files introduced, neither source code changed. If there are manual image validation steps, let me know... I've built things locally, but that's it. And if you want me to update some docs or CHANGELOG, just let me know!
I have:

 added the LICENSE template to new files
 compiled & tested this code
 included documentation (including possible behavior changes)","--no-check-certificate is clarified

@msune : i am supportive for that one: it happened a couple of times - only couple of times in, dunno, few years but still it did happen - that certificates at an https website randomly expire (and i clearly remember one of the apache.org websites did that!) and then the CI does not complete.

If this is the case we have three options:
a) Leave it as is, understanding the security risks (essentially DNS attacks etc..)
b) Remove it, and run the risk of breaking pipelines. If this would happen again, a commit re-enabling it would be needed
c) In deps.sh check for the env variable DEPS_DONT_CHECK_CERTIFICATE and conditionally add --no-check-certificate accordingly. By default this env variable would not be defined. This has the advantage that, by just defining this variable in Github Action's env variables, the behaviour could be changed, without the need of an extra commit.
I have a strong preference for c).",True,{}
pmacct/pmacct,https://github.com/pmacct/pmacct,503,2021-05-13T09:51:28Z,2021-05-20T00:12:14Z,2021-05-20T09:25:53Z,MERGED,True,92,14,5,https://github.com/dcaba,Minor docker build improvements,9,[],https://github.com/pmacct/pmacct/pull/503,https://github.com/paololucente,5,https://github.com/pmacct/pmacct/pull/503#issuecomment-841707964,"Short description
This PR just bring minor build improvements, mostly in the base Dockerfile:

In order to reduce disk usage a bit (suggesting to move to the Debian slim flavor, disabling man pages installation, cleaning apt dirs)
In order to improve build performance (compiling with -j CPU_CORES)
to avoid surprises (.dockerignore pointing to .gitignore, that ensures we don't add to the build context/resulting images things that we normally don't commit to git, as artifacts from builds in your workstation, and pulling software checking certificates... not doing that is a sec risk)
And, finally, minor formatting improvements (the MAINTAINER statement is deprecated)

A second PR will bring a multi-stage proposal as, despite the explicit package cleanup, that is partial and we are keeping lots of build requirements in the final runtime image.
Checklist
Note: no new files introduced, neither source code changed. If there are manual image validation steps, let me know... I've built things locally, but that's it. And if you want me to update some docs or CHANGELOG, just let me know!
I have:

 added the LICENSE template to new files
 compiled & tested this code
 included documentation (including possible behavior changes)","@msune : with your C option, you indeed made a proposal that can't be refused. Support.",True,{}
pmacct/pmacct,https://github.com/pmacct/pmacct,503,2021-05-13T09:51:28Z,2021-05-20T00:12:14Z,2021-05-20T09:25:53Z,MERGED,True,92,14,5,https://github.com/dcaba,Minor docker build improvements,9,[],https://github.com/pmacct/pmacct/pull/503,https://github.com/dcaba,6,https://github.com/pmacct/pmacct/pull/503#issuecomment-842238686,"Short description
This PR just bring minor build improvements, mostly in the base Dockerfile:

In order to reduce disk usage a bit (suggesting to move to the Debian slim flavor, disabling man pages installation, cleaning apt dirs)
In order to improve build performance (compiling with -j CPU_CORES)
to avoid surprises (.dockerignore pointing to .gitignore, that ensures we don't add to the build context/resulting images things that we normally don't commit to git, as artifacts from builds in your workstation, and pulling software checking certificates... not doing that is a sec risk)
And, finally, minor formatting improvements (the MAINTAINER statement is deprecated)

A second PR will bring a multi-stage proposal as, despite the explicit package cleanup, that is partial and we are keeping lots of build requirements in the final runtime image.
Checklist
Note: no new files introduced, neither source code changed. If there are manual image validation steps, let me know... I've built things locally, but that's it. And if you want me to update some docs or CHANGELOG, just let me know!
I have:

 added the LICENSE template to new files
 compiled & tested this code
 included documentation (including possible behavior changes)","@msune : with your C option, you indeed made a proposal that can't be refused. Support.

I'm implementing C, but I really think shipping software (and dependencies) from sources we cannot verify is just something that should not be done, you can be distributing malicious software. These checks are there because of something; it's like unit tests execution: if one fails, it means something, and you cannot just skip CI tests execution in order to ship a new version. If you ever find yourself in that situation, I would recommend you to even vendor your dependency (by manually pulling it, verify signatures, and do your best to ensure no-one is taking advantage of this cert error to inject malicious stuff) rather than making CI to trust unverified sources (it's not just about DNS attacks: you know better than me that IP traffic can be hijacked also with BGP announcements, for instance)",True,{}
pmacct/pmacct,https://github.com/pmacct/pmacct,503,2021-05-13T09:51:28Z,2021-05-20T00:12:14Z,2021-05-20T09:25:53Z,MERGED,True,92,14,5,https://github.com/dcaba,Minor docker build improvements,9,[],https://github.com/pmacct/pmacct/pull/503,https://github.com/msune,7,https://github.com/pmacct/pmacct/pull/503#issuecomment-842306987,"Short description
This PR just bring minor build improvements, mostly in the base Dockerfile:

In order to reduce disk usage a bit (suggesting to move to the Debian slim flavor, disabling man pages installation, cleaning apt dirs)
In order to improve build performance (compiling with -j CPU_CORES)
to avoid surprises (.dockerignore pointing to .gitignore, that ensures we don't add to the build context/resulting images things that we normally don't commit to git, as artifacts from builds in your workstation, and pulling software checking certificates... not doing that is a sec risk)
And, finally, minor formatting improvements (the MAINTAINER statement is deprecated)

A second PR will bring a multi-stage proposal as, despite the explicit package cleanup, that is partial and we are keeping lots of build requirements in the final runtime image.
Checklist
Note: no new files introduced, neither source code changed. If there are manual image validation steps, let me know... I've built things locally, but that's it. And if you want me to update some docs or CHANGELOG, just let me know!
I have:

 added the LICENSE template to new files
 compiled & tested this code
 included documentation (including possible behavior changes)","I'm implementing C, but I really think shipping software (and dependencies) from sources we cannot verify is just something that should not be done, you can be distributing malicious software.

No one is arguing against this, I think. I am with you on this.

These checks are there because of something; it's like unit tests execution: if one fails, it means something, and you cannot just skip CI tests execution in order to ship a new version. If you ever find yourself in that situation, I would recommend you to even vendor your dependency (by manually pulling it, verify signatures, and do your best to ensure no-one is taking advantage of this cert error to inject malicious stuff) rather than making CI to trust unverified sources (it's not just about DNS attacks: you know better than me that IP traffic can be hijacked also with BGP announcements, for instance)

Yes, I think everyone here is aware of it. I am not sure you are aware/understanding the trade offs and consequences here of not doing option C (everything has a trade-off ðŸ˜‰ ), so let me briefly comment.
First and foremost; yes there are attacks, but the most common cause for that check to fail is an expired cert, so really a bad sysadmin somewhere in apache foundation (according to Paolo's), not an attack. See more below on the consequences of a true BGP hijack.
Second, this script is not used by users to build pmacct. This is only used by:

Build tests
Docker images

Users compiling pmacct from the sources have to download and install  dependencies, so it's up to them to do any security assessment.
Third, on pmacct's official Docker containers...
I furiously agree with you that we should NEVER release with DEPS_DONT_CHECK_CERTIFICATE=1. We can set this as a rule @paololucente.
There is still the case of bleeding-edge tag, which is distributed on every push to master (with green pipeline).
As said before, DEPS_DONT_CHECK_CERTIFICATE  should never be defined, just during exceptional times, and this is the proposal. If some of the issues listed above would happen, with option C the pipeline will fail... RED.
It's still under the control of @paololucente to see why the pipeline failed, and assess on which exact case we are. @paololucente should diligently decide if it's safe to enable DEPS_DONT_CHECK_CERTIFICATE or not (perhaps by downloading the sw from a container in GA nad verifying md5sum etc..) or stop because there is really a security concern.
This is no different than your proposal (B)...

Just a minor observation; if someone truly BGP hijacks some prefixes containing web servers for the deps in the script (e.g. apache), github servers will be affected by that. So we are going to talk on a massive security issue that will for sure be top priority to be solved

I see a risk of leaving that setting on beyond the problem, but again I am sure @paololucente will be diligent on this.
Now the downside of your proposal. All that you have said, including mirroring deps (see below for this) etc.. as a result of that check failed is a substantial amount of work to be done and will severely block any PR and commits of Paolo.
Since the most probably cause is a temporary error like a sysadmin not having renewed a cert (or not having uploaded it correctly...), it's possible the hardcoded --no-check-certificate will be integrated again, which is far worst than having C temporally enabled. The community needs to continue to build, integrate patches etc... If one of the libs (plugins) is really in serious trouble because it has been hacked, no worries that as part of the assessment it will be either disabled from the docker bleeding-edge image or temporally removed.

I would recommend you to even vendor your dependency (by manually pulling it, verify signatures, and do your best to ensure no-one is taking advantage of this cert error to inject malicious stuff)

For the record, I've been proposing to @paololucente  for sometime to mirror pmacct deps in a controlled env, and use submodules. This would simplify the process of building pmacct from the sources, and we will have more control. This would completely avoid the  problem you are mentioning. In fact we did it already for libcdada this way, as a proof-of-concept (except for the part of mirroring the dep (still submodule SHA ensures it's the same code or will fail to checkout if commit is gone).",True,{}
pmacct/pmacct,https://github.com/pmacct/pmacct,503,2021-05-13T09:51:28Z,2021-05-20T00:12:14Z,2021-05-20T09:25:53Z,MERGED,True,92,14,5,https://github.com/dcaba,Minor docker build improvements,9,[],https://github.com/pmacct/pmacct/pull/503,https://github.com/paololucente,8,https://github.com/pmacct/pmacct/pull/503#issuecomment-842400698,"Short description
This PR just bring minor build improvements, mostly in the base Dockerfile:

In order to reduce disk usage a bit (suggesting to move to the Debian slim flavor, disabling man pages installation, cleaning apt dirs)
In order to improve build performance (compiling with -j CPU_CORES)
to avoid surprises (.dockerignore pointing to .gitignore, that ensures we don't add to the build context/resulting images things that we normally don't commit to git, as artifacts from builds in your workstation, and pulling software checking certificates... not doing that is a sec risk)
And, finally, minor formatting improvements (the MAINTAINER statement is deprecated)

A second PR will bring a multi-stage proposal as, despite the explicit package cleanup, that is partial and we are keeping lots of build requirements in the final runtime image.
Checklist
Note: no new files introduced, neither source code changed. If there are manual image validation steps, let me know... I've built things locally, but that's it. And if you want me to update some docs or CHANGELOG, just let me know!
I have:

 added the LICENSE template to new files
 compiled & tested this code
 included documentation (including possible behavior changes)","Dear Dani ( @dcaba ) and Marc ( @msune ),
I would like to thank you so much for all the time spent for the good conversation here and the effort to make a nicely improved contribution (that was already great to start with, as Marc said a couple of times already).
Shall i summarize where we are:

we are getting all green from CI
letâ€™s go for this duplication of the ignore files, easy to fix it later (if we want to go down that path)
you guys have agreed / sorted out the CPU parallel usage
even if we have the big red button to disable cert checks, we should try avoid using it

On the last point, i see the improvements that you guys are proposing but that is not for this very Issue to solve.
In essence it looks to me that we are good to merge! Time for any objections or i will gladly proceed later today.
Paolo",True,{}
pmacct/pmacct,https://github.com/pmacct/pmacct,503,2021-05-13T09:51:28Z,2021-05-20T00:12:14Z,2021-05-20T09:25:53Z,MERGED,True,92,14,5,https://github.com/dcaba,Minor docker build improvements,9,[],https://github.com/pmacct/pmacct/pull/503,https://github.com/msune,9,https://github.com/pmacct/pmacct/pull/503#issuecomment-842414260,"Short description
This PR just bring minor build improvements, mostly in the base Dockerfile:

In order to reduce disk usage a bit (suggesting to move to the Debian slim flavor, disabling man pages installation, cleaning apt dirs)
In order to improve build performance (compiling with -j CPU_CORES)
to avoid surprises (.dockerignore pointing to .gitignore, that ensures we don't add to the build context/resulting images things that we normally don't commit to git, as artifacts from builds in your workstation, and pulling software checking certificates... not doing that is a sec risk)
And, finally, minor formatting improvements (the MAINTAINER statement is deprecated)

A second PR will bring a multi-stage proposal as, despite the explicit package cleanup, that is partial and we are keeping lots of build requirements in the final runtime image.
Checklist
Note: no new files introduced, neither source code changed. If there are manual image validation steps, let me know... I've built things locally, but that's it. And if you want me to update some docs or CHANGELOG, just let me know!
I have:

 added the LICENSE template to new files
 compiled & tested this code
 included documentation (including possible behavior changes)","@paololucente I think @dcaba is still working on the PR, so please don't merge just yet.
@dcaba whenver you are done, please ping me to have another review.",True,{'THUMBS_UP': ['https://github.com/paololucente']}
pmacct/pmacct,https://github.com/pmacct/pmacct,503,2021-05-13T09:51:28Z,2021-05-20T00:12:14Z,2021-05-20T09:25:53Z,MERGED,True,92,14,5,https://github.com/dcaba,Minor docker build improvements,9,[],https://github.com/pmacct/pmacct/pull/503,https://github.com/dcaba,10,https://github.com/pmacct/pmacct/pull/503#issuecomment-842438226,"Short description
This PR just bring minor build improvements, mostly in the base Dockerfile:

In order to reduce disk usage a bit (suggesting to move to the Debian slim flavor, disabling man pages installation, cleaning apt dirs)
In order to improve build performance (compiling with -j CPU_CORES)
to avoid surprises (.dockerignore pointing to .gitignore, that ensures we don't add to the build context/resulting images things that we normally don't commit to git, as artifacts from builds in your workstation, and pulling software checking certificates... not doing that is a sec risk)
And, finally, minor formatting improvements (the MAINTAINER statement is deprecated)

A second PR will bring a multi-stage proposal as, despite the explicit package cleanup, that is partial and we are keeping lots of build requirements in the final runtime image.
Checklist
Note: no new files introduced, neither source code changed. If there are manual image validation steps, let me know... I've built things locally, but that's it. And if you want me to update some docs or CHANGELOG, just let me know!
I have:

 added the LICENSE template to new files
 compiled & tested this code
 included documentation (including possible behavior changes)","@paololucente I think @dcaba is still working on the PR, so please don't merge just yet.
@dcaba whenver you are done, please ping me to have another review.

Hi @msune : I think you can review it again:

NUM_WORKERS with a default of 2 has been declared in the base Dockerfile
I'm expecting to cover the parallelism of the deps installation in the next PR, that is where it was initially introduced, honoring the previous setting
cert-checks as a configurable variable (that inherits its value from an optional env var) has been introduced in the ci/deps.sh script

So, unless I'm missing something, hopefully we are ready to go and focus in the next one.",True,{}
pmacct/pmacct,https://github.com/pmacct/pmacct,503,2021-05-13T09:51:28Z,2021-05-20T00:12:14Z,2021-05-20T09:25:53Z,MERGED,True,92,14,5,https://github.com/dcaba,Minor docker build improvements,9,[],https://github.com/pmacct/pmacct/pull/503,https://github.com/msune,11,https://github.com/pmacct/pmacct/pull/503#issuecomment-843459742,"Short description
This PR just bring minor build improvements, mostly in the base Dockerfile:

In order to reduce disk usage a bit (suggesting to move to the Debian slim flavor, disabling man pages installation, cleaning apt dirs)
In order to improve build performance (compiling with -j CPU_CORES)
to avoid surprises (.dockerignore pointing to .gitignore, that ensures we don't add to the build context/resulting images things that we normally don't commit to git, as artifacts from builds in your workstation, and pulling software checking certificates... not doing that is a sec risk)
And, finally, minor formatting improvements (the MAINTAINER statement is deprecated)

A second PR will bring a multi-stage proposal as, despite the explicit package cleanup, that is partial and we are keeping lots of build requirements in the final runtime image.
Checklist
Note: no new files introduced, neither source code changed. If there are manual image validation steps, let me know... I've built things locally, but that's it. And if you want me to update some docs or CHANGELOG, just let me know!
I have:

 added the LICENSE template to new files
 compiled & tested this code
 included documentation (including possible behavior changes)","(sorry for the late reply, but has been a busy day and Debian servers are ridiculously slow for me today, so validation took me a lot)
I've tried it locally. I've added some prints locally in deps.sh to see (NUM_WORKERS and DEPS_DONT_CHECK_CERTIFICATE) , might make sense adding them for real.
NUM_WORKERS works fine, but  DEPS_DONT_CHECK_CERTIFICATE doesn't seem to be set as ARG (or ENV for that matter) in the Dockerfile of the base container, so obviously --build-arg DEPS_DONT_CHECK_CERTIFICATE=1 doesn't work as expected.
Btw, (sorry for not having spotted this before) the CI code is not passing any of these variables - env variables set by github actions (NUM_WORKERS and DEPS_DONT_CHECK_CERTIFICATE) - via --build-arg to docker build for the base container, which is the only one that needs them:
https://github.com/dcaba/pmacct/blob/docker-build-improvements/.github/workflows/ci.yaml#L149
Might be worth fixing it here too.",True,{}
pmacct/pmacct,https://github.com/pmacct/pmacct,503,2021-05-13T09:51:28Z,2021-05-20T00:12:14Z,2021-05-20T09:25:53Z,MERGED,True,92,14,5,https://github.com/dcaba,Minor docker build improvements,9,[],https://github.com/pmacct/pmacct/pull/503,https://github.com/msune,12,https://github.com/pmacct/pmacct/pull/503#issuecomment-843494740,"Short description
This PR just bring minor build improvements, mostly in the base Dockerfile:

In order to reduce disk usage a bit (suggesting to move to the Debian slim flavor, disabling man pages installation, cleaning apt dirs)
In order to improve build performance (compiling with -j CPU_CORES)
to avoid surprises (.dockerignore pointing to .gitignore, that ensures we don't add to the build context/resulting images things that we normally don't commit to git, as artifacts from builds in your workstation, and pulling software checking certificates... not doing that is a sec risk)
And, finally, minor formatting improvements (the MAINTAINER statement is deprecated)

A second PR will bring a multi-stage proposal as, despite the explicit package cleanup, that is partial and we are keeping lots of build requirements in the final runtime image.
Checklist
Note: no new files introduced, neither source code changed. If there are manual image validation steps, let me know... I've built things locally, but that's it. And if you want me to update some docs or CHANGELOG, just let me know!
I have:

 added the LICENSE template to new files
 compiled & tested this code
 included documentation (including possible behavior changes)","Paolo suggested to post this here, since I have it locally (I am sure you don't need it ðŸ˜‰ ):
marc@Mriya:~/personal/pmacct$ git diff
diff --git a/ci/deps.sh b/ci/deps.sh
index 72811bb8..3240afb3 100755
--- a/ci/deps.sh
+++ b/ci/deps.sh
@@ -28,6 +28,9 @@ if [ ""${DEPS_DONT_CHECK_CERTIFICATE}"" ]; then
     WGET_FLAGS=""${WGET_FLAGS} --no-check-certificate""
 fi
+echo ""WGET_FLAGS: ${WGET_FLAGS}""
+echo ""NUM_WORKERS: ${NUM_WORKERS}""
+
 #Don't pollute /
 mkdir -p /tmp
 cd /tmp
diff --git a/docker/base/Dockerfile b/docker/base/Dockerfile
index 3cf0e222..dd92919d 100644
--- a/docker/base/Dockerfile
+++ b/docker/base/Dockerfile
@@ -18,6 +18,8 @@ COPY . /tmp/pmacct/
 #Parallelism: 2 looks a reasonable default, and bear in mind CI specs
 ARG NUM_WORKERS=2

+#Do not check certificates in wget for external deps
+ARG DEPS_DONT_CHECK_CERTIFICATE
+
 #Create a single compressed layer, as some dependencies (deps.sh)
 #can be later removed, so try to make it as lightweight as possible
 RUN apt-get update && \
marc@Mriya:~/personal/pmacct$

The line in the ci.yml:
docker build --build-arg NUM_WORKERS=$CI_NUM_WORKERS --build-arg DEPS_DONT_CHECK_CERTIFICATE=$CI_DEPS_DONT_CHECK_CERTIFICATE -f docker/base/Dockerfile -t base:_build",True,{}
pmacct/pmacct,https://github.com/pmacct/pmacct,503,2021-05-13T09:51:28Z,2021-05-20T00:12:14Z,2021-05-20T09:25:53Z,MERGED,True,92,14,5,https://github.com/dcaba,Minor docker build improvements,9,[],https://github.com/pmacct/pmacct/pull/503,https://github.com/dcaba,13,https://github.com/pmacct/pmacct/pull/503#issuecomment-844149872,"Short description
This PR just bring minor build improvements, mostly in the base Dockerfile:

In order to reduce disk usage a bit (suggesting to move to the Debian slim flavor, disabling man pages installation, cleaning apt dirs)
In order to improve build performance (compiling with -j CPU_CORES)
to avoid surprises (.dockerignore pointing to .gitignore, that ensures we don't add to the build context/resulting images things that we normally don't commit to git, as artifacts from builds in your workstation, and pulling software checking certificates... not doing that is a sec risk)
And, finally, minor formatting improvements (the MAINTAINER statement is deprecated)

A second PR will bring a multi-stage proposal as, despite the explicit package cleanup, that is partial and we are keeping lots of build requirements in the final runtime image.
Checklist
Note: no new files introduced, neither source code changed. If there are manual image validation steps, let me know... I've built things locally, but that's it. And if you want me to update some docs or CHANGELOG, just let me know!
I have:

 added the LICENSE template to new files
 compiled & tested this code
 included documentation (including possible behavior changes)","Hi @msune . I brought your diff to this branch, but not injecting in the deps.ci script NUM_WORKERS and echoing that, as this was planned to be part of the next PR (https://github.com/pmacct/pmacct/pull/504/files#r633555182 - given we started some discussions there, and other improvements in the same script, I didnt want to bring more noise here). Let me know if that is ok and I can rebase the parallel one with these approved+merged changes. And dont worry about the delay in the responses: this is OSS, I dont expect immediate & 24/7 availability.",True,{}
pmacct/pmacct,https://github.com/pmacct/pmacct,503,2021-05-13T09:51:28Z,2021-05-20T00:12:14Z,2021-05-20T09:25:53Z,MERGED,True,92,14,5,https://github.com/dcaba,Minor docker build improvements,9,[],https://github.com/pmacct/pmacct/pull/503,https://github.com/msune,14,https://github.com/pmacct/pmacct/pull/503#issuecomment-844538835,"Short description
This PR just bring minor build improvements, mostly in the base Dockerfile:

In order to reduce disk usage a bit (suggesting to move to the Debian slim flavor, disabling man pages installation, cleaning apt dirs)
In order to improve build performance (compiling with -j CPU_CORES)
to avoid surprises (.dockerignore pointing to .gitignore, that ensures we don't add to the build context/resulting images things that we normally don't commit to git, as artifacts from builds in your workstation, and pulling software checking certificates... not doing that is a sec risk)
And, finally, minor formatting improvements (the MAINTAINER statement is deprecated)

A second PR will bring a multi-stage proposal as, despite the explicit package cleanup, that is partial and we are keeping lots of build requirements in the final runtime image.
Checklist
Note: no new files introduced, neither source code changed. If there are manual image validation steps, let me know... I've built things locally, but that's it. And if you want me to update some docs or CHANGELOG, just let me know!
I have:

 added the LICENSE template to new files
 compiled & tested this code
 included documentation (including possible behavior changes)","@paololucente can you enable the CI for this last commit? This is annoying...
If pipeline is green, ACK. Please merge.
@dcaba since you will work on 504, I think this is superflous 8455736#diff-a7a92e793e13161b58a26c6c1c0445fef52d1b6fd0cc081d71701d0a1ad5c089R56",True,{}
pmacct/pmacct,https://github.com/pmacct/pmacct,503,2021-05-13T09:51:28Z,2021-05-20T00:12:14Z,2021-05-20T09:25:53Z,MERGED,True,92,14,5,https://github.com/dcaba,Minor docker build improvements,9,[],https://github.com/pmacct/pmacct/pull/503,https://github.com/dcaba,15,https://github.com/pmacct/pmacct/pull/503#issuecomment-844906994,"Short description
This PR just bring minor build improvements, mostly in the base Dockerfile:

In order to reduce disk usage a bit (suggesting to move to the Debian slim flavor, disabling man pages installation, cleaning apt dirs)
In order to improve build performance (compiling with -j CPU_CORES)
to avoid surprises (.dockerignore pointing to .gitignore, that ensures we don't add to the build context/resulting images things that we normally don't commit to git, as artifacts from builds in your workstation, and pulling software checking certificates... not doing that is a sec risk)
And, finally, minor formatting improvements (the MAINTAINER statement is deprecated)

A second PR will bring a multi-stage proposal as, despite the explicit package cleanup, that is partial and we are keeping lots of build requirements in the final runtime image.
Checklist
Note: no new files introduced, neither source code changed. If there are manual image validation steps, let me know... I've built things locally, but that's it. And if you want me to update some docs or CHANGELOG, just let me know!
I have:

 added the LICENSE template to new files
 compiled & tested this code
 included documentation (including possible behavior changes)","@dcaba since you will work on 504, I think this is superflous 8455736#diff-a7a92e793e13161b58a26c6c1c0445fef52d1b6fd0cc081d71701d0a1ad5c089R56

You are right @msune : bad habit of passing thins explicitly, but vars declared as ARGs also become env vars for all the following RUN statements. I will remove that redundant env definition in the parallel PR, thanks!",True,{}
pmacct/pmacct,https://github.com/pmacct/pmacct,504,2021-05-13T12:19:00Z,2021-05-26T15:14:24Z,2021-05-28T00:13:21Z,MERGED,True,56,55,2,https://github.com/dcaba,"Base docker build using multistage, which makes runtime more efficient and secure",5,[],https://github.com/pmacct/pmacct/pull/504,https://github.com/dcaba,1,https://github.com/pmacct/pmacct/pull/504,"Short description
This PR introduces more build related improvements (so it continues the work we started here: #503):

we have incorporated an stage in the CI logic / actions that builds in parallel the base image we use for dockerhub, so if we break something in that specific build flavor, we are not going to detect this when merging to master, that is when this specific docker flow is used to publish images to dockerhub.
in the ci/deps.sh we stop using ""&&"" to concatenate installation commands, as one failing in the middle was not breaking the build process. We also enabled parallelism when building these deps, to improve performance
And major changes in the base Dockerfile:

We move to multistage, so we don't need to clean build related deps and we just explicitly declare the ones that are required at runtime
In the build stage, we stop installing everything in a single layer, to improve cacheability, and that does not affect to the final/runtime stage anymore
With the definition of the previous introduction of .dockerignore, we no longer need to call maintainer-clean, as files not under version control are excluded.



That results in a base image that is around 300MB, 150MB less than what we already won in the previous batch of improvements. And it's not just about size (that is good for the planet): less runtime software means less chances of security issues.
Checklist
Note: no new files introduced, neither source code changed. If there are manual image validation steps, let me know... I've built things locally, already ldd'ed most of the assets in the resulting image /usr/local, and I didn't find anything linked to non existing libs. And if you want me to update some docs or CHANGELOG, just let me know!
I have:

[] added the LICENSE template to new files
[] compiled & tested this code
[] included documentation (including possible behaviour changes)","Short description
This PR introduces more build related improvements (so it continues the work we started here: #503):

we have incorporated an stage in the CI logic / actions that builds in parallel the base image we use for dockerhub, so if we break something in that specific build flavor, we are not going to detect this when merging to master, that is when this specific docker flow is used to publish images to dockerhub.
in the ci/deps.sh we stop using ""&&"" to concatenate installation commands, as one failing in the middle was not breaking the build process. We also enabled parallelism when building these deps, to improve performance
And major changes in the base Dockerfile:

We move to multistage, so we don't need to clean build related deps and we just explicitly declare the ones that are required at runtime
In the build stage, we stop installing everything in a single layer, to improve cacheability, and that does not affect to the final/runtime stage anymore
With the definition of the previous introduction of .dockerignore, we no longer need to call maintainer-clean, as files not under version control are excluded.



That results in a base image that is around 300MB, 150MB less than what we already won in the previous batch of improvements. And it's not just about size (that is good for the planet): less runtime software means less chances of security issues.
Checklist
Note: no new files introduced, neither source code changed. If there are manual image validation steps, let me know... I've built things locally, already ldd'ed most of the assets in the resulting image /usr/local, and I didn't find anything linked to non existing libs. And if you want me to update some docs or CHANGELOG, just let me know!
I have:

[] added the LICENSE template to new files
[] compiled & tested this code
[] included documentation (including possible behaviour changes)",True,{}
pmacct/pmacct,https://github.com/pmacct/pmacct,504,2021-05-13T12:19:00Z,2021-05-26T15:14:24Z,2021-05-28T00:13:21Z,MERGED,True,56,55,2,https://github.com/dcaba,"Base docker build using multistage, which makes runtime more efficient and secure",5,[],https://github.com/pmacct/pmacct/pull/504,https://github.com/msune,2,https://github.com/pmacct/pmacct/pull/504#issuecomment-841541647,"Short description
This PR introduces more build related improvements (so it continues the work we started here: #503):

we have incorporated an stage in the CI logic / actions that builds in parallel the base image we use for dockerhub, so if we break something in that specific build flavor, we are not going to detect this when merging to master, that is when this specific docker flow is used to publish images to dockerhub.
in the ci/deps.sh we stop using ""&&"" to concatenate installation commands, as one failing in the middle was not breaking the build process. We also enabled parallelism when building these deps, to improve performance
And major changes in the base Dockerfile:

We move to multistage, so we don't need to clean build related deps and we just explicitly declare the ones that are required at runtime
In the build stage, we stop installing everything in a single layer, to improve cacheability, and that does not affect to the final/runtime stage anymore
With the definition of the previous introduction of .dockerignore, we no longer need to call maintainer-clean, as files not under version control are excluded.



That results in a base image that is around 300MB, 150MB less than what we already won in the previous batch of improvements. And it's not just about size (that is good for the planet): less runtime software means less chances of security issues.
Checklist
Note: no new files introduced, neither source code changed. If there are manual image validation steps, let me know... I've built things locally, already ldd'ed most of the assets in the resulting image /usr/local, and I didn't find anything linked to non existing libs. And if you want me to update some docs or CHANGELOG, just let me know!
I have:

[] added the LICENSE template to new files
[] compiled & tested this code
[] included documentation (including possible behaviour changes)","Hi @dcaba. I will have a look into this PR in detail after #503. however two comments:

dockerignore adds the Dockerfiles and .git in this repo, to make the most of the build cache

If I read changes correctly, here you are directly (manually) copying the contents of .gitignore in .dockerignore. I would like to avoid that (symlink or simply generated by the CI itself).
I would also suggest to move this to #503, and focus this PR on multi-stange.
Btw, I suspect (after seeing this PR) that .dockerignore in #503 is not doing what was intended to be done, but I didn't try it myself.

and we stop using ""&&"" to concatenate installation commands, as one failing in the middle was not breaking the build process

I am confused...
marc@Mriya:~$ /bin/true && /bin/false && /bin/true
marc@Mriya:~$ echo $?
1
marc@Mriya:~$ /bin/true && /bin/true && /bin/true
marc@Mriya:~$ echo $?
0

To the best of my knowledge operator && has always worked this way in all shell incarnations. Could you clarify what was actually breaking the build?",True,{}
pmacct/pmacct,https://github.com/pmacct/pmacct,504,2021-05-13T12:19:00Z,2021-05-26T15:14:24Z,2021-05-28T00:13:21Z,MERGED,True,56,55,2,https://github.com/dcaba,"Base docker build using multistage, which makes runtime more efficient and secure",5,[],https://github.com/pmacct/pmacct/pull/504,https://github.com/dcaba,3,https://github.com/pmacct/pmacct/pull/504#issuecomment-842351684,"Short description
This PR introduces more build related improvements (so it continues the work we started here: #503):

we have incorporated an stage in the CI logic / actions that builds in parallel the base image we use for dockerhub, so if we break something in that specific build flavor, we are not going to detect this when merging to master, that is when this specific docker flow is used to publish images to dockerhub.
in the ci/deps.sh we stop using ""&&"" to concatenate installation commands, as one failing in the middle was not breaking the build process. We also enabled parallelism when building these deps, to improve performance
And major changes in the base Dockerfile:

We move to multistage, so we don't need to clean build related deps and we just explicitly declare the ones that are required at runtime
In the build stage, we stop installing everything in a single layer, to improve cacheability, and that does not affect to the final/runtime stage anymore
With the definition of the previous introduction of .dockerignore, we no longer need to call maintainer-clean, as files not under version control are excluded.



That results in a base image that is around 300MB, 150MB less than what we already won in the previous batch of improvements. And it's not just about size (that is good for the planet): less runtime software means less chances of security issues.
Checklist
Note: no new files introduced, neither source code changed. If there are manual image validation steps, let me know... I've built things locally, already ldd'ed most of the assets in the resulting image /usr/local, and I didn't find anything linked to non existing libs. And if you want me to update some docs or CHANGELOG, just let me know!
I have:

[] added the LICENSE template to new files
[] compiled & tested this code
[] included documentation (including possible behaviour changes)","To the best of my knowledge operator && has always worked this way in all shell incarnations. Could you clarify what was actually breaking the build?

Better an example than me trying to provide an explanation:
 $ bash -e -c ""/bin/true && /bin/false && /bin/true; echo I did finish""
I did finish
 $ bash -e -c ""/bin/true; /bin/false; /bin/true; echo I did finish""
 $ 
So the expectation of your shell is that, if you are using &&, you are doing error handling by your own, and -e will not interrupt the script execution. While if you just execute the different statements in sequence, an error will interrupt the flow, that is what we want to avoid continuing with an intermediate error. I reproduced that bug when I tried to remove the ""sudo"" binary from the docker image: the deps.sh script continues and then the main make fails because of something was not properly installed.",True,{}
pmacct/pmacct,https://github.com/pmacct/pmacct,504,2021-05-13T12:19:00Z,2021-05-26T15:14:24Z,2021-05-28T00:13:21Z,MERGED,True,56,55,2,https://github.com/dcaba,"Base docker build using multistage, which makes runtime more efficient and secure",5,[],https://github.com/pmacct/pmacct/pull/504,https://github.com/dcaba,4,https://github.com/pmacct/pmacct/pull/504#issuecomment-845010858,"Short description
This PR introduces more build related improvements (so it continues the work we started here: #503):

we have incorporated an stage in the CI logic / actions that builds in parallel the base image we use for dockerhub, so if we break something in that specific build flavor, we are not going to detect this when merging to master, that is when this specific docker flow is used to publish images to dockerhub.
in the ci/deps.sh we stop using ""&&"" to concatenate installation commands, as one failing in the middle was not breaking the build process. We also enabled parallelism when building these deps, to improve performance
And major changes in the base Dockerfile:

We move to multistage, so we don't need to clean build related deps and we just explicitly declare the ones that are required at runtime
In the build stage, we stop installing everything in a single layer, to improve cacheability, and that does not affect to the final/runtime stage anymore
With the definition of the previous introduction of .dockerignore, we no longer need to call maintainer-clean, as files not under version control are excluded.



That results in a base image that is around 300MB, 150MB less than what we already won in the previous batch of improvements. And it's not just about size (that is good for the planet): less runtime software means less chances of security issues.
Checklist
Note: no new files introduced, neither source code changed. If there are manual image validation steps, let me know... I've built things locally, already ldd'ed most of the assets in the resulting image /usr/local, and I didn't find anything linked to non existing libs. And if you want me to update some docs or CHANGELOG, just let me know!
I have:

[] added the LICENSE template to new files
[] compiled & tested this code
[] included documentation (including possible behaviour changes)","Converted back into a draft, as its going to be rebased and refined a bit. I will mark as ready once it has been revisited",True,{}
pmacct/pmacct,https://github.com/pmacct/pmacct,504,2021-05-13T12:19:00Z,2021-05-26T15:14:24Z,2021-05-28T00:13:21Z,MERGED,True,56,55,2,https://github.com/dcaba,"Base docker build using multistage, which makes runtime more efficient and secure",5,[],https://github.com/pmacct/pmacct/pull/504,https://github.com/dcaba,5,https://github.com/pmacct/pmacct/pull/504#issuecomment-845880085,"Short description
This PR introduces more build related improvements (so it continues the work we started here: #503):

we have incorporated an stage in the CI logic / actions that builds in parallel the base image we use for dockerhub, so if we break something in that specific build flavor, we are not going to detect this when merging to master, that is when this specific docker flow is used to publish images to dockerhub.
in the ci/deps.sh we stop using ""&&"" to concatenate installation commands, as one failing in the middle was not breaking the build process. We also enabled parallelism when building these deps, to improve performance
And major changes in the base Dockerfile:

We move to multistage, so we don't need to clean build related deps and we just explicitly declare the ones that are required at runtime
In the build stage, we stop installing everything in a single layer, to improve cacheability, and that does not affect to the final/runtime stage anymore
With the definition of the previous introduction of .dockerignore, we no longer need to call maintainer-clean, as files not under version control are excluded.



That results in a base image that is around 300MB, 150MB less than what we already won in the previous batch of improvements. And it's not just about size (that is good for the planet): less runtime software means less chances of security issues.
Checklist
Note: no new files introduced, neither source code changed. If there are manual image validation steps, let me know... I've built things locally, already ldd'ed most of the assets in the resulting image /usr/local, and I didn't find anything linked to non existing libs. And if you want me to update some docs or CHANGELOG, just let me know!
I have:

[] added the LICENSE template to new files
[] compiled & tested this code
[] included documentation (including possible behaviour changes)","Hi! This has been revisited and rebased, so it no longer conflicts with old implementations of the previous PR. PR description has been updated: as you can read there, I also took the freedom to add an extra stage to the CI, so the base Dockerfile you use to push to dockerhub is also built in PR jobs (other-ways, I think you were only stressing that flow when merging to master). Marking as ready to review again.",True,{}
pmacct/pmacct,https://github.com/pmacct/pmacct,504,2021-05-13T12:19:00Z,2021-05-26T15:14:24Z,2021-05-28T00:13:21Z,MERGED,True,56,55,2,https://github.com/dcaba,"Base docker build using multistage, which makes runtime more efficient and secure",5,[],https://github.com/pmacct/pmacct/pull/504,https://github.com/msune,6,https://github.com/pmacct/pmacct/pull/504#issuecomment-846462983,"Short description
This PR introduces more build related improvements (so it continues the work we started here: #503):

we have incorporated an stage in the CI logic / actions that builds in parallel the base image we use for dockerhub, so if we break something in that specific build flavor, we are not going to detect this when merging to master, that is when this specific docker flow is used to publish images to dockerhub.
in the ci/deps.sh we stop using ""&&"" to concatenate installation commands, as one failing in the middle was not breaking the build process. We also enabled parallelism when building these deps, to improve performance
And major changes in the base Dockerfile:

We move to multistage, so we don't need to clean build related deps and we just explicitly declare the ones that are required at runtime
In the build stage, we stop installing everything in a single layer, to improve cacheability, and that does not affect to the final/runtime stage anymore
With the definition of the previous introduction of .dockerignore, we no longer need to call maintainer-clean, as files not under version control are excluded.



That results in a base image that is around 300MB, 150MB less than what we already won in the previous batch of improvements. And it's not just about size (that is good for the planet): less runtime software means less chances of security issues.
Checklist
Note: no new files introduced, neither source code changed. If there are manual image validation steps, let me know... I've built things locally, already ldd'ed most of the assets in the resulting image /usr/local, and I didn't find anything linked to non existing libs. And if you want me to update some docs or CHANGELOG, just let me know!
I have:

[] added the LICENSE template to new files
[] compiled & tested this code
[] included documentation (including possible behaviour changes)","Better an example than me trying to provide an explanation:

Humm... yeah. I've stumbled across this, and it's annoying. I don't quite follow the rationale on why that fails.
To be honest I don't know why these wget && autogen && configure.. were being done, but I suspect someone just copy pasted the commands he/he was using.
I am fine replacing them with ;, but we could very well just have it in different lines, to avoid lines getting very long (and probably put some breaklines between deps to have a a visual grouping), but really this is a matter of taste, and it's perfectly fine as is.
I miss Gitlab's comment threads here... sigh",True,{}
pmacct/pmacct,https://github.com/pmacct/pmacct,504,2021-05-13T12:19:00Z,2021-05-26T15:14:24Z,2021-05-28T00:13:21Z,MERGED,True,56,55,2,https://github.com/dcaba,"Base docker build using multistage, which makes runtime more efficient and secure",5,[],https://github.com/pmacct/pmacct/pull/504,https://github.com/msune,7,https://github.com/pmacct/pmacct/pull/504#issuecomment-846468476,"Short description
This PR introduces more build related improvements (so it continues the work we started here: #503):

we have incorporated an stage in the CI logic / actions that builds in parallel the base image we use for dockerhub, so if we break something in that specific build flavor, we are not going to detect this when merging to master, that is when this specific docker flow is used to publish images to dockerhub.
in the ci/deps.sh we stop using ""&&"" to concatenate installation commands, as one failing in the middle was not breaking the build process. We also enabled parallelism when building these deps, to improve performance
And major changes in the base Dockerfile:

We move to multistage, so we don't need to clean build related deps and we just explicitly declare the ones that are required at runtime
In the build stage, we stop installing everything in a single layer, to improve cacheability, and that does not affect to the final/runtime stage anymore
With the definition of the previous introduction of .dockerignore, we no longer need to call maintainer-clean, as files not under version control are excluded.



That results in a base image that is around 300MB, 150MB less than what we already won in the previous batch of improvements. And it's not just about size (that is good for the planet): less runtime software means less chances of security issues.
Checklist
Note: no new files introduced, neither source code changed. If there are manual image validation steps, let me know... I've built things locally, already ldd'ed most of the assets in the resulting image /usr/local, and I didn't find anything linked to non existing libs. And if you want me to update some docs or CHANGELOG, just let me know!
I have:

[] added the LICENSE template to new files
[] compiled & tested this code
[] included documentation (including possible behaviour changes)","Hi Daniel,
First, thank you (again) for this contribution. The space saved is massive, so kudos!
A couple of general comments:

First, very nice using (or keep using) /tmp with the multi-stage, as the cleanup is done there for free.
I think multi-stage is a nice concept (and a pity it wasn't there sooner in docker). The code looks cleaner when using it. For pmacct in particular, however, I am not sure it's useful beyond that. All daemons just change the entrypoint from base container, as the SW package is compiled ""all at once"". So unless I am missing something, there is no ""other gain"". I am wondering if you were thinking in some follow-up contribs that would make more use of it.
If we would ever get some benefit from multi-stage, we might as well consider a 4-tier (docker) stage: 1) apt-get 2) ci-deps 3) pmacct build 4) cleanup. There is absolutely no sense on doing all this work now given my comment above.

Issues:
a) First and foremost, I didn't run it in my user's env, but I think the docker-hub stage is broken. Registries are not kept between jobs, so you need to export them as artifact, else docker-hub job won't find docker container base. See:
https://github.com/pmacct/pmacct/blob/master/.github/workflows/ci.yaml#L33
https://github.com/pmacct/pmacct/blob/master/.github/workflows/ci.yaml#L36
Comments:

(Something I was just too lazy to do it in the initial docker effort) We should probably split in two different stages the docker container (pmacct) generation from docker-hub upload. In this way PRs would only skip upload (for sec. reasons), and we would see the failures in the docker generation in the PR itself - now it's skipped.
I recommend to test docker stage in your fork (you need to enable GA). The upload is a bit painful, as you have to create all these dockerhub repos... ping me if you want me to test it, as a I have a mirror of them. But you should see it reaches the upload part of the CI
I don't quite understand the moving of base container, see below

b) Is there a reason why the new job builder-base-docker is created and runs before the build-and-test? The thing is, the  builder-* stage (btw we should put a name on it) was meant to generate the auxiliary containers to build pmacct (hence the name builder). The base container is actually having already pmacct compiled in, which is a bit different. In fact, it was intentional to compile the base container at the docker-hub stage, as if the tests were not passing in the stage before, there is really no point (waste of cycles) on compiling the base container, as it's only used for the docker images.
c) Setting aside the comment above, in the current PR's proposed workflow docker-hub stage job needs to depend on builder-base-docker. It's highly improbable, but it's a bug in the pipeline definition.",True,{}
pmacct/pmacct,https://github.com/pmacct/pmacct,504,2021-05-13T12:19:00Z,2021-05-26T15:14:24Z,2021-05-28T00:13:21Z,MERGED,True,56,55,2,https://github.com/dcaba,"Base docker build using multistage, which makes runtime more efficient and secure",5,[],https://github.com/pmacct/pmacct/pull/504,https://github.com/dcaba,8,https://github.com/pmacct/pmacct/pull/504#issuecomment-847936521,"Short description
This PR introduces more build related improvements (so it continues the work we started here: #503):

we have incorporated an stage in the CI logic / actions that builds in parallel the base image we use for dockerhub, so if we break something in that specific build flavor, we are not going to detect this when merging to master, that is when this specific docker flow is used to publish images to dockerhub.
in the ci/deps.sh we stop using ""&&"" to concatenate installation commands, as one failing in the middle was not breaking the build process. We also enabled parallelism when building these deps, to improve performance
And major changes in the base Dockerfile:

We move to multistage, so we don't need to clean build related deps and we just explicitly declare the ones that are required at runtime
In the build stage, we stop installing everything in a single layer, to improve cacheability, and that does not affect to the final/runtime stage anymore
With the definition of the previous introduction of .dockerignore, we no longer need to call maintainer-clean, as files not under version control are excluded.



That results in a base image that is around 300MB, 150MB less than what we already won in the previous batch of improvements. And it's not just about size (that is good for the planet): less runtime software means less chances of security issues.
Checklist
Note: no new files introduced, neither source code changed. If there are manual image validation steps, let me know... I've built things locally, already ldd'ed most of the assets in the resulting image /usr/local, and I didn't find anything linked to non existing libs. And if you want me to update some docs or CHANGELOG, just let me know!
I have:

[] added the LICENSE template to new files
[] compiled & tested this code
[] included documentation (including possible behaviour changes)","Hi @msune ! I'm sorry, as I think the last-minute CI change is adding more noise than noise: my aim with that extra stage was just to execute a docker-build against the base image also in non-master build jobs, as if someone breaks the base image build (it could be me with this PR, for instance), you are not going to realize of this until you merge the contribution, as the base image is only used in that publish-to-github flow. But yes, in order to make this better / reuse work in the master job, we should plug this to the docker-hub step, rather than being something independent which duplicates a portion of the job. Not a Github Actions expert, very very basic user, but I can help with that if you need (but I would prefer to keep that outside of this PR, as yes, it will imply testing the entire flow in my fork VS this harmless change)
So the main contribution here is the multistage transition, and the fix in the deps script that means the build process is not going to stop in some failure scenarios. And, just with it, we are delivering a cleaner and more slim docker image, compared to the previous approach (450MB vs 300MB, uncompressed, and by far, less packages installed in the resulting image), as the ""pmacct"" clean-up stage was not comprehensive, and yes, I find this easier to maintain than needing to care of removing and cleaning each compile-time dependency we may ever need.",True,{}
pmacct/pmacct,https://github.com/pmacct/pmacct,504,2021-05-13T12:19:00Z,2021-05-26T15:14:24Z,2021-05-28T00:13:21Z,MERGED,True,56,55,2,https://github.com/dcaba,"Base docker build using multistage, which makes runtime more efficient and secure",5,[],https://github.com/pmacct/pmacct/pull/504,https://github.com/msune,9,https://github.com/pmacct/pmacct/pull/504#issuecomment-848111224,"Short description
This PR introduces more build related improvements (so it continues the work we started here: #503):

we have incorporated an stage in the CI logic / actions that builds in parallel the base image we use for dockerhub, so if we break something in that specific build flavor, we are not going to detect this when merging to master, that is when this specific docker flow is used to publish images to dockerhub.
in the ci/deps.sh we stop using ""&&"" to concatenate installation commands, as one failing in the middle was not breaking the build process. We also enabled parallelism when building these deps, to improve performance
And major changes in the base Dockerfile:

We move to multistage, so we don't need to clean build related deps and we just explicitly declare the ones that are required at runtime
In the build stage, we stop installing everything in a single layer, to improve cacheability, and that does not affect to the final/runtime stage anymore
With the definition of the previous introduction of .dockerignore, we no longer need to call maintainer-clean, as files not under version control are excluded.



That results in a base image that is around 300MB, 150MB less than what we already won in the previous batch of improvements. And it's not just about size (that is good for the planet): less runtime software means less chances of security issues.
Checklist
Note: no new files introduced, neither source code changed. If there are manual image validation steps, let me know... I've built things locally, already ldd'ed most of the assets in the resulting image /usr/local, and I didn't find anything linked to non existing libs. And if you want me to update some docs or CHANGELOG, just let me know!
I have:

[] added the LICENSE template to new files
[] compiled & tested this code
[] included documentation (including possible behaviour changes)","OK. Please revert/remove commits that changes the CI, and let's stick to the multi-stage contribution here.
Please notify me when ready, and I will have a final (and manual) check for Dockers",True,{}
pmacct/pmacct,https://github.com/pmacct/pmacct,504,2021-05-13T12:19:00Z,2021-05-26T15:14:24Z,2021-05-28T00:13:21Z,MERGED,True,56,55,2,https://github.com/dcaba,"Base docker build using multistage, which makes runtime more efficient and secure",5,[],https://github.com/pmacct/pmacct/pull/504,https://github.com/msune,10,https://github.com/pmacct/pmacct/pull/504#issuecomment-848305724,"Short description
This PR introduces more build related improvements (so it continues the work we started here: #503):

we have incorporated an stage in the CI logic / actions that builds in parallel the base image we use for dockerhub, so if we break something in that specific build flavor, we are not going to detect this when merging to master, that is when this specific docker flow is used to publish images to dockerhub.
in the ci/deps.sh we stop using ""&&"" to concatenate installation commands, as one failing in the middle was not breaking the build process. We also enabled parallelism when building these deps, to improve performance
And major changes in the base Dockerfile:

We move to multistage, so we don't need to clean build related deps and we just explicitly declare the ones that are required at runtime
In the build stage, we stop installing everything in a single layer, to improve cacheability, and that does not affect to the final/runtime stage anymore
With the definition of the previous introduction of .dockerignore, we no longer need to call maintainer-clean, as files not under version control are excluded.



That results in a base image that is around 300MB, 150MB less than what we already won in the previous batch of improvements. And it's not just about size (that is good for the planet): less runtime software means less chances of security issues.
Checklist
Note: no new files introduced, neither source code changed. If there are manual image validation steps, let me know... I've built things locally, already ldd'ed most of the assets in the resulting image /usr/local, and I didn't find anything linked to non existing libs. And if you want me to update some docs or CHANGELOG, just let me know!
I have:

[] added the LICENSE template to new files
[] compiled & tested this code
[] included documentation (including possible behaviour changes)","Split docker in two different jobs in #506., which should probably be integrated before this PR to make sure we run docker image generation cc @paololucente",True,{}
pmacct/pmacct,https://github.com/pmacct/pmacct,504,2021-05-13T12:19:00Z,2021-05-26T15:14:24Z,2021-05-28T00:13:21Z,MERGED,True,56,55,2,https://github.com/dcaba,"Base docker build using multistage, which makes runtime more efficient and secure",5,[],https://github.com/pmacct/pmacct/pull/504,https://github.com/dcaba,11,https://github.com/pmacct/pmacct/pull/504#issuecomment-848556996,"Short description
This PR introduces more build related improvements (so it continues the work we started here: #503):

we have incorporated an stage in the CI logic / actions that builds in parallel the base image we use for dockerhub, so if we break something in that specific build flavor, we are not going to detect this when merging to master, that is when this specific docker flow is used to publish images to dockerhub.
in the ci/deps.sh we stop using ""&&"" to concatenate installation commands, as one failing in the middle was not breaking the build process. We also enabled parallelism when building these deps, to improve performance
And major changes in the base Dockerfile:

We move to multistage, so we don't need to clean build related deps and we just explicitly declare the ones that are required at runtime
In the build stage, we stop installing everything in a single layer, to improve cacheability, and that does not affect to the final/runtime stage anymore
With the definition of the previous introduction of .dockerignore, we no longer need to call maintainer-clean, as files not under version control are excluded.



That results in a base image that is around 300MB, 150MB less than what we already won in the previous batch of improvements. And it's not just about size (that is good for the planet): less runtime software means less chances of security issues.
Checklist
Note: no new files introduced, neither source code changed. If there are manual image validation steps, let me know... I've built things locally, already ldd'ed most of the assets in the resulting image /usr/local, and I didn't find anything linked to non existing libs. And if you want me to update some docs or CHANGELOG, just let me know!
I have:

[] added the LICENSE template to new files
[] compiled & tested this code
[] included documentation (including possible behaviour changes)","OK. Please revert/remove commits that changes the CI, and let's stick to the multi-stage contribution here.
Please notify me when ready, and I will have a final (and manual) check for Dockers

Done, @msune !",True,{}
pmacct/pmacct,https://github.com/pmacct/pmacct,504,2021-05-13T12:19:00Z,2021-05-26T15:14:24Z,2021-05-28T00:13:21Z,MERGED,True,56,55,2,https://github.com/dcaba,"Base docker build using multistage, which makes runtime more efficient and secure",5,[],https://github.com/pmacct/pmacct/pull/504,https://github.com/msune,12,https://github.com/pmacct/pmacct/pull/504#issuecomment-848840041,"Short description
This PR introduces more build related improvements (so it continues the work we started here: #503):

we have incorporated an stage in the CI logic / actions that builds in parallel the base image we use for dockerhub, so if we break something in that specific build flavor, we are not going to detect this when merging to master, that is when this specific docker flow is used to publish images to dockerhub.
in the ci/deps.sh we stop using ""&&"" to concatenate installation commands, as one failing in the middle was not breaking the build process. We also enabled parallelism when building these deps, to improve performance
And major changes in the base Dockerfile:

We move to multistage, so we don't need to clean build related deps and we just explicitly declare the ones that are required at runtime
In the build stage, we stop installing everything in a single layer, to improve cacheability, and that does not affect to the final/runtime stage anymore
With the definition of the previous introduction of .dockerignore, we no longer need to call maintainer-clean, as files not under version control are excluded.



That results in a base image that is around 300MB, 150MB less than what we already won in the previous batch of improvements. And it's not just about size (that is good for the planet): less runtime software means less chances of security issues.
Checklist
Note: no new files introduced, neither source code changed. If there are manual image validation steps, let me know... I've built things locally, already ldd'ed most of the assets in the resulting image /usr/local, and I didn't find anything linked to non existing libs. And if you want me to update some docs or CHANGELOG, just let me know!
I have:

[] added the LICENSE template to new files
[] compiled & tested this code
[] included documentation (including possible behaviour changes)","Glanced through it. I won't retest, I trust @dcaba. ACK @paololucente",True,{'THUMBS_UP': ['https://github.com/dcaba']}
pmacct/pmacct,https://github.com/pmacct/pmacct,504,2021-05-13T12:19:00Z,2021-05-26T15:14:24Z,2021-05-28T00:13:21Z,MERGED,True,56,55,2,https://github.com/dcaba,"Base docker build using multistage, which makes runtime more efficient and secure",5,[],https://github.com/pmacct/pmacct/pull/504,https://github.com/dcaba,13,https://github.com/pmacct/pmacct/pull/504#issuecomment-848892991,"Short description
This PR introduces more build related improvements (so it continues the work we started here: #503):

we have incorporated an stage in the CI logic / actions that builds in parallel the base image we use for dockerhub, so if we break something in that specific build flavor, we are not going to detect this when merging to master, that is when this specific docker flow is used to publish images to dockerhub.
in the ci/deps.sh we stop using ""&&"" to concatenate installation commands, as one failing in the middle was not breaking the build process. We also enabled parallelism when building these deps, to improve performance
And major changes in the base Dockerfile:

We move to multistage, so we don't need to clean build related deps and we just explicitly declare the ones that are required at runtime
In the build stage, we stop installing everything in a single layer, to improve cacheability, and that does not affect to the final/runtime stage anymore
With the definition of the previous introduction of .dockerignore, we no longer need to call maintainer-clean, as files not under version control are excluded.



That results in a base image that is around 300MB, 150MB less than what we already won in the previous batch of improvements. And it's not just about size (that is good for the planet): less runtime software means less chances of security issues.
Checklist
Note: no new files introduced, neither source code changed. If there are manual image validation steps, let me know... I've built things locally, already ldd'ed most of the assets in the resulting image /usr/local, and I didn't find anything linked to non existing libs. And if you want me to update some docs or CHANGELOG, just let me know!
I have:

[] added the LICENSE template to new files
[] compiled & tested this code
[] included documentation (including possible behaviour changes)","@paololucente , do you plan to release a new version soon? Thanks to the improvements we have introduced, the resulting docker images are prettier.. see compressed size:

and number of sec findings:
 $ trivy --quiet pmacct/sfacctd:v1.7.6 | head

pmacct/sfacctd:v1.7.6 (debian 10.8)
===================================
Total: 406 (UNKNOWN: 0, LOW: 193, MEDIUM: 93, HIGH: 103, CRITICAL: 17)

$ trivy --quiet pmacct/sfacctd:bleeding-edge | head

pmacct/sfacctd:bleeding-edge (debian 10.9)
==========================================
Total: 137 (UNKNOWN: 0, LOW: 95, MEDIUM: 15, HIGH: 25, CRITICAL: 2)
So despite there may not be major code updates, I think the community can benefit from this.",True,{}
pmacct/pmacct,https://github.com/pmacct/pmacct,504,2021-05-13T12:19:00Z,2021-05-26T15:14:24Z,2021-05-28T00:13:21Z,MERGED,True,56,55,2,https://github.com/dcaba,"Base docker build using multistage, which makes runtime more efficient and secure",5,[],https://github.com/pmacct/pmacct/pull/504,https://github.com/paololucente,14,https://github.com/pmacct/pmacct/pull/504#issuecomment-849070848,"Short description
This PR introduces more build related improvements (so it continues the work we started here: #503):

we have incorporated an stage in the CI logic / actions that builds in parallel the base image we use for dockerhub, so if we break something in that specific build flavor, we are not going to detect this when merging to master, that is when this specific docker flow is used to publish images to dockerhub.
in the ci/deps.sh we stop using ""&&"" to concatenate installation commands, as one failing in the middle was not breaking the build process. We also enabled parallelism when building these deps, to improve performance
And major changes in the base Dockerfile:

We move to multistage, so we don't need to clean build related deps and we just explicitly declare the ones that are required at runtime
In the build stage, we stop installing everything in a single layer, to improve cacheability, and that does not affect to the final/runtime stage anymore
With the definition of the previous introduction of .dockerignore, we no longer need to call maintainer-clean, as files not under version control are excluded.



That results in a base image that is around 300MB, 150MB less than what we already won in the previous batch of improvements. And it's not just about size (that is good for the planet): less runtime software means less chances of security issues.
Checklist
Note: no new files introduced, neither source code changed. If there are manual image validation steps, let me know... I've built things locally, already ldd'ed most of the assets in the resulting image /usr/local, and I didn't find anything linked to non existing libs. And if you want me to update some docs or CHANGELOG, just let me know!
I have:

[] added the LICENSE template to new files
[] compiled & tested this code
[] included documentation (including possible behaviour changes)","Hi Dani ( @dcaba ),
I am wrapping up one important feature before code freeze. It will maybe take a week or so to be completely done. So sometime in July (first half / mid) seems realistic for the new release.
Let me take this chance to repeat how grateful i am for your contributions, thanks very much.
Paolo",True,{}
pmacct/pmacct,https://github.com/pmacct/pmacct,504,2021-05-13T12:19:00Z,2021-05-26T15:14:24Z,2021-05-28T00:13:21Z,MERGED,True,56,55,2,https://github.com/dcaba,"Base docker build using multistage, which makes runtime more efficient and secure",5,[],https://github.com/pmacct/pmacct/pull/504,https://github.com/dcaba,15,https://github.com/pmacct/pmacct/pull/504#issuecomment-849422619,"Short description
This PR introduces more build related improvements (so it continues the work we started here: #503):

we have incorporated an stage in the CI logic / actions that builds in parallel the base image we use for dockerhub, so if we break something in that specific build flavor, we are not going to detect this when merging to master, that is when this specific docker flow is used to publish images to dockerhub.
in the ci/deps.sh we stop using ""&&"" to concatenate installation commands, as one failing in the middle was not breaking the build process. We also enabled parallelism when building these deps, to improve performance
And major changes in the base Dockerfile:

We move to multistage, so we don't need to clean build related deps and we just explicitly declare the ones that are required at runtime
In the build stage, we stop installing everything in a single layer, to improve cacheability, and that does not affect to the final/runtime stage anymore
With the definition of the previous introduction of .dockerignore, we no longer need to call maintainer-clean, as files not under version control are excluded.



That results in a base image that is around 300MB, 150MB less than what we already won in the previous batch of improvements. And it's not just about size (that is good for the planet): less runtime software means less chances of security issues.
Checklist
Note: no new files introduced, neither source code changed. If there are manual image validation steps, let me know... I've built things locally, already ldd'ed most of the assets in the resulting image /usr/local, and I didn't find anything linked to non existing libs. And if you want me to update some docs or CHANGELOG, just let me know!
I have:

[] added the LICENSE template to new files
[] compiled & tested this code
[] included documentation (including possible behaviour changes)","Hi Dani ( @dcaba ),
I am wrapping up one important feature before code freeze. It will maybe take a week or so to be completely done. So sometime in July (first half / mid) seems realistic for the new release.
Let me take this chance to repeat how grateful i am for your contributions, thanks very much.
Paolo

Got it, @paololucente , thanks for sharing these plans!
And thanks also for maintaining this project! A pleasure to help and discuss with @msune : Marc, if you ever need some fresh eyes to review some build-related stuff in this repo, feel free to mention or assign me the review role.",True,{}
pmacct/pmacct,https://github.com/pmacct/pmacct,504,2021-05-13T12:19:00Z,2021-05-26T15:14:24Z,2021-05-28T00:13:21Z,MERGED,True,56,55,2,https://github.com/dcaba,"Base docker build using multistage, which makes runtime more efficient and secure",5,[],https://github.com/pmacct/pmacct/pull/504,https://github.com/msune,16,https://github.com/pmacct/pmacct/pull/504#issuecomment-850022880,"Short description
This PR introduces more build related improvements (so it continues the work we started here: #503):

we have incorporated an stage in the CI logic / actions that builds in parallel the base image we use for dockerhub, so if we break something in that specific build flavor, we are not going to detect this when merging to master, that is when this specific docker flow is used to publish images to dockerhub.
in the ci/deps.sh we stop using ""&&"" to concatenate installation commands, as one failing in the middle was not breaking the build process. We also enabled parallelism when building these deps, to improve performance
And major changes in the base Dockerfile:

We move to multistage, so we don't need to clean build related deps and we just explicitly declare the ones that are required at runtime
In the build stage, we stop installing everything in a single layer, to improve cacheability, and that does not affect to the final/runtime stage anymore
With the definition of the previous introduction of .dockerignore, we no longer need to call maintainer-clean, as files not under version control are excluded.



That results in a base image that is around 300MB, 150MB less than what we already won in the previous batch of improvements. And it's not just about size (that is good for the planet): less runtime software means less chances of security issues.
Checklist
Note: no new files introduced, neither source code changed. If there are manual image validation steps, let me know... I've built things locally, already ldd'ed most of the assets in the resulting image /usr/local, and I didn't find anything linked to non existing libs. And if you want me to update some docs or CHANGELOG, just let me know!
I have:

[] added the LICENSE template to new files
[] compiled & tested this code
[] included documentation (including possible behaviour changes)","Daniel, likewise. I will sure put you in the loop in future contribs

Thanks!
â€¦
On Thu, 27 May 2021, 09:57 Daniel Caballero, ***@***.***> wrote:
 Hi Dani ( @dcaba <https://github.com/dcaba> ),

 I am wrapping up one important feature before code freeze. It will maybe
 take a week or so to be completely done. So sometime in July (first half /
 mid) seems realistic for the new release.

 Let me take this chance to repeat how grateful i am for your
 contributions, thanks very much.

 Paolo

 Got it, @paololucente <https://github.com/paololucente> , thanks for
 sharing these plans!

 And thanks also for maintaining this project! A pleasure to help and
 discuss with @msune <https://github.com/msune> : Marc, if you ever need
 some fresh eyes to review some build-related stuff in this repo, feel free
 to mention or assign me the review role.

 â€”
 You are receiving this because you were mentioned.
 Reply to this email directly, view it on GitHub
 <#504 (comment)>, or
 unsubscribe
 <https://github.com/notifications/unsubscribe-auth/AAOLI6T3P54ACCZKZOZMBZDTPX3PFANCNFSM442OROZQ>
 .",True,{}
pmacct/pmacct,https://github.com/pmacct/pmacct,506,2021-05-25T22:14:59Z,2021-05-26T15:14:45Z,2021-05-26T15:14:45Z,MERGED,True,45,8,1,https://github.com/msune,ci: split docker and dockerhub upload jobs,2,[],https://github.com/pmacct/pmacct/pull/506,https://github.com/msune,1,https://github.com/pmacct/pmacct/pull/506,"Short description
Prior to this commit both the generation of docker images and
uploading them to dockerhub was done in the same job. This meant
that docker generation was skipped for all PRs.
In order to solve this, this commit splits this in two sequential
jobs, one to compile the docker images (and exporting the registry
as artifact) and one publishing them to dockerhub (having imported
the registry artifact). The former will also run for PRs.
Checklist
I have:

 added the LICENSE template to new files
 compiled & tested this code
 included documentation (including possible behaviour changes)","Short description
Prior to this commit both the generation of docker images and
uploading them to dockerhub was done in the same job. This meant
that docker generation was skipped for all PRs.
In order to solve this, this commit splits this in two sequential
jobs, one to compile the docker images (and exporting the registry
as artifact) and one publishing them to dockerhub (having imported
the registry artifact). The former will also run for PRs.
Checklist
I have:

 added the LICENSE template to new files
 compiled & tested this code
 included documentation (including possible behaviour changes)",True,{}
pmacct/pmacct,https://github.com/pmacct/pmacct,506,2021-05-25T22:14:59Z,2021-05-26T15:14:45Z,2021-05-26T15:14:45Z,MERGED,True,45,8,1,https://github.com/msune,ci: split docker and dockerhub upload jobs,2,[],https://github.com/pmacct/pmacct/pull/506,https://github.com/msune,2,https://github.com/pmacct/pmacct/pull/506#issuecomment-848801151,"Short description
Prior to this commit both the generation of docker images and
uploading them to dockerhub was done in the same job. This meant
that docker generation was skipped for all PRs.
In order to solve this, this commit splits this in two sequential
jobs, one to compile the docker images (and exporting the registry
as artifact) and one publishing them to dockerhub (having imported
the registry artifact). The former will also run for PRs.
Checklist
I have:

 added the LICENSE template to new files
 compiled & tested this code
 included documentation (including possible behaviour changes)","(@paololucente Please don't merge, parallel tests in progress in the same branch - only master works if I want to test pipeline e2e)",True,{}
pmacct/pmacct,https://github.com/pmacct/pmacct,506,2021-05-25T22:14:59Z,2021-05-26T15:14:45Z,2021-05-26T15:14:45Z,MERGED,True,45,8,1,https://github.com/msune,ci: split docker and dockerhub upload jobs,2,[],https://github.com/pmacct/pmacct/pull/506,https://github.com/msune,3,https://github.com/pmacct/pmacct/pull/506#issuecomment-848834508,"Short description
Prior to this commit both the generation of docker images and
uploading them to dockerhub was done in the same job. This meant
that docker generation was skipped for all PRs.
In order to solve this, this commit splits this in two sequential
jobs, one to compile the docker images (and exporting the registry
as artifact) and one publishing them to dockerhub (having imported
the registry artifact). The former will also run for PRs.
Checklist
I have:

 added the LICENSE template to new files
 compiled & tested this code
 included documentation (including possible behaviour changes)","@dcaba I've pushed a commit on top with your suggestion, slightly modified. I made the dockerhub job depend on both the pmacct-docker and the build-and-test, so that we never publish with bleeding-edge tag any build that didn't pass the tests (none, beyond compiling at the moment). Saves around 9minutes ðŸ‘
@paololucente ready to merge",True,{'THUMBS_UP': ['https://github.com/dcaba']}
pmacct/pmacct,https://github.com/pmacct/pmacct,518,2021-07-30T07:02:43Z,2021-08-02T22:52:00Z,2021-08-02T22:52:01Z,MERGED,True,1,1,1,https://github.com/olk,pmacct: use correct IP option for jumbo payload packets,1,[],https://github.com/pmacct/pmacct/pull/518,https://github.com/olk,1,https://github.com/pmacct/pmacct/pull/518,"According to RFC 2292 'Advanced Sockets API for IPv6' datagrams
with more than 65535 bytes payload require the jumbo payload option,
e.g the length of the payload ist stored in the hop-by-hop header.
In order to exclude jumbo payload packets we have to check if the
IP6 header payload length is zero and the next header is a
hop-by-hop header.
Short description

Checklist

I have:

 added the LICENSE template to new files
 compiled & tested this code
 included documentation (including possible behaviour changes)","According to RFC 2292 'Advanced Sockets API for IPv6' datagrams
with more than 65535 bytes payload require the jumbo payload option,
e.g the length of the payload ist stored in the hop-by-hop header.
In order to exclude jumbo payload packets we have to check if the
IP6 header payload length is zero and the next header is a
hop-by-hop header.
Short description

Checklist

I have:

 added the LICENSE template to new files
 compiled & tested this code
 included documentation (including possible behaviour changes)",True,{}
pmacct/pmacct,https://github.com/pmacct/pmacct,522,2021-08-06T19:28:12Z,2021-08-07T17:54:24Z,2021-08-19T22:37:40Z,MERGED,True,3,3,1,https://github.com/job,Add contextual out-of-bounds checks for maxLength,1,[],https://github.com/pmacct/pmacct/pull/522,https://github.com/job,1,https://github.com/pmacct/pmacct/pull/522,"Short description
The RTR PDU element for maxLength is uint8, therefor it might be possible to transmit incorrect RTR data. pmacct should be aware of this situation
Checklist

 compiled
 tested this code","Short description
The RTR PDU element for maxLength is uint8, therefor it might be possible to transmit incorrect RTR data. pmacct should be aware of this situation
Checklist

 compiled
 tested this code",True,{}
pmacct/pmacct,https://github.com/pmacct/pmacct,529,2021-09-14T15:35:51Z,2021-10-29T02:50:20Z,2021-10-29T02:50:20Z,MERGED,True,40,0,9,https://github.com/singularsyntax,Implement aggregate_unknown_etype config key and functionality.,1,[],https://github.com/pmacct/pmacct/pull/529,https://github.com/singularsyntax,1,https://github.com/pmacct/pmacct/pull/529,"Hello Paolo,
Per our discussion in email, I've made changes to support aggregation/counting of unknown EtherType(s) by pmacctd.
Short description
Added aggregate_unknown_etype boolean config key, which defaults to false, and which if enabled makes Ethernet frames with unsupported EtherTypes ""visible"" to the pmacctd aggregation engine.
Checklist
I have:

[N/A] added the LICENSE template to new files
[Y] compiled & tested this code
[Y] included documentation (including possible behaviour changes)","Hello Paolo,
Per our discussion in email, I've made changes to support aggregation/counting of unknown EtherType(s) by pmacctd.
Short description
Added aggregate_unknown_etype boolean config key, which defaults to false, and which if enabled makes Ethernet frames with unsupported EtherTypes ""visible"" to the pmacctd aggregation engine.
Checklist
I have:

[N/A] added the LICENSE template to new files
[Y] compiled & tested this code
[Y] included documentation (including possible behaviour changes)",True,{}
pmacct/pmacct,https://github.com/pmacct/pmacct,533,2021-09-28T09:24:24Z,2022-04-01T20:38:38Z,2022-04-01T20:38:38Z,CLOSED,False,60,27,2,https://github.com/rbarazzutti,Docker build,6,[],https://github.com/pmacct/pmacct/pull/533,https://github.com/rbarazzutti,1,https://github.com/pmacct/pmacct/pull/533,"Short description
Refactoring of Dockerfile/build scripts:

in order to speed up the build (re-order of the operations)
3rd party libraries dependencies pointing to specific versions instead of the latest version.
some minor refactoring

Checklist

I have:

 added the LICENSE template to new files
 compiled & tested this code
 included documentation (including possible behaviour changes)","Short description
Refactoring of Dockerfile/build scripts:

in order to speed up the build (re-order of the operations)
3rd party libraries dependencies pointing to specific versions instead of the latest version.
some minor refactoring

Checklist

I have:

 added the LICENSE template to new files
 compiled & tested this code
 included documentation (including possible behaviour changes)",True,{}
pmacct/pmacct,https://github.com/pmacct/pmacct,533,2021-09-28T09:24:24Z,2022-04-01T20:38:38Z,2022-04-01T20:38:38Z,CLOSED,False,60,27,2,https://github.com/rbarazzutti,Docker build,6,[],https://github.com/pmacct/pmacct/pull/533,https://github.com/msune,2,https://github.com/pmacct/pmacct/pull/533#issuecomment-930066597,"Short description
Refactoring of Dockerfile/build scripts:

in order to speed up the build (re-order of the operations)
3rd party libraries dependencies pointing to specific versions instead of the latest version.
some minor refactoring

Checklist

I have:

 added the LICENSE template to new files
 compiled & tested this code
 included documentation (including possible behaviour changes)","@rbarazzutti First of all thank you for this PR.
On fixing or clamping the version of the libraries... absolutely! I didn't realise some were not checking out a particular tag or commit, sigh. It has been pure luck the HEAD of their main branch did not change or break APIs, or introduce regressions. Sort of related but tangential topic, you can read here: #503 (comment) what are my thoughts on how dependecies could be handled in pmacct, avoiding this problem in general.
On the gh_ functions, seems like a good idea. I will do some minor comments in the code.
On the reordering of the copying of /usr/local/, I am not sure what's the rationale there.. My gut feeling would think that this is slower, not faster, because there are two extra RUN statements (ldconfig and removing apt.lists). I will try it locally see if I understand that change better (cc @dcaba)",True,{}
pmacct/pmacct,https://github.com/pmacct/pmacct,533,2021-09-28T09:24:24Z,2022-04-01T20:38:38Z,2022-04-01T20:38:38Z,CLOSED,False,60,27,2,https://github.com/rbarazzutti,Docker build,6,[],https://github.com/pmacct/pmacct/pull/533,https://github.com/rbarazzutti,3,https://github.com/pmacct/pmacct/pull/533#issuecomment-932826182,"Short description
Refactoring of Dockerfile/build scripts:

in order to speed up the build (re-order of the operations)
3rd party libraries dependencies pointing to specific versions instead of the latest version.
some minor refactoring

Checklist

I have:

 added the LICENSE template to new files
 compiled & tested this code
 included documentation (including possible behaviour changes)","Thanks so much, Paolo and Marc for your reviews.
Marc, about speed, this change makes the build quicker in case of a subsequent build after a change in the source code of pmacct.
The following build command of docker/base/Dockerfile doesn't need to run again when a change is done in the code source of pmacct.
# Runtime deps
RUN apt-get update && \
  apt-get install -y \
    libmariadb3 \
    libnuma1 \
    libcurl4 \
    libpcap0.8 \
    libpq5 \
    libsnappy1v5 \
    libsqlite3-0 \
    libssl1.1 && \
  apt-get -y clean && \
  rm -rf /var/lib/apt/lists/* && \
  ldconfig

It saves some build time and some storage space since builds share one more common layer.
And bonus, we are not querying Debian's servers at each build.
In my local CI-CD pipeline, it reduced the build time by 16% (from 376 to 326 secs) and also reduced the size of the changes between two images by 5%.
Cheers",True,{}
pmacct/pmacct,https://github.com/pmacct/pmacct,533,2021-09-28T09:24:24Z,2022-04-01T20:38:38Z,2022-04-01T20:38:38Z,CLOSED,False,60,27,2,https://github.com/rbarazzutti,Docker build,6,[],https://github.com/pmacct/pmacct/pull/533,https://github.com/msune,4,https://github.com/pmacct/pmacct/pull/533#issuecomment-933013174,"Short description
Refactoring of Dockerfile/build scripts:

in order to speed up the build (re-order of the operations)
3rd party libraries dependencies pointing to specific versions instead of the latest version.
some minor refactoring

Checklist

I have:

 added the LICENSE template to new files
 compiled & tested this code
 included documentation (including possible behaviour changes)","@rbarazzutti Thank you for your explanation. I think I understand now the point, and makes sense.
I tried to do some measurements myself, but I believe my results are not relevant, as I guess the variance is higher than this 5%, because the first build with the patch is 11s slower, which doesn't make sense to me... (should be very similar).
In any case, the cached build seems to be consistently a bit faster. Mind though, pmacct's CI will not benefit from this, as the CI itself runs in a docker container, so there is no reusable cache (we would have to use an external artifact to reuse layers across jobs/builds).
ACK from my side on this specific part of the review.
Details on the measurements.
For the record, I tried doing docker system prune to remove all cache layers etc.. before every First build.
Without patch
Size: 303
First build:
Successfully built 84e306b48084

real	7m0.551s
user	0m0.792s
sys	0m0.549s

Second build:
Successfully built f005664e979b

real	2m16.780s
user	0m0.541s
sys	0m0.329s

With patch
Size: 303
First build:
Successfully built 7ec66521c50b

real	7m11.957s
user	0m0.709s
sys	0m0.585s

Second build:
Successfully built 1f0739a26572

real	2m4.311s
user	0m0.432s
sys	0m0.350s",True,{}
pmacct/pmacct,https://github.com/pmacct/pmacct,533,2021-09-28T09:24:24Z,2022-04-01T20:38:38Z,2022-04-01T20:38:38Z,CLOSED,False,60,27,2,https://github.com/rbarazzutti,Docker build,6,[],https://github.com/pmacct/pmacct/pull/533,https://github.com/dcaba,5,https://github.com/pmacct/pmacct/pull/533#issuecomment-933412896,"Short description
Refactoring of Dockerfile/build scripts:

in order to speed up the build (re-order of the operations)
3rd party libraries dependencies pointing to specific versions instead of the latest version.
some minor refactoring

Checklist

I have:

 added the LICENSE template to new files
 compiled & tested this code
 included documentation (including possible behaviour changes)","e patch is 11s slower

This may be explained by the fact that the new Dockerfile ships an extra layer with the image (as there is one extra RUN command, the final ldconfig; in the original implementation, we were able to concatenate that to the deps installation step, given the old ordering). But I think the proposal makes sense; its ok to prioritize caching of that deps-installation step.",True,{}
pmacct/pmacct,https://github.com/pmacct/pmacct,533,2021-09-28T09:24:24Z,2022-04-01T20:38:38Z,2022-04-01T20:38:38Z,CLOSED,False,60,27,2,https://github.com/rbarazzutti,Docker build,6,[],https://github.com/pmacct/pmacct/pull/533,https://github.com/msune,6,https://github.com/pmacct/pmacct/pull/533#issuecomment-994116365,"Short description
Refactoring of Dockerfile/build scripts:

in order to speed up the build (re-order of the operations)
3rd party libraries dependencies pointing to specific versions instead of the latest version.
some minor refactoring

Checklist

I have:

 added the LICENSE template to new files
 compiled & tested this code
 included documentation (including possible behaviour changes)","@dcaba

But I think the proposal makes sense; its ok to prioritize caching of that deps-installation step.

I hope it's not too late... I think I disagree. Very few people - to my understanding - builds docker images for pmacct. CI does not currently cache layers, all containers are built from scratch.
If layers would be cached somehow (and that requires a separate discussion and it's more work), it's still more important IMO the overall size of the container than the pipeline speed, if I have to choose. It is both speed to download and boot pmacct containers and BW required, if you have to do it frequently on new nodes.",True,{}
pmacct/pmacct,https://github.com/pmacct/pmacct,536,2021-10-08T14:51:15Z,2021-10-15T03:04:32Z,2021-10-15T03:04:32Z,MERGED,True,50,2,5,https://github.com/arlake228,Add preprocess support for maxp and maxb in non-SQL plugins,1,[],https://github.com/pmacct/pmacct/pull/536,https://github.com/arlake228,1,https://github.com/pmacct/pmacct/pull/536,"Short description
The <plugin>_preprocess configuration directive supports a number of options for filtering aggregates. Filtering on the maximum number of packets (maxp) and maximum number of bytes (maxb) is currently only supported for SQL plugins. This pull request adds support to non-SQL plugins. It has been tested with nfacctd and Kafka, but I believe should work with other plugins just as the min equivalents already do.
Our specific use case for this was our network wants to be able to send ""small"" flows to a different Kafka topic (and ultimately a different processing pipeline) than larger flows. The existing min options got us part of the way there by allowing us to only forward the larger flows to kafka, but the max options allows us to forward the small flows using a second kafka plugin configuration (as opposed to dropping them or forwarding all traffic the separating at a later step in the pipeline).
Checklist

I have:

 added the LICENSE template to new files
 compiled & tested this code
 included documentation (including possible behaviour changes)","Short description
The <plugin>_preprocess configuration directive supports a number of options for filtering aggregates. Filtering on the maximum number of packets (maxp) and maximum number of bytes (maxb) is currently only supported for SQL plugins. This pull request adds support to non-SQL plugins. It has been tested with nfacctd and Kafka, but I believe should work with other plugins just as the min equivalents already do.
Our specific use case for this was our network wants to be able to send ""small"" flows to a different Kafka topic (and ultimately a different processing pipeline) than larger flows. The existing min options got us part of the way there by allowing us to only forward the larger flows to kafka, but the max options allows us to forward the small flows using a second kafka plugin configuration (as opposed to dropping them or forwarding all traffic the separating at a later step in the pipeline).
Checklist

I have:

 added the LICENSE template to new files
 compiled & tested this code
 included documentation (including possible behaviour changes)",True,{}
pmacct/pmacct,https://github.com/pmacct/pmacct,541,2021-10-22T14:59:38Z,2021-10-23T00:44:37Z,2021-10-23T10:55:23Z,MERGED,True,34,0,7,https://github.com/WRMSRwasTaken,Add VRF listening support for the BGP daemon,1,[],https://github.com/pmacct/pmacct/pull/541,https://github.com/WRMSRwasTaken,1,https://github.com/pmacct/pmacct/pull/541,"Short description
This commit adds the new option called bgp_daemon_interface. This option allows user to specify an interface for the BGP daemon to bind to using SO_BINDTODEVICE. This can be used for example to place the BGP listening into a VRF by specifying the VRF master device name in that option, as documented in the Linux kernel VRF documentation and somewhat explained in #538 which enables to set up a BGP (fulltable) peering in one VRF alone without having to leak any routes.
Checklist

I have:

 added the LICENSE template to new files
 compiled & tested this code
 included documentation (including possible behaviour changes)","Short description
This commit adds the new option called bgp_daemon_interface. This option allows user to specify an interface for the BGP daemon to bind to using SO_BINDTODEVICE. This can be used for example to place the BGP listening into a VRF by specifying the VRF master device name in that option, as documented in the Linux kernel VRF documentation and somewhat explained in #538 which enables to set up a BGP (fulltable) peering in one VRF alone without having to leak any routes.
Checklist

I have:

 added the LICENSE template to new files
 compiled & tested this code
 included documentation (including possible behaviour changes)",True,{}
pmacct/pmacct,https://github.com/pmacct/pmacct,541,2021-10-22T14:59:38Z,2021-10-23T00:44:37Z,2021-10-23T10:55:23Z,MERGED,True,34,0,7,https://github.com/WRMSRwasTaken,Add VRF listening support for the BGP daemon,1,[],https://github.com/pmacct/pmacct/pull/541,https://github.com/WRMSRwasTaken,2,https://github.com/pmacct/pmacct/pull/541#issuecomment-949778854,"Short description
This commit adds the new option called bgp_daemon_interface. This option allows user to specify an interface for the BGP daemon to bind to using SO_BINDTODEVICE. This can be used for example to place the BGP listening into a VRF by specifying the VRF master device name in that option, as documented in the Linux kernel VRF documentation and somewhat explained in #538 which enables to set up a BGP (fulltable) peering in one VRF alone without having to leak any routes.
Checklist

I have:

 added the LICENSE template to new files
 compiled & tested this code
 included documentation (including possible behaviour changes)","Hi Paolo,
actually both can be used at the same time, they are not mutually exclusive. You actually need to set both when you want to bind to a certain IP in a certain VRF. :-)
A lot of software allows specifying both options at the same time, for example HAProxy: bind 2a0f:85c1:beef:2021::443:443 interface vrf-as207781 v6only
... or dnsdist: addLocal(""2a0f:85c1:beef:2021::53"", { interface=""vrf-as207781-l"" })
I see your point though, because the docs mention an ""interface"" for the ""ip"" config option which might confuse some users...
My config to test this is:
pmacctd_as: bgp
bgp_daemon: true
bgp_daemon_ip: 2a0f:85c1:beef:1012::1
bgp_agent_map: /etc/pmacct/bgp_agent.map
bgp_daemon_port: 17917
!bgp_peer_as_skip_subas: true
bgp_peer_src_as_type: bgp
bgp_daemon_interface: vrf-as207781
bgp_daemon_ipv6_only: true

Which results in the listening socket [2a0f:85c1:beef:1012::1]%vrf-as207781:17917. Without bgp_daemon_ip being set above, it'd be result in [::]%vrf-as207781:17917 because vrf-as207781 in my case is the ""VRF master device"" / ""l3mdev"".
Just specifying bgp_daemon_interface will bind it to the interface, but still listen on ::. I'd have to look how other software handles this, if it's even allowed to just specify an interface and what the results will be...
Error-ing when both are set would reduce the flexibility in my opinion. When a user enters an invalid combination of both, for example an interface directly in a VRF but an IP not being on that interface with nonlocal bind turned off in the kernel, bind() will fail anyways with cannot assign requested address (error number 99), so the kernel already takes care of this. HAProxy for example doesn't do any own checks for invalid combinations for both aswell (as much as I can see it) but also just relies on the return codes for setsockopt() and bind().",True,{}
pmacct/pmacct,https://github.com/pmacct/pmacct,541,2021-10-22T14:59:38Z,2021-10-23T00:44:37Z,2021-10-23T10:55:23Z,MERGED,True,34,0,7,https://github.com/WRMSRwasTaken,Add VRF listening support for the BGP daemon,1,[],https://github.com/pmacct/pmacct/pull/541,https://github.com/paololucente,3,https://github.com/pmacct/pmacct/pull/541#issuecomment-949792202,"Short description
This commit adds the new option called bgp_daemon_interface. This option allows user to specify an interface for the BGP daemon to bind to using SO_BINDTODEVICE. This can be used for example to place the BGP listening into a VRF by specifying the VRF master device name in that option, as documented in the Linux kernel VRF documentation and somewhat explained in #538 which enables to set up a BGP (fulltable) peering in one VRF alone without having to leak any routes.
Checklist

I have:

 added the LICENSE template to new files
 compiled & tested this code
 included documentation (including possible behaviour changes)","Cool, perfect & makes sense! Paolo",True,{}
pmacct/pmacct,https://github.com/pmacct/pmacct,543,2021-10-28T14:26:13Z,2021-11-01T21:58:20Z,2022-01-10T09:47:36Z,MERGED,True,203,5,6,https://github.com/scuzzilla,MAP Label data structure for both JSON & AVRO encodings ,60,[],https://github.com/pmacct/pmacct/pull/543,https://github.com/scuzzilla,1,https://github.com/pmacct/pmacct/pull/543,"Summary of the new functionalities
PMACCT's pre-tagging function can be used to enrich the data stream:
In the use case below when the IP address <ip> is matched a new label is generated & added to the data stream.
The final label is going to be a string composed by the N fields specified within the <set_label> statements.

Comma ("","") is currently used as a default field separator between the two (or more) lables: ""<key1-value1>,<key2-value2>, ... ,
<keyN-valueN>"".
Each sub-label is composed of two fields (key and value) disposed in a strict order, first key second label,  and
separated by a delimiter (example ""-""): ""<key-value>""

set_label=<key1-value1,key2-value2,...,keyN-valueN>   ip=<specific_ip_1>
set_label=<key1-value1,key2-value2,...,keyN-valueN>   ip=<specific_ip_2>
...
set_label=<key1-value1,key2-value2,...,keyN-valueN>   ip=<specific_ip_N>
!
!default label   
set_label=<key1-value1,key2-value2,...,keyN-valueN>

More about PMACCT's pre-tagging can be found here.
A configuration flag is required ot enable the new implementation:
pre_tag_label_encode_as_map: true

Current JSON output
The way pre-tagging is currently developed doesn't easily allow to load the ""multi-labels"" string in a
database without a sort of post-processing job in charge of manipulating the string itself (like splitting the string
into two sub-strings).
Here below is an abstraction of the current data-stream JSON output.
{
  ""label"": ""key1-value1,key2-value2,...,keyN-valueN"",
  ""other1"": ""other1"",
  ""other2"": ""other2"",
  ""other3"": ""other3""
}
New JSON output
The main goal of extending the PMACCT's pre-tagging functionality is to avoid any kind of post-processing job on the
produced ""multi-labels"" string.
Here below is an abstraction of the future data-stream JSON ouput.
{
  ""label"": {
    ""key1"": ""value1"",
    ""key2"": ""value2""
  },
  ""other1"": ""other1"",
  ""other2"": ""other2"",
  ""other3"": ""other3""
}
Current AVRO schema
Here below is reported the current AVRO schema used to define the label field (type string).
{                                                                                                                                                                                                                                                                                                                             
  ""type"": ""record"",                                                                                                                                                                                                                                                                                                           
  ""name"": ""acct_data"",                                                                                                                                                                                                                                                                                                        
  ""fields"": [                                                                                                                                                                                                                                                                                                                 
    {                                                                                                                                                                                                                                                                                                                         
      ""name"": ""label"",                                                                                                                                                                                                                                                                                                        
      ""type"": {                                                                                                                                                                                                                                                                                                               
        ""type"": ""string""                                                                                                                                                                                                                                                                                                      
      }                                                                                                                                                                                                                                                                                                                       
    }
  ]
}
New AVRO schema
Here below is reported the future AVRO schema which will be used to define the new label field (type map fo strings).
{
  ""type"": ""record"",
  ""name"": ""acct_data"",
  ""fields"": [
    {
      ""name"": ""label"",
      ""type"": {
        ""type"": ""map"",
        ""values"": {
          ""type"": ""string""
        }
      }
    }
  ]
}
Checklist
I have:

 added the LICENSE template to new files
 compiled & tested this code
 included documentation (including possible behavior changes)","Summary of the new functionalities
PMACCT's pre-tagging function can be used to enrich the data stream:
In the use case below when the IP address <ip> is matched a new label is generated & added to the data stream.
The final label is going to be a string composed by the N fields specified within the <set_label> statements.

Comma ("","") is currently used as a default field separator between the two (or more) lables: ""<key1-value1>,<key2-value2>, ... ,
<keyN-valueN>"".
Each sub-label is composed of two fields (key and value) disposed in a strict order, first key second label,  and
separated by a delimiter (example ""-""): ""<key-value>""

set_label=<key1-value1,key2-value2,...,keyN-valueN>   ip=<specific_ip_1>
set_label=<key1-value1,key2-value2,...,keyN-valueN>   ip=<specific_ip_2>
...
set_label=<key1-value1,key2-value2,...,keyN-valueN>   ip=<specific_ip_N>
!
!default label   
set_label=<key1-value1,key2-value2,...,keyN-valueN>

More about PMACCT's pre-tagging can be found here.
A configuration flag is required ot enable the new implementation:
pre_tag_label_encode_as_map: true

Current JSON output
The way pre-tagging is currently developed doesn't easily allow to load the ""multi-labels"" string in a
database without a sort of post-processing job in charge of manipulating the string itself (like splitting the string
into two sub-strings).
Here below is an abstraction of the current data-stream JSON output.
{
  ""label"": ""key1-value1,key2-value2,...,keyN-valueN"",
  ""other1"": ""other1"",
  ""other2"": ""other2"",
  ""other3"": ""other3""
}
New JSON output
The main goal of extending the PMACCT's pre-tagging functionality is to avoid any kind of post-processing job on the
produced ""multi-labels"" string.
Here below is an abstraction of the future data-stream JSON ouput.
{
  ""label"": {
    ""key1"": ""value1"",
    ""key2"": ""value2""
  },
  ""other1"": ""other1"",
  ""other2"": ""other2"",
  ""other3"": ""other3""
}
Current AVRO schema
Here below is reported the current AVRO schema used to define the label field (type string).
{                                                                                                                                                                                                                                                                                                                             
  ""type"": ""record"",                                                                                                                                                                                                                                                                                                           
  ""name"": ""acct_data"",                                                                                                                                                                                                                                                                                                        
  ""fields"": [                                                                                                                                                                                                                                                                                                                 
    {                                                                                                                                                                                                                                                                                                                         
      ""name"": ""label"",                                                                                                                                                                                                                                                                                                        
      ""type"": {                                                                                                                                                                                                                                                                                                               
        ""type"": ""string""                                                                                                                                                                                                                                                                                                      
      }                                                                                                                                                                                                                                                                                                                       
    }
  ]
}
New AVRO schema
Here below is reported the future AVRO schema which will be used to define the new label field (type map fo strings).
{
  ""type"": ""record"",
  ""name"": ""acct_data"",
  ""fields"": [
    {
      ""name"": ""label"",
      ""type"": {
        ""type"": ""map"",
        ""values"": {
          ""type"": ""string""
        }
      }
    }
  ]
}
Checklist
I have:

 added the LICENSE template to new files
 compiled & tested this code
 included documentation (including possible behavior changes)",True,{}
pmacct/pmacct,https://github.com/pmacct/pmacct,547,2021-11-19T16:15:33Z,2021-11-30T13:19:27Z,2021-11-30T13:19:28Z,CLOSED,False,317,281,30,https://github.com/scuzzilla,ARRAY TCP-Flags data structure for both JSON & AVRO encodings,44,[],https://github.com/pmacct/pmacct/pull/547,https://github.com/scuzzilla,1,https://github.com/pmacct/pmacct/pull/547,"Summary of the new functionalities
The current implementation of PMACCT is encoding the six TCP's Control bits into decimal format. It's the responsibility
of an external entity to decode them into the human-readable format  URG, ACK, PSH, RST, SYN, FIN.
The main aim of the new implementation is to embed the decoding action within PMACCT.
The TCP-Flags can be, for example, serialized to KAFKA (via the kafka-plugin) using either JSON or AVRO. The new implementation
is taking care of loading the active TCP-Flags into an array before proceeding with the actual serialization.
Main 'Decoding' Steps (Per TCP Flow):

PMACCT is returning the decimal representation of the TCP's Control bits
An intermediate step is translating from decimal to binary
Finally, only the active TCP-Flags are loaded into an array data structure ready to be serialized

4 ---> [0, 0, 0, 1, 0, 0] ---> ['RST']                                                                                                                                                                                                                                                                                        

JSON output

Current

{                                                                                                                                                              
  ""tcp_flags"": ""decimal_value"",                                                                                                                                               
  ""other1"": ""other1"",
  ""other2"": ""other2"",
  ""other3"": ""other3""
} 

Future

{                                                                                                                                                              
  ""tcp_flags"": [                                                                                                                                             
    ""URG"",                                                                                                                                                  
    ""ACK"",                                                                                                                                                  
    ""PSH"",
    ""RST"",                                                                                                                                                     
    ""SYN"",                                                                                                                                                     
    ""FIN""                                                                                                                                                     
  ],                                                                                                                                                            
  ""other1"": ""other1"",
  ""other2"": ""other2"",
  ""other3"": ""other3""
} 
AVRO schema

Current

{                                                                                                                                                                                                                                                                                                                             
  ""type"": ""record"",                                                                                                                                                                                                                                                                                                           
  ""name"": ""acct_data"",                                                                                                                                                                                                                                                                                                        
  ""fields"": [                                                                                                                                                                                                                                                                                                                 
    {                                                                                                                                                                                                                                                                                                                         
      ""name"": ""tcp_flags"",                                                                                                                                                                                                                                                                                                        
      ""type"": {                                                                                                                                                                                                                                                                                                               
        ""type"": ""string""                                                                                                                                                                                                                                                                                                      
      }                                                                                                                                                                                                                                                                                                                       
    }
  ]
}

Future

{                                                                                                                                                              
  ""type"": ""record"",                                                                                                                                                                                                                                 
  ""name"": ""acct_data"",                                                                                                                                        
  ""fields"": [                                                                                                                                                 
    {                                                                                                                                                          
      ""name"": ""tcp_flags"",                                                                                                                                     
      ""type"": {                                                                                                                                               
        ""type"": ""array"",                                                                                                                                       
        ""items"": {                                                                                                                                             
          ""type"": ""string""                                                                                                                                     
        }                                                                                                                                                      
      }                                                                                                                                                        
    }                                                                                                                                                          
  ] 
}       
Checklist
I have:

 added the LICENSE template to new files
 compiled & tested this code
 included documentation (including possible behaviour changes)","Summary of the new functionalities
The current implementation of PMACCT is encoding the six TCP's Control bits into decimal format. It's the responsibility
of an external entity to decode them into the human-readable format  URG, ACK, PSH, RST, SYN, FIN.
The main aim of the new implementation is to embed the decoding action within PMACCT.
The TCP-Flags can be, for example, serialized to KAFKA (via the kafka-plugin) using either JSON or AVRO. The new implementation
is taking care of loading the active TCP-Flags into an array before proceeding with the actual serialization.
Main 'Decoding' Steps (Per TCP Flow):

PMACCT is returning the decimal representation of the TCP's Control bits
An intermediate step is translating from decimal to binary
Finally, only the active TCP-Flags are loaded into an array data structure ready to be serialized

4 ---> [0, 0, 0, 1, 0, 0] ---> ['RST']                                                                                                                                                                                                                                                                                        

JSON output

Current

{                                                                                                                                                              
  ""tcp_flags"": ""decimal_value"",                                                                                                                                               
  ""other1"": ""other1"",
  ""other2"": ""other2"",
  ""other3"": ""other3""
} 

Future

{                                                                                                                                                              
  ""tcp_flags"": [                                                                                                                                             
    ""URG"",                                                                                                                                                  
    ""ACK"",                                                                                                                                                  
    ""PSH"",
    ""RST"",                                                                                                                                                     
    ""SYN"",                                                                                                                                                     
    ""FIN""                                                                                                                                                     
  ],                                                                                                                                                            
  ""other1"": ""other1"",
  ""other2"": ""other2"",
  ""other3"": ""other3""
} 
AVRO schema

Current

{                                                                                                                                                                                                                                                                                                                             
  ""type"": ""record"",                                                                                                                                                                                                                                                                                                           
  ""name"": ""acct_data"",                                                                                                                                                                                                                                                                                                        
  ""fields"": [                                                                                                                                                                                                                                                                                                                 
    {                                                                                                                                                                                                                                                                                                                         
      ""name"": ""tcp_flags"",                                                                                                                                                                                                                                                                                                        
      ""type"": {                                                                                                                                                                                                                                                                                                               
        ""type"": ""string""                                                                                                                                                                                                                                                                                                      
      }                                                                                                                                                                                                                                                                                                                       
    }
  ]
}

Future

{                                                                                                                                                              
  ""type"": ""record"",                                                                                                                                                                                                                                 
  ""name"": ""acct_data"",                                                                                                                                        
  ""fields"": [                                                                                                                                                 
    {                                                                                                                                                          
      ""name"": ""tcp_flags"",                                                                                                                                     
      ""type"": {                                                                                                                                               
        ""type"": ""array"",                                                                                                                                       
        ""items"": {                                                                                                                                             
          ""type"": ""string""                                                                                                                                     
        }                                                                                                                                                      
      }                                                                                                                                                        
    }                                                                                                                                                          
  ] 
}       
Checklist
I have:

 added the LICENSE template to new files
 compiled & tested this code
 included documentation (including possible behaviour changes)",True,{}
pmacct/pmacct,https://github.com/pmacct/pmacct,547,2021-11-19T16:15:33Z,2021-11-30T13:19:27Z,2021-11-30T13:19:28Z,CLOSED,False,317,281,30,https://github.com/scuzzilla,ARRAY TCP-Flags data structure for both JSON & AVRO encodings,44,[],https://github.com/pmacct/pmacct/pull/547,https://github.com/scuzzilla,2,https://github.com/pmacct/pmacct/pull/547#issuecomment-982629120,"Summary of the new functionalities
The current implementation of PMACCT is encoding the six TCP's Control bits into decimal format. It's the responsibility
of an external entity to decode them into the human-readable format  URG, ACK, PSH, RST, SYN, FIN.
The main aim of the new implementation is to embed the decoding action within PMACCT.
The TCP-Flags can be, for example, serialized to KAFKA (via the kafka-plugin) using either JSON or AVRO. The new implementation
is taking care of loading the active TCP-Flags into an array before proceeding with the actual serialization.
Main 'Decoding' Steps (Per TCP Flow):

PMACCT is returning the decimal representation of the TCP's Control bits
An intermediate step is translating from decimal to binary
Finally, only the active TCP-Flags are loaded into an array data structure ready to be serialized

4 ---> [0, 0, 0, 1, 0, 0] ---> ['RST']                                                                                                                                                                                                                                                                                        

JSON output

Current

{                                                                                                                                                              
  ""tcp_flags"": ""decimal_value"",                                                                                                                                               
  ""other1"": ""other1"",
  ""other2"": ""other2"",
  ""other3"": ""other3""
} 

Future

{                                                                                                                                                              
  ""tcp_flags"": [                                                                                                                                             
    ""URG"",                                                                                                                                                  
    ""ACK"",                                                                                                                                                  
    ""PSH"",
    ""RST"",                                                                                                                                                     
    ""SYN"",                                                                                                                                                     
    ""FIN""                                                                                                                                                     
  ],                                                                                                                                                            
  ""other1"": ""other1"",
  ""other2"": ""other2"",
  ""other3"": ""other3""
} 
AVRO schema

Current

{                                                                                                                                                                                                                                                                                                                             
  ""type"": ""record"",                                                                                                                                                                                                                                                                                                           
  ""name"": ""acct_data"",                                                                                                                                                                                                                                                                                                        
  ""fields"": [                                                                                                                                                                                                                                                                                                                 
    {                                                                                                                                                                                                                                                                                                                         
      ""name"": ""tcp_flags"",                                                                                                                                                                                                                                                                                                        
      ""type"": {                                                                                                                                                                                                                                                                                                               
        ""type"": ""string""                                                                                                                                                                                                                                                                                                      
      }                                                                                                                                                                                                                                                                                                                       
    }
  ]
}

Future

{                                                                                                                                                              
  ""type"": ""record"",                                                                                                                                                                                                                                 
  ""name"": ""acct_data"",                                                                                                                                        
  ""fields"": [                                                                                                                                                 
    {                                                                                                                                                          
      ""name"": ""tcp_flags"",                                                                                                                                     
      ""type"": {                                                                                                                                               
        ""type"": ""array"",                                                                                                                                       
        ""items"": {                                                                                                                                             
          ""type"": ""string""                                                                                                                                     
        }                                                                                                                                                      
      }                                                                                                                                                        
    }                                                                                                                                                          
  ] 
}       
Checklist
I have:

 added the LICENSE template to new files
 compiled & tested this code
 included documentation (including possible behaviour changes)",trying to solve the conflicts ...,True,{}
pmacct/pmacct,https://github.com/pmacct/pmacct,550,2021-11-30T13:45:23Z,2021-12-03T04:03:16Z,2022-01-10T09:48:32Z,MERGED,True,191,17,10,https://github.com/scuzzilla,ARRAY TCP-Flags data structure for both JSON & AVRO encodings,5,[],https://github.com/pmacct/pmacct/pull/550,https://github.com/scuzzilla,1,https://github.com/pmacct/pmacct/pull/550,"Summary of the new functionalities
The current implementation of PMACCT is encoding the six TCP's Control bits into decimal format. It's the responsibility
of an external entity to decode them into the human-readable format  URG, ACK, PSH, RST, SYN, FIN.
The main aim of the new implementation is to embed the decoding action within PMACCT.
The TCP-Flags can be, for example, serialized to KAFKA (via the kafka-plugin) using either JSON or AVRO. The new implementation
is taking care of loading the active TCP-Flags into an array before proceeding with the actual serialization.
Main 'Decoding' Steps (Per TCP Flow):

PMACCT is returning the decimal representation of the TCP's Control bits
An intermediate step is translating from decimal to binary
Finally, only the active TCP-Flags are loaded into an array data structure ready to be serialized

4 ---> [0, 0, 0, 1, 0, 0] ---> ['RST']                                                                                                                                                                                                                                                                                        

A configuration flag is required ot enable the new implementation:
tcpflags_encode_as_array: true

JSON output

Current

{                                                                                                                                                              
  ""tcp_flags"": ""decimal_value"",                                                                                                                                               
  ""other1"": ""other1"",
  ""other2"": ""other2"",
  ""other3"": ""other3""
} 

Future

{                                                                                                                                                              
  ""tcp_flags"": [                                                                                                                                             
    ""URG"",                                                                                                                                                  
    ""ACK"",                                                                                                                                                  
    ""PSH"",
    ""RST"",                                                                                                                                                     
    ""SYN"",                                                                                                                                                     
    ""FIN""                                                                                                                                                     
  ],                                                                                                                                                            
  ""other1"": ""other1"",
  ""other2"": ""other2"",
  ""other3"": ""other3""
} 
AVRO schema

Current

{                                                                                                                                                                                                                                                                                                                             
  ""type"": ""record"",                                                                                                                                                                                                                                                                                                           
  ""name"": ""acct_data"",                                                                                                                                                                                                                                                                                                        
  ""fields"": [                                                                                                                                                                                                                                                                                                                 
    {                                                                                                                                                                                                                                                                                                                         
      ""name"": ""tcp_flags"",                                                                                                                                                                                                                                                                                                        
      ""type"": {                                                                                                                                                                                                                                                                                                               
        ""type"": ""string""                                                                                                                                                                                                                                                                                                      
      }                                                                                                                                                                                                                                                                                                                       
    }
  ]
}

Future

{                                                                                                                                                              
  ""type"": ""record"",                                                                                                                                                                                                                                 
  ""name"": ""acct_data"",                                                                                                                                        
  ""fields"": [                                                                                                                                                 
    {                                                                                                                                                          
      ""name"": ""tcp_flags"",                                                                                                                                     
      ""type"": {                                                                                                                                               
        ""type"": ""array"",                                                                                                                                       
        ""items"": {                                                                                                                                             
          ""type"": ""string""                                                                                                                                     
        }                                                                                                                                                      
      }                                                                                                                                                        
    }                                                                                                                                                          
  ] 
}       
Checklist
I have:

 added the LICENSE template to new files
 compiled & tested this code
 included documentation (including possible behavior changes)","Summary of the new functionalities
The current implementation of PMACCT is encoding the six TCP's Control bits into decimal format. It's the responsibility
of an external entity to decode them into the human-readable format  URG, ACK, PSH, RST, SYN, FIN.
The main aim of the new implementation is to embed the decoding action within PMACCT.
The TCP-Flags can be, for example, serialized to KAFKA (via the kafka-plugin) using either JSON or AVRO. The new implementation
is taking care of loading the active TCP-Flags into an array before proceeding with the actual serialization.
Main 'Decoding' Steps (Per TCP Flow):

PMACCT is returning the decimal representation of the TCP's Control bits
An intermediate step is translating from decimal to binary
Finally, only the active TCP-Flags are loaded into an array data structure ready to be serialized

4 ---> [0, 0, 0, 1, 0, 0] ---> ['RST']                                                                                                                                                                                                                                                                                        

A configuration flag is required ot enable the new implementation:
tcpflags_encode_as_array: true

JSON output

Current

{                                                                                                                                                              
  ""tcp_flags"": ""decimal_value"",                                                                                                                                               
  ""other1"": ""other1"",
  ""other2"": ""other2"",
  ""other3"": ""other3""
} 

Future

{                                                                                                                                                              
  ""tcp_flags"": [                                                                                                                                             
    ""URG"",                                                                                                                                                  
    ""ACK"",                                                                                                                                                  
    ""PSH"",
    ""RST"",                                                                                                                                                     
    ""SYN"",                                                                                                                                                     
    ""FIN""                                                                                                                                                     
  ],                                                                                                                                                            
  ""other1"": ""other1"",
  ""other2"": ""other2"",
  ""other3"": ""other3""
} 
AVRO schema

Current

{                                                                                                                                                                                                                                                                                                                             
  ""type"": ""record"",                                                                                                                                                                                                                                                                                                           
  ""name"": ""acct_data"",                                                                                                                                                                                                                                                                                                        
  ""fields"": [                                                                                                                                                                                                                                                                                                                 
    {                                                                                                                                                                                                                                                                                                                         
      ""name"": ""tcp_flags"",                                                                                                                                                                                                                                                                                                        
      ""type"": {                                                                                                                                                                                                                                                                                                               
        ""type"": ""string""                                                                                                                                                                                                                                                                                                      
      }                                                                                                                                                                                                                                                                                                                       
    }
  ]
}

Future

{                                                                                                                                                              
  ""type"": ""record"",                                                                                                                                                                                                                                 
  ""name"": ""acct_data"",                                                                                                                                        
  ""fields"": [                                                                                                                                                 
    {                                                                                                                                                          
      ""name"": ""tcp_flags"",                                                                                                                                     
      ""type"": {                                                                                                                                               
        ""type"": ""array"",                                                                                                                                       
        ""items"": {                                                                                                                                             
          ""type"": ""string""                                                                                                                                     
        }                                                                                                                                                      
      }                                                                                                                                                        
    }                                                                                                                                                          
  ] 
}       
Checklist
I have:

 added the LICENSE template to new files
 compiled & tested this code
 included documentation (including possible behavior changes)",True,{}
pmacct/pmacct,https://github.com/pmacct/pmacct,552,2021-11-30T19:14:47Z,2021-12-01T18:00:03Z,2021-12-01T18:00:04Z,MERGED,True,65,30,1,https://github.com/scuzzilla,Implementing AVRO Unions to handle null label's values for both BMP & BGP,1,[],https://github.com/pmacct/pmacct/pull/552,https://github.com/scuzzilla,1,https://github.com/pmacct/pmacct/pull/552,"Summary of the new functionalities
The previously developed functions, utilized to define the pretag-map's label field shaped as MAP, are now refactored to be able to extend the MAP label's field to both BMP & BGP protocols.
As you can see from the prototypes here below a new ""boolean parameter"" (int) has been added to be able to toggle the functions support between IPFIX & BMP/BGP:
/* prototypes */
extern void compose_label_avro_schema(avro_schema_t, int);
extern int compose_label_avro_data(char *, avro_value_t, int);
the boolean set to FALSE is used to activate the right behavior for IPFIX while TRUE is the right value for BMP/BGP.
the main difference between the two behaviors is the way the label's null values are handled:

for what concerning BMP/BGP, ""avro unions"" are used to define the map's default value to null (This is necessary to cover the BMP/BGP messages with null label information associated).
for what concerning IPFIX, the default value is not necessary since the label field is either with or without value but never set to null.

Checklist
I have:

 added the LICENSE template to new files
 compiled & tested this code
 included documentation (including possible behavior changes)","Summary of the new functionalities
The previously developed functions, utilized to define the pretag-map's label field shaped as MAP, are now refactored to be able to extend the MAP label's field to both BMP & BGP protocols.
As you can see from the prototypes here below a new ""boolean parameter"" (int) has been added to be able to toggle the functions support between IPFIX & BMP/BGP:
/* prototypes */
extern void compose_label_avro_schema(avro_schema_t, int);
extern int compose_label_avro_data(char *, avro_value_t, int);
the boolean set to FALSE is used to activate the right behavior for IPFIX while TRUE is the right value for BMP/BGP.
the main difference between the two behaviors is the way the label's null values are handled:

for what concerning BMP/BGP, ""avro unions"" are used to define the map's default value to null (This is necessary to cover the BMP/BGP messages with null label information associated).
for what concerning IPFIX, the default value is not necessary since the label field is either with or without value but never set to null.

Checklist
I have:

 added the LICENSE template to new files
 compiled & tested this code
 included documentation (including possible behavior changes)",True,{}
pmacct/pmacct,https://github.com/pmacct/pmacct,558,2022-01-10T08:31:00Z,2022-01-18T14:52:38Z,2022-01-18T19:33:20Z,CLOSED,False,413,132,20,https://github.com/scuzzilla,IPFIX forwardingStatus data structure for both JSON & AVRO encodings,145,[],https://github.com/pmacct/pmacct/pull/558,https://github.com/scuzzilla,1,https://github.com/pmacct/pmacct/pull/558,"Summary
The current implementation of PMACCT is encoding the ""IPFIX forwardingStatus"" field into decimal format.
It's the job of an external entity to map it into the human-readable format like described by RFC-7270 Section 4.12.
At the moment the ""IPFIX forwardingStatus"" is handled by defining an ah-hoc custom-defined primitive.
The main goals of the new implementation are:

Embedding the mapping action within PMACCT (decimal-to-humanReadable).
Hanndling the ""IPFIX forwardingStatus"" field like a standard primitive (avoid custom-defined primitive).

The ""IPFIX forwardingStatus"" field can be, for example, serialized to KAFKA (via the kafka-plugin) using either JSON or AVRO.
The new implementation is taking care of pairing the decimal format with the associated human-readable string before serializing.
A configuration flag is required ot enable the new implementation:
nf9_fwdstatus_encode_as_string: true

Main 'Mapping' Steps:

PMACCT is returning the decimal representation of the ""IPFIX forwardingStatus"" field
An intermediate step is translating from the decimal to the related human-readable string
Finally, the string is ready to be serialized (for example to kafka, JSON or AVRO supported)

  /* RFC-7270: forwardingStatus with a compliant reason code */
  const unsigned int nf9_fwdstatus_decimal[23] = {
    64, 65, 66,
    128, 129, 130,
    131, 132, 133,
    134, 135, 136,
    137, 138, 139,
    140, 141, 142,
    143, 192, 193,
    194, 195
  };

  const char nf9_fwdstatus_description[23][50] = {
    ""FORWARDED Unknown"",
    ""FORWARDED Fragmented"",
    ""FORWARDED Not Fragmented"",
    ""DROPPED Unknown"",
    ""DROPPED ACL deny"",
    ""DROPPED ACL drop"",
    ""DROPPED Unroutable"",
    ""DROPPED Adjacency"",
    ""DROPPED Fragmentation and DF set"",
    ""DROPPED Bad header checksum"",
    ""DROPPED Bad total Length"",
    ""DROPPED Bad header length"",
    ""DROPPED bad TTL"",
    ""DROPPED Policer"",
    ""DROPPED WRED"",
    ""DROPPED RPF"",
    ""DROPPED For us"",
    ""DROPPED Bad output interface"",
    ""DROPPED Hardware"",
    ""CONSUMED Unknown"",
    ""CONSUMED Punt Adjacency"",
    ""CONSUMED Incomplete Adjacency"",
    ""CONSUMED For us"",
  };

JSON output

Current

{                                                                                                                                                              
  ""forwarding_status"": ""decimal_value_string"",                                                                                                                                               
  ""other1"": ""other1"",
  ""other2"": ""other2"",
  ""other3"": ""other3""
} 

Future

{                                                                                                                                                              
  ""forwarding_status"": ""human_readable_string"",                                                                                                                                              
  ""other1"": ""other1"",
  ""other2"": ""other2"",
  ""other3"": ""other3""
} 
AVRO schema

Current

/* the decimal is converted to string directly and then serialized*/
{                                                                                                                                                                                                    
  ""type"": ""record"",

  ""name"": ""acct_data"",                                                                                                             
  ""fields"": [                                                                                         
    {                                                                                                                                                        
      ""name"": ""forwarding_status"",                                                              
      ""type"": {                                                                                                                                                                         
        ""type"": ""string""                                                                                                            
      }                                                                                                                           
    }
  ]
}

Future

/* the decimal is first mapped to the human-readable format and then serialized*/
{                                                                                                                                                                                                    
  ""type"": ""record"",

  ""name"": ""acct_data"",                                                                                                             
  ""fields"": [                                                                                         
    {                                                                                                                                                        
      ""name"": ""forwarding_status"",                                                              
      ""type"": {                                                                                                                                                                         
        ""type"": ""string""                                                                                                            
      }                                                                                                                           
    }
  ]
}
Checklist
I have:

 added the LICENSE template to new files
 compiled & tested this code
 included documentation (including possible behavior changes)","Summary
The current implementation of PMACCT is encoding the ""IPFIX forwardingStatus"" field into decimal format.
It's the job of an external entity to map it into the human-readable format like described by RFC-7270 Section 4.12.
At the moment the ""IPFIX forwardingStatus"" is handled by defining an ah-hoc custom-defined primitive.
The main goals of the new implementation are:

Embedding the mapping action within PMACCT (decimal-to-humanReadable).
Hanndling the ""IPFIX forwardingStatus"" field like a standard primitive (avoid custom-defined primitive).

The ""IPFIX forwardingStatus"" field can be, for example, serialized to KAFKA (via the kafka-plugin) using either JSON or AVRO.
The new implementation is taking care of pairing the decimal format with the associated human-readable string before serializing.
A configuration flag is required ot enable the new implementation:
nf9_fwdstatus_encode_as_string: true

Main 'Mapping' Steps:

PMACCT is returning the decimal representation of the ""IPFIX forwardingStatus"" field
An intermediate step is translating from the decimal to the related human-readable string
Finally, the string is ready to be serialized (for example to kafka, JSON or AVRO supported)

  /* RFC-7270: forwardingStatus with a compliant reason code */
  const unsigned int nf9_fwdstatus_decimal[23] = {
    64, 65, 66,
    128, 129, 130,
    131, 132, 133,
    134, 135, 136,
    137, 138, 139,
    140, 141, 142,
    143, 192, 193,
    194, 195
  };

  const char nf9_fwdstatus_description[23][50] = {
    ""FORWARDED Unknown"",
    ""FORWARDED Fragmented"",
    ""FORWARDED Not Fragmented"",
    ""DROPPED Unknown"",
    ""DROPPED ACL deny"",
    ""DROPPED ACL drop"",
    ""DROPPED Unroutable"",
    ""DROPPED Adjacency"",
    ""DROPPED Fragmentation and DF set"",
    ""DROPPED Bad header checksum"",
    ""DROPPED Bad total Length"",
    ""DROPPED Bad header length"",
    ""DROPPED bad TTL"",
    ""DROPPED Policer"",
    ""DROPPED WRED"",
    ""DROPPED RPF"",
    ""DROPPED For us"",
    ""DROPPED Bad output interface"",
    ""DROPPED Hardware"",
    ""CONSUMED Unknown"",
    ""CONSUMED Punt Adjacency"",
    ""CONSUMED Incomplete Adjacency"",
    ""CONSUMED For us"",
  };

JSON output

Current

{                                                                                                                                                              
  ""forwarding_status"": ""decimal_value_string"",                                                                                                                                               
  ""other1"": ""other1"",
  ""other2"": ""other2"",
  ""other3"": ""other3""
} 

Future

{                                                                                                                                                              
  ""forwarding_status"": ""human_readable_string"",                                                                                                                                              
  ""other1"": ""other1"",
  ""other2"": ""other2"",
  ""other3"": ""other3""
} 
AVRO schema

Current

/* the decimal is converted to string directly and then serialized*/
{                                                                                                                                                                                                    
  ""type"": ""record"",

  ""name"": ""acct_data"",                                                                                                             
  ""fields"": [                                                                                         
    {                                                                                                                                                        
      ""name"": ""forwarding_status"",                                                              
      ""type"": {                                                                                                                                                                         
        ""type"": ""string""                                                                                                            
      }                                                                                                                           
    }
  ]
}

Future

/* the decimal is first mapped to the human-readable format and then serialized*/
{                                                                                                                                                                                                    
  ""type"": ""record"",

  ""name"": ""acct_data"",                                                                                                             
  ""fields"": [                                                                                         
    {                                                                                                                                                        
      ""name"": ""forwarding_status"",                                                              
      ""type"": {                                                                                                                                                                         
        ""type"": ""string""                                                                                                            
      }                                                                                                                           
    }
  ]
}
Checklist
I have:

 added the LICENSE template to new files
 compiled & tested this code
 included documentation (including possible behavior changes)",True,{}
pmacct/pmacct,https://github.com/pmacct/pmacct,560,2022-01-13T12:14:24Z,2022-01-13T17:06:19Z,2022-01-13T17:06:19Z,CLOSED,False,2,2,1,https://github.com/drushadrusha,MySQLv8 problem with DATETIME types,2,[],https://github.com/pmacct/pmacct/pull/560,https://github.com/drushadrusha,1,https://github.com/pmacct/pmacct/pull/560,"Short description

When I'm trying to run sfacctd with mysql plugin it throw errors like:
Incorrect datetime value: '1642073401' for column 'stamp_updated' at row 1
MySQL datetime is not supposed to store unix timestamps.
Changed columns 'stamp_inserted' and 'stamp_updated' to bigint and now it works.
In other way, we can pass UNIX_TIMESTAMP() function in INSERT's and keep datetime types, but in my opinion timestamps more easy to deal with than mysql date types.
Checklist

I have:

 added the LICENSE template to new files
 compiled & tested this code
 included documentation (including possible behaviour changes)","Short description

When I'm trying to run sfacctd with mysql plugin it throw errors like:
Incorrect datetime value: '1642073401' for column 'stamp_updated' at row 1
MySQL datetime is not supposed to store unix timestamps.
Changed columns 'stamp_inserted' and 'stamp_updated' to bigint and now it works.
In other way, we can pass UNIX_TIMESTAMP() function in INSERT's and keep datetime types, but in my opinion timestamps more easy to deal with than mysql date types.
Checklist

I have:

 added the LICENSE template to new files
 compiled & tested this code
 included documentation (including possible behaviour changes)",True,{}
pmacct/pmacct,https://github.com/pmacct/pmacct,562,2022-01-18T15:10:39Z,2022-01-18T16:28:34Z,2022-01-18T16:28:34Z,MERGED,True,114,116,7,https://github.com/scuzzilla,fix the memleak created by label(MAP) + AVRO + BxP (msglog) ...,1,[],https://github.com/pmacct/pmacct/pull/562,https://github.com/scuzzilla,1,https://github.com/pmacct/pmacct/pull/562,"Changelog


The function ""avro_generic_value_new()"" has been deleted from each single sub-function related to label(MAP) | TCP-FLAGs | ... : the AVRO's interfaces & associated values (including their decref) are handled by the upper-level functions. (memory leak main issue).


The function in charge to serialize the LABEL(MAP) data to KAFKA is now divided into two separated functions: the first one for IPFIX while the second one for BxP (BGP/BMP)


The exact same approach as above is now adopted for the schema generation, there are now two separated functions: one for IPFIX & one for BxP.


The function in charge of generating the linked-list storing the label(MAP) information has been refactored replacing strdup() with strcpy().


Checklist
I have:

 added the LICENSE template to new files
 compiled & tested this code
 included documentation (including possible behavior changes)","Changelog


The function ""avro_generic_value_new()"" has been deleted from each single sub-function related to label(MAP) | TCP-FLAGs | ... : the AVRO's interfaces & associated values (including their decref) are handled by the upper-level functions. (memory leak main issue).


The function in charge to serialize the LABEL(MAP) data to KAFKA is now divided into two separated functions: the first one for IPFIX while the second one for BxP (BGP/BMP)


The exact same approach as above is now adopted for the schema generation, there are now two separated functions: one for IPFIX & one for BxP.


The function in charge of generating the linked-list storing the label(MAP) information has been refactored replacing strdup() with strcpy().


Checklist
I have:

 added the LICENSE template to new files
 compiled & tested this code
 included documentation (including possible behavior changes)",True,{}
pmacct/pmacct,https://github.com/pmacct/pmacct,563,2022-01-18T19:22:12Z,2022-01-19T14:47:53Z,2022-02-16T12:23:55Z,MERGED,True,293,10,16,https://github.com/scuzzilla,IPFIX forwardingStatus data structure for both JSON & AVRO encodings,3,[],https://github.com/pmacct/pmacct/pull/563,https://github.com/scuzzilla,1,https://github.com/pmacct/pmacct/pull/563,"Summary
The current implementation of PMACCT is encoding the ""IPFIX forwardingStatus"" field into decimal format.
It's the job of an external entity to map it into the human-readable format like described by RFC-7270 Section 4.12.
At the moment the ""IPFIX forwardingStatus"" is handled by defining an ah-hoc custom-defined primitive.
The main goals of the new implementation are:

Embedding the mapping action within PMACCT (decimal-to-humanReadable).
Handling the ""IPFIX forwardingStatus"" field like a standard primitive (avoid custom-defined primitive).

The ""IPFIX forwardingStatus"" field can be, for example, serialized to KAFKA (via the kafka-plugin) using either JSON or AVRO.
The new implementation is taking care of pairing the decimal format with the associated human-readable string before serializing.
A configuration flag is required to enable the new implementation:
fwd_status_encode_as_string: true

The new primitive is named:
fwd_status

Main 'Mapping' Steps:

PMACCT is returning the decimal representation of the ""IPFIX forwardingStatus"" field
An intermediate step is translating from the decimal to the related human-readable string
Finally, the string is ready to be serialized (for example to kafka, JSON or AVRO supported)

  /* RFC-7270: forwardingStatus with a compliant reason code */
  const unsigned int nf9_fwdstatus_decimal[23] = {
    64, 65, 66,
    128, 129, 130,
    131, 132, 133,
    134, 135, 136,
    137, 138, 139,
    140, 141, 142,
    143, 192, 193,
    194, 195
  };

  const char nf9_fwdstatus_description[23][50] = {
    ""FORWARDED Unknown"",
    ""FORWARDED Fragmented"",
    ""FORWARDED Not Fragmented"",
    ""DROPPED Unknown"",
    ""DROPPED ACL deny"",
    ""DROPPED ACL drop"",
    ""DROPPED Unroutable"",
    ""DROPPED Adjacency"",
    ""DROPPED Fragmentation and DF set"",
    ""DROPPED Bad header checksum"",
    ""DROPPED Bad total Length"",
    ""DROPPED Bad header length"",
    ""DROPPED bad TTL"",
    ""DROPPED Policer"",
    ""DROPPED WRED"",
    ""DROPPED RPF"",
    ""DROPPED For us"",
    ""DROPPED Bad output interface"",
    ""DROPPED Hardware"",
    ""CONSUMED Unknown"",
    ""CONSUMED Punt Adjacency"",
    ""CONSUMED Incomplete Adjacency"",
    ""CONSUMED For us"",
  };

JSON output

Current

{                                                                                                                                                              
  ""forwarding_status"": ""decimal_value_string"",                                                                                                                                               
  ""other1"": ""other1"",
  ""other2"": ""other2"",
  ""other3"": ""other3""
} 

Future

{                                                                                                                                                              
  ""forwarding_status"": ""human_readable_string"",                                                                                                                                              
  ""other1"": ""other1"",
  ""other2"": ""other2"",
  ""other3"": ""other3""
} 
AVRO schema

Current

/* the decimal is converted to string directly and then serialized*/
{                                                                                                                                                                                                    
  ""type"": ""record"",

  ""name"": ""acct_data"",                                                                                                             
  ""fields"": [                                                                                         
    {                                                                                                                                                        
      ""name"": ""forwarding_status"",                                                              
      ""type"": {                                                                                                                                                                         
        ""type"": ""string""                                                                                                            
      }                                                                                                                           
    }
  ]
}

Future

/* the decimal is first mapped to the human-readable format and then serialized*/
{                                                                                                                                                                                                    
  ""type"": ""record"",

  ""name"": ""acct_data"",                                                                                                             
  ""fields"": [                                                                                         
    {                                                                                                                                                        
      ""name"": ""forwarding_status"",                                                              
      ""type"": {                                                                                                                                                                         
        ""type"": ""string""                                                                                                            
      }                                                                                                                           
    }
  ]
}
Checklist
I have:

 added the LICENSE template to new files
 compiled & tested this code
 included documentation (including possible behavior changes)","Summary
The current implementation of PMACCT is encoding the ""IPFIX forwardingStatus"" field into decimal format.
It's the job of an external entity to map it into the human-readable format like described by RFC-7270 Section 4.12.
At the moment the ""IPFIX forwardingStatus"" is handled by defining an ah-hoc custom-defined primitive.
The main goals of the new implementation are:

Embedding the mapping action within PMACCT (decimal-to-humanReadable).
Handling the ""IPFIX forwardingStatus"" field like a standard primitive (avoid custom-defined primitive).

The ""IPFIX forwardingStatus"" field can be, for example, serialized to KAFKA (via the kafka-plugin) using either JSON or AVRO.
The new implementation is taking care of pairing the decimal format with the associated human-readable string before serializing.
A configuration flag is required to enable the new implementation:
fwd_status_encode_as_string: true

The new primitive is named:
fwd_status

Main 'Mapping' Steps:

PMACCT is returning the decimal representation of the ""IPFIX forwardingStatus"" field
An intermediate step is translating from the decimal to the related human-readable string
Finally, the string is ready to be serialized (for example to kafka, JSON or AVRO supported)

  /* RFC-7270: forwardingStatus with a compliant reason code */
  const unsigned int nf9_fwdstatus_decimal[23] = {
    64, 65, 66,
    128, 129, 130,
    131, 132, 133,
    134, 135, 136,
    137, 138, 139,
    140, 141, 142,
    143, 192, 193,
    194, 195
  };

  const char nf9_fwdstatus_description[23][50] = {
    ""FORWARDED Unknown"",
    ""FORWARDED Fragmented"",
    ""FORWARDED Not Fragmented"",
    ""DROPPED Unknown"",
    ""DROPPED ACL deny"",
    ""DROPPED ACL drop"",
    ""DROPPED Unroutable"",
    ""DROPPED Adjacency"",
    ""DROPPED Fragmentation and DF set"",
    ""DROPPED Bad header checksum"",
    ""DROPPED Bad total Length"",
    ""DROPPED Bad header length"",
    ""DROPPED bad TTL"",
    ""DROPPED Policer"",
    ""DROPPED WRED"",
    ""DROPPED RPF"",
    ""DROPPED For us"",
    ""DROPPED Bad output interface"",
    ""DROPPED Hardware"",
    ""CONSUMED Unknown"",
    ""CONSUMED Punt Adjacency"",
    ""CONSUMED Incomplete Adjacency"",
    ""CONSUMED For us"",
  };

JSON output

Current

{                                                                                                                                                              
  ""forwarding_status"": ""decimal_value_string"",                                                                                                                                               
  ""other1"": ""other1"",
  ""other2"": ""other2"",
  ""other3"": ""other3""
} 

Future

{                                                                                                                                                              
  ""forwarding_status"": ""human_readable_string"",                                                                                                                                              
  ""other1"": ""other1"",
  ""other2"": ""other2"",
  ""other3"": ""other3""
} 
AVRO schema

Current

/* the decimal is converted to string directly and then serialized*/
{                                                                                                                                                                                                    
  ""type"": ""record"",

  ""name"": ""acct_data"",                                                                                                             
  ""fields"": [                                                                                         
    {                                                                                                                                                        
      ""name"": ""forwarding_status"",                                                              
      ""type"": {                                                                                                                                                                         
        ""type"": ""string""                                                                                                            
      }                                                                                                                           
    }
  ]
}

Future

/* the decimal is first mapped to the human-readable format and then serialized*/
{                                                                                                                                                                                                    
  ""type"": ""record"",

  ""name"": ""acct_data"",                                                                                                             
  ""fields"": [                                                                                         
    {                                                                                                                                                        
      ""name"": ""forwarding_status"",                                                              
      ""type"": {                                                                                                                                                                         
        ""type"": ""string""                                                                                                            
      }                                                                                                                           
    }
  ]
}
Checklist
I have:

 added the LICENSE template to new files
 compiled & tested this code
 included documentation (including possible behavior changes)",True,{}
pmacct/pmacct,https://github.com/pmacct/pmacct,569,2022-02-09T21:34:53Z,2022-02-17T18:42:37Z,2022-02-17T18:42:37Z,MERGED,True,242,8,13,https://github.com/scuzzilla,IPFIX mplsLabelStack data structure for both JSON & AVRO encodings,11,[],https://github.com/pmacct/pmacct/pull/569,https://github.com/scuzzilla,1,https://github.com/pmacct/pmacct/pull/569,"Summary
The current implementation of PMACCT allows the configuration of two MPLS's label primitives:

mpls_label_top
mpls_label_bottom (Bottom-of-Stack bit set)

with them we are able to export information regarding both the fisrt & the last MPLS label on the stack.
The main goal of the new implementation are:

Handling the mplsLabelStack's IPFIX fields (from 70 to 75) like standard primitives (avoid custom-defined primitive).

Name used to enable the new primitive:

mpls_label_stack 


Giving to the operator an immediate feedback about the position of the label on the stack:

Example per IPFIX flow:

0-1200, 1-3411, 2-5622, 3-7833 ...

left to right all labels are marked with an index N- signifying (top to bottom) the position of the label on the stack
The ""mplsLabelStack's IPFIX"" fields can be, for example, serialized to KAFKA (via the kafka-plugin) using either JSON or AVRO.
A configuration flag is required ot enable the ARRAY format for the new implementation:
mpls_label_stack_encode_as_array: true

enabling the ""mpls_label_stack"" without enabling the ARRAY format will generate an unidexed coma-separated value string including all labels related to a specific IPFIX flow.

JSON output (ARRAY format)
{                                                                                                                                                              
  ""mpls_label_stack"": [                                                                                                                                             
    ""0-label0"",
    ""1-label1"",
    ""2-label2"",
    ""3-label3"",
    ""4-label4"",
    ""5-label5""                                                                                                                                        
  ],                                                                                                                                                            
  ""other1"": ""other1"",
  ""other2"": ""other2"",
  ""other3"": ""other3""
} 
AVRO schema (ARRAY format)
{                                                                                                                                                              
  ""type"": ""record"",                                                                                                                                                                                                                                 
  ""name"": ""acct_data"",                                                                                                                                        
  ""fields"": [                                                                                                                                                 
    {                                                                                                                                                          
      ""name"": ""mpls_label_stack"",                                                                                                                                     
      ""type"": {                                                                                                                                               
        ""type"": ""array"",                                                                                                                                       
        ""items"": {                                                                                                                                             
          ""type"": ""string""                                                                                                                                     
        }                                                                                                                                                      
      }                                                                                                                                                        
    }                                                                                                                                                          
  ] 
} 
Checklist
I have:

 added the LICENSE template to new files
 compiled & tested this code
 included documentation (including possible behavior changes)","Summary
The current implementation of PMACCT allows the configuration of two MPLS's label primitives:

mpls_label_top
mpls_label_bottom (Bottom-of-Stack bit set)

with them we are able to export information regarding both the fisrt & the last MPLS label on the stack.
The main goal of the new implementation are:

Handling the mplsLabelStack's IPFIX fields (from 70 to 75) like standard primitives (avoid custom-defined primitive).

Name used to enable the new primitive:

mpls_label_stack 


Giving to the operator an immediate feedback about the position of the label on the stack:

Example per IPFIX flow:

0-1200, 1-3411, 2-5622, 3-7833 ...

left to right all labels are marked with an index N- signifying (top to bottom) the position of the label on the stack
The ""mplsLabelStack's IPFIX"" fields can be, for example, serialized to KAFKA (via the kafka-plugin) using either JSON or AVRO.
A configuration flag is required ot enable the ARRAY format for the new implementation:
mpls_label_stack_encode_as_array: true

enabling the ""mpls_label_stack"" without enabling the ARRAY format will generate an unidexed coma-separated value string including all labels related to a specific IPFIX flow.

JSON output (ARRAY format)
{                                                                                                                                                              
  ""mpls_label_stack"": [                                                                                                                                             
    ""0-label0"",
    ""1-label1"",
    ""2-label2"",
    ""3-label3"",
    ""4-label4"",
    ""5-label5""                                                                                                                                        
  ],                                                                                                                                                            
  ""other1"": ""other1"",
  ""other2"": ""other2"",
  ""other3"": ""other3""
} 
AVRO schema (ARRAY format)
{                                                                                                                                                              
  ""type"": ""record"",                                                                                                                                                                                                                                 
  ""name"": ""acct_data"",                                                                                                                                        
  ""fields"": [                                                                                                                                                 
    {                                                                                                                                                          
      ""name"": ""mpls_label_stack"",                                                                                                                                     
      ""type"": {                                                                                                                                               
        ""type"": ""array"",                                                                                                                                       
        ""items"": {                                                                                                                                             
          ""type"": ""string""                                                                                                                                     
        }                                                                                                                                                      
      }                                                                                                                                                        
    }                                                                                                                                                          
  ] 
} 
Checklist
I have:

 added the LICENSE template to new files
 compiled & tested this code
 included documentation (including possible behavior changes)",True,{}
pmacct/pmacct,https://github.com/pmacct/pmacct,574,2022-02-17T14:17:23Z,2022-02-18T14:35:05Z,2022-02-18T14:35:05Z,MERGED,True,47,35,4,https://github.com/scuzzilla,Minor safety & code efficiency enhancements related to my recent plugin-common (AVRO & JSON) functions ,3,[],https://github.com/pmacct/pmacct/pull/574,https://github.com/scuzzilla,1,https://github.com/pmacct/pmacct/pull/574,"Summary
The main aim of this PR is to bring some improvements from both security & code efficiency perspective for what concerning the plugin-common (AVRO & JSON) functions I've recently developed:

memset to ""zeroize"" some variables before utilization
Indexes switched to ""size_t"" instead of ""int"" type
whenever possible switching to safe string functions (example strNcpy instead of strcpy)
whenever possible setting variables at pre-processor level, using the most appropriate header file, to improve code readability

Checklist
I have:

 added the LICENSE template to new files
 compiled & tested this code
 included documentation (including possible behavior changes)","Summary
The main aim of this PR is to bring some improvements from both security & code efficiency perspective for what concerning the plugin-common (AVRO & JSON) functions I've recently developed:

memset to ""zeroize"" some variables before utilization
Indexes switched to ""size_t"" instead of ""int"" type
whenever possible switching to safe string functions (example strNcpy instead of strcpy)
whenever possible setting variables at pre-processor level, using the most appropriate header file, to improve code readability

Checklist
I have:

 added the LICENSE template to new files
 compiled & tested this code
 included documentation (including possible behavior changes)",True,{}
pmacct/pmacct,https://github.com/pmacct/pmacct,575,2022-02-19T20:21:20Z,2022-02-19T21:44:16Z,2022-02-19T21:44:16Z,MERGED,True,22,13,3,https://github.com/scuzzilla,Fixing the way strncat's size value is calculated to generate the mpls-label-stack string/array ... ,6,[],https://github.com/pmacct/pmacct/pull/575,https://github.com/scuzzilla,1,https://github.com/pmacct/pmacct/pull/575,"Summary
This PR is adding on top of PR #574 & as described within the title is to fix the way strncat's size value is calculated to generate the mpls-label-stack string/array
Checklist
I have:

 added the LICENSE template to new files
 compiled & tested this code
 included documentation (including possible behavior changes)","Summary
This PR is adding on top of PR #574 & as described within the title is to fix the way strncat's size value is calculated to generate the mpls-label-stack string/array
Checklist
I have:

 added the LICENSE template to new files
 compiled & tested this code
 included documentation (including possible behavior changes)",True,{}
pmacct/pmacct,https://github.com/pmacct/pmacct,576,2022-02-21T19:51:45Z,2022-02-21T22:14:39Z,2022-02-21T22:14:39Z,MERGED,True,6,18,2,https://github.com/scuzzilla,Optimizing the memory usage for the mpls_label_stack's data-struct primitive ,2,[],https://github.com/pmacct/pmacct/pull/576,https://github.com/scuzzilla,1,https://github.com/pmacct/pmacct/pull/576,"Summary
This PR is adding on top of PR #574 & as described within the title is to optimize the memory usage with the mpls_label_stack's data-struct primitive
Checklist
I have:

 added the LICENSE template to new files
 compiled & tested this code
 included documentation (including possible behavior changes)","Summary
This PR is adding on top of PR #574 & as described within the title is to optimize the memory usage with the mpls_label_stack's data-struct primitive
Checklist
I have:

 added the LICENSE template to new files
 compiled & tested this code
 included documentation (including possible behavior changes)",True,{}
pmacct/pmacct,https://github.com/pmacct/pmacct,583,2022-03-02T15:32:58Z,2022-03-03T10:20:25Z,2022-03-03T10:20:25Z,MERGED,True,23,23,8,https://github.com/scuzzilla,"Renaming the variable ""labels_cycle"" to ""label_stack"" ",1,[],https://github.com/pmacct/pmacct/pull/583,https://github.com/scuzzilla,1,https://github.com/pmacct/pmacct/pull/583,"Summary
This PR is adding on top of PR #574 & as described within the title is to renaming the variable ""labels_cycle"" to the more appropriate name ""label_stack""
Checklist
I have:

 added the LICENSE template to new files
 compiled & tested this code
 included documentation (including possible behavior changes)","Summary
This PR is adding on top of PR #574 & as described within the title is to renaming the variable ""labels_cycle"" to the more appropriate name ""label_stack""
Checklist
I have:

 added the LICENSE template to new files
 compiled & tested this code
 included documentation (including possible behavior changes)",True,{}
pmacct/pmacct,https://github.com/pmacct/pmacct,588,2022-03-09T10:16:12Z,2022-03-10T01:56:29Z,2022-03-10T01:56:29Z,MERGED,True,52,16,1,https://github.com/rodecker,Add FRR documentation,1,[],https://github.com/pmacct/pmacct/pull/588,https://github.com/rodecker,1,https://github.com/pmacct/pmacct/pull/588,"Short description
Add documentation about peering with FRR for BGP info. Solves #549.
Checklist

I have:

[N/A] added the LICENSE template to new files
[N/A] compiled & tested this code
[V] included documentation (including possible behaviour changes)","Short description
Add documentation about peering with FRR for BGP info. Solves #549.
Checklist

I have:

[N/A] added the LICENSE template to new files
[N/A] compiled & tested this code
[V] included documentation (including possible behaviour changes)",True,{}
pmacct/pmacct,https://github.com/pmacct/pmacct,593,2022-04-03T06:37:08Z,2022-05-10T00:19:32Z,2022-05-10T00:19:32Z,CLOSED,False,5,0,2,https://github.com/JudeSafo,digital-copyright,1,[],https://github.com/pmacct/pmacct/pull/593,https://github.com/JudeSafo,1,https://github.com/pmacct/pmacct/pull/593,"Hey pmacct!ðŸ‘‹, Jude here: MIT, Applied Plamsa Physics, 2014 & Twitter, Applied Machine Learning, 2021. I've been working on this tool to enable developers to digitally sign their source code and track it across the entire software supply-chain (e.g. github, aws, gitlab, linode, azure, etc.). Still perfecting it so feel free offer feeback or flat out tell me 'this sucks'ðŸ˜‚ (FYI:working on an intuitive demo). Hashes are verified via a blockchain (filecoin) node for transparency but only you, the owner, manage the private key. Simply compare the byte encrypted signature in your yml with the hash written to your blockchain node. If they ever differ you know to escalate. See the pmacct-digital-copyright-instructions.yml for complete instructions. Feel free to contact me directly to review any questions before accepting. ~~Best, pi@haiphenai.com","Hey pmacct!ðŸ‘‹, Jude here: MIT, Applied Plamsa Physics, 2014 & Twitter, Applied Machine Learning, 2021. I've been working on this tool to enable developers to digitally sign their source code and track it across the entire software supply-chain (e.g. github, aws, gitlab, linode, azure, etc.). Still perfecting it so feel free offer feeback or flat out tell me 'this sucks'ðŸ˜‚ (FYI:working on an intuitive demo). Hashes are verified via a blockchain (filecoin) node for transparency but only you, the owner, manage the private key. Simply compare the byte encrypted signature in your yml with the hash written to your blockchain node. If they ever differ you know to escalate. See the pmacct-digital-copyright-instructions.yml for complete instructions. Feel free to contact me directly to review any questions before accepting. ~~Best, pi@haiphenai.com",True,{}
pmacct/pmacct,https://github.com/pmacct/pmacct,597,2022-04-15T11:22:32Z,2022-04-15T15:27:09Z,2022-04-15T15:27:09Z,MERGED,True,17,0,2,https://github.com/msune,build: fix git safe directory errors,1,[],https://github.com/pmacct/pmacct/pull/597,https://github.com/msune,1,https://github.com/pmacct/pmacct/pull/597,"Short description
As a result of patches introduced in git to protect against
the defects CVE-2022-24765, CVE-2022-24767, pipelines started to
fail due to the new git versions being rolled out, with the error
Error: fatal: unsafe repository.
This commit fixes this by setting the appropriate exceptions for
the repository itself and for the git submodules
Checklist
I have:

 added the LICENSE template to new files
 compiled & tested this code
 included documentation (including possible behaviour changes)","Short description
As a result of patches introduced in git to protect against
the defects CVE-2022-24765, CVE-2022-24767, pipelines started to
fail due to the new git versions being rolled out, with the error
Error: fatal: unsafe repository.
This commit fixes this by setting the appropriate exceptions for
the repository itself and for the git submodules
Checklist
I have:

 added the LICENSE template to new files
 compiled & tested this code
 included documentation (including possible behaviour changes)",True,{}
pmacct/pmacct,https://github.com/pmacct/pmacct,605,2022-05-09T15:27:32Z,2022-05-10T00:19:21Z,2022-05-10T00:19:21Z,MERGED,True,1,1,1,https://github.com/racompton,Update FLOW_AUGMENTATION_PROCESS_DESCRIPTION.md,1,[],https://github.com/pmacct/pmacct/pull/605,https://github.com/racompton,1,https://github.com/pmacct/pmacct/pull/605,"Make it clearer that the peer_src_ip is the netflow exporter IP.
Short description

Checklist

I have:

 added the LICENSE template to new files
 compiled & tested this code
 included documentation (including possible behaviour changes)","Make it clearer that the peer_src_ip is the netflow exporter IP.
Short description

Checklist

I have:

 added the LICENSE template to new files
 compiled & tested this code
 included documentation (including possible behaviour changes)",True,{}
pmacct/pmacct,https://github.com/pmacct/pmacct,607,2022-05-18T07:34:48Z,2022-05-19T17:44:03Z,2022-05-19T17:44:03Z,MERGED,True,137,11,12,https://github.com/rbarazzutti,BMP & BGP dumps flattening,47,[],https://github.com/pmacct/pmacct/pull/607,https://github.com/rbarazzutti,1,https://github.com/pmacct/pmacct/pull/607,"Short description
This contribution aims to reduce the spikes induced by BGP and BMP table dumps.
Each device (each device, each peer_ip_src), has a deterministic time slot to be dumped. The time slots are attributed to a hashing mechanism (Dan Bernstein's hash #2).
Example:
In a specific deployment we have config.bgp_table_dump_refresh_time set to 3600. Each hour all devices got their bgp table dumps at the same exact time (0:00, 1:00, 2:00, ...)
By setting the flag bgp_table_dump_time_slots to 60, each device will have a defined time slot within the hour, so one device will get its BGP table dumped at 0:14, 1:14, 2:14, etc while another at 0:31, 1:31, 2:31, etc.
This contribution doesn't change the behavior of the software when not enabled in the configuration. These feature can be enabled with the bgp_table_dump_time_slots or the bmp_dump_time_slots configuration flags.
Checklist

I have:

 compiled & tested this code (thanks @scuzzilla for the great support during the tests)
 included documentation (including possible behavior changes)","Short description
This contribution aims to reduce the spikes induced by BGP and BMP table dumps.
Each device (each device, each peer_ip_src), has a deterministic time slot to be dumped. The time slots are attributed to a hashing mechanism (Dan Bernstein's hash #2).
Example:
In a specific deployment we have config.bgp_table_dump_refresh_time set to 3600. Each hour all devices got their bgp table dumps at the same exact time (0:00, 1:00, 2:00, ...)
By setting the flag bgp_table_dump_time_slots to 60, each device will have a defined time slot within the hour, so one device will get its BGP table dumped at 0:14, 1:14, 2:14, etc while another at 0:31, 1:31, 2:31, etc.
This contribution doesn't change the behavior of the software when not enabled in the configuration. These feature can be enabled with the bgp_table_dump_time_slots or the bmp_dump_time_slots configuration flags.
Checklist

I have:

 compiled & tested this code (thanks @scuzzilla for the great support during the tests)
 included documentation (including possible behavior changes)",True,{}
