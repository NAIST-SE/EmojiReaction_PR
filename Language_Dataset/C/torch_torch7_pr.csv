torch/torch7,https://github.com/torch/torch7,585,2016-03-09T08:14:46Z,2016-03-09T08:14:57Z,2016-03-09T08:28:35Z,MERGED,True,1,2,1,https://github.com/soumith,removing sys dependency for now,1,[],https://github.com/torch/torch7/pull/585,https://github.com/soumith,1,https://github.com/torch/torch7/pull/585,"cc: @borisfom
for now removing the sys dependency. I've already made the require behind a pcall in an earlier commit, so the dependency is very much optional.","cc: @borisfom
for now removing the sys dependency. I've already made the require behind a pcall in an earlier commit, so the dependency is very much optional.",True,{}
torch/torch7,https://github.com/torch/torch7,585,2016-03-09T08:14:46Z,2016-03-09T08:14:57Z,2016-03-09T08:28:35Z,MERGED,True,1,2,1,https://github.com/soumith,removing sys dependency for now,1,[],https://github.com/torch/torch7/pull/585,https://github.com/borisfom,2,https://github.com/torch/torch7/pull/585#issuecomment-194182121,"cc: @borisfom
for now removing the sys dependency. I've already made the require behind a pcall in an earlier commit, so the dependency is very much optional.",Right thing to do right now. Impressed :),True,{}
torch/torch7,https://github.com/torch/torch7,587,2016-03-11T00:20:05Z,2016-03-15T18:17:16Z,2016-03-15T18:17:16Z,MERGED,True,116,30,5,https://github.com/lukealonso,Lua 5.3 integer support,2,[],https://github.com/torch/torch7/pull/587,https://github.com/lukealonso,1,https://github.com/torch/torch7/pull/587,"This shouldn't change any behavior when compiled with Lua < 5.3 or LuaJIT.
Will post the corresponding change to cwrap. This works fine independent of that, but some functions like :fill() on LongTensor won't properly support 64-bit inputs until cwrap is updated.","This shouldn't change any behavior when compiled with Lua < 5.3 or LuaJIT.
Will post the corresponding change to cwrap. This works fine independent of that, but some functions like :fill() on LongTensor won't properly support 64-bit inputs until cwrap is updated.",True,{}
torch/torch7,https://github.com/torch/torch7,587,2016-03-11T00:20:05Z,2016-03-15T18:17:16Z,2016-03-15T18:17:16Z,MERGED,True,116,30,5,https://github.com/lukealonso,Lua 5.3 integer support,2,[],https://github.com/torch/torch7/pull/587,https://github.com/soumith,2,https://github.com/torch/torch7/pull/587#issuecomment-195151454,"This shouldn't change any behavior when compiled with Lua < 5.3 or LuaJIT.
Will post the corresponding change to cwrap. This works fine independent of that, but some functions like :fill() on LongTensor won't properly support 64-bit inputs until cwrap is updated.",@clementfarabet is this ready for merge? I haven't had a look,True,{}
torch/torch7,https://github.com/torch/torch7,587,2016-03-11T00:20:05Z,2016-03-15T18:17:16Z,2016-03-15T18:17:16Z,MERGED,True,116,30,5,https://github.com/lukealonso,Lua 5.3 integer support,2,[],https://github.com/torch/torch7/pull/587,https://github.com/clementfarabet,3,https://github.com/torch/torch7/pull/587#issuecomment-196957600,"This shouldn't change any behavior when compiled with Lua < 5.3 or LuaJIT.
Will post the corresponding change to cwrap. This works fine independent of that, but some functions like :fill() on LongTensor won't properly support 64-bit inputs until cwrap is updated.","It's been tested on our side, I think it's safe to merge.",True,{}
torch/torch7,https://github.com/torch/torch7,590,2016-03-12T23:27:57Z,2016-03-15T02:43:58Z,2016-03-15T02:45:08Z,MERGED,True,1,1,1,https://github.com/Atcold,Fix bad size evaluation,1,[],https://github.com/torch/torch7/pull/590,https://github.com/Atcold,1,https://github.com/torch/torch7/pull/590,"xmax, xmin and step are accreal. Therefore, computations are carried out in both int and double logic. The separate division does introduce a bug when int logic is used.
Before
th> torch.range(1, 25, 5, 'torch.LongTensor')
  1
  6
 11
 16
 21
 26
[torch.LongTensor of size 6]

th> torch.range(1, 25, 5, 'torch.FloatTensor')
  1
  6
 11
 16
 21
[torch.FloatTensor of size 5]
After
th> torch.range(1, 25, 5, 'torch.FloatTensor')
  1
  6
 11
 16
 21
[torch.FloatTensor of size 5]

th> torch.range(1, 25, 5, 'torch.LongTensor')
  1
  6
 11
 16
 21
[torch.LongTensor of size 5]

th> torch.range(1, 25, 5, 'torch.DoubleTensor')                                                      
  1
  6
 11
 16
 21
[torch.DoubleTensor of size 5]

th> torch.range(1, 25, 5, 'torch.IntTensor')                                                         
  1
  6
 11
 16
 21
[torch.IntTensor of size 5]

th> torch.range(1, 25, 5, 'torch.ByteTensor')                                                        
  1
  6
 11
 16
 21
[torch.ByteTensor of size 5]","xmax, xmin and step are accreal. Therefore, computations are carried out in both int and double logic. The separate division does introduce a bug when int logic is used.
Before
th> torch.range(1, 25, 5, 'torch.LongTensor')
  1
  6
 11
 16
 21
 26
[torch.LongTensor of size 6]

th> torch.range(1, 25, 5, 'torch.FloatTensor')
  1
  6
 11
 16
 21
[torch.FloatTensor of size 5]
After
th> torch.range(1, 25, 5, 'torch.FloatTensor')
  1
  6
 11
 16
 21
[torch.FloatTensor of size 5]

th> torch.range(1, 25, 5, 'torch.LongTensor')
  1
  6
 11
 16
 21
[torch.LongTensor of size 5]

th> torch.range(1, 25, 5, 'torch.DoubleTensor')                                                      
  1
  6
 11
 16
 21
[torch.DoubleTensor of size 5]

th> torch.range(1, 25, 5, 'torch.IntTensor')                                                         
  1
  6
 11
 16
 21
[torch.IntTensor of size 5]

th> torch.range(1, 25, 5, 'torch.ByteTensor')                                                        
  1
  6
 11
 16
 21
[torch.ByteTensor of size 5]",True,{}
torch/torch7,https://github.com/torch/torch7,590,2016-03-12T23:27:57Z,2016-03-15T02:43:58Z,2016-03-15T02:45:08Z,MERGED,True,1,1,1,https://github.com/Atcold,Fix bad size evaluation,1,[],https://github.com/torch/torch7/pull/590,https://github.com/andreaskoepf,2,https://github.com/torch/torch7/pull/590#issuecomment-195836172,"xmax, xmin and step are accreal. Therefore, computations are carried out in both int and double logic. The separate division does introduce a bug when int logic is used.
Before
th> torch.range(1, 25, 5, 'torch.LongTensor')
  1
  6
 11
 16
 21
 26
[torch.LongTensor of size 6]

th> torch.range(1, 25, 5, 'torch.FloatTensor')
  1
  6
 11
 16
 21
[torch.FloatTensor of size 5]
After
th> torch.range(1, 25, 5, 'torch.FloatTensor')
  1
  6
 11
 16
 21
[torch.FloatTensor of size 5]

th> torch.range(1, 25, 5, 'torch.LongTensor')
  1
  6
 11
 16
 21
[torch.LongTensor of size 5]

th> torch.range(1, 25, 5, 'torch.DoubleTensor')                                                      
  1
  6
 11
 16
 21
[torch.DoubleTensor of size 5]

th> torch.range(1, 25, 5, 'torch.IntTensor')                                                         
  1
  6
 11
 16
 21
[torch.IntTensor of size 5]

th> torch.range(1, 25, 5, 'torch.ByteTensor')                                                        
  1
  6
 11
 16
 21
[torch.ByteTensor of size 5]","This PR would revert the changes introduced in #252 ... maybe we should cast min, max & step to double always?",True,{}
torch/torch7,https://github.com/torch/torch7,590,2016-03-12T23:27:57Z,2016-03-15T02:43:58Z,2016-03-15T02:45:08Z,MERGED,True,1,1,1,https://github.com/Atcold,Fix bad size evaluation,1,[],https://github.com/torch/torch7/pull/590,https://github.com/Atcold,3,https://github.com/torch/torch7/pull/590#issuecomment-195836486,"xmax, xmin and step are accreal. Therefore, computations are carried out in both int and double logic. The separate division does introduce a bug when int logic is used.
Before
th> torch.range(1, 25, 5, 'torch.LongTensor')
  1
  6
 11
 16
 21
 26
[torch.LongTensor of size 6]

th> torch.range(1, 25, 5, 'torch.FloatTensor')
  1
  6
 11
 16
 21
[torch.FloatTensor of size 5]
After
th> torch.range(1, 25, 5, 'torch.FloatTensor')
  1
  6
 11
 16
 21
[torch.FloatTensor of size 5]

th> torch.range(1, 25, 5, 'torch.LongTensor')
  1
  6
 11
 16
 21
[torch.LongTensor of size 5]

th> torch.range(1, 25, 5, 'torch.DoubleTensor')                                                      
  1
  6
 11
 16
 21
[torch.DoubleTensor of size 5]

th> torch.range(1, 25, 5, 'torch.IntTensor')                                                         
  1
  6
 11
 16
 21
[torch.IntTensor of size 5]

th> torch.range(1, 25, 5, 'torch.ByteTensor')                                                        
  1
  6
 11
 16
 21
[torch.ByteTensor of size 5]","@andreaskoepf, I think that one simple cast would do the job. I.e.
size = (long) (((double) (xmax - xmin) / step) + 1);
Shall I re-push this +  reverting name=real instead of name=accreal?
Actually, it is not necessary, since
th> torch.FloatTensor():range(0.6, 0.9, 0.1)
 0.6000
 0.7000
 0.8000
 0.9000
[torch.FloatTensor of size 4]

th> torch.DoubleTensor():range(0.6, 0.9, 0.1)                                                        
 0.6000
 0.7000
 0.8000
 0.9000
[torch.DoubleTensor of size 4]
So, my commit simply fixes a bad ""more robust to rounding error"".",True,{}
torch/torch7,https://github.com/torch/torch7,590,2016-03-12T23:27:57Z,2016-03-15T02:43:58Z,2016-03-15T02:45:08Z,MERGED,True,1,1,1,https://github.com/Atcold,Fix bad size evaluation,1,[],https://github.com/torch/torch7/pull/590,https://github.com/andreaskoepf,4,https://github.com/torch/torch7/pull/590#issuecomment-195836991,"xmax, xmin and step are accreal. Therefore, computations are carried out in both int and double logic. The separate division does introduce a bug when int logic is used.
Before
th> torch.range(1, 25, 5, 'torch.LongTensor')
  1
  6
 11
 16
 21
 26
[torch.LongTensor of size 6]

th> torch.range(1, 25, 5, 'torch.FloatTensor')
  1
  6
 11
 16
 21
[torch.FloatTensor of size 5]
After
th> torch.range(1, 25, 5, 'torch.FloatTensor')
  1
  6
 11
 16
 21
[torch.FloatTensor of size 5]

th> torch.range(1, 25, 5, 'torch.LongTensor')
  1
  6
 11
 16
 21
[torch.LongTensor of size 5]

th> torch.range(1, 25, 5, 'torch.DoubleTensor')                                                      
  1
  6
 11
 16
 21
[torch.DoubleTensor of size 5]

th> torch.range(1, 25, 5, 'torch.IntTensor')                                                         
  1
  6
 11
 16
 21
[torch.IntTensor of size 5]

th> torch.range(1, 25, 5, 'torch.ByteTensor')                                                        
  1
  6
 11
 16
 21
[torch.ByteTensor of size 5]","If we want to keep the original 'fix' which especially works well if xmin/xmax are divisible by step we could use something like size = (long)((xmax/(double)step - xmin/(double)step)+1); ... regarding real/accreal I am not sure .. real becomes char/byte/short/int/long for the integer tensor types. accreal is probably the next larger type, e.g. long for IntTensor... probably it was introduced to get double inputs for FloatTensor?",True,{}
torch/torch7,https://github.com/torch/torch7,590,2016-03-12T23:27:57Z,2016-03-15T02:43:58Z,2016-03-15T02:45:08Z,MERGED,True,1,1,1,https://github.com/Atcold,Fix bad size evaluation,1,[],https://github.com/torch/torch7/pull/590,https://github.com/andreaskoepf,5,https://github.com/torch/torch7/pull/590#issuecomment-195837380,"xmax, xmin and step are accreal. Therefore, computations are carried out in both int and double logic. The separate division does introduce a bug when int logic is used.
Before
th> torch.range(1, 25, 5, 'torch.LongTensor')
  1
  6
 11
 16
 21
 26
[torch.LongTensor of size 6]

th> torch.range(1, 25, 5, 'torch.FloatTensor')
  1
  6
 11
 16
 21
[torch.FloatTensor of size 5]
After
th> torch.range(1, 25, 5, 'torch.FloatTensor')
  1
  6
 11
 16
 21
[torch.FloatTensor of size 5]

th> torch.range(1, 25, 5, 'torch.LongTensor')
  1
  6
 11
 16
 21
[torch.LongTensor of size 5]

th> torch.range(1, 25, 5, 'torch.DoubleTensor')                                                      
  1
  6
 11
 16
 21
[torch.DoubleTensor of size 5]

th> torch.range(1, 25, 5, 'torch.IntTensor')                                                         
  1
  6
 11
 16
 21
[torch.IntTensor of size 5]

th> torch.range(1, 25, 5, 'torch.ByteTensor')                                                        
  1
  6
 11
 16
 21
[torch.ByteTensor of size 5]",I do not know if people want to work with really high ranges...but a double casts for values > 2^52 could of course also become a problem since double mantissa has only 52 bits. Probably the 'perfect' solution would be to use (long)((xmax-ymin)/step+1) for integer types and the (long)((xmax/step - xmin/step) + 1) for floating point... that would be possible by #ifdef  for TH_REAL_IS_FLOAT or TH_REAL_IS_DOUBLE. @Atcold what do you think?,True,{}
torch/torch7,https://github.com/torch/torch7,590,2016-03-12T23:27:57Z,2016-03-15T02:43:58Z,2016-03-15T02:45:08Z,MERGED,True,1,1,1,https://github.com/Atcold,Fix bad size evaluation,1,[],https://github.com/torch/torch7/pull/590,https://github.com/Atcold,6,https://github.com/torch/torch7/pull/590#issuecomment-195845823,"xmax, xmin and step are accreal. Therefore, computations are carried out in both int and double logic. The separate division does introduce a bug when int logic is used.
Before
th> torch.range(1, 25, 5, 'torch.LongTensor')
  1
  6
 11
 16
 21
 26
[torch.LongTensor of size 6]

th> torch.range(1, 25, 5, 'torch.FloatTensor')
  1
  6
 11
 16
 21
[torch.FloatTensor of size 5]
After
th> torch.range(1, 25, 5, 'torch.FloatTensor')
  1
  6
 11
 16
 21
[torch.FloatTensor of size 5]

th> torch.range(1, 25, 5, 'torch.LongTensor')
  1
  6
 11
 16
 21
[torch.LongTensor of size 5]

th> torch.range(1, 25, 5, 'torch.DoubleTensor')                                                      
  1
  6
 11
 16
 21
[torch.DoubleTensor of size 5]

th> torch.range(1, 25, 5, 'torch.IntTensor')                                                         
  1
  6
 11
 16
 21
[torch.IntTensor of size 5]

th> torch.range(1, 25, 5, 'torch.ByteTensor')                                                        
  1
  6
 11
 16
 21
[torch.ByteTensor of size 5]","I don't think it is necessary. Right now the division is already done in double, for the case in which it matters. Otherwise, long is fine, as long as the subtraction is done berfore the division.",True,{}
torch/torch7,https://github.com/torch/torch7,590,2016-03-12T23:27:57Z,2016-03-15T02:43:58Z,2016-03-15T02:45:08Z,MERGED,True,1,1,1,https://github.com/Atcold,Fix bad size evaluation,1,[],https://github.com/torch/torch7/pull/590,https://github.com/andreaskoepf,7,https://github.com/torch/torch7/pull/590#issuecomment-195917129,"xmax, xmin and step are accreal. Therefore, computations are carried out in both int and double logic. The separate division does introduce a bug when int logic is used.
Before
th> torch.range(1, 25, 5, 'torch.LongTensor')
  1
  6
 11
 16
 21
 26
[torch.LongTensor of size 6]

th> torch.range(1, 25, 5, 'torch.FloatTensor')
  1
  6
 11
 16
 21
[torch.FloatTensor of size 5]
After
th> torch.range(1, 25, 5, 'torch.FloatTensor')
  1
  6
 11
 16
 21
[torch.FloatTensor of size 5]

th> torch.range(1, 25, 5, 'torch.LongTensor')
  1
  6
 11
 16
 21
[torch.LongTensor of size 5]

th> torch.range(1, 25, 5, 'torch.DoubleTensor')                                                      
  1
  6
 11
 16
 21
[torch.DoubleTensor of size 5]

th> torch.range(1, 25, 5, 'torch.IntTensor')                                                         
  1
  6
 11
 16
 21
[torch.IntTensor of size 5]

th> torch.range(1, 25, 5, 'torch.ByteTensor')                                                        
  1
  6
 11
 16
 21
[torch.ByteTensor of size 5]","@Atcold My comment regarding >2^52 might be relevant only for cases in which TH is used from C or with lua > 5.3. Since cwrap is used it is currently not possible to pass values in luajit as native long long type, e.g. torch.range(1LL, 100LL, 1LL) fails since it cannot handle the args.
For luajit you see something like:
th> x = 1152921504606846977
th> ffi.typeof('long')(x)
1152921504606846976LL
th> ffi.typeof('long')(ffi.typeof('double')(1152921504606846977LL))
1152921504606846976LL

or
th> y = 1152921504606846977LL
th> y
1152921504606846977LL
th> ffi.typeof('long')(ffi.typeof('double')(y))
1152921504606846976LL

So for xmax = 1152921504606846977, xmin = 0, step = 1 I think the size calculation would be wrong (this is of course a hypothetical view since it will out-of-memory anyway).",True,{}
torch/torch7,https://github.com/torch/torch7,590,2016-03-12T23:27:57Z,2016-03-15T02:43:58Z,2016-03-15T02:45:08Z,MERGED,True,1,1,1,https://github.com/Atcold,Fix bad size evaluation,1,[],https://github.com/torch/torch7/pull/590,https://github.com/andreaskoepf,8,https://github.com/torch/torch7/pull/590#issuecomment-195919252,"xmax, xmin and step are accreal. Therefore, computations are carried out in both int and double logic. The separate division does introduce a bug when int logic is used.
Before
th> torch.range(1, 25, 5, 'torch.LongTensor')
  1
  6
 11
 16
 21
 26
[torch.LongTensor of size 6]

th> torch.range(1, 25, 5, 'torch.FloatTensor')
  1
  6
 11
 16
 21
[torch.FloatTensor of size 5]
After
th> torch.range(1, 25, 5, 'torch.FloatTensor')
  1
  6
 11
 16
 21
[torch.FloatTensor of size 5]

th> torch.range(1, 25, 5, 'torch.LongTensor')
  1
  6
 11
 16
 21
[torch.LongTensor of size 5]

th> torch.range(1, 25, 5, 'torch.DoubleTensor')                                                      
  1
  6
 11
 16
 21
[torch.DoubleTensor of size 5]

th> torch.range(1, 25, 5, 'torch.IntTensor')                                                         
  1
  6
 11
 16
 21
[torch.IntTensor of size 5]

th> torch.range(1, 25, 5, 'torch.ByteTensor')                                                        
  1
  6
 11
 16
 21
[torch.ByteTensor of size 5]",@d11 maybe you want to comment on this too?,True,{}
torch/torch7,https://github.com/torch/torch7,590,2016-03-12T23:27:57Z,2016-03-15T02:43:58Z,2016-03-15T02:45:08Z,MERGED,True,1,1,1,https://github.com/Atcold,Fix bad size evaluation,1,[],https://github.com/torch/torch7/pull/590,https://github.com/Atcold,9,https://github.com/torch/torch7/pull/590#issuecomment-196625118,"xmax, xmin and step are accreal. Therefore, computations are carried out in both int and double logic. The separate division does introduce a bug when int logic is used.
Before
th> torch.range(1, 25, 5, 'torch.LongTensor')
  1
  6
 11
 16
 21
 26
[torch.LongTensor of size 6]

th> torch.range(1, 25, 5, 'torch.FloatTensor')
  1
  6
 11
 16
 21
[torch.FloatTensor of size 5]
After
th> torch.range(1, 25, 5, 'torch.FloatTensor')
  1
  6
 11
 16
 21
[torch.FloatTensor of size 5]

th> torch.range(1, 25, 5, 'torch.LongTensor')
  1
  6
 11
 16
 21
[torch.LongTensor of size 5]

th> torch.range(1, 25, 5, 'torch.DoubleTensor')                                                      
  1
  6
 11
 16
 21
[torch.DoubleTensor of size 5]

th> torch.range(1, 25, 5, 'torch.IntTensor')                                                         
  1
  6
 11
 16
 21
[torch.IntTensor of size 5]

th> torch.range(1, 25, 5, 'torch.ByteTensor')                                                        
  1
  6
 11
 16
 21
[torch.ByteTensor of size 5]","Isn't this supposed to be an easy merge, which reverts a change that introduced a bug?",True,{}
torch/torch7,https://github.com/torch/torch7,590,2016-03-12T23:27:57Z,2016-03-15T02:43:58Z,2016-03-15T02:45:08Z,MERGED,True,1,1,1,https://github.com/Atcold,Fix bad size evaluation,1,[],https://github.com/torch/torch7/pull/590,https://github.com/soumith,10,https://github.com/torch/torch7/pull/590#issuecomment-196625609,"xmax, xmin and step are accreal. Therefore, computations are carried out in both int and double logic. The separate division does introduce a bug when int logic is used.
Before
th> torch.range(1, 25, 5, 'torch.LongTensor')
  1
  6
 11
 16
 21
 26
[torch.LongTensor of size 6]

th> torch.range(1, 25, 5, 'torch.FloatTensor')
  1
  6
 11
 16
 21
[torch.FloatTensor of size 5]
After
th> torch.range(1, 25, 5, 'torch.FloatTensor')
  1
  6
 11
 16
 21
[torch.FloatTensor of size 5]

th> torch.range(1, 25, 5, 'torch.LongTensor')
  1
  6
 11
 16
 21
[torch.LongTensor of size 5]

th> torch.range(1, 25, 5, 'torch.DoubleTensor')                                                      
  1
  6
 11
 16
 21
[torch.DoubleTensor of size 5]

th> torch.range(1, 25, 5, 'torch.IntTensor')                                                         
  1
  6
 11
 16
 21
[torch.IntTensor of size 5]

th> torch.range(1, 25, 5, 'torch.ByteTensor')                                                        
  1
  6
 11
 16
 21
[torch.ByteTensor of size 5]",Thanks Alfredo!,True,{'THUMBS_UP': ['https://github.com/Atcold']}
torch/torch7,https://github.com/torch/torch7,591,2016-03-13T02:07:10Z,2016-03-21T19:53:33Z,2016-03-21T19:53:40Z,MERGED,True,0,1,1,https://github.com/bamos,Remove unnecessary rank check in potrs call.,1,[],https://github.com/torch/torch7/pull/591,https://github.com/bamos,1,https://github.com/torch/torch7/pull/591,"Hi, this PR removes an unnecessary rank check in the potrs code.
To illustrate why I think this is unnecessary, consider the following example.
A = torch.Tensor({
    {1.2705,  0.9971,  0.4948,  0.1389,  0.2381},
    {0.9971,  0.9966,  0.6752,  0.0686,  0.1196},
    {0.4948,  0.6752,  1.1434,  0.0314,  0.0582},
    {0.1389,  0.0686,  0.0314,  0.0270,  0.0526},
    {0.2381,  0.1196,  0.0582,  0.0526,  0.3957}})

B = torch.Tensor({
    {0.6219,  0.3439,  0.0431},
    {0.5642,  0.1756,  0.0153},
    {0.2334,  0.8594,  0.4103},
    {0.7556,  0.1966,  0.9637},
    {0.1420,  0.7185,  0.7476}})

chol = torch.potrf(A)
torch.potrs(B, chol)
torch.potrs(torch.cat({B,B}, 2), chol)
cat(B,B) fails the rank check, but this is irrelevant and
shouldn't result in an error.
With the unnecessary check
th> torch.potrs(B, chol)
 -94.2808   -9.3526 -121.8567
  85.0306    7.3517  108.4600
 -17.9665   -0.5677  -22.3295
 339.4752   36.4924  437.1153
 -11.0946    0.4539  -12.3903
[torch.DoubleTensor of size 5x3]

                                                                      [0.0001s] 
th> torch.potrs(torch.cat({B,B}, 2), chol)
bad argument #2 to '?' (Matrix B is rank-deficient at /home/bamos/torch/pkg/torch/lib/TH/generic/THTensorLapack.c:483)
stack traceback:
    [C]: at 0x7f6b7f8d7b90
    [C]: in function 'potrs'
    [string ""_RESULT={torch.potrs(torch.cat({B,B}, 2), chol)}""]:1: in main chunk
    [C]: in function 'xpcall'
    /home/bamos/torch/install/share/lua/5.1/trepl/init.lua:650: in function 'repl'
    ...amos/torch/install/lib/luarocks/rocks/trepl/scm-1/bin/th:199: in main chunk
    [C]: at 0x00405be0
Without the unnecessary check (this PR)
th> torch.potrs(B, chol)
 -94.2808   -9.3526 -121.8567
  85.0306    7.3517  108.4600
 -17.9665   -0.5677  -22.3295
 339.4752   36.4924  437.1153
 -11.0946    0.4539  -12.3903
[torch.DoubleTensor of size 5x3]

                                                                      [0.0004s] 
th> torch.potrs(torch.cat({B,B}, 2), chol)
 -94.2808   -9.3526 -121.8567  -94.2808   -9.3526 -121.8567
  85.0306    7.3517  108.4600   85.0306    7.3517  108.4600
 -17.9665   -0.5677  -22.3295  -17.9665   -0.5677  -22.3295
 339.4752   36.4924  437.1153  339.4752   36.4924  437.1153
 -11.0946    0.4539  -12.3903  -11.0946    0.4539  -12.3903
[torch.DoubleTensor of size 5x6]","Hi, this PR removes an unnecessary rank check in the potrs code.
To illustrate why I think this is unnecessary, consider the following example.
A = torch.Tensor({
    {1.2705,  0.9971,  0.4948,  0.1389,  0.2381},
    {0.9971,  0.9966,  0.6752,  0.0686,  0.1196},
    {0.4948,  0.6752,  1.1434,  0.0314,  0.0582},
    {0.1389,  0.0686,  0.0314,  0.0270,  0.0526},
    {0.2381,  0.1196,  0.0582,  0.0526,  0.3957}})

B = torch.Tensor({
    {0.6219,  0.3439,  0.0431},
    {0.5642,  0.1756,  0.0153},
    {0.2334,  0.8594,  0.4103},
    {0.7556,  0.1966,  0.9637},
    {0.1420,  0.7185,  0.7476}})

chol = torch.potrf(A)
torch.potrs(B, chol)
torch.potrs(torch.cat({B,B}, 2), chol)
cat(B,B) fails the rank check, but this is irrelevant and
shouldn't result in an error.
With the unnecessary check
th> torch.potrs(B, chol)
 -94.2808   -9.3526 -121.8567
  85.0306    7.3517  108.4600
 -17.9665   -0.5677  -22.3295
 339.4752   36.4924  437.1153
 -11.0946    0.4539  -12.3903
[torch.DoubleTensor of size 5x3]

                                                                      [0.0001s] 
th> torch.potrs(torch.cat({B,B}, 2), chol)
bad argument #2 to '?' (Matrix B is rank-deficient at /home/bamos/torch/pkg/torch/lib/TH/generic/THTensorLapack.c:483)
stack traceback:
    [C]: at 0x7f6b7f8d7b90
    [C]: in function 'potrs'
    [string ""_RESULT={torch.potrs(torch.cat({B,B}, 2), chol)}""]:1: in main chunk
    [C]: in function 'xpcall'
    /home/bamos/torch/install/share/lua/5.1/trepl/init.lua:650: in function 'repl'
    ...amos/torch/install/lib/luarocks/rocks/trepl/scm-1/bin/th:199: in main chunk
    [C]: at 0x00405be0
Without the unnecessary check (this PR)
th> torch.potrs(B, chol)
 -94.2808   -9.3526 -121.8567
  85.0306    7.3517  108.4600
 -17.9665   -0.5677  -22.3295
 339.4752   36.4924  437.1153
 -11.0946    0.4539  -12.3903
[torch.DoubleTensor of size 5x3]

                                                                      [0.0004s] 
th> torch.potrs(torch.cat({B,B}, 2), chol)
 -94.2808   -9.3526 -121.8567  -94.2808   -9.3526 -121.8567
  85.0306    7.3517  108.4600   85.0306    7.3517  108.4600
 -17.9665   -0.5677  -22.3295  -17.9665   -0.5677  -22.3295
 339.4752   36.4924  437.1153  339.4752   36.4924  437.1153
 -11.0946    0.4539  -12.3903  -11.0946    0.4539  -12.3903
[torch.DoubleTensor of size 5x6]",True,{}
torch/torch7,https://github.com/torch/torch7,591,2016-03-13T02:07:10Z,2016-03-21T19:53:33Z,2016-03-21T19:53:40Z,MERGED,True,0,1,1,https://github.com/bamos,Remove unnecessary rank check in potrs call.,1,[],https://github.com/torch/torch7/pull/591,https://github.com/j-wilson,2,https://github.com/torch/torch7/pull/591#issuecomment-199379907,"Hi, this PR removes an unnecessary rank check in the potrs code.
To illustrate why I think this is unnecessary, consider the following example.
A = torch.Tensor({
    {1.2705,  0.9971,  0.4948,  0.1389,  0.2381},
    {0.9971,  0.9966,  0.6752,  0.0686,  0.1196},
    {0.4948,  0.6752,  1.1434,  0.0314,  0.0582},
    {0.1389,  0.0686,  0.0314,  0.0270,  0.0526},
    {0.2381,  0.1196,  0.0582,  0.0526,  0.3957}})

B = torch.Tensor({
    {0.6219,  0.3439,  0.0431},
    {0.5642,  0.1756,  0.0153},
    {0.2334,  0.8594,  0.4103},
    {0.7556,  0.1966,  0.9637},
    {0.1420,  0.7185,  0.7476}})

chol = torch.potrf(A)
torch.potrs(B, chol)
torch.potrs(torch.cat({B,B}, 2), chol)
cat(B,B) fails the rank check, but this is irrelevant and
shouldn't result in an error.
With the unnecessary check
th> torch.potrs(B, chol)
 -94.2808   -9.3526 -121.8567
  85.0306    7.3517  108.4600
 -17.9665   -0.5677  -22.3295
 339.4752   36.4924  437.1153
 -11.0946    0.4539  -12.3903
[torch.DoubleTensor of size 5x3]

                                                                      [0.0001s] 
th> torch.potrs(torch.cat({B,B}, 2), chol)
bad argument #2 to '?' (Matrix B is rank-deficient at /home/bamos/torch/pkg/torch/lib/TH/generic/THTensorLapack.c:483)
stack traceback:
    [C]: at 0x7f6b7f8d7b90
    [C]: in function 'potrs'
    [string ""_RESULT={torch.potrs(torch.cat({B,B}, 2), chol)}""]:1: in main chunk
    [C]: in function 'xpcall'
    /home/bamos/torch/install/share/lua/5.1/trepl/init.lua:650: in function 'repl'
    ...amos/torch/install/lib/luarocks/rocks/trepl/scm-1/bin/th:199: in main chunk
    [C]: at 0x00405be0
Without the unnecessary check (this PR)
th> torch.potrs(B, chol)
 -94.2808   -9.3526 -121.8567
  85.0306    7.3517  108.4600
 -17.9665   -0.5677  -22.3295
 339.4752   36.4924  437.1153
 -11.0946    0.4539  -12.3903
[torch.DoubleTensor of size 5x3]

                                                                      [0.0004s] 
th> torch.potrs(torch.cat({B,B}, 2), chol)
 -94.2808   -9.3526 -121.8567  -94.2808   -9.3526 -121.8567
  85.0306    7.3517  108.4600   85.0306    7.3517  108.4600
 -17.9665   -0.5677  -22.3295  -17.9665   -0.5677  -22.3295
 339.4752   36.4924  437.1153  339.4752   36.4924  437.1153
 -11.0946    0.4539  -12.3903  -11.0946    0.4539  -12.3903
[torch.DoubleTensor of size 5x6]",Was about to make a similar PR; nice catch.,True,{}
torch/torch7,https://github.com/torch/torch7,591,2016-03-13T02:07:10Z,2016-03-21T19:53:33Z,2016-03-21T19:53:40Z,MERGED,True,0,1,1,https://github.com/bamos,Remove unnecessary rank check in potrs call.,1,[],https://github.com/torch/torch7/pull/591,https://github.com/soumith,3,https://github.com/torch/torch7/pull/591#issuecomment-199448053,"Hi, this PR removes an unnecessary rank check in the potrs code.
To illustrate why I think this is unnecessary, consider the following example.
A = torch.Tensor({
    {1.2705,  0.9971,  0.4948,  0.1389,  0.2381},
    {0.9971,  0.9966,  0.6752,  0.0686,  0.1196},
    {0.4948,  0.6752,  1.1434,  0.0314,  0.0582},
    {0.1389,  0.0686,  0.0314,  0.0270,  0.0526},
    {0.2381,  0.1196,  0.0582,  0.0526,  0.3957}})

B = torch.Tensor({
    {0.6219,  0.3439,  0.0431},
    {0.5642,  0.1756,  0.0153},
    {0.2334,  0.8594,  0.4103},
    {0.7556,  0.1966,  0.9637},
    {0.1420,  0.7185,  0.7476}})

chol = torch.potrf(A)
torch.potrs(B, chol)
torch.potrs(torch.cat({B,B}, 2), chol)
cat(B,B) fails the rank check, but this is irrelevant and
shouldn't result in an error.
With the unnecessary check
th> torch.potrs(B, chol)
 -94.2808   -9.3526 -121.8567
  85.0306    7.3517  108.4600
 -17.9665   -0.5677  -22.3295
 339.4752   36.4924  437.1153
 -11.0946    0.4539  -12.3903
[torch.DoubleTensor of size 5x3]

                                                                      [0.0001s] 
th> torch.potrs(torch.cat({B,B}, 2), chol)
bad argument #2 to '?' (Matrix B is rank-deficient at /home/bamos/torch/pkg/torch/lib/TH/generic/THTensorLapack.c:483)
stack traceback:
    [C]: at 0x7f6b7f8d7b90
    [C]: in function 'potrs'
    [string ""_RESULT={torch.potrs(torch.cat({B,B}, 2), chol)}""]:1: in main chunk
    [C]: in function 'xpcall'
    /home/bamos/torch/install/share/lua/5.1/trepl/init.lua:650: in function 'repl'
    ...amos/torch/install/lib/luarocks/rocks/trepl/scm-1/bin/th:199: in main chunk
    [C]: at 0x00405be0
Without the unnecessary check (this PR)
th> torch.potrs(B, chol)
 -94.2808   -9.3526 -121.8567
  85.0306    7.3517  108.4600
 -17.9665   -0.5677  -22.3295
 339.4752   36.4924  437.1153
 -11.0946    0.4539  -12.3903
[torch.DoubleTensor of size 5x3]

                                                                      [0.0004s] 
th> torch.potrs(torch.cat({B,B}, 2), chol)
 -94.2808   -9.3526 -121.8567  -94.2808   -9.3526 -121.8567
  85.0306    7.3517  108.4600   85.0306    7.3517  108.4600
 -17.9665   -0.5677  -22.3295  -17.9665   -0.5677  -22.3295
 339.4752   36.4924  437.1153  339.4752   36.4924  437.1153
 -11.0946    0.4539  -12.3903  -11.0946    0.4539  -12.3903
[torch.DoubleTensor of size 5x6]",Thanks Brandon!,True,{}
torch/torch7,https://github.com/torch/torch7,593,2016-03-14T20:23:04Z,2016-03-14T20:41:46Z,2016-03-14T20:54:40Z,MERGED,True,43,16,2,https://github.com/nkoumchatzky,Fix multinomial regression.,1,[],https://github.com/torch/torch7/pull/593,https://github.com/nkoumchatzky,1,https://github.com/torch/torch7/pull/593,"Basically forces all the cumulative distribution to DoubleTensor and all the intermediate states to doubles as well whatever the input probs type.
Also added a perf+rare-event test based on #418 @andresy's  example for further testing but I didn't notice any major slowdown.","Basically forces all the cumulative distribution to DoubleTensor and all the intermediate states to doubles as well whatever the input probs type.
Also added a perf+rare-event test based on #418 @andresy's  example for further testing but I didn't notice any major slowdown.",True,{}
torch/torch7,https://github.com/torch/torch7,593,2016-03-14T20:23:04Z,2016-03-14T20:41:46Z,2016-03-14T20:54:40Z,MERGED,True,43,16,2,https://github.com/nkoumchatzky,Fix multinomial regression.,1,[],https://github.com/torch/torch7/pull/593,https://github.com/soumith,2,https://github.com/torch/torch7/pull/593#issuecomment-196513473,"Basically forces all the cumulative distribution to DoubleTensor and all the intermediate states to doubles as well whatever the input probs type.
Also added a perf+rare-event test based on #418 @andresy's  example for further testing but I didn't notice any major slowdown.",Thanks!,True,{}
torch/torch7,https://github.com/torch/torch7,593,2016-03-14T20:23:04Z,2016-03-14T20:41:46Z,2016-03-14T20:54:40Z,MERGED,True,43,16,2,https://github.com/nkoumchatzky,Fix multinomial regression.,1,[],https://github.com/torch/torch7/pull/593,https://github.com/nkoumchatzky,3,https://github.com/torch/torch7/pull/593#issuecomment-196516840,"Basically forces all the cumulative distribution to DoubleTensor and all the intermediate states to doubles as well whatever the input probs type.
Also added a perf+rare-event test based on #418 @andresy's  example for further testing but I didn't notice any major slowdown.",Np!,True,{}
torch/torch7,https://github.com/torch/torch7,599,2016-03-16T12:19:54Z,2016-03-16T13:57:57Z,2016-03-16T13:57:57Z,MERGED,True,52,16,2,None,Document non-global RNG functionality.,1,[],https://github.com/torch/torch7/pull/599,None,1,https://github.com/torch/torch7/pull/599,,,True,{}
torch/torch7,https://github.com/torch/torch7,600,2016-03-16T20:36:20Z,2016-03-17T14:26:11Z,2016-03-17T14:26:11Z,MERGED,True,8,4,1,https://github.com/JoostvDoorn,Fix stack cleanup in torch_istensorarray,1,[],https://github.com/torch/torch7/pull/600,https://github.com/JoostvDoorn,1,https://github.com/torch/torch7/pull/600,Fixes from discussion with @dominikgrewe regarding #583. I hope everything is correct now. :),Fixes from discussion with @dominikgrewe regarding #583. I hope everything is correct now. :),True,{}
torch/torch7,https://github.com/torch/torch7,600,2016-03-16T20:36:20Z,2016-03-17T14:26:11Z,2016-03-17T14:26:11Z,MERGED,True,8,4,1,https://github.com/JoostvDoorn,Fix stack cleanup in torch_istensorarray,1,[],https://github.com/torch/torch7/pull/600,https://github.com/dominikgrewe,2,https://github.com/torch/torch7/pull/600#issuecomment-197790938,Fixes from discussion with @dominikgrewe regarding #583. I hope everything is correct now. :),Thanks. Looks good to me.,True,{}
torch/torch7,https://github.com/torch/torch7,602,2016-03-17T00:19:29Z,2016-03-17T14:08:44Z,2016-03-17T14:08:46Z,MERGED,True,26,47,2,https://github.com/jgehring,Revert to pattern matching in isTypeOf(),1,[],https://github.com/torch/torch7/pull/602,https://github.com/jgehring,1,https://github.com/torch/torch7/pull/602,"This reverts the ""enclosed"" class matching introduced in 9c75e57, which
broke matching against patterns. In contrast to the original implementation,
non-pattern strings are always matched fully against type names to minimize
surprises caused by e.g. common prefix.","This reverts the ""enclosed"" class matching introduced in 9c75e57, which
broke matching against patterns. In contrast to the original implementation,
non-pattern strings are always matched fully against type names to minimize
surprises caused by e.g. common prefix.",True,{}
torch/torch7,https://github.com/torch/torch7,602,2016-03-17T00:19:29Z,2016-03-17T14:08:44Z,2016-03-17T14:08:46Z,MERGED,True,26,47,2,https://github.com/jgehring,Revert to pattern matching in isTypeOf(),1,[],https://github.com/torch/torch7/pull/602,https://github.com/andresy,2,https://github.com/torch/torch7/pull/602#issuecomment-197893667,"This reverts the ""enclosed"" class matching introduced in 9c75e57, which
broke matching against patterns. In contrast to the original implementation,
non-pattern strings are always matched fully against type names to minimize
surprises caused by e.g. common prefix.",Thanks for fixing this! 👍,True,{}
torch/torch7,https://github.com/torch/torch7,608,2016-03-21T06:21:00Z,2016-03-29T03:34:28Z,2016-03-29T06:38:37Z,CLOSED,False,80,19,7,https://github.com/liboyue,Fix torch.mod and torch.cmod behaviors.,9,[],https://github.com/torch/torch7/pull/608,https://github.com/liboyue,1,https://github.com/torch/torch7/pull/608,"I met this issue #607 too. The function fmod in the math library behaves a little bit different with the % operator.
I replaced fmod(a, b) by a - b*floor(a/b) and modified test. Computing 100,000,000 remainders by
a - b*floor(a/b)  will cost 0.1s more than fmod(a, b), so this change will not affect the performance too much.","I met this issue #607 too. The function fmod in the math library behaves a little bit different with the % operator.
I replaced fmod(a, b) by a - b*floor(a/b) and modified test. Computing 100,000,000 remainders by
a - b*floor(a/b)  will cost 0.1s more than fmod(a, b), so this change will not affect the performance too much.",True,{}
torch/torch7,https://github.com/torch/torch7,608,2016-03-21T06:21:00Z,2016-03-29T03:34:28Z,2016-03-29T06:38:37Z,CLOSED,False,80,19,7,https://github.com/liboyue,Fix torch.mod and torch.cmod behaviors.,9,[],https://github.com/torch/torch7/pull/608,https://github.com/andreaskoepf,2,https://github.com/torch/torch7/pull/608#issuecomment-202024513,"I met this issue #607 too. The function fmod in the math library behaves a little bit different with the % operator.
I replaced fmod(a, b) by a - b*floor(a/b) and modified test. Computing 100,000,000 remainders by
a - b*floor(a/b)  will cost 0.1s more than fmod(a, b), so this change will not affect the performance too much.","@j-wilson @liboyue Thank for reporting and providing a patch.
Should we rename the original impl to fmod in order to provide the behaviour of the standard C fmod() function as well? Note that the lua math module itself also defines fmod to offer the symmetric modulo operation.
There are probably a lot of situations where users expect a -10.1 modulo 10 operation to return -0.1 instead of 9.9.",True,{}
torch/torch7,https://github.com/torch/torch7,608,2016-03-21T06:21:00Z,2016-03-29T03:34:28Z,2016-03-29T06:38:37Z,CLOSED,False,80,19,7,https://github.com/liboyue,Fix torch.mod and torch.cmod behaviors.,9,[],https://github.com/torch/torch7/pull/608,https://github.com/liboyue,3,https://github.com/torch/torch7/pull/608#issuecomment-202060532,"I met this issue #607 too. The function fmod in the math library behaves a little bit different with the % operator.
I replaced fmod(a, b) by a - b*floor(a/b) and modified test. Computing 100,000,000 remainders by
a - b*floor(a/b)  will cost 0.1s more than fmod(a, b), so this change will not affect the performance too much.","@andreaskoepf Yes, I think we need to add fmod() and modf() for the two types of mod, and leave mod() as it is for compatibility. I can provide a new patch later :)",True,{}
torch/torch7,https://github.com/torch/torch7,609,2016-03-22T14:42:19Z,2016-03-22T14:42:29Z,2016-03-22T14:42:29Z,CLOSED,False,2415,462,30,https://github.com/davidsaxton,Update from upstream,23,[],https://github.com/torch/torch7/pull/609,https://github.com/davidsaxton,1,https://github.com/torch/torch7/pull/609,,,True,{}
torch/torch7,https://github.com/torch/torch7,611,2016-03-22T16:51:16Z,2016-03-23T16:59:03Z,2016-03-23T16:59:08Z,MERGED,True,43,5,2,https://github.com/davidsaxton,"Improvements to torch.{linspace, logspace, range}.",1,[],https://github.com/torch/torch7/pull/611,https://github.com/davidsaxton,1,https://github.com/torch/torch7/pull/611,"Allow end < start in linspace and logspace (no mathematical reason to forbid
this, and it makes it match numpy behaviour).
Also don't raw-resize tensor unnecessarily. This makes it work with
non-contiguous tensors (e.g., can now do a range on a slice of a tensor).","Allow end < start in linspace and logspace (no mathematical reason to forbid
this, and it makes it match numpy behaviour).
Also don't raw-resize tensor unnecessarily. This makes it work with
non-contiguous tensors (e.g., can now do a range on a slice of a tensor).",True,{}
torch/torch7,https://github.com/torch/torch7,611,2016-03-22T16:51:16Z,2016-03-23T16:59:03Z,2016-03-23T16:59:08Z,MERGED,True,43,5,2,https://github.com/davidsaxton,"Improvements to torch.{linspace, logspace, range}.",1,[],https://github.com/torch/torch7/pull/611,https://github.com/soumith,2,https://github.com/torch/torch7/pull/611#issuecomment-200441724,"Allow end < start in linspace and logspace (no mathematical reason to forbid
this, and it makes it match numpy behaviour).
Also don't raw-resize tensor unnecessarily. This makes it work with
non-contiguous tensors (e.g., can now do a range on a slice of a tensor).",Thanks David!,True,{}
torch/torch7,https://github.com/torch/torch7,612,2016-03-22T19:51:52Z,2016-03-29T19:24:29Z,2016-03-29T19:24:29Z,MERGED,True,47,21,1,https://github.com/iamalbert,separate handler for binary/unary operators in luaT,1,[],https://github.com/torch/torch7/pull/612,https://github.com/iamalbert,1,https://github.com/torch/torch7/pull/612,"#610
For binary operators, follow the original Lua behavior. (http://www.lua.org/manual/5.1/manual.html#2.10.1)
First, try the first operand. If its type does not have a metatable defining a handler for such operation, then try the second operand.","#610
For binary operators, follow the original Lua behavior. (http://www.lua.org/manual/5.1/manual.html#2.10.1)
First, try the first operand. If its type does not have a metatable defining a handler for such operation, then try the second operand.",True,{'HOORAY': ['https://github.com/Atcold']}
torch/torch7,https://github.com/torch/torch7,612,2016-03-22T19:51:52Z,2016-03-29T19:24:29Z,2016-03-29T19:24:29Z,MERGED,True,47,21,1,https://github.com/iamalbert,separate handler for binary/unary operators in luaT,1,[],https://github.com/torch/torch7/pull/612,https://github.com/soumith,2,https://github.com/torch/torch7/pull/612#issuecomment-203062126,"#610
For binary operators, follow the original Lua behavior. (http://www.lua.org/manual/5.1/manual.html#2.10.1)
First, try the first operand. If its type does not have a metatable defining a handler for such operation, then try the second operand.","Thanks Albert. I think now one just has to fix div and mod to support this case. https://github.com/torch/torch7/blob/master/generic/TensorOperator.c#L5 , add, sub and mul already handle it correctly",True,{}
torch/torch7,https://github.com/torch/torch7,614,2016-03-24T17:03:29Z,2016-03-24T17:07:04Z,2016-03-24T17:07:12Z,CLOSED,False,213,65,11,https://github.com/jampunkramadhan,Master,12,[],https://github.com/torch/torch7/pull/614,https://github.com/jampunkramadhan,1,https://github.com/torch/torch7/pull/614,,,True,{}
torch/torch7,https://github.com/torch/torch7,614,2016-03-24T17:03:29Z,2016-03-24T17:07:04Z,2016-03-24T17:07:12Z,CLOSED,False,213,65,11,https://github.com/jampunkramadhan,Master,12,[],https://github.com/torch/torch7/pull/614,https://github.com/soumith,2,https://github.com/torch/torch7/pull/614#issuecomment-200929470,,not sure what this is.,True,{}
torch/torch7,https://github.com/torch/torch7,615,2016-03-24T18:07:30Z,2016-03-28T00:54:44Z,2016-03-28T00:54:47Z,MERGED,True,28,4,4,https://github.com/colesbury,Add DiskFile:noBuffer(),1,[],https://github.com/torch/torch7/pull/615,https://github.com/colesbury,1,https://github.com/torch/torch7/pull/615,"The noBuffer function disables read and write buffering via setvbuf.
This is particularly useful for random reads from files on networked storage, which otherwise results in much larger network transfers.","The noBuffer function disables read and write buffering via setvbuf.
This is particularly useful for random reads from files on networked storage, which otherwise results in much larger network transfers.",True,{}
torch/torch7,https://github.com/torch/torch7,615,2016-03-24T18:07:30Z,2016-03-28T00:54:44Z,2016-03-28T00:54:47Z,MERGED,True,28,4,4,https://github.com/colesbury,Add DiskFile:noBuffer(),1,[],https://github.com/torch/torch7/pull/615,https://github.com/soumith,2,https://github.com/torch/torch7/pull/615#issuecomment-200953857,"The noBuffer function disables read and write buffering via setvbuf.
This is particularly useful for random reads from files on networked storage, which otherwise results in much larger network transfers.",i'm surprised that setvbuf exists on Windoze. Looks good to me.,True,{}
torch/torch7,https://github.com/torch/torch7,615,2016-03-24T18:07:30Z,2016-03-28T00:54:44Z,2016-03-28T00:54:47Z,MERGED,True,28,4,4,https://github.com/colesbury,Add DiskFile:noBuffer(),1,[],https://github.com/torch/torch7/pull/615,https://github.com/soumith,3,https://github.com/torch/torch7/pull/615#issuecomment-202179186,"The noBuffer function disables read and write buffering via setvbuf.
This is particularly useful for random reads from files on networked storage, which otherwise results in much larger network transfers.",Thanks!,True,{}
torch/torch7,https://github.com/torch/torch7,616,2016-03-25T04:18:29Z,2016-03-25T05:21:43Z,2016-03-25T05:21:43Z,CLOSED,False,0,0,0,https://github.com/jampunkramadhan,Merge remote-tracking branch 'refs/remotes/torch/master',1,[],https://github.com/torch/torch7/pull/616,https://github.com/jampunkramadhan,1,https://github.com/torch/torch7/pull/616,,,True,{}
torch/torch7,https://github.com/torch/torch7,619,2016-03-27T11:28:18Z,2016-04-17T14:28:07Z,2016-04-17T14:28:07Z,CLOSED,False,18,8,2,https://github.com/andreaskoepf,Alternate implementation of torch.chunk(),1,[],https://github.com/torch/torch7/pull/619,https://github.com/andreaskoepf,1,https://github.com/torch/torch7/pull/619,"This PR tries to address #617. The new implementation returns a table with nChunk entries, even in the case that the tensor has < nChunk entries in the specified dimension.
TODO: If new behaviour is as intended the documentation has to be updated since it currently says chunk() would use split().
Old behaviour:
th> torch.rand(11):chunk(5)
{
  1 : DoubleTensor - size: 3
  2 : DoubleTensor - size: 3
  3 : DoubleTensor - size: 3
  4 : DoubleTensor - size: 2
}

th> torch.rand(10):chunk(7)
{
  1 : DoubleTensor - size: 2
  2 : DoubleTensor - size: 2
  3 : DoubleTensor - size: 2
  4 : DoubleTensor - size: 2
  5 : DoubleTensor - size: 2
}

th> torch.rand(5):chunk(10)
{
  1 : DoubleTensor - size: 1
  2 : DoubleTensor - size: 1
  3 : DoubleTensor - size: 1
  4 : DoubleTensor - size: 1
  5 : DoubleTensor - size: 1
}

New behaviour:
th> torch.rand(11):chunk(5)
{
  1 : DoubleTensor - size: 3
  2 : DoubleTensor - size: 2
  3 : DoubleTensor - size: 2
  4 : DoubleTensor - size: 2
  5 : DoubleTensor - size: 2
}

th> torch.rand(10):chunk(7)
{
  1 : DoubleTensor - size: 2
  2 : DoubleTensor - size: 1
  3 : DoubleTensor - size: 2
  4 : DoubleTensor - size: 1
  5 : DoubleTensor - size: 2
  6 : DoubleTensor - size: 1
  7 : DoubleTensor - size: 1
}

th> torch.rand(5):chunk(10)
{
  1 : DoubleTensor - size: 1
  2 : DoubleTensor - empty
  3 : DoubleTensor - size: 1
  4 : DoubleTensor - empty
  5 : DoubleTensor - size: 1
  6 : DoubleTensor - empty
  7 : DoubleTensor - size: 1
  8 : DoubleTensor - empty
  9 : DoubleTensor - size: 1
  10 : DoubleTensor - empty
}","This PR tries to address #617. The new implementation returns a table with nChunk entries, even in the case that the tensor has < nChunk entries in the specified dimension.
TODO: If new behaviour is as intended the documentation has to be updated since it currently says chunk() would use split().
Old behaviour:
th> torch.rand(11):chunk(5)
{
  1 : DoubleTensor - size: 3
  2 : DoubleTensor - size: 3
  3 : DoubleTensor - size: 3
  4 : DoubleTensor - size: 2
}

th> torch.rand(10):chunk(7)
{
  1 : DoubleTensor - size: 2
  2 : DoubleTensor - size: 2
  3 : DoubleTensor - size: 2
  4 : DoubleTensor - size: 2
  5 : DoubleTensor - size: 2
}

th> torch.rand(5):chunk(10)
{
  1 : DoubleTensor - size: 1
  2 : DoubleTensor - size: 1
  3 : DoubleTensor - size: 1
  4 : DoubleTensor - size: 1
  5 : DoubleTensor - size: 1
}

New behaviour:
th> torch.rand(11):chunk(5)
{
  1 : DoubleTensor - size: 3
  2 : DoubleTensor - size: 2
  3 : DoubleTensor - size: 2
  4 : DoubleTensor - size: 2
  5 : DoubleTensor - size: 2
}

th> torch.rand(10):chunk(7)
{
  1 : DoubleTensor - size: 2
  2 : DoubleTensor - size: 1
  3 : DoubleTensor - size: 2
  4 : DoubleTensor - size: 1
  5 : DoubleTensor - size: 2
  6 : DoubleTensor - size: 1
  7 : DoubleTensor - size: 1
}

th> torch.rand(5):chunk(10)
{
  1 : DoubleTensor - size: 1
  2 : DoubleTensor - empty
  3 : DoubleTensor - size: 1
  4 : DoubleTensor - empty
  5 : DoubleTensor - size: 1
  6 : DoubleTensor - empty
  7 : DoubleTensor - size: 1
  8 : DoubleTensor - empty
  9 : DoubleTensor - size: 1
  10 : DoubleTensor - empty
}",True,{}
torch/torch7,https://github.com/torch/torch7,619,2016-03-27T11:28:18Z,2016-04-17T14:28:07Z,2016-04-17T14:28:07Z,CLOSED,False,18,8,2,https://github.com/andreaskoepf,Alternate implementation of torch.chunk(),1,[],https://github.com/torch/torch7/pull/619,https://github.com/vivanov879,2,https://github.com/torch/torch7/pull/619#issuecomment-202339298,"This PR tries to address #617. The new implementation returns a table with nChunk entries, even in the case that the tensor has < nChunk entries in the specified dimension.
TODO: If new behaviour is as intended the documentation has to be updated since it currently says chunk() would use split().
Old behaviour:
th> torch.rand(11):chunk(5)
{
  1 : DoubleTensor - size: 3
  2 : DoubleTensor - size: 3
  3 : DoubleTensor - size: 3
  4 : DoubleTensor - size: 2
}

th> torch.rand(10):chunk(7)
{
  1 : DoubleTensor - size: 2
  2 : DoubleTensor - size: 2
  3 : DoubleTensor - size: 2
  4 : DoubleTensor - size: 2
  5 : DoubleTensor - size: 2
}

th> torch.rand(5):chunk(10)
{
  1 : DoubleTensor - size: 1
  2 : DoubleTensor - size: 1
  3 : DoubleTensor - size: 1
  4 : DoubleTensor - size: 1
  5 : DoubleTensor - size: 1
}

New behaviour:
th> torch.rand(11):chunk(5)
{
  1 : DoubleTensor - size: 3
  2 : DoubleTensor - size: 2
  3 : DoubleTensor - size: 2
  4 : DoubleTensor - size: 2
  5 : DoubleTensor - size: 2
}

th> torch.rand(10):chunk(7)
{
  1 : DoubleTensor - size: 2
  2 : DoubleTensor - size: 1
  3 : DoubleTensor - size: 2
  4 : DoubleTensor - size: 1
  5 : DoubleTensor - size: 2
  6 : DoubleTensor - size: 1
  7 : DoubleTensor - size: 1
}

th> torch.rand(5):chunk(10)
{
  1 : DoubleTensor - size: 1
  2 : DoubleTensor - empty
  3 : DoubleTensor - size: 1
  4 : DoubleTensor - empty
  5 : DoubleTensor - size: 1
  6 : DoubleTensor - empty
  7 : DoubleTensor - size: 1
  8 : DoubleTensor - empty
  9 : DoubleTensor - size: 1
  10 : DoubleTensor - empty
}",works great -- thanks,True,{}
torch/torch7,https://github.com/torch/torch7,619,2016-03-27T11:28:18Z,2016-04-17T14:28:07Z,2016-04-17T14:28:07Z,CLOSED,False,18,8,2,https://github.com/andreaskoepf,Alternate implementation of torch.chunk(),1,[],https://github.com/torch/torch7/pull/619,https://github.com/andreaskoepf,3,https://github.com/torch/torch7/pull/619#issuecomment-203102503,"This PR tries to address #617. The new implementation returns a table with nChunk entries, even in the case that the tensor has < nChunk entries in the specified dimension.
TODO: If new behaviour is as intended the documentation has to be updated since it currently says chunk() would use split().
Old behaviour:
th> torch.rand(11):chunk(5)
{
  1 : DoubleTensor - size: 3
  2 : DoubleTensor - size: 3
  3 : DoubleTensor - size: 3
  4 : DoubleTensor - size: 2
}

th> torch.rand(10):chunk(7)
{
  1 : DoubleTensor - size: 2
  2 : DoubleTensor - size: 2
  3 : DoubleTensor - size: 2
  4 : DoubleTensor - size: 2
  5 : DoubleTensor - size: 2
}

th> torch.rand(5):chunk(10)
{
  1 : DoubleTensor - size: 1
  2 : DoubleTensor - size: 1
  3 : DoubleTensor - size: 1
  4 : DoubleTensor - size: 1
  5 : DoubleTensor - size: 1
}

New behaviour:
th> torch.rand(11):chunk(5)
{
  1 : DoubleTensor - size: 3
  2 : DoubleTensor - size: 2
  3 : DoubleTensor - size: 2
  4 : DoubleTensor - size: 2
  5 : DoubleTensor - size: 2
}

th> torch.rand(10):chunk(7)
{
  1 : DoubleTensor - size: 2
  2 : DoubleTensor - size: 1
  3 : DoubleTensor - size: 2
  4 : DoubleTensor - size: 1
  5 : DoubleTensor - size: 2
  6 : DoubleTensor - size: 1
  7 : DoubleTensor - size: 1
}

th> torch.rand(5):chunk(10)
{
  1 : DoubleTensor - size: 1
  2 : DoubleTensor - empty
  3 : DoubleTensor - size: 1
  4 : DoubleTensor - empty
  5 : DoubleTensor - size: 1
  6 : DoubleTensor - empty
  7 : DoubleTensor - size: 1
  8 : DoubleTensor - empty
  9 : DoubleTensor - size: 1
  10 : DoubleTensor - empty
}",@nicholas-leonard I think you added torch.chunk() originally ... what do you think?,True,{}
torch/torch7,https://github.com/torch/torch7,619,2016-03-27T11:28:18Z,2016-04-17T14:28:07Z,2016-04-17T14:28:07Z,CLOSED,False,18,8,2,https://github.com/andreaskoepf,Alternate implementation of torch.chunk(),1,[],https://github.com/torch/torch7/pull/619,https://github.com/soumith,4,https://github.com/torch/torch7/pull/619#issuecomment-210925735,"This PR tries to address #617. The new implementation returns a table with nChunk entries, even in the case that the tensor has < nChunk entries in the specified dimension.
TODO: If new behaviour is as intended the documentation has to be updated since it currently says chunk() would use split().
Old behaviour:
th> torch.rand(11):chunk(5)
{
  1 : DoubleTensor - size: 3
  2 : DoubleTensor - size: 3
  3 : DoubleTensor - size: 3
  4 : DoubleTensor - size: 2
}

th> torch.rand(10):chunk(7)
{
  1 : DoubleTensor - size: 2
  2 : DoubleTensor - size: 2
  3 : DoubleTensor - size: 2
  4 : DoubleTensor - size: 2
  5 : DoubleTensor - size: 2
}

th> torch.rand(5):chunk(10)
{
  1 : DoubleTensor - size: 1
  2 : DoubleTensor - size: 1
  3 : DoubleTensor - size: 1
  4 : DoubleTensor - size: 1
  5 : DoubleTensor - size: 1
}

New behaviour:
th> torch.rand(11):chunk(5)
{
  1 : DoubleTensor - size: 3
  2 : DoubleTensor - size: 2
  3 : DoubleTensor - size: 2
  4 : DoubleTensor - size: 2
  5 : DoubleTensor - size: 2
}

th> torch.rand(10):chunk(7)
{
  1 : DoubleTensor - size: 2
  2 : DoubleTensor - size: 1
  3 : DoubleTensor - size: 2
  4 : DoubleTensor - size: 1
  5 : DoubleTensor - size: 2
  6 : DoubleTensor - size: 1
  7 : DoubleTensor - size: 1
}

th> torch.rand(5):chunk(10)
{
  1 : DoubleTensor - size: 1
  2 : DoubleTensor - empty
  3 : DoubleTensor - size: 1
  4 : DoubleTensor - empty
  5 : DoubleTensor - size: 1
  6 : DoubleTensor - empty
  7 : DoubleTensor - size: 1
  8 : DoubleTensor - empty
  9 : DoubleTensor - size: 1
  10 : DoubleTensor - empty
}",@nicholas-leonard can you please see if returning interleaved empty tensors as a behavior is okay? I am leaning towards it being not intuitive.,True,{}
torch/torch7,https://github.com/torch/torch7,619,2016-03-27T11:28:18Z,2016-04-17T14:28:07Z,2016-04-17T14:28:07Z,CLOSED,False,18,8,2,https://github.com/andreaskoepf,Alternate implementation of torch.chunk(),1,[],https://github.com/torch/torch7/pull/619,https://github.com/soumith,5,https://github.com/torch/torch7/pull/619#issuecomment-210925770,"This PR tries to address #617. The new implementation returns a table with nChunk entries, even in the case that the tensor has < nChunk entries in the specified dimension.
TODO: If new behaviour is as intended the documentation has to be updated since it currently says chunk() would use split().
Old behaviour:
th> torch.rand(11):chunk(5)
{
  1 : DoubleTensor - size: 3
  2 : DoubleTensor - size: 3
  3 : DoubleTensor - size: 3
  4 : DoubleTensor - size: 2
}

th> torch.rand(10):chunk(7)
{
  1 : DoubleTensor - size: 2
  2 : DoubleTensor - size: 2
  3 : DoubleTensor - size: 2
  4 : DoubleTensor - size: 2
  5 : DoubleTensor - size: 2
}

th> torch.rand(5):chunk(10)
{
  1 : DoubleTensor - size: 1
  2 : DoubleTensor - size: 1
  3 : DoubleTensor - size: 1
  4 : DoubleTensor - size: 1
  5 : DoubleTensor - size: 1
}

New behaviour:
th> torch.rand(11):chunk(5)
{
  1 : DoubleTensor - size: 3
  2 : DoubleTensor - size: 2
  3 : DoubleTensor - size: 2
  4 : DoubleTensor - size: 2
  5 : DoubleTensor - size: 2
}

th> torch.rand(10):chunk(7)
{
  1 : DoubleTensor - size: 2
  2 : DoubleTensor - size: 1
  3 : DoubleTensor - size: 2
  4 : DoubleTensor - size: 1
  5 : DoubleTensor - size: 2
  6 : DoubleTensor - size: 1
  7 : DoubleTensor - size: 1
}

th> torch.rand(5):chunk(10)
{
  1 : DoubleTensor - size: 1
  2 : DoubleTensor - empty
  3 : DoubleTensor - size: 1
  4 : DoubleTensor - empty
  5 : DoubleTensor - size: 1
  6 : DoubleTensor - empty
  7 : DoubleTensor - size: 1
  8 : DoubleTensor - empty
  9 : DoubleTensor - size: 1
  10 : DoubleTensor - empty
}","The alternative is to update the docs of torch.chunk to reflect that it's a best-effort chunking, and wont guarantee exactly those many chunks.",True,{}
torch/torch7,https://github.com/torch/torch7,619,2016-03-27T11:28:18Z,2016-04-17T14:28:07Z,2016-04-17T14:28:07Z,CLOSED,False,18,8,2,https://github.com/andreaskoepf,Alternate implementation of torch.chunk(),1,[],https://github.com/torch/torch7/pull/619,https://github.com/nicholas-leonard,6,https://github.com/torch/torch7/pull/619#issuecomment-210933146,"This PR tries to address #617. The new implementation returns a table with nChunk entries, even in the case that the tensor has < nChunk entries in the specified dimension.
TODO: If new behaviour is as intended the documentation has to be updated since it currently says chunk() would use split().
Old behaviour:
th> torch.rand(11):chunk(5)
{
  1 : DoubleTensor - size: 3
  2 : DoubleTensor - size: 3
  3 : DoubleTensor - size: 3
  4 : DoubleTensor - size: 2
}

th> torch.rand(10):chunk(7)
{
  1 : DoubleTensor - size: 2
  2 : DoubleTensor - size: 2
  3 : DoubleTensor - size: 2
  4 : DoubleTensor - size: 2
  5 : DoubleTensor - size: 2
}

th> torch.rand(5):chunk(10)
{
  1 : DoubleTensor - size: 1
  2 : DoubleTensor - size: 1
  3 : DoubleTensor - size: 1
  4 : DoubleTensor - size: 1
  5 : DoubleTensor - size: 1
}

New behaviour:
th> torch.rand(11):chunk(5)
{
  1 : DoubleTensor - size: 3
  2 : DoubleTensor - size: 2
  3 : DoubleTensor - size: 2
  4 : DoubleTensor - size: 2
  5 : DoubleTensor - size: 2
}

th> torch.rand(10):chunk(7)
{
  1 : DoubleTensor - size: 2
  2 : DoubleTensor - size: 1
  3 : DoubleTensor - size: 2
  4 : DoubleTensor - size: 1
  5 : DoubleTensor - size: 2
  6 : DoubleTensor - size: 1
  7 : DoubleTensor - size: 1
}

th> torch.rand(5):chunk(10)
{
  1 : DoubleTensor - size: 1
  2 : DoubleTensor - empty
  3 : DoubleTensor - size: 1
  4 : DoubleTensor - empty
  5 : DoubleTensor - size: 1
  6 : DoubleTensor - empty
  7 : DoubleTensor - size: 1
  8 : DoubleTensor - empty
  9 : DoubleTensor - size: 1
  10 : DoubleTensor - empty
}",@soumith I think you are right that we should just update the docs.,True,{}
torch/torch7,https://github.com/torch/torch7,619,2016-03-27T11:28:18Z,2016-04-17T14:28:07Z,2016-04-17T14:28:07Z,CLOSED,False,18,8,2,https://github.com/andreaskoepf,Alternate implementation of torch.chunk(),1,[],https://github.com/torch/torch7/pull/619,https://github.com/soumith,7,https://github.com/torch/torch7/pull/619#issuecomment-210933245,"This PR tries to address #617. The new implementation returns a table with nChunk entries, even in the case that the tensor has < nChunk entries in the specified dimension.
TODO: If new behaviour is as intended the documentation has to be updated since it currently says chunk() would use split().
Old behaviour:
th> torch.rand(11):chunk(5)
{
  1 : DoubleTensor - size: 3
  2 : DoubleTensor - size: 3
  3 : DoubleTensor - size: 3
  4 : DoubleTensor - size: 2
}

th> torch.rand(10):chunk(7)
{
  1 : DoubleTensor - size: 2
  2 : DoubleTensor - size: 2
  3 : DoubleTensor - size: 2
  4 : DoubleTensor - size: 2
  5 : DoubleTensor - size: 2
}

th> torch.rand(5):chunk(10)
{
  1 : DoubleTensor - size: 1
  2 : DoubleTensor - size: 1
  3 : DoubleTensor - size: 1
  4 : DoubleTensor - size: 1
  5 : DoubleTensor - size: 1
}

New behaviour:
th> torch.rand(11):chunk(5)
{
  1 : DoubleTensor - size: 3
  2 : DoubleTensor - size: 2
  3 : DoubleTensor - size: 2
  4 : DoubleTensor - size: 2
  5 : DoubleTensor - size: 2
}

th> torch.rand(10):chunk(7)
{
  1 : DoubleTensor - size: 2
  2 : DoubleTensor - size: 1
  3 : DoubleTensor - size: 2
  4 : DoubleTensor - size: 1
  5 : DoubleTensor - size: 2
  6 : DoubleTensor - size: 1
  7 : DoubleTensor - size: 1
}

th> torch.rand(5):chunk(10)
{
  1 : DoubleTensor - size: 1
  2 : DoubleTensor - empty
  3 : DoubleTensor - size: 1
  4 : DoubleTensor - empty
  5 : DoubleTensor - size: 1
  6 : DoubleTensor - empty
  7 : DoubleTensor - size: 1
  8 : DoubleTensor - empty
  9 : DoubleTensor - size: 1
  10 : DoubleTensor - empty
}","@andreaskoepf what do you think, do you think my POV is valid? I feel like it is super unintuitive to me that i would get a bunch of chunks with empty tensors (especially in between). One option is to go towards what people do in image resizing and aspect ratios. The default is that you put a best-effort-forward wrt chunking, and there's an option in the function signature that forces you to return the exact desired chunks, even if there's empty tensors).",True,{}
torch/torch7,https://github.com/torch/torch7,619,2016-03-27T11:28:18Z,2016-04-17T14:28:07Z,2016-04-17T14:28:07Z,CLOSED,False,18,8,2,https://github.com/andreaskoepf,Alternate implementation of torch.chunk(),1,[],https://github.com/torch/torch7/pull/619,https://github.com/andreaskoepf,8,https://github.com/torch/torch7/pull/619#issuecomment-211028721,"This PR tries to address #617. The new implementation returns a table with nChunk entries, even in the case that the tensor has < nChunk entries in the specified dimension.
TODO: If new behaviour is as intended the documentation has to be updated since it currently says chunk() would use split().
Old behaviour:
th> torch.rand(11):chunk(5)
{
  1 : DoubleTensor - size: 3
  2 : DoubleTensor - size: 3
  3 : DoubleTensor - size: 3
  4 : DoubleTensor - size: 2
}

th> torch.rand(10):chunk(7)
{
  1 : DoubleTensor - size: 2
  2 : DoubleTensor - size: 2
  3 : DoubleTensor - size: 2
  4 : DoubleTensor - size: 2
  5 : DoubleTensor - size: 2
}

th> torch.rand(5):chunk(10)
{
  1 : DoubleTensor - size: 1
  2 : DoubleTensor - size: 1
  3 : DoubleTensor - size: 1
  4 : DoubleTensor - size: 1
  5 : DoubleTensor - size: 1
}

New behaviour:
th> torch.rand(11):chunk(5)
{
  1 : DoubleTensor - size: 3
  2 : DoubleTensor - size: 2
  3 : DoubleTensor - size: 2
  4 : DoubleTensor - size: 2
  5 : DoubleTensor - size: 2
}

th> torch.rand(10):chunk(7)
{
  1 : DoubleTensor - size: 2
  2 : DoubleTensor - size: 1
  3 : DoubleTensor - size: 2
  4 : DoubleTensor - size: 1
  5 : DoubleTensor - size: 2
  6 : DoubleTensor - size: 1
  7 : DoubleTensor - size: 1
}

th> torch.rand(5):chunk(10)
{
  1 : DoubleTensor - size: 1
  2 : DoubleTensor - empty
  3 : DoubleTensor - size: 1
  4 : DoubleTensor - empty
  5 : DoubleTensor - size: 1
  6 : DoubleTensor - empty
  7 : DoubleTensor - size: 1
  8 : DoubleTensor - empty
  9 : DoubleTensor - size: 1
  10 : DoubleTensor - empty
}","OK, #617 solved as 'by-design'.",True,{}
torch/torch7,https://github.com/torch/torch7,620,2016-03-29T09:56:41Z,2016-04-16T23:54:34Z,2016-04-16T23:54:53Z,MERGED,True,261,44,6,https://github.com/liboyue,"Add fmod(), remainder(), remove mod() and fix tensor operator % behavior",4,[],https://github.com/torch/torch7/pull/620,https://github.com/liboyue,1,https://github.com/torch/torch7/pull/620,"To resolve #607, I added torch.fmod(), torch.cfmod() for computing remainder of division (rounded towards zero) and torch.remainder(), torch.cremainder() for computing a % b = a - b * floor(a/b).

torch.fmod() and torch.cfmod() are the same as torch.mod() and torch.fmod().
torch.remainder() and torch.cremainder() compute remainders by a - b * floor(a/b) and it will return nan if b == 0. The function remainder() in the math library computes by a - b * round(a/b). In my laptop, torch.remainder() is faster than torch.fmod().

th> a = torch.Tensor(10000, 10000):uniform(-10, 10);
                                                                      [0.9528s]
th> b = torch.fmod(a, 2);
                                                                      [1.0508s]
th> c = torch.remainder(a, 2);
                                                                      [0.5180s]

The tensor operator uses THTensor_(mod) as its backend. To make sure it's the same as Lua % operator I changed it with THTensor_(remainder).","To resolve #607, I added torch.fmod(), torch.cfmod() for computing remainder of division (rounded towards zero) and torch.remainder(), torch.cremainder() for computing a % b = a - b * floor(a/b).

torch.fmod() and torch.cfmod() are the same as torch.mod() and torch.fmod().
torch.remainder() and torch.cremainder() compute remainders by a - b * floor(a/b) and it will return nan if b == 0. The function remainder() in the math library computes by a - b * round(a/b). In my laptop, torch.remainder() is faster than torch.fmod().

th> a = torch.Tensor(10000, 10000):uniform(-10, 10);
                                                                      [0.9528s]
th> b = torch.fmod(a, 2);
                                                                      [1.0508s]
th> c = torch.remainder(a, 2);
                                                                      [0.5180s]

The tensor operator uses THTensor_(mod) as its backend. To make sure it's the same as Lua % operator I changed it with THTensor_(remainder).",True,{}
torch/torch7,https://github.com/torch/torch7,620,2016-03-29T09:56:41Z,2016-04-16T23:54:34Z,2016-04-16T23:54:53Z,MERGED,True,261,44,6,https://github.com/liboyue,"Add fmod(), remainder(), remove mod() and fix tensor operator % behavior",4,[],https://github.com/torch/torch7/pull/620,https://github.com/liboyue,2,https://github.com/torch/torch7/pull/620#issuecomment-202813409,"To resolve #607, I added torch.fmod(), torch.cfmod() for computing remainder of division (rounded towards zero) and torch.remainder(), torch.cremainder() for computing a % b = a - b * floor(a/b).

torch.fmod() and torch.cfmod() are the same as torch.mod() and torch.fmod().
torch.remainder() and torch.cremainder() compute remainders by a - b * floor(a/b) and it will return nan if b == 0. The function remainder() in the math library computes by a - b * round(a/b). In my laptop, torch.remainder() is faster than torch.fmod().

th> a = torch.Tensor(10000, 10000):uniform(-10, 10);
                                                                      [0.9528s]
th> b = torch.fmod(a, 2);
                                                                      [1.0508s]
th> c = torch.remainder(a, 2);
                                                                      [0.5180s]

The tensor operator uses THTensor_(mod) as its backend. To make sure it's the same as Lua % operator I changed it with THTensor_(remainder).",@andreaskoepf Is this PR better?,True,{}
torch/torch7,https://github.com/torch/torch7,620,2016-03-29T09:56:41Z,2016-04-16T23:54:34Z,2016-04-16T23:54:53Z,MERGED,True,261,44,6,https://github.com/liboyue,"Add fmod(), remainder(), remove mod() and fix tensor operator % behavior",4,[],https://github.com/torch/torch7/pull/620,https://github.com/andreaskoepf,3,https://github.com/torch/torch7/pull/620#issuecomment-203090813,"To resolve #607, I added torch.fmod(), torch.cfmod() for computing remainder of division (rounded towards zero) and torch.remainder(), torch.cremainder() for computing a % b = a - b * floor(a/b).

torch.fmod() and torch.cfmod() are the same as torch.mod() and torch.fmod().
torch.remainder() and torch.cremainder() compute remainders by a - b * floor(a/b) and it will return nan if b == 0. The function remainder() in the math library computes by a - b * round(a/b). In my laptop, torch.remainder() is faster than torch.fmod().

th> a = torch.Tensor(10000, 10000):uniform(-10, 10);
                                                                      [0.9528s]
th> b = torch.fmod(a, 2);
                                                                      [1.0508s]
th> c = torch.remainder(a, 2);
                                                                      [0.5180s]

The tensor operator uses THTensor_(mod) as its backend. To make sure it's the same as Lua % operator I changed it with THTensor_(remainder).","@liboyue remainder reminds me very much of the C remainder() function which implements the IEEE 754-1985 remainder standard operation. The standard remainder() rounds towards the nearest integer. From Wikipedia:

Floatingpoint remainder. This is not like a normal modulo operation, it can be negative for two positive numbers. It returns the exact value of x–(round(x/y)·y).

I think it would be confusing to introduce a remainder function with a different meaning. Also I am not really happy with introducing an alias and declaring mod as deprecated...
There are (at least) four different modulo-styles:

r = a - b * trunc(a/b) (truncated)
r = a - b * floor(a/b) (floored)
r = a - abs(b) * floor(a/abs(b)) (Euclidean)
r = a - b * round(a/b) (IEEE 754)

If we really want to offer all four flavours I wonder whether it would be better to have a single mod() function with a default behaviour (e.g. truncated/fmod style) and an optional mode-argument (similar to the mode of torch.eig) which would allow to switch to floored/Euclidean mode. This would be also fully backward compatible.
Maybe @soumith or @andresy could comment on this?",True,{}
torch/torch7,https://github.com/torch/torch7,620,2016-03-29T09:56:41Z,2016-04-16T23:54:34Z,2016-04-16T23:54:53Z,MERGED,True,261,44,6,https://github.com/liboyue,"Add fmod(), remainder(), remove mod() and fix tensor operator % behavior",4,[],https://github.com/torch/torch7/pull/620,https://github.com/liboyue,4,https://github.com/torch/torch7/pull/620#issuecomment-203185764,"To resolve #607, I added torch.fmod(), torch.cfmod() for computing remainder of division (rounded towards zero) and torch.remainder(), torch.cremainder() for computing a % b = a - b * floor(a/b).

torch.fmod() and torch.cfmod() are the same as torch.mod() and torch.fmod().
torch.remainder() and torch.cremainder() compute remainders by a - b * floor(a/b) and it will return nan if b == 0. The function remainder() in the math library computes by a - b * round(a/b). In my laptop, torch.remainder() is faster than torch.fmod().

th> a = torch.Tensor(10000, 10000):uniform(-10, 10);
                                                                      [0.9528s]
th> b = torch.fmod(a, 2);
                                                                      [1.0508s]
th> c = torch.remainder(a, 2);
                                                                      [0.5180s]

The tensor operator uses THTensor_(mod) as its backend. To make sure it's the same as Lua % operator I changed it with THTensor_(remainder).","@andreaskoepf In fact, I am confused by the names too...
How about this, just offer torch.fmod() and % operator. I don't think we need all the four mod, when people need other mods, they can implement it in Lua.
But It's important to clarify that torch.mod() is different with %, so in my opinion, we should mark it as deprecated...",True,{}
torch/torch7,https://github.com/torch/torch7,620,2016-03-29T09:56:41Z,2016-04-16T23:54:34Z,2016-04-16T23:54:53Z,MERGED,True,261,44,6,https://github.com/liboyue,"Add fmod(), remainder(), remove mod() and fix tensor operator % behavior",4,[],https://github.com/torch/torch7/pull/620,https://github.com/andreaskoepf,5,https://github.com/torch/torch7/pull/620#issuecomment-203274008,"To resolve #607, I added torch.fmod(), torch.cfmod() for computing remainder of division (rounded towards zero) and torch.remainder(), torch.cremainder() for computing a % b = a - b * floor(a/b).

torch.fmod() and torch.cfmod() are the same as torch.mod() and torch.fmod().
torch.remainder() and torch.cremainder() compute remainders by a - b * floor(a/b) and it will return nan if b == 0. The function remainder() in the math library computes by a - b * round(a/b). In my laptop, torch.remainder() is faster than torch.fmod().

th> a = torch.Tensor(10000, 10000):uniform(-10, 10);
                                                                      [0.9528s]
th> b = torch.fmod(a, 2);
                                                                      [1.0508s]
th> c = torch.remainder(a, 2);
                                                                      [0.5180s]

The tensor operator uses THTensor_(mod) as its backend. To make sure it's the same as Lua % operator I changed it with THTensor_(remainder).",Since C does not have a % operator for floating point values I would be fine with defining it as floored-mod for tensors.,True,{}
torch/torch7,https://github.com/torch/torch7,620,2016-03-29T09:56:41Z,2016-04-16T23:54:34Z,2016-04-16T23:54:53Z,MERGED,True,261,44,6,https://github.com/liboyue,"Add fmod(), remainder(), remove mod() and fix tensor operator % behavior",4,[],https://github.com/torch/torch7/pull/620,https://github.com/liboyue,6,https://github.com/torch/torch7/pull/620#issuecomment-203309575,"To resolve #607, I added torch.fmod(), torch.cfmod() for computing remainder of division (rounded towards zero) and torch.remainder(), torch.cremainder() for computing a % b = a - b * floor(a/b).

torch.fmod() and torch.cfmod() are the same as torch.mod() and torch.fmod().
torch.remainder() and torch.cremainder() compute remainders by a - b * floor(a/b) and it will return nan if b == 0. The function remainder() in the math library computes by a - b * round(a/b). In my laptop, torch.remainder() is faster than torch.fmod().

th> a = torch.Tensor(10000, 10000):uniform(-10, 10);
                                                                      [0.9528s]
th> b = torch.fmod(a, 2);
                                                                      [1.0508s]
th> c = torch.remainder(a, 2);
                                                                      [0.5180s]

The tensor operator uses THTensor_(mod) as its backend. To make sure it's the same as Lua % operator I changed it with THTensor_(remainder).","Yes.
th> (-1) % 2
1
th> torch.Tensor({-1}) % 2
-1
[torch.DoubleTensor of size 1]
% should have the same behavior.",True,{}
torch/torch7,https://github.com/torch/torch7,620,2016-03-29T09:56:41Z,2016-04-16T23:54:34Z,2016-04-16T23:54:53Z,MERGED,True,261,44,6,https://github.com/liboyue,"Add fmod(), remainder(), remove mod() and fix tensor operator % behavior",4,[],https://github.com/torch/torch7/pull/620,https://github.com/soumith,7,https://github.com/torch/torch7/pull/620#issuecomment-210925657,"To resolve #607, I added torch.fmod(), torch.cfmod() for computing remainder of division (rounded towards zero) and torch.remainder(), torch.cremainder() for computing a % b = a - b * floor(a/b).

torch.fmod() and torch.cfmod() are the same as torch.mod() and torch.fmod().
torch.remainder() and torch.cremainder() compute remainders by a - b * floor(a/b) and it will return nan if b == 0. The function remainder() in the math library computes by a - b * round(a/b). In my laptop, torch.remainder() is faster than torch.fmod().

th> a = torch.Tensor(10000, 10000):uniform(-10, 10);
                                                                      [0.9528s]
th> b = torch.fmod(a, 2);
                                                                      [1.0508s]
th> c = torch.remainder(a, 2);
                                                                      [0.5180s]

The tensor operator uses THTensor_(mod) as its backend. To make sure it's the same as Lua % operator I changed it with THTensor_(remainder).",Thanks a lot 李博约!,True,{}
torch/torch7,https://github.com/torch/torch7,625,2016-03-31T20:53:24Z,2016-03-31T20:55:44Z,2016-03-31T20:55:44Z,MERGED,True,2,3,1,https://github.com/colesbury,Only require sys.colors from sys,1,[],https://github.com/torch/torch7/pull/625,https://github.com/colesbury,1,https://github.com/torch/torch7/pull/625,"Loading the sys package can cause a deadlock in some OpenBLAS
configurations when multiple Torch threads require sys (via the torch
package) at the same time.
Depends on torch/sys#8","Loading the sys package can cause a deadlock in some OpenBLAS
configurations when multiple Torch threads require sys (via the torch
package) at the same time.
Depends on torch/sys#8",True,{}
torch/torch7,https://github.com/torch/torch7,632,2016-04-07T15:37:12Z,2016-04-07T15:56:41Z,2016-04-07T15:56:44Z,MERGED,True,2,2,1,https://github.com/lejlot,Update maths.md,1,[],https://github.com/torch/torch7/pull/632,https://github.com/lejlot,1,https://github.com/torch/torch7/pull/632,"Current implementation of .eig and .symeig returns eigenvalues such that A = V diag(e) V', not A = V' diag(e) V.","Current implementation of .eig and .symeig returns eigenvalues such that A = V diag(e) V', not A = V' diag(e) V.",True,{}
torch/torch7,https://github.com/torch/torch7,632,2016-04-07T15:37:12Z,2016-04-07T15:56:41Z,2016-04-07T15:56:44Z,MERGED,True,2,2,1,https://github.com/lejlot,Update maths.md,1,[],https://github.com/torch/torch7/pull/632,https://github.com/soumith,2,https://github.com/torch/torch7/pull/632#issuecomment-206967923,"Current implementation of .eig and .symeig returns eigenvalues such that A = V diag(e) V', not A = V' diag(e) V.",Thanks!,True,{}
torch/torch7,https://github.com/torch/torch7,634,2016-04-10T19:31:34Z,2016-04-12T16:05:43Z,2016-04-12T23:11:09Z,MERGED,True,31,3,5,https://github.com/tkoeppe,Make THInf type-specific,1,[],https://github.com/torch/torch7/pull/634,https://github.com/tkoeppe,1,https://github.com/torch/torch7/pull/634,"I have not tested this change with libraries other than torch7 itself. From a search though our internal codebase, I believe that whatever code does break (e.g. in cunn) is actually incorrect in its current form and needs to be fixed (usually something like float x = -THInf;, which needs to become float x = -FLT_MAX;).
What do you think?","I have not tested this change with libraries other than torch7 itself. From a search though our internal codebase, I believe that whatever code does break (e.g. in cunn) is actually incorrect in its current form and needs to be fixed (usually something like float x = -THInf;, which needs to become float x = -FLT_MAX;).
What do you think?",True,{}
torch/torch7,https://github.com/torch/torch7,634,2016-04-10T19:31:34Z,2016-04-12T16:05:43Z,2016-04-12T23:11:09Z,MERGED,True,31,3,5,https://github.com/tkoeppe,Make THInf type-specific,1,[],https://github.com/torch/torch7/pull/634,https://github.com/tkoeppe,2,https://github.com/torch/torch7/pull/634#issuecomment-208263372,"I have not tested this change with libraries other than torch7 itself. From a search though our internal codebase, I believe that whatever code does break (e.g. in cunn) is actually incorrect in its current form and needs to be fixed (usually something like float x = -THInf;, which needs to become float x = -FLT_MAX;).
What do you think?","@soumith, @dominikgrewe: I'd be very grateful for your advice and guidance on these PRs.",True,{}
torch/torch7,https://github.com/torch/torch7,634,2016-04-10T19:31:34Z,2016-04-12T16:05:43Z,2016-04-12T23:11:09Z,MERGED,True,31,3,5,https://github.com/tkoeppe,Make THInf type-specific,1,[],https://github.com/torch/torch7/pull/634,https://github.com/noa,3,https://github.com/torch/torch7/pull/634#issuecomment-209027927,"I have not tested this change with libraries other than torch7 itself. From a search though our internal codebase, I believe that whatever code does break (e.g. in cunn) is actually incorrect in its current form and needs to be fixed (usually something like float x = -THInf;, which needs to become float x = -FLT_MAX;).
What do you think?","I think this broke cutorch:
/tmp/luarocks_cutorch-scm-1-9578/cutorch/lib/THC/THCTensorMath.cu(280): error: identifier ""THInf"" is undefined
/tmp/luarocks_cutorch-scm-1-9578/cutorch/lib/THC/THCTensorMath.cu(295): error: identifier ""THInf"" is undefined
2 errors detected in the compilation of ""/tmp/tmpxft_00007690_00000000-7_THCTensorMath.cpp1.ii"".
CMake Error at THC_generated_THCTensorMath.cu.o.cmake:264 (message):
Error generating file
/tmp/luarocks_cutorch-scm-1-9578/cutorch/build/lib/THC/CMakeFiles/THC.dir//./THC_generated_THCTensorMath.cu.o",True,{}
torch/torch7,https://github.com/torch/torch7,634,2016-04-10T19:31:34Z,2016-04-12T16:05:43Z,2016-04-12T23:11:09Z,MERGED,True,31,3,5,https://github.com/tkoeppe,Make THInf type-specific,1,[],https://github.com/torch/torch7/pull/634,https://github.com/soumith,4,https://github.com/torch/torch7/pull/634#issuecomment-209032752,"I have not tested this change with libraries other than torch7 itself. From a search though our internal codebase, I believe that whatever code does break (e.g. in cunn) is actually incorrect in its current form and needs to be fixed (usually something like float x = -THInf;, which needs to become float x = -FLT_MAX;).
What do you think?",Should be fixed now.,True,{}
torch/torch7,https://github.com/torch/torch7,634,2016-04-10T19:31:34Z,2016-04-12T16:05:43Z,2016-04-12T23:11:09Z,MERGED,True,31,3,5,https://github.com/tkoeppe,Make THInf type-specific,1,[],https://github.com/torch/torch7/pull/634,https://github.com/noa,5,https://github.com/torch/torch7/pull/634#issuecomment-209033294,"I have not tested this change with libraries other than torch7 itself. From a search though our internal codebase, I believe that whatever code does break (e.g. in cunn) is actually incorrect in its current form and needs to be fixed (usually something like float x = -THInf;, which needs to become float x = -FLT_MAX;).
What do you think?",@soumith thanks,True,{}
torch/torch7,https://github.com/torch/torch7,639,2016-04-12T15:44:05Z,2016-04-12T16:04:52Z,2016-04-12T16:04:52Z,MERGED,True,1,1,1,https://github.com/marioyc,Fix link in serialization.md,1,[],https://github.com/torch/torch7/pull/639,https://github.com/marioyc,1,https://github.com/torch/torch7/pull/639,,,True,{}
torch/torch7,https://github.com/torch/torch7,642,2016-04-14T16:42:58Z,2016-04-14T17:12:54Z,2016-04-14T17:13:16Z,MERGED,True,13,0,2,https://github.com/deltheil,File: get rid of extra NULL character (serialize),1,[],https://github.com/torch/torch7/pull/642,https://github.com/deltheil,1,https://github.com/torch/torch7/pull/642,This extra slot comes from here. IMHO we should get rid of it in the context of serialize / serializeToStorage so that archives are identical to those produced by save.,This extra slot comes from here. IMHO we should get rid of it in the context of serialize / serializeToStorage so that archives are identical to those produced by save.,True,{}
torch/torch7,https://github.com/torch/torch7,642,2016-04-14T16:42:58Z,2016-04-14T17:12:54Z,2016-04-14T17:13:16Z,MERGED,True,13,0,2,https://github.com/deltheil,File: get rid of extra NULL character (serialize),1,[],https://github.com/torch/torch7/pull/642,https://github.com/soumith,2,https://github.com/torch/torch7/pull/642#issuecomment-210043722,This extra slot comes from here. IMHO we should get rid of it in the context of serialize / serializeToStorage so that archives are identical to those produced by save.,"i assume this poses no problems for loading serialized files, as this is a different codepath",True,{}
torch/torch7,https://github.com/torch/torch7,642,2016-04-14T16:42:58Z,2016-04-14T17:12:54Z,2016-04-14T17:13:16Z,MERGED,True,13,0,2,https://github.com/deltheil,File: get rid of extra NULL character (serialize),1,[],https://github.com/torch/torch7/pull/642,https://github.com/deltheil,3,https://github.com/torch/torch7/pull/642#issuecomment-210049176,This extra slot comes from here. IMHO we should get rid of it in the context of serialize / serializeToStorage so that archives are identical to those produced by save.,"Yes. Also using the current deserializeFromStorage with or without this extra slot is transparent, e.g. (before this fix):
> x = torch.serializeToStorage(""foo"")
> print(torch.deserializeFromStorage(x))
foo
> x:resize(x:size()-1)
> print(torch.deserializeFromStorage(x))
foo
I was just wondering why the deserialization part adds an extra slot (here) but it's another question.",True,{}
torch/torch7,https://github.com/torch/torch7,642,2016-04-14T16:42:58Z,2016-04-14T17:12:54Z,2016-04-14T17:13:16Z,MERGED,True,13,0,2,https://github.com/deltheil,File: get rid of extra NULL character (serialize),1,[],https://github.com/torch/torch7/pull/642,https://github.com/soumith,4,https://github.com/torch/torch7/pull/642#issuecomment-210055159,This extra slot comes from here. IMHO we should get rid of it in the context of serialize / serializeToStorage so that archives are identical to those produced by save.,Thanks Cedric.,True,{}
torch/torch7,https://github.com/torch/torch7,645,2016-04-16T15:40:44Z,2016-04-16T15:49:34Z,2016-04-16T15:49:39Z,MERGED,True,1,1,1,https://github.com/clcarwin,Fix _msize crash on windows xp,1,[],https://github.com/torch/torch7/pull/645,https://github.com/clcarwin,1,https://github.com/torch/torch7/pull/645,It will crash application when calling _msize(NULL) on windows xp.,It will crash application when calling _msize(NULL) on windows xp.,True,{'HOORAY': ['https://github.com/0mp']}
torch/torch7,https://github.com/torch/torch7,645,2016-04-16T15:40:44Z,2016-04-16T15:49:34Z,2016-04-16T15:49:39Z,MERGED,True,1,1,1,https://github.com/clcarwin,Fix _msize crash on windows xp,1,[],https://github.com/torch/torch7/pull/645,https://github.com/soumith,2,https://github.com/torch/torch7/pull/645#issuecomment-210843311,It will crash application when calling _msize(NULL) on windows xp.,Thanks!,True,{}
torch/torch7,https://github.com/torch/torch7,647,2016-04-18T16:56:01Z,2016-04-18T17:30:07Z,2016-04-18T17:30:07Z,MERGED,True,17,8,1,https://github.com/apaszke,Check for overflows when converting pointers in luaT,1,[],https://github.com/torch/torch7/pull/647,https://github.com/apaszke,1,https://github.com/torch/torch7/pull/647,"luaT_lua_pointer didn't check for overflows when converting pointers, and it used a signed long type, that could overflow and stay negative even after a cast to double. This addresses torch/threads#55.","luaT_lua_pointer didn't check for overflows when converting pointers, and it used a signed long type, that could overflow and stay negative even after a cast to double. This addresses torch/threads#55.",True,{'THUMBS_UP': ['https://github.com/hohoCode']}
torch/torch7,https://github.com/torch/torch7,647,2016-04-18T16:56:01Z,2016-04-18T17:30:07Z,2016-04-18T17:30:07Z,MERGED,True,17,8,1,https://github.com/apaszke,Check for overflows when converting pointers in luaT,1,[],https://github.com/torch/torch7/pull/647,https://github.com/apaszke,2,https://github.com/torch/torch7/pull/647#issuecomment-211477032,"luaT_lua_pointer didn't check for overflows when converting pointers, and it used a signed long type, that could overflow and stay negative even after a cast to double. This addresses torch/threads#55.","There are integers greater than 2^53 that are representable by doubles, but I don't really know how to check if we're dealing with one at the moment, so I made it a hard threshold.",True,{}
torch/torch7,https://github.com/torch/torch7,650,2016-04-19T10:13:30Z,2016-04-19T15:27:55Z,2016-04-19T15:27:59Z,MERGED,True,39,4,2,https://github.com/adria-p,Make torch.Generator serializable,1,[],https://github.com/torch/torch7/pull/650,https://github.com/adria-p,1,https://github.com/torch/torch7/pull/650,"This implements read/write naively as a block of bytes for a given torch.Generator (in the style of set/getRNGstate, which use memcpy).","This implements read/write naively as a block of bytes for a given torch.Generator (in the style of set/getRNGstate, which use memcpy).",True,{}
torch/torch7,https://github.com/torch/torch7,650,2016-04-19T10:13:30Z,2016-04-19T15:27:55Z,2016-04-19T15:27:59Z,MERGED,True,39,4,2,https://github.com/adria-p,Make torch.Generator serializable,1,[],https://github.com/torch/torch7/pull/650,https://github.com/adria-p,2,https://github.com/torch/torch7/pull/650#issuecomment-211876032,"This implements read/write naively as a block of bytes for a given torch.Generator (in the style of set/getRNGstate, which use memcpy).","Thanks for the comment, done.",True,{}
torch/torch7,https://github.com/torch/torch7,650,2016-04-19T10:13:30Z,2016-04-19T15:27:55Z,2016-04-19T15:27:59Z,MERGED,True,39,4,2,https://github.com/adria-p,Make torch.Generator serializable,1,[],https://github.com/torch/torch7/pull/650,https://github.com/soumith,3,https://github.com/torch/torch7/pull/650#issuecomment-211978405,"This implements read/write naively as a block of bytes for a given torch.Generator (in the style of set/getRNGstate, which use memcpy).",Thanks!,True,{}
torch/torch7,https://github.com/torch/torch7,652,2016-04-19T15:58:55Z,2016-04-19T16:05:22Z,2016-04-19T16:05:25Z,MERGED,True,4,4,1,https://github.com/apaszke,Fix pointer to long conversions in luaT,1,[],https://github.com/torch/torch7/pull/652,https://github.com/apaszke,1,https://github.com/torch/torch7/pull/652,I came across more segfaults when debugging threads. Apparently there's more of this conversion stuff 😕,I came across more segfaults when debugging threads. Apparently there's more of this conversion stuff 😕,True,{}
torch/torch7,https://github.com/torch/torch7,652,2016-04-19T15:58:55Z,2016-04-19T16:05:22Z,2016-04-19T16:05:25Z,MERGED,True,4,4,1,https://github.com/apaszke,Fix pointer to long conversions in luaT,1,[],https://github.com/torch/torch7/pull/652,https://github.com/soumith,2,https://github.com/torch/torch7/pull/652#issuecomment-211996740,I came across more segfaults when debugging threads. Apparently there's more of this conversion stuff 😕,Thanks!,True,{}
torch/torch7,https://github.com/torch/torch7,653,2016-04-19T20:40:58Z,2016-04-19T21:53:34Z,2016-04-19T21:53:34Z,MERGED,True,29,5,1,https://github.com/soumith,allow THAllocator to have a NULL realloc,1,[],https://github.com/torch/torch7/pull/653,https://github.com/soumith,1,https://github.com/torch/torch7/pull/653,"This is a pre-diff that's needed to fix cutorch's THCudaHostAllocator_realloc which at the moment has a bug where it does not copy the old memory over to the new memory.
https://github.com/torch/cutorch/blob/master/lib/THC/THCAllocator.c#L21-L31
cc: @tudor","This is a pre-diff that's needed to fix cutorch's THCudaHostAllocator_realloc which at the moment has a bug where it does not copy the old memory over to the new memory.
https://github.com/torch/cutorch/blob/master/lib/THC/THCAllocator.c#L21-L31
cc: @tudor",True,{}
torch/torch7,https://github.com/torch/torch7,653,2016-04-19T20:40:58Z,2016-04-19T21:53:34Z,2016-04-19T21:53:34Z,MERGED,True,29,5,1,https://github.com/soumith,allow THAllocator to have a NULL realloc,1,[],https://github.com/torch/torch7/pull/653,https://github.com/soumith,2,https://github.com/torch/torch7/pull/653#issuecomment-212144599,"This is a pre-diff that's needed to fix cutorch's THCudaHostAllocator_realloc which at the moment has a bug where it does not copy the old memory over to the new memory.
https://github.com/torch/cutorch/blob/master/lib/THC/THCAllocator.c#L21-L31
cc: @tudor","tested via locally setting the default allocator's realloc to NULL and running tests.
Reviewed by @tudor",True,{}
torch/torch7,https://github.com/torch/torch7,657,2016-04-25T10:31:27Z,2016-04-25T16:47:47Z,2016-04-25T17:00:43Z,MERGED,True,2,2,2,https://github.com/albanD,Remove some warning when compiling TH,1,[],https://github.com/torch/torch7/pull/657,https://github.com/albanD,1,https://github.com/torch/torch7/pull/657,These were introduce when adding these two features to TH: #13 and #213,These were introduce when adding these two features to TH: #13 and #213,True,{}
torch/torch7,https://github.com/torch/torch7,657,2016-04-25T10:31:27Z,2016-04-25T16:47:47Z,2016-04-25T17:00:43Z,MERGED,True,2,2,2,https://github.com/albanD,Remove some warning when compiling TH,1,[],https://github.com/torch/torch7/pull/657,https://github.com/soumith,2,https://github.com/torch/torch7/pull/657#issuecomment-214437851,These were introduce when adding these two features to TH: #13 and #213,Thanks!,True,{}
torch/torch7,https://github.com/torch/torch7,658,2016-04-25T17:24:34Z,2016-04-26T06:12:43Z,2016-04-26T06:12:46Z,MERGED,True,1,0,1,https://github.com/jacobmenick,Tester rethrows in better place when rethrow enabled.,1,[],https://github.com/torch/torch7/pull/658,https://github.com/jacobmenick,1,https://github.com/torch/torch7/pull/658,,,True,{}
torch/torch7,https://github.com/torch/torch7,658,2016-04-25T17:24:34Z,2016-04-26T06:12:43Z,2016-04-26T06:12:46Z,MERGED,True,1,0,1,https://github.com/jacobmenick,Tester rethrows in better place when rethrow enabled.,1,[],https://github.com/torch/torch7/pull/658,https://github.com/soumith,2,https://github.com/torch/torch7/pull/658#issuecomment-214625742,,Thanks!,True,{}
torch/torch7,https://github.com/torch/torch7,659,2016-04-26T00:02:18Z,2016-04-26T01:23:07Z,2016-04-26T01:23:07Z,MERGED,True,1,0,1,https://github.com/ebetica,tensor.md missing LongTensor,1,[],https://github.com/torch/torch7/pull/659,https://github.com/ebetica,1,https://github.com/torch/torch7/pull/659,,,True,{}
torch/torch7,https://github.com/torch/torch7,660,2016-04-26T08:26:46Z,2016-06-05T23:41:24Z,2016-06-05T23:41:24Z,CLOSED,False,26,27,5,https://github.com/Ark-kun,Fixed the Visual Studio CMake build.,1,[],https://github.com/torch/torch7/pull/660,https://github.com/Ark-kun,1,https://github.com/torch/torch7/pull/660,"Added missing CMake function include.
snprintf returns int, not ssize_t. ssize_t is not standard C++.
Added arguments for the variadic macro parameters. (We should probably remove the variadic parameter from the macro as it's never used)
Avoided preprocessor directives inside macro arguments.
Not linking to LibM when building under MSVC as the math.h implementations are already available in the msvcrt*.dll. (Does torch need the LibM or just some math.h implementation?)","Added missing CMake function include.
snprintf returns int, not ssize_t. ssize_t is not standard C++.
Added arguments for the variadic macro parameters. (We should probably remove the variadic parameter from the macro as it's never used)
Avoided preprocessor directives inside macro arguments.
Not linking to LibM when building under MSVC as the math.h implementations are already available in the msvcrt*.dll. (Does torch need the LibM or just some math.h implementation?)",True,{}
torch/torch7,https://github.com/torch/torch7,660,2016-04-26T08:26:46Z,2016-06-05T23:41:24Z,2016-06-05T23:41:24Z,CLOSED,False,26,27,5,https://github.com/Ark-kun,Fixed the Visual Studio CMake build.,1,[],https://github.com/torch/torch7/pull/660,https://github.com/soumith,2,https://github.com/torch/torch7/pull/660#issuecomment-223845082,"Added missing CMake function include.
snprintf returns int, not ssize_t. ssize_t is not standard C++.
Added arguments for the variadic macro parameters. (We should probably remove the variadic parameter from the macro as it's never used)
Avoided preprocessor directives inside macro arguments.
Not linking to LibM when building under MSVC as the math.h implementations are already available in the msvcrt*.dll. (Does torch need the LibM or just some math.h implementation?)",fixed via #694,True,{}
torch/torch7,https://github.com/torch/torch7,662,2016-04-27T19:40:55Z,2016-04-27T19:49:18Z,2016-04-27T19:49:22Z,MERGED,True,2,2,1,https://github.com/fbesse,Fixed torch.equal entry in the doc,1,[],https://github.com/torch/torch7/pull/662,https://github.com/fbesse,1,https://github.com/torch/torch7/pull/662,,,True,{}
torch/torch7,https://github.com/torch/torch7,662,2016-04-27T19:40:55Z,2016-04-27T19:49:18Z,2016-04-27T19:49:22Z,MERGED,True,2,2,1,https://github.com/fbesse,Fixed torch.equal entry in the doc,1,[],https://github.com/torch/torch7/pull/662,https://github.com/soumith,2,https://github.com/torch/torch7/pull/662#issuecomment-215207401,,Thanks!,True,{}
torch/torch7,https://github.com/torch/torch7,668,2016-05-01T14:18:01Z,2016-05-06T15:00:54Z,2016-05-06T15:00:54Z,MERGED,True,7,0,1,https://github.com/szagoruyko,Add OS X to travis builds,3,[],https://github.com/torch/torch7/pull/668,https://github.com/szagoruyko,1,https://github.com/torch/torch7/pull/668,"Adds one OS X job, luajit2.1 with clang.","Adds one OS X job, luajit2.1 with clang.",True,{}
torch/torch7,https://github.com/torch/torch7,672,2016-05-04T07:49:17Z,2016-05-04T14:03:01Z,2016-05-04T15:45:23Z,MERGED,True,2,2,1,https://github.com/fmassa,Fix addmm for non-contiguous result tensor,1,[],https://github.com/torch/torch7/pull/672,https://github.com/fmassa,1,https://github.com/torch/torch7/pull/672,"addmm didn't support properly non-contiguous result tensors.
The goal of this part of the code is to copy the contents of r_ into r__ such that r__ contains the transpose of r_ contiguous in memory.
The way it was doing was by considering c_inv = torch.Tensor(c:size(2),c:size(1)):copy(c), which doesn't transpose the tensor but only change its shape.
This problem was introduced in 0d534e6 .
I'll add more unit tests later if you want.
Should fix #671
Also, something similar should be added to cutorch.","addmm didn't support properly non-contiguous result tensors.
The goal of this part of the code is to copy the contents of r_ into r__ such that r__ contains the transpose of r_ contiguous in memory.
The way it was doing was by considering c_inv = torch.Tensor(c:size(2),c:size(1)):copy(c), which doesn't transpose the tensor but only change its shape.
This problem was introduced in 0d534e6 .
I'll add more unit tests later if you want.
Should fix #671
Also, something similar should be added to cutorch.",True,{}
torch/torch7,https://github.com/torch/torch7,672,2016-05-04T07:49:17Z,2016-05-04T14:03:01Z,2016-05-04T15:45:23Z,MERGED,True,2,2,1,https://github.com/fmassa,Fix addmm for non-contiguous result tensor,1,[],https://github.com/torch/torch7/pull/672,https://github.com/soumith,2,https://github.com/torch/torch7/pull/672#issuecomment-216874928,"addmm didn't support properly non-contiguous result tensors.
The goal of this part of the code is to copy the contents of r_ into r__ such that r__ contains the transpose of r_ contiguous in memory.
The way it was doing was by considering c_inv = torch.Tensor(c:size(2),c:size(1)):copy(c), which doesn't transpose the tensor but only change its shape.
This problem was introduced in 0d534e6 .
I'll add more unit tests later if you want.
Should fix #671
Also, something similar should be added to cutorch.",Thanks Francisco!,True,{}
torch/torch7,https://github.com/torch/torch7,672,2016-05-04T07:49:17Z,2016-05-04T14:03:01Z,2016-05-04T15:45:23Z,MERGED,True,2,2,1,https://github.com/fmassa,Fix addmm for non-contiguous result tensor,1,[],https://github.com/torch/torch7/pull/672,https://github.com/JeremyMorvan,3,https://github.com/torch/torch7/pull/672#issuecomment-216875415,"addmm didn't support properly non-contiguous result tensors.
The goal of this part of the code is to copy the contents of r_ into r__ such that r__ contains the transpose of r_ contiguous in memory.
The way it was doing was by considering c_inv = torch.Tensor(c:size(2),c:size(1)):copy(c), which doesn't transpose the tensor but only change its shape.
This problem was introduced in 0d534e6 .
I'll add more unit tests later if you want.
Should fix #671
Also, something similar should be added to cutorch.",Thank you !,True,{}
torch/torch7,https://github.com/torch/torch7,673,2016-05-04T12:38:49Z,2016-05-04T13:59:19Z,2016-05-04T14:33:28Z,MERGED,True,1,1,1,https://github.com/tkoeppe,"[TensorOperator.c] Make unary negation expression have real type, not double",1,[],https://github.com/torch/torch7/pull/673,https://github.com/tkoeppe,1,https://github.com/torch/torch7/pull/673,"Before this change, the expression -luaL_checknumber(L, ...) has type double, and its value may not be representable as real, leading to undefined behaviour. After this change, the unary negation is applied to a value of type real, and the arithmetic promotion and conversion rules give this well-defined behaviour.","Before this change, the expression -luaL_checknumber(L, ...) has type double, and its value may not be representable as real, leading to undefined behaviour. After this change, the unary negation is applied to a value of type real, and the arithmetic promotion and conversion rules give this well-defined behaviour.",True,{}
torch/torch7,https://github.com/torch/torch7,673,2016-05-04T12:38:49Z,2016-05-04T13:59:19Z,2016-05-04T14:33:28Z,MERGED,True,1,1,1,https://github.com/tkoeppe,"[TensorOperator.c] Make unary negation expression have real type, not double",1,[],https://github.com/torch/torch7/pull/673,https://github.com/soumith,2,https://github.com/torch/torch7/pull/673#issuecomment-216873857,"Before this change, the expression -luaL_checknumber(L, ...) has type double, and its value may not be representable as real, leading to undefined behaviour. After this change, the unary negation is applied to a value of type real, and the arithmetic promotion and conversion rules give this well-defined behaviour.",Thanks!,True,{}
torch/torch7,https://github.com/torch/torch7,675,2016-05-06T23:07:48Z,2016-05-07T02:09:49Z,2016-05-07T02:10:07Z,MERGED,True,1,0,1,https://github.com/fmassa,Fix memory leak in addmm,1,[],https://github.com/torch/torch7/pull/675,https://github.com/fmassa,1,https://github.com/torch/torch7/pull/675,"I'm really sorry about that, but I think I introduced a memory leak in #672
This should address the leak, but please someone double check in case I'm missing something.","I'm really sorry about that, but I think I introduced a memory leak in #672
This should address the leak, but please someone double check in case I'm missing something.",True,{}
torch/torch7,https://github.com/torch/torch7,675,2016-05-06T23:07:48Z,2016-05-07T02:09:49Z,2016-05-07T02:10:07Z,MERGED,True,1,0,1,https://github.com/fmassa,Fix memory leak in addmm,1,[],https://github.com/torch/torch7/pull/675,https://github.com/soumith,2,https://github.com/torch/torch7/pull/675#issuecomment-217599086,"I'm really sorry about that, but I think I introduced a memory leak in #672
This should address the leak, but please someone double check in case I'm missing something.",Thanks Francisco!,True,{}
torch/torch7,https://github.com/torch/torch7,676,2016-05-07T12:01:56Z,,2017-03-23T03:07:44Z,OPEN,False,13,0,3,https://github.com/fmassa,Add function to get the amount of memory used by torch tensors,1,[],https://github.com/torch/torch7/pull/676,https://github.com/fmassa,1,https://github.com/torch/torch7/pull/676,"Allows to get the amount of memory used by torch tensors.
In conjunction with collectgarbage('count'), we can have some good estimates of the total amount of memory used (except if other data structures are used, like tds).
Note: as heapSize is only updated by batches of ~ 1MB, so creating really small tensors won't show any immediate difference.
I will add docs if we agree to merge this.","Allows to get the amount of memory used by torch tensors.
In conjunction with collectgarbage('count'), we can have some good estimates of the total amount of memory used (except if other data structures are used, like tds).
Note: as heapSize is only updated by batches of ~ 1MB, so creating really small tensors won't show any immediate difference.
I will add docs if we agree to merge this.",True,"{'THUMBS_UP': ['https://github.com/apaszke', 'https://github.com/alykhantejani']}"
torch/torch7,https://github.com/torch/torch7,676,2016-05-07T12:01:56Z,,2017-03-23T03:07:44Z,OPEN,False,13,0,3,https://github.com/fmassa,Add function to get the amount of memory used by torch tensors,1,[],https://github.com/torch/torch7/pull/676,https://github.com/fmassa,2,https://github.com/torch/torch7/pull/676#issuecomment-217641557,"Allows to get the amount of memory used by torch tensors.
In conjunction with collectgarbage('count'), we can have some good estimates of the total amount of memory used (except if other data structures are used, like tds).
Note: as heapSize is only updated by batches of ~ 1MB, so creating really small tensors won't show any immediate difference.
I will add docs if we agree to merge this.","Also, I could add heapDelta to heapSize to have a better estimate of the count of memory.",True,{}
torch/torch7,https://github.com/torch/torch7,677,2016-05-07T15:48:30Z,2016-05-07T15:49:05Z,2016-05-07T15:49:11Z,MERGED,True,5,1,1,https://github.com/tpltnt,FreeBSD port,1,[],https://github.com/torch/torch7/pull/677,https://github.com/tpltnt,1,https://github.com/torch/torch7/pull/677,"Hi there,
i made torch compile and run on FreeBSD.
Cheers,
tpltnt","Hi there,
i made torch compile and run on FreeBSD.
Cheers,
tpltnt",True,{}
torch/torch7,https://github.com/torch/torch7,677,2016-05-07T15:48:30Z,2016-05-07T15:49:05Z,2016-05-07T15:49:11Z,MERGED,True,5,1,1,https://github.com/tpltnt,FreeBSD port,1,[],https://github.com/torch/torch7/pull/677,https://github.com/soumith,2,https://github.com/torch/torch7/pull/677#issuecomment-217646293,"Hi there,
i made torch compile and run on FreeBSD.
Cheers,
tpltnt",thanks!,True,{}
torch/torch7,https://github.com/torch/torch7,680,2016-05-13T09:29:02Z,2016-05-13T14:41:52Z,2016-05-13T14:41:55Z,MERGED,True,21,0,2,https://github.com/lake4790k,added THAtomicSetLong,1,[],https://github.com/torch/torch7/pull/680,https://github.com/lake4790k,1,https://github.com/torch/torch7/pull/680,"Added the missing long version for atomic set. I'm adding an atomic counter in tds and adding support for that in threads, this is needed for that.","Added the missing long version for atomic set. I'm adding an atomic counter in tds and adding support for that in threads, this is needed for that.",True,{}
torch/torch7,https://github.com/torch/torch7,680,2016-05-13T09:29:02Z,2016-05-13T14:41:52Z,2016-05-13T14:41:55Z,MERGED,True,21,0,2,https://github.com/lake4790k,added THAtomicSetLong,1,[],https://github.com/torch/torch7/pull/680,https://github.com/soumith,2,https://github.com/torch/torch7/pull/680#issuecomment-219063152,"Added the missing long version for atomic set. I'm adding an atomic counter in tds and adding support for that in threads, this is needed for that.",Thanks!,True,{}
torch/torch7,https://github.com/torch/torch7,686,2016-05-18T14:11:27Z,2016-05-18T14:47:58Z,2016-05-18T14:48:07Z,MERGED,True,2,1,1,https://github.com/davidsaxton,torch.Tester early abort and disable: make them work when used together,1,[],https://github.com/torch/torch7/pull/686,https://github.com/davidsaxton,1,https://github.com/torch/torch7/pull/686,"Previously: if early abort was enabled, then it would abort on any disabled tests.","Previously: if early abort was enabled, then it would abort on any disabled tests.",True,{}
torch/torch7,https://github.com/torch/torch7,686,2016-05-18T14:11:27Z,2016-05-18T14:47:58Z,2016-05-18T14:48:07Z,MERGED,True,2,1,1,https://github.com/davidsaxton,torch.Tester early abort and disable: make them work when used together,1,[],https://github.com/torch/torch7/pull/686,https://github.com/soumith,2,https://github.com/torch/torch7/pull/686#issuecomment-220049939,"Previously: if early abort was enabled, then it would abort on any disabled tests.",Thanks david!,True,{}
torch/torch7,https://github.com/torch/torch7,690,2016-05-25T14:38:12Z,2016-05-25T15:33:40Z,2016-05-25T15:33:40Z,MERGED,True,3,2,2,https://github.com/cpuhrsch,"Make CmdLine:log check whether file is a file descriptor, before creating a new file",2,[],https://github.com/torch/torch7/pull/690,https://github.com/cpuhrsch,1,https://github.com/torch/torch7/pull/690,Allows the user to provide his own file (for example in a different mode or from a different source).,Allows the user to provide his own file (for example in a different mode or from a different source).,True,{}
torch/torch7,https://github.com/torch/torch7,694,2016-05-31T17:57:51Z,2016-06-01T20:30:03Z,2016-06-01T20:30:08Z,MERGED,True,34,28,5,https://github.com/elikosan,fix compilation issues under Windows,9,[],https://github.com/torch/torch7/pull/694,https://github.com/elikosan,1,https://github.com/torch/torch7/pull/694,"With these fixes i can install torch7 with luarocks on Windows.
Compiler: Visual Studio 2015
OS: Windows 7 Pro","With these fixes i can install torch7 with luarocks on Windows.
Compiler: Visual Studio 2015
OS: Windows 7 Pro",True,{'THUMBS_UP': ['https://github.com/hohoCode']}
torch/torch7,https://github.com/torch/torch7,694,2016-05-31T17:57:51Z,2016-06-01T20:30:03Z,2016-06-01T20:30:08Z,MERGED,True,34,28,5,https://github.com/elikosan,fix compilation issues under Windows,9,[],https://github.com/torch/torch7/pull/694,https://github.com/fmassa,2,https://github.com/torch/torch7/pull/694#issuecomment-222808451,"With these fixes i can install torch7 with luarocks on Windows.
Compiler: Visual Studio 2015
OS: Windows 7 Pro","Thanks for the fixes!
This seems quite similar to #660 though.
@soumith maybe its time to merge one of the PRs ?",True,{}
torch/torch7,https://github.com/torch/torch7,694,2016-05-31T17:57:51Z,2016-06-01T20:30:03Z,2016-06-01T20:30:08Z,MERGED,True,34,28,5,https://github.com/elikosan,fix compilation issues under Windows,9,[],https://github.com/torch/torch7/pull/694,https://github.com/soumith,3,https://github.com/torch/torch7/pull/694#issuecomment-223101781,"With these fixes i can install torch7 with luarocks on Windows.
Compiler: Visual Studio 2015
OS: Windows 7 Pro",This looks better than #660 but fails contbuilds. made in-line comments.,True,{}
torch/torch7,https://github.com/torch/torch7,694,2016-05-31T17:57:51Z,2016-06-01T20:30:03Z,2016-06-01T20:30:08Z,MERGED,True,34,28,5,https://github.com/elikosan,fix compilation issues under Windows,9,[],https://github.com/torch/torch7/pull/694,https://github.com/elikosan,4,https://github.com/torch/torch7/pull/694#issuecomment-223110996,"With these fixes i can install torch7 with luarocks on Windows.
Compiler: Visual Studio 2015
OS: Windows 7 Pro",I just committed a fix to the ssize_t issue. Let's see if Travis likes it better...,True,{}
torch/torch7,https://github.com/torch/torch7,694,2016-05-31T17:57:51Z,2016-06-01T20:30:03Z,2016-06-01T20:30:08Z,MERGED,True,34,28,5,https://github.com/elikosan,fix compilation issues under Windows,9,[],https://github.com/torch/torch7/pull/694,https://github.com/soumith,5,https://github.com/torch/torch7/pull/694#issuecomment-223115091,"With these fixes i can install torch7 with luarocks on Windows.
Compiler: Visual Studio 2015
OS: Windows 7 Pro",Thanks! :),True,{}
torch/torch7,https://github.com/torch/torch7,695,2016-06-04T18:10:00Z,2016-06-04T18:36:37Z,2016-06-05T09:13:49Z,MERGED,True,1,0,1,https://github.com/deltheil,"fix ""macro redefined"" warning (th_isnan)",1,[],https://github.com/torch/torch7/pull/695,https://github.com/deltheil,1,https://github.com/torch/torch7/pull/695,See #694,See #694,True,{}
torch/torch7,https://github.com/torch/torch7,695,2016-06-04T18:10:00Z,2016-06-04T18:36:37Z,2016-06-05T09:13:49Z,MERGED,True,1,0,1,https://github.com/deltheil,"fix ""macro redefined"" warning (th_isnan)",1,[],https://github.com/torch/torch7/pull/695,https://github.com/soumith,2,https://github.com/torch/torch7/pull/695#issuecomment-223771060,See #694,"thanks, was about to do it myself.",True,{}
torch/torch7,https://github.com/torch/torch7,698,2016-06-09T18:33:24Z,2016-06-09T19:37:12Z,2016-06-09T19:37:16Z,MERGED,True,3,3,1,https://github.com/lanpa,Update maths.md,1,[],https://github.com/torch/torch7/pull/698,https://github.com/lanpa,1,https://github.com/torch/torch7/pull/698,add missing comma to optional argument.,add missing comma to optional argument.,True,{}
torch/torch7,https://github.com/torch/torch7,698,2016-06-09T18:33:24Z,2016-06-09T19:37:12Z,2016-06-09T19:37:16Z,MERGED,True,3,3,1,https://github.com/lanpa,Update maths.md,1,[],https://github.com/torch/torch7/pull/698,https://github.com/soumith,2,https://github.com/torch/torch7/pull/698#issuecomment-225003195,add missing comma to optional argument.,Thanks!,True,{}
torch/torch7,https://github.com/torch/torch7,700,2016-06-13T16:36:51Z,2016-06-13T17:22:14Z,2016-06-13T17:22:20Z,MERGED,True,3,0,1,https://github.com/colesbury,Enable heap tracking by default in torch,1,[],https://github.com/torch/torch7/pull/700,https://github.com/colesbury,1,https://github.com/torch/torch7/pull/700,,,True,{}
torch/torch7,https://github.com/torch/torch7,700,2016-06-13T16:36:51Z,2016-06-13T17:22:14Z,2016-06-13T17:22:20Z,MERGED,True,3,0,1,https://github.com/colesbury,Enable heap tracking by default in torch,1,[],https://github.com/torch/torch7/pull/700,https://github.com/soumith,2,https://github.com/torch/torch7/pull/700#issuecomment-225649281,,cc: @clementfarabet,True,{}
torch/torch7,https://github.com/torch/torch7,710,2016-07-05T18:46:10Z,2016-07-05T18:56:42Z,2016-07-05T20:58:57Z,CLOSED,False,51,32,1,https://github.com/bonanzhang,optimization,1,[],https://github.com/torch/torch7/pull/710,https://github.com/bonanzhang,1,https://github.com/torch/torch7/pull/710,optimized for matrix matrix multiplication with size: [AxB] * [Bx2],optimized for matrix matrix multiplication with size: [AxB] * [Bx2],True,{}
torch/torch7,https://github.com/torch/torch7,712,2016-07-07T15:05:19Z,2016-07-27T06:22:40Z,2016-07-27T06:22:46Z,MERGED,True,3,1,1,https://github.com/achalddave,"Document vec:addmv(scalar, mat, vec2) call",1,[],https://github.com/torch/torch7/pull/712,https://github.com/achalddave,1,https://github.com/torch/torch7/pull/712,,,True,{}
torch/torch7,https://github.com/torch/torch7,712,2016-07-07T15:05:19Z,2016-07-27T06:22:40Z,2016-07-27T06:22:46Z,MERGED,True,3,1,1,https://github.com/achalddave,"Document vec:addmv(scalar, mat, vec2) call",1,[],https://github.com/torch/torch7/pull/712,https://github.com/soumith,2,https://github.com/torch/torch7/pull/712#issuecomment-235497472,,Thanks man!,True,{}
torch/torch7,https://github.com/torch/torch7,720,2016-07-20T19:40:27Z,2016-07-20T20:08:20Z,2016-07-26T16:36:32Z,MERGED,True,53,47,3,https://github.com/apaszke,Make TH more usable and consistent,2,[],https://github.com/torch/torch7/pull/720,https://github.com/apaszke,1,https://github.com/torch/torch7/pull/720,,,True,{}
torch/torch7,https://github.com/torch/torch7,720,2016-07-20T19:40:27Z,2016-07-20T20:08:20Z,2016-07-26T16:36:32Z,MERGED,True,53,47,3,https://github.com/apaszke,Make TH more usable and consistent,2,[],https://github.com/torch/torch7/pull/720,https://github.com/soumith,2,https://github.com/torch/torch7/pull/720#issuecomment-234066363,,LGTM! thanks!,True,{}
torch/torch7,https://github.com/torch/torch7,720,2016-07-20T19:40:27Z,2016-07-20T20:08:20Z,2016-07-26T16:36:32Z,MERGED,True,53,47,3,https://github.com/apaszke,Make TH more usable and consistent,2,[],https://github.com/torch/torch7/pull/720,https://github.com/ghostcow,3,https://github.com/torch/torch7/pull/720#issuecomment-235280980,,what does this mean for everyday usage?,True,{}
torch/torch7,https://github.com/torch/torch7,720,2016-07-20T19:40:27Z,2016-07-20T20:08:20Z,2016-07-26T16:36:32Z,MERGED,True,53,47,3,https://github.com/apaszke,Make TH more usable and consistent,2,[],https://github.com/torch/torch7/pull/720,https://github.com/fmassa,4,https://github.com/torch/torch7/pull/720#issuecomment-235326878,,"You can more easily use it in standalone C applications, because you don't need to convert the returned indices from 1-based to 0-based at every function call.
But if you only use the lua interface, it doesn't change anything.
BTW, @apaszke , do you plan do do the same for cutorch?",True,"{'THUMBS_UP': ['https://github.com/ghostcow', 'https://github.com/soumith']}"
torch/torch7,https://github.com/torch/torch7,723,2016-07-25T21:08:44Z,2016-07-27T04:07:45Z,2016-07-27T04:07:54Z,MERGED,True,6,18,1,https://github.com/andreaskoepf,Fix torch.histc problem #718,1,[],https://github.com/torch/torch7/pull/723,https://github.com/andreaskoepf,1,https://github.com/torch/torch7/pull/723,No longer cloning input tensor nor using tensor operations but computing destination bin index directly in TH_TENSOR_APPLY() block and ensuring that bin index does not exceed nbins-1.,No longer cloning input tensor nor using tensor operations but computing destination bin index directly in TH_TENSOR_APPLY() block and ensuring that bin index does not exceed nbins-1.,True,{'THUMBS_UP': ['https://github.com/fmassa']}
torch/torch7,https://github.com/torch/torch7,723,2016-07-25T21:08:44Z,2016-07-27T04:07:45Z,2016-07-27T04:07:54Z,MERGED,True,6,18,1,https://github.com/andreaskoepf,Fix torch.histc problem #718,1,[],https://github.com/torch/torch7/pull/723,https://github.com/soumith,2,https://github.com/torch/torch7/pull/723#issuecomment-235479018,No longer cloning input tensor nor using tensor operations but computing destination bin index directly in TH_TENSOR_APPLY() block and ensuring that bin index does not exceed nbins-1.,Thanks a lot Andreas!,True,{}
torch/torch7,https://github.com/torch/torch7,726,2016-07-30T00:30:23Z,2016-07-30T00:30:27Z,2016-07-30T00:30:29Z,MERGED,True,1,1,1,https://github.com/soumith,fixing division / to be consistent with true division and not mul(1/a),1,[],https://github.com/torch/torch7/pull/726,https://github.com/soumith,1,https://github.com/torch/torch7/pull/726,fixes #665,fixes #665,True,{}
torch/torch7,https://github.com/torch/torch7,729,2016-08-03T03:57:44Z,2016-08-03T03:57:47Z,2016-08-03T03:57:56Z,MERGED,True,20,5,1,https://github.com/soumith,fix gemm bug when beta = 0,1,[],https://github.com/torch/torch7/pull/729,https://github.com/soumith,1,https://github.com/torch/torch7/pull/729,See: torch/nn#898,See: torch/nn#898,True,{}
torch/torch7,https://github.com/torch/torch7,729,2016-08-03T03:57:44Z,2016-08-03T03:57:47Z,2016-08-03T03:57:56Z,MERGED,True,20,5,1,https://github.com/soumith,fix gemm bug when beta = 0,1,[],https://github.com/torch/torch7/pull/729,https://github.com/soumith,2,https://github.com/torch/torch7/pull/729#issuecomment-237130201,See: torch/nn#898,cc: @apaszke,True,{}
torch/torch7,https://github.com/torch/torch7,730,2016-08-03T19:47:37Z,2016-08-03T19:48:44Z,2016-08-03T19:48:49Z,MERGED,True,13,2,1,https://github.com/soumith,fixing a truncation bug when numbers are given as strings,1,[],https://github.com/torch/torch7/pull/730,https://github.com/soumith,1,https://github.com/torch/torch7/pull/730,"Fixes #706
t = torch.LongTensor(1)
str = '10151188881778163'
t[1] = str
print(str)
print(t:data()[0])
Output:
10151188881778163
10151188881778163LL","Fixes #706
t = torch.LongTensor(1)
str = '10151188881778163'
t[1] = str
print(str)
print(t:data()[0])
Output:
10151188881778163
10151188881778163LL",True,{}
torch/torch7,https://github.com/torch/torch7,733,2016-08-08T14:35:52Z,2016-08-08T14:37:36Z,2016-08-08T14:38:39Z,MERGED,True,4,0,1,https://github.com/cdluminate,cmake: add soversion for luaT,1,[],https://github.com/torch/torch7/pull/733,https://github.com/cdluminate,1,https://github.com/torch/torch7/pull/733,"Imported from Debian package.
https://anonscm.debian.org/cgit/debian-science/packages/lua-torch-torch7.git/tree/debian/patches/luaT-cmake-add-version","Imported from Debian package.
https://anonscm.debian.org/cgit/debian-science/packages/lua-torch-torch7.git/tree/debian/patches/luaT-cmake-add-version",True,{}
torch/torch7,https://github.com/torch/torch7,733,2016-08-08T14:35:52Z,2016-08-08T14:37:36Z,2016-08-08T14:38:39Z,MERGED,True,4,0,1,https://github.com/cdluminate,cmake: add soversion for luaT,1,[],https://github.com/torch/torch7/pull/733,https://github.com/soumith,2,https://github.com/torch/torch7/pull/733#issuecomment-238257737,"Imported from Debian package.
https://anonscm.debian.org/cgit/debian-science/packages/lua-torch-torch7.git/tree/debian/patches/luaT-cmake-add-version",Thanks!,True,{}
torch/torch7,https://github.com/torch/torch7,733,2016-08-08T14:35:52Z,2016-08-08T14:37:36Z,2016-08-08T14:38:39Z,MERGED,True,4,0,1,https://github.com/cdluminate,cmake: add soversion for luaT,1,[],https://github.com/torch/torch7/pull/733,https://github.com/cdluminate,3,https://github.com/torch/torch7/pull/733#issuecomment-238258033,"Imported from Debian package.
https://anonscm.debian.org/cgit/debian-science/packages/lua-torch-torch7.git/tree/debian/patches/luaT-cmake-add-version",Amazingly fast!!,True,{}
torch/torch7,https://github.com/torch/torch7,736,2016-08-10T22:20:43Z,2016-08-10T22:22:23Z,2016-08-10T22:22:28Z,MERGED,True,5,9,1,https://github.com/apaszke,Fix tril and logical operations when input == result,1,[],https://github.com/torch/torch7/pull/736,https://github.com/apaszke,1,https://github.com/torch/torch7/pull/736,,,True,{}
torch/torch7,https://github.com/torch/torch7,736,2016-08-10T22:20:43Z,2016-08-10T22:22:23Z,2016-08-10T22:22:28Z,MERGED,True,5,9,1,https://github.com/apaszke,Fix tril and logical operations when input == result,1,[],https://github.com/torch/torch7/pull/736,https://github.com/soumith,2,https://github.com/torch/torch7/pull/736#issuecomment-239023160,,Thanks!,True,{}
torch/torch7,https://github.com/torch/torch7,738,2016-08-11T12:09:24Z,2016-08-12T14:55:20Z,2016-08-12T14:55:24Z,MERGED,True,4,0,1,https://github.com/cdluminate,cmake: add soversion for libTH,1,[],https://github.com/torch/torch7/pull/738,https://github.com/cdluminate,1,https://github.com/torch/torch7/pull/738,Imported from Debian package.,Imported from Debian package.,True,{}
torch/torch7,https://github.com/torch/torch7,738,2016-08-11T12:09:24Z,2016-08-12T14:55:20Z,2016-08-12T14:55:24Z,MERGED,True,4,0,1,https://github.com/cdluminate,cmake: add soversion for libTH,1,[],https://github.com/torch/torch7/pull/738,https://github.com/cdluminate,2,https://github.com/torch/torch7/pull/738#issuecomment-239143491,Imported from Debian package.,"This is similar to #733
Original patch can be found here
https://anonscm.debian.org/cgit/debian-science/packages/lua-torch-torch7.git/tree/debian/patches/TH-cmake-add-version
luaT and TH both have soversion now.",True,{}
torch/torch7,https://github.com/torch/torch7,738,2016-08-11T12:09:24Z,2016-08-12T14:55:20Z,2016-08-12T14:55:24Z,MERGED,True,4,0,1,https://github.com/cdluminate,cmake: add soversion for libTH,1,[],https://github.com/torch/torch7/pull/738,https://github.com/cdluminate,3,https://github.com/torch/torch7/pull/738#issuecomment-239143804,Imported from Debian package.,Adding soversion to these shared libraries brings convenience to distributions.,True,{}
torch/torch7,https://github.com/torch/torch7,738,2016-08-11T12:09:24Z,2016-08-12T14:55:20Z,2016-08-12T14:55:24Z,MERGED,True,4,0,1,https://github.com/cdluminate,cmake: add soversion for libTH,1,[],https://github.com/torch/torch7/pull/738,https://github.com/soumith,4,https://github.com/torch/torch7/pull/738#issuecomment-239468626,Imported from Debian package.,Thanks!,True,{}
torch/torch7,https://github.com/torch/torch7,739,2016-08-11T12:28:24Z,2016-08-11T13:37:50Z,2016-08-11T13:37:59Z,MERGED,True,7,7,7,https://github.com/cdluminate,fix several spelling errors,1,[],https://github.com/torch/torch7/pull/739,https://github.com/cdluminate,1,https://github.com/torch/torch7/pull/739,,,True,{}
torch/torch7,https://github.com/torch/torch7,739,2016-08-11T12:28:24Z,2016-08-11T13:37:50Z,2016-08-11T13:37:59Z,MERGED,True,7,7,7,https://github.com/cdluminate,fix several spelling errors,1,[],https://github.com/torch/torch7/pull/739,https://github.com/cdluminate,2,https://github.com/torch/torch7/pull/739#issuecomment-239147444,,"This changed is automatically done by tool codespell.
However I can't make sure the second change in the collowing patch is really a typo:
diff --git a/lib/luaT/luaT.c b/lib/luaT/luaT.c
index 657cca2..9031c5a 100644
--- a/lib/luaT/luaT.c
+++ b/lib/luaT/luaT.c
@@ -552,7 +552,7 @@ int luaT_fullparentname(const char *tname, char *parent_name)
   return tname[idx] == '.';
 }

-/* alias for ensuring backwards compatibilty;
+/* alias for ensuring backwards compatibility;
  * use of luaT_fullparentname is preferred.
  */
 int luaT_classmodulename(const char *tname, char *parent_name)
@@ -1228,7 +1228,7 @@ static int luaT_mt__newindex(lua_State *L)
             luaT_typename(L, 1), luaT_typename(L,2))

 MT_DECLARE_BIN_OPERATOR(add,    BIN_OPERATOR_ERROR(addition) )
-MT_DECLARE_BIN_OPERATOR(sub,    BIN_OPERATOR_ERROR(substraction) )
+MT_DECLARE_BIN_OPERATOR(sub,    BIN_OPERATOR_ERROR(subtraction) )
 MT_DECLARE_BIN_OPERATOR(mul,    BIN_OPERATOR_ERROR(multiplication) )
 MT_DECLARE_BIN_OPERATOR(div,    BIN_OPERATOR_ERROR(division) )
 MT_DECLARE_BIN_OPERATOR(mod,    BIN_OPERATOR_ERROR(modulo) )
that is ./lib/luaT/luaT.c:1231: substraction  ==> subtraction
Please confirm it.",True,{}
torch/torch7,https://github.com/torch/torch7,739,2016-08-11T12:28:24Z,2016-08-11T13:37:50Z,2016-08-11T13:37:59Z,MERGED,True,7,7,7,https://github.com/cdluminate,fix several spelling errors,1,[],https://github.com/torch/torch7/pull/739,https://github.com/soumith,3,https://github.com/torch/torch7/pull/739#issuecomment-239162924,,Thanks!,True,{}
torch/torch7,https://github.com/torch/torch7,745,2016-08-23T12:34:08Z,2016-08-23T13:53:56Z,2016-08-23T13:54:02Z,MERGED,True,5,5,1,https://github.com/temerick,clean up equations in documentation,1,[],https://github.com/torch/torch7/pull/745,https://github.com/temerick,1,https://github.com/torch/torch7/pull/745,No new content. Just standardizing the formatting of the numbers and equations in the random docs.,No new content. Just standardizing the formatting of the numbers and equations in the random docs.,True,{}
torch/torch7,https://github.com/torch/torch7,745,2016-08-23T12:34:08Z,2016-08-23T13:53:56Z,2016-08-23T13:54:02Z,MERGED,True,5,5,1,https://github.com/temerick,clean up equations in documentation,1,[],https://github.com/torch/torch7/pull/745,https://github.com/soumith,2,https://github.com/torch/torch7/pull/745#issuecomment-241738285,No new content. Just standardizing the formatting of the numbers and equations in the random docs.,Thanks!,True,{}
torch/torch7,https://github.com/torch/torch7,746,2016-08-23T15:13:58Z,2016-08-23T15:29:07Z,2016-08-23T15:44:35Z,MERGED,True,6,1,1,https://github.com/cdluminate,test: fix 32-bit system abs test failure.,1,[],https://github.com/torch/torch7/pull/746,https://github.com/cdluminate,1,https://github.com/torch/torch7/pull/746,According to #627,According to #627,True,{}
torch/torch7,https://github.com/torch/torch7,746,2016-08-23T15:13:58Z,2016-08-23T15:29:07Z,2016-08-23T15:44:35Z,MERGED,True,6,1,1,https://github.com/cdluminate,test: fix 32-bit system abs test failure.,1,[],https://github.com/torch/torch7/pull/746,https://github.com/cdluminate,2,https://github.com/torch/torch7/pull/746#issuecomment-241765079,According to #627,This closes issue #627,True,{}
torch/torch7,https://github.com/torch/torch7,746,2016-08-23T15:13:58Z,2016-08-23T15:29:07Z,2016-08-23T15:44:35Z,MERGED,True,6,1,1,https://github.com/cdluminate,test: fix 32-bit system abs test failure.,1,[],https://github.com/torch/torch7/pull/746,https://github.com/soumith,3,https://github.com/torch/torch7/pull/746#issuecomment-241770849,According to #627,Thanks @cdluminate,True,{}
torch/torch7,https://github.com/torch/torch7,746,2016-08-23T15:13:58Z,2016-08-23T15:29:07Z,2016-08-23T15:44:35Z,MERGED,True,6,1,1,https://github.com/cdluminate,test: fix 32-bit system abs test failure.,1,[],https://github.com/torch/torch7/pull/746,https://github.com/cdluminate,4,https://github.com/torch/torch7/pull/746#issuecomment-241777717,According to #627,"@soumith
I'm impressed by your merge speed, really.",True,{}
torch/torch7,https://github.com/torch/torch7,748,2016-08-24T23:25:35Z,2016-08-27T14:44:19Z,2016-08-27T14:44:25Z,MERGED,True,2,2,1,https://github.com/soumith,changing function load errors to warnings,1,[],https://github.com/torch/torch7/pull/748,https://github.com/soumith,1,https://github.com/torch/torch7/pull/748,"This seems like a needed change, now that we have LuaJIT 2.1, 2.0, Lua 5.2 and soon Lua 5.3 used by torch users.
The models which are serialized usually have no functions, but occasionally have some functions which usually are not needed.
Fixes #641 and #741","This seems like a needed change, now that we have LuaJIT 2.1, 2.0, Lua 5.2 and soon Lua 5.3 used by torch users.
The models which are serialized usually have no functions, but occasionally have some functions which usually are not needed.
Fixes #641 and #741",True,{}
torch/torch7,https://github.com/torch/torch7,748,2016-08-24T23:25:35Z,2016-08-27T14:44:19Z,2016-08-27T14:44:25Z,MERGED,True,2,2,1,https://github.com/soumith,changing function load errors to warnings,1,[],https://github.com/torch/torch7/pull/748,https://github.com/soumith,2,https://github.com/torch/torch7/pull/748#issuecomment-242239127,"This seems like a needed change, now that we have LuaJIT 2.1, 2.0, Lua 5.2 and soon Lua 5.3 used by torch users.
The models which are serialized usually have no functions, but occasionally have some functions which usually are not needed.
Fixes #641 and #741",@szagoruyko @fmassa what do you think?,True,{}
torch/torch7,https://github.com/torch/torch7,748,2016-08-24T23:25:35Z,2016-08-27T14:44:19Z,2016-08-27T14:44:25Z,MERGED,True,2,2,1,https://github.com/soumith,changing function load errors to warnings,1,[],https://github.com/torch/torch7/pull/748,https://github.com/fmassa,3,https://github.com/torch/torch7/pull/748#issuecomment-242242098,"This seems like a needed change, now that we have LuaJIT 2.1, 2.0, Lua 5.2 and soon Lua 5.3 used by torch users.
The models which are serialized usually have no functions, but occasionally have some functions which usually are not needed.
Fixes #641 and #741","In general, I'm ok with this change.
But this might also lead to subtle bugs in some cases.
For example, in optnet, I overwrite the backward pass with a function which raises an error if the network was optimized for inference (to avoid computing wrong gradients). With this change, a network saved in inference mode could potentially be deserialized without this error function, leading to completely wrong gradients without any note or warning that the gradients might be wrong.
Maybe the best would be to change optnet to avoid overwriting functions as I'm currently doing, but I'm not yet sure how to do it in a robust manner.",True,{}
torch/torch7,https://github.com/torch/torch7,748,2016-08-24T23:25:35Z,2016-08-27T14:44:19Z,2016-08-27T14:44:25Z,MERGED,True,2,2,1,https://github.com/soumith,changing function load errors to warnings,1,[],https://github.com/torch/torch7/pull/748,https://github.com/soumith,4,https://github.com/torch/torch7/pull/748#issuecomment-242242314,"This seems like a needed change, now that we have LuaJIT 2.1, 2.0, Lua 5.2 and soon Lua 5.3 used by torch users.
The models which are serialized usually have no functions, but occasionally have some functions which usually are not needed.
Fixes #641 and #741","when you load the model it does print a warning. I could make the warning to be more scary. ""Warning: bad things might happen!!!!!!!!! :-# :-#""",True,{}
torch/torch7,https://github.com/torch/torch7,748,2016-08-24T23:25:35Z,2016-08-27T14:44:19Z,2016-08-27T14:44:25Z,MERGED,True,2,2,1,https://github.com/soumith,changing function load errors to warnings,1,[],https://github.com/torch/torch7/pull/748,https://github.com/fmassa,5,https://github.com/torch/torch7/pull/748#issuecomment-242243667,"This seems like a needed change, now that we have LuaJIT 2.1, 2.0, Lua 5.2 and soon Lua 5.3 used by torch users.
The models which are serialized usually have no functions, but occasionally have some functions which usually are not needed.
Fixes #641 and #741","Yes, it does print a warning. No need to scare people that much I think :)
I think I'll add an error while serializing models with optimization on, forcing the user to remove the optimizations before saving.",True,{}
torch/torch7,https://github.com/torch/torch7,753,2016-08-31T20:45:34Z,2016-09-01T02:01:45Z,2016-09-01T02:01:53Z,MERGED,True,176,29,5,https://github.com/apaszke,Add new shared memory allocator to TH,1,[],https://github.com/torch/torch7/pull/753,https://github.com/apaszke,1,https://github.com/torch/torch7/pull/753,,,True,{}
torch/torch7,https://github.com/torch/torch7,753,2016-08-31T20:45:34Z,2016-09-01T02:01:45Z,2016-09-01T02:01:53Z,MERGED,True,176,29,5,https://github.com/apaszke,Add new shared memory allocator to TH,1,[],https://github.com/torch/torch7/pull/753,https://github.com/soumith,2,https://github.com/torch/torch7/pull/753#issuecomment-243955433,,Looks good! Thanks Adam.,True,{}
torch/torch7,https://github.com/torch/torch7,755,2016-09-04T23:35:52Z,2016-09-08T03:31:04Z,2016-09-18T08:04:06Z,MERGED,True,742,574,11,https://github.com/rguthrie3,Added dyanamic dispatch for x86,14,[],https://github.com/torch/torch7/pull/755,https://github.com/rguthrie3,1,https://github.com/torch/torch7/pull/755,"This should allow distribution of binaries which will use SSE instructions for THVector operations when available on the host hardware, and back off to a default implementation otherwise.  The following bullets summarize the changes:

It looks like the vector functions were exported in the header TH.h publicly as inline functions or macros.  obviously the dynamic dispatch functions can't be inlined, so I changed THVector.h to simply include a generic/THVector.h header which provides declarations for all of the functions with all of the data types to be exported publicly (this might not be necessary if you don't care about exporting them publicly).  These will still be included as macros / inlines if compiled for ARM.
I renamed the SSE functions to, e.g THVector_(fill_SSE) and the scalar versions to THVector_(fill_SCALAR) (which are still in generic/THVector.c).  The SSE ones are in a file I named THVectorImpls.h.  I also wrapped the macros with functions (so that I can take the functions address).
generic/THVectorDispatch.c defines the function pointers that point to the most optimized function available on the host, the tables describing what implementations are available, and the code for initializing these function pointers (which needs to be called by the user).  It also has the entry points for client code calling THVector functions (which just calls the pointer to the actual implementation).
With all that above, THTensor.c just includes THVectorImpls.h (which generates all SSE and SCALAR functions), then generates generic/THVectorDispatch.c, which uses the definitions of the SSE and SCALAR functions.","This should allow distribution of binaries which will use SSE instructions for THVector operations when available on the host hardware, and back off to a default implementation otherwise.  The following bullets summarize the changes:

It looks like the vector functions were exported in the header TH.h publicly as inline functions or macros.  obviously the dynamic dispatch functions can't be inlined, so I changed THVector.h to simply include a generic/THVector.h header which provides declarations for all of the functions with all of the data types to be exported publicly (this might not be necessary if you don't care about exporting them publicly).  These will still be included as macros / inlines if compiled for ARM.
I renamed the SSE functions to, e.g THVector_(fill_SSE) and the scalar versions to THVector_(fill_SCALAR) (which are still in generic/THVector.c).  The SSE ones are in a file I named THVectorImpls.h.  I also wrapped the macros with functions (so that I can take the functions address).
generic/THVectorDispatch.c defines the function pointers that point to the most optimized function available on the host, the tables describing what implementations are available, and the code for initializing these function pointers (which needs to be called by the user).  It also has the entry points for client code calling THVector functions (which just calls the pointer to the actual implementation).
With all that above, THTensor.c just includes THVectorImpls.h (which generates all SSE and SCALAR functions), then generates generic/THVectorDispatch.c, which uses the definitions of the SSE and SCALAR functions.",True,{'HOORAY': ['https://github.com/apaszke']}
torch/torch7,https://github.com/torch/torch7,755,2016-09-04T23:35:52Z,2016-09-08T03:31:04Z,2016-09-18T08:04:06Z,MERGED,True,742,574,11,https://github.com/rguthrie3,Added dyanamic dispatch for x86,14,[],https://github.com/torch/torch7/pull/755,https://github.com/apaszke,2,https://github.com/torch/torch7/pull/755#issuecomment-244789415,"This should allow distribution of binaries which will use SSE instructions for THVector operations when available on the host hardware, and back off to a default implementation otherwise.  The following bullets summarize the changes:

It looks like the vector functions were exported in the header TH.h publicly as inline functions or macros.  obviously the dynamic dispatch functions can't be inlined, so I changed THVector.h to simply include a generic/THVector.h header which provides declarations for all of the functions with all of the data types to be exported publicly (this might not be necessary if you don't care about exporting them publicly).  These will still be included as macros / inlines if compiled for ARM.
I renamed the SSE functions to, e.g THVector_(fill_SSE) and the scalar versions to THVector_(fill_SCALAR) (which are still in generic/THVector.c).  The SSE ones are in a file I named THVectorImpls.h.  I also wrapped the macros with functions (so that I can take the functions address).
generic/THVectorDispatch.c defines the function pointers that point to the most optimized function available on the host, the tables describing what implementations are available, and the code for initializing these function pointers (which needs to be called by the user).  It also has the entry points for client code calling THVector functions (which just calls the pointer to the actual implementation).
With all that above, THTensor.c just includes THVectorImpls.h (which generates all SSE and SCALAR functions), then generates generic/THVectorDispatch.c, which uses the definitions of the SSE and SCALAR functions.","Is there any reason for keeping the SSE and SCALAR function definitions in header files instead of .c? The whole point of having automatic dispatch is to figure out the best implementation available automatically, instead of hardcoding it. What's more, they will have to be compiled multiple times if used in several files. Also, it results in this weird pattern that THTensor.c includes THVectorImpl.h that includes generic/THVector.c. I'm not sure if it's that important to keep them as inlines (especially that they are no longer inlines on x86).
IMHO it would be better to use automatic dispatch for all architectures. Separating x86 and ARM results in quite weird conditional includes and requires changing the outside code (vectorDispatchInit is undefined on ARM). You could just generate all of the implementations that can be generated, and use preprocessor conditionals to include or exclude them from dispatch tables. _SCALAR versions would be always defined. This could greatly simplify the code. You could refactor SCALAR, NEON, SSE and code for any other extension to be in separate files.
Also, THVectorDispatch.c should be compiled separately, not as part of THTensor.o.
But overall, great work! Can't wait to see it merged. 👍",True,{}
torch/torch7,https://github.com/torch/torch7,755,2016-09-04T23:35:52Z,2016-09-08T03:31:04Z,2016-09-18T08:04:06Z,MERGED,True,742,574,11,https://github.com/rguthrie3,Added dyanamic dispatch for x86,14,[],https://github.com/torch/torch7/pull/755,https://github.com/rguthrie3,3,https://github.com/torch/torch7/pull/755#issuecomment-244791406,"This should allow distribution of binaries which will use SSE instructions for THVector operations when available on the host hardware, and back off to a default implementation otherwise.  The following bullets summarize the changes:

It looks like the vector functions were exported in the header TH.h publicly as inline functions or macros.  obviously the dynamic dispatch functions can't be inlined, so I changed THVector.h to simply include a generic/THVector.h header which provides declarations for all of the functions with all of the data types to be exported publicly (this might not be necessary if you don't care about exporting them publicly).  These will still be included as macros / inlines if compiled for ARM.
I renamed the SSE functions to, e.g THVector_(fill_SSE) and the scalar versions to THVector_(fill_SCALAR) (which are still in generic/THVector.c).  The SSE ones are in a file I named THVectorImpls.h.  I also wrapped the macros with functions (so that I can take the functions address).
generic/THVectorDispatch.c defines the function pointers that point to the most optimized function available on the host, the tables describing what implementations are available, and the code for initializing these function pointers (which needs to be called by the user).  It also has the entry points for client code calling THVector functions (which just calls the pointer to the actual implementation).
With all that above, THTensor.c just includes THVectorImpls.h (which generates all SSE and SCALAR functions), then generates generic/THVectorDispatch.c, which uses the definitions of the SSE and SCALAR functions.","Good points.  The reason I did not implement anything for ARM dynamic dispatch yet is that I don't have any ARM experience or hardware (although I could set up an environment for simulation).  The obnoxious conditional compilation for ARM was only intended to be temporary, as I believe Soumith said he was facing a deadline that needed dynamic dispatch code in order to distribute binaries.  ARM dynamic dispatch would not be hard to add on to the code, provided I can set up my environment to test it.
I also agree that moving all implementations to a .c file is fine.  There is no good reason to keep them in .h files since they cannot be inlined.
I will move the impls around tonight and push a change.  I will also try and get ARM functions in the dispatch tables.  If there are no problems testing it, I can push it tonight as well.
Thanks!",True,{}
torch/torch7,https://github.com/torch/torch7,755,2016-09-04T23:35:52Z,2016-09-08T03:31:04Z,2016-09-18T08:04:06Z,MERGED,True,742,574,11,https://github.com/rguthrie3,Added dyanamic dispatch for x86,14,[],https://github.com/torch/torch7/pull/755,https://github.com/apaszke,4,https://github.com/torch/torch7/pull/755#issuecomment-244797103,"This should allow distribution of binaries which will use SSE instructions for THVector operations when available on the host hardware, and back off to a default implementation otherwise.  The following bullets summarize the changes:

It looks like the vector functions were exported in the header TH.h publicly as inline functions or macros.  obviously the dynamic dispatch functions can't be inlined, so I changed THVector.h to simply include a generic/THVector.h header which provides declarations for all of the functions with all of the data types to be exported publicly (this might not be necessary if you don't care about exporting them publicly).  These will still be included as macros / inlines if compiled for ARM.
I renamed the SSE functions to, e.g THVector_(fill_SSE) and the scalar versions to THVector_(fill_SCALAR) (which are still in generic/THVector.c).  The SSE ones are in a file I named THVectorImpls.h.  I also wrapped the macros with functions (so that I can take the functions address).
generic/THVectorDispatch.c defines the function pointers that point to the most optimized function available on the host, the tables describing what implementations are available, and the code for initializing these function pointers (which needs to be called by the user).  It also has the entry points for client code calling THVector functions (which just calls the pointer to the actual implementation).
With all that above, THTensor.c just includes THVectorImpls.h (which generates all SSE and SCALAR functions), then generates generic/THVectorDispatch.c, which uses the definitions of the SSE and SCALAR functions.","Also, since the implementations can be moved to .c files, there's no longer any reason to put the code in macros.
I have a computer with an ARM CPU supporting NEON, so I can try to test your new changes.",True,{}
torch/torch7,https://github.com/torch/torch7,755,2016-09-04T23:35:52Z,2016-09-08T03:31:04Z,2016-09-18T08:04:06Z,MERGED,True,742,574,11,https://github.com/rguthrie3,Added dyanamic dispatch for x86,14,[],https://github.com/torch/torch7/pull/755,https://github.com/rguthrie3,5,https://github.com/torch/torch7/pull/755#issuecomment-244839475,"This should allow distribution of binaries which will use SSE instructions for THVector operations when available on the host hardware, and back off to a default implementation otherwise.  The following bullets summarize the changes:

It looks like the vector functions were exported in the header TH.h publicly as inline functions or macros.  obviously the dynamic dispatch functions can't be inlined, so I changed THVector.h to simply include a generic/THVector.h header which provides declarations for all of the functions with all of the data types to be exported publicly (this might not be necessary if you don't care about exporting them publicly).  These will still be included as macros / inlines if compiled for ARM.
I renamed the SSE functions to, e.g THVector_(fill_SSE) and the scalar versions to THVector_(fill_SCALAR) (which are still in generic/THVector.c).  The SSE ones are in a file I named THVectorImpls.h.  I also wrapped the macros with functions (so that I can take the functions address).
generic/THVectorDispatch.c defines the function pointers that point to the most optimized function available on the host, the tables describing what implementations are available, and the code for initializing these function pointers (which needs to be called by the user).  It also has the entry points for client code calling THVector functions (which just calls the pointer to the actual implementation).
With all that above, THTensor.c just includes THVectorImpls.h (which generates all SSE and SCALAR functions), then generates generic/THVectorDispatch.c, which uses the definitions of the SSE and SCALAR functions.","I pushed some changes.  This code will probably not build on ARM, but I can do the dynamic dispatch for ARM tomorrow most likely.

I renamed the _SCALAR functions to _DEFAULT.  THVectorScalar sounds very confusing.
I put declarations for all implementations in the header THVectorImpls.h, which is included by THVectorDispatch.c (which just generates all the types for generic/THVectorDispatch.c, so this file is compiled separately of THTensor.c now).
SIMD impls are in THVectorSIMD.c, and default impls are generics in a file I renamed generic/THVectorDefault.c.  Similar for the generic header that declares the default impls.
I made the pointers NULL and put the asserts there.  I originally was initializing them to NULL and had the asserts for that reason, then changed to initializing them to the default, but I think the bug catching will be better by just leaving the asserts and initializing to NULL

Let me know what you think.  I will get ARM dynamic dispatch implemented as soon as possible.",True,{}
torch/torch7,https://github.com/torch/torch7,755,2016-09-04T23:35:52Z,2016-09-08T03:31:04Z,2016-09-18T08:04:06Z,MERGED,True,742,574,11,https://github.com/rguthrie3,Added dyanamic dispatch for x86,14,[],https://github.com/torch/torch7/pull/755,https://github.com/apaszke,6,https://github.com/torch/torch7/pull/755#issuecomment-244843101,"This should allow distribution of binaries which will use SSE instructions for THVector operations when available on the host hardware, and back off to a default implementation otherwise.  The following bullets summarize the changes:

It looks like the vector functions were exported in the header TH.h publicly as inline functions or macros.  obviously the dynamic dispatch functions can't be inlined, so I changed THVector.h to simply include a generic/THVector.h header which provides declarations for all of the functions with all of the data types to be exported publicly (this might not be necessary if you don't care about exporting them publicly).  These will still be included as macros / inlines if compiled for ARM.
I renamed the SSE functions to, e.g THVector_(fill_SSE) and the scalar versions to THVector_(fill_SCALAR) (which are still in generic/THVector.c).  The SSE ones are in a file I named THVectorImpls.h.  I also wrapped the macros with functions (so that I can take the functions address).
generic/THVectorDispatch.c defines the function pointers that point to the most optimized function available on the host, the tables describing what implementations are available, and the code for initializing these function pointers (which needs to be called by the user).  It also has the entry points for client code calling THVector functions (which just calls the pointer to the actual implementation).
With all that above, THTensor.c just includes THVectorImpls.h (which generates all SSE and SCALAR functions), then generates generic/THVectorDispatch.c, which uses the definitions of the SSE and SCALAR functions.","This looks much better to me. The only things I'd change would be to compile the vector code separately from tensors and isolate NEON and SSE implementations into different files e.g. vector/NEON.c and vector/SSE.c.
So:

THVectorDispatch.c -> THVector.c (including arch-specific implementations definitions)
THVectorSIMD.c -> divide into multiple files, each for one SIMD extension
All generic files look good to me.

After adding the ARM dispatch it should be much cleaner. Thanks!",True,{}
torch/torch7,https://github.com/torch/torch7,755,2016-09-04T23:35:52Z,2016-09-08T03:31:04Z,2016-09-18T08:04:06Z,MERGED,True,742,574,11,https://github.com/rguthrie3,Added dyanamic dispatch for x86,14,[],https://github.com/torch/torch7/pull/755,https://github.com/rguthrie3,7,https://github.com/torch/torch7/pull/755#issuecomment-245078625,"This should allow distribution of binaries which will use SSE instructions for THVector operations when available on the host hardware, and back off to a default implementation otherwise.  The following bullets summarize the changes:

It looks like the vector functions were exported in the header TH.h publicly as inline functions or macros.  obviously the dynamic dispatch functions can't be inlined, so I changed THVector.h to simply include a generic/THVector.h header which provides declarations for all of the functions with all of the data types to be exported publicly (this might not be necessary if you don't care about exporting them publicly).  These will still be included as macros / inlines if compiled for ARM.
I renamed the SSE functions to, e.g THVector_(fill_SSE) and the scalar versions to THVector_(fill_SCALAR) (which are still in generic/THVector.c).  The SSE ones are in a file I named THVectorImpls.h.  I also wrapped the macros with functions (so that I can take the functions address).
generic/THVectorDispatch.c defines the function pointers that point to the most optimized function available on the host, the tables describing what implementations are available, and the code for initializing these function pointers (which needs to be called by the user).  It also has the entry points for client code calling THVector functions (which just calls the pointer to the actual implementation).
With all that above, THTensor.c just includes THVectorImpls.h (which generates all SSE and SCALAR functions), then generates generic/THVectorDispatch.c, which uses the definitions of the SSE and SCALAR functions.","I pushed changes that add the NEON functions to the dispatch tables conditioning on the NEON macro,  and factored the vector functions into their own files.  I also got rid of the weird conditional compilation of the default implementations, so ARM NEON should be ""dynamic dispatched"", which I put quotes around because this still depends on determining the availability of NEON at compile time.  Should there be an ARM macro that is defined whether or not the host supports NEON so that this choice can be deferred until runtime?",True,{}
torch/torch7,https://github.com/torch/torch7,755,2016-09-04T23:35:52Z,2016-09-08T03:31:04Z,2016-09-18T08:04:06Z,MERGED,True,742,574,11,https://github.com/rguthrie3,Added dyanamic dispatch for x86,14,[],https://github.com/torch/torch7/pull/755,https://github.com/apaszke,8,https://github.com/torch/torch7/pull/755#issuecomment-245337751,"This should allow distribution of binaries which will use SSE instructions for THVector operations when available on the host hardware, and back off to a default implementation otherwise.  The following bullets summarize the changes:

It looks like the vector functions were exported in the header TH.h publicly as inline functions or macros.  obviously the dynamic dispatch functions can't be inlined, so I changed THVector.h to simply include a generic/THVector.h header which provides declarations for all of the functions with all of the data types to be exported publicly (this might not be necessary if you don't care about exporting them publicly).  These will still be included as macros / inlines if compiled for ARM.
I renamed the SSE functions to, e.g THVector_(fill_SSE) and the scalar versions to THVector_(fill_SCALAR) (which are still in generic/THVector.c).  The SSE ones are in a file I named THVectorImpls.h.  I also wrapped the macros with functions (so that I can take the functions address).
generic/THVectorDispatch.c defines the function pointers that point to the most optimized function available on the host, the tables describing what implementations are available, and the code for initializing these function pointers (which needs to be called by the user).  It also has the entry points for client code calling THVector functions (which just calls the pointer to the actual implementation).
With all that above, THTensor.c just includes THVectorImpls.h (which generates all SSE and SCALAR functions), then generates generic/THVectorDispatch.c, which uses the definitions of the SSE and SCALAR functions.","I see one last problem. I don't think the checking for __NEON__ and otherwise assuming you're on x86 is a good idea. TH tries to be quite portable. It would be better to use the USE_SSE* macros for this reason both in THVector.h and in THVectorDispatch.c (and omit the else clause - just compile whatever you can):
#ifdef __NEON__
#include ""vector/NEON.c""
#endif

#if defined USE_SSE2 || defined USE_SSE3 || defined USE_SSSE3 \
  || defined USE_SSE4_1 || defined USE_SSE4_2
#include ""vector/SSE.c""
#endif
Also, a smal nit: NEON functions are static, SSE functions are not. It would be better to stay consistent.",True,{}
torch/torch7,https://github.com/torch/torch7,755,2016-09-04T23:35:52Z,2016-09-08T03:31:04Z,2016-09-18T08:04:06Z,MERGED,True,742,574,11,https://github.com/rguthrie3,Added dyanamic dispatch for x86,14,[],https://github.com/torch/torch7/pull/755,https://github.com/rguthrie3,9,https://github.com/torch/torch7/pull/755#issuecomment-245346619,"This should allow distribution of binaries which will use SSE instructions for THVector operations when available on the host hardware, and back off to a default implementation otherwise.  The following bullets summarize the changes:

It looks like the vector functions were exported in the header TH.h publicly as inline functions or macros.  obviously the dynamic dispatch functions can't be inlined, so I changed THVector.h to simply include a generic/THVector.h header which provides declarations for all of the functions with all of the data types to be exported publicly (this might not be necessary if you don't care about exporting them publicly).  These will still be included as macros / inlines if compiled for ARM.
I renamed the SSE functions to, e.g THVector_(fill_SSE) and the scalar versions to THVector_(fill_SCALAR) (which are still in generic/THVector.c).  The SSE ones are in a file I named THVectorImpls.h.  I also wrapped the macros with functions (so that I can take the functions address).
generic/THVectorDispatch.c defines the function pointers that point to the most optimized function available on the host, the tables describing what implementations are available, and the code for initializing these function pointers (which needs to be called by the user).  It also has the entry points for client code calling THVector functions (which just calls the pointer to the actual implementation).
With all that above, THTensor.c just includes THVectorImpls.h (which generates all SSE and SCALAR functions), then generates generic/THVectorDispatch.c, which uses the definitions of the SSE and SCALAR functions.",I just pushed both changes you requested.  Let me know if there is anything else,True,{}
torch/torch7,https://github.com/torch/torch7,755,2016-09-04T23:35:52Z,2016-09-08T03:31:04Z,2016-09-18T08:04:06Z,MERGED,True,742,574,11,https://github.com/rguthrie3,Added dyanamic dispatch for x86,14,[],https://github.com/torch/torch7/pull/755,https://github.com/apaszke,10,https://github.com/torch/torch7/pull/755#issuecomment-245477066,"This should allow distribution of binaries which will use SSE instructions for THVector operations when available on the host hardware, and back off to a default implementation otherwise.  The following bullets summarize the changes:

It looks like the vector functions were exported in the header TH.h publicly as inline functions or macros.  obviously the dynamic dispatch functions can't be inlined, so I changed THVector.h to simply include a generic/THVector.h header which provides declarations for all of the functions with all of the data types to be exported publicly (this might not be necessary if you don't care about exporting them publicly).  These will still be included as macros / inlines if compiled for ARM.
I renamed the SSE functions to, e.g THVector_(fill_SSE) and the scalar versions to THVector_(fill_SCALAR) (which are still in generic/THVector.c).  The SSE ones are in a file I named THVectorImpls.h.  I also wrapped the macros with functions (so that I can take the functions address).
generic/THVectorDispatch.c defines the function pointers that point to the most optimized function available on the host, the tables describing what implementations are available, and the code for initializing these function pointers (which needs to be called by the user).  It also has the entry points for client code calling THVector functions (which just calls the pointer to the actual implementation).
With all that above, THTensor.c just includes THVectorImpls.h (which generates all SSE and SCALAR functions), then generates generic/THVectorDispatch.c, which uses the definitions of the SSE and SCALAR functions.","The code looks good to me 👍
But before the merge you have to make the tests pass, apparently they all segfault on the same test. There must be some small bug somewhere.",True,{}
torch/torch7,https://github.com/torch/torch7,755,2016-09-04T23:35:52Z,2016-09-08T03:31:04Z,2016-09-18T08:04:06Z,MERGED,True,742,574,11,https://github.com/rguthrie3,Added dyanamic dispatch for x86,14,[],https://github.com/torch/torch7/pull/755,https://github.com/rguthrie3,11,https://github.com/torch/torch7/pull/755#issuecomment-245478607,"This should allow distribution of binaries which will use SSE instructions for THVector operations when available on the host hardware, and back off to a default implementation otherwise.  The following bullets summarize the changes:

It looks like the vector functions were exported in the header TH.h publicly as inline functions or macros.  obviously the dynamic dispatch functions can't be inlined, so I changed THVector.h to simply include a generic/THVector.h header which provides declarations for all of the functions with all of the data types to be exported publicly (this might not be necessary if you don't care about exporting them publicly).  These will still be included as macros / inlines if compiled for ARM.
I renamed the SSE functions to, e.g THVector_(fill_SSE) and the scalar versions to THVector_(fill_SCALAR) (which are still in generic/THVector.c).  The SSE ones are in a file I named THVectorImpls.h.  I also wrapped the macros with functions (so that I can take the functions address).
generic/THVectorDispatch.c defines the function pointers that point to the most optimized function available on the host, the tables describing what implementations are available, and the code for initializing these function pointers (which needs to be called by the user).  It also has the entry points for client code calling THVector functions (which just calls the pointer to the actual implementation).
With all that above, THTensor.c just includes THVectorImpls.h (which generates all SSE and SCALAR functions), then generates generic/THVectorDispatch.c, which uses the definitions of the SSE and SCALAR functions.","Oh, the reason for that is the null pointer initialization of the dispatch pointers.  I can fix this probably by just adding the correct vectorDispatchInit() call somewhere (or just changing the pointers back to being initialized to the default implementation)",True,{}
torch/torch7,https://github.com/torch/torch7,755,2016-09-04T23:35:52Z,2016-09-08T03:31:04Z,2016-09-18T08:04:06Z,MERGED,True,742,574,11,https://github.com/rguthrie3,Added dyanamic dispatch for x86,14,[],https://github.com/torch/torch7/pull/755,https://github.com/apaszke,12,https://github.com/torch/torch7/pull/755#issuecomment-245479153,"This should allow distribution of binaries which will use SSE instructions for THVector operations when available on the host hardware, and back off to a default implementation otherwise.  The following bullets summarize the changes:

It looks like the vector functions were exported in the header TH.h publicly as inline functions or macros.  obviously the dynamic dispatch functions can't be inlined, so I changed THVector.h to simply include a generic/THVector.h header which provides declarations for all of the functions with all of the data types to be exported publicly (this might not be necessary if you don't care about exporting them publicly).  These will still be included as macros / inlines if compiled for ARM.
I renamed the SSE functions to, e.g THVector_(fill_SSE) and the scalar versions to THVector_(fill_SCALAR) (which are still in generic/THVector.c).  The SSE ones are in a file I named THVectorImpls.h.  I also wrapped the macros with functions (so that I can take the functions address).
generic/THVectorDispatch.c defines the function pointers that point to the most optimized function available on the host, the tables describing what implementations are available, and the code for initializing these function pointers (which needs to be called by the user).  It also has the entry points for client code calling THVector functions (which just calls the pointer to the actual implementation).
With all that above, THTensor.c just includes THVectorImpls.h (which generates all SSE and SCALAR functions), then generates generic/THVectorDispatch.c, which uses the definitions of the SSE and SCALAR functions.","Well, merging the PR should never cause segfaults. It's better to add vectorDispatchInit()  here.",True,{}
torch/torch7,https://github.com/torch/torch7,755,2016-09-04T23:35:52Z,2016-09-08T03:31:04Z,2016-09-18T08:04:06Z,MERGED,True,742,574,11,https://github.com/rguthrie3,Added dyanamic dispatch for x86,14,[],https://github.com/torch/torch7/pull/755,https://github.com/soumith,13,https://github.com/torch/torch7/pull/755#issuecomment-245479879,"This should allow distribution of binaries which will use SSE instructions for THVector operations when available on the host hardware, and back off to a default implementation otherwise.  The following bullets summarize the changes:

It looks like the vector functions were exported in the header TH.h publicly as inline functions or macros.  obviously the dynamic dispatch functions can't be inlined, so I changed THVector.h to simply include a generic/THVector.h header which provides declarations for all of the functions with all of the data types to be exported publicly (this might not be necessary if you don't care about exporting them publicly).  These will still be included as macros / inlines if compiled for ARM.
I renamed the SSE functions to, e.g THVector_(fill_SSE) and the scalar versions to THVector_(fill_SCALAR) (which are still in generic/THVector.c).  The SSE ones are in a file I named THVectorImpls.h.  I also wrapped the macros with functions (so that I can take the functions address).
generic/THVectorDispatch.c defines the function pointers that point to the most optimized function available on the host, the tables describing what implementations are available, and the code for initializing these function pointers (which needs to be called by the user).  It also has the entry points for client code calling THVector functions (which just calls the pointer to the actual implementation).
With all that above, THTensor.c just includes THVectorImpls.h (which generates all SSE and SCALAR functions), then generates generic/THVectorDispatch.c, which uses the definitions of the SSE and SCALAR functions.","@rguthrie3 i think it's better to make the dispatch pointers be initialized to the default implementation, and add a call to THVector_(vectorDispatchInit)()  here.",True,{}
torch/torch7,https://github.com/torch/torch7,755,2016-09-04T23:35:52Z,2016-09-08T03:31:04Z,2016-09-18T08:04:06Z,MERGED,True,742,574,11,https://github.com/rguthrie3,Added dyanamic dispatch for x86,14,[],https://github.com/torch/torch7/pull/755,https://github.com/rguthrie3,14,https://github.com/torch/torch7/pull/755#issuecomment-245484025,"This should allow distribution of binaries which will use SSE instructions for THVector operations when available on the host hardware, and back off to a default implementation otherwise.  The following bullets summarize the changes:

It looks like the vector functions were exported in the header TH.h publicly as inline functions or macros.  obviously the dynamic dispatch functions can't be inlined, so I changed THVector.h to simply include a generic/THVector.h header which provides declarations for all of the functions with all of the data types to be exported publicly (this might not be necessary if you don't care about exporting them publicly).  These will still be included as macros / inlines if compiled for ARM.
I renamed the SSE functions to, e.g THVector_(fill_SSE) and the scalar versions to THVector_(fill_SCALAR) (which are still in generic/THVector.c).  The SSE ones are in a file I named THVectorImpls.h.  I also wrapped the macros with functions (so that I can take the functions address).
generic/THVectorDispatch.c defines the function pointers that point to the most optimized function available on the host, the tables describing what implementations are available, and the code for initializing these function pointers (which needs to be called by the user).  It also has the entry points for client code calling THVector functions (which just calls the pointer to the actual implementation).
With all that above, THTensor.c just includes THVectorImpls.h (which generates all SSE and SCALAR functions), then generates generic/THVectorDispatch.c, which uses the definitions of the SSE and SCALAR functions.","Yes sorry about that, I wasn't sure where to add the init call.  I have added it, and also changed the dispatch pointer initialization, so I think that all looks good now.",True,{}
torch/torch7,https://github.com/torch/torch7,755,2016-09-04T23:35:52Z,2016-09-08T03:31:04Z,2016-09-18T08:04:06Z,MERGED,True,742,574,11,https://github.com/rguthrie3,Added dyanamic dispatch for x86,14,[],https://github.com/torch/torch7/pull/755,https://github.com/soumith,15,https://github.com/torch/torch7/pull/755#issuecomment-245484124,"This should allow distribution of binaries which will use SSE instructions for THVector operations when available on the host hardware, and back off to a default implementation otherwise.  The following bullets summarize the changes:

It looks like the vector functions were exported in the header TH.h publicly as inline functions or macros.  obviously the dynamic dispatch functions can't be inlined, so I changed THVector.h to simply include a generic/THVector.h header which provides declarations for all of the functions with all of the data types to be exported publicly (this might not be necessary if you don't care about exporting them publicly).  These will still be included as macros / inlines if compiled for ARM.
I renamed the SSE functions to, e.g THVector_(fill_SSE) and the scalar versions to THVector_(fill_SCALAR) (which are still in generic/THVector.c).  The SSE ones are in a file I named THVectorImpls.h.  I also wrapped the macros with functions (so that I can take the functions address).
generic/THVectorDispatch.c defines the function pointers that point to the most optimized function available on the host, the tables describing what implementations are available, and the code for initializing these function pointers (which needs to be called by the user).  It also has the entry points for client code calling THVector functions (which just calls the pointer to the actual implementation).
With all that above, THTensor.c just includes THVectorImpls.h (which generates all SSE and SCALAR functions), then generates generic/THVectorDispatch.c, which uses the definitions of the SSE and SCALAR functions.",Thanks Robert! This is really great.,True,{}
torch/torch7,https://github.com/torch/torch7,757,2016-09-06T17:39:36Z,2016-09-06T18:29:33Z,2016-11-08T17:19:50Z,MERGED,True,108,35,2,https://github.com/apaszke,Add new flags for THMapAllocator,1,[],https://github.com/torch/torch7/pull/757,https://github.com/apaszke,1,https://github.com/torch/torch7/pull/757,"TH_ALLOCATOR_MAPPED_FROMFD uses an existing file descriptor for
mapping (and steals it)
TH_ALLOCATOR_MAPPED_KEEPFD doesn't close the file descriptor
until the mapping is freed
TH_ALLOCATOR_MAPPED_UNLINK unlinks the file immediately after
mapping it to memory

Also, now it's using fstat to check the file size (instead of lseek, which alters the fd state).","TH_ALLOCATOR_MAPPED_FROMFD uses an existing file descriptor for
mapping (and steals it)
TH_ALLOCATOR_MAPPED_KEEPFD doesn't close the file descriptor
until the mapping is freed
TH_ALLOCATOR_MAPPED_UNLINK unlinks the file immediately after
mapping it to memory

Also, now it's using fstat to check the file size (instead of lseek, which alters the fd state).",True,{}
torch/torch7,https://github.com/torch/torch7,757,2016-09-06T17:39:36Z,2016-09-06T18:29:33Z,2016-11-08T17:19:50Z,MERGED,True,108,35,2,https://github.com/apaszke,Add new flags for THMapAllocator,1,[],https://github.com/torch/torch7/pull/757,https://github.com/soumith,2,https://github.com/torch/torch7/pull/757#issuecomment-245030073,"TH_ALLOCATOR_MAPPED_FROMFD uses an existing file descriptor for
mapping (and steals it)
TH_ALLOCATOR_MAPPED_KEEPFD doesn't close the file descriptor
until the mapping is freed
TH_ALLOCATOR_MAPPED_UNLINK unlinks the file immediately after
mapping it to memory

Also, now it's using fstat to check the file size (instead of lseek, which alters the fd state).","cc: @akfidjeland @timharley wrt the change from lseek to fstat. It's harmless, but want to keep you in the loop.",True,{}
torch/torch7,https://github.com/torch/torch7,757,2016-09-06T17:39:36Z,2016-09-06T18:29:33Z,2016-11-08T17:19:50Z,MERGED,True,108,35,2,https://github.com/apaszke,Add new flags for THMapAllocator,1,[],https://github.com/torch/torch7/pull/757,https://github.com/timharley,3,https://github.com/torch/torch7/pull/757#issuecomment-245044426,"TH_ALLOCATOR_MAPPED_FROMFD uses an existing file descriptor for
mapping (and steals it)
TH_ALLOCATOR_MAPPED_KEEPFD doesn't close the file descriptor
until the mapping is freed
TH_ALLOCATOR_MAPPED_UNLINK unlinks the file immediately after
mapping it to memory

Also, now it's using fstat to check the file size (instead of lseek, which alters the fd state).",Thanks @soumith,True,{}
torch/torch7,https://github.com/torch/torch7,757,2016-09-06T17:39:36Z,2016-09-06T18:29:33Z,2016-11-08T17:19:50Z,MERGED,True,108,35,2,https://github.com/apaszke,Add new flags for THMapAllocator,1,[],https://github.com/torch/torch7/pull/757,https://github.com/soumith,4,https://github.com/torch/torch7/pull/757#issuecomment-245044679,"TH_ALLOCATOR_MAPPED_FROMFD uses an existing file descriptor for
mapping (and steals it)
TH_ALLOCATOR_MAPPED_KEEPFD doesn't close the file descriptor
until the mapping is freed
TH_ALLOCATOR_MAPPED_UNLINK unlinks the file immediately after
mapping it to memory

Also, now it's using fstat to check the file size (instead of lseek, which alters the fd state).",Thanks @apaszke !,True,{}
torch/torch7,https://github.com/torch/torch7,757,2016-09-06T17:39:36Z,2016-09-06T18:29:33Z,2016-11-08T17:19:50Z,MERGED,True,108,35,2,https://github.com/apaszke,Add new flags for THMapAllocator,1,[],https://github.com/torch/torch7/pull/757,https://github.com/tworec,5,https://github.com/torch/torch7/pull/757#issuecomment-255320376,"TH_ALLOCATOR_MAPPED_FROMFD uses an existing file descriptor for
mapping (and steals it)
TH_ALLOCATOR_MAPPED_KEEPFD doesn't close the file descriptor
until the mapping is freed
TH_ALLOCATOR_MAPPED_UNLINK unlinks the file immediately after
mapping it to memory

Also, now it's using fstat to check the file size (instead of lseek, which alters the fd state).","I have problem with THAllocator.c line 262
In [1]:
s=torch.FloatStorage('/tmp/2rec', true, 10000)

gives me
unable to stretch file </tmp/2rec> to the right size at /home/tworec/torch/pkg/torch/lib/TH/THAllocator.c:262
stack traceback:
    [C]: at 0x7fd308444330
    [C]: in function 'FloatStorage'
    [string ""s=torch.FloatStorage('/tmp/2rec', true, 10000...""]:1: in main chunk
    [C]: in function 'xpcall'
    /home/tworec/torch/install/share/lua/5.1/itorch/main.lua:210: in function </home/tworec/torch/install/share/lua/5.1/itorch/main.lua:174>
    /home/tworec/torch/install/share/lua/5.1/lzmq/poller.lua:75: in function 'poll'
    /home/tworec/torch/install/share/lua/5.1/lzmq/impl/loop.lua:307: in function 'poll'
    /home/tworec/torch/install/share/lua/5.1/lzmq/impl/loop.lua:325: in function 'sleep_ex'
    /home/tworec/torch/install/share/lua/5.1/lzmq/impl/loop.lua:370: in function 'start'
    /home/tworec/torch/install/share/lua/5.1/itorch/main.lua:389: in main chunk
    [C]: in function 'require'
    (command line):1: in main chunk
    [C]: at 0x00406670",True,{}
torch/torch7,https://github.com/torch/torch7,757,2016-09-06T17:39:36Z,2016-09-06T18:29:33Z,2016-11-08T17:19:50Z,MERGED,True,108,35,2,https://github.com/apaszke,Add new flags for THMapAllocator,1,[],https://github.com/torch/torch7/pull/757,https://github.com/soumith,6,https://github.com/torch/torch7/pull/757#issuecomment-259164691,"TH_ALLOCATOR_MAPPED_FROMFD uses an existing file descriptor for
mapping (and steals it)
TH_ALLOCATOR_MAPPED_KEEPFD doesn't close the file descriptor
until the mapping is freed
TH_ALLOCATOR_MAPPED_UNLINK unlinks the file immediately after
mapping it to memory

Also, now it's using fstat to check the file size (instead of lseek, which alters the fd state).",@tworec this was fixed 13 days ago via #817,True,{}
torch/torch7,https://github.com/torch/torch7,757,2016-09-06T17:39:36Z,2016-09-06T18:29:33Z,2016-11-08T17:19:50Z,MERGED,True,108,35,2,https://github.com/apaszke,Add new flags for THMapAllocator,1,[],https://github.com/torch/torch7/pull/757,https://github.com/tworec,7,https://github.com/torch/torch7/pull/757#issuecomment-259199975,"TH_ALLOCATOR_MAPPED_FROMFD uses an existing file descriptor for
mapping (and steals it)
TH_ALLOCATOR_MAPPED_KEEPFD doesn't close the file descriptor
until the mapping is freed
TH_ALLOCATOR_MAPPED_UNLINK unlinks the file immediately after
mapping it to memory

Also, now it's using fstat to check the file size (instead of lseek, which alters the fd state).","yes, indeed",True,{}
torch/torch7,https://github.com/torch/torch7,758,2016-09-08T16:39:02Z,2016-09-08T16:49:14Z,2016-09-10T04:24:38Z,MERGED,True,55,23,3,https://github.com/apaszke,Various improvements to THMapAllocator,4,[],https://github.com/torch/torch7/pull/758,https://github.com/apaszke,1,https://github.com/torch/torch7/pull/758,"Fixed segfault when calling THError
Improved shm_open and shm_unlink cmake feature tests
Created incref, decref functions for refcounted shared memory objects
Fixed an OS X bug in THMapAllocator (writing to fd to shm object is not supported)","Fixed segfault when calling THError
Improved shm_open and shm_unlink cmake feature tests
Created incref, decref functions for refcounted shared memory objects
Fixed an OS X bug in THMapAllocator (writing to fd to shm object is not supported)",True,{}
torch/torch7,https://github.com/torch/torch7,758,2016-09-08T16:39:02Z,2016-09-08T16:49:14Z,2016-09-10T04:24:38Z,MERGED,True,55,23,3,https://github.com/apaszke,Various improvements to THMapAllocator,4,[],https://github.com/torch/torch7/pull/758,https://github.com/soumith,2,https://github.com/torch/torch7/pull/758#issuecomment-245662418,"Fixed segfault when calling THError
Improved shm_open and shm_unlink cmake feature tests
Created incref, decref functions for refcounted shared memory objects
Fixed an OS X bug in THMapAllocator (writing to fd to shm object is not supported)",Thanks!,True,{}
torch/torch7,https://github.com/torch/torch7,760,2016-09-09T16:23:33Z,2016-09-09T16:41:06Z,2016-09-09T16:41:06Z,CLOSED,False,54,50,3,https://github.com/colesbury,Use stdint types for tensors,1,[],https://github.com/torch/torch7/pull/760,https://github.com/colesbury,1,https://github.com/torch/torch7/pull/760,"The most important changes are that:

LongTensor is now 64-bit on all platforms
CharTensor is always signed

cc @szagoruyko @apaszke","The most important changes are that:

LongTensor is now 64-bit on all platforms
CharTensor is always signed

cc @szagoruyko @apaszke",True,{}
torch/torch7,https://github.com/torch/torch7,761,2016-09-10T04:24:14Z,2016-09-10T17:02:53Z,2016-09-10T17:02:57Z,MERGED,True,5,1,1,https://github.com/cdluminate,"prevent Unknown CMake command ""check_function_exists"".",2,[],https://github.com/torch/torch7/pull/761,https://github.com/cdluminate,1,https://github.com/torch/torch7/pull/761,,,True,{}
torch/torch7,https://github.com/torch/torch7,761,2016-09-10T04:24:14Z,2016-09-10T17:02:53Z,2016-09-10T17:02:57Z,MERGED,True,5,1,1,https://github.com/cdluminate,"prevent Unknown CMake command ""check_function_exists"".",2,[],https://github.com/torch/torch7/pull/761,https://github.com/apaszke,2,https://github.com/torch/torch7/pull/761#issuecomment-246089217,,"Right, sorry for that. You can probably delete this one then.",True,{}
torch/torch7,https://github.com/torch/torch7,761,2016-09-10T04:24:14Z,2016-09-10T17:02:53Z,2016-09-10T17:02:57Z,MERGED,True,5,1,1,https://github.com/cdluminate,"prevent Unknown CMake command ""check_function_exists"".",2,[],https://github.com/torch/torch7/pull/761,https://github.com/cdluminate,3,https://github.com/torch/torch7/pull/761#issuecomment-246089620,,"@apaszke Oh yes, done.",True,{}
torch/torch7,https://github.com/torch/torch7,761,2016-09-10T04:24:14Z,2016-09-10T17:02:53Z,2016-09-10T17:02:57Z,MERGED,True,5,1,1,https://github.com/cdluminate,"prevent Unknown CMake command ""check_function_exists"".",2,[],https://github.com/torch/torch7/pull/761,https://github.com/soumith,4,https://github.com/torch/torch7/pull/761#issuecomment-246122934,,Thank you!,True,{}
torch/torch7,https://github.com/torch/torch7,765,2016-09-15T14:38:08Z,2016-09-15T14:57:28Z,2016-09-15T14:57:32Z,MERGED,True,10,2,2,https://github.com/apaszke,Fix mode,1,[],https://github.com/torch/torch7/pull/765,https://github.com/apaszke,1,https://github.com/torch/torch7/pull/765,,,True,{}
torch/torch7,https://github.com/torch/torch7,765,2016-09-15T14:38:08Z,2016-09-15T14:57:28Z,2016-09-15T14:57:32Z,MERGED,True,10,2,2,https://github.com/apaszke,Fix mode,1,[],https://github.com/torch/torch7/pull/765,https://github.com/soumith,2,https://github.com/torch/torch7/pull/765#issuecomment-247352926,,Thanks!,True,{}
torch/torch7,https://github.com/torch/torch7,768,2016-09-19T06:52:50Z,2016-09-19T15:24:14Z,2016-09-19T15:24:23Z,MERGED,True,24,1,3,https://github.com/BTNC,fix cpuid ecx; change to compile with msvc; compile with MKL on windows,1,[],https://github.com/torch/torch7/pull/768,https://github.com/BTNC,1,https://github.com/torch/torch7/pull/768,"Hi,
This small change contains three parts:

fix one line cupid code from ""+c"" to ""=c""
change master branch to compile with msvc
compile against mkl on windows with msvc

This change only affect those who use msvc, please let me know if any change is improper or can be improved.
Thanks","Hi,
This small change contains three parts:

fix one line cupid code from ""+c"" to ""=c""
change master branch to compile with msvc
compile against mkl on windows with msvc

This change only affect those who use msvc, please let me know if any change is improper or can be improved.
Thanks",True,{}
torch/torch7,https://github.com/torch/torch7,768,2016-09-19T06:52:50Z,2016-09-19T15:24:14Z,2016-09-19T15:24:23Z,MERGED,True,24,1,3,https://github.com/BTNC,fix cpuid ecx; change to compile with msvc; compile with MKL on windows,1,[],https://github.com/torch/torch7/pull/768,https://github.com/soumith,2,https://github.com/torch/torch7/pull/768#issuecomment-248025381,"Hi,
This small change contains three parts:

fix one line cupid code from ""+c"" to ""=c""
change master branch to compile with msvc
compile against mkl on windows with msvc

This change only affect those who use msvc, please let me know if any change is improper or can be improved.
Thanks",Thank you so much for this patch!,True,{}
torch/torch7,https://github.com/torch/torch7,774,2016-09-26T03:10:12Z,2016-09-26T04:50:50Z,2016-09-26T04:50:54Z,MERGED,True,84,50,3,https://github.com/BTNC,bug fix for read/writeLong in THMemoryFile and 2 test files change for windows,2,[],https://github.com/torch/torch7/pull/774,https://github.com/BTNC,1,https://github.com/torch/torch7/pull/774,"Hi,
There is a bug in THMemoryFile read/writelong for following cases:
(1) sizeof(long) is 8 while longSize is set to 4;
(2) sizeof(long) is 4 while longSize is set to 8;
This change comes along with 2 test files change for windows.
Thanks,","Hi,
There is a bug in THMemoryFile read/writelong for following cases:
(1) sizeof(long) is 8 while longSize is set to 4;
(2) sizeof(long) is 4 while longSize is set to 8;
This change comes along with 2 test files change for windows.
Thanks,",True,{}
torch/torch7,https://github.com/torch/torch7,774,2016-09-26T03:10:12Z,2016-09-26T04:50:50Z,2016-09-26T04:50:54Z,MERGED,True,84,50,3,https://github.com/BTNC,bug fix for read/writeLong in THMemoryFile and 2 test files change for windows,2,[],https://github.com/torch/torch7/pull/774,https://github.com/soumith,2,https://github.com/torch/torch7/pull/774#issuecomment-249478906,"Hi,
There is a bug in THMemoryFile read/writelong for following cases:
(1) sizeof(long) is 8 while longSize is set to 4;
(2) sizeof(long) is 4 while longSize is set to 8;
This change comes along with 2 test files change for windows.
Thanks,",Thank you!,True,{}
torch/torch7,https://github.com/torch/torch7,776,2016-09-27T07:19:44Z,2016-09-27T12:59:27Z,2016-09-27T12:59:31Z,MERGED,True,111,36,5,https://github.com/BTNC,windows high resolution timer with a few makefile changes,4,[],https://github.com/torch/torch7/pull/776,https://github.com/BTNC,1,https://github.com/torch/torch7/pull/776,"Hi,
This pr contains:
(1) windows high resolution timer with a test file (the old impl's precision is seconds);
(2) makefile changes for msvc;
Thanks,","Hi,
This pr contains:
(1) windows high resolution timer with a test file (the old impl's precision is seconds);
(2) makefile changes for msvc;
Thanks,",True,{}
torch/torch7,https://github.com/torch/torch7,776,2016-09-27T07:19:44Z,2016-09-27T12:59:27Z,2016-09-27T12:59:31Z,MERGED,True,111,36,5,https://github.com/BTNC,windows high resolution timer with a few makefile changes,4,[],https://github.com/torch/torch7/pull/776,https://github.com/soumith,2,https://github.com/torch/torch7/pull/776#issuecomment-249857183,"Hi,
This pr contains:
(1) windows high resolution timer with a test file (the old impl's precision is seconds);
(2) makefile changes for msvc;
Thanks,",Thank you!,True,{}
torch/torch7,https://github.com/torch/torch7,779,2016-09-29T15:35:56Z,2016-10-01T19:27:30Z,2016-10-01T19:27:34Z,MERGED,True,79,52,7,https://github.com/BTNC,a couple of changes for win32,10,[],https://github.com/torch/torch7/pull/779,https://github.com/BTNC,1,https://github.com/torch/torch7/pull/779,"Hi,
This change contains:
(1) thread local in THGeneral for msvc
(2) 64bit ftell/fseek for win32
(3) choose the recommended *Ex functions for file operation
(4) reimplement some util functions
etc
Thanks,","Hi,
This change contains:
(1) thread local in THGeneral for msvc
(2) 64bit ftell/fseek for win32
(3) choose the recommended *Ex functions for file operation
(4) reimplement some util functions
etc
Thanks,",True,{}
torch/torch7,https://github.com/torch/torch7,779,2016-09-29T15:35:56Z,2016-10-01T19:27:30Z,2016-10-01T19:27:34Z,MERGED,True,79,52,7,https://github.com/BTNC,a couple of changes for win32,10,[],https://github.com/torch/torch7/pull/779,https://github.com/soumith,2,https://github.com/torch/torch7/pull/779#issuecomment-250835860,"Hi,
This change contains:
(1) thread local in THGeneral for msvc
(2) 64bit ftell/fseek for win32
(3) choose the recommended *Ex functions for file operation
(4) reimplement some util functions
etc
Thanks,","are you still working on the PR, or is it good to merge? no comments from me, i will merge it whenever you are ready.",True,{}
torch/torch7,https://github.com/torch/torch7,779,2016-09-29T15:35:56Z,2016-10-01T19:27:30Z,2016-10-01T19:27:34Z,MERGED,True,79,52,7,https://github.com/BTNC,a couple of changes for win32,10,[],https://github.com/torch/torch7/pull/779,https://github.com/BTNC,3,https://github.com/torch/torch7/pull/779#issuecomment-250884796,"Hi,
This change contains:
(1) thread local in THGeneral for msvc
(2) 64bit ftell/fseek for win32
(3) choose the recommended *Ex functions for file operation
(4) reimplement some util functions
etc
Thanks,","There is no more commit for this PR, please help merge it. Thanks.",True,{}
torch/torch7,https://github.com/torch/torch7,779,2016-09-29T15:35:56Z,2016-10-01T19:27:30Z,2016-10-01T19:27:34Z,MERGED,True,79,52,7,https://github.com/BTNC,a couple of changes for win32,10,[],https://github.com/torch/torch7/pull/779,https://github.com/soumith,4,https://github.com/torch/torch7/pull/779#issuecomment-250932030,"Hi,
This change contains:
(1) thread local in THGeneral for msvc
(2) 64bit ftell/fseek for win32
(3) choose the recommended *Ex functions for file operation
(4) reimplement some util functions
etc
Thanks,",Thank you!,True,{}
torch/torch7,https://github.com/torch/torch7,782,2016-09-30T19:39:15Z,2016-09-30T19:47:07Z,2016-09-30T19:47:07Z,MERGED,True,49,21,2,https://github.com/apaszke,Allow changing the default error handler for all threads,1,[],https://github.com/torch/torch7/pull/782,https://github.com/apaszke,1,https://github.com/torch/torch7/pull/782,"THSetErrorHandler still modifies per-thread pointers, but THSetDefaultErrorHandler allows to set a handler that's used by all threads that haven't specified any function.","THSetErrorHandler still modifies per-thread pointers, but THSetDefaultErrorHandler allows to set a handler that's used by all threads that haven't specified any function.",True,{}
torch/torch7,https://github.com/torch/torch7,784,2016-10-02T15:36:42Z,2016-10-02T15:57:14Z,2016-10-03T01:02:35Z,CLOSED,False,516,276,29,https://github.com/BTNC,replace long with ptrdiff_t for memory size/offset etc,13,[],https://github.com/torch/torch7/pull/784,https://github.com/BTNC,1,https://github.com/torch/torch7/pull/784,"Hi,
Type long is used extensively in torch mainly for (1) memory size/offset, element counts; (2) representing data; (3) meeting 3rd party api in/out specification. This PR is about long usage in (1).
Impact:
(1) for users on linux/mac where rang of long is same as range of ptrdiff_t, this PR is expected to introduce no behavior change.
(2) for users on windows/arm x64 etc where long is 32bit, the maximum managed memory by torch used to be 2G, but now the limit is extended to 2^63, same as linux/mac users.
Please let me know if any part in this PR is not about usage (1) or this PR miss any change about usage (1).
For usage (2), my proposal is to have a torch long type like th_Long which can be configured for integer types other than long. So windows/arm x64 users can also use 64bit integer. THGenerateIntTypes.h is now providing very limited configuration.
Thanks,","Hi,
Type long is used extensively in torch mainly for (1) memory size/offset, element counts; (2) representing data; (3) meeting 3rd party api in/out specification. This PR is about long usage in (1).
Impact:
(1) for users on linux/mac where rang of long is same as range of ptrdiff_t, this PR is expected to introduce no behavior change.
(2) for users on windows/arm x64 etc where long is 32bit, the maximum managed memory by torch used to be 2G, but now the limit is extended to 2^63, same as linux/mac users.
Please let me know if any part in this PR is not about usage (1) or this PR miss any change about usage (1).
For usage (2), my proposal is to have a torch long type like th_Long which can be configured for integer types other than long. So windows/arm x64 users can also use 64bit integer. THGenerateIntTypes.h is now providing very limited configuration.
Thanks,",True,{}
torch/torch7,https://github.com/torch/torch7,784,2016-10-02T15:36:42Z,2016-10-02T15:57:14Z,2016-10-03T01:02:35Z,CLOSED,False,516,276,29,https://github.com/BTNC,replace long with ptrdiff_t for memory size/offset etc,13,[],https://github.com/torch/torch7/pull/784,https://github.com/soumith,2,https://github.com/torch/torch7/pull/784#issuecomment-250977474,"Hi,
Type long is used extensively in torch mainly for (1) memory size/offset, element counts; (2) representing data; (3) meeting 3rd party api in/out specification. This PR is about long usage in (1).
Impact:
(1) for users on linux/mac where rang of long is same as range of ptrdiff_t, this PR is expected to introduce no behavior change.
(2) for users on windows/arm x64 etc where long is 32bit, the maximum managed memory by torch used to be 2G, but now the limit is extended to 2^63, same as linux/mac users.
Please let me know if any part in this PR is not about usage (1) or this PR miss any change about usage (1).
For usage (2), my proposal is to have a torch long type like th_Long which can be configured for integer types other than long. So windows/arm x64 users can also use 64bit integer. THGenerateIntTypes.h is now providing very limited configuration.
Thanks,","hi, i see that you have built your commit ""replace long with ptrdiff_t for memory size/offset etc"" on top of your other branch that fixes time.
Hence there are lot of merge conflicts.
I am reviewing that commit now, but I think you should isolate that commit on top of master.",True,{}
torch/torch7,https://github.com/torch/torch7,784,2016-10-02T15:36:42Z,2016-10-02T15:57:14Z,2016-10-03T01:02:35Z,CLOSED,False,516,276,29,https://github.com/BTNC,replace long with ptrdiff_t for memory size/offset etc,13,[],https://github.com/torch/torch7/pull/784,https://github.com/soumith,3,https://github.com/torch/torch7/pull/784#issuecomment-250977872,"Hi,
Type long is used extensively in torch mainly for (1) memory size/offset, element counts; (2) representing data; (3) meeting 3rd party api in/out specification. This PR is about long usage in (1).
Impact:
(1) for users on linux/mac where rang of long is same as range of ptrdiff_t, this PR is expected to introduce no behavior change.
(2) for users on windows/arm x64 etc where long is 32bit, the maximum managed memory by torch used to be 2G, but now the limit is extended to 2^63, same as linux/mac users.
Please let me know if any part in this PR is not about usage (1) or this PR miss any change about usage (1).
For usage (2), my proposal is to have a torch long type like th_Long which can be configured for integer types other than long. So windows/arm x64 users can also use 64bit integer. THGenerateIntTypes.h is now providing very limited configuration.
Thanks,","ok the PR largely looks good. I will re-review it again in a bit.
I'll also test its downstream dependencies like cutorch.
Are you planning to send an equivalent PR to cutorch, or is torch7 your main interest?
For now, please close this PR, and isolate and rebase the commit ""replace long with ptrdiff_t for memory size/offset etc"" onto torch7/master and send a new PR.
Thank you :)",True,{}
torch/torch7,https://github.com/torch/torch7,784,2016-10-02T15:36:42Z,2016-10-02T15:57:14Z,2016-10-03T01:02:35Z,CLOSED,False,516,276,29,https://github.com/BTNC,replace long with ptrdiff_t for memory size/offset etc,13,[],https://github.com/torch/torch7/pull/784,https://github.com/BTNC,4,https://github.com/torch/torch7/pull/784#issuecomment-250978413,"Hi,
Type long is used extensively in torch mainly for (1) memory size/offset, element counts; (2) representing data; (3) meeting 3rd party api in/out specification. This PR is about long usage in (1).
Impact:
(1) for users on linux/mac where rang of long is same as range of ptrdiff_t, this PR is expected to introduce no behavior change.
(2) for users on windows/arm x64 etc where long is 32bit, the maximum managed memory by torch used to be 2G, but now the limit is extended to 2^63, same as linux/mac users.
Please let me know if any part in this PR is not about usage (1) or this PR miss any change about usage (1).
For usage (2), my proposal is to have a torch long type like th_Long which can be configured for integer types other than long. So windows/arm x64 users can also use 64bit integer. THGenerateIntTypes.h is now providing very limited configuration.
Thanks,","I have not tried the new cuda toolkit8 which may finally compile with vs2015. But as long as I can try it out, I will send an equivalent PR for cutorch. I will close this PR and resend a new PR.",True,{}
torch/torch7,https://github.com/torch/torch7,784,2016-10-02T15:36:42Z,2016-10-02T15:57:14Z,2016-10-03T01:02:35Z,CLOSED,False,516,276,29,https://github.com/BTNC,replace long with ptrdiff_t for memory size/offset etc,13,[],https://github.com/torch/torch7/pull/784,https://github.com/apaszke,5,https://github.com/torch/torch7/pull/784#issuecomment-250981733,"Hi,
Type long is used extensively in torch mainly for (1) memory size/offset, element counts; (2) representing data; (3) meeting 3rd party api in/out specification. This PR is about long usage in (1).
Impact:
(1) for users on linux/mac where rang of long is same as range of ptrdiff_t, this PR is expected to introduce no behavior change.
(2) for users on windows/arm x64 etc where long is 32bit, the maximum managed memory by torch used to be 2G, but now the limit is extended to 2^63, same as linux/mac users.
Please let me know if any part in this PR is not about usage (1) or this PR miss any change about usage (1).
For usage (2), my proposal is to have a torch long type like th_Long which can be configured for integer types other than long. So windows/arm x64 users can also use 64bit integer. THGenerateIntTypes.h is now providing very limited configuration.
Thanks,","Actually ptrdiff_t it's a correct type for this purpose. You should probably use size_t, as that's what's used by malloc and similar.
e.g. this is taken from the C++ standard
ptrdiff_t is used for pointer arithmetic and array indexing, if negative values are possible.
size_t can store the maximum size of a theoretically possible object of any type (including array).",True,{}
torch/torch7,https://github.com/torch/torch7,784,2016-10-02T15:36:42Z,2016-10-02T15:57:14Z,2016-10-03T01:02:35Z,CLOSED,False,516,276,29,https://github.com/BTNC,replace long with ptrdiff_t for memory size/offset etc,13,[],https://github.com/torch/torch7/pull/784,https://github.com/BTNC,6,https://github.com/torch/torch7/pull/784#issuecomment-251008665,"Hi,
Type long is used extensively in torch mainly for (1) memory size/offset, element counts; (2) representing data; (3) meeting 3rd party api in/out specification. This PR is about long usage in (1).
Impact:
(1) for users on linux/mac where rang of long is same as range of ptrdiff_t, this PR is expected to introduce no behavior change.
(2) for users on windows/arm x64 etc where long is 32bit, the maximum managed memory by torch used to be 2G, but now the limit is extended to 2^63, same as linux/mac users.
Please let me know if any part in this PR is not about usage (1) or this PR miss any change about usage (1).
For usage (2), my proposal is to have a torch long type like th_Long which can be configured for integer types other than long. So windows/arm x64 users can also use 64bit integer. THGenerateIntTypes.h is now providing very limited configuration.
Thanks,","I did find many places that should better use size_t, but after evaluation I found it may require code logic change in many places due to sign/unsign (which can easily introduce bugs if not addressed properly), and may introduce behavior changes for most users(Linux/Mac). As a result, I decided to separate the update to several phases instead of one go.
To be clear, my plan was: (1) categorize long usage and address one by one.  As described in this PR description, this PR is to address long usage for memory size/offset. (2) addressing each in small phases until the best shape. Replacing long with ptrdiff_t is a relatively smooth change, though identifying which long should be replaced is laborious. size_t change can be on top of the ptrdiff_t change, i.e. replacing some ptrdiff_t with size_t. Changing to size_t is not my current concern since I am on 64bit systems, but 32bit users may benefit from it.",True,{}
torch/torch7,https://github.com/torch/torch7,785,2016-10-03T05:58:48Z,2016-10-07T14:43:56Z,2016-10-07T14:44:13Z,MERGED,True,521,271,31,https://github.com/BTNC,"replace long with ptrdiff_t for memory size/offset, element counts etc",2,[],https://github.com/torch/torch7/pull/785,https://github.com/BTNC,1,https://github.com/torch/torch7/pull/785,"Hi,
Type long is used extensively in torch mainly for (1) memory size/offset, element counts; (2) representing data; (3) meeting 3rd party api in/out specification. This PR is about long usage in (1).
Impact:
(1) for users on linux/mac where rang of long is same as range of ptrdiff_t, this PR is expected to introduce no behavior change.
(2) for users on windows/arm x64 etc where long is 32bit, the maximum managed memory by torch used to be 2G, but now the limit is extended to 2^63, same as linux/mac users.
Replacing long with ptrdiff_t is a relatively smooth change, though identifying which long should be replaced is laborious. For places that should better use size_t, size_t change can be on top of the ptrdiff_t change, i.e. replacing some ptrdiff_t with size_t. I did not replace some long with size_t because after evaluation I found replacement with size_t requires code logic changes in many places which could complicate this PR and introduce big behavior change for most users, and I am uncertain about that impact.
Please let me know if any part in this PR is not about usage (1) or this PR miss any change about usage (1).
For long usage (2), my proposal is to have a torch long type like th_Long which can be configured for integer types other than long. So windows/arm x64 users can also use 64bit integer. THGenerateIntTypes.h is now providing very limited configuration.
Thanks,","Hi,
Type long is used extensively in torch mainly for (1) memory size/offset, element counts; (2) representing data; (3) meeting 3rd party api in/out specification. This PR is about long usage in (1).
Impact:
(1) for users on linux/mac where rang of long is same as range of ptrdiff_t, this PR is expected to introduce no behavior change.
(2) for users on windows/arm x64 etc where long is 32bit, the maximum managed memory by torch used to be 2G, but now the limit is extended to 2^63, same as linux/mac users.
Replacing long with ptrdiff_t is a relatively smooth change, though identifying which long should be replaced is laborious. For places that should better use size_t, size_t change can be on top of the ptrdiff_t change, i.e. replacing some ptrdiff_t with size_t. I did not replace some long with size_t because after evaluation I found replacement with size_t requires code logic changes in many places which could complicate this PR and introduce big behavior change for most users, and I am uncertain about that impact.
Please let me know if any part in this PR is not about usage (1) or this PR miss any change about usage (1).
For long usage (2), my proposal is to have a torch long type like th_Long which can be configured for integer types other than long. So windows/arm x64 users can also use 64bit integer. THGenerateIntTypes.h is now providing very limited configuration.
Thanks,",True,{}
torch/torch7,https://github.com/torch/torch7,785,2016-10-03T05:58:48Z,2016-10-07T14:43:56Z,2016-10-07T14:44:13Z,MERGED,True,521,271,31,https://github.com/BTNC,"replace long with ptrdiff_t for memory size/offset, element counts etc",2,[],https://github.com/torch/torch7/pull/785,https://github.com/soumith,2,https://github.com/torch/torch7/pull/785#issuecomment-252271336,"Hi,
Type long is used extensively in torch mainly for (1) memory size/offset, element counts; (2) representing data; (3) meeting 3rd party api in/out specification. This PR is about long usage in (1).
Impact:
(1) for users on linux/mac where rang of long is same as range of ptrdiff_t, this PR is expected to introduce no behavior change.
(2) for users on windows/arm x64 etc where long is 32bit, the maximum managed memory by torch used to be 2G, but now the limit is extended to 2^63, same as linux/mac users.
Replacing long with ptrdiff_t is a relatively smooth change, though identifying which long should be replaced is laborious. For places that should better use size_t, size_t change can be on top of the ptrdiff_t change, i.e. replacing some ptrdiff_t with size_t. I did not replace some long with size_t because after evaluation I found replacement with size_t requires code logic changes in many places which could complicate this PR and introduce big behavior change for most users, and I am uncertain about that impact.
Please let me know if any part in this PR is not about usage (1) or this PR miss any change about usage (1).
For long usage (2), my proposal is to have a torch long type like th_Long which can be configured for integer types other than long. So windows/arm x64 users can also use 64bit integer. THGenerateIntTypes.h is now providing very limited configuration.
Thanks,",Thanks a lot @BTNC . Whoever you are you are a godsend for the windows users.,True,{}
torch/torch7,https://github.com/torch/torch7,793,2016-10-12T13:03:08Z,2016-10-12T13:20:18Z,2016-10-12T14:26:52Z,MERGED,True,5,5,1,https://github.com/howard0su,Fix build when NEON is supported,1,[],https://github.com/torch/torch7/pull/793,https://github.com/howard0su,1,https://github.com/torch/torch7/pull/793,This fix the build when I built torch in NVidia JETSON,This fix the build when I built torch in NVidia JETSON,True,{}
torch/torch7,https://github.com/torch/torch7,793,2016-10-12T13:03:08Z,2016-10-12T13:20:18Z,2016-10-12T14:26:52Z,MERGED,True,5,5,1,https://github.com/howard0su,Fix build when NEON is supported,1,[],https://github.com/torch/torch7/pull/793,https://github.com/soumith,2,https://github.com/torch/torch7/pull/793#issuecomment-253211194,This fix the build when I built torch in NVidia JETSON,Thanks a lot Howard,True,{}
torch/torch7,https://github.com/torch/torch7,793,2016-10-12T13:03:08Z,2016-10-12T13:20:18Z,2016-10-12T14:26:52Z,MERGED,True,5,5,1,https://github.com/howard0su,Fix build when NEON is supported,1,[],https://github.com/torch/torch7/pull/793,https://github.com/howard0su,3,https://github.com/torch/torch7/pull/793#issuecomment-253228577,This fix the build when I built torch in NVidia JETSON,"There are some issues about this,. We can close them as well.
Soumith Chintala notifications@github.com于2016年10月12日周三 下午9:20写道：

Thanks a lot Howard
—
You are receiving this because you authored the thread.
Reply to this email directly, view it on GitHub
#793 (comment), or mute
the thread
https://github.com/notifications/unsubscribe-auth/AAuzRKNwSRtElbIcT8ut6nYxd6ZTHSMuks5qzN6tgaJpZM4KUuoY
.

-Howard",True,{}
torch/torch7,https://github.com/torch/torch7,796,2016-10-13T07:19:17Z,2016-10-13T13:20:02Z,2016-10-13T13:20:16Z,MERGED,True,12,0,1,https://github.com/howard0su,Fix error when run torch.test multi times,1,[],https://github.com/torch/torch7/pull/796,https://github.com/howard0su,1,https://github.com/torch/torch7/pull/796,Fix issue #749,Fix issue #749,True,{}
torch/torch7,https://github.com/torch/torch7,796,2016-10-13T07:19:17Z,2016-10-13T13:20:02Z,2016-10-13T13:20:16Z,MERGED,True,12,0,1,https://github.com/howard0su,Fix error when run torch.test multi times,1,[],https://github.com/torch/torch7/pull/796,https://github.com/soumith,2,https://github.com/torch/torch7/pull/796#issuecomment-253510804,Fix issue #749,Thank you!,True,{}
torch/torch7,https://github.com/torch/torch7,797,2016-10-13T14:43:29Z,2016-10-13T16:10:51Z,2016-10-13T16:10:51Z,MERGED,True,6,1,1,https://github.com/BTNC,change timeSort test to work on windows,1,[],https://github.com/torch/torch7/pull/797,https://github.com/BTNC,1,https://github.com/torch/torch7/pull/797,"Hi,
Minor change to make timeSort.lua work on windows.
Thanks,","Hi,
Minor change to make timeSort.lua work on windows.
Thanks,",True,{}
torch/torch7,https://github.com/torch/torch7,800,2016-10-17T17:11:59Z,2016-10-17T17:18:41Z,2016-10-17T17:18:45Z,MERGED,True,35,18,3,https://github.com/colesbury,Expose OpenMP num threads through TH lib,1,[],https://github.com/torch/torch7/pull/800,https://github.com/colesbury,1,https://github.com/torch/torch7/pull/800,"Expose omp_set_num_threads and similar APIs through the TH lib. This
means a third-party libaries using TH don't need to be compiled with
OpenMP support just to control the number of TH OMP threads.","Expose omp_set_num_threads and similar APIs through the TH lib. This
means a third-party libaries using TH don't need to be compiled with
OpenMP support just to control the number of TH OMP threads.",True,{}
torch/torch7,https://github.com/torch/torch7,800,2016-10-17T17:11:59Z,2016-10-17T17:18:41Z,2016-10-17T17:18:45Z,MERGED,True,35,18,3,https://github.com/colesbury,Expose OpenMP num threads through TH lib,1,[],https://github.com/torch/torch7/pull/800,https://github.com/soumith,2,https://github.com/torch/torch7/pull/800#issuecomment-254272711,"Expose omp_set_num_threads and similar APIs through the TH lib. This
means a third-party libaries using TH don't need to be compiled with
OpenMP support just to control the number of TH OMP threads.",Thanks!,True,{}
torch/torch7,https://github.com/torch/torch7,801,2016-10-17T23:22:02Z,2016-12-30T16:44:58Z,2016-12-30T16:44:58Z,CLOSED,False,10,7,1,https://github.com/howard0su,Fix build on other platform than x86/arm with extension support.,1,[],https://github.com/torch/torch7/pull/801,https://github.com/howard0su,1,https://github.com/torch/torch7/pull/801,"try to fix the build issue reported by #762
I only has access to amd64 and arm box. so I only tested it on that platforms.","try to fix the build issue reported by #762
I only has access to amd64 and arm box. so I only tested it on that platforms.",True,{}
torch/torch7,https://github.com/torch/torch7,801,2016-10-17T23:22:02Z,2016-12-30T16:44:58Z,2016-12-30T16:44:58Z,CLOSED,False,10,7,1,https://github.com/howard0su,Fix build on other platform than x86/arm with extension support.,1,[],https://github.com/torch/torch7/pull/801,https://github.com/soumith,2,https://github.com/torch/torch7/pull/801#issuecomment-269793106,"try to fix the build issue reported by #762
I only has access to amd64 and arm box. so I only tested it on that platforms.",this should be fixed in master by other PRs,True,{}
torch/torch7,https://github.com/torch/torch7,803,2016-10-19T02:59:44Z,2016-10-19T05:24:52Z,2016-10-19T05:24:52Z,CLOSED,False,106,3,8,https://github.com/BTNC,add read/writePointer to be used in threads (de)serializePointer,4,[],https://github.com/torch/torch7/pull/803,https://github.com/BTNC,1,https://github.com/torch/torch7/pull/803,"Hi,
Added read/writePointer together with a test case 'readWritePointer' in test/test_sharedmem.lua. The implementation of (de)serializePointer in threads will be replaced with these new functions instead of relying on readWriteLong/Double.
Thanks,","Hi,
Added read/writePointer together with a test case 'readWritePointer' in test/test_sharedmem.lua. The implementation of (de)serializePointer in threads will be replaced with these new functions instead of relying on readWriteLong/Double.
Thanks,",True,{}
torch/torch7,https://github.com/torch/torch7,803,2016-10-19T02:59:44Z,2016-10-19T05:24:52Z,2016-10-19T05:24:52Z,CLOSED,False,106,3,8,https://github.com/BTNC,add read/writePointer to be used in threads (de)serializePointer,4,[],https://github.com/torch/torch7/pull/803,https://github.com/BTNC,2,https://github.com/torch/torch7/pull/803#issuecomment-254713521,"Hi,
Added read/writePointer together with a test case 'readWritePointer' in test/test_sharedmem.lua. The implementation of (de)serializePointer in threads will be replaced with these new functions instead of relying on readWriteLong/Double.
Thanks,",To resend a new pr in case commits to resolve *nix compilation error pollute master's log.,True,{}
torch/torch7,https://github.com/torch/torch7,804,2016-10-19T05:38:23Z,,2017-03-25T04:04:39Z,OPEN,False,105,3,8,https://github.com/BTNC,add read/writePointer to be used in threads (de)serializePointer,2,[],https://github.com/torch/torch7/pull/804,https://github.com/BTNC,1,https://github.com/torch/torch7/pull/804,"Hi,
Added read/writePointer together with a test case 'readWritePointer' in test/test_sharedmem.lua. The implementation of (de)serializePointer in threads will be replaced with these new functions instead of relying on readWriteLong/Double.
Travis-ci fail for osx with following error during test, but the compilation does succeed.
/Users/travis/torch/install/bin/luajit: /Users/travis/torch/install/share/lua/5.1/torch/init.lua:13: cannot load '/Users/travis/torch/install/lib/lua/5.1/libtorch.so'
Thanks,","Hi,
Added read/writePointer together with a test case 'readWritePointer' in test/test_sharedmem.lua. The implementation of (de)serializePointer in threads will be replaced with these new functions instead of relying on readWriteLong/Double.
Travis-ci fail for osx with following error during test, but the compilation does succeed.
/Users/travis/torch/install/bin/luajit: /Users/travis/torch/install/share/lua/5.1/torch/init.lua:13: cannot load '/Users/travis/torch/install/lib/lua/5.1/libtorch.so'
Thanks,",True,{}
torch/torch7,https://github.com/torch/torch7,804,2016-10-19T05:38:23Z,,2017-03-25T04:04:39Z,OPEN,False,105,3,8,https://github.com/BTNC,add read/writePointer to be used in threads (de)serializePointer,2,[],https://github.com/torch/torch7/pull/804,https://github.com/soumith,2,https://github.com/torch/torch7/pull/804#issuecomment-254716085,"Hi,
Added read/writePointer together with a test case 'readWritePointer' in test/test_sharedmem.lua. The implementation of (de)serializePointer in threads will be replaced with these new functions instead of relying on readWriteLong/Double.
Travis-ci fail for osx with following error during test, but the compilation does succeed.
/Users/travis/torch/install/bin/luajit: /Users/travis/torch/install/share/lua/5.1/torch/init.lua:13: cannot load '/Users/travis/torch/install/lib/lua/5.1/libtorch.so'
Thanks,","thanks. dont worry about the OSX build, it's been failing on travis, i have to take a look at it.",True,{}
torch/torch7,https://github.com/torch/torch7,804,2016-10-19T05:38:23Z,,2017-03-25T04:04:39Z,OPEN,False,105,3,8,https://github.com/BTNC,add read/writePointer to be used in threads (de)serializePointer,2,[],https://github.com/torch/torch7/pull/804,https://github.com/BTNC,3,https://github.com/torch/torch7/pull/804#issuecomment-289187089,"Hi,
Added read/writePointer together with a test case 'readWritePointer' in test/test_sharedmem.lua. The implementation of (de)serializePointer in threads will be replaced with these new functions instead of relying on readWriteLong/Double.
Travis-ci fail for osx with following error during test, but the compilation does succeed.
/Users/travis/torch/install/bin/luajit: /Users/travis/torch/install/share/lua/5.1/torch/init.lua:13: cannot load '/Users/travis/torch/install/lib/lua/5.1/libtorch.so'
Thanks,","Yes, it seems only useful for MemoryFile. However if remove this from DiskFile, it would be against the purpose of virtual table and will complicate the implementation and maintenance. By using virtual table (as in c++), it is not expected some sub class have null function pointers in virtual table. To remove this from DiskFile, the function pointers have to be initialized as null pointers and the callers have to check if the function pointers are null pointers before calling them. This special logic will make the implementation less straightforward and less maintainable. Besides, it will add unnecessary null pointer check for the normal case, the MemoryFile case.",True,{}
torch/torch7,https://github.com/torch/torch7,812,2016-10-24T13:34:08Z,2016-10-24T14:35:27Z,2016-10-24T14:35:31Z,MERGED,True,23,18,1,https://github.com/howard0su,Fix the reference so that trepl can find the right document,1,[],https://github.com/torch/torch7/pull/812,https://github.com/howard0su,1,https://github.com/torch/torch7/pull/812,,,True,{}
torch/torch7,https://github.com/torch/torch7,812,2016-10-24T13:34:08Z,2016-10-24T14:35:27Z,2016-10-24T14:35:31Z,MERGED,True,23,18,1,https://github.com/howard0su,Fix the reference so that trepl can find the right document,1,[],https://github.com/torch/torch7/pull/812,https://github.com/soumith,2,https://github.com/torch/torch7/pull/812#issuecomment-255758362,,Thanks!,True,{}
torch/torch7,https://github.com/torch/torch7,817,2016-10-26T14:27:23Z,2016-10-26T14:27:32Z,2016-11-08T17:20:48Z,MERGED,True,2,6,1,https://github.com/soumith,fix bug in mmaping,1,[],https://github.com/torch/torch7/pull/817,https://github.com/soumith,1,https://github.com/torch/torch7/pull/817,fixes #809,fixes #809,True,{}
torch/torch7,https://github.com/torch/torch7,817,2016-10-26T14:27:23Z,2016-10-26T14:27:32Z,2016-11-08T17:20:48Z,MERGED,True,2,6,1,https://github.com/soumith,fix bug in mmaping,1,[],https://github.com/torch/torch7/pull/817,https://github.com/tworec,2,https://github.com/torch/torch7/pull/817#issuecomment-256370210,fixes #809,This commit doesn't change condition which was cause of #809 but I'll check,True,{}
torch/torch7,https://github.com/torch/torch7,817,2016-10-26T14:27:23Z,2016-10-26T14:27:32Z,2016-11-08T17:20:48Z,MERGED,True,2,6,1,https://github.com/soumith,fix bug in mmaping,1,[],https://github.com/torch/torch7/pull/817,https://github.com/apaszke,3,https://github.com/torch/torch7/pull/817#issuecomment-256383169,fixes #809,"It doesn't change the condition, but it adds a call that performs a side effect of lseek.",True,{}
torch/torch7,https://github.com/torch/torch7,817,2016-10-26T14:27:23Z,2016-10-26T14:27:32Z,2016-11-08T17:20:48Z,MERGED,True,2,6,1,https://github.com/soumith,fix bug in mmaping,1,[],https://github.com/torch/torch7/pull/817,https://github.com/tworec,4,https://github.com/torch/torch7/pull/817#issuecomment-259200246,fixes #809,I confirm. This resolved #809,True,{}
torch/torch7,https://github.com/torch/torch7,818,2016-10-26T19:16:03Z,2016-10-29T23:03:26Z,2016-10-29T23:03:26Z,CLOSED,False,597,107,35,https://github.com/borisfom,torch.HalfTensor mostly working,7,[],https://github.com/torch/torch7/pull/818,https://github.com/borisfom,1,https://github.com/torch/torch7/pull/818,"@soumith, @szagoruyko , @ngimel:
Here, first cut, as we discussed in torch/cutorch#484.
Math for HalfTensor should be extended later - only for ARM and other CPUs with native __fp16 type.","@soumith, @szagoruyko , @ngimel:
Here, first cut, as we discussed in torch/cutorch#484.
Math for HalfTensor should be extended later - only for ARM and other CPUs with native __fp16 type.",True,"{'THUMBS_UP': ['https://github.com/fmassa', 'https://github.com/soumith']}"
torch/torch7,https://github.com/torch/torch7,818,2016-10-26T19:16:03Z,2016-10-29T23:03:26Z,2016-10-29T23:03:26Z,CLOSED,False,597,107,35,https://github.com/borisfom,torch.HalfTensor mostly working,7,[],https://github.com/torch/torch7/pull/818,https://github.com/ngimel,2,https://github.com/torch/torch7/pull/818#issuecomment-256474770,"@soumith, @szagoruyko , @ngimel:
Here, first cut, as we discussed in torch/cutorch#484.
Math for HalfTensor should be extended later - only for ARM and other CPUs with native __fp16 type.","torch.HalfTensor will also have to be added to cutorch, in particular Tensor.lua and generic/CTensor.c",True,{}
torch/torch7,https://github.com/torch/torch7,818,2016-10-26T19:16:03Z,2016-10-29T23:03:26Z,2016-10-29T23:03:26Z,CLOSED,False,597,107,35,https://github.com/borisfom,torch.HalfTensor mostly working,7,[],https://github.com/torch/torch7/pull/818,https://github.com/borisfom,3,https://github.com/torch/torch7/pull/818#issuecomment-256505832,"@soumith, @szagoruyko , @ngimel:
Here, first cut, as we discussed in torch/cutorch#484.
Math for HalfTensor should be extended later - only for ARM and other CPUs with native __fp16 type.",@ngimel : absolutely! I am also preparing to push a commit with (temporary) config switches so that the rest of modules build.,True,{}
torch/torch7,https://github.com/torch/torch7,818,2016-10-26T19:16:03Z,2016-10-29T23:03:26Z,2016-10-29T23:03:26Z,CLOSED,False,597,107,35,https://github.com/borisfom,torch.HalfTensor mostly working,7,[],https://github.com/torch/torch7/pull/818,https://github.com/borisfom,4,https://github.com/torch/torch7/pull/818#issuecomment-256520389,"@soumith, @szagoruyko , @ngimel:
Here, first cut, as we discussed in torch/cutorch#484.
Math for HalfTensor should be extended later - only for ARM and other CPUs with native __fp16 type.","@soumith: I think I have fixed it all as of commit b60e524, should be ready to go!",True,{}
torch/torch7,https://github.com/torch/torch7,818,2016-10-26T19:16:03Z,2016-10-29T23:03:26Z,2016-10-29T23:03:26Z,CLOSED,False,597,107,35,https://github.com/borisfom,torch.HalfTensor mostly working,7,[],https://github.com/torch/torch7/pull/818,https://github.com/borisfom,5,https://github.com/torch/torch7/pull/818#issuecomment-256855465,"@soumith, @szagoruyko , @ngimel:
Here, first cut, as we discussed in torch/cutorch#484.
Math for HalfTensor should be extended later - only for ARM and other CPUs with native __fp16 type.","@soumith: actually, please hold on on merging this: after intergrating with cutorch I think the type should go into cwrap to avoid duplication (torch's torchcwrap is not exported). Also the C type name should be the same - not so hard to do.",True,{}
torch/torch7,https://github.com/torch/torch7,818,2016-10-26T19:16:03Z,2016-10-29T23:03:26Z,2016-10-29T23:03:26Z,CLOSED,False,597,107,35,https://github.com/borisfom,torch.HalfTensor mostly working,7,[],https://github.com/torch/torch7/pull/818,https://github.com/borisfom,6,https://github.com/torch/torch7/pull/818#issuecomment-256855740,"@soumith, @szagoruyko , @ngimel:
Here, first cut, as we discussed in torch/cutorch#484.
Math for HalfTensor should be extended later - only for ARM and other CPUs with native __fp16 type.","@soumith: it you think we should leave cwrap alone, then I suppose, exporting torchcwrap is in order - please let me know!",True,{}
torch/torch7,https://github.com/torch/torch7,818,2016-10-26T19:16:03Z,2016-10-29T23:03:26Z,2016-10-29T23:03:26Z,CLOSED,False,597,107,35,https://github.com/borisfom,torch.HalfTensor mostly working,7,[],https://github.com/torch/torch7/pull/818,https://github.com/soumith,7,https://github.com/torch/torch7/pull/818#issuecomment-256944761,"@soumith, @szagoruyko , @ngimel:
Here, first cut, as we discussed in torch/cutorch#484.
Math for HalfTensor should be extended later - only for ARM and other CPUs with native __fp16 type.",i think the type being here is fine. it's not a fundamental type and does not have to go into cwrap.,True,{}
torch/torch7,https://github.com/torch/torch7,818,2016-10-26T19:16:03Z,2016-10-29T23:03:26Z,2016-10-29T23:03:26Z,CLOSED,False,597,107,35,https://github.com/borisfom,torch.HalfTensor mostly working,7,[],https://github.com/torch/torch7/pull/818,https://github.com/borisfom,8,https://github.com/torch/torch7/pull/818#issuecomment-256978709,"@soumith, @szagoruyko , @ngimel:
Here, first cut, as we discussed in torch/cutorch#484.
Math for HalfTensor should be extended later - only for ARM and other CPUs with native __fp16 type.","@soumith : right, I reached the same conclusion about the location of 'half' - should be in pkg/torch.
I am finishing my tweaks to use the same type name - it looks cleaner and let us avoid a few dummy type conversions, but it's a bit more tedious as I can't simply move it from cutorch as it relies on CUDA header so the definition needs to be conditional depending of where THHalf.h is included from.",True,{}
torch/torch7,https://github.com/torch/torch7,818,2016-10-26T19:16:03Z,2016-10-29T23:03:26Z,2016-10-29T23:03:26Z,CLOSED,False,597,107,35,https://github.com/borisfom,torch.HalfTensor mostly working,7,[],https://github.com/torch/torch7/pull/818,https://github.com/soumith,9,https://github.com/torch/torch7/pull/818#issuecomment-257046786,"@soumith, @szagoruyko , @ngimel:
Here, first cut, as we discussed in torch/cutorch#484.
Math for HalfTensor should be extended later - only for ARM and other CPUs with native __fp16 type.",cc: @gchanan,True,{}
torch/torch7,https://github.com/torch/torch7,818,2016-10-26T19:16:03Z,2016-10-29T23:03:26Z,2016-10-29T23:03:26Z,CLOSED,False,597,107,35,https://github.com/borisfom,torch.HalfTensor mostly working,7,[],https://github.com/torch/torch7/pull/818,https://github.com/borisfom,10,https://github.com/torch/torch7/pull/818#issuecomment-257121162,"@soumith, @szagoruyko , @ngimel:
Here, first cut, as we discussed in torch/cutorch#484.
Math for HalfTensor should be extended later - only for ARM and other CPUs with native __fp16 type.",Superceded by #826,True,{}
torch/torch7,https://github.com/torch/torch7,819,2016-10-26T19:33:08Z,2016-10-26T20:26:49Z,2016-10-26T20:26:49Z,MERGED,True,6,6,4,https://github.com/colesbury,Fix no-arg function prototypes,1,[],https://github.com/torch/torch7/pull/819,https://github.com/colesbury,1,https://github.com/torch/torch7/pull/819,"Fixes some compiler warnings.
(see http://stackoverflow.com/questions/416345/is-fvoid-deprecated-in-modern-c-and-c)","Fixes some compiler warnings.
(see http://stackoverflow.com/questions/416345/is-fvoid-deprecated-in-modern-c-and-c)",True,{'HOORAY': ['https://github.com/apaszke']}
torch/torch7,https://github.com/torch/torch7,820,2016-10-27T18:47:16Z,2016-10-27T19:06:19Z,2016-10-27T19:06:22Z,MERGED,True,1,1,1,https://github.com/jdmartin86,Updated maskedCopy example #1,1,[],https://github.com/torch/torch7/pull/820,https://github.com/jdmartin86,1,https://github.com/torch/torch7/pull/820,"Original call to maskedCopy() only included y, not the mask. Update added the mask to the first argument.","Original call to maskedCopy() only included y, not the mask. Update added the mask to the first argument.",True,{}
torch/torch7,https://github.com/torch/torch7,820,2016-10-27T18:47:16Z,2016-10-27T19:06:19Z,2016-10-27T19:06:22Z,MERGED,True,1,1,1,https://github.com/jdmartin86,Updated maskedCopy example #1,1,[],https://github.com/torch/torch7/pull/820,https://github.com/soumith,2,https://github.com/torch/torch7/pull/820#issuecomment-256739117,"Original call to maskedCopy() only included y, not the mask. Update added the mask to the first argument.",Thanks!,True,{}
torch/torch7,https://github.com/torch/torch7,821,2016-10-28T07:36:50Z,2016-10-29T12:49:11Z,2016-11-04T07:10:57Z,CLOSED,False,2,2,2,https://github.com/BTNC,fix one func call for lua52/53 on win; update rockspec for win,1,[],https://github.com/torch/torch7/pull/821,https://github.com/BTNC,1,https://github.com/torch/torch7/pull/821,,,True,{}
torch/torch7,https://github.com/torch/torch7,821,2016-10-28T07:36:50Z,2016-10-29T12:49:11Z,2016-11-04T07:10:57Z,CLOSED,False,2,2,2,https://github.com/BTNC,fix one func call for lua52/53 on win; update rockspec for win,1,[],https://github.com/torch/torch7/pull/821,https://github.com/BTNC,2,https://github.com/torch/torch7/pull/821#issuecomment-257089892,,"The (void) cast should be checked in because lua_pushstring is returning char * instead of void while lua_pushnil is still returning void from lua52 on which cause msvc fail to compile, but the rockspec change is not needed.",True,{}
torch/torch7,https://github.com/torch/torch7,823,2016-10-28T19:03:36Z,2016-10-28T19:49:19Z,2016-10-28T19:49:19Z,CLOSED,False,2,0,1,https://github.com/colesbury,Use -O3 when compiling TH,1,[],https://github.com/torch/torch7/pull/823,https://github.com/colesbury,1,https://github.com/torch/torch7/pull/823,"On my machine, this speeds up FloatTensor copies by 4x.","On my machine, this speeds up FloatTensor copies by 4x.",True,{}
torch/torch7,https://github.com/torch/torch7,826,2016-10-29T22:06:15Z,2016-12-30T11:00:17Z,2017-02-01T19:47:32Z,CLOSED,False,703,109,38,https://github.com/borisfom,"Inroducing CPU ""half"" type for 16-bit float, copy and init only",19,[],https://github.com/torch/torch7/pull/826,https://github.com/borisfom,1,https://github.com/torch/torch7/pull/826,This is a finished and squashed version of #818.,This is a finished and squashed version of #818.,True,{}
torch/torch7,https://github.com/torch/torch7,826,2016-10-29T22:06:15Z,2016-12-30T11:00:17Z,2017-02-01T19:47:32Z,CLOSED,False,703,109,38,https://github.com/borisfom,"Inroducing CPU ""half"" type for 16-bit float, copy and init only",19,[],https://github.com/torch/torch7/pull/826,https://github.com/borisfom,2,https://github.com/torch/torch7/pull/826#issuecomment-259772858,This is a finished and squashed version of #818.,@soumith: this one should be ready to go.,True,{}
torch/torch7,https://github.com/torch/torch7,826,2016-10-29T22:06:15Z,2016-12-30T11:00:17Z,2017-02-01T19:47:32Z,CLOSED,False,703,109,38,https://github.com/borisfom,"Inroducing CPU ""half"" type for 16-bit float, copy and init only",19,[],https://github.com/torch/torch7/pull/826,https://github.com/soumith,3,https://github.com/torch/torch7/pull/826#issuecomment-261095758,This is a finished and squashed version of #818.,thanks. will get this reviewed.,True,{}
torch/torch7,https://github.com/torch/torch7,826,2016-10-29T22:06:15Z,2016-12-30T11:00:17Z,2017-02-01T19:47:32Z,CLOSED,False,703,109,38,https://github.com/borisfom,"Inroducing CPU ""half"" type for 16-bit float, copy and init only",19,[],https://github.com/torch/torch7/pull/826,https://github.com/borisfom,4,https://github.com/torch/torch7/pull/826#issuecomment-263447869,This is a finished and squashed version of #818.,"@soumith, @gchanan : I believe I have addressed all the code review comments.
Please verify if it's ready to go!",True,{}
torch/torch7,https://github.com/torch/torch7,826,2016-10-29T22:06:15Z,2016-12-30T11:00:17Z,2017-02-01T19:47:32Z,CLOSED,False,703,109,38,https://github.com/borisfom,"Inroducing CPU ""half"" type for 16-bit float, copy and init only",19,[],https://github.com/torch/torch7/pull/826,https://github.com/borisfom,5,https://github.com/torch/torch7/pull/826#issuecomment-269758807,This is a finished and squashed version of #818.,Merged via #874,True,{}
torch/torch7,https://github.com/torch/torch7,827,2016-11-02T12:31:01Z,2016-11-02T20:10:50Z,2016-11-03T10:18:34Z,MERGED,True,0,2,1,https://github.com/howard0su,Fix compile error on freebsd,1,[],https://github.com/torch/torch7/pull/827,https://github.com/howard0su,1,https://github.com/torch/torch7/pull/827,"On FreeBSD, int32_t is defined in stdint.h as well. this change fix the problem. Since stdint.h is part of Posix, it should be safe to include it unconditional.","On FreeBSD, int32_t is defined in stdint.h as well. this change fix the problem. Since stdint.h is part of Posix, it should be safe to include it unconditional.",True,{}
torch/torch7,https://github.com/torch/torch7,828,2016-11-02T19:54:50Z,2016-11-02T22:53:45Z,2016-11-02T22:53:50Z,MERGED,True,73,16,1,https://github.com/apaszke,Add more size checks and improve some LAPACK error messages,1,[],https://github.com/torch/torch7/pull/828,https://github.com/apaszke,1,https://github.com/torch/torch7/pull/828,"Also, linear system solvers can now accept vectors (they had to be reshaped to Nx1 matrices before).","Also, linear system solvers can now accept vectors (they had to be reshaped to Nx1 matrices before).",True,{}
torch/torch7,https://github.com/torch/torch7,828,2016-11-02T19:54:50Z,2016-11-02T22:53:45Z,2016-11-02T22:53:50Z,MERGED,True,73,16,1,https://github.com/apaszke,Add more size checks and improve some LAPACK error messages,1,[],https://github.com/torch/torch7/pull/828,https://github.com/soumith,2,https://github.com/torch/torch7/pull/828#issuecomment-258024055,"Also, linear system solvers can now accept vectors (they had to be reshaped to Nx1 matrices before).",Thanks!,True,{}
torch/torch7,https://github.com/torch/torch7,834,2016-11-11T02:37:32Z,2016-11-11T18:29:50Z,2016-11-11T20:10:35Z,MERGED,True,12,3,2,https://github.com/killeent,Fix implementation of logNormal,1,[],https://github.com/torch/torch7/pull/834,https://github.com/killeent,1,https://github.com/torch/torch7/pull/834,"Current implementation was broken. See numpy:
https://github.com/numpy/numpy/blob/c90d7c94fd2077d0beca48fa89a423da2b0bb663/numpy/random/mtrand/distributions.c#L691
for a correct implementation, which we adopted here.
Test Plan:

Add unit test, stolen from cutorch","Current implementation was broken. See numpy:
https://github.com/numpy/numpy/blob/c90d7c94fd2077d0beca48fa89a423da2b0bb663/numpy/random/mtrand/distributions.c#L691
for a correct implementation, which we adopted here.
Test Plan:

Add unit test, stolen from cutorch",True,{}
torch/torch7,https://github.com/torch/torch7,834,2016-11-11T02:37:32Z,2016-11-11T18:29:50Z,2016-11-11T20:10:35Z,MERGED,True,12,3,2,https://github.com/killeent,Fix implementation of logNormal,1,[],https://github.com/torch/torch7/pull/834,https://github.com/fmassa,2,https://github.com/torch/torch7/pull/834#issuecomment-259987960,"Current implementation was broken. See numpy:
https://github.com/numpy/numpy/blob/c90d7c94fd2077d0beca48fa89a423da2b0bb663/numpy/random/mtrand/distributions.c#L691
for a correct implementation, which we adopted here.
Test Plan:

Add unit test, stolen from cutorch",Are you sure about this? Torch's implementation  seems to be consistent with Matlab's https://fr.mathworks.com/help/stats/lognstat.html,True,{}
torch/torch7,https://github.com/torch/torch7,834,2016-11-11T02:37:32Z,2016-11-11T18:29:50Z,2016-11-11T20:10:35Z,MERGED,True,12,3,2,https://github.com/killeent,Fix implementation of logNormal,1,[],https://github.com/torch/torch7/pull/834,https://github.com/soumith,3,https://github.com/torch/torch7/pull/834#issuecomment-259991279,"Current implementation was broken. See numpy:
https://github.com/numpy/numpy/blob/c90d7c94fd2077d0beca48fa89a423da2b0bb663/numpy/random/mtrand/distributions.c#L691
for a correct implementation, which we adopted here.
Test Plan:

Add unit test, stolen from cutorch","okay so it seems this:
numpy and curand generate a lognormal distribution taking in the mean and stdv of the underlying normal distribution.
torch (and seems like matlab) generate a lognormal distribution taking in the mean and stdv of the currently generated numbers. If one takes the log of the generated numbers, they also have to be normally distributed.
References:

https://docs.scipy.org/doc/numpy/reference/generated/numpy.random.lognormal.html
http://docs.nvidia.com/cuda/curand/host-api-overview.html#generation-functions
https://www.mathworks.com/help/stats/lognstat.html",True,{}
torch/torch7,https://github.com/torch/torch7,834,2016-11-11T02:37:32Z,2016-11-11T18:29:50Z,2016-11-11T20:10:35Z,MERGED,True,12,3,2,https://github.com/killeent,Fix implementation of logNormal,1,[],https://github.com/torch/torch7/pull/834,https://github.com/soumith,4,https://github.com/torch/torch7/pull/834#issuecomment-260022864,"Current implementation was broken. See numpy:
https://github.com/numpy/numpy/blob/c90d7c94fd2077d0beca48fa89a423da2b0bb663/numpy/random/mtrand/distributions.c#L691
for a correct implementation, which we adopted here.
Test Plan:

Add unit test, stolen from cutorch","actually, @fmassa you are wrong.
Torch has gotten logNormal wrong.
In Matlab, SciPy and in curand, logNormal does the same thing. It generates a distribution with mean and stdv given which are of the underlying normal distribution.
I'll merge this in.",True,{}
torch/torch7,https://github.com/torch/torch7,834,2016-11-11T02:37:32Z,2016-11-11T18:29:50Z,2016-11-11T20:10:35Z,MERGED,True,12,3,2,https://github.com/killeent,Fix implementation of logNormal,1,[],https://github.com/torch/torch7/pull/834,https://github.com/fmassa,5,https://github.com/torch/torch7/pull/834#issuecomment-260044837,"Current implementation was broken. See numpy:
https://github.com/numpy/numpy/blob/c90d7c94fd2077d0beca48fa89a423da2b0bb663/numpy/random/mtrand/distributions.c#L691
for a correct implementation, which we adopted here.
Test Plan:

Add unit test, stolen from cutorch","@soumith sorry, I was wrong and the behaviour of matlab, numpy and curand seems to be consistent, my bad.
I got confused because I thought that torch.logNormal was supposed to generate samples from a log normal distribution with the mean and std that is provided by the user.
So if one do
N = 10000
mean = 2
std = 4
t = torch.Tensor(N):logNormal(mean, std)
print(t:mean())
print(t:std())

I would expect t:mean() to be close to 2, and t:std() to be close to 4, and that's what the old behaviour of torch used to do.
But that's not what matlab/numpy/curand do, so it's better to be consistent with them.",True,{}
torch/torch7,https://github.com/torch/torch7,839,2016-11-15T19:48:29Z,2016-11-15T20:57:57Z,2016-11-15T21:15:50Z,MERGED,True,75,237,3,https://github.com/Atcold,"Fix compilation for ASIMD, fix #766",1,[],https://github.com/torch/torch7/pull/839,https://github.com/Atcold,1,https://github.com/torch/torch7/pull/839,"On ARMv8, neon is inherit and instead listed as 'asimd' in /proc/cpuinfo
Replace assembly with C
Original authors:

@dusty-nv
FindARM-patch.txt
CMakeLists-patch.txt
@rtarquini
NEON.c","On ARMv8, neon is inherit and instead listed as 'asimd' in /proc/cpuinfo
Replace assembly with C
Original authors:

@dusty-nv
FindARM-patch.txt
CMakeLists-patch.txt
@rtarquini
NEON.c",True,{}
torch/torch7,https://github.com/torch/torch7,839,2016-11-15T19:48:29Z,2016-11-15T20:57:57Z,2016-11-15T21:15:50Z,MERGED,True,75,237,3,https://github.com/Atcold,"Fix compilation for ASIMD, fix #766",1,[],https://github.com/torch/torch7/pull/839,https://github.com/soumith,2,https://github.com/torch/torch7/pull/839#issuecomment-260765872,"On ARMv8, neon is inherit and instead listed as 'asimd' in /proc/cpuinfo
Replace assembly with C
Original authors:

@dusty-nv
FindARM-patch.txt
CMakeLists-patch.txt
@rtarquini
NEON.c",thank you!,True,{}
torch/torch7,https://github.com/torch/torch7,839,2016-11-15T19:48:29Z,2016-11-15T20:57:57Z,2016-11-15T21:15:50Z,MERGED,True,75,237,3,https://github.com/Atcold,"Fix compilation for ASIMD, fix #766",1,[],https://github.com/torch/torch7/pull/839,https://github.com/Atcold,3,https://github.com/torch/torch7/pull/839#issuecomment-260770530,"On ARMv8, neon is inherit and instead listed as 'asimd' in /proc/cpuinfo
Replace assembly with C
Original authors:

@dusty-nv
FindARM-patch.txt
CMakeLists-patch.txt
@rtarquini
NEON.c","@soumith, Travis says it fails with clang and and LUA52... Is that OK?",True,{}
torch/torch7,https://github.com/torch/torch7,839,2016-11-15T19:48:29Z,2016-11-15T20:57:57Z,2016-11-15T21:15:50Z,MERGED,True,75,237,3,https://github.com/Atcold,"Fix compilation for ASIMD, fix #766",1,[],https://github.com/torch/torch7/pull/839,https://github.com/soumith,4,https://github.com/torch/torch7/pull/839#issuecomment-260770685,"On ARMv8, neon is inherit and instead listed as 'asimd' in /proc/cpuinfo
Replace assembly with C
Original authors:

@dusty-nv
FindARM-patch.txt
CMakeLists-patch.txt
@rtarquini
NEON.c","yes, i fixed the failure (it was a flaky test which i bumped up the tolerance)",True,{'THUMBS_UP': ['https://github.com/Atcold']}
torch/torch7,https://github.com/torch/torch7,842,2016-11-18T04:49:34Z,2016-11-18T05:03:37Z,2016-11-18T05:13:55Z,CLOSED,False,99,118,11,https://github.com/borisfom,Adding macros to control generic types selection,1,[],https://github.com/torch/torch7/pull/842,https://github.com/borisfom,1,https://github.com/torch/torch7/pull/842,Minimal changes to allow reduces set of types + refactoring of GenerateAllXX headers,Minimal changes to allow reduces set of types + refactoring of GenerateAllXX headers,True,{}
torch/torch7,https://github.com/torch/torch7,842,2016-11-18T04:49:34Z,2016-11-18T05:03:37Z,2016-11-18T05:13:55Z,CLOSED,False,99,118,11,https://github.com/borisfom,Adding macros to control generic types selection,1,[],https://github.com/torch/torch7/pull/842,https://github.com/soumith,2,https://github.com/torch/torch7/pull/842#issuecomment-261449559,Minimal changes to allow reduces set of types + refactoring of GenerateAllXX headers,"i'm sorry but you misunderstood me. we should only do this for cutorch, and not for torch.
torch binaries are < 2 MB, this stuff is not necessary at all.
A PR like this for cutorch, with THC_GENERIC_NO_* defined is fine with me.",True,{}
torch/torch7,https://github.com/torch/torch7,842,2016-11-18T04:49:34Z,2016-11-18T05:03:37Z,2016-11-18T05:13:55Z,CLOSED,False,99,118,11,https://github.com/borisfom,Adding macros to control generic types selection,1,[],https://github.com/torch/torch7/pull/842,https://github.com/borisfom,3,https://github.com/torch/torch7/pull/842#issuecomment-261450558,Minimal changes to allow reduces set of types + refactoring of GenerateAllXX headers,"No problem! You're right, not worth the trouble in this module.",True,{}
torch/torch7,https://github.com/torch/torch7,844,2016-11-20T19:53:07Z,2016-11-20T20:13:56Z,2016-11-20T20:13:56Z,CLOSED,False,0,0,0,https://github.com/ultrai,Merge pull request #1 from torch/master,1,[],https://github.com/torch/torch7/pull/844,https://github.com/ultrai,1,https://github.com/torch/torch7/pull/844,update,update,True,{}
torch/torch7,https://github.com/torch/torch7,845,2016-11-21T22:04:29Z,2016-11-21T22:05:24Z,2016-11-21T22:05:30Z,MERGED,True,103,3,2,https://github.com/killeent,Add some documentation for APPLY and DIM_APPLY macros,1,[],https://github.com/torch/torch7/pull/845,https://github.com/killeent,1,https://github.com/torch/torch7/pull/845,"Not sure if this would be beneficial to others, up to you.","Not sure if this would be beneficial to others, up to you.",True,{}
torch/torch7,https://github.com/torch/torch7,845,2016-11-21T22:04:29Z,2016-11-21T22:05:24Z,2016-11-21T22:05:30Z,MERGED,True,103,3,2,https://github.com/killeent,Add some documentation for APPLY and DIM_APPLY macros,1,[],https://github.com/torch/torch7/pull/845,https://github.com/soumith,2,https://github.com/torch/torch7/pull/845#issuecomment-262082407,"Not sure if this would be beneficial to others, up to you.",thanks! :),True,{}
torch/torch7,https://github.com/torch/torch7,851,2016-11-28T02:57:04Z,2016-11-28T05:55:18Z,2016-11-28T05:55:21Z,MERGED,True,2,2,2,https://github.com/BTNC,2 trivial changes to compile with msvc,1,[],https://github.com/torch/torch7/pull/851,https://github.com/BTNC,1,https://github.com/torch/torch7/pull/851,"Hi,

THTensorLapack.c : to be consistent with THLapackCheckWithCleanup
luaT.c : lua_pushstring() returns const char * instead of void since lua 5.2

Thanks,","Hi,

THTensorLapack.c : to be consistent with THLapackCheckWithCleanup
luaT.c : lua_pushstring() returns const char * instead of void since lua 5.2

Thanks,",True,{}
torch/torch7,https://github.com/torch/torch7,851,2016-11-28T02:57:04Z,2016-11-28T05:55:18Z,2016-11-28T05:55:21Z,MERGED,True,2,2,2,https://github.com/BTNC,2 trivial changes to compile with msvc,1,[],https://github.com/torch/torch7/pull/851,https://github.com/soumith,2,https://github.com/torch/torch7/pull/851#issuecomment-263189220,"Hi,

THTensorLapack.c : to be consistent with THLapackCheckWithCleanup
luaT.c : lua_pushstring() returns const char * instead of void since lua 5.2

Thanks,",thanks!,True,{}
torch/torch7,https://github.com/torch/torch7,852,2016-11-28T16:07:24Z,2017-02-28T17:32:02Z,2017-02-28T17:32:03Z,CLOSED,False,1354,567,18,https://github.com/soumith,Adding AVX Optimizations with Runtime Dispatch,73,[],https://github.com/torch/torch7/pull/852,https://github.com/soumith,1,https://github.com/torch/torch7/pull/852,"Work in Progress.
Initial work by Roberth Guthrie (@rguthrie3) and more additions ongoing by Keren Zhou (@Jokeren)
This speeds up our Tensor Math ops. On add, we have up to 11x speedup already, and more to come.","Work in Progress.
Initial work by Roberth Guthrie (@rguthrie3) and more additions ongoing by Keren Zhou (@Jokeren)
This speeds up our Tensor Math ops. On add, we have up to 11x speedup already, and more to come.",True,"{'HEART': ['https://github.com/colesbury', 'https://github.com/Jokeren', 'https://github.com/fmassa', 'https://github.com/apaszke'], 'HOORAY': ['https://github.com/apaszke']}"
torch/torch7,https://github.com/torch/torch7,852,2016-11-28T16:07:24Z,2017-02-28T17:32:02Z,2017-02-28T17:32:03Z,CLOSED,False,1354,567,18,https://github.com/soumith,Adding AVX Optimizations with Runtime Dispatch,73,[],https://github.com/torch/torch7/pull/852,https://github.com/Jokeren,2,https://github.com/torch/torch7/pull/852#issuecomment-263461323,"Work in Progress.
Initial work by Roberth Guthrie (@rguthrie3) and more additions ongoing by Keren Zhou (@Jokeren)
This speeds up our Tensor Math ops. On add, we have up to 11x speedup already, and more to come.","Hi @soumith,
Regarding the coding style, I notice that many functions, such as cadd, remainder, and mul, do not follow the two-space indentation rule. Hence, should I modify their styles?",True,{}
torch/torch7,https://github.com/torch/torch7,852,2016-11-28T16:07:24Z,2017-02-28T17:32:02Z,2017-02-28T17:32:03Z,CLOSED,False,1354,567,18,https://github.com/soumith,Adding AVX Optimizations with Runtime Dispatch,73,[],https://github.com/torch/torch7/pull/852,https://github.com/soumith,3,https://github.com/torch/torch7/pull/852#issuecomment-263645151,"Work in Progress.
Initial work by Roberth Guthrie (@rguthrie3) and more additions ongoing by Keren Zhou (@Jokeren)
This speeds up our Tensor Math ops. On add, we have up to 11x speedup already, and more to come.","@Jokeren you can modify their styles, yes.",True,{}
torch/torch7,https://github.com/torch/torch7,852,2016-11-28T16:07:24Z,2017-02-28T17:32:02Z,2017-02-28T17:32:03Z,CLOSED,False,1354,567,18,https://github.com/soumith,Adding AVX Optimizations with Runtime Dispatch,73,[],https://github.com/torch/torch7/pull/852,https://github.com/Jokeren,4,https://github.com/torch/torch7/pull/852#issuecomment-264389897,"Work in Progress.
Initial work by Roberth Guthrie (@rguthrie3) and more additions ongoing by Keren Zhou (@Jokeren)
This speeds up our Tensor Math ops. On add, we have up to 11x speedup already, and more to come.","@soumith , Should I include <omp.h> when using Clang?",True,{}
torch/torch7,https://github.com/torch/torch7,852,2016-11-28T16:07:24Z,2017-02-28T17:32:02Z,2017-02-28T17:32:03Z,CLOSED,False,1354,567,18,https://github.com/soumith,Adding AVX Optimizations with Runtime Dispatch,73,[],https://github.com/torch/torch7/pull/852,https://github.com/Jokeren,5,https://github.com/torch/torch7/pull/852#issuecomment-264628802,"Work in Progress.
Initial work by Roberth Guthrie (@rguthrie3) and more additions ongoing by Keren Zhou (@Jokeren)
This speeds up our Tensor Math ops. On add, we have up to 11x speedup already, and more to come.","@apaszke, I used <omp.h> to calculate start and end indices for each thread. But it appears that Clang does not support openmp well, at least for the environment on travis-ci. Could you help fix the problem?",True,{}
torch/torch7,https://github.com/torch/torch7,852,2016-11-28T16:07:24Z,2017-02-28T17:32:02Z,2017-02-28T17:32:03Z,CLOSED,False,1354,567,18,https://github.com/soumith,Adding AVX Optimizations with Runtime Dispatch,73,[],https://github.com/torch/torch7/pull/852,https://github.com/apaszke,6,https://github.com/torch/torch7/pull/852#issuecomment-264632541,"Work in Progress.
Initial work by Roberth Guthrie (@rguthrie3) and more additions ongoing by Keren Zhou (@Jokeren)
This speeds up our Tensor Math ops. On add, we have up to 11x speedup already, and more to come.",@Jokeren What's the exact issue you're having?,True,{}
torch/torch7,https://github.com/torch/torch7,852,2016-11-28T16:07:24Z,2017-02-28T17:32:02Z,2017-02-28T17:32:03Z,CLOSED,False,1354,567,18,https://github.com/soumith,Adding AVX Optimizations with Runtime Dispatch,73,[],https://github.com/torch/torch7/pull/852,https://github.com/apaszke,7,https://github.com/torch/torch7/pull/852#issuecomment-264632774,"Work in Progress.
Initial work by Roberth Guthrie (@rguthrie3) and more additions ongoing by Keren Zhou (@Jokeren)
This speeds up our Tensor Math ops. On add, we have up to 11x speedup already, and more to come.","Ok, I've checked the travis build. You shouldn't assume that the lib will be always compiled in an environment that has OpenMP. Use #ifdef _OPENMP to guard these parts like in the other files. You could only hide the #pragma omp parallel ... sections and have the i and i_end always computed as the whole array, if compiled without OpenMP.",True,{}
torch/torch7,https://github.com/torch/torch7,852,2016-11-28T16:07:24Z,2017-02-28T17:32:02Z,2017-02-28T17:32:03Z,CLOSED,False,1354,567,18,https://github.com/soumith,Adding AVX Optimizations with Runtime Dispatch,73,[],https://github.com/torch/torch7/pull/852,https://github.com/Jokeren,8,https://github.com/torch/torch7/pull/852#issuecomment-273473905,"Work in Progress.
Initial work by Roberth Guthrie (@rguthrie3) and more additions ongoing by Keren Zhou (@Jokeren)
This speeds up our Tensor Math ops. On add, we have up to 11x speedup already, and more to come.","Hi, @adamlerer, I have rebased the branch.",True,{}
torch/torch7,https://github.com/torch/torch7,852,2016-11-28T16:07:24Z,2017-02-28T17:32:02Z,2017-02-28T17:32:03Z,CLOSED,False,1354,567,18,https://github.com/soumith,Adding AVX Optimizations with Runtime Dispatch,73,[],https://github.com/torch/torch7/pull/852,https://github.com/pavanky,9,https://github.com/torch/torch7/pull/852#issuecomment-273510050,"Work in Progress.
Initial work by Roberth Guthrie (@rguthrie3) and more additions ongoing by Keren Zhou (@Jokeren)
This speeds up our Tensor Math ops. On add, we have up to 11x speedup already, and more to come.",@Jokeren something went wrong. There are commits appearing multiple times.,True,{}
torch/torch7,https://github.com/torch/torch7,852,2016-11-28T16:07:24Z,2017-02-28T17:32:02Z,2017-02-28T17:32:03Z,CLOSED,False,1354,567,18,https://github.com/soumith,Adding AVX Optimizations with Runtime Dispatch,73,[],https://github.com/torch/torch7/pull/852,https://github.com/Jokeren,10,https://github.com/torch/torch7/pull/852#issuecomment-273519274,"Work in Progress.
Initial work by Roberth Guthrie (@rguthrie3) and more additions ongoing by Keren Zhou (@Jokeren)
This speeds up our Tensor Math ops. On add, we have up to 11x speedup already, and more to come.","Thanks, @pavanky , I have noticed the problem.
I just rebased the branch again by git rebase -i. So it appears that all the previous commits are appended. Could you provide me some advises to fix it?",True,{}
torch/torch7,https://github.com/torch/torch7,852,2016-11-28T16:07:24Z,2017-02-28T17:32:02Z,2017-02-28T17:32:03Z,CLOSED,False,1354,567,18,https://github.com/soumith,Adding AVX Optimizations with Runtime Dispatch,73,[],https://github.com/torch/torch7/pull/852,https://github.com/pavanky,11,https://github.com/torch/torch7/pull/852#issuecomment-273522471,"Work in Progress.
Initial work by Roberth Guthrie (@rguthrie3) and more additions ongoing by Keren Zhou (@Jokeren)
This speeds up our Tensor Math ops. On add, we have up to 11x speedup already, and more to come.","I tried benchmarking the following code on master as well as the branch on this PR. I am getting identical performance.
local n = 1 << 24
local a = torch.DoubleTensor(1 << 24):uniform()
local b = torch.DoubleTensor(1 << 24):uniform()
local timer = torch.Timer()
local niter = 100
for i = 1, niter do
   a:add(b)
end
local ts = timer:time().real
print('Time taken (Size: ' .. n .. ', niter: '  .. niter .. '): ' .. ts .. ' s')

Is this may be related to cmake not finding AVX1 (which defines USE_AVX) but finding AVX2 (which defines USE_AVX2 but not USE_AVX) ? Or am I doing the benchmark incorrectly ?
This is the output I am seeing on i7-5500U with gcc 5.4.0.
-- Performing Test C_HAS_SSE1_1
-- Performing Test C_HAS_SSE1_1 - Success
-- Performing Test C_HAS_SSE2_1
-- Performing Test C_HAS_SSE2_1 - Success
-- Performing Test C_HAS_SSE3_1
-- Performing Test C_HAS_SSE3_1 - Failed
-- Performing Test C_HAS_SSE3_2
-- Performing Test C_HAS_SSE3_2 - Success
-- Performing Test C_HAS_SSE4_1_1
-- Performing Test C_HAS_SSE4_1_1 - Failed
-- Performing Test C_HAS_SSE4_1_2
-- Performing Test C_HAS_SSE4_1_2 - Success
-- Performing Test C_HAS_SSE4_2_1
-- Performing Test C_HAS_SSE4_2_1 - Failed
-- Performing Test C_HAS_SSE4_2_2
-- Performing Test C_HAS_SSE4_2_2 - Success
-- Performing Test C_HAS_AVX_1
-- Performing Test C_HAS_AVX_1 - Failed
-- Performing Test C_HAS_AVX_2
-- Performing Test C_HAS_AVX_2 - Success
-- Performing Test C_HAS_AVX2_1
-- Performing Test C_HAS_AVX2_1 - Failed
-- Performing Test C_HAS_AVX2_2
-- Performing Test C_HAS_AVX2_2 - Success
-- Performing Test CXX_HAS_SSE1_1
-- Performing Test CXX_HAS_SSE1_1 - Success
-- Performing Test CXX_HAS_SSE2_1
-- Performing Test CXX_HAS_SSE2_1 - Success
-- Performing Test CXX_HAS_SSE3_1
-- Performing Test CXX_HAS_SSE3_1 - Failed
-- Performing Test CXX_HAS_SSE3_2
-- Performing Test CXX_HAS_SSE3_2 - Success
-- Performing Test CXX_HAS_SSE4_1_1
-- Performing Test CXX_HAS_SSE4_1_1 - Failed
-- Performing Test CXX_HAS_SSE4_1_2
-- Performing Test CXX_HAS_SSE4_1_2 - Success
-- Performing Test CXX_HAS_SSE4_2_1
-- Performing Test CXX_HAS_SSE4_2_1 - Failed
-- Performing Test CXX_HAS_SSE4_2_2
-- Performing Test CXX_HAS_SSE4_2_2 - Success
-- Performing Test CXX_HAS_AVX_1
-- Performing Test CXX_HAS_AVX_1 - Failed
-- Performing Test CXX_HAS_AVX_2
-- Performing Test CXX_HAS_AVX_2 - Success
-- Performing Test CXX_HAS_AVX2_1
-- Performing Test CXX_HAS_AVX2_1 - Failed
-- Performing Test CXX_HAS_AVX2_2
-- Performing Test CXX_HAS_AVX2_2 - Success",True,{}
torch/torch7,https://github.com/torch/torch7,852,2016-11-28T16:07:24Z,2017-02-28T17:32:02Z,2017-02-28T17:32:03Z,CLOSED,False,1354,567,18,https://github.com/soumith,Adding AVX Optimizations with Runtime Dispatch,73,[],https://github.com/torch/torch7/pull/852,https://github.com/Jokeren,12,https://github.com/torch/torch7/pull/852#issuecomment-273532380,"Work in Progress.
Initial work by Roberth Guthrie (@rguthrie3) and more additions ongoing by Keren Zhou (@Jokeren)
This speeds up our Tensor Math ops. On add, we have up to 11x speedup already, and more to come.","Hi, @pavanky , I have tried your benchmark scripts.
a:add(b): this statement calls THBlas_(axpy) in the THTensor_(cadd) function. Thus, suppose you have MKL installed, it will automatically invokes the mkl_axpy_avx2 function no matter which branch you use.
If you want to test the optimization effects of cadd (contiguous add), you should allocate another array as the sum of a and b. You can reference my sample here:
https://github.com/Jokeren/torch-tensor-benchmark/blob/master/bench_cadd_contiguous.lua",True,{}
torch/torch7,https://github.com/torch/torch7,852,2016-11-28T16:07:24Z,2017-02-28T17:32:02Z,2017-02-28T17:32:03Z,CLOSED,False,1354,567,18,https://github.com/soumith,Adding AVX Optimizations with Runtime Dispatch,73,[],https://github.com/torch/torch7/pull/852,https://github.com/pavanky,13,https://github.com/torch/torch7/pull/852#issuecomment-273536208,"Work in Progress.
Initial work by Roberth Guthrie (@rguthrie3) and more additions ongoing by Keren Zhou (@Jokeren)
This speeds up our Tensor Math ops. On add, we have up to 11x speedup already, and more to come.","@Jokeren using your script
On master:
contiguous float cadd single thread:	
0.0051441192626953	
contiguous float cadd multi-thread:	
0.11501288414001	
contiguous double cadd single thread:	
0.0079038143157959	
contiguous double cadd multi-thread:	
0.21997499465942

On simd-opt
contiguous float cadd single thread:	
0.0054259300231934	
contiguous float cadd multi-thread:	
0.10986804962158	
contiguous double cadd single thread:	
0.0080790519714355	
contiguous double cadd multi-thread:	
0.21401214599609",True,{}
torch/torch7,https://github.com/torch/torch7,852,2016-11-28T16:07:24Z,2017-02-28T17:32:02Z,2017-02-28T17:32:03Z,CLOSED,False,1354,567,18,https://github.com/soumith,Adding AVX Optimizations with Runtime Dispatch,73,[],https://github.com/torch/torch7/pull/852,https://github.com/Jokeren,14,https://github.com/torch/torch7/pull/852#issuecomment-273563714,"Work in Progress.
Initial work by Roberth Guthrie (@rguthrie3) and more additions ongoing by Keren Zhou (@Jokeren)
This speeds up our Tensor Math ops. On add, we have up to 11x speedup already, and more to come.","@pavanky , regarding your question, I dig into the source code and asm, trying to find out where the problem lies.

In fact, on my E5-2680v3 with a single thread, there is a tiny speedup with the simd-opt branch. But the optimization effects are not as good as I expect. I will analyze their assembly codes for an reasonable explanation.

simd-opt

contiguous float cadd single thread:
0.0071110725402832
contiguous float cadd multi-thread:
0.048126935958862
contiguous double cadd single thread:
0.0081689357757568
contiguous double cadd multi-thread:
0.1191349029541

master

contiguous float cadd single thread:
0.0081889629364014
contiguous float cadd multi-thread:
0.056603908538818
contiguous double cadd single thread:
0.0092580318450928
contiguous double cadd multi-thread:
0.12701296806335


Another thing I found annoying is that the detectHostSIMDExtensions function fails to detect the AVX2 flag. So the cadd_AVX implementation is called instead of cadd_AVX2. Could you please examine whether AVX2 is detected on your computer? Whatever, FMA instructions do not help a lot in my tests.",True,{}
torch/torch7,https://github.com/torch/torch7,852,2016-11-28T16:07:24Z,2017-02-28T17:32:02Z,2017-02-28T17:32:03Z,CLOSED,False,1354,567,18,https://github.com/soumith,Adding AVX Optimizations with Runtime Dispatch,73,[],https://github.com/torch/torch7/pull/852,https://github.com/pavanky,15,https://github.com/torch/torch7/pull/852#issuecomment-273564160,"Work in Progress.
Initial work by Roberth Guthrie (@rguthrie3) and more additions ongoing by Keren Zhou (@Jokeren)
This speeds up our Tensor Math ops. On add, we have up to 11x speedup already, and more to come.",@Jokeren I posted what cmake is detecting in an earlier comment. It is detecting AVX2 but not AVX.,True,{}
torch/torch7,https://github.com/torch/torch7,852,2016-11-28T16:07:24Z,2017-02-28T17:32:02Z,2017-02-28T17:32:03Z,CLOSED,False,1354,567,18,https://github.com/soumith,Adding AVX Optimizations with Runtime Dispatch,73,[],https://github.com/torch/torch7/pull/852,https://github.com/Jokeren,16,https://github.com/torch/torch7/pull/852#issuecomment-273565508,"Work in Progress.
Initial work by Roberth Guthrie (@rguthrie3) and more additions ongoing by Keren Zhou (@Jokeren)
This speeds up our Tensor Math ops. On add, we have up to 11x speedup already, and more to come.","@pavanky , Yes, I know that cmake can enable USE_AVX2 which I previously added.
But what I want to check is the statement in lib/TH/generic/simd/simd.h:
cpuid(&eax, &ebx, &ecx, &edx);
if (ebx & CPUID_AVX2_BIT)
  hostSimdExts |= SIMDExtension_AVX2;

Whether hostSimdExts |= SIMDExtension_AVX2 will be executed or not?
Sorry that I have not pointed it out for you.",True,{}
torch/torch7,https://github.com/torch/torch7,852,2016-11-28T16:07:24Z,2017-02-28T17:32:02Z,2017-02-28T17:32:03Z,CLOSED,False,1354,567,18,https://github.com/soumith,Adding AVX Optimizations with Runtime Dispatch,73,[],https://github.com/torch/torch7/pull/852,https://github.com/pavanky,17,https://github.com/torch/torch7/pull/852#issuecomment-273567455,"Work in Progress.
Initial work by Roberth Guthrie (@rguthrie3) and more additions ongoing by Keren Zhou (@Jokeren)
This speeds up our Tensor Math ops. On add, we have up to 11x speedup already, and more to come.","I ran the benchmark with the following patch applied:
@@ -120,15 +120,21 @@ static inline uint32_t detectHostSIMDExtensions()
   // Check for AVX2. Requires separate CPUID
   eax = 0x7;
   cpuid(&eax, &ebx, &ecx, &edx);
-  if (ebx & CPUID_AVX2_BIT)
-    hostSimdExts |= SIMDExtension_AVX2;
+  if (ebx & CPUID_AVX2_BIT) {
+      printf(""------Has AVX2 BIT------------\n"");
+      hostSimdExts |= SIMDExtension_AVX2;
+  }
 
   eax = 0x1;
   cpuid(&eax, &ebx, &ecx, &edx);
-  if (ecx & CPUID_AVX_BIT)
-    hostSimdExts |= SIMDExtension_AVX;
-  if (edx & CPUID_SSE_BIT)
-    hostSimdExts |= SIMDExtension_SSE;
+  if (ecx & CPUID_AVX_BIT) {
+      printf(""------Has AVX BIT------------\n"");
+      hostSimdExts |= SIMDExtension_AVX;
+  }
+  if (edx & CPUID_SSE_BIT) {
+      printf(""------Has SSE BIT------------\n"");
+      hostSimdExts |= SIMDExtension_SSE;
+  }
 
   return hostSimdExts;
 }

This is the output:
------Has AVX BIT------------
------Has SSE BIT------------
------Has AVX BIT------------
------Has SSE BIT------------
------Has AVX BIT------------
------Has SSE BIT------------
------Has AVX BIT------------
------Has SSE BIT------------
------Has AVX BIT------------
------Has SSE BIT------------
------Has AVX BIT------------
------Has SSE BIT------------
------Has AVX BIT------------
------Has SSE BIT------------
contiguous float cadd single thread:	
0.0044651031494141	
contiguous float cadd multi-thread:	
0.11115121841431	
contiguous double cadd single thread:	
0.007871150970459	
contiguous double cadd multi-thread:	
0.21293210983276",True,{}
torch/torch7,https://github.com/torch/torch7,852,2016-11-28T16:07:24Z,2017-02-28T17:32:02Z,2017-02-28T17:32:03Z,CLOSED,False,1354,567,18,https://github.com/soumith,Adding AVX Optimizations with Runtime Dispatch,73,[],https://github.com/torch/torch7/pull/852,https://github.com/pavanky,18,https://github.com/torch/torch7/pull/852#issuecomment-273567548,"Work in Progress.
Initial work by Roberth Guthrie (@rguthrie3) and more additions ongoing by Keren Zhou (@Jokeren)
This speeds up our Tensor Math ops. On add, we have up to 11x speedup already, and more to come.",@Jokeren forgot to mention your name in previous commit ^,True,{}
torch/torch7,https://github.com/torch/torch7,852,2016-11-28T16:07:24Z,2017-02-28T17:32:02Z,2017-02-28T17:32:03Z,CLOSED,False,1354,567,18,https://github.com/soumith,Adding AVX Optimizations with Runtime Dispatch,73,[],https://github.com/torch/torch7/pull/852,https://github.com/Jokeren,19,https://github.com/torch/torch7/pull/852#issuecomment-273569431,"Work in Progress.
Initial work by Roberth Guthrie (@rguthrie3) and more additions ongoing by Keren Zhou (@Jokeren)
This speeds up our Tensor Math ops. On add, we have up to 11x speedup already, and more to come.","@pavanky , thanks a lot for your check!
So I guess it is a bug of detecting AXV2. Although we have not solved the mystery of performance, at least we find a bug😜. I will try to optimize or account for it in the next days. But I have to go to sleep now.",True,{}
torch/torch7,https://github.com/torch/torch7,852,2016-11-28T16:07:24Z,2017-02-28T17:32:02Z,2017-02-28T17:32:03Z,CLOSED,False,1354,567,18,https://github.com/soumith,Adding AVX Optimizations with Runtime Dispatch,73,[],https://github.com/torch/torch7/pull/852,https://github.com/Jokeren,20,https://github.com/torch/torch7/pull/852#issuecomment-273570521,"Work in Progress.
Initial work by Roberth Guthrie (@rguthrie3) and more additions ongoing by Keren Zhou (@Jokeren)
This speeds up our Tensor Math ops. On add, we have up to 11x speedup already, and more to come.","I will report that bug, seeing how to fix it, as it is not written by me originally.",True,{}
torch/torch7,https://github.com/torch/torch7,852,2016-11-28T16:07:24Z,2017-02-28T17:32:02Z,2017-02-28T17:32:03Z,CLOSED,False,1354,567,18,https://github.com/soumith,Adding AVX Optimizations with Runtime Dispatch,73,[],https://github.com/torch/torch7/pull/852,https://github.com/Jokeren,21,https://github.com/torch/torch7/pull/852#issuecomment-273675782,"Work in Progress.
Initial work by Roberth Guthrie (@rguthrie3) and more additions ongoing by Keren Zhou (@Jokeren)
This speeds up our Tensor Math ops. On add, we have up to 11x speedup already, and more to come.",@pavanky I think commit logs are correct now?,True,{}
torch/torch7,https://github.com/torch/torch7,852,2016-11-28T16:07:24Z,2017-02-28T17:32:02Z,2017-02-28T17:32:03Z,CLOSED,False,1354,567,18,https://github.com/soumith,Adding AVX Optimizations with Runtime Dispatch,73,[],https://github.com/torch/torch7/pull/852,https://github.com/Jokeren,22,https://github.com/torch/torch7/pull/852#issuecomment-273687409,"Work in Progress.
Initial work by Roberth Guthrie (@rguthrie3) and more additions ongoing by Keren Zhou (@Jokeren)
This speeds up our Tensor Math ops. On add, we have up to 11x speedup already, and more to come.","The AVX2 support issue:
#916",True,{}
torch/torch7,https://github.com/torch/torch7,852,2016-11-28T16:07:24Z,2017-02-28T17:32:02Z,2017-02-28T17:32:03Z,CLOSED,False,1354,567,18,https://github.com/soumith,Adding AVX Optimizations with Runtime Dispatch,73,[],https://github.com/torch/torch7/pull/852,https://github.com/Jokeren,23,https://github.com/torch/torch7/pull/852#issuecomment-273720290,"Work in Progress.
Initial work by Roberth Guthrie (@rguthrie3) and more additions ongoing by Keren Zhou (@Jokeren)
This speeds up our Tensor Math ops. On add, we have up to 11x speedup already, and more to come.","@pavanky , for the performance problem, I find it hard to beat cadd with compiler optimizations. In our case, three contiguous arrays should be loaded but only a single FMA is applied. Hence the arithmetic intensity is extremely low, and the performance is dominated by AVX loads. You can remove the FMA instruction to test. I think the results will be only about 30% better.
The compiler, by default, uses SSE and unrolls the loop for several times (I use objdump to see the assembly codes). Because multiple ports exist in a core, the speed will be similar to AVX.",True,{}
torch/torch7,https://github.com/torch/torch7,852,2016-11-28T16:07:24Z,2017-02-28T17:32:02Z,2017-02-28T17:32:03Z,CLOSED,False,1354,567,18,https://github.com/soumith,Adding AVX Optimizations with Runtime Dispatch,73,[],https://github.com/torch/torch7/pull/852,https://github.com/pavanky,24,https://github.com/torch/torch7/pull/852#issuecomment-273745083,"Work in Progress.
Initial work by Roberth Guthrie (@rguthrie3) and more additions ongoing by Keren Zhou (@Jokeren)
This speeds up our Tensor Math ops. On add, we have up to 11x speedup already, and more to come.","@Jokeren That is what I expected. The -O3 optimizations are usually very hard to beat for vector operations. Additionally an operation like vector add is usually memory bandwidth bound, not computationally bound.

This speeds up our Tensor Math ops. On add, we have up to 11x speedup already, and more to come.

I saw this in the description and was surprised (for the reasons described above) that the speedup can be that high. I wanted to check this out for myself to see what is happening.",True,{}
torch/torch7,https://github.com/torch/torch7,852,2016-11-28T16:07:24Z,2017-02-28T17:32:02Z,2017-02-28T17:32:03Z,CLOSED,False,1354,567,18,https://github.com/soumith,Adding AVX Optimizations with Runtime Dispatch,73,[],https://github.com/torch/torch7/pull/852,https://github.com/adamlerer,25,https://github.com/torch/torch7/pull/852#issuecomment-273917356,"Work in Progress.
Initial work by Roberth Guthrie (@rguthrie3) and more additions ongoing by Keren Zhou (@Jokeren)
This speeds up our Tensor Math ops. On add, we have up to 11x speedup already, and more to come.","@Jokeren using your script, the multi-thread version is very sensitive to OMP_NUM_THREADS:
/data/users/alerer/git/torch7$ OMP_NUM_THREADS=1 th cadd.lua
contiguous float cadd single thread:
0.0061922073364258
contiguous float cadd multi-thread:
0.075101137161255
contiguous double cadd single thread:
0.011461019515991
contiguous double cadd multi-thread:
0.20637106895447
/data/users/alerer/git/torch7$ OMP_NUM_THREADS=10 th cadd.lua
contiguous float cadd single thread:
0.006270170211792
contiguous float cadd multi-thread:
0.0083858966827393
contiguous double cadd single thread:
0.011456966400146
contiguous double cadd multi-thread:
0.016303777694702
/data/users/alerer/git/torch7$ OMP_NUM_THREADS=40 th cadd.lua
contiguous float cadd single thread:
0.0062191486358643
contiguous float cadd multi-thread:
0.48840308189392
contiguous double cadd single thread:
0.011584043502808
contiguous double cadd multi-thread:
0.51699090003967

The second thing to note, is that you're claiming that the pointwise operations are memory-bound, but you're looking at cadd which calls mkl_blas_avx_xdaxpy, so of course it's going to be efficient.
On master, I find that add is not memory bound. Looking at the timings below, note that for cadd, the float version is twice as fast, so memory (or SIMD) bound; whereas for add, they are both the same speed (and slower), meaning it's bound by executing non-SIMD instructions.
th> fs = torch.FloatTensor(100000):zero()
                                                                      [0.0004s]
th> ds = torch.DoubleTensor(100000):zero()
                                                                      [0.0006s]
th> for i = 1,100000 do fs:add(fs,fs) end
                                                                      [2.7250s]
th> for i = 1,100000 do ds:add(ds,ds) end
                                                                      [5.1225s]
th> for i = 1,100000 do fs:add(1) end
                                                                      [7.9280s]
th> for i = 1,100000 do ds:add(1) end
                                                                      [7.9726s]

However, on simd_opt, it looks like we are memory-bound (or SIMD-op bound)
th> for i = 1,100000 do fs:add(1) end
                                                                      [2.3672s]
th> for i = 1,100000 do ds:add(1) end
                                                                      [5.2245s]

and perf top tells me I'm now executing THFloatVector_add_AVX. I guess still waiting on the AVX2 bit to be fixed.
Bottom line: the SIMD does seem to be speeding things up a bit, but it seems we can't get a ton of speedup because of memory constraints.
This is running at 20GB/s which is supposedly the max main memory bandwidth per core (although I would expect that 400K of data will be sitting in cache). When I really force it to go to main memory by using a 10+GB tensor, I'm seeing closer to 10GB/s
th> f = torch.FloatTensor(2.5e9):zero()
                                                                      [6.6841s]
th> d = torch.DoubleTensor(2.5e9):zero()
                                                                      [10.9034s]
th> f:add(1);
                                                                      [1.5745s]
th> d:add(1);
                                                                      [2.2922s]",True,{}
torch/torch7,https://github.com/torch/torch7,852,2016-11-28T16:07:24Z,2017-02-28T17:32:02Z,2017-02-28T17:32:03Z,CLOSED,False,1354,567,18,https://github.com/soumith,Adding AVX Optimizations with Runtime Dispatch,73,[],https://github.com/torch/torch7/pull/852,https://github.com/pavanky,26,https://github.com/torch/torch7/pull/852#issuecomment-273923410,"Work in Progress.
Initial work by Roberth Guthrie (@rguthrie3) and more additions ongoing by Keren Zhou (@Jokeren)
This speeds up our Tensor Math ops. On add, we have up to 11x speedup already, and more to come.","@adamlerer You are right, I am seeing a 50% speedup on simd-opt branch for :add(1) at large sizes.",True,{}
torch/torch7,https://github.com/torch/torch7,852,2016-11-28T16:07:24Z,2017-02-28T17:32:02Z,2017-02-28T17:32:03Z,CLOSED,False,1354,567,18,https://github.com/soumith,Adding AVX Optimizations with Runtime Dispatch,73,[],https://github.com/torch/torch7/pull/852,https://github.com/adamlerer,27,https://github.com/torch/torch7/pull/852#issuecomment-273976931,"Work in Progress.
Initial work by Roberth Guthrie (@rguthrie3) and more additions ongoing by Keren Zhou (@Jokeren)
This speeds up our Tensor Math ops. On add, we have up to 11x speedup already, and more to come.","So, @colesbury and I took a look at the source code of the two versions (gcc 4.8.5), and the truth is stranger than expected. Here's the inner loop of THFloatVector_add_DEFAULT:
  244710:       c5 fc 10 1c 06          vmovups ymm3,YMMWORD PTR [rsi+rax*1]
  244715:       48 83 c1 10             add    rcx,0x10
  244719:       c4 c1 7c 10 14 02       vmovups ymm2,YMMWORD PTR [r10+rax*1]
  24471f:       c5 e4 58 d9             vaddps ymm3,ymm3,ymm1
  244723:       c5 ec 58 d1             vaddps ymm2,ymm2,ymm1
  244727:       c5 fc 11 1c 07          vmovups YMMWORD PTR [rdi+rax*1],ymm3
  24472c:       c4 c1 7c 11 14 01       vmovups YMMWORD PTR [r9+rax*1],ymm2
  244732:       48 83 c0 40             add    rax,0x40
  244736:       4c 39 c1                cmp    rcx,r8
  244739:       7c d5                   jl     244710 <THFloatVector_add_AVX+0x20>

And here's the inner loop of THFloatVector_add_AVX (I think... there are a couple loops):
  23c05b:       c5 f8 10 0c 0e          vmovups xmm1,XMMWORD PTR [rsi+rcx*1]
  23c060:       49 83 c0 01             add    r8,0x1
  23c064:       c4 e3 75 18 4c 0e 10    vinsertf128 ymm1,ymm1,XMMWORD PTR [rsi+rcx*1+0x10],0x1
  23c06b:       01
  23c06c:       c5 f4 58 ca             vaddps ymm1,ymm1,ymm2
  23c070:       c5 f8 11 0c 0f          vmovups XMMWORD PTR [rdi+rcx*1],xmm1
  23c075:       c4 e3 7d 19 4c 0f 10    vextractf128 XMMWORD PTR [rdi+rcx*1+0x10],ymm1,0x1
  23c07c:       01
  23c07d:       48 83 c1 20             add    rcx,0x20
  23c081:       49 39 c0                cmp    r8,rax
  23c084:       72 d5                   jb     23c05b <THFloatVector_add_AVX+0x5b>

First thing to notice is they both use vaddps ymmX ymmX ymmX which is a 256-bit AVX add (!) But you can see that while the AVX version does 256-bit loads (vmovups on ymmX), the DEFAULT version does 128-bit loads (vmovups on xmmX, and vinsertf128/vextractf128 to read/write the high bits). I'm not sure why gcc can't do 256-bit loads automatically.
If I unroll the DEFAULT verison further to do 16 at a time, the assembly is also unrolled (see below), but still only does 128-bit loads and stores. shrug.
   23c040:       c5 f8 10 4c 06 20       vmovups xmm1,XMMWORD PTR [rsi+rax*1+0x20]
  23c046:       c5 f8 10 14 06          vmovups xmm2,XMMWORD PTR [rsi+rax*1]
  23c04b:       c4 e3 75 18 4c 06 30    vinsertf128 ymm1,ymm1,XMMWORD PTR [rsi+rax*1+0x30],0x1
  23c052:       01
  23c053:       c4 e3 6d 18 54 06 10    vinsertf128 ymm2,ymm2,XMMWORD PTR [rsi+rax*1+0x10],0x1
  23c05a:       01
  23c05b:       c5 f4 58 cb             vaddps ymm1,ymm1,ymm3
  23c05f:       c5 ec 58 d3             vaddps ymm2,ymm2,ymm3
  23c063:       c5 f8 11 4c 07 20       vmovups XMMWORD PTR [rdi+rax*1+0x20],xmm1
  23c069:       c4 e3 7d 19 4c 07 30    vextractf128 XMMWORD PTR [rdi+rax*1+0x30],ymm1,0x1
  23c070:       01
  23c071:       c5 f8 11 14 07          vmovups XMMWORD PTR [rdi+rax*1],xmm2
  23c076:       c4 e3 7d 19 54 07 10    vextractf128 XMMWORD PTR [rdi+rax*1+0x10],ymm2,0x1
  23c07d:       01
  23c07e:       48 83 c0 40             add    rax,0x40
  23c082:       48 39 c8                cmp    rax,rcx
  23c085:       75 b9                   jne    23c040 <THFloatVector_add_AVX+0x40>",True,{}
torch/torch7,https://github.com/torch/torch7,852,2016-11-28T16:07:24Z,2017-02-28T17:32:02Z,2017-02-28T17:32:03Z,CLOSED,False,1354,567,18,https://github.com/soumith,Adding AVX Optimizations with Runtime Dispatch,73,[],https://github.com/torch/torch7/pull/852,https://github.com/adamlerer,28,https://github.com/torch/torch7/pull/852#issuecomment-273977551,"Work in Progress.
Initial work by Roberth Guthrie (@rguthrie3) and more additions ongoing by Keren Zhou (@Jokeren)
This speeds up our Tensor Math ops. On add, we have up to 11x speedup already, and more to come.","P.S. The upshot of all this is you can probably get 2-3x speedup by writing an AVX version of contiguous copy, if src and dst are in cache.
(blocked transpose is more important though)",True,{}
torch/torch7,https://github.com/torch/torch7,852,2016-11-28T16:07:24Z,2017-02-28T17:32:02Z,2017-02-28T17:32:03Z,CLOSED,False,1354,567,18,https://github.com/soumith,Adding AVX Optimizations with Runtime Dispatch,73,[],https://github.com/torch/torch7/pull/852,https://github.com/pavanky,29,https://github.com/torch/torch7/pull/852#issuecomment-273977675,"Work in Progress.
Initial work by Roberth Guthrie (@rguthrie3) and more additions ongoing by Keren Zhou (@Jokeren)
This speeds up our Tensor Math ops. On add, we have up to 11x speedup already, and more to come.","@adamlerer unless I am mistaken, isn't AVX just 128 bit while AVX2 is 256 bit?
Nevermind :)",True,{}
torch/torch7,https://github.com/torch/torch7,852,2016-11-28T16:07:24Z,2017-02-28T17:32:02Z,2017-02-28T17:32:03Z,CLOSED,False,1354,567,18,https://github.com/soumith,Adding AVX Optimizations with Runtime Dispatch,73,[],https://github.com/torch/torch7/pull/852,https://github.com/Jokeren,30,https://github.com/torch/torch7/pull/852#issuecomment-274073471,"Work in Progress.
Initial work by Roberth Guthrie (@rguthrie3) and more additions ongoing by Keren Zhou (@Jokeren)
This speeds up our Tensor Math ops. On add, we have up to 11x speedup already, and more to come.","@adamlerer , @pavanky
Thanks for discuss!
I am sorry that the benchmark is not as accurate as I expect. Something to claim:


I am talking about the cadd branch that handles three different pointers. It will only call MKL when the dest address is the same as one of the source addresses.


THFloatVector_add_AVX and THFloatVector_add_AVX2 are exactly the same thing. Because AVX2 instruction set does not change arithmetic operations for float and double data types.


the multi-thread version is very sensitive to OMP_NUM_THREADS, this is caused by the setting of my benchmark. I fixed the array size so that with the growth of the number of threads, each of them gets smaller granularity. Anyway, I am messed up with the benchmark, finding it hard to model CPU and memory behaviors. Many variables, such as the array size, number of threads, iterations, and hardware features, affect the final performance.


But I may finish AVX copy first. And I will look back into the problem, analyze assembly codes, build an accountable benchmark, and associate hardware features with sources code. I believe a benchmark is whatever meaningful for us to compare with numpy or other libraries.",True,{}
torch/torch7,https://github.com/torch/torch7,852,2016-11-28T16:07:24Z,2017-02-28T17:32:02Z,2017-02-28T17:32:03Z,CLOSED,False,1354,567,18,https://github.com/soumith,Adding AVX Optimizations with Runtime Dispatch,73,[],https://github.com/torch/torch7/pull/852,https://github.com/Jokeren,31,https://github.com/torch/torch7/pull/852#issuecomment-274100023,"Work in Progress.
Initial work by Roberth Guthrie (@rguthrie3) and more additions ongoing by Keren Zhou (@Jokeren)
This speeds up our Tensor Math ops. On add, we have up to 11x speedup already, and more to come.","Notice that after changing THVectorDispatch.c, some of the VSX vector operations do not work because they are designed for old THVectorDispatch.c. For instance, I remove diff and scale functions that can be implemented by cadd and mul. In this way I match the names of vector functions with tensor operations.",True,{}
torch/torch7,https://github.com/torch/torch7,852,2016-11-28T16:07:24Z,2017-02-28T17:32:02Z,2017-02-28T17:32:03Z,CLOSED,False,1354,567,18,https://github.com/soumith,Adding AVX Optimizations with Runtime Dispatch,73,[],https://github.com/torch/torch7/pull/852,https://github.com/Jokeren,32,https://github.com/torch/torch7/pull/852#issuecomment-275430866,"Work in Progress.
Initial work by Roberth Guthrie (@rguthrie3) and more additions ongoing by Keren Zhou (@Jokeren)
This speeds up our Tensor Math ops. On add, we have up to 11x speedup already, and more to come.","@soumith , This branch is not activated?",True,{}
torch/torch7,https://github.com/torch/torch7,852,2016-11-28T16:07:24Z,2017-02-28T17:32:02Z,2017-02-28T17:32:03Z,CLOSED,False,1354,567,18,https://github.com/soumith,Adding AVX Optimizations with Runtime Dispatch,73,[],https://github.com/torch/torch7/pull/852,https://github.com/soumith,33,https://github.com/torch/torch7/pull/852#issuecomment-275978471,"Work in Progress.
Initial work by Roberth Guthrie (@rguthrie3) and more additions ongoing by Keren Zhou (@Jokeren)
This speeds up our Tensor Math ops. On add, we have up to 11x speedup already, and more to come.",@Jokeren looks like it is: https://travis-ci.org/torch/torch7/builds/196482699,True,{}
torch/torch7,https://github.com/torch/torch7,852,2016-11-28T16:07:24Z,2017-02-28T17:32:02Z,2017-02-28T17:32:03Z,CLOSED,False,1354,567,18,https://github.com/soumith,Adding AVX Optimizations with Runtime Dispatch,73,[],https://github.com/torch/torch7/pull/852,https://github.com/Jokeren,34,https://github.com/torch/torch7/pull/852#issuecomment-276564691,"Work in Progress.
Initial work by Roberth Guthrie (@rguthrie3) and more additions ongoing by Keren Zhou (@Jokeren)
This speeds up our Tensor Math ops. On add, we have up to 11x speedup already, and more to come.","Tensor binds the type of a tensor with methods such as  isContiguous, but indeed they are independent of types.",True,{}
torch/torch7,https://github.com/torch/torch7,852,2016-11-28T16:07:24Z,2017-02-28T17:32:02Z,2017-02-28T17:32:03Z,CLOSED,False,1354,567,18,https://github.com/soumith,Adding AVX Optimizations with Runtime Dispatch,73,[],https://github.com/torch/torch7/pull/852,https://github.com/Jokeren,35,https://github.com/torch/torch7/pull/852#issuecomment-276999459,"Work in Progress.
Initial work by Roberth Guthrie (@rguthrie3) and more additions ongoing by Keren Zhou (@Jokeren)
This speeds up our Tensor Math ops. On add, we have up to 11x speedup already, and more to come.","Benchmark codes and results are updated. @adamlerer , @soumith, @pavanky, I hope this time the benchmark is more rigorous and reasonable.
contiguous
https://github.com/Jokeren/torch-tensor-benchmark/tree/master/contiguous
noncontiguous
https://github.com/Jokeren/torch-tensor-benchmark/tree/master/noncontiguous
transposed
https://github.com/Jokeren/torch-tensor-benchmark/tree/master/transposed
It seems that three times speedup is achieved with regard to transposed tensor operations, not to mention hundreds times speedup with the fill operation.
We have applied three ideas:

Dimension collapsing that compresses the dimension counter array.
Multi-threading for TH_TENSOR_APPLY with transposed tensors.
Indexing reduction for contiguous tensors in TH_TENSOR_APPLYs.",True,{}
torch/torch7,https://github.com/torch/torch7,852,2016-11-28T16:07:24Z,2017-02-28T17:32:02Z,2017-02-28T17:32:03Z,CLOSED,False,1354,567,18,https://github.com/soumith,Adding AVX Optimizations with Runtime Dispatch,73,[],https://github.com/torch/torch7/pull/852,https://github.com/Jokeren,36,https://github.com/torch/torch7/pull/852#issuecomment-277007267,"Work in Progress.
Initial work by Roberth Guthrie (@rguthrie3) and more additions ongoing by Keren Zhou (@Jokeren)
This speeds up our Tensor Math ops. On add, we have up to 11x speedup already, and more to come.","By the way, I am glad to say that the transposed copy is faster than numpy. Source code used for comparison is shown as below:
import numpy as np
import time

a = np.ndarray((10000, 1000))
b = np.ndarray((1000, 10000))

start = time.time()
for i in range(1,100):
    b = np.transpose(a).copy()
end = time.time()
print(end - start)",True,{}
torch/torch7,https://github.com/torch/torch7,852,2016-11-28T16:07:24Z,2017-02-28T17:32:02Z,2017-02-28T17:32:03Z,CLOSED,False,1354,567,18,https://github.com/soumith,Adding AVX Optimizations with Runtime Dispatch,73,[],https://github.com/torch/torch7/pull/852,https://github.com/Jokeren,37,https://github.com/torch/torch7/pull/852#issuecomment-278921326,"Work in Progress.
Initial work by Roberth Guthrie (@rguthrie3) and more additions ongoing by Keren Zhou (@Jokeren)
This speeds up our Tensor Math ops. On add, we have up to 11x speedup already, and more to come.","Dear @adamlerer , thanks a lot for your review!
I will answer some questions here as Github does not allow me to comment above. I do not know why.

My main question is, how are you testing all the SIMD extensions? Since you have runtime dispatch, I expect you should be able to have the relevant test (or all tests) run in all the different modes (AVX, SSE, default). Are there tests to do that already?

I downloaded torch without AVX and benchmarked it to compare with the AVX version I used. So Actually I compare AVX with SSE.

On a different note, can you explain why fill performance improved so much relative to the other ops? 1000x speedup is a bit surprising... and this corresponds to 200GB/s of fill, which I don't believe :)

The surprising speedup is caused by cache readings. I tried fill(1.0) that results in the same running time. My processor--E5 2680v3 has a large L3 cache (about 30MB), so when I increase the test size from 1e7 to 1e9, 40GB/s bandwidth is achieved.

What do you mean by ""stream instructions""?

I mean the stream load and store instructions that directly load and store data between main memory and registers, but I think they are not necessary as we need to use cache mostly.

This may be a stupid question, but I don't understand why your changes should speed up apply for transposed 2-dimensional tensors.

This is a good question. Let's compare two versions of THTensorApply. Take a look at the TENSOR##_size variable.
Original
    TENSOR##_size = 1; \
    for(TENSOR##_dim = TENSOR->nDimension-1; TENSOR##_dim >= 0; TENSOR##_dim--) \
    { \
      if(TENSOR->size[TENSOR##_dim] != 1) \
      { \
        if(TENSOR->stride[TENSOR##_dim] == TENSOR##_size) \
          TENSOR##_size *= TENSOR->size[TENSOR##_dim]; \
        else \
          break; \
      } \
    } \

Optimized
TENSOR3##_size = TENSOR3##_sizes[TENSOR3##_dim-1]
So in the original version, TENSOR##_size is assigned to the size of the longest contiguous region, while TENSOR##_size is assigned to the size of the last region in the optimized version. Think of a transposed matrix of which the stride is transposed from [10, 1] to [1, 10]. Then, TENSOR##_size=1 for the original version. So the original version will handle the counter array every time the stride is added, while the optimized version does not. I think the optimized version actually fixes a performance bug instead of devising a new idea.
    /* Loop through the contiguous section of the Tensor */ \
    for(TENSOR##_i = 0; TENSOR##_i < TENSOR##_size; TENSOR##_i++, TENSOR##_data += TENSOR##_stride) /* 0 et pas TENSOR##_dim! */ \
    { \
      CODE \
    } \
\
\
    /* Handle corner case where the entire Tensor was contiguous */ \
    if(TENSOR##_dim == -1) \
       break; \
 \
    /* Reset pointer to beginning of loop */ \
    TENSOR##_data -= TENSOR##_i*TENSOR##_stride; \
    for(TENSOR##_i = TENSOR##_dim; TENSOR##_i >= 0; TENSOR##_i--) \

After fixing the mentioned issues, I will rebase my branch to solve conflicts and prepare for merging.",True,{}
torch/torch7,https://github.com/torch/torch7,852,2016-11-28T16:07:24Z,2017-02-28T17:32:02Z,2017-02-28T17:32:03Z,CLOSED,False,1354,567,18,https://github.com/soumith,Adding AVX Optimizations with Runtime Dispatch,73,[],https://github.com/torch/torch7/pull/852,https://github.com/adamlerer,38,https://github.com/torch/torch7/pull/852#issuecomment-279055441,"Work in Progress.
Initial work by Roberth Guthrie (@rguthrie3) and more additions ongoing by Keren Zhou (@Jokeren)
This speeds up our Tensor Math ops. On add, we have up to 11x speedup already, and more to come.","@Jokeren thanks for the detailed responses. It took me a while to see what you meant about the original TH_TENSOR_APPLY... but you're right! The old version has a big performance big because TENSORX##_size should always be at least as big as the last dimension, but originally it was 1 for transposed tensors, yikes! Great catch.
P.S. if you haven't seen it, I was writing some related code yesterday:
#934
The code for max and min is not going to work now that you've changed TH_TENSOR_APPLY, so maybe we want to add some modified kernels for these reductions (max, min, cumsum, etc.).
P.P.S. I think you can still get more speedup for transpose ops if you do blocking. Have you tried that at all? I know you were playing around with it a bit before, but doesn't look like it made it into this PR.",True,{}
torch/torch7,https://github.com/torch/torch7,852,2016-11-28T16:07:24Z,2017-02-28T17:32:02Z,2017-02-28T17:32:03Z,CLOSED,False,1354,567,18,https://github.com/soumith,Adding AVX Optimizations with Runtime Dispatch,73,[],https://github.com/torch/torch7/pull/852,https://github.com/Jokeren,39,https://github.com/torch/torch7/pull/852#issuecomment-280971984,"Work in Progress.
Initial work by Roberth Guthrie (@rguthrie3) and more additions ongoing by Keren Zhou (@Jokeren)
This speeds up our Tensor Math ops. On add, we have up to 11x speedup already, and more to come.","Hi, all!
I have done these changes:


Following the instructions of @apaszke , I used C99 to clean up openmp pragmas. Beyond the previous modifications, I could also clean up other openmp usages in THTensorMath.c.


Added comments for #934.


Fixed the stride=0 case in isTranposed function.


Fixed coding styles.",True,{}
torch/torch7,https://github.com/torch/torch7,852,2016-11-28T16:07:24Z,2017-02-28T17:32:02Z,2017-02-28T17:32:03Z,CLOSED,False,1354,567,18,https://github.com/soumith,Adding AVX Optimizations with Runtime Dispatch,73,[],https://github.com/torch/torch7/pull/852,https://github.com/soumith,40,https://github.com/torch/torch7/pull/852#issuecomment-281358619,"Work in Progress.
Initial work by Roberth Guthrie (@rguthrie3) and more additions ongoing by Keren Zhou (@Jokeren)
This speeds up our Tensor Math ops. On add, we have up to 11x speedup already, and more to come.",this is starting to look good. I'll do some final checks today and merge today/tomorrow.,True,{}
torch/torch7,https://github.com/torch/torch7,852,2016-11-28T16:07:24Z,2017-02-28T17:32:02Z,2017-02-28T17:32:03Z,CLOSED,False,1354,567,18,https://github.com/soumith,Adding AVX Optimizations with Runtime Dispatch,73,[],https://github.com/torch/torch7/pull/852,https://github.com/soumith,41,https://github.com/torch/torch7/pull/852#issuecomment-283108690,"Work in Progress.
Initial work by Roberth Guthrie (@rguthrie3) and more additions ongoing by Keren Zhou (@Jokeren)
This speeds up our Tensor Math ops. On add, we have up to 11x speedup already, and more to come.","Thanks a TON to @Jokeren , @adamlerer and @rguthrie3 for making this happen. This is now merged into master!!!!!!!",True,"{'HOORAY': ['https://github.com/fmassa', 'https://github.com/Jokeren']}"
torch/torch7,https://github.com/torch/torch7,860,2016-12-02T12:14:07Z,2016-12-05T09:34:17Z,2016-12-07T08:08:18Z,CLOSED,False,1,1,1,https://github.com/drahnr,Prevent runover refcount of tensor,1,[],https://github.com/torch/torch7/pull/860,https://github.com/drahnr,1,https://github.com/torch/torch7/pull/860,This is necessary since for integrations with C one never knows what or when the lua gc kicks in.,This is necessary since for integrations with C one never knows what or when the lua gc kicks in.,True,{}
torch/torch7,https://github.com/torch/torch7,860,2016-12-02T12:14:07Z,2016-12-05T09:34:17Z,2016-12-07T08:08:18Z,CLOSED,False,1,1,1,https://github.com/drahnr,Prevent runover refcount of tensor,1,[],https://github.com/torch/torch7/pull/860,https://github.com/apaszke,2,https://github.com/torch/torch7/pull/860#issuecomment-264451284,This is necessary since for integrations with C one never knows what or when the lua gc kicks in.,"Why is that necessary? If you see refcount == 0 it likely means that the tensor struct has already been freed, and you're doing invalid memory accesses anyway (also, you're quite likely to see some garbage and stomp on reused memory because of the decref). If you want to use the tensor for time longer than the ffi call, you should call THTensor_(retain) and free it manually once you stop processing it.",True,{}
torch/torch7,https://github.com/torch/torch7,860,2016-12-02T12:14:07Z,2016-12-05T09:34:17Z,2016-12-07T08:08:18Z,CLOSED,False,1,1,1,https://github.com/drahnr,Prevent runover refcount of tensor,1,[],https://github.com/torch/torch7/pull/860,https://github.com/drahnr,3,https://github.com/torch/torch7/pull/860#issuecomment-264476070,This is necessary since for integrations with C one never knows what or when the lua gc kicks in.,"True, one would access invalidated memory.
In my case I run a C Application which embeds luajit.
I run my forward pass on a video stream and what I saw was a non deterministic behaviour of recounts on the returned tensor from a lua function to C (no ffi).
For this case I rather have an invalid access than a double free.
Yes, I do call the the garbage collector (lua_gc(..)) after calling the lua function (lua_pcall)",True,{}
torch/torch7,https://github.com/torch/torch7,860,2016-12-02T12:14:07Z,2016-12-05T09:34:17Z,2016-12-07T08:08:18Z,CLOSED,False,1,1,1,https://github.com/drahnr,Prevent runover refcount of tensor,1,[],https://github.com/torch/torch7/pull/860,https://github.com/apaszke,4,https://github.com/torch/torch7/pull/860#issuecomment-264650758,This is necessary since for integrations with C one never knows what or when the lua gc kicks in.,"Well, accessing freed memory sounds like a pretty serious thing to me - it's completely UB. If it happens to be read as a positive long you will also decref it (destroying its old contents), or if it happens to be 1, you'll also try to free it. I don't think that adding these checks is a good idea. If you need them, you likely have some logic error in your program.
Of course it might be ""nondeterministic"", because the GC might sometimes decide that it doesn't care and it's not the time to free the tensor yet. Still, my advice is the same, if you need to ensure that the tensor is alive for some time in your C code, just retain and free it manually. These operations are  cheap and you'll be sure that your program will work as expected every time.",True,{}
torch/torch7,https://github.com/torch/torch7,860,2016-12-02T12:14:07Z,2016-12-05T09:34:17Z,2016-12-07T08:08:18Z,CLOSED,False,1,1,1,https://github.com/drahnr,Prevent runover refcount of tensor,1,[],https://github.com/torch/torch7/pull/860,https://github.com/drahnr,5,https://github.com/torch/torch7/pull/860#issuecomment-264653943,This is necessary since for integrations with C one never knows what or when the lua gc kicks in.,"The problem is not that I want to retain them, I want to free the mem since I am working in a memory constrained env. So every tensor that survives an iteration of my 60 fps use case is an actual problem.
I understand that undef mem access is (almost) as bad (it usually does not segfault, it has been allocated at some point), but in that case there should be asserts added.",True,{}
torch/torch7,https://github.com/torch/torch7,860,2016-12-02T12:14:07Z,2016-12-05T09:34:17Z,2016-12-07T08:08:18Z,CLOSED,False,1,1,1,https://github.com/drahnr,Prevent runover refcount of tensor,1,[],https://github.com/torch/torch7/pull/860,https://github.com/soumith,6,https://github.com/torch/torch7/pull/860#issuecomment-264654545,This is necessary since for integrations with C one never knows what or when the lua gc kicks in.,"Edited my answer from :free() to :set()
@drahnr if the GC non-determinism is really a problem, you can free the tensor after it's use in lua-land with :set(). I dont think hacking around the refcount like this is really a good solution.
For example:
while true do
   frame = receive()
   -- let's say frame is a torch.*Tensor received from the C side   
   -- do something with frame
   -- now, you are done with using frame
    frame:set()
end
Only an empty tensor struct (in the order of 32 bytes) is retained for the gc, and the rest of the memory is freed.",True,{}
torch/torch7,https://github.com/torch/torch7,860,2016-12-02T12:14:07Z,2016-12-05T09:34:17Z,2016-12-07T08:08:18Z,CLOSED,False,1,1,1,https://github.com/drahnr,Prevent runover refcount of tensor,1,[],https://github.com/torch/torch7/pull/860,https://github.com/apaszke,7,https://github.com/torch/torch7/pull/860#issuecomment-264656493,This is necessary since for integrations with C one never knows what or when the lua gc kicks in.,"@soumith I don't think that's going to work. free only decrements the refcount but it doesn't invalidate the lua object, so the GC will try to free it once more for sure. frame:set() will detach the tensor, and free its data immediately (only the tensor struct will stay alive).",True,{}
torch/torch7,https://github.com/torch/torch7,860,2016-12-02T12:14:07Z,2016-12-05T09:34:17Z,2016-12-07T08:08:18Z,CLOSED,False,1,1,1,https://github.com/drahnr,Prevent runover refcount of tensor,1,[],https://github.com/torch/torch7/pull/860,https://github.com/apaszke,8,https://github.com/torch/torch7/pull/860#issuecomment-264656726,This is necessary since for integrations with C one never knows what or when the lua gc kicks in.,"@drahnr I would say that invalid memory accesses are one of the biggest problems you can possibly have in your program! The only way for them to be non-destructive is when you actually happen to hit them only when they're in the allocator cache. If they have been reused you'll end up modifying the bits of some random objects because of the decref. This is a terrible bug, as it's going to silently corrupt your state and crash your program only sometime later. If they were truly freed you'll get a SIGSEV. Both of them seem pretty bad to me.",True,{}
torch/torch7,https://github.com/torch/torch7,860,2016-12-02T12:14:07Z,2016-12-05T09:34:17Z,2016-12-07T08:08:18Z,CLOSED,False,1,1,1,https://github.com/drahnr,Prevent runover refcount of tensor,1,[],https://github.com/torch/torch7/pull/860,https://github.com/drahnr,9,https://github.com/torch/torch7/pull/860#issuecomment-265161722,This is necessary since for integrations with C one never knows what or when the lua gc kicks in.,"The :set() trick only works for passing an tensor as argument, but not for the case when returning the argument.
My use case is not a lua for loop, but a C for loop with one call per iteration to lua.

while(1) {
input = <alloc tensor>
luaT_push(input, ...)
lua_pcall(...)
luaT_pop(output, ...)
...
}

The only thing that worked to some extent was static allocation of two tensors in C and then resizing the output tensor as needed in lua. This is not really what I was looking for.",True,{}
torch/torch7,https://github.com/torch/torch7,860,2016-12-02T12:14:07Z,2016-12-05T09:34:17Z,2016-12-07T08:08:18Z,CLOSED,False,1,1,1,https://github.com/drahnr,Prevent runover refcount of tensor,1,[],https://github.com/torch/torch7/pull/860,https://github.com/drahnr,10,https://github.com/torch/torch7/pull/860#issuecomment-265382552,This is necessary since for integrations with C one never knows what or when the lua gc kicks in.,Please see related #862,True,{}
torch/torch7,https://github.com/torch/torch7,867,2016-12-13T15:51:48Z,2016-12-13T16:28:16Z,2016-12-13T16:28:16Z,CLOSED,False,78,53,3,https://github.com/elikosan,merge,18,[],https://github.com/torch/torch7/pull/867,https://github.com/elikosan,1,https://github.com/torch/torch7/pull/867,,,True,{}
torch/torch7,https://github.com/torch/torch7,868,2016-12-13T22:35:25Z,2016-12-13T22:49:41Z,2016-12-13T23:02:51Z,MERGED,True,9,0,1,https://github.com/vfonov,"Fixed compilation on Raspberry PI without NEON, fixes #807",1,[],https://github.com/torch/torch7/pull/868,https://github.com/vfonov,1,https://github.com/torch/torch7/pull/868,A fix for issue #807  - compilation on Raspberry PI without NEON ( BCM2708 chip),A fix for issue #807  - compilation on Raspberry PI without NEON ( BCM2708 chip),True,{}
torch/torch7,https://github.com/torch/torch7,868,2016-12-13T22:35:25Z,2016-12-13T22:49:41Z,2016-12-13T23:02:51Z,MERGED,True,9,0,1,https://github.com/vfonov,"Fixed compilation on Raspberry PI without NEON, fixes #807",1,[],https://github.com/torch/torch7/pull/868,https://github.com/soumith,2,https://github.com/torch/torch7/pull/868#issuecomment-266887335,A fix for issue #807  - compilation on Raspberry PI without NEON ( BCM2708 chip),thanks!,True,{}
torch/torch7,https://github.com/torch/torch7,870,2016-12-20T01:56:21Z,2016-12-20T01:56:39Z,2016-12-30T11:00:27Z,MERGED,True,703,109,38,https://github.com/soumith,Half type,19,[],https://github.com/torch/torch7/pull/870,https://github.com/soumith,1,https://github.com/torch/torch7/pull/870,,,True,{}
torch/torch7,https://github.com/torch/torch7,871,2016-12-20T01:58:15Z,2016-12-20T01:58:32Z,2016-12-30T11:00:27Z,CLOSED,False,703,109,38,https://github.com/soumith,Half type,19,[],https://github.com/torch/torch7/pull/871,https://github.com/soumith,1,https://github.com/torch/torch7/pull/871,,,True,{}
torch/torch7,https://github.com/torch/torch7,872,2016-12-20T19:42:29Z,2016-12-20T19:49:46Z,2016-12-20T20:48:57Z,MERGED,True,45,5,1,https://github.com/pavanky,correctness fixes for mod and remainder for integer type tensors.,1,[],https://github.com/torch/torch7/pull/872,https://github.com/pavanky,1,https://github.com/torch/torch7/pull/872,,,True,{}
torch/torch7,https://github.com/torch/torch7,872,2016-12-20T19:42:29Z,2016-12-20T19:49:46Z,2016-12-20T20:48:57Z,MERGED,True,45,5,1,https://github.com/pavanky,correctness fixes for mod and remainder for integer type tensors.,1,[],https://github.com/torch/torch7/pull/872,https://github.com/soumith,2,https://github.com/torch/torch7/pull/872#issuecomment-268340648,,thanks!,True,{}
torch/torch7,https://github.com/torch/torch7,873,2016-12-21T00:28:05Z,2017-01-01T17:47:03Z,2017-01-01T17:47:11Z,MERGED,True,141,25,4,https://github.com/nkoumchatzky,Different code path for contiguous tensors along dimension 1 in cat + fix cat bug + fix ambiguity,2,[],https://github.com/torch/torch7/pull/873,https://github.com/nkoumchatzky,1,https://github.com/torch/torch7/pull/873,"This is very useful for speed reasons, the generic code path has too many function calls.","This is very useful for speed reasons, the generic code path has too many function calls.",True,{}
torch/torch7,https://github.com/torch/torch7,873,2016-12-21T00:28:05Z,2017-01-01T17:47:03Z,2017-01-01T17:47:11Z,MERGED,True,141,25,4,https://github.com/nkoumchatzky,Different code path for contiguous tensors along dimension 1 in cat + fix cat bug + fix ambiguity,2,[],https://github.com/torch/torch7/pull/873,https://github.com/soumith,2,https://github.com/torch/torch7/pull/873#issuecomment-269098398,"This is very useful for speed reasons, the generic code path has too many function calls.","there's an additional optimization that would be really useful and very simple to add to this PR. If you think you will do it, let me know -- if not i'll merge your PR and do it.
The same code path (all contiguous, memcpy) can be taken if the dim to be cat is the 1st dim, i.e.:
torch.cat( {1 x 10 x 10, 1 x 10 x 10, 1 x 10 x 10}, 1)
Your 1D case is just a  small special case of that.",True,{}
torch/torch7,https://github.com/torch/torch7,873,2016-12-21T00:28:05Z,2017-01-01T17:47:03Z,2017-01-01T17:47:11Z,MERGED,True,141,25,4,https://github.com/nkoumchatzky,Different code path for contiguous tensors along dimension 1 in cat + fix cat bug + fix ambiguity,2,[],https://github.com/torch/torch7/pull/873,https://github.com/nkoumchatzky,3,https://github.com/torch/torch7/pull/873#issuecomment-269102005,"This is very useful for speed reasons, the generic code path has too many function calls.","Indeed, I figured I'd leave it for a next review :) I'll do it in this one.

Happy holidays!

Nico
…
On Saturday, December 24, 2016, Soumith Chintala ***@***.***> wrote:
 there's an additional optimization that would be really useful and very
 simple to add to this PR. If you think you will do it, let me know -- if
 not i'll merge your PR and do it.

 The same code path (all contiguous, memcpy) can be taken if the dim to be
 cat is the 1st dim, i.e.:
 torch.cat( {1 x 10 x 10, 1 x 10 x 10, 1 x 10 x 10}, 1)

 Your 1D case is just a small special case of that.

 —
 You are receiving this because you authored the thread.
 Reply to this email directly, view it on GitHub
 <#873 (comment)>, or mute
 the thread
 <https://github.com/notifications/unsubscribe-auth/AGlLXGqnH__RLsTp5UTTQpw4Ztkb9w7mks5rLXsegaJpZM4LScpg>
 .",True,{}
torch/torch7,https://github.com/torch/torch7,873,2016-12-21T00:28:05Z,2017-01-01T17:47:03Z,2017-01-01T17:47:11Z,MERGED,True,141,25,4,https://github.com/nkoumchatzky,Different code path for contiguous tensors along dimension 1 in cat + fix cat bug + fix ambiguity,2,[],https://github.com/torch/torch7/pull/873,https://github.com/nkoumchatzky,4,https://github.com/torch/torch7/pull/873#issuecomment-269103170,"This is very useful for speed reasons, the generic code path has too many function calls.",@soumith @pavanky Here is the augmented patch.,True,{}
torch/torch7,https://github.com/torch/torch7,873,2016-12-21T00:28:05Z,2017-01-01T17:47:03Z,2017-01-01T17:47:11Z,MERGED,True,141,25,4,https://github.com/nkoumchatzky,Different code path for contiguous tensors along dimension 1 in cat + fix cat bug + fix ambiguity,2,[],https://github.com/torch/torch7/pull/873,https://github.com/nkoumchatzky,5,https://github.com/torch/torch7/pull/873#issuecomment-269103605,"This is very useful for speed reasons, the generic code path has too many function calls.","I actually spotted a bug in cat, let me fix it before merging.",True,{}
torch/torch7,https://github.com/torch/torch7,873,2016-12-21T00:28:05Z,2017-01-01T17:47:03Z,2017-01-01T17:47:11Z,MERGED,True,141,25,4,https://github.com/nkoumchatzky,Different code path for contiguous tensors along dimension 1 in cat + fix cat bug + fix ambiguity,2,[],https://github.com/torch/torch7/pull/873,https://github.com/nkoumchatzky,6,https://github.com/torch/torch7/pull/873#issuecomment-269104461,"This is very useful for speed reasons, the generic code path has too many function calls.","Ok the bug was: when catting an empty tensor along the first dim, not exception was thrown and the size was incremented by 1. When catting along another dim, an exception was thrown.
I chose to allow empty tensors, just ignoring them when catting. And catting only empty tensors together results in an empty tensor, whichever the input dim.
Let me know if the behavior is ok @soumith",True,{}
torch/torch7,https://github.com/torch/torch7,873,2016-12-21T00:28:05Z,2017-01-01T17:47:03Z,2017-01-01T17:47:11Z,MERGED,True,141,25,4,https://github.com/nkoumchatzky,Different code path for contiguous tensors along dimension 1 in cat + fix cat bug + fix ambiguity,2,[],https://github.com/torch/torch7/pull/873,https://github.com/fmassa,7,https://github.com/torch/torch7/pull/873#issuecomment-269104554,"This is very useful for speed reasons, the generic code path has too many function calls.",@nkoumchatzky I think allowing empty tensors in torch.cat is a reasonable behaviour. But we should adapt the cuda version to follow the same behaviour.,True,{}
torch/torch7,https://github.com/torch/torch7,873,2016-12-21T00:28:05Z,2017-01-01T17:47:03Z,2017-01-01T17:47:11Z,MERGED,True,141,25,4,https://github.com/nkoumchatzky,Different code path for contiguous tensors along dimension 1 in cat + fix cat bug + fix ambiguity,2,[],https://github.com/torch/torch7/pull/873,https://github.com/nkoumchatzky,8,https://github.com/torch/torch7/pull/873#issuecomment-269104840,"This is very useful for speed reasons, the generic code path has too many function calls.",@fmassa Yes indeed. @pavanky will replicate that logic in cutorch soon.,True,{}
torch/torch7,https://github.com/torch/torch7,873,2016-12-21T00:28:05Z,2017-01-01T17:47:03Z,2017-01-01T17:47:11Z,MERGED,True,141,25,4,https://github.com/nkoumchatzky,Different code path for contiguous tensors along dimension 1 in cat + fix cat bug + fix ambiguity,2,[],https://github.com/torch/torch7/pull/873,https://github.com/pavanky,9,https://github.com/torch/torch7/pull/873#issuecomment-269152753,"This is very useful for speed reasons, the generic code path has too many function calls.","@nkoumchatzky @fmassa
Since we are allowing empty tensors, shouldn't the following also be allowed?
torch.cat({troch.Tensor(), torch.Tensor()})

This is currently (after applying the patch in this PR) throwing an error saying that the dimension 0 is invalid.
bad argument #4 to '?' (invalid dimension 0 at /home/pyalamanchili/Workspace/torch/torch7/lib/TH/generic/THTensorMath.c:2048)

I think returning an empty tensor should be the right behavior.",True,{}
torch/torch7,https://github.com/torch/torch7,873,2016-12-21T00:28:05Z,2017-01-01T17:47:03Z,2017-01-01T17:47:11Z,MERGED,True,141,25,4,https://github.com/nkoumchatzky,Different code path for contiguous tensors along dimension 1 in cat + fix cat bug + fix ambiguity,2,[],https://github.com/torch/torch7/pull/873,https://github.com/nkoumchatzky,10,https://github.com/torch/torch7/pull/873#issuecomment-269217868,"This is very useful for speed reasons, the generic code path has too many function calls.","@pavanky Just pushed a new patch that deals with that use case. Now a -1 or unspecified dimension will cat along the maximum of the last dimension over the input tensors, and will allow for empty tensors.",True,{}
torch/torch7,https://github.com/torch/torch7,873,2016-12-21T00:28:05Z,2017-01-01T17:47:03Z,2017-01-01T17:47:11Z,MERGED,True,141,25,4,https://github.com/nkoumchatzky,Different code path for contiguous tensors along dimension 1 in cat + fix cat bug + fix ambiguity,2,[],https://github.com/torch/torch7/pull/873,https://github.com/soumith,11,https://github.com/torch/torch7/pull/873#issuecomment-269219845,"This is very useful for speed reasons, the generic code path has too many function calls.",@nkoumchatzky all unit tests are failing: https://travis-ci.org/torch/torch7/builds/186789362,True,{}
torch/torch7,https://github.com/torch/torch7,873,2016-12-21T00:28:05Z,2017-01-01T17:47:03Z,2017-01-01T17:47:11Z,MERGED,True,141,25,4,https://github.com/nkoumchatzky,Different code path for contiguous tensors along dimension 1 in cat + fix cat bug + fix ambiguity,2,[],https://github.com/torch/torch7/pull/873,https://github.com/nkoumchatzky,12,https://github.com/torch/torch7/pull/873#issuecomment-269220063,"This is very useful for speed reasons, the generic code path has too many function calls.","@soumith I added the forgotten TensorMath.lua file, should work now.",True,{}
torch/torch7,https://github.com/torch/torch7,873,2016-12-21T00:28:05Z,2017-01-01T17:47:03Z,2017-01-01T17:47:11Z,MERGED,True,141,25,4,https://github.com/nkoumchatzky,Different code path for contiguous tensors along dimension 1 in cat + fix cat bug + fix ambiguity,2,[],https://github.com/torch/torch7/pull/873,https://github.com/pavanky,13,https://github.com/torch/torch7/pull/873#issuecomment-269386184,"This is very useful for speed reasons, the generic code path has too many function calls.",@nkoumchatzky @soumith updated the cutorch PR to have same behavior.,True,{}
torch/torch7,https://github.com/torch/torch7,873,2016-12-21T00:28:05Z,2017-01-01T17:47:03Z,2017-01-01T17:47:11Z,MERGED,True,141,25,4,https://github.com/nkoumchatzky,Different code path for contiguous tensors along dimension 1 in cat + fix cat bug + fix ambiguity,2,[],https://github.com/torch/torch7/pull/873,https://github.com/soumith,14,https://github.com/torch/torch7/pull/873#issuecomment-269912259,"This is very useful for speed reasons, the generic code path has too many function calls.",thanks Nico!,True,{}
torch/torch7,https://github.com/torch/torch7,874,2016-12-21T00:38:57Z,2016-12-29T19:23:27Z,2016-12-30T10:59:49Z,MERGED,True,872,197,40,https://github.com/gchanan,Add support for torch.HalfTensor,3,[],https://github.com/torch/torch7/pull/874,https://github.com/gchanan,1,https://github.com/torch/torch7/pull/874,"NOTE: this should not be merged until the cutorch equivalent is ready; I'm working on that based on torch/cutorch#578.  This pull request won't break anything but because cutorch's generics overwrite the torch generics, you can lose some HalfTensor functionality via ""require cutorch""
This is based on #826 (which is the first commit in this request), with the following improvements/simplifications (the second commit):
Improvements/Simplifications:

No longer generates math functions that are not actually defined
on torch.HalfTensor, e.g. maskedFill, map2, etc.
Adds tests for all available torch.HalfTensor functions.
Allows compiling without TH_GENERIC_USE_HALF (so if there's a
problem can just unset that in CMakeLists rather than backing out)
Some simplifications: removes a new copy optimization and
some TH_HALF literal definitions; I'd rather not include optimizations
to non-half types in this PR.

Limitations:
Because match functions are not defined, some ""non-math"" operators
on torch.HalfTensor give an error message, e.g. index/newindex
with a ByteTensor apply a mask, but masks aren't implemented.  These
limitations aren't always obvious (e.g. for documentation purposes), but
they should always give an error message.","NOTE: this should not be merged until the cutorch equivalent is ready; I'm working on that based on torch/cutorch#578.  This pull request won't break anything but because cutorch's generics overwrite the torch generics, you can lose some HalfTensor functionality via ""require cutorch""
This is based on #826 (which is the first commit in this request), with the following improvements/simplifications (the second commit):
Improvements/Simplifications:

No longer generates math functions that are not actually defined
on torch.HalfTensor, e.g. maskedFill, map2, etc.
Adds tests for all available torch.HalfTensor functions.
Allows compiling without TH_GENERIC_USE_HALF (so if there's a
problem can just unset that in CMakeLists rather than backing out)
Some simplifications: removes a new copy optimization and
some TH_HALF literal definitions; I'd rather not include optimizations
to non-half types in this PR.

Limitations:
Because match functions are not defined, some ""non-math"" operators
on torch.HalfTensor give an error message, e.g. index/newindex
with a ByteTensor apply a mask, but masks aren't implemented.  These
limitations aren't always obvious (e.g. for documentation purposes), but
they should always give an error message.",True,{}
torch/torch7,https://github.com/torch/torch7,874,2016-12-21T00:38:57Z,2016-12-29T19:23:27Z,2016-12-30T10:59:49Z,MERGED,True,872,197,40,https://github.com/gchanan,Add support for torch.HalfTensor,3,[],https://github.com/torch/torch7/pull/874,https://github.com/gchanan,2,https://github.com/torch/torch7/pull/874#issuecomment-269373499,"NOTE: this should not be merged until the cutorch equivalent is ready; I'm working on that based on torch/cutorch#578.  This pull request won't break anything but because cutorch's generics overwrite the torch generics, you can lose some HalfTensor functionality via ""require cutorch""
This is based on #826 (which is the first commit in this request), with the following improvements/simplifications (the second commit):
Improvements/Simplifications:

No longer generates math functions that are not actually defined
on torch.HalfTensor, e.g. maskedFill, map2, etc.
Adds tests for all available torch.HalfTensor functions.
Allows compiling without TH_GENERIC_USE_HALF (so if there's a
problem can just unset that in CMakeLists rather than backing out)
Some simplifications: removes a new copy optimization and
some TH_HALF literal definitions; I'd rather not include optimizations
to non-half types in this PR.

Limitations:
Because match functions are not defined, some ""non-math"" operators
on torch.HalfTensor give an error message, e.g. index/newindex
with a ByteTensor apply a mask, but masks aren't implemented.  These
limitations aren't always obvious (e.g. for documentation purposes), but
they should always give an error message.","This should be ready to merge once the cutorch changes are ready.  We now define a different TH_HALF type instead of defining 'half' as in cutorch; defining 'half' requires that files are included in the proper order so that cutorch does not redefine 'half'; this is probably doable, but tricky for downstream projects to get right.",True,{}
torch/torch7,https://github.com/torch/torch7,874,2016-12-21T00:38:57Z,2016-12-29T19:23:27Z,2016-12-30T10:59:49Z,MERGED,True,872,197,40,https://github.com/gchanan,Add support for torch.HalfTensor,3,[],https://github.com/torch/torch7/pull/874,https://github.com/gchanan,3,https://github.com/torch/torch7/pull/874#issuecomment-269516788,"NOTE: this should not be merged until the cutorch equivalent is ready; I'm working on that based on torch/cutorch#578.  This pull request won't break anything but because cutorch's generics overwrite the torch generics, you can lose some HalfTensor functionality via ""require cutorch""
This is based on #826 (which is the first commit in this request), with the following improvements/simplifications (the second commit):
Improvements/Simplifications:

No longer generates math functions that are not actually defined
on torch.HalfTensor, e.g. maskedFill, map2, etc.
Adds tests for all available torch.HalfTensor functions.
Allows compiling without TH_GENERIC_USE_HALF (so if there's a
problem can just unset that in CMakeLists rather than backing out)
Some simplifications: removes a new copy optimization and
some TH_HALF literal definitions; I'd rather not include optimizations
to non-half types in this PR.

Limitations:
Because match functions are not defined, some ""non-math"" operators
on torch.HalfTensor give an error message, e.g. index/newindex
with a ByteTensor apply a mask, but masks aren't implemented.  These
limitations aren't always obvious (e.g. for documentation purposes), but
they should always give an error message.",equivalent cutorch PR here: torch/cutorch#655,True,{}
torch/torch7,https://github.com/torch/torch7,874,2016-12-21T00:38:57Z,2016-12-29T19:23:27Z,2016-12-30T10:59:49Z,MERGED,True,872,197,40,https://github.com/gchanan,Add support for torch.HalfTensor,3,[],https://github.com/torch/torch7/pull/874,https://github.com/borisfom,4,https://github.com/torch/torch7/pull/874#issuecomment-269758768,"NOTE: this should not be merged until the cutorch equivalent is ready; I'm working on that based on torch/cutorch#578.  This pull request won't break anything but because cutorch's generics overwrite the torch generics, you can lose some HalfTensor functionality via ""require cutorch""
This is based on #826 (which is the first commit in this request), with the following improvements/simplifications (the second commit):
Improvements/Simplifications:

No longer generates math functions that are not actually defined
on torch.HalfTensor, e.g. maskedFill, map2, etc.
Adds tests for all available torch.HalfTensor functions.
Allows compiling without TH_GENERIC_USE_HALF (so if there's a
problem can just unset that in CMakeLists rather than backing out)
Some simplifications: removes a new copy optimization and
some TH_HALF literal definitions; I'd rather not include optimizations
to non-half types in this PR.

Limitations:
Because match functions are not defined, some ""non-math"" operators
on torch.HalfTensor give an error message, e.g. index/newindex
with a ByteTensor apply a mask, but masks aren't implemented.  These
limitations aren't always obvious (e.g. for documentation purposes), but
they should always give an error message.","Thanks for completing this! Yes using separate type for Half makes better sense (I started with that option, too). It should make it simpler to adopt a tricky __fp16 gcc ARM type, too.",True,{}
torch/torch7,https://github.com/torch/torch7,875,2016-12-22T02:38:20Z,2016-12-30T18:41:42Z,2016-12-31T00:11:14Z,MERGED,True,97,0,5,https://github.com/KaiyuYue,Add a new function bhistc to calculate histogram of batch of images only once,3,[],https://github.com/torch/torch7/pull/875,https://github.com/KaiyuYue,1,https://github.com/torch/torch7/pull/875,"[ Description ]
When a tensor contains multiple images, and one need to calculate the histogram of each image in this two dimension tensor. If using the original torch.histc function, should have to write a loop to calculate them. This way is not effective enough. So I write a new function named by torch.histc2 to output the histogram tensor only once, the input is the 2D tensor, each row indicates an image, the default bins is 100 as the original histc, calculate the histogram along the last dimension of input tensor.
[ Code Change ]

In lib/TH/generic/THTensorMath.c, add a new math function void THTensor_(histc2)
In lib/TH/generic/THTensorMath.h, add a statement for void THTensor_(histc2)
In TensorMath.lua, add the histc2 for wrap
In test/test.lua, add a test for new function histc2
In doc/maths.md, add a new description for torch.histc2 with some toy use instances

[ Compile ]
All these changes can be compiled pass locally.","[ Description ]
When a tensor contains multiple images, and one need to calculate the histogram of each image in this two dimension tensor. If using the original torch.histc function, should have to write a loop to calculate them. This way is not effective enough. So I write a new function named by torch.histc2 to output the histogram tensor only once, the input is the 2D tensor, each row indicates an image, the default bins is 100 as the original histc, calculate the histogram along the last dimension of input tensor.
[ Code Change ]

In lib/TH/generic/THTensorMath.c, add a new math function void THTensor_(histc2)
In lib/TH/generic/THTensorMath.h, add a statement for void THTensor_(histc2)
In TensorMath.lua, add the histc2 for wrap
In test/test.lua, add a test for new function histc2
In doc/maths.md, add a new description for torch.histc2 with some toy use instances

[ Compile ]
All these changes can be compiled pass locally.",True,{}
torch/torch7,https://github.com/torch/torch7,875,2016-12-22T02:38:20Z,2016-12-30T18:41:42Z,2016-12-31T00:11:14Z,MERGED,True,97,0,5,https://github.com/KaiyuYue,Add a new function bhistc to calculate histogram of batch of images only once,3,[],https://github.com/torch/torch7/pull/875,https://github.com/soumith,2,https://github.com/torch/torch7/pull/875#issuecomment-269346554,"[ Description ]
When a tensor contains multiple images, and one need to calculate the histogram of each image in this two dimension tensor. If using the original torch.histc function, should have to write a loop to calculate them. This way is not effective enough. So I write a new function named by torch.histc2 to output the histogram tensor only once, the input is the 2D tensor, each row indicates an image, the default bins is 100 as the original histc, calculate the histogram along the last dimension of input tensor.
[ Code Change ]

In lib/TH/generic/THTensorMath.c, add a new math function void THTensor_(histc2)
In lib/TH/generic/THTensorMath.h, add a statement for void THTensor_(histc2)
In TensorMath.lua, add the histc2 for wrap
In test/test.lua, add a test for new function histc2
In doc/maths.md, add a new description for torch.histc2 with some toy use instances

[ Compile ]
All these changes can be compiled pass locally.","there are a few functions that batch an operation, like bmm, baddmm, baddbmm.
To keep with the same convention, I suggest that you rename histc2 to bhistc",True,{}
torch/torch7,https://github.com/torch/torch7,875,2016-12-22T02:38:20Z,2016-12-30T18:41:42Z,2016-12-31T00:11:14Z,MERGED,True,97,0,5,https://github.com/KaiyuYue,Add a new function bhistc to calculate histogram of batch of images only once,3,[],https://github.com/torch/torch7/pull/875,https://github.com/KaiyuYue,3,https://github.com/torch/torch7/pull/875#issuecomment-269444643,"[ Description ]
When a tensor contains multiple images, and one need to calculate the histogram of each image in this two dimension tensor. If using the original torch.histc function, should have to write a loop to calculate them. This way is not effective enough. So I write a new function named by torch.histc2 to output the histogram tensor only once, the input is the 2D tensor, each row indicates an image, the default bins is 100 as the original histc, calculate the histogram along the last dimension of input tensor.
[ Code Change ]

In lib/TH/generic/THTensorMath.c, add a new math function void THTensor_(histc2)
In lib/TH/generic/THTensorMath.h, add a statement for void THTensor_(histc2)
In TensorMath.lua, add the histc2 for wrap
In test/test.lua, add a test for new function histc2
In doc/maths.md, add a new description for torch.histc2 with some toy use instances

[ Compile ]
All these changes can be compiled pass locally.","I have renamed histc2 to bhistc in all code files. Yeah~, indeed, this name is more formal and easy understanding. :)",True,{}
torch/torch7,https://github.com/torch/torch7,875,2016-12-22T02:38:20Z,2016-12-30T18:41:42Z,2016-12-31T00:11:14Z,MERGED,True,97,0,5,https://github.com/KaiyuYue,Add a new function bhistc to calculate histogram of batch of images only once,3,[],https://github.com/torch/torch7/pull/875,https://github.com/soumith,4,https://github.com/torch/torch7/pull/875#issuecomment-269806845,"[ Description ]
When a tensor contains multiple images, and one need to calculate the histogram of each image in this two dimension tensor. If using the original torch.histc function, should have to write a loop to calculate them. This way is not effective enough. So I write a new function named by torch.histc2 to output the histogram tensor only once, the input is the 2D tensor, each row indicates an image, the default bins is 100 as the original histc, calculate the histogram along the last dimension of input tensor.
[ Code Change ]

In lib/TH/generic/THTensorMath.c, add a new math function void THTensor_(histc2)
In lib/TH/generic/THTensorMath.h, add a statement for void THTensor_(histc2)
In TensorMath.lua, add the histc2 for wrap
In test/test.lua, add a test for new function histc2
In doc/maths.md, add a new description for torch.histc2 with some toy use instances

[ Compile ]
All these changes can be compiled pass locally.",thanks!,True,{}
torch/torch7,https://github.com/torch/torch7,882,2016-12-28T23:04:59Z,2016-12-29T19:24:57Z,2016-12-29T19:25:00Z,MERGED,True,1977,6,4,https://github.com/amrobbins,Add support for VSX vector instructions on PPC,1,[],https://github.com/torch/torch7/pull/882,https://github.com/amrobbins,1,https://github.com/torch/torch7/pull/882,"Added support for the fill, diff, scale, mul and add functions using
PPC CPU vector instructions. These are used in place of the versions
of these functions written for x86, when compiled on PPC.
This fixes a compile failure on PPC","Added support for the fill, diff, scale, mul and add functions using
PPC CPU vector instructions. These are used in place of the versions
of these functions written for x86, when compiled on PPC.
This fixes a compile failure on PPC",True,{}
torch/torch7,https://github.com/torch/torch7,882,2016-12-28T23:04:59Z,2016-12-29T19:24:57Z,2016-12-29T19:25:00Z,MERGED,True,1977,6,4,https://github.com/amrobbins,Add support for VSX vector instructions on PPC,1,[],https://github.com/torch/torch7/pull/882,https://github.com/soumith,2,https://github.com/torch/torch7/pull/882#issuecomment-269679945,"Added support for the fill, diff, scale, mul and add functions using
PPC CPU vector instructions. These are used in place of the versions
of these functions written for x86, when compiled on PPC.
This fixes a compile failure on PPC",thanks!,True,{}
torch/torch7,https://github.com/torch/torch7,886,2016-12-30T23:12:17Z,2016-12-30T23:17:29Z,2016-12-30T23:17:32Z,MERGED,True,29,9,2,https://github.com/soumith,use __get_cpuid when available,1,[],https://github.com/torch/torch7/pull/886,https://github.com/soumith,1,https://github.com/torch/torch7/pull/886,"Fixes #876
Thanks to @ius for the hint.","Fixes #876
Thanks to @ius for the hint.",True,{}
torch/torch7,https://github.com/torch/torch7,888,2017-01-01T03:54:59Z,2017-01-01T04:07:49Z,2017-01-01T04:07:49Z,MERGED,True,7,8,3,https://github.com/soumith,"TH_GENERIC_USE_HALF=1 by default, half enabled by default",1,[],https://github.com/torch/torch7/pull/888,https://github.com/soumith,1,https://github.com/torch/torch7/pull/888,"The needed declarations to enable this are causing issues with libraries downstream that use TH, which now have to enable this flag in their CMakeLists.txt (meaning lot of downstream libs needs build changes), and I've checked the use of the flag and there's really no need to put this behind an optional barrier. The code itself looks quite simple and portable.
So, enabling it as default.
Also fixing a #define leak of align
cc: @gchanan","The needed declarations to enable this are causing issues with libraries downstream that use TH, which now have to enable this flag in their CMakeLists.txt (meaning lot of downstream libs needs build changes), and I've checked the use of the flag and there's really no need to put this behind an optional barrier. The code itself looks quite simple and portable.
So, enabling it as default.
Also fixing a #define leak of align
cc: @gchanan",True,{}
torch/torch7,https://github.com/torch/torch7,889,2017-01-01T06:07:10Z,2017-01-01T06:07:19Z,2017-01-01T06:07:19Z,MERGED,True,8,7,3,https://github.com/soumith,"Revert ""TH_GENERIC_USE_HALF=1 by default, half enabled by default""",1,[],https://github.com/torch/torch7/pull/889,https://github.com/soumith,1,https://github.com/torch/torch7/pull/889,"Reverts #888
this has some side-effects wrt THGenerateAllTypes.h having half by default.
Reverting, and will re-patch a more robust version.","Reverts #888
this has some side-effects wrt THGenerateAllTypes.h having half by default.
Reverting, and will re-patch a more robust version.",True,{}
torch/torch7,https://github.com/torch/torch7,890,2017-01-01T08:58:12Z,2017-01-01T09:09:11Z,2017-01-01T09:09:32Z,MERGED,True,79,167,39,https://github.com/soumith,"Removing Half conditional macros, and enabling half compilation by default",1,[],https://github.com/torch/torch7/pull/890,https://github.com/soumith,1,https://github.com/torch/torch7/pull/890,"TH_GENERIC_USE_HALF, TH_NATIVE_HALF, TH_GENERIC_NO_MATH (replaced where appropriate with TH_REAL_IS_HALF), removed half from THGenerateAllTypes, added an explicit THGenerateHalfType.h
The needed declarations to enable this are causing issues with libraries downstream that use TH, which now have to enable this flag in their CMakeLists.txt (meaning lot of downstream libs needs build changes), and I've checked the use of the flag and there's really no need to put this behind an optional barrier. The code itself looks quite simple and portable.
So, enabling it as default.
Also fixing a #define leak of __align__","TH_GENERIC_USE_HALF, TH_NATIVE_HALF, TH_GENERIC_NO_MATH (replaced where appropriate with TH_REAL_IS_HALF), removed half from THGenerateAllTypes, added an explicit THGenerateHalfType.h
The needed declarations to enable this are causing issues with libraries downstream that use TH, which now have to enable this flag in their CMakeLists.txt (meaning lot of downstream libs needs build changes), and I've checked the use of the flag and there's really no need to put this behind an optional barrier. The code itself looks quite simple and portable.
So, enabling it as default.
Also fixing a #define leak of __align__",True,{}
torch/torch7,https://github.com/torch/torch7,892,2017-01-03T17:29:16Z,2017-01-03T17:29:53Z,2017-01-03T17:29:57Z,MERGED,True,2,2,1,https://github.com/temerick,Update FindARM.cmake,1,[],https://github.com/torch/torch7/pull/892,https://github.com/temerick,1,https://github.com/torch/torch7/pull/892,Fix typos,Fix typos,True,{}
torch/torch7,https://github.com/torch/torch7,892,2017-01-03T17:29:16Z,2017-01-03T17:29:53Z,2017-01-03T17:29:57Z,MERGED,True,2,2,1,https://github.com/temerick,Update FindARM.cmake,1,[],https://github.com/torch/torch7/pull/892,https://github.com/soumith,2,https://github.com/torch/torch7/pull/892#issuecomment-270171314,Fix typos,thanks!,True,{}
torch/torch7,https://github.com/torch/torch7,895,2017-01-05T00:44:12Z,2017-01-05T00:48:22Z,2017-01-05T00:48:22Z,MERGED,True,1,1,1,https://github.com/colesbury,Fix condition for threadArgErrorHandler,1,[],https://github.com/torch/torch7/pull/895,https://github.com/colesbury,1,https://github.com/torch/torch7/pull/895,Some error handlers may not have any data associated with them,Some error handlers may not have any data associated with them,True,{}
torch/torch7,https://github.com/torch/torch7,899,2017-01-06T16:05:33Z,2017-01-06T16:06:03Z,2017-01-06T16:06:03Z,MERGED,True,8,2,1,https://github.com/gchanan,Fix THHalf issues with MSVC.,1,[],https://github.com/torch/torch7/pull/899,https://github.com/gchanan,1,https://github.com/torch/torch7/pull/899,"Avoid casting TH_HALF to TH_HALF, as it breaks msvc.","Avoid casting TH_HALF to TH_HALF, as it breaks msvc.",True,{'THUMBS_UP': ['https://github.com/zhangxiangxiao']}
torch/torch7,https://github.com/torch/torch7,900,2017-01-06T23:51:15Z,2017-01-06T23:59:12Z,2017-01-07T00:01:43Z,CLOSED,False,1,1,1,https://github.com/karthikncode,Fix contiguous assertion in View implementation,1,[],https://github.com/torch/torch7/pull/900,https://github.com/karthikncode,1,https://github.com/torch/torch7/pull/900,Replace the assertion in View implementation with automatic cloning if tensor not contiguous.,Replace the assertion in View implementation with automatic cloning if tensor not contiguous.,True,{}
torch/torch7,https://github.com/torch/torch7,900,2017-01-06T23:51:15Z,2017-01-06T23:59:12Z,2017-01-07T00:01:43Z,CLOSED,False,1,1,1,https://github.com/karthikncode,Fix contiguous assertion in View implementation,1,[],https://github.com/torch/torch7/pull/900,https://github.com/fmassa,2,https://github.com/torch/torch7/pull/900#issuecomment-271042059,Replace the assertion in View implementation with automatic cloning if tensor not contiguous.,"Hum, I wonder if this is the desired behaviour.
There is an assumption that :view returns a view of the same storage, so cloning it might lead to unwanted behaviour at some places.",True,{}
torch/torch7,https://github.com/torch/torch7,900,2017-01-06T23:51:15Z,2017-01-06T23:59:12Z,2017-01-07T00:01:43Z,CLOSED,False,1,1,1,https://github.com/karthikncode,Fix contiguous assertion in View implementation,1,[],https://github.com/torch/torch7/pull/900,https://github.com/karthikncode,3,https://github.com/torch/torch7/pull/900#issuecomment-271042210,Replace the assertion in View implementation with automatic cloning if tensor not contiguous.,"Not sure if it breaks anything, but I was getting the assertion error with nn.View otherwise.",True,{}
torch/torch7,https://github.com/torch/torch7,900,2017-01-06T23:51:15Z,2017-01-06T23:59:12Z,2017-01-07T00:01:43Z,CLOSED,False,1,1,1,https://github.com/karthikncode,Fix contiguous assertion in View implementation,1,[],https://github.com/torch/torch7/pull/900,https://github.com/karthikncode,4,https://github.com/torch/torch7/pull/900#issuecomment-271042238,Replace the assertion in View implementation with automatic cloning if tensor not contiguous.,"Ah, I see. So maybe not a global change then?",True,{}
torch/torch7,https://github.com/torch/torch7,900,2017-01-06T23:51:15Z,2017-01-06T23:59:12Z,2017-01-07T00:01:43Z,CLOSED,False,1,1,1,https://github.com/karthikncode,Fix contiguous assertion in View implementation,1,[],https://github.com/torch/torch7/pull/900,https://github.com/fmassa,5,https://github.com/torch/torch7/pull/900#issuecomment-271042276,Replace the assertion in View implementation with automatic cloning if tensor not contiguous.,Can't you use a nn.Contiguous before nn.View?,True,{}
torch/torch7,https://github.com/torch/torch7,900,2017-01-06T23:51:15Z,2017-01-06T23:59:12Z,2017-01-07T00:01:43Z,CLOSED,False,1,1,1,https://github.com/karthikncode,Fix contiguous assertion in View implementation,1,[],https://github.com/torch/torch7/pull/900,https://github.com/karthikncode,6,https://github.com/torch/torch7/pull/900#issuecomment-271042442,Replace the assertion in View implementation with automatic cloning if tensor not contiguous.,Ah - didn't realize there was one for that. Sorry about that!,True,{}
torch/torch7,https://github.com/torch/torch7,901,2017-01-07T01:09:07Z,2017-02-04T02:00:14Z,2017-02-04T02:00:22Z,MERGED,True,38,29,4,https://github.com/borisfom,half<->float conversion cleanup,5,[],https://github.com/torch/torch7/pull/901,https://github.com/borisfom,1,https://github.com/torch/torch7/pull/901,"Refactored _raw methods from TH_half2float and TH_half2float - primarily for reusability in cutorch.
Fixed Inf constant for fp16.","Refactored _raw methods from TH_half2float and TH_half2float - primarily for reusability in cutorch.
Fixed Inf constant for fp16.",True,{}
torch/torch7,https://github.com/torch/torch7,901,2017-01-07T01:09:07Z,2017-02-04T02:00:14Z,2017-02-04T02:00:22Z,MERGED,True,38,29,4,https://github.com/borisfom,half<->float conversion cleanup,5,[],https://github.com/torch/torch7/pull/901,https://github.com/gchanan,2,https://github.com/torch/torch7/pull/901#issuecomment-272562059,"Refactored _raw methods from TH_half2float and TH_half2float - primarily for reusability in cutorch.
Fixed Inf constant for fp16.",FYI I think this is now conflicting because of 3caabb9 -- which changes the conversion functions slightly to avoid strict aliasing warnings.,True,{}
torch/torch7,https://github.com/torch/torch7,901,2017-01-07T01:09:07Z,2017-02-04T02:00:14Z,2017-02-04T02:00:22Z,MERGED,True,38,29,4,https://github.com/borisfom,half<->float conversion cleanup,5,[],https://github.com/torch/torch7/pull/901,https://github.com/borisfom,3,https://github.com/torch/torch7/pull/901#issuecomment-272990505,"Refactored _raw methods from TH_half2float and TH_half2float - primarily for reusability in cutorch.
Fixed Inf constant for fp16.",@gchanan : should be all ready to go now.,True,{}
torch/torch7,https://github.com/torch/torch7,901,2017-01-07T01:09:07Z,2017-02-04T02:00:14Z,2017-02-04T02:00:22Z,MERGED,True,38,29,4,https://github.com/borisfom,half<->float conversion cleanup,5,[],https://github.com/torch/torch7/pull/901,https://github.com/borisfom,4,https://github.com/torch/torch7/pull/901#issuecomment-276816903,"Refactored _raw methods from TH_half2float and TH_half2float - primarily for reusability in cutorch.
Fixed Inf constant for fp16.",@gchanan : can we finalize this?,True,{}
torch/torch7,https://github.com/torch/torch7,901,2017-01-07T01:09:07Z,2017-02-04T02:00:14Z,2017-02-04T02:00:22Z,MERGED,True,38,29,4,https://github.com/borisfom,half<->float conversion cleanup,5,[],https://github.com/torch/torch7/pull/901,https://github.com/soumith,5,https://github.com/torch/torch7/pull/901#issuecomment-277409618,"Refactored _raw methods from TH_half2float and TH_half2float - primarily for reusability in cutorch.
Fixed Inf constant for fp16.",thanks!,True,{}
torch/torch7,https://github.com/torch/torch7,906,2017-01-10T23:33:16Z,2017-01-10T23:56:26Z,2017-01-10T23:56:26Z,MERGED,True,14,1,2,https://github.com/andresy,provide install paths through require 'torch.paths',1,[],https://github.com/torch/torch7/pull/906,https://github.com/andresy,1,https://github.com/torch/torch7/pull/906,,,True,{}
torch/torch7,https://github.com/torch/torch7,908,2017-01-11T10:09:44Z,2017-01-11T15:24:36Z,2017-01-11T15:24:45Z,MERGED,True,2,2,1,https://github.com/cdluminate,simd.h: really fix the arm64 (i.e. Aarch64) build,1,[],https://github.com/torch/torch7/pull/908,https://github.com/cdluminate,1,https://github.com/torch/torch7/pull/908,__NEON__ is an ISA indicator instead of architecture indicator.,__NEON__ is an ISA indicator instead of architecture indicator.,True,{}
torch/torch7,https://github.com/torch/torch7,908,2017-01-11T10:09:44Z,2017-01-11T15:24:36Z,2017-01-11T15:24:45Z,MERGED,True,2,2,1,https://github.com/cdluminate,simd.h: really fix the arm64 (i.e. Aarch64) build,1,[],https://github.com/torch/torch7/pull/908,https://github.com/soumith,2,https://github.com/torch/torch7/pull/908#issuecomment-271897267,__NEON__ is an ISA indicator instead of architecture indicator.,thanks Lumin! makes sense.,True,{}
torch/torch7,https://github.com/torch/torch7,911,2017-01-13T22:01:58Z,2017-01-13T22:06:17Z,2017-01-13T22:06:17Z,MERGED,True,5,3,1,https://github.com/gchanan,Avoid strict aliasing warning in float/half conversions.,1,[],https://github.com/torch/torch7/pull/911,https://github.com/gchanan,1,https://github.com/torch/torch7/pull/911,Verified that at least for GCC 4.47 this generates identical code.,Verified that at least for GCC 4.47 this generates identical code.,True,{}
torch/torch7,https://github.com/torch/torch7,915,2017-01-17T23:15:46Z,2017-01-24T14:14:34Z,2017-01-24T15:29:59Z,MERGED,True,43,89,4,https://github.com/pavanky,Macros to convert between real and accreal,2,[],https://github.com/torch/torch7/pull/915,https://github.com/pavanky,1,https://github.com/torch/torch7/pull/915,Also cleans up THGenerateAllTypes.h to simply include THGenerateFloatTypes.h and THGenerateIntTypes.h. This is similar to what is happening in cutorch.,Also cleans up THGenerateAllTypes.h to simply include THGenerateFloatTypes.h and THGenerateIntTypes.h. This is similar to what is happening in cutorch.,True,{}
torch/torch7,https://github.com/torch/torch7,920,2017-01-25T06:02:01Z,,2017-03-23T03:07:44Z,OPEN,False,14,10,2,https://github.com/borisfom,Disabling versioning for Android,3,[],https://github.com/torch/torch7/pull/920,https://github.com/borisfom,1,https://github.com/torch/torch7/pull/920,,,True,{}
torch/torch7,https://github.com/torch/torch7,921,2017-01-29T20:27:03Z,2017-01-30T00:16:58Z,2017-01-30T00:16:58Z,MERGED,True,7,4,2,https://github.com/apaszke,Fix bug in cat (non-contiguous first input),2,[],https://github.com/torch/torch7/pull/921,https://github.com/apaszke,1,https://github.com/torch/torch7/pull/921,First input has never been checked for contiguity.,First input has never been checked for contiguity.,True,{}
torch/torch7,https://github.com/torch/torch7,922,2017-02-01T19:44:02Z,2017-02-02T00:12:22Z,2017-02-02T02:09:58Z,MERGED,True,1,49,1,https://github.com/cliffwoolley,Remove erroneous proprietary license header,2,[],https://github.com/torch/torch7/pull/922,https://github.com/cliffwoolley,1,https://github.com/torch/torch7/pull/922,"This change was approved by NVIDIA Legal, and I am authorized to make the change on behalf of the company.","This change was approved by NVIDIA Legal, and I am authorized to make the change on behalf of the company.",True,{'HOORAY': ['https://github.com/Maratyszcza']}
torch/torch7,https://github.com/torch/torch7,922,2017-02-01T19:44:02Z,2017-02-02T00:12:22Z,2017-02-02T02:09:58Z,MERGED,True,1,49,1,https://github.com/cliffwoolley,Remove erroneous proprietary license header,2,[],https://github.com/torch/torch7/pull/922,https://github.com/soumith,2,https://github.com/torch/torch7/pull/922#issuecomment-276825757,"This change was approved by NVIDIA Legal, and I am authorized to make the change on behalf of the company.",thanks a lot Cliff! that was quick :),True,{}
torch/torch7,https://github.com/torch/torch7,922,2017-02-01T19:44:02Z,2017-02-02T00:12:22Z,2017-02-02T02:09:58Z,MERGED,True,1,49,1,https://github.com/cliffwoolley,Remove erroneous proprietary license header,2,[],https://github.com/torch/torch7/pull/922,https://github.com/cdluminate,3,https://github.com/torch/torch7/pull/922#issuecomment-276846054,"This change was approved by NVIDIA Legal, and I am authorized to make the change on behalf of the company.",Thank you @cliffwoolley !,True,{}
torch/torch7,https://github.com/torch/torch7,924,2017-02-02T13:36:46Z,2017-02-23T09:59:58Z,2017-02-23T09:59:58Z,CLOSED,False,4,6,1,https://github.com/Jokeren,Fix AVX2 detect bug,1,[],https://github.com/torch/torch7/pull/924,https://github.com/Jokeren,1,https://github.com/torch/torch7/pull/924,"It seems unnecessary to use GCC builtins, as the function __get_cpuid calls an asm statement internally. Therefore, I use the asm statement directly, avoiding the possible GCC bug.
Another fix is the flag of AVX2.","It seems unnecessary to use GCC builtins, as the function __get_cpuid calls an asm statement internally. Therefore, I use the asm statement directly, avoiding the possible GCC bug.
Another fix is the flag of AVX2.",True,{}
torch/torch7,https://github.com/torch/torch7,924,2017-02-02T13:36:46Z,2017-02-23T09:59:58Z,2017-02-23T09:59:58Z,CLOSED,False,4,6,1,https://github.com/Jokeren,Fix AVX2 detect bug,1,[],https://github.com/torch/torch7/pull/924,https://github.com/soumith,2,https://github.com/torch/torch7/pull/924#issuecomment-277145853,"It seems unnecessary to use GCC builtins, as the function __get_cpuid calls an asm statement internally. Therefore, I use the asm statement directly, avoiding the possible GCC bug.
Another fix is the flag of AVX2.","the reason I started using __get_cpuid was to get around this issue:
#876
My patch is here: #886
and was made after @ius had correctly pointed out the problem: #876 (comment)
Your PR will reintroduce this issue.
What is the solution...?
Maybe we can have a compile-time snippet that checks if the asm will compile, if yes, use that codepath, if not fallback to GCC intrinsic",True,{}
torch/torch7,https://github.com/torch/torch7,924,2017-02-02T13:36:46Z,2017-02-23T09:59:58Z,2017-02-23T09:59:58Z,CLOSED,False,4,6,1,https://github.com/Jokeren,Fix AVX2 detect bug,1,[],https://github.com/torch/torch7/pull/924,https://github.com/soumith,3,https://github.com/torch/torch7/pull/924#issuecomment-281948253,"It seems unnecessary to use GCC builtins, as the function __get_cpuid calls an asm statement internally. Therefore, I use the asm statement directly, avoiding the possible GCC bug.
Another fix is the flag of AVX2.",see #943,True,{}
torch/torch7,https://github.com/torch/torch7,926,2017-02-06T22:01:17Z,2017-02-09T06:38:58Z,2017-02-09T06:38:58Z,MERGED,True,78,25,5,https://github.com/colesbury,Add code for 'view' to TH,1,[],https://github.com/torch/torch7/pull/926,https://github.com/colesbury,1,https://github.com/torch/torch7/pull/926,,,True,{'THUMBS_UP': ['https://github.com/apaszke']}
torch/torch7,https://github.com/torch/torch7,929,2017-02-08T16:51:26Z,2017-02-09T05:37:47Z,2017-02-09T05:37:47Z,MERGED,True,28,0,2,https://github.com/colesbury,Add unsqueeze1d to TH,1,[],https://github.com/torch/torch7/pull/929,https://github.com/colesbury,1,https://github.com/torch/torch7/pull/929,"Unsqueeze inserts a singleton dimension. Unlike view, it doesn't require
the tensor to be contiguous.","Unsqueeze inserts a singleton dimension. Unlike view, it doesn't require
the tensor to be contiguous.",True,{}
torch/torch7,https://github.com/torch/torch7,930,2017-02-08T17:03:01Z,2017-02-09T05:29:58Z,2017-02-09T05:29:58Z,MERGED,True,40,40,3,https://github.com/colesbury,Expose rawSet and rawResize as resizeNd and setStorageNd,1,[],https://github.com/torch/torch7/pull/930,https://github.com/colesbury,1,https://github.com/torch/torch7/pull/930,"These methods are useful from C because they don't require constructing
THLongStorages to wrap the sizes and strides, which can lead to leaked
memory in case of an error. Instead the sizes and strides can be
represented on the stack using standard C long arrays.
We already expose rawResize in THC for use with MAGMA.","These methods are useful from C because they don't require constructing
THLongStorages to wrap the sizes and strides, which can lead to leaked
memory in case of an error. Instead the sizes and strides can be
represented on the stack using standard C long arrays.
We already expose rawResize in THC for use with MAGMA.",True,{}
torch/torch7,https://github.com/torch/torch7,933,2017-02-10T02:39:43Z,2017-02-23T00:34:30Z,2017-02-27T21:20:19Z,MERGED,True,727,0,5,https://github.com/pavanky,Bitwise operations,7,[],https://github.com/torch/torch7/pull/933,https://github.com/pavanky,1,https://github.com/torch/torch7/pull/933,"Includes lshift, rshift, bitor, bitand, bitxor
Includes contributions from Conrado Miranda from twitter.","Includes lshift, rshift, bitor, bitand, bitxor
Includes contributions from Conrado Miranda from twitter.",True,{}
torch/torch7,https://github.com/torch/torch7,933,2017-02-10T02:39:43Z,2017-02-23T00:34:30Z,2017-02-27T21:20:19Z,MERGED,True,727,0,5,https://github.com/pavanky,Bitwise operations,7,[],https://github.com/torch/torch7/pull/933,https://github.com/pavanky,2,https://github.com/torch/torch7/pull/933#issuecomment-278842504,"Includes lshift, rshift, bitor, bitand, bitxor
Includes contributions from Conrado Miranda from twitter.","Considering that Lua does not have bitwise operations (atleast in the 5.2 version installed from torch distro), I couldn't come up with a proper test for xor. Any suggestions on what kind of test can be added are welcome.",True,{}
torch/torch7,https://github.com/torch/torch7,933,2017-02-10T02:39:43Z,2017-02-23T00:34:30Z,2017-02-27T21:20:19Z,MERGED,True,727,0,5,https://github.com/pavanky,Bitwise operations,7,[],https://github.com/torch/torch7/pull/933,https://github.com/pavanky,3,https://github.com/torch/torch7/pull/933#issuecomment-281817271,"Includes lshift, rshift, bitor, bitand, bitxor
Includes contributions from Conrado Miranda from twitter.",@soumith Does this PR require any changes ?,True,{}
torch/torch7,https://github.com/torch/torch7,934,2017-02-10T06:55:11Z,2017-02-28T22:23:45Z,2017-02-28T22:23:49Z,MERGED,True,379,639,3,https://github.com/adamlerer,Speed up reductions on non-contiguous dimensions,3,[],https://github.com/torch/torch7/pull/934,https://github.com/adamlerer,1,https://github.com/torch/torch7/pull/934,"I just did sum, prod, mean, max, and min for now, but you can do a bunch of the other pointwise reductions like this too.
before:
th> t = torch.randn(64,1024,1024)
                                                                      [5.6855s]
th> t:max(1);
                                                                      [1.7969s]
th> t:max(2);
                                                                      [0.9834s]
th> t:max(3);
                                                                      [0.1487s]
th> t:sum(1);
                                                                      [1.5895s]
th> t:sum(2);
                                                                      [0.9665s]
th> t:sum(3);
                                                                      [0.1610s]

after:
th> t = torch.randn(64,1024,1024)
                                                                      [5.7820s]
th> t:max(1);
                                                                      [0.3097s]
th> t:max(2);
                                                                      [0.2392s]
th> t:max(3);
                                                                      [0.1456s]
th> t:sum(1);
                                                                      [0.1985s]
th> t:sum(2);
                                                                      [0.2219s]
th> t:sum(3);

I think the optimizations for max and min will conflict with the TH_TENSOR_APPLY improvements in #852, but we can always add special macros for this purpose if we want to keep the max and min optimizations. That new macro could also support stuff like cumsum.","I just did sum, prod, mean, max, and min for now, but you can do a bunch of the other pointwise reductions like this too.
before:
th> t = torch.randn(64,1024,1024)
                                                                      [5.6855s]
th> t:max(1);
                                                                      [1.7969s]
th> t:max(2);
                                                                      [0.9834s]
th> t:max(3);
                                                                      [0.1487s]
th> t:sum(1);
                                                                      [1.5895s]
th> t:sum(2);
                                                                      [0.9665s]
th> t:sum(3);
                                                                      [0.1610s]

after:
th> t = torch.randn(64,1024,1024)
                                                                      [5.7820s]
th> t:max(1);
                                                                      [0.3097s]
th> t:max(2);
                                                                      [0.2392s]
th> t:max(3);
                                                                      [0.1456s]
th> t:sum(1);
                                                                      [0.1985s]
th> t:sum(2);
                                                                      [0.2219s]
th> t:sum(3);

I think the optimizations for max and min will conflict with the TH_TENSOR_APPLY improvements in #852, but we can always add special macros for this purpose if we want to keep the max and min optimizations. That new macro could also support stuff like cumsum.",True,"{'THUMBS_UP': ['https://github.com/apaszke', 'https://github.com/nicholas-leonard']}"
torch/torch7,https://github.com/torch/torch7,934,2017-02-10T06:55:11Z,2017-02-28T22:23:45Z,2017-02-28T22:23:49Z,MERGED,True,379,639,3,https://github.com/adamlerer,Speed up reductions on non-contiguous dimensions,3,[],https://github.com/torch/torch7/pull/934,https://github.com/soumith,2,https://github.com/torch/torch7/pull/934#issuecomment-278880676,"I just did sum, prod, mean, max, and min for now, but you can do a bunch of the other pointwise reductions like this too.
before:
th> t = torch.randn(64,1024,1024)
                                                                      [5.6855s]
th> t:max(1);
                                                                      [1.7969s]
th> t:max(2);
                                                                      [0.9834s]
th> t:max(3);
                                                                      [0.1487s]
th> t:sum(1);
                                                                      [1.5895s]
th> t:sum(2);
                                                                      [0.9665s]
th> t:sum(3);
                                                                      [0.1610s]

after:
th> t = torch.randn(64,1024,1024)
                                                                      [5.7820s]
th> t:max(1);
                                                                      [0.3097s]
th> t:max(2);
                                                                      [0.2392s]
th> t:max(3);
                                                                      [0.1456s]
th> t:sum(1);
                                                                      [0.1985s]
th> t:sum(2);
                                                                      [0.2219s]
th> t:sum(3);

I think the optimizations for max and min will conflict with the TH_TENSOR_APPLY improvements in #852, but we can always add special macros for this purpose if we want to keep the max and min optimizations. That new macro could also support stuff like cumsum.","it looks like we're lucky that TH_TENSOR_DIM_APPLY2 etc. dont have OpenMP optimizations yet. in the longer term, we need tree reductions.",True,{}
torch/torch7,https://github.com/torch/torch7,934,2017-02-10T06:55:11Z,2017-02-28T22:23:45Z,2017-02-28T22:23:49Z,MERGED,True,379,639,3,https://github.com/adamlerer,Speed up reductions on non-contiguous dimensions,3,[],https://github.com/torch/torch7/pull/934,https://github.com/adamlerer,3,https://github.com/torch/torch7/pull/934#issuecomment-278992081,"I just did sum, prod, mean, max, and min for now, but you can do a bunch of the other pointwise reductions like this too.
before:
th> t = torch.randn(64,1024,1024)
                                                                      [5.6855s]
th> t:max(1);
                                                                      [1.7969s]
th> t:max(2);
                                                                      [0.9834s]
th> t:max(3);
                                                                      [0.1487s]
th> t:sum(1);
                                                                      [1.5895s]
th> t:sum(2);
                                                                      [0.9665s]
th> t:sum(3);
                                                                      [0.1610s]

after:
th> t = torch.randn(64,1024,1024)
                                                                      [5.7820s]
th> t:max(1);
                                                                      [0.3097s]
th> t:max(2);
                                                                      [0.2392s]
th> t:max(3);
                                                                      [0.1456s]
th> t:sum(1);
                                                                      [0.1985s]
th> t:sum(2);
                                                                      [0.2219s]
th> t:sum(3);

I think the optimizations for max and min will conflict with the TH_TENSOR_APPLY improvements in #852, but we can always add special macros for this purpose if we want to keep the max and min optimizations. That new macro could also support stuff like cumsum.","@soumith re OpenMP: as I said to @apaszke this comes down to the specification of TH_TENSOR_APPLY (not TH_TENSOR_DIM_APPLY) w.r.t. stride-0 dimensions. Currently it handles them correctly. If someone changes it in a way that breaks that, we'll need to maintain a version that handles stride-0 correctly. Note that there's already existing code that sort of depends on handling this case correctly, e.g.
t = torch.Tensor(10, 10, 10)
u = torch.Tensor(1, 10, 10).expand_as(t).zero_()
u.add_(t)

so I'm not sure if it is now safe to break that (considering the specification was ill-defined).
Anyway, the easiest way to circumvent all these worries is to write a separate version of TH_APPLY that's almost the same but explicitly handles stride-0 dimension, deals with indexes more explicitly (for max etc.) and we can also add some stuff that lets it deal with cumsum etc. Do you want that now before merging this PR?",True,{}
torch/torch7,https://github.com/torch/torch7,934,2017-02-10T06:55:11Z,2017-02-28T22:23:45Z,2017-02-28T22:23:49Z,MERGED,True,379,639,3,https://github.com/adamlerer,Speed up reductions on non-contiguous dimensions,3,[],https://github.com/torch/torch7/pull/934,https://github.com/adamlerer,4,https://github.com/torch/torch7/pull/934#issuecomment-281503692,"I just did sum, prod, mean, max, and min for now, but you can do a bunch of the other pointwise reductions like this too.
before:
th> t = torch.randn(64,1024,1024)
                                                                      [5.6855s]
th> t:max(1);
                                                                      [1.7969s]
th> t:max(2);
                                                                      [0.9834s]
th> t:max(3);
                                                                      [0.1487s]
th> t:sum(1);
                                                                      [1.5895s]
th> t:sum(2);
                                                                      [0.9665s]
th> t:sum(3);
                                                                      [0.1610s]

after:
th> t = torch.randn(64,1024,1024)
                                                                      [5.7820s]
th> t:max(1);
                                                                      [0.3097s]
th> t:max(2);
                                                                      [0.2392s]
th> t:max(3);
                                                                      [0.1456s]
th> t:sum(1);
                                                                      [0.1985s]
th> t:sum(2);
                                                                      [0.2219s]
th> t:sum(3);

I think the optimizations for max and min will conflict with the TH_TENSOR_APPLY improvements in #852, but we can always add special macros for this purpose if we want to keep the max and min optimizations. That new macro could also support stuff like cumsum.","Alright, I rebased this on top of #852, refactored TH_TENSOR_APPLY to remove all the copypasta and added a variant that can handle these reduction cases.
You'll have to wait until #852 is landed to review this.",True,{}
torch/torch7,https://github.com/torch/torch7,934,2017-02-10T06:55:11Z,2017-02-28T22:23:45Z,2017-02-28T22:23:49Z,MERGED,True,379,639,3,https://github.com/adamlerer,Speed up reductions on non-contiguous dimensions,3,[],https://github.com/torch/torch7/pull/934,https://github.com/soumith,5,https://github.com/torch/torch7/pull/934#issuecomment-283180659,"I just did sum, prod, mean, max, and min for now, but you can do a bunch of the other pointwise reductions like this too.
before:
th> t = torch.randn(64,1024,1024)
                                                                      [5.6855s]
th> t:max(1);
                                                                      [1.7969s]
th> t:max(2);
                                                                      [0.9834s]
th> t:max(3);
                                                                      [0.1487s]
th> t:sum(1);
                                                                      [1.5895s]
th> t:sum(2);
                                                                      [0.9665s]
th> t:sum(3);
                                                                      [0.1610s]

after:
th> t = torch.randn(64,1024,1024)
                                                                      [5.7820s]
th> t:max(1);
                                                                      [0.3097s]
th> t:max(2);
                                                                      [0.2392s]
th> t:max(3);
                                                                      [0.1456s]
th> t:sum(1);
                                                                      [0.1985s]
th> t:sum(2);
                                                                      [0.2219s]
th> t:sum(3);

I think the optimizations for max and min will conflict with the TH_TENSOR_APPLY improvements in #852, but we can always add special macros for this purpose if we want to keep the max and min optimizations. That new macro could also support stuff like cumsum.",thanks Adam!,True,{}
torch/torch7,https://github.com/torch/torch7,943,2017-02-23T09:59:48Z,2017-02-23T10:07:08Z,2017-02-23T10:07:08Z,MERGED,True,25,5,2,https://github.com/soumith,fix AVX2 detection bugs,1,[],https://github.com/torch/torch7/pull/943,https://github.com/soumith,1,https://github.com/torch/torch7/pull/943,"A more proper fix of #924
In addition to #924, makes sure that we dont hit a particular GCC bug","A more proper fix of #924
In addition to #924, makes sure that we dont hit a particular GCC bug",True,{}
torch/torch7,https://github.com/torch/torch7,944,2017-02-23T10:31:58Z,2017-02-28T17:30:56Z,2017-02-28T17:30:56Z,MERGED,True,1439,574,20,https://github.com/soumith,Adding AVX Optimizations with Runtime Dispatch,54,[],https://github.com/torch/torch7/pull/944,https://github.com/soumith,1,https://github.com/torch/torch7/pull/944,Rebased version of #852,Rebased version of #852,True,{}
torch/torch7,https://github.com/torch/torch7,944,2017-02-23T10:31:58Z,2017-02-28T17:30:56Z,2017-02-28T17:30:56Z,MERGED,True,1439,574,20,https://github.com/soumith,Adding AVX Optimizations with Runtime Dispatch,54,[],https://github.com/torch/torch7/pull/944,https://github.com/soumith,2,https://github.com/torch/torch7/pull/944#issuecomment-281985972,Rebased version of #852,"this looks good now.
The main affected functions for SIMD are cadd, add, cmul, mul, cdiv, div, copy.
And the APPLY optimizations afffect almost all the functions.
The testing story around this PR is a little weak, especially around SIMD.
I'll work on this PR today to introduce a few environment variables that control enabling/disabling  SIMD features.
I'm thinking of introducing environment variables:
TH_NO_AVX2, TH_NO_AVX, TH_NO_SSE to control the runtime dispatch to skip certain features.
That way, we can easily test different SIMD code paths. For example: TH_NO_AVX2=1 th -ltorch -e""torch.test()"" will test AVX, and similarly setting TH_NO_AVX2=1 TH_NO_AVX=1 will test the SSE codepath.
I see little value in making these flags to be enabled at a lua level, and the code changes needed to enable them at the lua level is relatively large (dispatch pointers are initialized at the beginning of TH's init, so these have to be changed too).
Let me know of any objections. cc: @Jokeren @adamlerer @pavanky",True,{}
torch/torch7,https://github.com/torch/torch7,944,2017-02-23T10:31:58Z,2017-02-28T17:30:56Z,2017-02-28T17:30:56Z,MERGED,True,1439,574,20,https://github.com/soumith,Adding AVX Optimizations with Runtime Dispatch,54,[],https://github.com/torch/torch7/pull/944,https://github.com/Jokeren,3,https://github.com/torch/torch7/pull/944#issuecomment-282019203,Rebased version of #852,"@soumith , this is fine.
I am thinking of another problem. The VSX vector codes are implemented with the old interfaces. So after merging the simd-opt branch, they will not take effect anymore.",True,{}
torch/torch7,https://github.com/torch/torch7,944,2017-02-23T10:31:58Z,2017-02-28T17:30:56Z,2017-02-28T17:30:56Z,MERGED,True,1439,574,20,https://github.com/soumith,Adding AVX Optimizations with Runtime Dispatch,54,[],https://github.com/torch/torch7/pull/944,https://github.com/soumith,4,https://github.com/torch/torch7/pull/944#issuecomment-282270639,Rebased version of #852,cc: @amrobbins about the VSX breakage,True,{}
torch/torch7,https://github.com/torch/torch7,944,2017-02-23T10:31:58Z,2017-02-28T17:30:56Z,2017-02-28T17:30:56Z,MERGED,True,1439,574,20,https://github.com/soumith,Adding AVX Optimizations with Runtime Dispatch,54,[],https://github.com/torch/torch7/pull/944,https://github.com/soumith,5,https://github.com/torch/torch7/pull/944#issuecomment-282272530,Rebased version of #852,"@Jokeren there are still bugs in this PR, i'm seeing stack corruption.
I've isolated the bugs to not be in the SIMD optimizations themselves.
I suspect bugs in the optimizations of APPLY kernels.
To reproduce, you can install this branch and then run tests for the nn package:
th -lnn -e ""nn.test()""

Gives me double free and malloc failure errors:
luajit: malloc.c:3695: _int_malloc: Assertion `(unsigned long) (size) >= (unsigned long) (nb)' failed.

I'm running a bisect, i'll isolate it shortly.",True,{}
torch/torch7,https://github.com/torch/torch7,944,2017-02-23T10:31:58Z,2017-02-28T17:30:56Z,2017-02-28T17:30:56Z,MERGED,True,1439,574,20,https://github.com/soumith,Adding AVX Optimizations with Runtime Dispatch,54,[],https://github.com/torch/torch7/pull/944,https://github.com/soumith,6,https://github.com/torch/torch7/pull/944#issuecomment-282276397,Rebased version of #852,"i've isolated the bug to be starting at the commit ""Fill bug fix - 326cbf1"", but i'm not super confident about my bisect (it's a complex PR)",True,{}
torch/torch7,https://github.com/torch/torch7,944,2017-02-23T10:31:58Z,2017-02-28T17:30:56Z,2017-02-28T17:30:56Z,MERGED,True,1439,574,20,https://github.com/soumith,Adding AVX Optimizations with Runtime Dispatch,54,[],https://github.com/torch/torch7/pull/944,https://github.com/adamlerer,7,https://github.com/torch/torch7/pull/944#issuecomment-282418700,Rebased version of #852,"The bug is that 326cbf1 moves r__data forward by size, but TH_APPLY only moves it back by r__i (which is 0 since you break in the first iteration). Here's a patch that fixes the memory corruption bug.
diff --git a/lib/TH/generic/THTensorMath.c b/lib/TH/generic/THTensorMath.c
index f698e25..2b8c9e1 100644
--- a/lib/TH/generic/THTensorMath.c
+++ b/lib/TH/generic/THTensorMath.c
@@ -97,8 +97,9 @@ void THTensor_(fill)(THTensor *r_, real value)
     TH_TENSOR_APPLY(real, r_,
       if (r__stride == 1) {
         THVector_(fill)(r__data, value, r__size);
-       r__data += r__stride * r__size;
-       break;
+        r__i = r__size;
+        r__data += r__stride * r__size;
+        break;
       } else {
         *r__data = value;
       }

Unfortunately, once you do that, there are other failures in nn. These are caused by the fact tht @Jokeren has changed the semantics of THVector_(add) (while leaving the function signature the same), which is used by nn in a couple places. Here's an nn patch that fixes them:
diff --git a/lib/THNN/generic/VolumetricConvolutionMM.c b/lib/THNN/generic/VolumetricConvolutionMM.c
index 4085e2b..caaaa55 100644
--- a/lib/THNN/generic/VolumetricConvolutionMM.c
+++ b/lib/THNN/generic/VolumetricConvolutionMM.c
@@ -152,7 +152,8 @@ static void THNN_(unfolded_acc_vol)(
                   }
                   else
                   {
-                    THVector_(add)(dst+it*inputHeight*inputWidth+iy*inputWidth+ix, src+t*outputHeight*outputWidth+y*outputWidth+x, 1, 1);
+                    real *dst_slice = dst+it*inputHeight*inputWidth+iy*inputWidth+ix;
+                    THVector_(cadd)(dst_slice, dst_slice, src+t*outputHeight*outputWidth+y*outputWidth+x, 1, 1);
                   }
                 }
               }
@@ -169,7 +170,8 @@ static void THNN_(unfolded_acc_vol)(
                 for(x = 0; x < outputWidth; x++)
                 {
                   ix = x*dW + kw;
-                  THVector_(add)(dst+it*inputHeight*inputWidth+iy*inputWidth+ix, src+t*outputHeight*outputWidth+y*outputWidth+x, 1, 1);
+                  real *dst_slice = dst+it*inputHeight*inputWidth+iy*inputWidth+ix;
+                  THVector_(cadd)(dst_slice, dst_slice, src+t*outputHeight*outputWidth+y*outputWidth+x, 1, 1);
                 }
               }
             }
diff --git a/lib/THNN/generic/unfold.c b/lib/THNN/generic/unfold.c
index 25146c0..e718320 100644
--- a/lib/THNN/generic/unfold.c
+++ b/lib/THNN/generic/unfold.c
@@ -52,14 +52,17 @@ void THNN_(unfolded_acc)(
                  ix = (long long)(0 - padW + kw);
                  lpad = fmaxf(0,(int)(padW-kw));
                  rpad = fmaxf(0,(int)(padW-(kW-kw-1)));
-                 THVector_(add)(dst+(size_t)(iy*inputWidth+ix+lpad), src+(size_t)(y*outputWidth+lpad), 1, outputWidth - lpad - rpad); /* note: THVector_add could handle 1 value better */
+                 real *dst_slice = dst+(size_t)(iy*inputWidth+ix+lpad);
+                 THVector_(cadd)(dst_slice, dst_slice, src+(size_t)(y*outputWidth+lpad), 1, outputWidth - lpad - rpad); /* note: THVector_add could handle 1 value better */
               }
               else{
                 for (x=0; x<outputWidth; x++){
                    ix = (long long)(x*dW - padW + kw);
                    if (ix < 0 || ix >= inputWidth){
-                   }else
-                     THVector_(add)(dst+(size_t)(iy*inputWidth+ix), src+(size_t)(y*outputWidth+x), 1, 1);
+                   }else{
+                     real *dst_slice = dst+(size_t)(iy*inputWidth+ix);
+                     THVector_(cadd)(dst_slice, dst_slice, src+(size_t)(y*outputWidth+x), 1, 1);
+                   }
                 }
               }
             }
@@ -68,11 +71,14 @@ void THNN_(unfolded_acc)(
           for(y = 0; y < outputHeight; y++) {
             iy = (long long)(y*dH + kh);
             ix = (long long)(0 + kw);
-            if (dW == 1 )
-               THVector_(add)(dst+(size_t)(iy*inputWidth+ix), src+(size_t)(y*outputWidth), 1, outputWidth); /* note: THVector_add could handle 1 value better */
-            else{
-              for(x = 0; x < outputWidth; x++)
-                THVector_(add)(dst+(size_t)(iy*inputWidth+ix+x*dW), src+(size_t)(y*outputWidth+x), 1, 1);
+            if (dW == 1 ) {
+               real *dst_slice = dst+(size_t)(iy*inputWidth+ix);
+               THVector_(cadd)(dst_slice, dst_slice, src+(size_t)(y*outputWidth), 1, outputWidth); /* note: THVector_add could handle 1 value better */
+            }else{
+              for(x = 0; x < outputWidth; x++) {
+                real *dst_slice = dst+(size_t)(iy*inputWidth+ix+x*dW);
+                THVector_(cadd)(dst_slice, dst_slice, src+(size_t)(y*outputWidth+x), 1, 1);
+              }
             }
           }
         }

This patch isn't great though because it requires torch and nn to be updated in parallel, and will produce incorrect results silently if torch and nn are out of sync. Instead, @Jokeren needs to rename THVector_(add) to something like THVector_(addc) (for addConstant).",True,{}
torch/torch7,https://github.com/torch/torch7,944,2017-02-23T10:31:58Z,2017-02-28T17:30:56Z,2017-02-28T17:30:56Z,MERGED,True,1439,574,20,https://github.com/soumith,Adding AVX Optimizations with Runtime Dispatch,54,[],https://github.com/torch/torch7/pull/944,https://github.com/Jokeren,8,https://github.com/torch/torch7/pull/944#issuecomment-282465026,Rebased version of #852,"@adamlerer , @soumith , Thank you for comments!
I am curious about the bisect result because in the final commit I do not use that logic (326cbf1) for fill operation. I will take examine my code carefully. So sorry that I didn't consider testing THNN.",True,{}
torch/torch7,https://github.com/torch/torch7,944,2017-02-23T10:31:58Z,2017-02-28T17:30:56Z,2017-02-28T17:30:56Z,MERGED,True,1439,574,20,https://github.com/soumith,Adding AVX Optimizations with Runtime Dispatch,54,[],https://github.com/torch/torch7/pull/944,https://github.com/Jokeren,9,https://github.com/torch/torch7/pull/944#issuecomment-282467209,Rebased version of #852,"@adamlerer , it's very kind of you to do the debugging! But I support using the patch you proposed instead of renaming add API.
I think add stands for adding a vector with a constant just as mul and div. And cadd, cmul and cdiv are functions that handle two vectors (we have a constant in cadd to support fma). So in this way, I think the meaning of old add API is not correct. Why will torch and nn be out of sync? If we change both APIs now and reference the new version, it will be correct.",True,{}
torch/torch7,https://github.com/torch/torch7,944,2017-02-23T10:31:58Z,2017-02-28T17:30:56Z,2017-02-28T17:30:56Z,MERGED,True,1439,574,20,https://github.com/soumith,Adding AVX Optimizations with Runtime Dispatch,54,[],https://github.com/torch/torch7/pull/944,https://github.com/adamlerer,10,https://github.com/torch/torch7/pull/944#issuecomment-282498090,Rebased version of #852,"@Jokeren you're right that your new naming is better, but the problem is that you are changing the semantics of an existing function that downstream packages may be using, which will cause them to silently produce incorrect results.
Even if we update torch7 and nn at the same time, they are in separate repos so someone may sync only one of the repos and build it, which will cause their code to produce incorrect results. Similarly, some other downstream code (not nn) may be using THVector_(add) and will start silently producing incorrect results.",True,{}
torch/torch7,https://github.com/torch/torch7,944,2017-02-23T10:31:58Z,2017-02-28T17:30:56Z,2017-02-28T17:30:56Z,MERGED,True,1439,574,20,https://github.com/soumith,Adding AVX Optimizations with Runtime Dispatch,54,[],https://github.com/torch/torch7/pull/944,https://github.com/soumith,11,https://github.com/torch/torch7/pull/944#issuecomment-282520930,Rebased version of #852,"i'll rename THVector_(add) to THVector_(addscalar) (and other scalar functions renamed appropriately) and wrap up this work on monday.
Thanks a lot @adamlerer for debugging and giving the patches to fix the bugs.",True,{}
torch/torch7,https://github.com/torch/torch7,948,2017-03-01T00:04:06Z,2017-05-26T03:25:46Z,2017-05-26T03:25:46Z,MERGED,True,57,32,7,https://github.com/pavanky,New helper macro + fixes for old gcc ,2,[],https://github.com/torch/torch7/pull/948,https://github.com/pavanky,1,https://github.com/torch/torch7/pull/948,"ADD_TORCH_LIBRARY makes it easier to link libTH from various other packages.
Static version additionally checks for environment variable.","ADD_TORCH_LIBRARY makes it easier to link libTH from various other packages.
Static version additionally checks for environment variable.",True,{}
torch/torch7,https://github.com/torch/torch7,948,2017-03-01T00:04:06Z,2017-05-26T03:25:46Z,2017-05-26T03:25:46Z,MERGED,True,57,32,7,https://github.com/pavanky,New helper macro + fixes for old gcc ,2,[],https://github.com/torch/torch7/pull/948,https://github.com/pavanky,2,https://github.com/torch/torch7/pull/948#issuecomment-283477967,"ADD_TORCH_LIBRARY makes it easier to link libTH from various other packages.
Static version additionally checks for environment variable.",@soumith Removed that code segment. The current behavior is similar to the current version in master where both static and dynamic libs are built when static builds are requested. If not only dynamic libs are built.,True,{}
torch/torch7,https://github.com/torch/torch7,948,2017-03-01T00:04:06Z,2017-05-26T03:25:46Z,2017-05-26T03:25:46Z,MERGED,True,57,32,7,https://github.com/pavanky,New helper macro + fixes for old gcc ,2,[],https://github.com/torch/torch7/pull/948,https://github.com/soumith,3,https://github.com/torch/torch7/pull/948#issuecomment-288598499,"ADD_TORCH_LIBRARY makes it easier to link libTH from various other packages.
Static version additionally checks for environment variable.","if you rebase, i'll merge this.",True,{}
torch/torch7,https://github.com/torch/torch7,948,2017-03-01T00:04:06Z,2017-05-26T03:25:46Z,2017-05-26T03:25:46Z,MERGED,True,57,32,7,https://github.com/pavanky,New helper macro + fixes for old gcc ,2,[],https://github.com/torch/torch7/pull/948,https://github.com/pavanky,4,https://github.com/torch/torch7/pull/948#issuecomment-288610340,"ADD_TORCH_LIBRARY makes it easier to link libTH from various other packages.
Static version additionally checks for environment variable.","@soumith sorry about not getting back to this, I'll clean this up tomorrow.",True,{}
torch/torch7,https://github.com/torch/torch7,949,2017-03-01T00:11:10Z,2017-03-23T16:57:11Z,2017-03-23T16:57:11Z,CLOSED,False,860,228,9,https://github.com/pavanky,custom convolution kernels for 3x3,1,[],https://github.com/torch/torch7/pull/949,https://github.com/pavanky,1,https://github.com/torch/torch7/pull/949,,,True,{}
torch/torch7,https://github.com/torch/torch7,949,2017-03-01T00:11:10Z,2017-03-23T16:57:11Z,2017-03-23T16:57:11Z,CLOSED,False,860,228,9,https://github.com/pavanky,custom convolution kernels for 3x3,1,[],https://github.com/torch/torch7/pull/949,https://github.com/soumith,2,https://github.com/torch/torch7/pull/949#issuecomment-283204305,,"just fyi it's prob most efficient at this point on CPU to either use NNPack or MKLDNN:
https://github.com/Maratyszcza/NNPACK
https://github.com/szagoruyko/nnpack.torch",True,{}
torch/torch7,https://github.com/torch/torch7,949,2017-03-01T00:11:10Z,2017-03-23T16:57:11Z,2017-03-23T16:57:11Z,CLOSED,False,860,228,9,https://github.com/pavanky,custom convolution kernels for 3x3,1,[],https://github.com/torch/torch7/pull/949,https://github.com/pavanky,3,https://github.com/torch/torch7/pull/949#issuecomment-283205680,,@soumith Thanks for the links. This PR is not super important from our end :) Sent it in because it was the last commit on our fork that diverged from upstream torch.,True,{}
torch/torch7,https://github.com/torch/torch7,949,2017-03-01T00:11:10Z,2017-03-23T16:57:11Z,2017-03-23T16:57:11Z,CLOSED,False,860,228,9,https://github.com/pavanky,custom convolution kernels for 3x3,1,[],https://github.com/torch/torch7/pull/949,https://github.com/Jokeren,4,https://github.com/torch/torch7/pull/949#issuecomment-283248795,,"@soumith , @pavanky , Just want to point out that MKL kernels depend on special data layouts so that we need to allocate a large buffer before convolution and also transform the output format after convolution. I am not saying it will affect the whole performance. But this is not convenient to combine MKL into torchnn, as MKL only defines some nn functions that utilize the special data layout without transformation. Besides, what about other processors that do not support MKL? I think torch should maintain its own convolution implementations.",True,{}
torch/torch7,https://github.com/torch/torch7,949,2017-03-01T00:11:10Z,2017-03-23T16:57:11Z,2017-03-23T16:57:11Z,CLOSED,False,860,228,9,https://github.com/pavanky,custom convolution kernels for 3x3,1,[],https://github.com/torch/torch7/pull/949,https://github.com/apaszke,5,https://github.com/torch/torch7/pull/949#issuecomment-283262770,,Wouldn't it be better to just integrate nnpack then?,True,{}
torch/torch7,https://github.com/torch/torch7,949,2017-03-01T00:11:10Z,2017-03-23T16:57:11Z,2017-03-23T16:57:11Z,CLOSED,False,860,228,9,https://github.com/pavanky,custom convolution kernels for 3x3,1,[],https://github.com/torch/torch7/pull/949,https://github.com/pavanky,6,https://github.com/torch/torch7/pull/949#issuecomment-288786931,,Closing this as this does not seem relevant anymore.,True,{}
torch/torch7,https://github.com/torch/torch7,952,2017-03-02T05:25:18Z,2017-03-02T13:57:28Z,2017-03-02T13:57:33Z,MERGED,True,11,4,1,https://github.com/shivak,C99 cleanup broke MSVC,2,[],https://github.com/torch/torch7/pull/952,https://github.com/shivak,1,https://github.com/torch/torch7/pull/952,"The recent refactoring of THTensorMath.c uses C99's _Pragma. This isn't supported by MSVC, which uses __pragma. I confirm this builds on MSVC.","The recent refactoring of THTensorMath.c uses C99's _Pragma. This isn't supported by MSVC, which uses __pragma. I confirm this builds on MSVC.",True,{}
torch/torch7,https://github.com/torch/torch7,952,2017-03-02T05:25:18Z,2017-03-02T13:57:28Z,2017-03-02T13:57:33Z,MERGED,True,11,4,1,https://github.com/shivak,C99 cleanup broke MSVC,2,[],https://github.com/torch/torch7/pull/952,https://github.com/soumith,2,https://github.com/torch/torch7/pull/952#issuecomment-283659749,"The recent refactoring of THTensorMath.c uses C99's _Pragma. This isn't supported by MSVC, which uses __pragma. I confirm this builds on MSVC.",thanks!,True,{}
torch/torch7,https://github.com/torch/torch7,958,2017-03-02T18:11:36Z,2017-03-02T18:37:26Z,2017-03-02T18:44:00Z,MERGED,True,1,1,1,https://github.com/ngimel,make mkl link to threaded version with GCC,1,[],https://github.com/torch/torch7/pull/958,https://github.com/ngimel,1,https://github.com/torch/torch7/pull/958,,,True,{'THUMBS_UP': ['https://github.com/colesbury']}
torch/torch7,https://github.com/torch/torch7,958,2017-03-02T18:11:36Z,2017-03-02T18:37:26Z,2017-03-02T18:44:00Z,MERGED,True,1,1,1,https://github.com/ngimel,make mkl link to threaded version with GCC,1,[],https://github.com/torch/torch7/pull/958,https://github.com/soumith,2,https://github.com/torch/torch7/pull/958#issuecomment-283739860,,thanks!,True,{}
torch/torch7,https://github.com/torch/torch7,959,2017-03-03T00:43:41Z,2017-03-03T14:05:07Z,2017-03-03T17:08:55Z,MERGED,True,41,16,2,https://github.com/pavanky,Fix dimension check for cat,2,[],https://github.com/torch/torch7/pull/959,https://github.com/pavanky,1,https://github.com/torch/torch7/pull/959,"Fixes  default dimension check from -2 to use TH_INDEX_BASE. I think master probably fails on PyTorch.(haven't checked).
Also added tests for cat when no dimension is specified (Similar to what is happening in torch/cutorch#719)
As done with cutorch, changed the variable name to make life easier when debugging this function.","Fixes  default dimension check from -2 to use TH_INDEX_BASE. I think master probably fails on PyTorch.(haven't checked).
Also added tests for cat when no dimension is specified (Similar to what is happening in torch/cutorch#719)
As done with cutorch, changed the variable name to make life easier when debugging this function.",True,{}
torch/torch7,https://github.com/torch/torch7,959,2017-03-03T00:43:41Z,2017-03-03T14:05:07Z,2017-03-03T17:08:55Z,MERGED,True,41,16,2,https://github.com/pavanky,Fix dimension check for cat,2,[],https://github.com/torch/torch7/pull/959,https://github.com/soumith,2,https://github.com/torch/torch7/pull/959#issuecomment-283961264,"Fixes  default dimension check from -2 to use TH_INDEX_BASE. I think master probably fails on PyTorch.(haven't checked).
Also added tests for cat when no dimension is specified (Similar to what is happening in torch/cutorch#719)
As done with cutorch, changed the variable name to make life easier when debugging this function.","thanks!
pytorch does not use the no-dimension case (it computes it's own dims and passes them as it supports negative dims), and hence does not fail.",True,{}
torch/torch7,https://github.com/torch/torch7,961,2017-03-03T20:27:42Z,2017-03-03T20:37:27Z,2017-03-03T20:37:31Z,MERGED,True,172,150,9,https://github.com/soumith,TH CMake cleanup + AVX/AVX2 only on appropriate files,3,[],https://github.com/torch/torch7/pull/961,https://github.com/soumith,1,https://github.com/torch/torch7/pull/961,"Cleans up TH's cmake for old and unnecessary flags
compile AVX and AVX2 Vector ops via separate files
Set AVX/AVX2 flags only on files which have the appropriate ops","Cleans up TH's cmake for old and unnecessary flags
compile AVX and AVX2 Vector ops via separate files
Set AVX/AVX2 flags only on files which have the appropriate ops",True,{}
torch/torch7,https://github.com/torch/torch7,963,2017-03-04T16:40:32Z,2017-03-04T16:50:12Z,2017-03-04T16:50:28Z,MERGED,True,256,187,3,https://github.com/soumith,Fix critical bug in SSE scalar add + improve tests,3,[],https://github.com/torch/torch7/pull/963,https://github.com/soumith,1,https://github.com/torch/torch7/pull/963,,,True,{}
torch/torch7,https://github.com/torch/torch7,964,2017-03-04T16:56:30Z,2017-03-04T16:56:51Z,2017-03-04T16:56:51Z,MERGED,True,16,14,3,https://github.com/soumith,reintroduce USE_AVX* for files which dont have -mavx* set,1,[],https://github.com/torch/torch7/pull/964,https://github.com/soumith,1,https://github.com/torch/torch7/pull/964,this is need to enable AVX codepaths,this is need to enable AVX codepaths,True,{}
torch/torch7,https://github.com/torch/torch7,980,2017-03-22T20:16:07Z,2017-03-22T20:49:45Z,2017-03-22T20:49:45Z,MERGED,True,18,5,2,https://github.com/adamlerer,Fix TH_TENSOR_APPLYX_D in the case where dim is inner loop,1,[],https://github.com/torch/torch7/pull/980,https://github.com/adamlerer,1,https://github.com/torch/torch7/pull/980,Fixes pytorch/pytorch#1036,Fixes pytorch/pytorch#1036,True,{}
torch/torch7,https://github.com/torch/torch7,981,2017-03-22T22:05:41Z,2017-04-03T22:02:15Z,2017-04-12T15:27:30Z,MERGED,True,86,61,6,https://github.com/elikosan,make it compile on Windows + use ilp64 MKL,40,[],https://github.com/torch/torch7/pull/981,https://github.com/elikosan,1,https://github.com/torch/torch7/pull/981,"fix vector/avx Windows compile (missing _dllexport + AVX not defined on Windows)
fix BLAS functions to operate on 64 bits __int64 (equiv. to long on linux) when compiling 64bit platform
fix FindSSE.cmake so it does not crash on Windows (unitialized variable)","fix vector/avx Windows compile (missing _dllexport + AVX not defined on Windows)
fix BLAS functions to operate on 64 bits __int64 (equiv. to long on linux) when compiling 64bit platform
fix FindSSE.cmake so it does not crash on Windows (unitialized variable)",True,{}
torch/torch7,https://github.com/torch/torch7,981,2017-03-22T22:05:41Z,2017-04-03T22:02:15Z,2017-04-12T15:27:30Z,MERGED,True,86,61,6,https://github.com/elikosan,make it compile on Windows + use ilp64 MKL,40,[],https://github.com/torch/torch7/pull/981,https://github.com/elikosan,2,https://github.com/torch/torch7/pull/981#issuecomment-291283232,"fix vector/avx Windows compile (missing _dllexport + AVX not defined on Windows)
fix BLAS functions to operate on 64 bits __int64 (equiv. to long on linux) when compiling 64bit platform
fix FindSSE.cmake so it does not crash on Windows (unitialized variable)","I agree with BTNC reviews. Pull request #991 fixes the issues that i was having, thus i reverted the changes to avx.h,avx.c,avx2.h,avx2.c.",True,{}
torch/torch7,https://github.com/torch/torch7,981,2017-03-22T22:05:41Z,2017-04-03T22:02:15Z,2017-04-12T15:27:30Z,MERGED,True,86,61,6,https://github.com/elikosan,make it compile on Windows + use ilp64 MKL,40,[],https://github.com/torch/torch7/pull/981,https://github.com/soumith,3,https://github.com/torch/torch7/pull/981#issuecomment-291287246,"fix vector/avx Windows compile (missing _dllexport + AVX not defined on Windows)
fix BLAS functions to operate on 64 bits __int64 (equiv. to long on linux) when compiling 64bit platform
fix FindSSE.cmake so it does not crash on Windows (unitialized variable)",thanks Eric!,True,{}
torch/torch7,https://github.com/torch/torch7,983,2017-03-24T16:02:40Z,2017-03-24T18:44:36Z,2017-03-24T18:44:36Z,MERGED,True,18,12,1,https://github.com/apaszke,Make rinfo_ argument optional in btrifact,1,[],https://github.com/torch/torch7/pull/983,https://github.com/apaszke,1,https://github.com/torch/torch7/pull/983,cc: @bamos,cc: @bamos,True,{}
torch/torch7,https://github.com/torch/torch7,985,2017-03-26T17:03:31Z,2017-03-26T17:41:33Z,2017-03-26T17:41:33Z,MERGED,True,2,2,2,https://github.com/bamos,Update btrisolve argument order.,1,[],https://github.com/torch/torch7/pull/985,https://github.com/bamos,1,https://github.com/torch/torch7/pull/985,,,True,{}
torch/torch7,https://github.com/torch/torch7,990,2017-03-31T16:24:56Z,2017-03-31T18:41:04Z,2017-04-05T11:21:44Z,MERGED,True,111,111,1,https://github.com/gchanan,"THVector_(add),(mul) -> (adds),(mul) for VSX.",1,[],https://github.com/torch/torch7/pull/990,https://github.com/gchanan,1,https://github.com/torch/torch7/pull/990,This was previously completed for other architectures.,This was previously completed for other architectures.,True,{}
torch/torch7,https://github.com/torch/torch7,990,2017-03-31T16:24:56Z,2017-03-31T18:41:04Z,2017-04-05T11:21:44Z,MERGED,True,111,111,1,https://github.com/gchanan,"THVector_(add),(mul) -> (adds),(mul) for VSX.",1,[],https://github.com/torch/torch7/pull/990,https://github.com/gchanan,2,https://github.com/torch/torch7/pull/990#issuecomment-290760937,This was previously completed for other architectures.,"This is to fix: pytorch/pytorch#922
Disclaimer: my MiniCloud instance seems to have crashed before the build successfully completed, but it made it past the VSX errors.  I'm trying now with a larger instance type...",True,{}
torch/torch7,https://github.com/torch/torch7,990,2017-03-31T16:24:56Z,2017-03-31T18:41:04Z,2017-04-05T11:21:44Z,MERGED,True,111,111,1,https://github.com/gchanan,"THVector_(add),(mul) -> (adds),(mul) for VSX.",1,[],https://github.com/torch/torch7/pull/990,https://github.com/gchanan,3,https://github.com/torch/torch7/pull/990#issuecomment-290781849,This was previously completed for other architectures.,update: I was able to successfully build torch7 on ppc64le with this.,True,{}
torch/torch7,https://github.com/torch/torch7,990,2017-03-31T16:24:56Z,2017-03-31T18:41:04Z,2017-04-05T11:21:44Z,MERGED,True,111,111,1,https://github.com/gchanan,"THVector_(add),(mul) -> (adds),(mul) for VSX.",1,[],https://github.com/torch/torch7/pull/990,https://github.com/gut,4,https://github.com/torch/torch7/pull/990#issuecomment-291230005,This was previously completed for other architectures.,"@gchanan : How did you build it on ppc64le? I tried on Ubuntu 16.10 and newest gcc-6 but I still have this issue:
~/torch-distro/pkg/torch/lib/TH/vector]$ /usr/bin/gcc-6  -DHAVE_MALLOC_USABLE_SIZE=1 -DHAVE_MMAP=1 -DHAVE_SHM_OPEN=1 -DHAVE_SHM_UNLINK=1 -DTH_EXPORTS -DUSE_GCC_ATOMICS=1 -D_FILE_OFFSET_BITS=64 -I/home/gut/torch-distro/pkg/torch/build/lib/TH  -Werror=implicit-function-declaration -Werror=format -fopenmp -std=gnu99 -fopenmp -DUSE_GCC_GET_CPUID -DTH_HAVE_THREAD -O3 -DNDEBUG -fPIC   -o CMakeFiles/TH.dir/THVector.c.o   -c /home/gut/torch-distro/pkg/torch/lib/TH/THVector.c
In file included from /home/gut/torch-distro/pkg/torch/lib/TH/THVector.c:10:0:
/home/gut/torch-distro/pkg/torch/lib/TH/vector/VSX.c: In function ‘THFloatVector_muls_VSX’:
/home/gut/torch-distro/pkg/torch/lib/TH/vector/VSX.c:983:1: error: unrecognizable insn:
 }
 ^
(insn 26 25 28 4 (set (reg:V4SF 392)
        (vec_select:V4SF (vec_select:V4SF (mem:V4SF (reg/f:DI 393 [ _15 ]) [0  S16 A8])
                (parallel [
                        (const_int 3 [0x3])
                        (const_int 2 [0x2])
                        (const_int 1 [0x1])
                        (const_int 0 [0])
                    ]))
            (parallel:V4SF [
                    (const_int 2 [0x2])
                    (const_int 3 [0x3])
                    (const_int 0 [0])
                    (const_int 1 [0x1])
                ]))) /home/gut/torch-distro/pkg/torch/lib/TH/vector/VSX.c:900 -1
     (expr_list:REG_DEAD (reg/f:DI 393 [ _15 ])
        (expr_list:REG_EQUAL (vec_select:V4SF (vec_select:V4SF (mem:V4SF (reg:DI 342 [ ivtmp.3194 ]) [0  S16 A8])
                    (parallel [
                            (const_int 3 [0x3])
                            (const_int 2 [0x2])
                            (const_int 1 [0x1])
                            (const_int 0 [0])
                        ]))
                (parallel:V4SF [
                        (const_int 2 [0x2])
                        (const_int 3 [0x3])
                        (const_int 0 [0])
                        (const_int 1 [0x1])
                    ]))
            (nil))))
/home/gut/torch-distro/pkg/torch/lib/TH/vector/VSX.c:983:1: internal compiler error: in extract_insn, at recog.c:2287
Please submit a full bug report,
with preprocessed source if appropriate.
See <file:///usr/share/doc/gcc-6/README.Bugs> for instructions.

I upgraded torch myself on torch-distro and VSX.c is lastly modified by your 00fd283 change",True,{}
torch/torch7,https://github.com/torch/torch7,990,2017-03-31T16:24:56Z,2017-03-31T18:41:04Z,2017-04-05T11:21:44Z,MERGED,True,111,111,1,https://github.com/gchanan,"THVector_(add),(mul) -> (adds),(mul) for VSX.",1,[],https://github.com/torch/torch7/pull/990,https://github.com/soumith,5,https://github.com/torch/torch7/pull/990#issuecomment-291251891,This was previously completed for other architectures.,"@gut what you see seems like an actual compiler bug in gcc 6.xx.
We compiled with gcc 4.8.3 and it compiled fine.",True,{}
torch/torch7,https://github.com/torch/torch7,990,2017-03-31T16:24:56Z,2017-03-31T18:41:04Z,2017-04-05T11:21:44Z,MERGED,True,111,111,1,https://github.com/gchanan,"THVector_(add),(mul) -> (adds),(mul) for VSX.",1,[],https://github.com/torch/torch7/pull/990,https://github.com/gut,6,https://github.com/torch/torch7/pull/990#issuecomment-291255545,This was previously completed for other architectures.,checked! gcc-4.8.5-4ubuntu4 works! Thanks.,True,{}
torch/torch7,https://github.com/torch/torch7,990,2017-03-31T16:24:56Z,2017-03-31T18:41:04Z,2017-04-05T11:21:44Z,MERGED,True,111,111,1,https://github.com/gchanan,"THVector_(add),(mul) -> (adds),(mul) for VSX.",1,[],https://github.com/torch/torch7/pull/990,https://github.com/tomsercu,7,https://github.com/torch/torch7/pull/990#issuecomment-291257312,This was previously completed for other architectures.,"@gut will you update in the power distro?
Also on the compiled version do you see the segfaults described in
pytorch/pytorch#922 ?",True,{}
torch/torch7,https://github.com/torch/torch7,990,2017-03-31T16:24:56Z,2017-03-31T18:41:04Z,2017-04-05T11:21:44Z,MERGED,True,111,111,1,https://github.com/gchanan,"THVector_(add),(mul) -> (adds),(mul) for VSX.",1,[],https://github.com/torch/torch7/pull/990,https://github.com/gut,8,https://github.com/torch/torch7/pull/990#issuecomment-291262906,This was previously completed for other architectures.,"@tomsercu I just did on https://github.com/PPC64/torch-distro
I'll check the segfault and comment on that issue, if I face any.",True,{}
torch/torch7,https://github.com/torch/torch7,990,2017-03-31T16:24:56Z,2017-03-31T18:41:04Z,2017-04-05T11:21:44Z,MERGED,True,111,111,1,https://github.com/gchanan,"THVector_(add),(mul) -> (adds),(mul) for VSX.",1,[],https://github.com/torch/torch7/pull/990,https://github.com/tomsercu,9,https://github.com/torch/torch7/pull/990#issuecomment-291688950,This was previously completed for other architectures.,"@gut did you confirm torch.test() nn.test() etc pass? What kind of system are you on?
I'm still seeing segfaults with the current PPC64/distro (82fb7ea), actually segfaults at the same point as before this PR was merged.
I'm on a rhel7.2 p8 machine with the default /usr/bin/gcc version 4.8.5.
Do you see something similar?
dccpc278[/speech7/multimodal/torch/20170404-ppc64le]$ gdb --args bash $(which th) -e ""torch.test()""
GNU gdb (GDB) Red Hat Enterprise Linux 7.6.1-80.el7
Copyright (C) 2013 Free Software Foundation, Inc.
License GPLv3+: GNU GPL version 3 or later <http://gnu.org/licenses/gpl.html>
This is free software: you are free to change and redistribute it.
There is NO WARRANTY, to the extent permitted by law.  Type ""show copying""
and ""show warranty"" for details.
This GDB was configured as ""ppc64le-redhat-linux-gnu"".
For bug reporting instructions, please see:
<http://www.gnu.org/software/gdb/bugs/>...
Reading symbols from /usr/bin/bash...Reading symbols from /usr/bin/bash...(no debugging symbols found)...done.
(no debugging symbols found)...done.
Missing separate debuginfos, use: debuginfo-install bash-4.2.46-19.el7.ppc64le
(gdb) run
Starting program: /usr/bin/bash /speech7/multimodal/torch/20170404-ppc64le/install/bin/th -e torch.test\(\)
process 48000 is executing new program: /speech7/multimodal/torch/20170404-ppc64le/install/bin/luajit
[Thread debugging using libthread_db enabled]
Using host libthread_db library ""/lib64/power8/libthread_db.so.1"".
Detaching after fork from child process 48007.
Detaching after fork from child process 48014.
Detaching after fork from child process 48015.
Running 163 tests
  1/163 ceil ............................................................ [PASS]
  2/163 tan ............................................................. [PASS]
  3/163 baddbmm ......................................................... [WAIT]
Program received signal SIGSEGV, Segmentation fault.
0x0000100000bb42b4 in THDoubleVector_muls_VSX () from /speech7/multimodal/torch/20170404-ppc64le/install/lib/libTH.so.0",True,{}
torch/torch7,https://github.com/torch/torch7,990,2017-03-31T16:24:56Z,2017-03-31T18:41:04Z,2017-04-05T11:21:44Z,MERGED,True,111,111,1,https://github.com/gchanan,"THVector_(add),(mul) -> (adds),(mul) for VSX.",1,[],https://github.com/torch/torch7/pull/990,https://github.com/gut,10,https://github.com/torch/torch7/pull/990#issuecomment-291831232,This was previously completed for other architectures.,"No, they segfault as I posted on issue #922. The update on torch-distro focused on having the package built.
We're investigating it on #922, please take a closer look",True,{}
torch/torch7,https://github.com/torch/torch7,991,2017-04-02T13:03:24Z,2017-04-02T17:32:57Z,2017-04-02T17:33:02Z,MERGED,True,4,4,1,https://github.com/BTNC,add /arch:AVX /arch:AVX2 explicitly for msvc so it compiles on windows,1,[],https://github.com/torch/torch7/pull/991,https://github.com/BTNC,1,https://github.com/torch/torch7/pull/991,"Hi,
The AVX/AVX2 test for msvc can pass without /arch:AVX or /arch:AVX2 compiler options. And FindSSE.cmake will test with no flags at first, so the empty flag is returned. However the __AVX__ and __AVX2__ macro will only be defined automatically when /arch:AVX and /arch:AVX2 options are specified.
I am not sure about gcc or clang's behavior, so I did not change FindSSE.cmake directly. Instead I added those must have options directly in SET_SOURCE_FILES_PROPERTIES.
Thanks,","Hi,
The AVX/AVX2 test for msvc can pass without /arch:AVX or /arch:AVX2 compiler options. And FindSSE.cmake will test with no flags at first, so the empty flag is returned. However the __AVX__ and __AVX2__ macro will only be defined automatically when /arch:AVX and /arch:AVX2 options are specified.
I am not sure about gcc or clang's behavior, so I did not change FindSSE.cmake directly. Instead I added those must have options directly in SET_SOURCE_FILES_PROPERTIES.
Thanks,",True,{}
torch/torch7,https://github.com/torch/torch7,991,2017-04-02T13:03:24Z,2017-04-02T17:32:57Z,2017-04-02T17:33:02Z,MERGED,True,4,4,1,https://github.com/BTNC,add /arch:AVX /arch:AVX2 explicitly for msvc so it compiles on windows,1,[],https://github.com/torch/torch7/pull/991,https://github.com/soumith,2,https://github.com/torch/torch7/pull/991#issuecomment-291001192,"Hi,
The AVX/AVX2 test for msvc can pass without /arch:AVX or /arch:AVX2 compiler options. And FindSSE.cmake will test with no flags at first, so the empty flag is returned. However the __AVX__ and __AVX2__ macro will only be defined automatically when /arch:AVX and /arch:AVX2 options are specified.
I am not sure about gcc or clang's behavior, so I did not change FindSSE.cmake directly. Instead I added those must have options directly in SET_SOURCE_FILES_PROPERTIES.
Thanks,",thanks!,True,{}
torch/torch7,https://github.com/torch/torch7,992,2017-04-02T23:16:38Z,2017-04-05T01:51:12Z,2017-04-05T01:51:12Z,MERGED,True,1,1,1,https://github.com/JacksonGL,"E.g. = for example. ""For e.g.,"" is a typo",1,[],https://github.com/torch/torch7/pull/992,https://github.com/JacksonGL,1,https://github.com/torch/torch7/pull/992,,,True,{}
torch/torch7,https://github.com/torch/torch7,1000,2017-04-10T13:50:40Z,2017-04-10T15:03:22Z,2017-04-10T15:19:01Z,MERGED,True,2,2,1,https://github.com/willfrey,Update CmdLine.lua,1,[],https://github.com/torch/torch7/pull/1000,https://github.com/willfrey,1,https://github.com/torch/torch7/pull/1000,"Very small cosmetic change to make positional arguments look better.
Previously, positional arguments would look like:
$ th my_script.lua --help
Usage: [options] <arg1><arg2>
-opt1 blah blah [default]

and now it should look like
$ th my_script.lua --help
Usage: [options] <arg1> <arg2>
-opt1 blah blah [default]

Very minorly cosmetic.","Very small cosmetic change to make positional arguments look better.
Previously, positional arguments would look like:
$ th my_script.lua --help
Usage: [options] <arg1><arg2>
-opt1 blah blah [default]

and now it should look like
$ th my_script.lua --help
Usage: [options] <arg1> <arg2>
-opt1 blah blah [default]

Very minorly cosmetic.",True,{}
torch/torch7,https://github.com/torch/torch7,1000,2017-04-10T13:50:40Z,2017-04-10T15:03:22Z,2017-04-10T15:19:01Z,MERGED,True,2,2,1,https://github.com/willfrey,Update CmdLine.lua,1,[],https://github.com/torch/torch7/pull/1000,https://github.com/soumith,2,https://github.com/torch/torch7/pull/1000#issuecomment-292977269,"Very small cosmetic change to make positional arguments look better.
Previously, positional arguments would look like:
$ th my_script.lua --help
Usage: [options] <arg1><arg2>
-opt1 blah blah [default]

and now it should look like
$ th my_script.lua --help
Usage: [options] <arg1> <arg2>
-opt1 blah blah [default]

Very minorly cosmetic.",thanks will!,True,{}
torch/torch7,https://github.com/torch/torch7,1002,2017-04-12T19:06:46Z,2017-04-12T19:07:16Z,2017-04-12T19:07:16Z,MERGED,True,57,82,5,https://github.com/soumith,"Revert ""make it compile on Windows + use ilp64 MKL""",9,[],https://github.com/torch/torch7/pull/1002,https://github.com/soumith,1,https://github.com/torch/torch7/pull/1002,"Reverts #981
This PR was specific to MKL, and kind of accidentally worked in other cases. I'm reverting it.","Reverts #981
This PR was specific to MKL, and kind of accidentally worked in other cases. I'm reverting it.",True,{}
torch/torch7,https://github.com/torch/torch7,1010,2017-04-21T09:08:46Z,2017-04-21T14:04:38Z,2017-04-21T14:04:38Z,MERGED,True,7,7,1,https://github.com/LambdaWill,Fix typo in tensor.md,1,[],https://github.com/torch/torch7/pull/1010,https://github.com/LambdaWill,1,https://github.com/torch/torch7/pull/1010,,,True,{}
torch/torch7,https://github.com/torch/torch7,1011,2017-04-21T18:29:25Z,2017-04-24T18:15:16Z,2017-04-24T18:59:04Z,CLOSED,False,56,1,3,https://github.com/killeent,TH implementation of Tensor expansion,5,[],https://github.com/torch/torch7/pull/1011,https://github.com/killeent,1,https://github.com/torch/torch7/pull/1011,"@soumith not sure if you wanted me to migrate the Lua-level expand to this too, or if this should just be wrapped from PyTorch.","@soumith not sure if you wanted me to migrate the Lua-level expand to this too, or if this should just be wrapped from PyTorch.",True,{}
torch/torch7,https://github.com/torch/torch7,1011,2017-04-21T18:29:25Z,2017-04-24T18:15:16Z,2017-04-24T18:59:04Z,CLOSED,False,56,1,3,https://github.com/killeent,TH implementation of Tensor expansion,5,[],https://github.com/torch/torch7/pull/1011,https://github.com/killeent,2,https://github.com/torch/torch7/pull/1011#issuecomment-296302429,"@soumith not sure if you wanted me to migrate the Lua-level expand to this too, or if this should just be wrapped from PyTorch.","@gchanan I went with the first one, in any event they will probably look to the docs for further clarification.",True,{}
torch/torch7,https://github.com/torch/torch7,1011,2017-04-21T18:29:25Z,2017-04-24T18:15:16Z,2017-04-24T18:59:04Z,CLOSED,False,56,1,3,https://github.com/killeent,TH implementation of Tensor expansion,5,[],https://github.com/torch/torch7/pull/1011,https://github.com/gchanan,3,https://github.com/torch/torch7/pull/1011#issuecomment-296303498,"@soumith not sure if you wanted me to migrate the Lua-level expand to this too, or if this should just be wrapped from PyTorch.",thanks!  BTW -- is there any reason expandNd is not declared in generic/THTensor.h?,True,{}
torch/torch7,https://github.com/torch/torch7,1011,2017-04-21T18:29:25Z,2017-04-24T18:15:16Z,2017-04-24T18:59:04Z,CLOSED,False,56,1,3,https://github.com/killeent,TH implementation of Tensor expansion,5,[],https://github.com/torch/torch7/pull/1011,https://github.com/killeent,4,https://github.com/torch/torch7/pull/1011#issuecomment-296304138,"@soumith not sure if you wanted me to migrate the Lua-level expand to this too, or if this should just be wrapped from PyTorch.","Its helper code, while expand/expandAs are top-level API calls we will wrap. I'm not sure (however) with how the code is generated, whether or not the header will be needed for it to work.",True,{}
torch/torch7,https://github.com/torch/torch7,1011,2017-04-21T18:29:25Z,2017-04-24T18:15:16Z,2017-04-24T18:59:04Z,CLOSED,False,56,1,3,https://github.com/killeent,TH implementation of Tensor expansion,5,[],https://github.com/torch/torch7/pull/1011,https://github.com/killeent,5,https://github.com/torch/torch7/pull/1011#issuecomment-296777209,"@soumith not sure if you wanted me to migrate the Lua-level expand to this too, or if this should just be wrapped from PyTorch.",I'll just do this in pytorch,True,{}
torch/torch7,https://github.com/torch/torch7,1014,2017-04-26T18:04:16Z,2017-04-26T23:11:57Z,2017-04-26T23:11:57Z,MERGED,True,206,122,12,https://github.com/apaszke,Add THGenerate*Type.h for all types,1,[],https://github.com/torch/torch7/pull/1014,https://github.com/apaszke,1,https://github.com/torch/torch7/pull/1014,This allows to use the generated/ trick from THC and speed up builds of large files.,This allows to use the generated/ trick from THC and speed up builds of large files.,True,{}
torch/torch7,https://github.com/torch/torch7,1016,2017-04-28T22:26:48Z,2017-04-28T22:33:08Z,2017-04-28T22:33:12Z,MERGED,True,23,10,4,https://github.com/nicholas-leonard,fix cdiv/div unit tests; fix Mac OS X require ffi bug,1,[],https://github.com/torch/torch7/pull/1016,https://github.com/nicholas-leonard,1,https://github.com/torch/torch7/pull/1016,"fixes cdiv and div unit tests.
fixes issue on Mac OS X where cd torch7 && th -e ""torch.[anything]"" would have FFI.lua require itself (because the filesystem isn't case sensitive...)
so basically, within torch directory, require 'ffi' in FFI.lua requires itself because to Mac OS X, the FFI.lua and ffi.lua files are equivalent. And require looks in the local dir first.","fixes cdiv and div unit tests.
fixes issue on Mac OS X where cd torch7 && th -e ""torch.[anything]"" would have FFI.lua require itself (because the filesystem isn't case sensitive...)
so basically, within torch directory, require 'ffi' in FFI.lua requires itself because to Mac OS X, the FFI.lua and ffi.lua files are equivalent. And require looks in the local dir first.",True,{}
torch/torch7,https://github.com/torch/torch7,1016,2017-04-28T22:26:48Z,2017-04-28T22:33:08Z,2017-04-28T22:33:12Z,MERGED,True,23,10,4,https://github.com/nicholas-leonard,fix cdiv/div unit tests; fix Mac OS X require ffi bug,1,[],https://github.com/torch/torch7/pull/1016,https://github.com/soumith,2,https://github.com/torch/torch7/pull/1016#issuecomment-298121647,"fixes cdiv and div unit tests.
fixes issue on Mac OS X where cd torch7 && th -e ""torch.[anything]"" would have FFI.lua require itself (because the filesystem isn't case sensitive...)
so basically, within torch directory, require 'ffi' in FFI.lua requires itself because to Mac OS X, the FFI.lua and ffi.lua files are equivalent. And require looks in the local dir first.",thanks!,True,{}
torch/torch7,https://github.com/torch/torch7,1023,2017-05-08T16:20:57Z,2017-05-12T01:57:35Z,2017-05-12T14:21:29Z,MERGED,True,5,5,2,https://github.com/elikosan,fix uninitialized variable in cmake FindSSE,44,[],https://github.com/torch/torch7/pull/1023,https://github.com/elikosan,1,https://github.com/torch/torch7/pull/1023,Leaving the variable uninitialized causes the program to crash on Windows,Leaving the variable uninitialized causes the program to crash on Windows,True,{}
torch/torch7,https://github.com/torch/torch7,1023,2017-05-08T16:20:57Z,2017-05-12T01:57:35Z,2017-05-12T14:21:29Z,MERGED,True,5,5,2,https://github.com/elikosan,fix uninitialized variable in cmake FindSSE,44,[],https://github.com/torch/torch7/pull/1023,https://github.com/nicholas-leonard,2,https://github.com/torch/torch7/pull/1023#issuecomment-300913761,Leaving the variable uninitialized causes the program to crash on Windows,@elikosan Can you rebase to one commit?,True,{}
torch/torch7,https://github.com/torch/torch7,1023,2017-05-08T16:20:57Z,2017-05-12T01:57:35Z,2017-05-12T14:21:29Z,MERGED,True,5,5,2,https://github.com/elikosan,fix uninitialized variable in cmake FindSSE,44,[],https://github.com/torch/torch7/pull/1023,https://github.com/elikosan,3,https://github.com/torch/torch7/pull/1023#issuecomment-300918134,Leaving the variable uninitialized causes the program to crash on Windows,"Sorry for the newbie question... is this the right resource to understand how to use rebase in this context ? https://github.com/edx/edx-platform/wiki/How-to-Rebase-a-Pull-Request
Thanks",True,{}
torch/torch7,https://github.com/torch/torch7,1023,2017-05-08T16:20:57Z,2017-05-12T01:57:35Z,2017-05-12T14:21:29Z,MERGED,True,5,5,2,https://github.com/elikosan,fix uninitialized variable in cmake FindSSE,44,[],https://github.com/torch/torch7/pull/1023,https://github.com/soumith,4,https://github.com/torch/torch7/pull/1023#issuecomment-300963384,Leaving the variable uninitialized causes the program to crash on Windows,thanks Eric!,True,{}
torch/torch7,https://github.com/torch/torch7,1023,2017-05-08T16:20:57Z,2017-05-12T01:57:35Z,2017-05-12T14:21:29Z,MERGED,True,5,5,2,https://github.com/elikosan,fix uninitialized variable in cmake FindSSE,44,[],https://github.com/torch/torch7/pull/1023,https://github.com/nicholas-leonard,5,https://github.com/torch/torch7/pull/1023#issuecomment-301090131,Leaving the variable uninitialized causes the program to crash on Windows,"@elikosan I like to use :
git checkout master
git pull upstream master
git checkout mybranch
git rebase -i master
[s: squash all commits but the first]
[make a nice commit message]
git push -f origin mybranch",True,{}
torch/torch7,https://github.com/torch/torch7,1025,2017-05-09T21:36:39Z,2017-05-09T21:44:36Z,2017-05-09T21:44:36Z,MERGED,True,18,9,1,https://github.com/gchanan,Add keepdim to lua cwrap.,1,[],https://github.com/torch/torch7/pull/1025,https://github.com/gchanan,1,https://github.com/torch/torch7/pull/1025,,,True,{}
torch/torch7,https://github.com/torch/torch7,1027,2017-05-11T16:10:20Z,2017-05-11T16:14:47Z,2017-05-11T17:03:05Z,CLOSED,False,6,2,2,https://github.com/nicholas-leonard,fast sigmoid,1,[],https://github.com/torch/torch7/pull/1027,https://github.com/nicholas-leonard,1,https://github.com/torch/torch7/pull/1027,"This PR provides a faster implement of torch.Tensor.sigmoid. As outlined in  #1026, the rationale for this PR is that TensorFlow has a 1.77x speedup over Torch LSTMs on CPU using Float tensors. This PR brings us to par with TF LSTMs.
The downside is that torch.Tensor.sigmoid will lose some precision. The rough approximation introduces about 0.2% error. The speedup seems to warrant the approximation because the throughput for a typical LSTM goes from 455 samples/sec to 910 samples/sec, which compares well to the 809 samples/sec for TF. For a smaller LSTM those numbers are respectively, 3087 to 3900 for Torch vs  4139 for TF.
An alternative could be to implement a separate THTensor_(fastsigmoid) for this rough approximation.","This PR provides a faster implement of torch.Tensor.sigmoid. As outlined in  #1026, the rationale for this PR is that TensorFlow has a 1.77x speedup over Torch LSTMs on CPU using Float tensors. This PR brings us to par with TF LSTMs.
The downside is that torch.Tensor.sigmoid will lose some precision. The rough approximation introduces about 0.2% error. The speedup seems to warrant the approximation because the throughput for a typical LSTM goes from 455 samples/sec to 910 samples/sec, which compares well to the 809 samples/sec for TF. For a smaller LSTM those numbers are respectively, 3087 to 3900 for Torch vs  4139 for TF.
An alternative could be to implement a separate THTensor_(fastsigmoid) for this rough approximation.",True,{}
torch/torch7,https://github.com/torch/torch7,1027,2017-05-11T16:10:20Z,2017-05-11T16:14:47Z,2017-05-11T17:03:05Z,CLOSED,False,6,2,2,https://github.com/nicholas-leonard,fast sigmoid,1,[],https://github.com/torch/torch7/pull/1027,https://github.com/soumith,2,https://github.com/torch/torch7/pull/1027#issuecomment-300839986,"This PR provides a faster implement of torch.Tensor.sigmoid. As outlined in  #1026, the rationale for this PR is that TensorFlow has a 1.77x speedup over Torch LSTMs on CPU using Float tensors. This PR brings us to par with TF LSTMs.
The downside is that torch.Tensor.sigmoid will lose some precision. The rough approximation introduces about 0.2% error. The speedup seems to warrant the approximation because the throughput for a typical LSTM goes from 455 samples/sec to 910 samples/sec, which compares well to the 809 samples/sec for TF. For a smaller LSTM those numbers are respectively, 3087 to 3900 for Torch vs  4139 for TF.
An alternative could be to implement a separate THTensor_(fastsigmoid) for this rough approximation.","we used to have fast exponential approximations in TH for various things, and we phased them all out because of very subtle training issues for various use-cases. So I am going to reject this PR for the same reason.
However, I think @pavanky the wizard can easily speed this up without loss of precision.",True,{}
torch/torch7,https://github.com/torch/torch7,1027,2017-05-11T16:10:20Z,2017-05-11T16:14:47Z,2017-05-11T17:03:05Z,CLOSED,False,6,2,2,https://github.com/nicholas-leonard,fast sigmoid,1,[],https://github.com/torch/torch7/pull/1027,https://github.com/pavanky,3,https://github.com/torch/torch7/pull/1027#issuecomment-300845971,"This PR provides a faster implement of torch.Tensor.sigmoid. As outlined in  #1026, the rationale for this PR is that TensorFlow has a 1.77x speedup over Torch LSTMs on CPU using Float tensors. This PR brings us to par with TF LSTMs.
The downside is that torch.Tensor.sigmoid will lose some precision. The rough approximation introduces about 0.2% error. The speedup seems to warrant the approximation because the throughput for a typical LSTM goes from 455 samples/sec to 910 samples/sec, which compares well to the 809 samples/sec for TF. For a smaller LSTM those numbers are respectively, 3087 to 3900 for Torch vs  4139 for TF.
An alternative could be to implement a separate THTensor_(fastsigmoid) for this rough approximation.",@soumith The easiest thing to try is to try and incorporate the float versions of the math functions instead of defaulting to double versions for everything. This does still lose a bit of accuracy to existing versions but it should have the expected precision for floating point numbers.,True,{}
torch/torch7,https://github.com/torch/torch7,1027,2017-05-11T16:10:20Z,2017-05-11T16:14:47Z,2017-05-11T17:03:05Z,CLOSED,False,6,2,2,https://github.com/nicholas-leonard,fast sigmoid,1,[],https://github.com/torch/torch7/pull/1027,https://github.com/soumith,4,https://github.com/torch/torch7/pull/1027#issuecomment-300847878,"This PR provides a faster implement of torch.Tensor.sigmoid. As outlined in  #1026, the rationale for this PR is that TensorFlow has a 1.77x speedup over Torch LSTMs on CPU using Float tensors. This PR brings us to par with TF LSTMs.
The downside is that torch.Tensor.sigmoid will lose some precision. The rough approximation introduces about 0.2% error. The speedup seems to warrant the approximation because the throughput for a typical LSTM goes from 455 samples/sec to 910 samples/sec, which compares well to the 809 samples/sec for TF. For a smaller LSTM those numbers are respectively, 3087 to 3900 for Torch vs  4139 for TF.
An alternative could be to implement a separate THTensor_(fastsigmoid) for this rough approximation.",that does seem reasonable,True,{}
torch/torch7,https://github.com/torch/torch7,1027,2017-05-11T16:10:20Z,2017-05-11T16:14:47Z,2017-05-11T17:03:05Z,CLOSED,False,6,2,2,https://github.com/nicholas-leonard,fast sigmoid,1,[],https://github.com/torch/torch7/pull/1027,https://github.com/nicholas-leonard,5,https://github.com/torch/torch7/pull/1027#issuecomment-300853220,"This PR provides a faster implement of torch.Tensor.sigmoid. As outlined in  #1026, the rationale for this PR is that TensorFlow has a 1.77x speedup over Torch LSTMs on CPU using Float tensors. This PR brings us to par with TF LSTMs.
The downside is that torch.Tensor.sigmoid will lose some precision. The rough approximation introduces about 0.2% error. The speedup seems to warrant the approximation because the throughput for a typical LSTM goes from 455 samples/sec to 910 samples/sec, which compares well to the 809 samples/sec for TF. For a smaller LSTM those numbers are respectively, 3087 to 3900 for Torch vs  4139 for TF.
An alternative could be to implement a separate THTensor_(fastsigmoid) for this rough approximation.",@soumith Wizard @pavanky is sending a PR with TH_sigmoidf that gives about the same speedup. Wait out.,True,{}
torch/torch7,https://github.com/torch/torch7,1028,2017-05-11T17:41:43Z,2017-05-11T17:50:39Z,2017-05-11T22:00:33Z,MERGED,True,66,40,2,https://github.com/pavanky,Ensuring float tensors call float versions of math functions,1,[],https://github.com/torch/torch7/pull/1028,https://github.com/pavanky,1,https://github.com/torch/torch7/pull/1028,Fixes #1026,Fixes #1026,True,"{'HOORAY': ['https://github.com/nicholas-leonard', 'https://github.com/fmassa', 'https://github.com/apaszke']}"
torch/torch7,https://github.com/torch/torch7,1028,2017-05-11T17:41:43Z,2017-05-11T17:50:39Z,2017-05-11T22:00:33Z,MERGED,True,66,40,2,https://github.com/pavanky,Ensuring float tensors call float versions of math functions,1,[],https://github.com/torch/torch7/pull/1028,https://github.com/nicholas-leonard,2,https://github.com/torch/torch7/pull/1028#issuecomment-300866706,Fixes #1026,This brings our Float-CPU LSTM on par with TensorFlow. Thanks @pavanky !,True,{}
torch/torch7,https://github.com/torch/torch7,1040,2017-05-26T20:24:07Z,,2018-01-02T20:18:19Z,OPEN,False,392,230,10,https://github.com/elikosan,allow torch to link with Intel MKL _ilp64 model (large integers),79,[],https://github.com/torch/torch7/pull/1040,https://github.com/elikosan,1,https://github.com/torch/torch7/pull/1040,"Sorry for the large pull request.
I improved upon the reverted pull request #981. Thanks for the suggestions.
The goal is to allow compilation and link with Intel MKL _ILP64 (64b integers) model. This is necessary when Torch is linked with another application that itself is linked to _ILP64, otherwise the linker on linux links to the incorrect version resulting in a MKL runtime error.
The default build will not change will not be impacted. To trigger linking to _ILP64, the environment variable MKL_ILP64 has to be defined prior to building.
The INTEL_MKL_DIR environment variable now can be set so that Intel MKL can be found by the build.
I tested this patch on both linux and Windows, both with and without MKL_ILP64 defined.
Again, there should be no impact when MKL_ILP64 is not defined.","Sorry for the large pull request.
I improved upon the reverted pull request #981. Thanks for the suggestions.
The goal is to allow compilation and link with Intel MKL _ILP64 (64b integers) model. This is necessary when Torch is linked with another application that itself is linked to _ILP64, otherwise the linker on linux links to the incorrect version resulting in a MKL runtime error.
The default build will not change will not be impacted. To trigger linking to _ILP64, the environment variable MKL_ILP64 has to be defined prior to building.
The INTEL_MKL_DIR environment variable now can be set so that Intel MKL can be found by the build.
I tested this patch on both linux and Windows, both with and without MKL_ILP64 defined.
Again, there should be no impact when MKL_ILP64 is not defined.",True,{}
torch/torch7,https://github.com/torch/torch7,1040,2017-05-26T20:24:07Z,,2018-01-02T20:18:19Z,OPEN,False,392,230,10,https://github.com/elikosan,allow torch to link with Intel MKL _ilp64 model (large integers),79,[],https://github.com/torch/torch7/pull/1040,https://github.com/elikosan,2,https://github.com/torch/torch7/pull/1040#issuecomment-304380553,"Sorry for the large pull request.
I improved upon the reverted pull request #981. Thanks for the suggestions.
The goal is to allow compilation and link with Intel MKL _ILP64 (64b integers) model. This is necessary when Torch is linked with another application that itself is linked to _ILP64, otherwise the linker on linux links to the incorrect version resulting in a MKL runtime error.
The default build will not change will not be impacted. To trigger linking to _ILP64, the environment variable MKL_ILP64 has to be defined prior to building.
The INTEL_MKL_DIR environment variable now can be set so that Intel MKL can be found by the build.
I tested this patch on both linux and Windows, both with and without MKL_ILP64 defined.
Again, there should be no impact when MKL_ILP64 is not defined.","Also, apologies for the un-squashed commits. I yet have to learn the intricacies of Git (I'm a SVN user).",True,{}
torch/torch7,https://github.com/torch/torch7,1046,2017-06-14T21:26:38Z,2017-07-11T17:22:36Z,2017-07-11T17:22:44Z,MERGED,True,269,10,7,https://github.com/amartya18x,Implementation of Alias Multinomial for faster Multinomial sampling,4,[],https://github.com/torch/torch7/pull/1046,https://github.com/amartya18x,1,https://github.com/torch/torch7/pull/1046,"This allows for faster sampling from a multinomial distribution using the Alias method which is described in this paper.
It allows almost 5x speed in sampling than torch.multinomial. To see the difference on your machine run  th test/test_aliasMultinomial.lua","This allows for faster sampling from a multinomial distribution using the Alias method which is described in this paper.
It allows almost 5x speed in sampling than torch.multinomial. To see the difference on your machine run  th test/test_aliasMultinomial.lua",True,{'THUMBS_UP': ['https://github.com/nicholas-leonard']}
torch/torch7,https://github.com/torch/torch7,1046,2017-06-14T21:26:38Z,2017-07-11T17:22:36Z,2017-07-11T17:22:44Z,MERGED,True,269,10,7,https://github.com/amartya18x,Implementation of Alias Multinomial for faster Multinomial sampling,4,[],https://github.com/torch/torch7/pull/1046,https://github.com/amartya18x,2,https://github.com/torch/torch7/pull/1046#issuecomment-308627842,"This allows for faster sampling from a multinomial distribution using the Alias method which is described in this paper.
It allows almost 5x speed in sampling than torch.multinomial. To see the difference on your machine run  th test/test_aliasMultinomial.lua","@soumith @pavanky This doesn't include any class but includes two function calls.
th> probs = torch.DoubleTensor({0.2, 0.2, 0.3, 0.3})
                                                                      [0.0001s]
                                                                      
th> prob_tbl, alias_tbl = torch.multinomialAliasSetup(probs)
                                                                      [0.0002s]
                                                                      
th> output = torch.LongTensor(2,3)
                                                                      [0.0001s]
                                                                      

th> alias_tbl.multinomialAliasDraw(output:view(-1), prob_tbl, alias_tbl)
 3
 4
 3
 1
 2
 3
[torch.LongTensor of size 6]

                                                                      [0.0003s]
                                                                      
th> output
 3  4  3
 1  2  3
[torch.LongTensor of size 2x3]",True,{}
torch/torch7,https://github.com/torch/torch7,1046,2017-06-14T21:26:38Z,2017-07-11T17:22:36Z,2017-07-11T17:22:44Z,MERGED,True,269,10,7,https://github.com/amartya18x,Implementation of Alias Multinomial for faster Multinomial sampling,4,[],https://github.com/torch/torch7/pull/1046,https://github.com/amartya18x,3,https://github.com/torch/torch7/pull/1046#issuecomment-308627950,"This allows for faster sampling from a multinomial distribution using the Alias method which is described in this paper.
It allows almost 5x speed in sampling than torch.multinomial. To see the difference on your machine run  th test/test_aliasMultinomial.lua",@apaszke Thanks for suggesting those. It made the functions pretty fast. :),True,{}
torch/torch7,https://github.com/torch/torch7,1046,2017-06-14T21:26:38Z,2017-07-11T17:22:36Z,2017-07-11T17:22:44Z,MERGED,True,269,10,7,https://github.com/amartya18x,Implementation of Alias Multinomial for faster Multinomial sampling,4,[],https://github.com/torch/torch7/pull/1046,https://github.com/soumith,4,https://github.com/torch/torch7/pull/1046#issuecomment-309111518,"This allows for faster sampling from a multinomial distribution using the Alias method which is described in this paper.
It allows almost 5x speed in sampling than torch.multinomial. To see the difference on your machine run  th test/test_aliasMultinomial.lua","why would you need the table return?
can you just return one Tensor that contains all state?",True,{}
torch/torch7,https://github.com/torch/torch7,1046,2017-06-14T21:26:38Z,2017-07-11T17:22:36Z,2017-07-11T17:22:44Z,MERGED,True,269,10,7,https://github.com/amartya18x,Implementation of Alias Multinomial for faster Multinomial sampling,4,[],https://github.com/torch/torch7/pull/1046,https://github.com/soumith,5,https://github.com/torch/torch7/pull/1046#issuecomment-309111806,"This allows for faster sampling from a multinomial distribution using the Alias method which is described in this paper.
It allows almost 5x speed in sampling than torch.multinomial. To see the difference on your machine run  th test/test_aliasMultinomial.lua","and two functions
state = torch.aliasMultinomialSetup(probs)
torch.aliasMultinomial(out, state)",True,{}
torch/torch7,https://github.com/torch/torch7,1046,2017-06-14T21:26:38Z,2017-07-11T17:22:36Z,2017-07-11T17:22:44Z,MERGED,True,269,10,7,https://github.com/amartya18x,Implementation of Alias Multinomial for faster Multinomial sampling,4,[],https://github.com/torch/torch7/pull/1046,https://github.com/amartya18x,6,https://github.com/torch/torch7/pull/1046#issuecomment-309112585,"This allows for faster sampling from a multinomial distribution using the Alias method which is described in this paper.
It allows almost 5x speed in sampling than torch.multinomial. To see the difference on your machine run  th test/test_aliasMultinomial.lua","Its two tensors. They are of different types. One is Long and the other is double.
And torch.aliasMultinomial isn't working because the output needs to be a longTensor and torch.aliasMultinomial says that the function isn't defined for torch.LongTensor. This is the only workaround I got.
Could you suggest someway to make that work ?",True,{}
torch/torch7,https://github.com/torch/torch7,1046,2017-06-14T21:26:38Z,2017-07-11T17:22:36Z,2017-07-11T17:22:44Z,MERGED,True,269,10,7,https://github.com/amartya18x,Implementation of Alias Multinomial for faster Multinomial sampling,4,[],https://github.com/torch/torch7/pull/1046,https://github.com/soumith,7,https://github.com/torch/torch7/pull/1046#issuecomment-309113094,"This allows for faster sampling from a multinomial distribution using the Alias method which is described in this paper.
It allows almost 5x speed in sampling than torch.multinomial. To see the difference on your machine run  th test/test_aliasMultinomial.lua","in cwrap, define aliasMultinomial_
in lua, maybe in random.lua or something, define torch.aliasMultinomial as a lua function that calls torch.LongTensor.multinomial_(...)",True,{}
torch/torch7,https://github.com/torch/torch7,1046,2017-06-14T21:26:38Z,2017-07-11T17:22:36Z,2017-07-11T17:22:44Z,MERGED,True,269,10,7,https://github.com/amartya18x,Implementation of Alias Multinomial for faster Multinomial sampling,4,[],https://github.com/torch/torch7/pull/1046,https://github.com/amartya18x,8,https://github.com/torch/torch7/pull/1046#issuecomment-309116977,"This allows for faster sampling from a multinomial distribution using the Alias method which is described in this paper.
It allows almost 5x speed in sampling than torch.multinomial. To see the difference on your machine run  th test/test_aliasMultinomial.lua",@soumith Is it okay to do that init.lua ?torch isn't available in random.lua.,True,{}
torch/torch7,https://github.com/torch/torch7,1046,2017-06-14T21:26:38Z,2017-07-11T17:22:36Z,2017-07-11T17:22:44Z,MERGED,True,269,10,7,https://github.com/amartya18x,Implementation of Alias Multinomial for faster Multinomial sampling,4,[],https://github.com/torch/torch7/pull/1046,https://github.com/soumith,9,https://github.com/torch/torch7/pull/1046#issuecomment-309117189,"This allows for faster sampling from a multinomial distribution using the Alias method which is described in this paper.
It allows almost 5x speed in sampling than torch.multinomial. To see the difference on your machine run  th test/test_aliasMultinomial.lua","sure, I'll move it if i need to, but do it  in init.lua",True,{}
torch/torch7,https://github.com/torch/torch7,1046,2017-06-14T21:26:38Z,2017-07-11T17:22:36Z,2017-07-11T17:22:44Z,MERGED,True,269,10,7,https://github.com/amartya18x,Implementation of Alias Multinomial for faster Multinomial sampling,4,[],https://github.com/torch/torch7/pull/1046,https://github.com/amartya18x,10,https://github.com/torch/torch7/pull/1046#issuecomment-309117389,"This allows for faster sampling from a multinomial distribution using the Alias method which is described in this paper.
It allows almost 5x speed in sampling than torch.multinomial. To see the difference on your machine run  th test/test_aliasMultinomial.lua","Great. That helps everything.
…
 On Jun 16, 2017, at 3:42 PM, Soumith Chintala ***@***.***> wrote:

 sure, I'll move it if i need to, but do it in init.lua

 —
 You are receiving this because you were mentioned.
 Reply to this email directly, view it on GitHub <#1046 (comment)>, or mute the thread <https://github.com/notifications/unsubscribe-auth/AHWRc7FNP8WI1otN4dai6y1SjV8tjNmIks5sEtqigaJpZM4N6dn_>.",True,{}
torch/torch7,https://github.com/torch/torch7,1046,2017-06-14T21:26:38Z,2017-07-11T17:22:36Z,2017-07-11T17:22:44Z,MERGED,True,269,10,7,https://github.com/amartya18x,Implementation of Alias Multinomial for faster Multinomial sampling,4,[],https://github.com/torch/torch7/pull/1046,https://github.com/amartya18x,11,https://github.com/torch/torch7/pull/1046#issuecomment-309122627,"This allows for faster sampling from a multinomial distribution using the Alias method which is described in this paper.
It allows almost 5x speed in sampling than torch.multinomial. To see the difference on your machine run  th test/test_aliasMultinomial.lua","@soumith
th> probs = torch.DoubleTensor({0.2, 0.2, 0.3, 0.3})
                                                                      [0.0001s]
th> state = torch.multinomialAliasSetup(probs)
                                                                      [0.0001s]
th> state
{
  1 : LongTensor - size: 4
  2 : DoubleTensor - size: 4
}
                                                                      [0.0002s]
th> output = torch.LongTensor(2,3)
 
                                                                      [0.0003s]
th> torch.multinomialAlias(output:view(-1), state)
 4
 1
 2
 3
 2
 2
[torch.LongTensor of size 6]

                                                                      [0.0002s]
th> output
 4  1  2
 3  2  2
[torch.LongTensor of size 2x3]

                                                                      [0.0001s]",True,{}
torch/torch7,https://github.com/torch/torch7,1046,2017-06-14T21:26:38Z,2017-07-11T17:22:36Z,2017-07-11T17:22:44Z,MERGED,True,269,10,7,https://github.com/amartya18x,Implementation of Alias Multinomial for faster Multinomial sampling,4,[],https://github.com/torch/torch7/pull/1046,https://github.com/nicholas-leonard,12,https://github.com/torch/torch7/pull/1046#issuecomment-311988467,"This allows for faster sampling from a multinomial distribution using the Alias method which is described in this paper.
It allows almost 5x speed in sampling than torch.multinomial. To see the difference on your machine run  th test/test_aliasMultinomial.lua",This seems ready to merge,True,{}
torch/torch7,https://github.com/torch/torch7,1046,2017-06-14T21:26:38Z,2017-07-11T17:22:36Z,2017-07-11T17:22:44Z,MERGED,True,269,10,7,https://github.com/amartya18x,Implementation of Alias Multinomial for faster Multinomial sampling,4,[],https://github.com/torch/torch7/pull/1046,https://github.com/amartya18x,13,https://github.com/torch/torch7/pull/1046#issuecomment-311989385,"This allows for faster sampling from a multinomial distribution using the Alias method which is described in this paper.
It allows almost 5x speed in sampling than torch.multinomial. To see the difference on your machine run  th test/test_aliasMultinomial.lua",@nicholas-leonard There is a failing test but that is due the log-normal and it is on clang.,True,{}
torch/torch7,https://github.com/torch/torch7,1046,2017-06-14T21:26:38Z,2017-07-11T17:22:36Z,2017-07-11T17:22:44Z,MERGED,True,269,10,7,https://github.com/amartya18x,Implementation of Alias Multinomial for faster Multinomial sampling,4,[],https://github.com/torch/torch7/pull/1046,https://github.com/pavanky,14,https://github.com/torch/torch7/pull/1046#issuecomment-311993808,"This allows for faster sampling from a multinomial distribution using the Alias method which is described in this paper.
It allows almost 5x speed in sampling than torch.multinomial. To see the difference on your machine run  th test/test_aliasMultinomial.lua",@amartya18x can you rebase tis PR into a single commit?,True,{}
torch/torch7,https://github.com/torch/torch7,1046,2017-06-14T21:26:38Z,2017-07-11T17:22:36Z,2017-07-11T17:22:44Z,MERGED,True,269,10,7,https://github.com/amartya18x,Implementation of Alias Multinomial for faster Multinomial sampling,4,[],https://github.com/torch/torch7/pull/1046,https://github.com/amartya18x,15,https://github.com/torch/torch7/pull/1046#issuecomment-311994337,"This allows for faster sampling from a multinomial distribution using the Alias method which is described in this paper.
It allows almost 5x speed in sampling than torch.multinomial. To see the difference on your machine run  th test/test_aliasMultinomial.lua",@pavanky Yes ofcourse,True,{}
torch/torch7,https://github.com/torch/torch7,1046,2017-06-14T21:26:38Z,2017-07-11T17:22:36Z,2017-07-11T17:22:44Z,MERGED,True,269,10,7,https://github.com/amartya18x,Implementation of Alias Multinomial for faster Multinomial sampling,4,[],https://github.com/torch/torch7/pull/1046,https://github.com/amartya18x,16,https://github.com/torch/torch7/pull/1046#issuecomment-313835143,"This allows for faster sampling from a multinomial distribution using the Alias method which is described in this paper.
It allows almost 5x speed in sampling than torch.multinomial. To see the difference on your machine run  th test/test_aliasMultinomial.lua",Does this look good ? I will rebase the commits then.,True,{}
torch/torch7,https://github.com/torch/torch7,1046,2017-06-14T21:26:38Z,2017-07-11T17:22:36Z,2017-07-11T17:22:44Z,MERGED,True,269,10,7,https://github.com/amartya18x,Implementation of Alias Multinomial for faster Multinomial sampling,4,[],https://github.com/torch/torch7/pull/1046,https://github.com/soumith,17,https://github.com/torch/torch7/pull/1046#issuecomment-314513961,"This allows for faster sampling from a multinomial distribution using the Alias method which is described in this paper.
It allows almost 5x speed in sampling than torch.multinomial. To see the difference on your machine run  th test/test_aliasMultinomial.lua",Thanks Amartya!,True,{'LAUGH': ['https://github.com/amartya18x']}
torch/torch7,https://github.com/torch/torch7,1054,2017-07-03T03:35:14Z,,2017-07-03T04:00:28Z,OPEN,False,38,2,1,https://github.com/ahmadh84,Improved CmdLine help message formatting,1,[],https://github.com/torch/torch7/pull/1054,https://github.com/ahmadh84,1,https://github.com/torch/torch7/pull/1054,"Before, the -h help flag wrote each option in its own line as follows:
 -opt1    This is my long winded help message which requires more space and makes it ugly [default option for -opt1]
 -myopt2  help for myopt2 [default option for -myopt2]

These messages were extremely hard to read. This commit now breaks each
option help message as follows:
 -opt1    This is my long winded help message
          which requires more space and makes
          it ugly [default option for -opt1]
 -myopt2  help for myopt2 [default option for -myopt2]

This is done in a way to limit the help message to around 80 characters.","Before, the -h help flag wrote each option in its own line as follows:
 -opt1    This is my long winded help message which requires more space and makes it ugly [default option for -opt1]
 -myopt2  help for myopt2 [default option for -myopt2]

These messages were extremely hard to read. This commit now breaks each
option help message as follows:
 -opt1    This is my long winded help message
          which requires more space and makes
          it ugly [default option for -opt1]
 -myopt2  help for myopt2 [default option for -myopt2]

This is done in a way to limit the help message to around 80 characters.",True,{}
torch/torch7,https://github.com/torch/torch7,1060,2017-07-12T15:16:54Z,2017-07-12T15:36:22Z,2017-07-12T15:36:25Z,MERGED,True,2,1,1,https://github.com/amartya18x,Fixing arctan2 documentation,1,[],https://github.com/torch/torch7/pull/1060,https://github.com/amartya18x,1,https://github.com/torch/torch7/pull/1060,Fixing Issue #1059,Fixing Issue #1059,True,{}
torch/torch7,https://github.com/torch/torch7,1060,2017-07-12T15:16:54Z,2017-07-12T15:36:22Z,2017-07-12T15:36:25Z,MERGED,True,2,1,1,https://github.com/amartya18x,Fixing arctan2 documentation,1,[],https://github.com/torch/torch7/pull/1060,https://github.com/soumith,2,https://github.com/torch/torch7/pull/1060#issuecomment-314808308,Fixing Issue #1059,thanks!,True,{}
torch/torch7,https://github.com/torch/torch7,1062,2017-07-20T14:54:57Z,2017-07-20T14:56:10Z,2017-07-20T14:56:15Z,MERGED,True,49,49,7,https://github.com/LambdaWill,update docs,9,[],https://github.com/torch/torch7/pull/1062,https://github.com/LambdaWill,1,https://github.com/torch/torch7/pull/1062,,,True,{}
torch/torch7,https://github.com/torch/torch7,1062,2017-07-20T14:54:57Z,2017-07-20T14:56:10Z,2017-07-20T14:56:15Z,MERGED,True,49,49,7,https://github.com/LambdaWill,update docs,9,[],https://github.com/torch/torch7/pull/1062,https://github.com/soumith,2,https://github.com/torch/torch7/pull/1062#issuecomment-316729427,,thanks a lot!,True,{}
torch/torch7,https://github.com/torch/torch7,1070,2017-09-08T10:37:34Z,,2017-09-08T10:37:34Z,OPEN,False,7,0,1,https://github.com/LambdaWill,Update tensor.md,1,[],https://github.com/torch/torch7/pull/1070,https://github.com/LambdaWill,1,https://github.com/torch/torch7/pull/1070,Add how to get the value of specific position in a tensor.,Add how to get the value of specific position in a tensor.,True,{}
torch/torch7,https://github.com/torch/torch7,1073,2017-09-16T23:06:55Z,2020-01-22T22:49:04Z,2020-01-22T22:49:05Z,CLOSED,False,2,2,1,https://github.com/brettkoonce,minor spelling tweaks,1,[],https://github.com/torch/torch7/pull/1073,https://github.com/brettkoonce,1,https://github.com/torch/torch7/pull/1073,,,True,{}
torch/torch7,https://github.com/torch/torch7,1073,2017-09-16T23:06:55Z,2020-01-22T22:49:04Z,2020-01-22T22:49:05Z,CLOSED,False,2,2,1,https://github.com/brettkoonce,minor spelling tweaks,1,[],https://github.com/torch/torch7/pull/1073,https://github.com/brettkoonce,2,https://github.com/torch/torch7/pull/1073#issuecomment-577422853,,👻,True,{}
torch/torch7,https://github.com/torch/torch7,1118,2018-01-08T11:05:51Z,,2018-01-08T11:05:51Z,OPEN,False,1,1,1,https://github.com/mathemage,Update tensor.md,1,[],https://github.com/torch/torch7/pull/1118,https://github.com/mathemage,1,https://github.com/torch/torch7/pull/1118,fix typo in maskedCopy(),fix typo in maskedCopy(),True,{}
torch/torch7,https://github.com/torch/torch7,1124,2018-01-29T08:27:41Z,,2018-01-29T08:27:41Z,OPEN,False,3,0,1,https://github.com/ubaidsworld,for ubuntu 16.04,1,[],https://github.com/torch/torch7/pull/1124,https://github.com/ubaidsworld,1,https://github.com/torch/torch7/pull/1124,Updated the readme file with requirement for Ubuntu 16.04,Updated the readme file with requirement for Ubuntu 16.04,True,{}
torch/torch7,https://github.com/torch/torch7,1130,2018-02-20T21:38:24Z,,2018-02-20T21:38:24Z,OPEN,False,1,0,1,https://github.com/aDotInTheVoid,resolve torch/distro/issues/32,1,[],https://github.com/torch/torch7/pull/1130,https://github.com/aDotInTheVoid,1,https://github.com/torch/torch7/pull/1130,"Torch fails to build on some macOS versions as outlined here
This pull request fixes that
Full credit to @yongduek for finding the fix","Torch fails to build on some macOS versions as outlined here
This pull request fixes that
Full credit to @yongduek for finding the fix",True,{}
torch/torch7,https://github.com/torch/torch7,1158,2018-07-02T23:12:53Z,2018-07-02T23:25:12Z,2018-07-02T23:25:12Z,MERGED,True,7,0,1,https://github.com/stites,Development Status in README.md,2,[],https://github.com/torch/torch7/pull/1158,https://github.com/stites,1,https://github.com/torch/torch7/pull/1158,,,True,{}
torch/torch7,https://github.com/torch/torch7,1161,2018-07-18T18:18:01Z,,2018-07-25T19:40:11Z,OPEN,False,13,4,1,https://github.com/Taicanium,Minor change to TH/CMakeLists.txt,2,[],https://github.com/torch/torch7/pull/1161,https://github.com/Taicanium,1,https://github.com/torch/torch7/pull/1161,"With this change, I've managed to get Torch to compile on Windows via LuaRocks and MSVC. By configuring LuaRocks for Lua 5.3 and maintaining a directory with the corresponding Lua source and binaries, in addition to the headers and libraries for GNU Readline as a dependency for TREPL, the installation proceeds with a few equation warnings, but no compiler errors. I plan to publish a Batch file for ease of installation via command line using the MSVC tools, but in the meantime this pull is more pressing.","With this change, I've managed to get Torch to compile on Windows via LuaRocks and MSVC. By configuring LuaRocks for Lua 5.3 and maintaining a directory with the corresponding Lua source and binaries, in addition to the headers and libraries for GNU Readline as a dependency for TREPL, the installation proceeds with a few equation warnings, but no compiler errors. I plan to publish a Batch file for ease of installation via command line using the MSVC tools, but in the meantime this pull is more pressing.",True,{}
torch/torch7,https://github.com/torch/torch7,1161,2018-07-18T18:18:01Z,,2018-07-25T19:40:11Z,OPEN,False,13,4,1,https://github.com/Taicanium,Minor change to TH/CMakeLists.txt,2,[],https://github.com/torch/torch7/pull/1161,https://github.com/Taicanium,2,https://github.com/torch/torch7/pull/1161#issuecomment-406027382,"With this change, I've managed to get Torch to compile on Windows via LuaRocks and MSVC. By configuring LuaRocks for Lua 5.3 and maintaining a directory with the corresponding Lua source and binaries, in addition to the headers and libraries for GNU Readline as a dependency for TREPL, the installation proceeds with a few equation warnings, but no compiler errors. I plan to publish a Batch file for ease of installation via command line using the MSVC tools, but in the meantime this pull is more pressing.","Should be noted that a custom CMake command needed to be used, in order to specify generation of NMake Makefiles. This .cmd file can be provided if necessary.",True,{}
torch/torch7,https://github.com/torch/torch7,1187,2018-12-27T08:31:26Z,,2018-12-27T08:31:26Z,OPEN,False,2,2,1,https://github.com/0xflotus,fixed development,1,[],https://github.com/torch/torch7/pull/1187,https://github.com/0xflotus,1,https://github.com/torch/torch7/pull/1187,,,True,{}
torch/torch7,https://github.com/torch/torch7,1232,2021-04-25T06:04:10Z,,2021-04-25T06:04:10Z,OPEN,False,1,1,1,https://github.com/plan-do-break-fix,fix(docs): corrects typo in project README,1,[],https://github.com/torch/torch7/pull/1232,https://github.com/plan-do-break-fix,1,https://github.com/torch/torch7/pull/1232,Scope of change is restricted to docs only. Change is in agreement with Standard American English.,Scope of change is restricted to docs only. Change is in agreement with Standard American English.,True,{}
