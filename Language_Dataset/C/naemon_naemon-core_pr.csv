naemon/naemon-core,https://github.com/naemon/naemon-core,128,2016-03-04T07:30:27Z,2016-03-04T08:56:06Z,2016-03-08T07:31:24Z,MERGED,True,14,4,1,https://github.com/catharsis,Sample config updates,3,[],https://github.com/naemon/naemon-core/pull/128,https://github.com/catharsis,1,https://github.com/naemon/naemon-core/pull/128,,,True,{}
naemon/naemon-core,https://github.com/naemon/naemon-core,128,2016-03-04T07:30:27Z,2016-03-04T08:56:06Z,2016-03-08T07:31:24Z,MERGED,True,14,4,1,https://github.com/catharsis,Sample config updates,3,[],https://github.com/naemon/naemon-core/pull/128,https://github.com/sni,2,https://github.com/naemon/naemon-core/pull/128#issuecomment-192190455,,thanks,True,{}
naemon/naemon-core,https://github.com/naemon/naemon-core,129,2016-03-23T13:12:22Z,2016-04-18T12:30:52Z,2016-04-18T12:30:52Z,CLOSED,False,4,2,2,https://github.com/sni,make adjust_host_check_attempt public again,1,[],https://github.com/naemon/naemon-core/pull/129,https://github.com/sni,1,https://github.com/naemon/naemon-core/pull/129,This function is used by mod-gearmans host check handler.,This function is used by mod-gearmans host check handler.,True,{}
naemon/naemon-core,https://github.com/naemon/naemon-core,129,2016-03-23T13:12:22Z,2016-04-18T12:30:52Z,2016-04-18T12:30:52Z,CLOSED,False,4,2,2,https://github.com/sni,make adjust_host_check_attempt public again,1,[],https://github.com/naemon/naemon-core/pull/129,https://github.com/catharsis,2,https://github.com/naemon/naemon-core/pull/129#issuecomment-211358737,This function is used by mod-gearmans host check handler.,Merged in 315f78a.,True,{}
naemon/naemon-core,https://github.com/naemon/naemon-core,130,2016-04-05T11:58:42Z,2016-05-06T11:49:12Z,2016-05-06T11:49:12Z,MERGED,True,83,54,2,https://github.com/sni,Shadownaemon iobroker,2,[],https://github.com/naemon/naemon-core/pull/130,https://github.com/sni,1,https://github.com/naemon/naemon-core/pull/130,shadownaemon updates. Use fixed dummy command instead of the first one and change to iobroker based event poll since that is required for livestatus now.,shadownaemon updates. Use fixed dummy command instead of the first one and change to iobroker based event poll since that is required for livestatus now.,True,{}
naemon/naemon-core,https://github.com/naemon/naemon-core,132,2016-05-23T08:44:58Z,2016-05-23T09:04:38Z,2016-05-23T09:04:39Z,MERGED,True,12,5,2,https://github.com/sni,make it possible to build official packages,1,[],https://github.com/naemon/naemon-core/pull/132,https://github.com/sni,1,https://github.com/naemon/naemon-core/pull/132,"Right now, the -source appendix is hard coded in the version script,
so add a way to flag official builds by an empty file.
Signed-off-by: Sven Nierlein Sven.Nierlein@consol.de","Right now, the -source appendix is hard coded in the version script,
so add a way to flag official builds by an empty file.
Signed-off-by: Sven Nierlein Sven.Nierlein@consol.de",True,{}
naemon/naemon-core,https://github.com/naemon/naemon-core,134,2016-06-01T09:36:00Z,2016-06-02T06:51:21Z,2016-06-02T06:51:21Z,MERGED,True,0,15,1,https://github.com/sni,remove enable_environment_macros from sample config,1,[],https://github.com/naemon/naemon-core/pull/134,https://github.com/sni,1,https://github.com/naemon/naemon-core/pull/134,"new installations should not come with warnings like:
Warning: enable_environment_macros is deprecated and will be removed.","new installations should not come with warnings like:
Warning: enable_environment_macros is deprecated and will be removed.",True,{}
naemon/naemon-core,https://github.com/naemon/naemon-core,135,2016-06-03T13:23:51Z,2016-06-07T07:46:54Z,2016-06-07T07:46:54Z,MERGED,True,54,2,1,https://github.com/sni,update news file,1,[],https://github.com/naemon/naemon-core/pull/135,https://github.com/sni,1,https://github.com/naemon/naemon-core/pull/135,,,True,{}
naemon/naemon-core,https://github.com/naemon/naemon-core,142,2016-06-10T21:06:07Z,2016-06-15T07:59:03Z,2016-06-15T07:59:03Z,MERGED,True,8,139,4,https://github.com/stromsoe,remove temp_file usage from retention.dat and status.dat file creation,5,[],https://github.com/naemon/naemon-core/pull/142,https://github.com/stromsoe,1,https://github.com/naemon/naemon-core/pull/142,"Make xrddefault_save_state_information() use retention_file as pattern to mkstemp()
Make xrddefault_save_state_information() check ferror() at file close and abort my_rename() if there are any errors
Make xsddefault_save_status_data() use status_file as pattern to mkstemp()
Make xsddefault_save_status_data() check ferror() at file close and abort my_rename() if there are any errors
Modify my_rename() to check for errno==XDEV and remove calls to my_fcopy()
Remove now unused my_fcopy() and my_fdcopy()","Make xrddefault_save_state_information() use retention_file as pattern to mkstemp()
Make xrddefault_save_state_information() check ferror() at file close and abort my_rename() if there are any errors
Make xsddefault_save_status_data() use status_file as pattern to mkstemp()
Make xsddefault_save_status_data() check ferror() at file close and abort my_rename() if there are any errors
Modify my_rename() to check for errno==XDEV and remove calls to my_fcopy()
Remove now unused my_fcopy() and my_fdcopy()",True,{}
naemon/naemon-core,https://github.com/naemon/naemon-core,142,2016-06-10T21:06:07Z,2016-06-15T07:59:03Z,2016-06-15T07:59:03Z,MERGED,True,8,139,4,https://github.com/stromsoe,remove temp_file usage from retention.dat and status.dat file creation,5,[],https://github.com/naemon/naemon-core/pull/142,https://github.com/catharsis,2,https://github.com/naemon/naemon-core/pull/142#issuecomment-225802276,"Make xrddefault_save_state_information() use retention_file as pattern to mkstemp()
Make xrddefault_save_state_information() check ferror() at file close and abort my_rename() if there are any errors
Make xsddefault_save_status_data() use status_file as pattern to mkstemp()
Make xsddefault_save_status_data() check ferror() at file close and abort my_rename() if there are any errors
Modify my_rename() to check for errno==XDEV and remove calls to my_fcopy()
Remove now unused my_fcopy() and my_fdcopy()","Thanks a lot!
LGTM, I've submitted this patch for internal review.",True,{}
naemon/naemon-core,https://github.com/naemon/naemon-core,146,2016-07-29T18:25:43Z,2016-08-22T10:18:42Z,2016-08-22T10:19:20Z,MERGED,True,4,2,1,https://github.com/ipstatic,Checking for Git binary,1,[],https://github.com/naemon/naemon-core/pull/146,https://github.com/ipstatic,1,https://github.com/naemon/naemon-core/pull/146,"In some build systems, git is not present unless defined as a build
dependency. However the .git directory may still be copied in along with
the source. This change will still allow the required behavior in case
.git is present but the Git binary is not.","In some build systems, git is not present unless defined as a build
dependency. However the .git directory may still be copied in along with
the source. This change will still allow the required behavior in case
.git is present but the Git binary is not.",True,{}
naemon/naemon-core,https://github.com/naemon/naemon-core,146,2016-07-29T18:25:43Z,2016-08-22T10:18:42Z,2016-08-22T10:19:20Z,MERGED,True,4,2,1,https://github.com/ipstatic,Checking for Git binary,1,[],https://github.com/naemon/naemon-core/pull/146,https://github.com/sni,2,https://github.com/naemon/naemon-core/pull/146#issuecomment-236957550,"In some build systems, git is not present unless defined as a build
dependency. However the .git directory may still be copied in along with
the source. This change will still allow the required behavior in case
.git is present but the Git binary is not.",Looks good to me.,True,{}
naemon/naemon-core,https://github.com/naemon/naemon-core,146,2016-07-29T18:25:43Z,2016-08-22T10:18:42Z,2016-08-22T10:19:20Z,MERGED,True,4,2,1,https://github.com/ipstatic,Checking for Git binary,1,[],https://github.com/naemon/naemon-core/pull/146,https://github.com/pengi,3,https://github.com/naemon/naemon-core/pull/146#issuecomment-241371330,"In some build systems, git is not present unless defined as a build
dependency. However the .git directory may still be copied in along with
the source. This change will still allow the required behavior in case
.git is present but the Git binary is not.",Merged,True,{}
naemon/naemon-core,https://github.com/naemon/naemon-core,147,2016-08-26T10:47:17Z,2016-08-26T14:43:48Z,2016-08-26T14:43:48Z,CLOSED,False,1,1,1,https://github.com/eythori-sensa,Only reload with systemd if configtest is ok,2,[],https://github.com/naemon/naemon-core/pull/147,https://github.com/eythori-sensa,1,https://github.com/naemon/naemon-core/pull/147,"Configuration errors followed by a reload causes the server to stop without an error in systemd.
Adding a check before running the reload.","Configuration errors followed by a reload causes the server to stop without an error in systemd.
Adding a check before running the reload.",True,{}
naemon/naemon-core,https://github.com/naemon/naemon-core,147,2016-08-26T10:47:17Z,2016-08-26T14:43:48Z,2016-08-26T14:43:48Z,CLOSED,False,1,1,1,https://github.com/eythori-sensa,Only reload with systemd if configtest is ok,2,[],https://github.com/naemon/naemon-core/pull/147,https://github.com/eythori-sensa,2,https://github.com/naemon/naemon-core/pull/147#issuecomment-242755144,"Configuration errors followed by a reload causes the server to stop without an error in systemd.
Adding a check before running the reload.",Further checks show this does not work :( Closing this pull request,True,{}
naemon/naemon-core,https://github.com/naemon/naemon-core,155,2016-10-31T14:37:59Z,2017-01-10T14:14:30Z,2017-01-10T14:14:30Z,MERGED,True,4,2,1,https://github.com/sni,fix memory leak in broker_notification_data,1,[],https://github.com/naemon/naemon-core/pull/155,https://github.com/sni,1,https://github.com/naemon/naemon-core/pull/155,"broker_notification_data returns a neb_resultset which needs to be freed, even if we
don't use it.","broker_notification_data returns a neb_resultset which needs to be freed, even if we
don't use it.",True,{}
naemon/naemon-core,https://github.com/naemon/naemon-core,157,2016-11-08T10:00:48Z,2017-01-10T14:21:49Z,2017-01-10T14:21:49Z,MERGED,True,5,1,1,https://github.com/sni,lower loglevel for exiting worker,1,[],https://github.com/naemon/naemon-core/pull/157,https://github.com/sni,1,https://github.com/naemon/naemon-core/pull/157,"If a worker exits because the main naemon process has exited it writes
one last log entry. This entry is likely to end up in an already rotated
old logfile which breaks livestatus.
It then exits with EXIT_SUCCESS, so its not really an error anyway and
therefor we can just lower the loglevel.
Signed-off-by: Sven Nierlein sven@nierlein.de","If a worker exits because the main naemon process has exited it writes
one last log entry. This entry is likely to end up in an already rotated
old logfile which breaks livestatus.
It then exits with EXIT_SUCCESS, so its not really an error anyway and
therefor we can just lower the loglevel.
Signed-off-by: Sven Nierlein sven@nierlein.de",True,{}
naemon/naemon-core,https://github.com/naemon/naemon-core,158,2016-11-08T21:05:31Z,2017-01-10T13:14:43Z,2017-01-19T14:38:00Z,MERGED,True,1,1,1,https://github.com/sni,fix typo in commands.c,1,[],https://github.com/naemon/naemon-core/pull/158,https://github.com/sni,1,https://github.com/naemon/naemon-core/pull/158,just a simple typo...,just a simple typo...,True,{}
naemon/naemon-core,https://github.com/naemon/naemon-core,160,2017-01-11T22:50:44Z,2017-01-12T15:50:33Z,2017-01-12T15:50:33Z,MERGED,True,7,122,6,https://github.com/sni,Remove drop privileges,2,[],https://github.com/naemon/naemon-core/pull/160,https://github.com/sni,1,https://github.com/naemon/naemon-core/pull/160,"This PR remove the drop_privileges function and bails out if naemon is started as root.
It also changes the init script to change into the naemon user before starting the core.
This fixes CVE-2016-9566.","This PR remove the drop_privileges function and bails out if naemon is started as root.
It also changes the init script to change into the naemon user before starting the core.
This fixes CVE-2016-9566.",True,{'THUMBS_UP': ['https://github.com/ipstatic']}
naemon/naemon-core,https://github.com/naemon/naemon-core,161,2017-01-13T09:40:16Z,2017-01-17T09:57:21Z,2017-01-17T09:57:21Z,MERGED,True,2,2,1,https://github.com/sni,make systemd config check use a valid shell,2,[],https://github.com/naemon/naemon-core/pull/161,https://github.com/sni,1,https://github.com/naemon/naemon-core/pull/161,"Usually the naemon account is a system user and has no login shell. So in order
to make the config preflight check work, we have to specify a valid shell.
Signed-off-by: Sven Nierlein sven@nierlein.de","Usually the naemon account is a system user and has no login shell. So in order
to make the config preflight check work, we have to specify a valid shell.
Signed-off-by: Sven Nierlein sven@nierlein.de",True,{}
naemon/naemon-core,https://github.com/naemon/naemon-core,161,2017-01-13T09:40:16Z,2017-01-17T09:57:21Z,2017-01-17T09:57:21Z,MERGED,True,2,2,1,https://github.com/sni,make systemd config check use a valid shell,2,[],https://github.com/naemon/naemon-core/pull/161,https://github.com/sni,2,https://github.com/naemon/naemon-core/pull/161#issuecomment-272904978,"Usually the naemon account is a system user and has no login shell. So in order
to make the config preflight check work, we have to specify a valid shell.
Signed-off-by: Sven Nierlein sven@nierlein.de",Here we go,True,{}
naemon/naemon-core,https://github.com/naemon/naemon-core,163,2017-01-19T12:31:56Z,2017-01-19T15:05:15Z,2017-01-19T15:05:16Z,MERGED,True,27,17,1,https://github.com/tvestelind,allow running as root if --allow-root flag is given,2,[],https://github.com/naemon/naemon-core/pull/163,https://github.com/tvestelind,1,https://github.com/naemon/naemon-core/pull/163,"Even though running as root is not recommended, there are cases where running
as root facilitates things a lot. An example could be a test environment and
another a Docker container.
Signed-off-by: Tomas Vestelind tvestelind@op5.com","Even though running as root is not recommended, there are cases where running
as root facilitates things a lot. An example could be a test environment and
another a Docker container.
Signed-off-by: Tomas Vestelind tvestelind@op5.com",True,{}
naemon/naemon-core,https://github.com/naemon/naemon-core,164,2017-01-19T13:50:42Z,2017-01-19T14:38:13Z,2017-01-19T14:38:25Z,CLOSED,False,1,1,1,https://github.com/sni,fix typo in systemd file,2,[],https://github.com/naemon/naemon-core/pull/164,https://github.com/sni,1,https://github.com/naemon/naemon-core/pull/164,naemon does not start with that typo in its systemd file.,naemon does not start with that typo in its systemd file.,True,{}
naemon/naemon-core,https://github.com/naemon/naemon-core,165,2017-01-19T14:40:05Z,2017-01-19T15:03:56Z,2017-01-19T15:40:17Z,MERGED,True,2,2,1,https://github.com/sni,systemd: do not use precached objects,1,[],https://github.com/naemon/naemon-core/pull/165,https://github.com/sni,1,https://github.com/naemon/naemon-core/pull/165,Precached objects file was created before but never used. So there is no need to create it.,Precached objects file was created before but never used. So there is no need to create it.,True,{}
naemon/naemon-core,https://github.com/naemon/naemon-core,166,2017-01-19T15:42:48Z,2017-01-26T10:04:42Z,2018-12-03T13:13:36Z,MERGED,True,0,1,1,https://github.com/sni,remove NICELEVEL from systemd service,2,[],https://github.com/naemon/naemon-core/pull/166,https://github.com/sni,1,https://github.com/naemon/naemon-core/pull/166,"NICELEVEL is not set by default, so systemd constantly complains about the value not beeing set.","NICELEVEL is not set by default, so systemd constantly complains about the value not beeing set.",True,{}
naemon/naemon-core,https://github.com/naemon/naemon-core,166,2017-01-19T15:42:48Z,2017-01-26T10:04:42Z,2018-12-03T13:13:36Z,MERGED,True,0,1,1,https://github.com/sni,remove NICELEVEL from systemd service,2,[],https://github.com/naemon/naemon-core/pull/166,https://github.com/sni,2,https://github.com/naemon/naemon-core/pull/166#issuecomment-273844051,"NICELEVEL is not set by default, so systemd constantly complains about the value not beeing set.","well, its commented in /etc/sysconfig/naemon as well, so i didn't want to completly remove it.",True,{}
naemon/naemon-core,https://github.com/naemon/naemon-core,166,2017-01-19T15:42:48Z,2017-01-26T10:04:42Z,2018-12-03T13:13:36Z,MERGED,True,0,1,1,https://github.com/sni,remove NICELEVEL from systemd service,2,[],https://github.com/naemon/naemon-core/pull/166,https://github.com/tvestelind,3,https://github.com/naemon/naemon-core/pull/166#issuecomment-273847928,"NICELEVEL is not set by default, so systemd constantly complains about the value not beeing set.","Ok, that makes sense.",True,{}
naemon/naemon-core,https://github.com/naemon/naemon-core,166,2017-01-19T15:42:48Z,2017-01-26T10:04:42Z,2018-12-03T13:13:36Z,MERGED,True,0,1,1,https://github.com/sni,remove NICELEVEL from systemd service,2,[],https://github.com/naemon/naemon-core/pull/166,https://github.com/tvestelind,4,https://github.com/naemon/naemon-core/pull/166#issuecomment-274085907,"NICELEVEL is not set by default, so systemd constantly complains about the value not beeing set.","One other reason I think it should be removed is that NICE is documented in systemd.unit while the sample config configuration values are not documented anywhere else, thus it's fine if those are commented.",True,{}
naemon/naemon-core,https://github.com/naemon/naemon-core,167,2017-01-19T15:44:48Z,2017-01-23T08:33:42Z,2018-12-03T13:13:40Z,MERGED,True,1,1,1,https://github.com/sni,systemd requires absolute path in execstart,1,[],https://github.com/naemon/naemon-core/pull/167,https://github.com/sni,1,https://github.com/naemon/naemon-core/pull/167,"from man systemd.service ""For each of the specified commands, the first argument must be an absolute path to an executable.""","from man systemd.service ""For each of the specified commands, the first argument must be an absolute path to an executable.""",True,{}
naemon/naemon-core,https://github.com/naemon/naemon-core,170,2017-03-24T15:01:53Z,2017-03-27T09:06:43Z,2017-03-27T09:06:43Z,MERGED,True,0,19,3,https://github.com/mattiasr,Minimize the amount of verify-config,1,[],https://github.com/naemon/naemon-core/pull/170,https://github.com/mattiasr,1,https://github.com/naemon/naemon-core/pull/170,"In a large setup where it can take more than 30 seconds to verify a config.
I propose a implementation similar to other systems, like Apache httpd.

If you change your config and restart Apache httpd, the server won't start
and you have a broken config. (Expected)
If you run a verify on the config, it will verify without interrupt with
your current running instance.

But doing this we can speed up the actual start/stop/restart of the naemon
process.
Signed-off-by: Mattias Ryrlén mattiasr@op5.com","In a large setup where it can take more than 30 seconds to verify a config.
I propose a implementation similar to other systems, like Apache httpd.

If you change your config and restart Apache httpd, the server won't start
and you have a broken config. (Expected)
If you run a verify on the config, it will verify without interrupt with
your current running instance.

But doing this we can speed up the actual start/stop/restart of the naemon
process.
Signed-off-by: Mattias Ryrlén mattiasr@op5.com",True,{}
naemon/naemon-core,https://github.com/naemon/naemon-core,170,2017-03-24T15:01:53Z,2017-03-27T09:06:43Z,2017-03-27T09:06:43Z,MERGED,True,0,19,3,https://github.com/mattiasr,Minimize the amount of verify-config,1,[],https://github.com/naemon/naemon-core/pull/170,https://github.com/sni,2,https://github.com/naemon/naemon-core/pull/170#issuecomment-289048442,"In a large setup where it can take more than 30 seconds to verify a config.
I propose a implementation similar to other systems, like Apache httpd.

If you change your config and restart Apache httpd, the server won't start
and you have a broken config. (Expected)
If you run a verify on the config, it will verify without interrupt with
your current running instance.

But doing this we can speed up the actual start/stop/restart of the naemon
process.
Signed-off-by: Mattias Ryrlén mattiasr@op5.com",Looks good to me,True,{}
naemon/naemon-core,https://github.com/naemon/naemon-core,170,2017-03-24T15:01:53Z,2017-03-27T09:06:43Z,2017-03-27T09:06:43Z,MERGED,True,0,19,3,https://github.com/mattiasr,Minimize the amount of verify-config,1,[],https://github.com/naemon/naemon-core/pull/170,https://github.com/fmikker,3,https://github.com/naemon/naemon-core/pull/170#issuecomment-289068241,"In a large setup where it can take more than 30 seconds to verify a config.
I propose a implementation similar to other systems, like Apache httpd.

If you change your config and restart Apache httpd, the server won't start
and you have a broken config. (Expected)
If you run a verify on the config, it will verify without interrupt with
your current running instance.

But doing this we can speed up the actual start/stop/restart of the naemon
process.
Signed-off-by: Mattias Ryrlén mattiasr@op5.com",Nice. We should remember to update the documentation accordingly if needed.,True,{}
naemon/naemon-core,https://github.com/naemon/naemon-core,172,2017-04-04T12:30:46Z,2017-05-31T07:58:09Z,2017-05-31T07:58:10Z,MERGED,True,2,0,1,https://github.com/sni,fix segfault in notifications when there is no eventbroker,1,[],https://github.com/naemon/naemon-core/pull/172,https://github.com/sni,1,https://github.com/naemon/naemon-core/pull/172,"nebcallback resultset may be null if eventbroker options prevent
notifications from beeing brokered. Do not segfault in those
cases.
Signed-off-by: Sven Nierlein sven@nierlein.de","nebcallback resultset may be null if eventbroker options prevent
notifications from beeing brokered. Do not segfault in those
cases.
Signed-off-by: Sven Nierlein sven@nierlein.de",True,{}
naemon/naemon-core,https://github.com/naemon/naemon-core,172,2017-04-04T12:30:46Z,2017-05-31T07:58:09Z,2017-05-31T07:58:10Z,MERGED,True,2,0,1,https://github.com/sni,fix segfault in notifications when there is no eventbroker,1,[],https://github.com/naemon/naemon-core/pull/172,https://github.com/tvestelind,2,https://github.com/naemon/naemon-core/pull/172#issuecomment-301700504,"nebcallback resultset may be null if eventbroker options prevent
notifications from beeing brokered. Do not segfault in those
cases.
Signed-off-by: Sven Nierlein sven@nierlein.de",I wouldn't mind having a unit test for this but I'm fine with merging this.,True,{}
naemon/naemon-core,https://github.com/naemon/naemon-core,172,2017-04-04T12:30:46Z,2017-05-31T07:58:09Z,2017-05-31T07:58:10Z,MERGED,True,2,0,1,https://github.com/sni,fix segfault in notifications when there is no eventbroker,1,[],https://github.com/naemon/naemon-core/pull/172,https://github.com/tvestelind,3,https://github.com/naemon/naemon-core/pull/172#issuecomment-301830668,"nebcallback resultset may be null if eventbroker options prevent
notifications from beeing brokered. Do not segfault in those
cases.
Signed-off-by: Sven Nierlein sven@nierlein.de",I added a unit test for this and it's currently on review internally.,True,{}
naemon/naemon-core,https://github.com/naemon/naemon-core,172,2017-04-04T12:30:46Z,2017-05-31T07:58:09Z,2017-05-31T07:58:10Z,MERGED,True,2,0,1,https://github.com/sni,fix segfault in notifications when there is no eventbroker,1,[],https://github.com/naemon/naemon-core/pull/172,https://github.com/tvestelind,4,https://github.com/naemon/naemon-core/pull/172#issuecomment-301833834,"nebcallback resultset may be null if eventbroker options prevent
notifications from beeing brokered. Do not segfault in those
cases.
Signed-off-by: Sven Nierlein sven@nierlein.de",If you would like to see and review it then I can either create a new PR (if I understand GH's documentation correctly) or you can look at my commit here: tvestelind@cf2b6b3,True,{}
naemon/naemon-core,https://github.com/naemon/naemon-core,172,2017-04-04T12:30:46Z,2017-05-31T07:58:09Z,2017-05-31T07:58:10Z,MERGED,True,2,0,1,https://github.com/sni,fix segfault in notifications when there is no eventbroker,1,[],https://github.com/naemon/naemon-core/pull/172,https://github.com/sni,5,https://github.com/naemon/naemon-core/pull/172#issuecomment-301858674,"nebcallback resultset may be null if eventbroker options prevent
notifications from beeing brokered. Do not segfault in those
cases.
Signed-off-by: Sven Nierlein sven@nierlein.de","Great, looks good to me. Afaik you could even push the test case onto this PR branch.",True,{}
naemon/naemon-core,https://github.com/naemon/naemon-core,172,2017-04-04T12:30:46Z,2017-05-31T07:58:09Z,2017-05-31T07:58:10Z,MERGED,True,2,0,1,https://github.com/sni,fix segfault in notifications when there is no eventbroker,1,[],https://github.com/naemon/naemon-core/pull/172,https://github.com/tvestelind,6,https://github.com/naemon/naemon-core/pull/172#issuecomment-302001702,"nebcallback resultset may be null if eventbroker options prevent
notifications from beeing brokered. Do not segfault in those
cases.
Signed-off-by: Sven Nierlein sven@nierlein.de","Nice, then I'll consider it ready to merge.
It says Add more commits by pushing to the fix_segfault_without_eventbroker branch on sni/naemon-core. but I don't think it I have access to push to your fork, or am I missing something?",True,{}
naemon/naemon-core,https://github.com/naemon/naemon-core,172,2017-04-04T12:30:46Z,2017-05-31T07:58:09Z,2017-05-31T07:58:10Z,MERGED,True,2,0,1,https://github.com/sni,fix segfault in notifications when there is no eventbroker,1,[],https://github.com/naemon/naemon-core/pull/172,https://github.com/sni,7,https://github.com/naemon/naemon-core/pull/172#issuecomment-302344052,"nebcallback resultset may be null if eventbroker options prevent
notifications from beeing brokered. Do not segfault in those
cases.
Signed-off-by: Sven Nierlein sven@nierlein.de","but I don't think it I have access to push to your fork, or am I missing something?

Thats a checkbox when creating a pull request. So yes, you should have permission to push to that branch in my repo. But i don't care, we can also do 2 PRs if thats easier :-)",True,{}
naemon/naemon-core,https://github.com/naemon/naemon-core,172,2017-04-04T12:30:46Z,2017-05-31T07:58:09Z,2017-05-31T07:58:10Z,MERGED,True,2,0,1,https://github.com/sni,fix segfault in notifications when there is no eventbroker,1,[],https://github.com/naemon/naemon-core/pull/172,https://github.com/tvestelind,8,https://github.com/naemon/naemon-core/pull/172#issuecomment-302377656,"nebcallback resultset may be null if eventbroker options prevent
notifications from beeing brokered. Do not segfault in those
cases.
Signed-off-by: Sven Nierlein sven@nierlein.de","Ah ok, good to know for the future. I'll try to remember to do the same :)",True,{}
naemon/naemon-core,https://github.com/naemon/naemon-core,173,2017-04-04T15:05:25Z,2017-09-06T14:46:52Z,2017-09-06T14:46:52Z,MERGED,True,52,16,4,https://github.com/sni,fix comment ids increment on reload,3,[],https://github.com/naemon/naemon-core/pull/173,https://github.com/sni,1,https://github.com/naemon/naemon-core/pull/173,"Comments related to downtimes are beeing added with persistance=false flag. This
leads to the comment beeing removed during a reload. The downtime then immediatly
adds the comment again but the id increments each reload. Furter this results in
a warning from livestatus for each downtime on every reload:
livestatus: Cannot delete non-existing downtime/comment
Signed-off-by: Sven Nierlein sven@nierlein.de","Comments related to downtimes are beeing added with persistance=false flag. This
leads to the comment beeing removed during a reload. The downtime then immediatly
adds the comment again but the id increments each reload. Furter this results in
a warning from livestatus for each downtime on every reload:
livestatus: Cannot delete non-existing downtime/comment
Signed-off-by: Sven Nierlein sven@nierlein.de",True,{}
naemon/naemon-core,https://github.com/naemon/naemon-core,173,2017-04-04T15:05:25Z,2017-09-06T14:46:52Z,2017-09-06T14:46:52Z,MERGED,True,52,16,4,https://github.com/sni,fix comment ids increment on reload,3,[],https://github.com/naemon/naemon-core/pull/173,https://github.com/sni,2,https://github.com/naemon/naemon-core/pull/173#issuecomment-292112868,"Comments related to downtimes are beeing added with persistance=false flag. This
leads to the comment beeing removed during a reload. The downtime then immediatly
adds the comment again but the id increments each reload. Furter this results in
a warning from livestatus for each downtime on every reload:
livestatus: Cannot delete non-existing downtime/comment
Signed-off-by: Sven Nierlein sven@nierlein.de","I don't think so, otherwise we would have noticed this earlier :-) I am not aware of any unit tests which cover reloading the core, are you?",True,{}
naemon/naemon-core,https://github.com/naemon/naemon-core,173,2017-04-04T15:05:25Z,2017-09-06T14:46:52Z,2017-09-06T14:46:52Z,MERGED,True,52,16,4,https://github.com/sni,fix comment ids increment on reload,3,[],https://github.com/naemon/naemon-core/pull/173,https://github.com/tvestelind,3,https://github.com/naemon/naemon-core/pull/173#issuecomment-292114081,"Comments related to downtimes are beeing added with persistance=false flag. This
leads to the comment beeing removed during a reload. The downtime then immediatly
adds the comment again but the id increments each reload. Furter this results in
a warning from livestatus for each downtime on every reload:
livestatus: Cannot delete non-existing downtime/comment
Signed-off-by: Sven Nierlein sven@nierlein.de",Nope. I know that there are some tests but I'm not sure what they do and how they do it.,True,{}
naemon/naemon-core,https://github.com/naemon/naemon-core,173,2017-04-04T15:05:25Z,2017-09-06T14:46:52Z,2017-09-06T14:46:52Z,MERGED,True,52,16,4,https://github.com/sni,fix comment ids increment on reload,3,[],https://github.com/naemon/naemon-core/pull/173,https://github.com/sni,4,https://github.com/naemon/naemon-core/pull/173#issuecomment-317397474,"Comments related to downtimes are beeing added with persistance=false flag. This
leads to the comment beeing removed during a reload. The downtime then immediatly
adds the comment again but the id increments each reload. Furter this results in
a warning from livestatus for each downtime on every reload:
livestatus: Cannot delete non-existing downtime/comment
Signed-off-by: Sven Nierlein sven@nierlein.de",I added a test case and fixed a issue with the previous patch. Should be fine now(tm),True,{}
naemon/naemon-core,https://github.com/naemon/naemon-core,173,2017-04-04T15:05:25Z,2017-09-06T14:46:52Z,2017-09-06T14:46:52Z,MERGED,True,52,16,4,https://github.com/sni,fix comment ids increment on reload,3,[],https://github.com/naemon/naemon-core/pull/173,https://github.com/sni,5,https://github.com/naemon/naemon-core/pull/173#issuecomment-327421798,"Comments related to downtimes are beeing added with persistance=false flag. This
leads to the comment beeing removed during a reload. The downtime then immediatly
adds the comment again but the id increments each reload. Furter this results in
a warning from livestatus for each downtime on every reload:
livestatus: Cannot delete non-existing downtime/comment
Signed-off-by: Sven Nierlein sven@nierlein.de",Any more comments?,True,{}
naemon/naemon-core,https://github.com/naemon/naemon-core,175,2017-04-12T12:54:22Z,2017-04-19T09:22:29Z,2017-04-19T09:22:29Z,MERGED,True,486,100,16,https://github.com/roengstrom,Feature/mon 10198 fail naemon start when daemonizing,4,['bug'],https://github.com/naemon/naemon-core/pull/175,https://github.com/roengstrom,1,https://github.com/naemon/naemon-core/pull/175,"Doing configuration changes in a large environments with big configuration files and then restarting naemon will take a very long time. We would like to remove all verification done when starting and restarting and make configuration verification explicit through naemon -v.
This patch removes the verification done in the init script. It also makes the parent wait for the child to be fully initialized before exiting the parent when daemonizing, making it possible to catch if the child fails during the initialization.","Doing configuration changes in a large environments with big configuration files and then restarting naemon will take a very long time. We would like to remove all verification done when starting and restarting and make configuration verification explicit through naemon -v.
This patch removes the verification done in the init script. It also makes the parent wait for the child to be fully initialized before exiting the parent when daemonizing, making it possible to catch if the child fails during the initialization.",True,{}
naemon/naemon-core,https://github.com/naemon/naemon-core,175,2017-04-12T12:54:22Z,2017-04-19T09:22:29Z,2017-04-19T09:22:29Z,MERGED,True,486,100,16,https://github.com/roengstrom,Feature/mon 10198 fail naemon start when daemonizing,4,['bug'],https://github.com/naemon/naemon-core/pull/175,https://github.com/tvestelind,2,https://github.com/naemon/naemon-core/pull/175#issuecomment-293570797,"Doing configuration changes in a large environments with big configuration files and then restarting naemon will take a very long time. We would like to remove all verification done when starting and restarting and make configuration verification explicit through naemon -v.
This patch removes the verification done in the init script. It also makes the parent wait for the child to be fully initialized before exiting the parent when daemonizing, making it possible to catch if the child fails during the initialization.",Several behave tests fail in travis but that is not reflected here. That must be fixed.,True,{}
naemon/naemon-core,https://github.com/naemon/naemon-core,175,2017-04-12T12:54:22Z,2017-04-19T09:22:29Z,2017-04-19T09:22:29Z,MERGED,True,486,100,16,https://github.com/roengstrom,Feature/mon 10198 fail naemon start when daemonizing,4,['bug'],https://github.com/naemon/naemon-core/pull/175,https://github.com/roengstrom,3,https://github.com/naemon/naemon-core/pull/175#issuecomment-293617955,"Doing configuration changes in a large environments with big configuration files and then restarting naemon will take a very long time. We would like to remove all verification done when starting and restarting and make configuration verification explicit through naemon -v.
This patch removes the verification done in the init script. It also makes the parent wait for the child to be fully initialized before exiting the parent when daemonizing, making it possible to catch if the child fails during the initialization.",Fixed the Travis build. Now all tests are running and are passing.,True,{}
naemon/naemon-core,https://github.com/naemon/naemon-core,175,2017-04-12T12:54:22Z,2017-04-19T09:22:29Z,2017-04-19T09:22:29Z,MERGED,True,486,100,16,https://github.com/roengstrom,Feature/mon 10198 fail naemon start when daemonizing,4,['bug'],https://github.com/naemon/naemon-core/pull/175,https://github.com/roengstrom,4,https://github.com/naemon/naemon-core/pull/175#issuecomment-295137330,"Doing configuration changes in a large environments with big configuration files and then restarting naemon will take a very long time. We would like to remove all verification done when starting and restarting and make configuration verification explicit through naemon -v.
This patch removes the verification done in the init script. It also makes the parent wait for the child to be fully initialized before exiting the parent when daemonizing, making it possible to catch if the child fails during the initialization.",@tvestelind So I've taken all your comments into consideration and adjusted the code accordingly.,True,{'THUMBS_UP': ['https://github.com/roengstrom']}
naemon/naemon-core,https://github.com/naemon/naemon-core,175,2017-04-12T12:54:22Z,2017-04-19T09:22:29Z,2017-04-19T09:22:29Z,MERGED,True,486,100,16,https://github.com/roengstrom,Feature/mon 10198 fail naemon start when daemonizing,4,['bug'],https://github.com/naemon/naemon-core/pull/175,https://github.com/tvestelind,5,https://github.com/naemon/naemon-core/pull/175#issuecomment-295176320,"Doing configuration changes in a large environments with big configuration files and then restarting naemon will take a very long time. We would like to remove all verification done when starting and restarting and make configuration verification explicit through naemon -v.
This patch removes the verification done in the init script. It also makes the parent wait for the child to be fully initialized before exiting the parent when daemonizing, making it possible to catch if the child fails during the initialization.","Good, then we can merge it.
Could you push it internally and we'll merge it.
To clarify for anyone else reading this. We'll try to review more on Github in the future but for now we must merge in-house. We're working on changing that as well so that we can merge here directly in the future.",True,{}
naemon/naemon-core,https://github.com/naemon/naemon-core,176,2017-04-20T13:20:42Z,2017-04-25T12:20:51Z,2017-04-25T12:20:51Z,MERGED,True,16,11,4,https://github.com/sni,reduce log spam,1,[],https://github.com/naemon/naemon-core/pull/176,https://github.com/sni,1,https://github.com/naemon/naemon-core/pull/176,"Do not log timeouts 3 times for each timeout. Timeouts are a normal thing in
monitoring and state changes are logged already, there is no need to add 2
extra log entries, one from wproc and one from the check handler. Especially since
the timeout is quite visible already and the extra logging does not add any value.
Signed-off-by: Sven Nierlein sven@nierlein.de","Do not log timeouts 3 times for each timeout. Timeouts are a normal thing in
monitoring and state changes are logged already, there is no need to add 2
extra log entries, one from wproc and one from the check handler. Especially since
the timeout is quite visible already and the extra logging does not add any value.
Signed-off-by: Sven Nierlein sven@nierlein.de",True,{}
naemon/naemon-core,https://github.com/naemon/naemon-core,176,2017-04-20T13:20:42Z,2017-04-25T12:20:51Z,2017-04-25T12:20:51Z,MERGED,True,16,11,4,https://github.com/sni,reduce log spam,1,[],https://github.com/naemon/naemon-core/pull/176,https://github.com/sni,2,https://github.com/naemon/naemon-core/pull/176#issuecomment-295737109,"Do not log timeouts 3 times for each timeout. Timeouts are a normal thing in
monitoring and state changes are logged already, there is no need to add 2
extra log entries, one from wproc and one from the check handler. Especially since
the timeout is quite visible already and the extra logging does not add any value.
Signed-off-by: Sven Nierlein sven@nierlein.de","You are right, i changed that. Thanks for the fast review :-)",True,{'LAUGH': ['https://github.com/mattiasr']}
naemon/naemon-core,https://github.com/naemon/naemon-core,176,2017-04-20T13:20:42Z,2017-04-25T12:20:51Z,2017-04-25T12:20:51Z,MERGED,True,16,11,4,https://github.com/sni,reduce log spam,1,[],https://github.com/naemon/naemon-core/pull/176,https://github.com/sni,3,https://github.com/naemon/naemon-core/pull/176#issuecomment-295741378,"Do not log timeouts 3 times for each timeout. Timeouts are a normal thing in
monitoring and state changes are logged already, there is no need to add 2
extra log entries, one from wproc and one from the check handler. Especially since
the timeout is quite visible already and the extra logging does not add any value.
Signed-off-by: Sven Nierlein sven@nierlein.de",alright :-) i remove them as well.,True,{}
naemon/naemon-core,https://github.com/naemon/naemon-core,176,2017-04-20T13:20:42Z,2017-04-25T12:20:51Z,2017-04-25T12:20:51Z,MERGED,True,16,11,4,https://github.com/sni,reduce log spam,1,[],https://github.com/naemon/naemon-core/pull/176,https://github.com/tvestelind,4,https://github.com/naemon/naemon-core/pull/176#issuecomment-296099965,"Do not log timeouts 3 times for each timeout. Timeouts are a normal thing in
monitoring and state changes are logged already, there is no need to add 2
extra log entries, one from wproc and one from the check handler. Especially since
the timeout is quite visible already and the extra logging does not add any value.
Signed-off-by: Sven Nierlein sven@nierlein.de",Can you rebase this to the latest commit on master? I think that we might have introduced a bug in the last PR so we reverted it.,True,{}
naemon/naemon-core,https://github.com/naemon/naemon-core,176,2017-04-20T13:20:42Z,2017-04-25T12:20:51Z,2017-04-25T12:20:51Z,MERGED,True,16,11,4,https://github.com/sni,reduce log spam,1,[],https://github.com/naemon/naemon-core/pull/176,https://github.com/roengstrom,5,https://github.com/naemon/naemon-core/pull/176#issuecomment-296102675,"Do not log timeouts 3 times for each timeout. Timeouts are a normal thing in
monitoring and state changes are logged already, there is no need to add 2
extra log entries, one from wproc and one from the check handler. Especially since
the timeout is quite visible already and the extra logging does not add any value.
Signed-off-by: Sven Nierlein sven@nierlein.de",@tvestelind We're going to re-revert it and also add a fix for the introduced bug in a few minutes.,True,{}
naemon/naemon-core,https://github.com/naemon/naemon-core,176,2017-04-20T13:20:42Z,2017-04-25T12:20:51Z,2017-04-25T12:20:51Z,MERGED,True,16,11,4,https://github.com/sni,reduce log spam,1,[],https://github.com/naemon/naemon-core/pull/176,https://github.com/sni,6,https://github.com/naemon/naemon-core/pull/176#issuecomment-296131209,"Do not log timeouts 3 times for each timeout. Timeouts are a normal thing in
monitoring and state changes are logged already, there is no need to add 2
extra log entries, one from wproc and one from the check handler. Especially since
the timeout is quite visible already and the extra logging does not add any value.
Signed-off-by: Sven Nierlein sven@nierlein.de",rebased...,True,{}
naemon/naemon-core,https://github.com/naemon/naemon-core,176,2017-04-20T13:20:42Z,2017-04-25T12:20:51Z,2017-04-25T12:20:51Z,MERGED,True,16,11,4,https://github.com/sni,reduce log spam,1,[],https://github.com/naemon/naemon-core/pull/176,https://github.com/tvestelind,7,https://github.com/naemon/naemon-core/pull/176#issuecomment-296147477,"Do not log timeouts 3 times for each timeout. Timeouts are a normal thing in
monitoring and state changes are logged already, there is no need to add 2
extra log entries, one from wproc and one from the check handler. Especially since
the timeout is quite visible already and the extra logging does not add any value.
Signed-off-by: Sven Nierlein sven@nierlein.de","From tests:
tests/test-worker.c:99:F:worker reaping tests:worker_test_timeout:0: Expected 'due to timeout' to be present in log

FAIL: tests/test-worker",True,{}
naemon/naemon-core,https://github.com/naemon/naemon-core,176,2017-04-20T13:20:42Z,2017-04-25T12:20:51Z,2017-04-25T12:20:51Z,MERGED,True,16,11,4,https://github.com/sni,reduce log spam,1,[],https://github.com/naemon/naemon-core/pull/176,https://github.com/sni,8,https://github.com/naemon/naemon-core/pull/176#issuecomment-296150314,"Do not log timeouts 3 times for each timeout. Timeouts are a normal thing in
monitoring and state changes are logged already, there is no need to add 2
extra log entries, one from wproc and one from the check handler. Especially since
the timeout is quite visible already and the extra logging does not add any value.
Signed-off-by: Sven Nierlein sven@nierlein.de","oh man, these tests are a nightmare... We have t/ and tests/ and tap/ and even a t-tap/, it feels like everyone created his own test folder.",True,{'THUMBS_UP': ['https://github.com/rhagman']}
naemon/naemon-core,https://github.com/naemon/naemon-core,176,2017-04-20T13:20:42Z,2017-04-25T12:20:51Z,2017-04-25T12:20:51Z,MERGED,True,16,11,4,https://github.com/sni,reduce log spam,1,[],https://github.com/naemon/naemon-core/pull/176,https://github.com/tvestelind,9,https://github.com/naemon/naemon-core/pull/176#issuecomment-296171789,"Do not log timeouts 3 times for each timeout. Timeouts are a normal thing in
monitoring and state changes are logged already, there is no need to add 2
extra log entries, one from wproc and one from the check handler. Especially since
the timeout is quite visible already and the extra logging does not add any value.
Signed-off-by: Sven Nierlein sven@nierlein.de","Yea, it's not very clear what each folder tests either :/",True,{}
naemon/naemon-core,https://github.com/naemon/naemon-core,176,2017-04-20T13:20:42Z,2017-04-25T12:20:51Z,2017-04-25T12:20:51Z,MERGED,True,16,11,4,https://github.com/sni,reduce log spam,1,[],https://github.com/naemon/naemon-core/pull/176,https://github.com/sni,10,https://github.com/naemon/naemon-core/pull/176#issuecomment-296232043,"Do not log timeouts 3 times for each timeout. Timeouts are a normal thing in
monitoring and state changes are logged already, there is no need to add 2
extra log entries, one from wproc and one from the check handler. Especially since
the timeout is quite visible already and the extra logging does not add any value.
Signed-off-by: Sven Nierlein sven@nierlein.de","Ok, i moved most wproc: message to the debug logfile and adjusted the tests.",True,{}
naemon/naemon-core,https://github.com/naemon/naemon-core,176,2017-04-20T13:20:42Z,2017-04-25T12:20:51Z,2017-04-25T12:20:51Z,MERGED,True,16,11,4,https://github.com/sni,reduce log spam,1,[],https://github.com/naemon/naemon-core/pull/176,https://github.com/rhagman,11,https://github.com/naemon/naemon-core/pull/176#issuecomment-296941323,"Do not log timeouts 3 times for each timeout. Timeouts are a normal thing in
monitoring and state changes are logged already, there is no need to add 2
extra log entries, one from wproc and one from the check handler. Especially since
the timeout is quite visible already and the extra logging does not add any value.
Signed-off-by: Sven Nierlein sven@nierlein.de","Looks good to me, well done!",True,{}
naemon/naemon-core,https://github.com/naemon/naemon-core,177,2017-04-21T07:34:01Z,2017-04-21T08:34:57Z,2017-04-21T08:34:57Z,MERGED,True,481,102,16,https://github.com/roengstrom,Bugfix/daemon child dies when worker dies,3,[],https://github.com/naemon/naemon-core/pull/177,https://github.com/roengstrom,1,https://github.com/naemon/naemon-core/pull/177,,,True,{}
naemon/naemon-core,https://github.com/naemon/naemon-core,177,2017-04-21T07:34:01Z,2017-04-21T08:34:57Z,2017-04-21T08:34:57Z,MERGED,True,481,102,16,https://github.com/roengstrom,Bugfix/daemon child dies when worker dies,3,[],https://github.com/naemon/naemon-core/pull/177,https://github.com/roengstrom,2,https://github.com/naemon/naemon-core/pull/177#issuecomment-296109142,,This is a re-revert of the pull request merged a few days ago followed with a fix for the bug it introduced.,True,{}
naemon/naemon-core,https://github.com/naemon/naemon-core,177,2017-04-21T07:34:01Z,2017-04-21T08:34:57Z,2017-04-21T08:34:57Z,MERGED,True,481,102,16,https://github.com/roengstrom,Bugfix/daemon child dies when worker dies,3,[],https://github.com/naemon/naemon-core/pull/177,https://github.com/tvestelind,3,https://github.com/naemon/naemon-core/pull/177#issuecomment-296120198,,I'm OK with merging internally 👍,True,{'THUMBS_UP': ['https://github.com/roengstrom']}
naemon/naemon-core,https://github.com/naemon/naemon-core,179,2017-04-27T07:33:58Z,2017-04-27T12:17:15Z,2017-04-27T12:17:15Z,MERGED,True,15,12,3,https://github.com/roengstrom,naemon executable path,1,[],https://github.com/naemon/naemon-core/pull/179,https://github.com/roengstrom,1,https://github.com/naemon/naemon-core/pull/179,"Added ability to omit setting the executable path to the
naemon binary and the shared libraries. Also added possibility
to use absolute paths.
Signed-off-by: Robin Engström robin.engstrom@op5.com","Added ability to omit setting the executable path to the
naemon binary and the shared libraries. Also added possibility
to use absolute paths.
Signed-off-by: Robin Engström robin.engstrom@op5.com",True,{}
naemon/naemon-core,https://github.com/naemon/naemon-core,179,2017-04-27T07:33:58Z,2017-04-27T12:17:15Z,2017-04-27T12:17:15Z,MERGED,True,15,12,3,https://github.com/roengstrom,naemon executable path,1,[],https://github.com/naemon/naemon-core/pull/179,https://github.com/mattiasr,2,https://github.com/naemon/naemon-core/pull/179#issuecomment-297638010,"Added ability to omit setting the executable path to the
naemon binary and the shared libraries. Also added possibility
to use absolute paths.
Signed-off-by: Robin Engström robin.engstrom@op5.com",Looks good to me,True,{}
naemon/naemon-core,https://github.com/naemon/naemon-core,180,2017-05-05T09:37:38Z,2017-05-16T13:39:49Z,2017-05-16T13:40:51Z,MERGED,True,0,7,1,https://github.com/sni,undeprecate check_result_path,1,[],https://github.com/naemon/naemon-core/pull/180,https://github.com/sni,1,https://github.com/naemon/naemon-core/pull/180,"Reading check results from disk is not a bad thing. Lot of addons use this for good reasons.
Ex. Thruk submits business process results by spool folder. We could make check_result_path
optional, but i don't recommend removing it. Therefor we remove the deprecation warning.
Signed-off-by: Sven Nierlein sven@nierlein.de","Reading check results from disk is not a bad thing. Lot of addons use this for good reasons.
Ex. Thruk submits business process results by spool folder. We could make check_result_path
optional, but i don't recommend removing it. Therefor we remove the deprecation warning.
Signed-off-by: Sven Nierlein sven@nierlein.de",True,{}
naemon/naemon-core,https://github.com/naemon/naemon-core,180,2017-05-05T09:37:38Z,2017-05-16T13:39:49Z,2017-05-16T13:40:51Z,MERGED,True,0,7,1,https://github.com/sni,undeprecate check_result_path,1,[],https://github.com/naemon/naemon-core/pull/180,https://github.com/tvestelind,2,https://github.com/naemon/naemon-core/pull/180#issuecomment-300111394,"Reading check results from disk is not a bad thing. Lot of addons use this for good reasons.
Ex. Thruk submits business process results by spool folder. We could make check_result_path
optional, but i don't recommend removing it. Therefor we remove the deprecation warning.
Signed-off-by: Sven Nierlein sven@nierlein.de",Github seems to be a bit buggy for me so I cannot make comments in the file. But looking quickly there might be issues related to these features. This one is from the commit that deprecated that feature https://jira.op5.com/browse/MON-8381. I'll try and look more to see why we decided to no longer support it.,True,{}
naemon/naemon-core,https://github.com/naemon/naemon-core,180,2017-05-05T09:37:38Z,2017-05-16T13:39:49Z,2017-05-16T13:40:51Z,MERGED,True,0,7,1,https://github.com/sni,undeprecate check_result_path,1,[],https://github.com/naemon/naemon-core/pull/180,https://github.com/sni,3,https://github.com/naemon/naemon-core/pull/180#issuecomment-300284606,"Reading check results from disk is not a bad thing. Lot of addons use this for good reasons.
Ex. Thruk submits business process results by spool folder. We could make check_result_path
optional, but i don't recommend removing it. Therefor we remove the deprecation warning.
Signed-off-by: Sven Nierlein sven@nierlein.de",I don't think the mentionen issue is a problem. It is even solved already.,True,{}
naemon/naemon-core,https://github.com/naemon/naemon-core,180,2017-05-05T09:37:38Z,2017-05-16T13:39:49Z,2017-05-16T13:40:51Z,MERGED,True,0,7,1,https://github.com/sni,undeprecate check_result_path,1,[],https://github.com/naemon/naemon-core/pull/180,https://github.com/tvestelind,4,https://github.com/naemon/naemon-core/pull/180#issuecomment-301700360,"Reading check results from disk is not a bad thing. Lot of addons use this for good reasons.
Ex. Thruk submits business process results by spool folder. We could make check_result_path
optional, but i don't recommend removing it. Therefor we remove the deprecation warning.
Signed-off-by: Sven Nierlein sven@nierlein.de",From my perspective this is merge:able. Why do you say @roengstrom and @mattiasr ?,True,{}
naemon/naemon-core,https://github.com/naemon/naemon-core,180,2017-05-05T09:37:38Z,2017-05-16T13:39:49Z,2017-05-16T13:40:51Z,MERGED,True,0,7,1,https://github.com/sni,undeprecate check_result_path,1,[],https://github.com/naemon/naemon-core/pull/180,https://github.com/mattiasr,5,https://github.com/naemon/naemon-core/pull/180#issuecomment-301702291,"Reading check results from disk is not a bad thing. Lot of addons use this for good reasons.
Ex. Thruk submits business process results by spool folder. We could make check_result_path
optional, but i don't recommend removing it. Therefor we remove the deprecation warning.
Signed-off-by: Sven Nierlein sven@nierlein.de","Sure this can be merged, we don't use it so for our point of view it's safe.",True,{}
naemon/naemon-core,https://github.com/naemon/naemon-core,180,2017-05-05T09:37:38Z,2017-05-16T13:39:49Z,2017-05-16T13:40:51Z,MERGED,True,0,7,1,https://github.com/sni,undeprecate check_result_path,1,[],https://github.com/naemon/naemon-core/pull/180,https://github.com/roengstrom,6,https://github.com/naemon/naemon-core/pull/180#issuecomment-301704253,"Reading check results from disk is not a bad thing. Lot of addons use this for good reasons.
Ex. Thruk submits business process results by spool folder. We could make check_result_path
optional, but i don't recommend removing it. Therefor we remove the deprecation warning.
Signed-off-by: Sven Nierlein sven@nierlein.de","I agree, don't have any problem with this being merged.",True,{}
naemon/naemon-core,https://github.com/naemon/naemon-core,180,2017-05-05T09:37:38Z,2017-05-16T13:39:49Z,2017-05-16T13:40:51Z,MERGED,True,0,7,1,https://github.com/sni,undeprecate check_result_path,1,[],https://github.com/naemon/naemon-core/pull/180,https://github.com/sni,7,https://github.com/naemon/naemon-core/pull/180#issuecomment-301785471,"Reading check results from disk is not a bad thing. Lot of addons use this for good reasons.
Ex. Thruk submits business process results by spool folder. We could make check_result_path
optional, but i don't recommend removing it. Therefor we remove the deprecation warning.
Signed-off-by: Sven Nierlein sven@nierlein.de",Thanks,True,{}
naemon/naemon-core,https://github.com/naemon/naemon-core,181,2017-05-09T10:11:40Z,2017-05-17T08:51:22Z,2017-05-17T08:51:22Z,MERGED,True,57,30,1,https://github.com/tvestelind,Feature/more timing points,3,[],https://github.com/naemon/naemon-core/pull/181,https://github.com/tvestelind,1,https://github.com/naemon/naemon-core/pull/181,"I think more and tighter timing points will be good when measuring performance in large monitoring clusters.
Example of ouput:
[1494399523] Timing point: [0.0001 (+0.0001)] Reset variables
[1494399523] Timing point: [0.0002 (+0.0001)] Reading main config file
[1494399523] Timing point: [0.0007 (+0.0005)] Read main config file
[1494399523] Timing point: [0.0011 (+0.0004)] Initializing NEB module API
[1494399523] Timing point: [0.0011 (+0.0000)] Initialized NEB module API
[1494399523] Timing point: [0.0011 (+0.0000)] Initializing Query handler
[1494399523] Timing point: [0.0012 (+0.0001)] Initialized Query handler
[1494399523] Timing point: [0.0012 (+0.0000)] Initializing NERD
[1494399523] Timing point: [0.0013 (+0.0000)] Initialized NERD
[1494399523] Timing point: [0.0013 (+0.0000)] Spawning 0 workers
[1494399523] Timing point: [0.0019 (+0.0006)] Spawned 6 workers
[1494399523] Timing point: [0.0019 (+0.0001)] Connecting 0 workers
[1494399523] Timing point: [0.0036 (+0.0017)] Connected 6 workers
[1494399523] Timing point: [0.0037 (+0.0000)] Reading all object data
[1494399523] Timing point: [0.0037 (+0.0000)] Reading config data from '/opt/monitor/etc/naemon.cfg'
[1494399524] Timing point: [0.4650 (+0.4613)] Done parsing config files
[1494399524] Timing point: [0.4969 (+0.0319)] Done resolving objects
[1494399524] Timing point: [0.5022 (+0.0054)] Done recombobulating contactgroups
[1494399524] Timing point: [0.5066 (+0.0044)] Done recombobulating hostgroups
[1494399524] Timing point: [0.6150 (+0.1084)] Created 160082 services (dupes possible)
[1494399524] Timing point: [0.6189 (+0.0039)] Done recombobulating servicegroups
[1494399524] Timing point: [0.6190 (+0.0001)] Created 0 hostescalations (dupes possible)
[1494399524] Timing point: [0.6190 (+0.0000)] Created 0 serviceescalations (dupes possible)
[1494399524] Timing point: [0.6191 (+0.0000)] Done merging hostextinfo
[1494399524] Timing point: [0.6191 (+0.0000)] Done merging serviceextinfo
[1494399524] Timing point: [0.6367 (+0.0176)] Done propagating inherited object properties
[1494399524] Timing point: [0.8314 (+0.1947)] 0 unique / 0 total servicedependencies registered
[1494399524] Timing point: [0.8315 (+0.0001)] 0 serviceescalations registered
[1494399524] Timing point: [0.8315 (+0.0000)] 0 unique / 0 total hostdependencies registered
[1494399524] Timing point: [0.8315 (+0.0000)] 0 hostescalations registered
[1494399524] Timing point: [0.8528 (+0.0213)] Read all object data
[1494399524] Timing point: [0.8693 (+0.0165)] Initializing Event queue
[1494399524] Timing point: [0.8729 (+0.0036)] Initialized Event queue
[1494399524] Timing point: [0.8730 (+0.0001)] Loading modules
[1494399524] Timing point: [1.0832 (+0.2102)] Loaded modules
[1494399524] Timing point: [1.0832 (+0.0000)] Making first callback
[1494399524] Timing point: [1.0832 (+0.0000)] Made first callback
[1494399524] Timing point: [1.0832 (+0.0000)] Running pre flight check
[1494399526] Timing point: [2.5725 (+1.4892)] Ran pre flight check
[1494399526] Timing point: [2.5725 (+0.0000)] Caching objects
[1494399527] Timing point: [3.3357 (+0.7632)] Cached objects
[1494399530] Timing point: [6.2743 (+2.9387)] Initializing status data
[1494399530] Timing point: [6.2928 (+0.0185)] Initialized status data
[1494399530] Timing point: [6.2929 (+0.0001)] Initializing downtime data
[1494399530] Timing point: [6.2929 (+0.0000)] Initialized downtime data
[1494399530] Timing point: [6.2929 (+0.0000)] Initializing retention data
[1494399530] Timing point: [6.2931 (+0.0001)] Initialized retention data
[1494399530] Timing point: [6.2931 (+0.0000)] Reading initial state information
[1494399532] Timing point: [8.3981 (+2.1050)] Read initial state information
[1494399532] Timing point: [8.3996 (+0.0015)] Initializing comment data
[1494399532] Timing point: [8.3997 (+0.0000)] Initialized comment data
[1494399532] Timing point: [8.3997 (+0.0000)] Initializing performance data
[1494399532] Timing point: [8.3997 (+0.0000)] Initialized performance data
[1494399532] Timing point: [8.3997 (+0.0000)] Initializing check execution scheduling
[1494399532] Timing point: [8.4696 (+0.0699)] Initialized check execution scheduling
[1494399532] Timing point: [8.4697 (+0.0001)] Initializing check stats
[1494399532] Timing point: [8.4697 (+0.0000)] Initialized check stats
[1494399532] Timing point: [8.4697 (+0.0000)] Updating status data
[1494399535] Timing point: [11.3166 (+2.8469)] Updated status data
[1494399535] Timing point: [11.3167 (+0.0001)] Logging initial states
[1494399536] Timing point: [12.8815 (+1.5648)] Logged initial states
[1494399536] Timing point: [12.8818 (+0.0003)] Launching command file worker
[1494399536] Timing point: [12.8837 (+0.0019)] Launched command file worker
[1494399536] Timing point: [12.9027 (+0.0190)] Entering event execution loop","I think more and tighter timing points will be good when measuring performance in large monitoring clusters.
Example of ouput:
[1494399523] Timing point: [0.0001 (+0.0001)] Reset variables
[1494399523] Timing point: [0.0002 (+0.0001)] Reading main config file
[1494399523] Timing point: [0.0007 (+0.0005)] Read main config file
[1494399523] Timing point: [0.0011 (+0.0004)] Initializing NEB module API
[1494399523] Timing point: [0.0011 (+0.0000)] Initialized NEB module API
[1494399523] Timing point: [0.0011 (+0.0000)] Initializing Query handler
[1494399523] Timing point: [0.0012 (+0.0001)] Initialized Query handler
[1494399523] Timing point: [0.0012 (+0.0000)] Initializing NERD
[1494399523] Timing point: [0.0013 (+0.0000)] Initialized NERD
[1494399523] Timing point: [0.0013 (+0.0000)] Spawning 0 workers
[1494399523] Timing point: [0.0019 (+0.0006)] Spawned 6 workers
[1494399523] Timing point: [0.0019 (+0.0001)] Connecting 0 workers
[1494399523] Timing point: [0.0036 (+0.0017)] Connected 6 workers
[1494399523] Timing point: [0.0037 (+0.0000)] Reading all object data
[1494399523] Timing point: [0.0037 (+0.0000)] Reading config data from '/opt/monitor/etc/naemon.cfg'
[1494399524] Timing point: [0.4650 (+0.4613)] Done parsing config files
[1494399524] Timing point: [0.4969 (+0.0319)] Done resolving objects
[1494399524] Timing point: [0.5022 (+0.0054)] Done recombobulating contactgroups
[1494399524] Timing point: [0.5066 (+0.0044)] Done recombobulating hostgroups
[1494399524] Timing point: [0.6150 (+0.1084)] Created 160082 services (dupes possible)
[1494399524] Timing point: [0.6189 (+0.0039)] Done recombobulating servicegroups
[1494399524] Timing point: [0.6190 (+0.0001)] Created 0 hostescalations (dupes possible)
[1494399524] Timing point: [0.6190 (+0.0000)] Created 0 serviceescalations (dupes possible)
[1494399524] Timing point: [0.6191 (+0.0000)] Done merging hostextinfo
[1494399524] Timing point: [0.6191 (+0.0000)] Done merging serviceextinfo
[1494399524] Timing point: [0.6367 (+0.0176)] Done propagating inherited object properties
[1494399524] Timing point: [0.8314 (+0.1947)] 0 unique / 0 total servicedependencies registered
[1494399524] Timing point: [0.8315 (+0.0001)] 0 serviceescalations registered
[1494399524] Timing point: [0.8315 (+0.0000)] 0 unique / 0 total hostdependencies registered
[1494399524] Timing point: [0.8315 (+0.0000)] 0 hostescalations registered
[1494399524] Timing point: [0.8528 (+0.0213)] Read all object data
[1494399524] Timing point: [0.8693 (+0.0165)] Initializing Event queue
[1494399524] Timing point: [0.8729 (+0.0036)] Initialized Event queue
[1494399524] Timing point: [0.8730 (+0.0001)] Loading modules
[1494399524] Timing point: [1.0832 (+0.2102)] Loaded modules
[1494399524] Timing point: [1.0832 (+0.0000)] Making first callback
[1494399524] Timing point: [1.0832 (+0.0000)] Made first callback
[1494399524] Timing point: [1.0832 (+0.0000)] Running pre flight check
[1494399526] Timing point: [2.5725 (+1.4892)] Ran pre flight check
[1494399526] Timing point: [2.5725 (+0.0000)] Caching objects
[1494399527] Timing point: [3.3357 (+0.7632)] Cached objects
[1494399530] Timing point: [6.2743 (+2.9387)] Initializing status data
[1494399530] Timing point: [6.2928 (+0.0185)] Initialized status data
[1494399530] Timing point: [6.2929 (+0.0001)] Initializing downtime data
[1494399530] Timing point: [6.2929 (+0.0000)] Initialized downtime data
[1494399530] Timing point: [6.2929 (+0.0000)] Initializing retention data
[1494399530] Timing point: [6.2931 (+0.0001)] Initialized retention data
[1494399530] Timing point: [6.2931 (+0.0000)] Reading initial state information
[1494399532] Timing point: [8.3981 (+2.1050)] Read initial state information
[1494399532] Timing point: [8.3996 (+0.0015)] Initializing comment data
[1494399532] Timing point: [8.3997 (+0.0000)] Initialized comment data
[1494399532] Timing point: [8.3997 (+0.0000)] Initializing performance data
[1494399532] Timing point: [8.3997 (+0.0000)] Initialized performance data
[1494399532] Timing point: [8.3997 (+0.0000)] Initializing check execution scheduling
[1494399532] Timing point: [8.4696 (+0.0699)] Initialized check execution scheduling
[1494399532] Timing point: [8.4697 (+0.0001)] Initializing check stats
[1494399532] Timing point: [8.4697 (+0.0000)] Initialized check stats
[1494399532] Timing point: [8.4697 (+0.0000)] Updating status data
[1494399535] Timing point: [11.3166 (+2.8469)] Updated status data
[1494399535] Timing point: [11.3167 (+0.0001)] Logging initial states
[1494399536] Timing point: [12.8815 (+1.5648)] Logged initial states
[1494399536] Timing point: [12.8818 (+0.0003)] Launching command file worker
[1494399536] Timing point: [12.8837 (+0.0019)] Launched command file worker
[1494399536] Timing point: [12.9027 (+0.0190)] Entering event execution loop",True,{}
naemon/naemon-core,https://github.com/naemon/naemon-core,181,2017-05-09T10:11:40Z,2017-05-17T08:51:22Z,2017-05-17T08:51:22Z,MERGED,True,57,30,1,https://github.com/tvestelind,Feature/more timing points,3,[],https://github.com/naemon/naemon-core/pull/181,https://github.com/sni,2,https://github.com/naemon/naemon-core/pull/181#issuecomment-300125773,"I think more and tighter timing points will be good when measuring performance in large monitoring clusters.
Example of ouput:
[1494399523] Timing point: [0.0001 (+0.0001)] Reset variables
[1494399523] Timing point: [0.0002 (+0.0001)] Reading main config file
[1494399523] Timing point: [0.0007 (+0.0005)] Read main config file
[1494399523] Timing point: [0.0011 (+0.0004)] Initializing NEB module API
[1494399523] Timing point: [0.0011 (+0.0000)] Initialized NEB module API
[1494399523] Timing point: [0.0011 (+0.0000)] Initializing Query handler
[1494399523] Timing point: [0.0012 (+0.0001)] Initialized Query handler
[1494399523] Timing point: [0.0012 (+0.0000)] Initializing NERD
[1494399523] Timing point: [0.0013 (+0.0000)] Initialized NERD
[1494399523] Timing point: [0.0013 (+0.0000)] Spawning 0 workers
[1494399523] Timing point: [0.0019 (+0.0006)] Spawned 6 workers
[1494399523] Timing point: [0.0019 (+0.0001)] Connecting 0 workers
[1494399523] Timing point: [0.0036 (+0.0017)] Connected 6 workers
[1494399523] Timing point: [0.0037 (+0.0000)] Reading all object data
[1494399523] Timing point: [0.0037 (+0.0000)] Reading config data from '/opt/monitor/etc/naemon.cfg'
[1494399524] Timing point: [0.4650 (+0.4613)] Done parsing config files
[1494399524] Timing point: [0.4969 (+0.0319)] Done resolving objects
[1494399524] Timing point: [0.5022 (+0.0054)] Done recombobulating contactgroups
[1494399524] Timing point: [0.5066 (+0.0044)] Done recombobulating hostgroups
[1494399524] Timing point: [0.6150 (+0.1084)] Created 160082 services (dupes possible)
[1494399524] Timing point: [0.6189 (+0.0039)] Done recombobulating servicegroups
[1494399524] Timing point: [0.6190 (+0.0001)] Created 0 hostescalations (dupes possible)
[1494399524] Timing point: [0.6190 (+0.0000)] Created 0 serviceescalations (dupes possible)
[1494399524] Timing point: [0.6191 (+0.0000)] Done merging hostextinfo
[1494399524] Timing point: [0.6191 (+0.0000)] Done merging serviceextinfo
[1494399524] Timing point: [0.6367 (+0.0176)] Done propagating inherited object properties
[1494399524] Timing point: [0.8314 (+0.1947)] 0 unique / 0 total servicedependencies registered
[1494399524] Timing point: [0.8315 (+0.0001)] 0 serviceescalations registered
[1494399524] Timing point: [0.8315 (+0.0000)] 0 unique / 0 total hostdependencies registered
[1494399524] Timing point: [0.8315 (+0.0000)] 0 hostescalations registered
[1494399524] Timing point: [0.8528 (+0.0213)] Read all object data
[1494399524] Timing point: [0.8693 (+0.0165)] Initializing Event queue
[1494399524] Timing point: [0.8729 (+0.0036)] Initialized Event queue
[1494399524] Timing point: [0.8730 (+0.0001)] Loading modules
[1494399524] Timing point: [1.0832 (+0.2102)] Loaded modules
[1494399524] Timing point: [1.0832 (+0.0000)] Making first callback
[1494399524] Timing point: [1.0832 (+0.0000)] Made first callback
[1494399524] Timing point: [1.0832 (+0.0000)] Running pre flight check
[1494399526] Timing point: [2.5725 (+1.4892)] Ran pre flight check
[1494399526] Timing point: [2.5725 (+0.0000)] Caching objects
[1494399527] Timing point: [3.3357 (+0.7632)] Cached objects
[1494399530] Timing point: [6.2743 (+2.9387)] Initializing status data
[1494399530] Timing point: [6.2928 (+0.0185)] Initialized status data
[1494399530] Timing point: [6.2929 (+0.0001)] Initializing downtime data
[1494399530] Timing point: [6.2929 (+0.0000)] Initialized downtime data
[1494399530] Timing point: [6.2929 (+0.0000)] Initializing retention data
[1494399530] Timing point: [6.2931 (+0.0001)] Initialized retention data
[1494399530] Timing point: [6.2931 (+0.0000)] Reading initial state information
[1494399532] Timing point: [8.3981 (+2.1050)] Read initial state information
[1494399532] Timing point: [8.3996 (+0.0015)] Initializing comment data
[1494399532] Timing point: [8.3997 (+0.0000)] Initialized comment data
[1494399532] Timing point: [8.3997 (+0.0000)] Initializing performance data
[1494399532] Timing point: [8.3997 (+0.0000)] Initialized performance data
[1494399532] Timing point: [8.3997 (+0.0000)] Initializing check execution scheduling
[1494399532] Timing point: [8.4696 (+0.0699)] Initialized check execution scheduling
[1494399532] Timing point: [8.4697 (+0.0001)] Initializing check stats
[1494399532] Timing point: [8.4697 (+0.0000)] Initialized check stats
[1494399532] Timing point: [8.4697 (+0.0000)] Updating status data
[1494399535] Timing point: [11.3166 (+2.8469)] Updated status data
[1494399535] Timing point: [11.3167 (+0.0001)] Logging initial states
[1494399536] Timing point: [12.8815 (+1.5648)] Logged initial states
[1494399536] Timing point: [12.8818 (+0.0003)] Launching command file worker
[1494399536] Timing point: [12.8837 (+0.0019)] Launched command file worker
[1494399536] Timing point: [12.9027 (+0.0190)] Entering event execution loop","I don't think its a good idea to log the timing information to a file. If you want, you can still use shell redirects to do that.",True,{}
naemon/naemon-core,https://github.com/naemon/naemon-core,181,2017-05-09T10:11:40Z,2017-05-17T08:51:22Z,2017-05-17T08:51:22Z,MERGED,True,57,30,1,https://github.com/tvestelind,Feature/more timing points,3,[],https://github.com/naemon/naemon-core/pull/181,https://github.com/tvestelind,3,https://github.com/naemon/naemon-core/pull/181#issuecomment-300400231,"I think more and tighter timing points will be good when measuring performance in large monitoring clusters.
Example of ouput:
[1494399523] Timing point: [0.0001 (+0.0001)] Reset variables
[1494399523] Timing point: [0.0002 (+0.0001)] Reading main config file
[1494399523] Timing point: [0.0007 (+0.0005)] Read main config file
[1494399523] Timing point: [0.0011 (+0.0004)] Initializing NEB module API
[1494399523] Timing point: [0.0011 (+0.0000)] Initialized NEB module API
[1494399523] Timing point: [0.0011 (+0.0000)] Initializing Query handler
[1494399523] Timing point: [0.0012 (+0.0001)] Initialized Query handler
[1494399523] Timing point: [0.0012 (+0.0000)] Initializing NERD
[1494399523] Timing point: [0.0013 (+0.0000)] Initialized NERD
[1494399523] Timing point: [0.0013 (+0.0000)] Spawning 0 workers
[1494399523] Timing point: [0.0019 (+0.0006)] Spawned 6 workers
[1494399523] Timing point: [0.0019 (+0.0001)] Connecting 0 workers
[1494399523] Timing point: [0.0036 (+0.0017)] Connected 6 workers
[1494399523] Timing point: [0.0037 (+0.0000)] Reading all object data
[1494399523] Timing point: [0.0037 (+0.0000)] Reading config data from '/opt/monitor/etc/naemon.cfg'
[1494399524] Timing point: [0.4650 (+0.4613)] Done parsing config files
[1494399524] Timing point: [0.4969 (+0.0319)] Done resolving objects
[1494399524] Timing point: [0.5022 (+0.0054)] Done recombobulating contactgroups
[1494399524] Timing point: [0.5066 (+0.0044)] Done recombobulating hostgroups
[1494399524] Timing point: [0.6150 (+0.1084)] Created 160082 services (dupes possible)
[1494399524] Timing point: [0.6189 (+0.0039)] Done recombobulating servicegroups
[1494399524] Timing point: [0.6190 (+0.0001)] Created 0 hostescalations (dupes possible)
[1494399524] Timing point: [0.6190 (+0.0000)] Created 0 serviceescalations (dupes possible)
[1494399524] Timing point: [0.6191 (+0.0000)] Done merging hostextinfo
[1494399524] Timing point: [0.6191 (+0.0000)] Done merging serviceextinfo
[1494399524] Timing point: [0.6367 (+0.0176)] Done propagating inherited object properties
[1494399524] Timing point: [0.8314 (+0.1947)] 0 unique / 0 total servicedependencies registered
[1494399524] Timing point: [0.8315 (+0.0001)] 0 serviceescalations registered
[1494399524] Timing point: [0.8315 (+0.0000)] 0 unique / 0 total hostdependencies registered
[1494399524] Timing point: [0.8315 (+0.0000)] 0 hostescalations registered
[1494399524] Timing point: [0.8528 (+0.0213)] Read all object data
[1494399524] Timing point: [0.8693 (+0.0165)] Initializing Event queue
[1494399524] Timing point: [0.8729 (+0.0036)] Initialized Event queue
[1494399524] Timing point: [0.8730 (+0.0001)] Loading modules
[1494399524] Timing point: [1.0832 (+0.2102)] Loaded modules
[1494399524] Timing point: [1.0832 (+0.0000)] Making first callback
[1494399524] Timing point: [1.0832 (+0.0000)] Made first callback
[1494399524] Timing point: [1.0832 (+0.0000)] Running pre flight check
[1494399526] Timing point: [2.5725 (+1.4892)] Ran pre flight check
[1494399526] Timing point: [2.5725 (+0.0000)] Caching objects
[1494399527] Timing point: [3.3357 (+0.7632)] Cached objects
[1494399530] Timing point: [6.2743 (+2.9387)] Initializing status data
[1494399530] Timing point: [6.2928 (+0.0185)] Initialized status data
[1494399530] Timing point: [6.2929 (+0.0001)] Initializing downtime data
[1494399530] Timing point: [6.2929 (+0.0000)] Initialized downtime data
[1494399530] Timing point: [6.2929 (+0.0000)] Initializing retention data
[1494399530] Timing point: [6.2931 (+0.0001)] Initialized retention data
[1494399530] Timing point: [6.2931 (+0.0000)] Reading initial state information
[1494399532] Timing point: [8.3981 (+2.1050)] Read initial state information
[1494399532] Timing point: [8.3996 (+0.0015)] Initializing comment data
[1494399532] Timing point: [8.3997 (+0.0000)] Initialized comment data
[1494399532] Timing point: [8.3997 (+0.0000)] Initializing performance data
[1494399532] Timing point: [8.3997 (+0.0000)] Initialized performance data
[1494399532] Timing point: [8.3997 (+0.0000)] Initializing check execution scheduling
[1494399532] Timing point: [8.4696 (+0.0699)] Initialized check execution scheduling
[1494399532] Timing point: [8.4697 (+0.0001)] Initializing check stats
[1494399532] Timing point: [8.4697 (+0.0000)] Initialized check stats
[1494399532] Timing point: [8.4697 (+0.0000)] Updating status data
[1494399535] Timing point: [11.3166 (+2.8469)] Updated status data
[1494399535] Timing point: [11.3167 (+0.0001)] Logging initial states
[1494399536] Timing point: [12.8815 (+1.5648)] Logged initial states
[1494399536] Timing point: [12.8818 (+0.0003)] Launching command file worker
[1494399536] Timing point: [12.8837 (+0.0019)] Launched command file worker
[1494399536] Timing point: [12.9027 (+0.0190)] Entering event execution loop","Why not? For performance reasons?
I think we close all file descriptors in the parent and all its forks when we daemonize. If we print to stdout the we would only get performance data when running in foreground.  Did I miss something and is that the behavior that we would like?",True,{}
naemon/naemon-core,https://github.com/naemon/naemon-core,181,2017-05-09T10:11:40Z,2017-05-17T08:51:22Z,2017-05-17T08:51:22Z,MERGED,True,57,30,1,https://github.com/tvestelind,Feature/more timing points,3,[],https://github.com/naemon/naemon-core/pull/181,https://github.com/tvestelind,4,https://github.com/naemon/naemon-core/pull/181#issuecomment-301700085,"I think more and tighter timing points will be good when measuring performance in large monitoring clusters.
Example of ouput:
[1494399523] Timing point: [0.0001 (+0.0001)] Reset variables
[1494399523] Timing point: [0.0002 (+0.0001)] Reading main config file
[1494399523] Timing point: [0.0007 (+0.0005)] Read main config file
[1494399523] Timing point: [0.0011 (+0.0004)] Initializing NEB module API
[1494399523] Timing point: [0.0011 (+0.0000)] Initialized NEB module API
[1494399523] Timing point: [0.0011 (+0.0000)] Initializing Query handler
[1494399523] Timing point: [0.0012 (+0.0001)] Initialized Query handler
[1494399523] Timing point: [0.0012 (+0.0000)] Initializing NERD
[1494399523] Timing point: [0.0013 (+0.0000)] Initialized NERD
[1494399523] Timing point: [0.0013 (+0.0000)] Spawning 0 workers
[1494399523] Timing point: [0.0019 (+0.0006)] Spawned 6 workers
[1494399523] Timing point: [0.0019 (+0.0001)] Connecting 0 workers
[1494399523] Timing point: [0.0036 (+0.0017)] Connected 6 workers
[1494399523] Timing point: [0.0037 (+0.0000)] Reading all object data
[1494399523] Timing point: [0.0037 (+0.0000)] Reading config data from '/opt/monitor/etc/naemon.cfg'
[1494399524] Timing point: [0.4650 (+0.4613)] Done parsing config files
[1494399524] Timing point: [0.4969 (+0.0319)] Done resolving objects
[1494399524] Timing point: [0.5022 (+0.0054)] Done recombobulating contactgroups
[1494399524] Timing point: [0.5066 (+0.0044)] Done recombobulating hostgroups
[1494399524] Timing point: [0.6150 (+0.1084)] Created 160082 services (dupes possible)
[1494399524] Timing point: [0.6189 (+0.0039)] Done recombobulating servicegroups
[1494399524] Timing point: [0.6190 (+0.0001)] Created 0 hostescalations (dupes possible)
[1494399524] Timing point: [0.6190 (+0.0000)] Created 0 serviceescalations (dupes possible)
[1494399524] Timing point: [0.6191 (+0.0000)] Done merging hostextinfo
[1494399524] Timing point: [0.6191 (+0.0000)] Done merging serviceextinfo
[1494399524] Timing point: [0.6367 (+0.0176)] Done propagating inherited object properties
[1494399524] Timing point: [0.8314 (+0.1947)] 0 unique / 0 total servicedependencies registered
[1494399524] Timing point: [0.8315 (+0.0001)] 0 serviceescalations registered
[1494399524] Timing point: [0.8315 (+0.0000)] 0 unique / 0 total hostdependencies registered
[1494399524] Timing point: [0.8315 (+0.0000)] 0 hostescalations registered
[1494399524] Timing point: [0.8528 (+0.0213)] Read all object data
[1494399524] Timing point: [0.8693 (+0.0165)] Initializing Event queue
[1494399524] Timing point: [0.8729 (+0.0036)] Initialized Event queue
[1494399524] Timing point: [0.8730 (+0.0001)] Loading modules
[1494399524] Timing point: [1.0832 (+0.2102)] Loaded modules
[1494399524] Timing point: [1.0832 (+0.0000)] Making first callback
[1494399524] Timing point: [1.0832 (+0.0000)] Made first callback
[1494399524] Timing point: [1.0832 (+0.0000)] Running pre flight check
[1494399526] Timing point: [2.5725 (+1.4892)] Ran pre flight check
[1494399526] Timing point: [2.5725 (+0.0000)] Caching objects
[1494399527] Timing point: [3.3357 (+0.7632)] Cached objects
[1494399530] Timing point: [6.2743 (+2.9387)] Initializing status data
[1494399530] Timing point: [6.2928 (+0.0185)] Initialized status data
[1494399530] Timing point: [6.2929 (+0.0001)] Initializing downtime data
[1494399530] Timing point: [6.2929 (+0.0000)] Initialized downtime data
[1494399530] Timing point: [6.2929 (+0.0000)] Initializing retention data
[1494399530] Timing point: [6.2931 (+0.0001)] Initialized retention data
[1494399530] Timing point: [6.2931 (+0.0000)] Reading initial state information
[1494399532] Timing point: [8.3981 (+2.1050)] Read initial state information
[1494399532] Timing point: [8.3996 (+0.0015)] Initializing comment data
[1494399532] Timing point: [8.3997 (+0.0000)] Initialized comment data
[1494399532] Timing point: [8.3997 (+0.0000)] Initializing performance data
[1494399532] Timing point: [8.3997 (+0.0000)] Initialized performance data
[1494399532] Timing point: [8.3997 (+0.0000)] Initializing check execution scheduling
[1494399532] Timing point: [8.4696 (+0.0699)] Initialized check execution scheduling
[1494399532] Timing point: [8.4697 (+0.0001)] Initializing check stats
[1494399532] Timing point: [8.4697 (+0.0000)] Initialized check stats
[1494399532] Timing point: [8.4697 (+0.0000)] Updating status data
[1494399535] Timing point: [11.3166 (+2.8469)] Updated status data
[1494399535] Timing point: [11.3167 (+0.0001)] Logging initial states
[1494399536] Timing point: [12.8815 (+1.5648)] Logged initial states
[1494399536] Timing point: [12.8818 (+0.0003)] Launching command file worker
[1494399536] Timing point: [12.8837 (+0.0019)] Launched command file worker
[1494399536] Timing point: [12.9027 (+0.0190)] Entering event execution loop","Bump. I think these timing points would be really useful but I need to know if I should revert the printing from log to stdout, and if so, why.",True,{}
naemon/naemon-core,https://github.com/naemon/naemon-core,181,2017-05-09T10:11:40Z,2017-05-17T08:51:22Z,2017-05-17T08:51:22Z,MERGED,True,57,30,1,https://github.com/tvestelind,Feature/more timing points,3,[],https://github.com/naemon/naemon-core/pull/181,https://github.com/mattiasr,5,https://github.com/naemon/naemon-core/pull/181#issuecomment-301701850,"I think more and tighter timing points will be good when measuring performance in large monitoring clusters.
Example of ouput:
[1494399523] Timing point: [0.0001 (+0.0001)] Reset variables
[1494399523] Timing point: [0.0002 (+0.0001)] Reading main config file
[1494399523] Timing point: [0.0007 (+0.0005)] Read main config file
[1494399523] Timing point: [0.0011 (+0.0004)] Initializing NEB module API
[1494399523] Timing point: [0.0011 (+0.0000)] Initialized NEB module API
[1494399523] Timing point: [0.0011 (+0.0000)] Initializing Query handler
[1494399523] Timing point: [0.0012 (+0.0001)] Initialized Query handler
[1494399523] Timing point: [0.0012 (+0.0000)] Initializing NERD
[1494399523] Timing point: [0.0013 (+0.0000)] Initialized NERD
[1494399523] Timing point: [0.0013 (+0.0000)] Spawning 0 workers
[1494399523] Timing point: [0.0019 (+0.0006)] Spawned 6 workers
[1494399523] Timing point: [0.0019 (+0.0001)] Connecting 0 workers
[1494399523] Timing point: [0.0036 (+0.0017)] Connected 6 workers
[1494399523] Timing point: [0.0037 (+0.0000)] Reading all object data
[1494399523] Timing point: [0.0037 (+0.0000)] Reading config data from '/opt/monitor/etc/naemon.cfg'
[1494399524] Timing point: [0.4650 (+0.4613)] Done parsing config files
[1494399524] Timing point: [0.4969 (+0.0319)] Done resolving objects
[1494399524] Timing point: [0.5022 (+0.0054)] Done recombobulating contactgroups
[1494399524] Timing point: [0.5066 (+0.0044)] Done recombobulating hostgroups
[1494399524] Timing point: [0.6150 (+0.1084)] Created 160082 services (dupes possible)
[1494399524] Timing point: [0.6189 (+0.0039)] Done recombobulating servicegroups
[1494399524] Timing point: [0.6190 (+0.0001)] Created 0 hostescalations (dupes possible)
[1494399524] Timing point: [0.6190 (+0.0000)] Created 0 serviceescalations (dupes possible)
[1494399524] Timing point: [0.6191 (+0.0000)] Done merging hostextinfo
[1494399524] Timing point: [0.6191 (+0.0000)] Done merging serviceextinfo
[1494399524] Timing point: [0.6367 (+0.0176)] Done propagating inherited object properties
[1494399524] Timing point: [0.8314 (+0.1947)] 0 unique / 0 total servicedependencies registered
[1494399524] Timing point: [0.8315 (+0.0001)] 0 serviceescalations registered
[1494399524] Timing point: [0.8315 (+0.0000)] 0 unique / 0 total hostdependencies registered
[1494399524] Timing point: [0.8315 (+0.0000)] 0 hostescalations registered
[1494399524] Timing point: [0.8528 (+0.0213)] Read all object data
[1494399524] Timing point: [0.8693 (+0.0165)] Initializing Event queue
[1494399524] Timing point: [0.8729 (+0.0036)] Initialized Event queue
[1494399524] Timing point: [0.8730 (+0.0001)] Loading modules
[1494399524] Timing point: [1.0832 (+0.2102)] Loaded modules
[1494399524] Timing point: [1.0832 (+0.0000)] Making first callback
[1494399524] Timing point: [1.0832 (+0.0000)] Made first callback
[1494399524] Timing point: [1.0832 (+0.0000)] Running pre flight check
[1494399526] Timing point: [2.5725 (+1.4892)] Ran pre flight check
[1494399526] Timing point: [2.5725 (+0.0000)] Caching objects
[1494399527] Timing point: [3.3357 (+0.7632)] Cached objects
[1494399530] Timing point: [6.2743 (+2.9387)] Initializing status data
[1494399530] Timing point: [6.2928 (+0.0185)] Initialized status data
[1494399530] Timing point: [6.2929 (+0.0001)] Initializing downtime data
[1494399530] Timing point: [6.2929 (+0.0000)] Initialized downtime data
[1494399530] Timing point: [6.2929 (+0.0000)] Initializing retention data
[1494399530] Timing point: [6.2931 (+0.0001)] Initialized retention data
[1494399530] Timing point: [6.2931 (+0.0000)] Reading initial state information
[1494399532] Timing point: [8.3981 (+2.1050)] Read initial state information
[1494399532] Timing point: [8.3996 (+0.0015)] Initializing comment data
[1494399532] Timing point: [8.3997 (+0.0000)] Initialized comment data
[1494399532] Timing point: [8.3997 (+0.0000)] Initializing performance data
[1494399532] Timing point: [8.3997 (+0.0000)] Initialized performance data
[1494399532] Timing point: [8.3997 (+0.0000)] Initializing check execution scheduling
[1494399532] Timing point: [8.4696 (+0.0699)] Initialized check execution scheduling
[1494399532] Timing point: [8.4697 (+0.0001)] Initializing check stats
[1494399532] Timing point: [8.4697 (+0.0000)] Initialized check stats
[1494399532] Timing point: [8.4697 (+0.0000)] Updating status data
[1494399535] Timing point: [11.3166 (+2.8469)] Updated status data
[1494399535] Timing point: [11.3167 (+0.0001)] Logging initial states
[1494399536] Timing point: [12.8815 (+1.5648)] Logged initial states
[1494399536] Timing point: [12.8818 (+0.0003)] Launching command file worker
[1494399536] Timing point: [12.8837 (+0.0019)] Launched command file worker
[1494399536] Timing point: [12.9027 (+0.0190)] Entering event execution loop","I think that adding/decide what value in debug they should appear on, and log to the debuglog instead.",True,{}
naemon/naemon-core,https://github.com/naemon/naemon-core,181,2017-05-09T10:11:40Z,2017-05-17T08:51:22Z,2017-05-17T08:51:22Z,MERGED,True,57,30,1,https://github.com/tvestelind,Feature/more timing points,3,[],https://github.com/naemon/naemon-core/pull/181,https://github.com/sni,6,https://github.com/naemon/naemon-core/pull/181#issuecomment-301702606,"I think more and tighter timing points will be good when measuring performance in large monitoring clusters.
Example of ouput:
[1494399523] Timing point: [0.0001 (+0.0001)] Reset variables
[1494399523] Timing point: [0.0002 (+0.0001)] Reading main config file
[1494399523] Timing point: [0.0007 (+0.0005)] Read main config file
[1494399523] Timing point: [0.0011 (+0.0004)] Initializing NEB module API
[1494399523] Timing point: [0.0011 (+0.0000)] Initialized NEB module API
[1494399523] Timing point: [0.0011 (+0.0000)] Initializing Query handler
[1494399523] Timing point: [0.0012 (+0.0001)] Initialized Query handler
[1494399523] Timing point: [0.0012 (+0.0000)] Initializing NERD
[1494399523] Timing point: [0.0013 (+0.0000)] Initialized NERD
[1494399523] Timing point: [0.0013 (+0.0000)] Spawning 0 workers
[1494399523] Timing point: [0.0019 (+0.0006)] Spawned 6 workers
[1494399523] Timing point: [0.0019 (+0.0001)] Connecting 0 workers
[1494399523] Timing point: [0.0036 (+0.0017)] Connected 6 workers
[1494399523] Timing point: [0.0037 (+0.0000)] Reading all object data
[1494399523] Timing point: [0.0037 (+0.0000)] Reading config data from '/opt/monitor/etc/naemon.cfg'
[1494399524] Timing point: [0.4650 (+0.4613)] Done parsing config files
[1494399524] Timing point: [0.4969 (+0.0319)] Done resolving objects
[1494399524] Timing point: [0.5022 (+0.0054)] Done recombobulating contactgroups
[1494399524] Timing point: [0.5066 (+0.0044)] Done recombobulating hostgroups
[1494399524] Timing point: [0.6150 (+0.1084)] Created 160082 services (dupes possible)
[1494399524] Timing point: [0.6189 (+0.0039)] Done recombobulating servicegroups
[1494399524] Timing point: [0.6190 (+0.0001)] Created 0 hostescalations (dupes possible)
[1494399524] Timing point: [0.6190 (+0.0000)] Created 0 serviceescalations (dupes possible)
[1494399524] Timing point: [0.6191 (+0.0000)] Done merging hostextinfo
[1494399524] Timing point: [0.6191 (+0.0000)] Done merging serviceextinfo
[1494399524] Timing point: [0.6367 (+0.0176)] Done propagating inherited object properties
[1494399524] Timing point: [0.8314 (+0.1947)] 0 unique / 0 total servicedependencies registered
[1494399524] Timing point: [0.8315 (+0.0001)] 0 serviceescalations registered
[1494399524] Timing point: [0.8315 (+0.0000)] 0 unique / 0 total hostdependencies registered
[1494399524] Timing point: [0.8315 (+0.0000)] 0 hostescalations registered
[1494399524] Timing point: [0.8528 (+0.0213)] Read all object data
[1494399524] Timing point: [0.8693 (+0.0165)] Initializing Event queue
[1494399524] Timing point: [0.8729 (+0.0036)] Initialized Event queue
[1494399524] Timing point: [0.8730 (+0.0001)] Loading modules
[1494399524] Timing point: [1.0832 (+0.2102)] Loaded modules
[1494399524] Timing point: [1.0832 (+0.0000)] Making first callback
[1494399524] Timing point: [1.0832 (+0.0000)] Made first callback
[1494399524] Timing point: [1.0832 (+0.0000)] Running pre flight check
[1494399526] Timing point: [2.5725 (+1.4892)] Ran pre flight check
[1494399526] Timing point: [2.5725 (+0.0000)] Caching objects
[1494399527] Timing point: [3.3357 (+0.7632)] Cached objects
[1494399530] Timing point: [6.2743 (+2.9387)] Initializing status data
[1494399530] Timing point: [6.2928 (+0.0185)] Initialized status data
[1494399530] Timing point: [6.2929 (+0.0001)] Initializing downtime data
[1494399530] Timing point: [6.2929 (+0.0000)] Initialized downtime data
[1494399530] Timing point: [6.2929 (+0.0000)] Initializing retention data
[1494399530] Timing point: [6.2931 (+0.0001)] Initialized retention data
[1494399530] Timing point: [6.2931 (+0.0000)] Reading initial state information
[1494399532] Timing point: [8.3981 (+2.1050)] Read initial state information
[1494399532] Timing point: [8.3996 (+0.0015)] Initializing comment data
[1494399532] Timing point: [8.3997 (+0.0000)] Initialized comment data
[1494399532] Timing point: [8.3997 (+0.0000)] Initializing performance data
[1494399532] Timing point: [8.3997 (+0.0000)] Initialized performance data
[1494399532] Timing point: [8.3997 (+0.0000)] Initializing check execution scheduling
[1494399532] Timing point: [8.4696 (+0.0699)] Initialized check execution scheduling
[1494399532] Timing point: [8.4697 (+0.0001)] Initializing check stats
[1494399532] Timing point: [8.4697 (+0.0000)] Initialized check stats
[1494399532] Timing point: [8.4697 (+0.0000)] Updating status data
[1494399535] Timing point: [11.3166 (+2.8469)] Updated status data
[1494399535] Timing point: [11.3167 (+0.0001)] Logging initial states
[1494399536] Timing point: [12.8815 (+1.5648)] Logged initial states
[1494399536] Timing point: [12.8818 (+0.0003)] Launching command file worker
[1494399536] Timing point: [12.8837 (+0.0019)] Launched command file worker
[1494399536] Timing point: [12.9027 (+0.0190)] Entering event execution loop","I like the possibility to run naemon during development in der foreground and get everything on the console without having to look into a second logfile. So i am fine with the timing points, but would make writing to a logfile at least optional if you really need that.",True,{}
naemon/naemon-core,https://github.com/naemon/naemon-core,181,2017-05-09T10:11:40Z,2017-05-17T08:51:22Z,2017-05-17T08:51:22Z,MERGED,True,57,30,1,https://github.com/tvestelind,Feature/more timing points,3,[],https://github.com/naemon/naemon-core/pull/181,https://github.com/tvestelind,7,https://github.com/naemon/naemon-core/pull/181#issuecomment-301705179,"I think more and tighter timing points will be good when measuring performance in large monitoring clusters.
Example of ouput:
[1494399523] Timing point: [0.0001 (+0.0001)] Reset variables
[1494399523] Timing point: [0.0002 (+0.0001)] Reading main config file
[1494399523] Timing point: [0.0007 (+0.0005)] Read main config file
[1494399523] Timing point: [0.0011 (+0.0004)] Initializing NEB module API
[1494399523] Timing point: [0.0011 (+0.0000)] Initialized NEB module API
[1494399523] Timing point: [0.0011 (+0.0000)] Initializing Query handler
[1494399523] Timing point: [0.0012 (+0.0001)] Initialized Query handler
[1494399523] Timing point: [0.0012 (+0.0000)] Initializing NERD
[1494399523] Timing point: [0.0013 (+0.0000)] Initialized NERD
[1494399523] Timing point: [0.0013 (+0.0000)] Spawning 0 workers
[1494399523] Timing point: [0.0019 (+0.0006)] Spawned 6 workers
[1494399523] Timing point: [0.0019 (+0.0001)] Connecting 0 workers
[1494399523] Timing point: [0.0036 (+0.0017)] Connected 6 workers
[1494399523] Timing point: [0.0037 (+0.0000)] Reading all object data
[1494399523] Timing point: [0.0037 (+0.0000)] Reading config data from '/opt/monitor/etc/naemon.cfg'
[1494399524] Timing point: [0.4650 (+0.4613)] Done parsing config files
[1494399524] Timing point: [0.4969 (+0.0319)] Done resolving objects
[1494399524] Timing point: [0.5022 (+0.0054)] Done recombobulating contactgroups
[1494399524] Timing point: [0.5066 (+0.0044)] Done recombobulating hostgroups
[1494399524] Timing point: [0.6150 (+0.1084)] Created 160082 services (dupes possible)
[1494399524] Timing point: [0.6189 (+0.0039)] Done recombobulating servicegroups
[1494399524] Timing point: [0.6190 (+0.0001)] Created 0 hostescalations (dupes possible)
[1494399524] Timing point: [0.6190 (+0.0000)] Created 0 serviceescalations (dupes possible)
[1494399524] Timing point: [0.6191 (+0.0000)] Done merging hostextinfo
[1494399524] Timing point: [0.6191 (+0.0000)] Done merging serviceextinfo
[1494399524] Timing point: [0.6367 (+0.0176)] Done propagating inherited object properties
[1494399524] Timing point: [0.8314 (+0.1947)] 0 unique / 0 total servicedependencies registered
[1494399524] Timing point: [0.8315 (+0.0001)] 0 serviceescalations registered
[1494399524] Timing point: [0.8315 (+0.0000)] 0 unique / 0 total hostdependencies registered
[1494399524] Timing point: [0.8315 (+0.0000)] 0 hostescalations registered
[1494399524] Timing point: [0.8528 (+0.0213)] Read all object data
[1494399524] Timing point: [0.8693 (+0.0165)] Initializing Event queue
[1494399524] Timing point: [0.8729 (+0.0036)] Initialized Event queue
[1494399524] Timing point: [0.8730 (+0.0001)] Loading modules
[1494399524] Timing point: [1.0832 (+0.2102)] Loaded modules
[1494399524] Timing point: [1.0832 (+0.0000)] Making first callback
[1494399524] Timing point: [1.0832 (+0.0000)] Made first callback
[1494399524] Timing point: [1.0832 (+0.0000)] Running pre flight check
[1494399526] Timing point: [2.5725 (+1.4892)] Ran pre flight check
[1494399526] Timing point: [2.5725 (+0.0000)] Caching objects
[1494399527] Timing point: [3.3357 (+0.7632)] Cached objects
[1494399530] Timing point: [6.2743 (+2.9387)] Initializing status data
[1494399530] Timing point: [6.2928 (+0.0185)] Initialized status data
[1494399530] Timing point: [6.2929 (+0.0001)] Initializing downtime data
[1494399530] Timing point: [6.2929 (+0.0000)] Initialized downtime data
[1494399530] Timing point: [6.2929 (+0.0000)] Initializing retention data
[1494399530] Timing point: [6.2931 (+0.0001)] Initialized retention data
[1494399530] Timing point: [6.2931 (+0.0000)] Reading initial state information
[1494399532] Timing point: [8.3981 (+2.1050)] Read initial state information
[1494399532] Timing point: [8.3996 (+0.0015)] Initializing comment data
[1494399532] Timing point: [8.3997 (+0.0000)] Initialized comment data
[1494399532] Timing point: [8.3997 (+0.0000)] Initializing performance data
[1494399532] Timing point: [8.3997 (+0.0000)] Initialized performance data
[1494399532] Timing point: [8.3997 (+0.0000)] Initializing check execution scheduling
[1494399532] Timing point: [8.4696 (+0.0699)] Initialized check execution scheduling
[1494399532] Timing point: [8.4697 (+0.0001)] Initializing check stats
[1494399532] Timing point: [8.4697 (+0.0000)] Initialized check stats
[1494399532] Timing point: [8.4697 (+0.0000)] Updating status data
[1494399535] Timing point: [11.3166 (+2.8469)] Updated status data
[1494399535] Timing point: [11.3167 (+0.0001)] Logging initial states
[1494399536] Timing point: [12.8815 (+1.5648)] Logged initial states
[1494399536] Timing point: [12.8818 (+0.0003)] Launching command file worker
[1494399536] Timing point: [12.8837 (+0.0019)] Launched command file worker
[1494399536] Timing point: [12.9027 (+0.0190)] Entering event execution loop","Sure, I can get behind that logic. I'll revert that logic for now and we'll just stick to the additional timing points.
We can make a PR later if we feel that we need the values in the log-file.",True,{}
naemon/naemon-core,https://github.com/naemon/naemon-core,181,2017-05-09T10:11:40Z,2017-05-17T08:51:22Z,2017-05-17T08:51:22Z,MERGED,True,57,30,1,https://github.com/tvestelind,Feature/more timing points,3,[],https://github.com/naemon/naemon-core/pull/181,https://github.com/sni,8,https://github.com/naemon/naemon-core/pull/181#issuecomment-301705306,"I think more and tighter timing points will be good when measuring performance in large monitoring clusters.
Example of ouput:
[1494399523] Timing point: [0.0001 (+0.0001)] Reset variables
[1494399523] Timing point: [0.0002 (+0.0001)] Reading main config file
[1494399523] Timing point: [0.0007 (+0.0005)] Read main config file
[1494399523] Timing point: [0.0011 (+0.0004)] Initializing NEB module API
[1494399523] Timing point: [0.0011 (+0.0000)] Initialized NEB module API
[1494399523] Timing point: [0.0011 (+0.0000)] Initializing Query handler
[1494399523] Timing point: [0.0012 (+0.0001)] Initialized Query handler
[1494399523] Timing point: [0.0012 (+0.0000)] Initializing NERD
[1494399523] Timing point: [0.0013 (+0.0000)] Initialized NERD
[1494399523] Timing point: [0.0013 (+0.0000)] Spawning 0 workers
[1494399523] Timing point: [0.0019 (+0.0006)] Spawned 6 workers
[1494399523] Timing point: [0.0019 (+0.0001)] Connecting 0 workers
[1494399523] Timing point: [0.0036 (+0.0017)] Connected 6 workers
[1494399523] Timing point: [0.0037 (+0.0000)] Reading all object data
[1494399523] Timing point: [0.0037 (+0.0000)] Reading config data from '/opt/monitor/etc/naemon.cfg'
[1494399524] Timing point: [0.4650 (+0.4613)] Done parsing config files
[1494399524] Timing point: [0.4969 (+0.0319)] Done resolving objects
[1494399524] Timing point: [0.5022 (+0.0054)] Done recombobulating contactgroups
[1494399524] Timing point: [0.5066 (+0.0044)] Done recombobulating hostgroups
[1494399524] Timing point: [0.6150 (+0.1084)] Created 160082 services (dupes possible)
[1494399524] Timing point: [0.6189 (+0.0039)] Done recombobulating servicegroups
[1494399524] Timing point: [0.6190 (+0.0001)] Created 0 hostescalations (dupes possible)
[1494399524] Timing point: [0.6190 (+0.0000)] Created 0 serviceescalations (dupes possible)
[1494399524] Timing point: [0.6191 (+0.0000)] Done merging hostextinfo
[1494399524] Timing point: [0.6191 (+0.0000)] Done merging serviceextinfo
[1494399524] Timing point: [0.6367 (+0.0176)] Done propagating inherited object properties
[1494399524] Timing point: [0.8314 (+0.1947)] 0 unique / 0 total servicedependencies registered
[1494399524] Timing point: [0.8315 (+0.0001)] 0 serviceescalations registered
[1494399524] Timing point: [0.8315 (+0.0000)] 0 unique / 0 total hostdependencies registered
[1494399524] Timing point: [0.8315 (+0.0000)] 0 hostescalations registered
[1494399524] Timing point: [0.8528 (+0.0213)] Read all object data
[1494399524] Timing point: [0.8693 (+0.0165)] Initializing Event queue
[1494399524] Timing point: [0.8729 (+0.0036)] Initialized Event queue
[1494399524] Timing point: [0.8730 (+0.0001)] Loading modules
[1494399524] Timing point: [1.0832 (+0.2102)] Loaded modules
[1494399524] Timing point: [1.0832 (+0.0000)] Making first callback
[1494399524] Timing point: [1.0832 (+0.0000)] Made first callback
[1494399524] Timing point: [1.0832 (+0.0000)] Running pre flight check
[1494399526] Timing point: [2.5725 (+1.4892)] Ran pre flight check
[1494399526] Timing point: [2.5725 (+0.0000)] Caching objects
[1494399527] Timing point: [3.3357 (+0.7632)] Cached objects
[1494399530] Timing point: [6.2743 (+2.9387)] Initializing status data
[1494399530] Timing point: [6.2928 (+0.0185)] Initialized status data
[1494399530] Timing point: [6.2929 (+0.0001)] Initializing downtime data
[1494399530] Timing point: [6.2929 (+0.0000)] Initialized downtime data
[1494399530] Timing point: [6.2929 (+0.0000)] Initializing retention data
[1494399530] Timing point: [6.2931 (+0.0001)] Initialized retention data
[1494399530] Timing point: [6.2931 (+0.0000)] Reading initial state information
[1494399532] Timing point: [8.3981 (+2.1050)] Read initial state information
[1494399532] Timing point: [8.3996 (+0.0015)] Initializing comment data
[1494399532] Timing point: [8.3997 (+0.0000)] Initialized comment data
[1494399532] Timing point: [8.3997 (+0.0000)] Initializing performance data
[1494399532] Timing point: [8.3997 (+0.0000)] Initialized performance data
[1494399532] Timing point: [8.3997 (+0.0000)] Initializing check execution scheduling
[1494399532] Timing point: [8.4696 (+0.0699)] Initialized check execution scheduling
[1494399532] Timing point: [8.4697 (+0.0001)] Initializing check stats
[1494399532] Timing point: [8.4697 (+0.0000)] Initialized check stats
[1494399532] Timing point: [8.4697 (+0.0000)] Updating status data
[1494399535] Timing point: [11.3166 (+2.8469)] Updated status data
[1494399535] Timing point: [11.3167 (+0.0001)] Logging initial states
[1494399536] Timing point: [12.8815 (+1.5648)] Logged initial states
[1494399536] Timing point: [12.8818 (+0.0003)] Launching command file worker
[1494399536] Timing point: [12.8837 (+0.0019)] Launched command file worker
[1494399536] Timing point: [12.9027 (+0.0190)] Entering event execution loop","sounds good to me, thanks",True,{}
naemon/naemon-core,https://github.com/naemon/naemon-core,182,2017-05-09T10:12:35Z,2017-05-11T14:06:30Z,2017-05-12T14:03:51Z,CLOSED,False,23,19,1,https://github.com/tvestelind,daemon: return faster when daemonizing,1,[],https://github.com/naemon/naemon-core/pull/182,https://github.com/tvestelind,1,https://github.com/naemon/naemon-core/pull/182,"Return after all the APIs that naemon supply are ready, i.e. the query handler
This is of course up for discussion. Do you feel that something else needs to be initialized before we consider the start complete?
Signed-off-by: Tomas Vestelind tomas.vestelind@op5.com","Return after all the APIs that naemon supply are ready, i.e. the query handler
This is of course up for discussion. Do you feel that something else needs to be initialized before we consider the start complete?
Signed-off-by: Tomas Vestelind tomas.vestelind@op5.com",True,{}
naemon/naemon-core,https://github.com/naemon/naemon-core,182,2017-05-09T10:12:35Z,2017-05-11T14:06:30Z,2017-05-12T14:03:51Z,CLOSED,False,23,19,1,https://github.com/tvestelind,daemon: return faster when daemonizing,1,[],https://github.com/naemon/naemon-core/pull/182,https://github.com/sni,2,https://github.com/naemon/naemon-core/pull/182#issuecomment-300125308,"Return after all the APIs that naemon supply are ready, i.e. the query handler
This is of course up for discussion. Do you feel that something else needs to be initialized before we consider the start complete?
Signed-off-by: Tomas Vestelind tomas.vestelind@op5.com","This means when NEB modules fail to load, the parent won't notice.",True,{}
naemon/naemon-core,https://github.com/naemon/naemon-core,182,2017-05-09T10:12:35Z,2017-05-11T14:06:30Z,2017-05-12T14:03:51Z,CLOSED,False,23,19,1,https://github.com/tvestelind,daemon: return faster when daemonizing,1,[],https://github.com/naemon/naemon-core/pull/182,https://github.com/roengstrom,3,https://github.com/naemon/naemon-core/pull/182#issuecomment-300402021,"Return after all the APIs that naemon supply are ready, i.e. the query handler
This is of course up for discussion. Do you feel that something else needs to be initialized before we consider the start complete?
Signed-off-by: Tomas Vestelind tomas.vestelind@op5.com","So this would equal the behaviour we had earlier when daemonizing, that we say we've started OK if the configuration looks okay and the fork succeeds. So I'd be okay with this behaviour.",True,{}
naemon/naemon-core,https://github.com/naemon/naemon-core,182,2017-05-09T10:12:35Z,2017-05-11T14:06:30Z,2017-05-12T14:03:51Z,CLOSED,False,23,19,1,https://github.com/tvestelind,daemon: return faster when daemonizing,1,[],https://github.com/naemon/naemon-core/pull/182,https://github.com/tvestelind,4,https://github.com/naemon/naemon-core/pull/182#issuecomment-300464824,"Return after all the APIs that naemon supply are ready, i.e. the query handler
This is of course up for discussion. Do you feel that something else needs to be initialized before we consider the start complete?
Signed-off-by: Tomas Vestelind tomas.vestelind@op5.com","I'm looking at other projects and did some tests:

Load non-existing module
Load a random file of the same type as a module
Pass an invalid argument to the proper module

Results:

Apache


Returns 1 with print out about the module not existing
Returns 1 with print out about the module not implementing the specified API
Returns 0, crashes shortly after with errors in error-log


nginx


Returns 1 with print out about the module not existing
Returns 1 with print out about the module not implementing the specified API
Returns 1 with print with argument not being correct

So, I think we can go either way. Both of the software that I've looked at at least load the modules and verify that their API is correct. I think the easiest here would to load and init the modules before saying that we've started correctly.",True,{}
naemon/naemon-core,https://github.com/naemon/naemon-core,182,2017-05-09T10:12:35Z,2017-05-11T14:06:30Z,2017-05-12T14:03:51Z,CLOSED,False,23,19,1,https://github.com/tvestelind,daemon: return faster when daemonizing,1,[],https://github.com/naemon/naemon-core/pull/182,https://github.com/sni,5,https://github.com/naemon/naemon-core/pull/182#issuecomment-300748407,"Return after all the APIs that naemon supply are ready, i.e. the query handler
This is of course up for discussion. Do you feel that something else needs to be initialized before we consider the start complete?
Signed-off-by: Tomas Vestelind tomas.vestelind@op5.com","What are we trying to achieve here? We just made the parent wait till everything has started and now we start excluding some things from that. We should either rollback the previous way and just fire/forget the daemon or accept that loading modules takes some time. We don't make anything
faster if the child signals the parent earlier, its just like before then when the child said ""yes, it might work, but we cannot be sure yet, just assume everything is ok""",True,{}
naemon/naemon-core,https://github.com/naemon/naemon-core,182,2017-05-09T10:12:35Z,2017-05-11T14:06:30Z,2017-05-12T14:03:51Z,CLOSED,False,23,19,1,https://github.com/tvestelind,daemon: return faster when daemonizing,1,[],https://github.com/naemon/naemon-core/pull/182,https://github.com/tvestelind,6,https://github.com/naemon/naemon-core/pull/182#issuecomment-300799989,"Return after all the APIs that naemon supply are ready, i.e. the query handler
This is of course up for discussion. Do you feel that something else needs to be initialized before we consider the start complete?
Signed-off-by: Tomas Vestelind tomas.vestelind@op5.com","You are right. The reason we wanted to return earlier is to speed up restarting a whole cluster using merlin. There are things that we do in the start process that takes considerable time but maybe we should work on solving that (or remove them if they are not necessary) instead of returning early.
Thank you for your feedback. I will close this PR.",True,{}
naemon/naemon-core,https://github.com/naemon/naemon-core,185,2017-06-01T12:32:49Z,2017-06-01T13:23:33Z,2017-06-01T13:23:33Z,MERGED,True,1,1,1,https://github.com/anfoe1111,external command CHANGE_SVC_MODATTR does not work,1,[],https://github.com/naemon/naemon-core/pull/185,https://github.com/anfoe1111,1,https://github.com/naemon/naemon-core/pull/185,see issue #174,see issue #174,True,{}
naemon/naemon-core,https://github.com/naemon/naemon-core,185,2017-06-01T12:32:49Z,2017-06-01T13:23:33Z,2017-06-01T13:23:33Z,MERGED,True,1,1,1,https://github.com/anfoe1111,external command CHANGE_SVC_MODATTR does not work,1,[],https://github.com/naemon/naemon-core/pull/185,https://github.com/sni,2,https://github.com/naemon/naemon-core/pull/185#issuecomment-305483950,see issue #174,"Looks good, thanks.",True,{}
naemon/naemon-core,https://github.com/naemon/naemon-core,185,2017-06-01T12:32:49Z,2017-06-01T13:23:33Z,2017-06-01T13:23:33Z,MERGED,True,1,1,1,https://github.com/anfoe1111,external command CHANGE_SVC_MODATTR does not work,1,[],https://github.com/naemon/naemon-core/pull/185,https://github.com/tvestelind,3,https://github.com/naemon/naemon-core/pull/185#issuecomment-305484273,see issue #174,If it looks good to @sni then I'm fine with this. I'll create a PR in-house.,True,{}
naemon/naemon-core,https://github.com/naemon/naemon-core,187,2017-06-08T15:18:35Z,2017-06-27T07:09:36Z,2017-06-27T07:09:36Z,MERGED,True,1,1,1,https://github.com/anfoe1111,fix external command CHANGE_SVC_MODATTR,1,[],https://github.com/naemon/naemon-core/pull/187,https://github.com/anfoe1111,1,https://github.com/naemon/naemon-core/pull/187,fix for issues #174 and #186,fix for issues #174 and #186,True,{}
naemon/naemon-core,https://github.com/naemon/naemon-core,187,2017-06-08T15:18:35Z,2017-06-27T07:09:36Z,2017-06-27T07:09:36Z,MERGED,True,1,1,1,https://github.com/anfoe1111,fix external command CHANGE_SVC_MODATTR,1,[],https://github.com/naemon/naemon-core/pull/187,https://github.com/sni,2,https://github.com/naemon/naemon-core/pull/187#issuecomment-307929663,fix for issues #174 and #186,I guess you are right :-),True,{}
naemon/naemon-core,https://github.com/naemon/naemon-core,187,2017-06-08T15:18:35Z,2017-06-27T07:09:36Z,2017-06-27T07:09:36Z,MERGED,True,1,1,1,https://github.com/anfoe1111,fix external command CHANGE_SVC_MODATTR,1,[],https://github.com/naemon/naemon-core/pull/187,https://github.com/roengstrom,3,https://github.com/naemon/naemon-core/pull/187#issuecomment-308406607,fix for issues #174 and #186,Looks okay to me. Will run this PR internally and see if we can merge it.,True,{}
naemon/naemon-core,https://github.com/naemon/naemon-core,187,2017-06-08T15:18:35Z,2017-06-27T07:09:36Z,2017-06-27T07:09:36Z,MERGED,True,1,1,1,https://github.com/anfoe1111,fix external command CHANGE_SVC_MODATTR,1,[],https://github.com/naemon/naemon-core/pull/187,https://github.com/rhagman,4,https://github.com/naemon/naemon-core/pull/187#issuecomment-308703388,fix for issues #174 and #186,Would be nice to be able to test this so that we don't break it in the future.,True,{}
naemon/naemon-core,https://github.com/naemon/naemon-core,187,2017-06-08T15:18:35Z,2017-06-27T07:09:36Z,2017-06-27T07:09:36Z,MERGED,True,1,1,1,https://github.com/anfoe1111,fix external command CHANGE_SVC_MODATTR,1,[],https://github.com/naemon/naemon-core/pull/187,https://github.com/tsjondin,5,https://github.com/naemon/naemon-core/pull/187#issuecomment-309363270,fix for issues #174 and #186,"Nice catch, though a test for the command is required for merge.",True,{}
naemon/naemon-core,https://github.com/naemon/naemon-core,187,2017-06-08T15:18:35Z,2017-06-27T07:09:36Z,2017-06-27T07:09:36Z,MERGED,True,1,1,1,https://github.com/anfoe1111,fix external command CHANGE_SVC_MODATTR,1,[],https://github.com/naemon/naemon-core/pull/187,https://github.com/sni,6,https://github.com/naemon/naemon-core/pull/187#issuecomment-309369036,fix for issues #174 and #186,"The fact that we don't have test cases for all commands should not prevent this PR from being merged, especially if its just a simple typo fixed.",True,{}
naemon/naemon-core,https://github.com/naemon/naemon-core,187,2017-06-08T15:18:35Z,2017-06-27T07:09:36Z,2017-06-27T07:09:36Z,MERGED,True,1,1,1,https://github.com/anfoe1111,fix external command CHANGE_SVC_MODATTR,1,[],https://github.com/naemon/naemon-core/pull/187,https://github.com/tvestelind,7,https://github.com/naemon/naemon-core/pull/187#issuecomment-309395071,fix for issues #174 and #186,"It's not just a typo, the functionality is changed.
I don't think it's unreasonable to demand a test for this, unless the test is very hard to write. That being said, we should help to check whether we can write such a test or not.",True,{}
naemon/naemon-core,https://github.com/naemon/naemon-core,187,2017-06-08T15:18:35Z,2017-06-27T07:09:36Z,2017-06-27T07:09:36Z,MERGED,True,1,1,1,https://github.com/anfoe1111,fix external command CHANGE_SVC_MODATTR,1,[],https://github.com/naemon/naemon-core/pull/187,https://github.com/tsjondin,8,https://github.com/naemon/naemon-core/pull/187#issuecomment-309401865,fix for issues #174 and #186,"I believe a simple test, creating a host, a service and then running this command against the service is sufficient. Not all commands have to be tested in this test, just the functionality that has changed.",True,{}
naemon/naemon-core,https://github.com/naemon/naemon-core,187,2017-06-08T15:18:35Z,2017-06-27T07:09:36Z,2017-06-27T07:09:36Z,MERGED,True,1,1,1,https://github.com/anfoe1111,fix external command CHANGE_SVC_MODATTR,1,[],https://github.com/naemon/naemon-core/pull/187,https://github.com/tvestelind,9,https://github.com/naemon/naemon-core/pull/187#issuecomment-309662440,fix for issues #174 and #186,If making a test is hard then I can go with @sni's suggestion and merge it,True,{}
naemon/naemon-core,https://github.com/naemon/naemon-core,188,2017-06-12T20:24:26Z,2018-04-04T12:41:40Z,2018-04-04T12:41:40Z,CLOSED,False,59,4,6,https://github.com/modulatix,Support for GCC 7,1,[],https://github.com/naemon/naemon-core/pull/188,https://github.com/modulatix,1,https://github.com/naemon/naemon-core/pull/188,Done via preprocessor macros so users of pre-7 GCC will notice no changes,Done via preprocessor macros so users of pre-7 GCC will notice no changes,True,{}
naemon/naemon-core,https://github.com/naemon/naemon-core,188,2017-06-12T20:24:26Z,2018-04-04T12:41:40Z,2018-04-04T12:41:40Z,CLOSED,False,59,4,6,https://github.com/modulatix,Support for GCC 7,1,[],https://github.com/naemon/naemon-core/pull/188,https://github.com/tvestelind,2,https://github.com/naemon/naemon-core/pull/188#issuecomment-309661274,Done via preprocessor macros so users of pre-7 GCC will notice no changes,"Thank you for your PR! I'm sorry but I think using the CPP to {in,ex}clude code like this make a code base very messy.
What are the GCC-errors that you are trying to solve? Is it the one about format truncation?",True,{}
naemon/naemon-core,https://github.com/naemon/naemon-core,188,2017-06-12T20:24:26Z,2018-04-04T12:41:40Z,2018-04-04T12:41:40Z,CLOSED,False,59,4,6,https://github.com/modulatix,Support for GCC 7,1,[],https://github.com/naemon/naemon-core/pull/188,https://github.com/modulatix,3,https://github.com/naemon/naemon-core/pull/188#issuecomment-309805191,Done via preprocessor macros so users of pre-7 GCC will notice no changes,"Format-truncation and switch-case fall-through
Format-truncation fix could be made global without preprocessor magic (just checking return value of sprintf is fine AFAIK), so GCC7 will not complain, but fall-through warning can be only suppressed by attribute or -Wimplicit-fallthrough=0. Or disabling -Wall",True,{}
naemon/naemon-core,https://github.com/naemon/naemon-core,188,2017-06-12T20:24:26Z,2018-04-04T12:41:40Z,2018-04-04T12:41:40Z,CLOSED,False,59,4,6,https://github.com/modulatix,Support for GCC 7,1,[],https://github.com/naemon/naemon-core/pull/188,https://github.com/tvestelind,4,https://github.com/naemon/naemon-core/pull/188#issuecomment-309809262,Done via preprocessor macros so users of pre-7 GCC will notice no changes,"@modulatix nice summary. I think that checking the return value should be the way to go. Maybe they could even be rewritten to use the nm_asprinf function.
As for the implicit fall through. I.m.o. we should go with either using -Wimplicit-fallthrough=0 or rewriting the code but not disabling -Wall. A re-write would be the nicest but that could mean a lot of work..",True,{}
naemon/naemon-core,https://github.com/naemon/naemon-core,188,2017-06-12T20:24:26Z,2018-04-04T12:41:40Z,2018-04-04T12:41:40Z,CLOSED,False,59,4,6,https://github.com/modulatix,Support for GCC 7,1,[],https://github.com/naemon/naemon-core/pull/188,https://github.com/modulatix,5,https://github.com/naemon/naemon-core/pull/188#issuecomment-309831711,Done via preprocessor macros so users of pre-7 GCC will notice no changes,@tvestelind i can try to do this hard way without preprocessor. I think i can do this before monday,True,{}
naemon/naemon-core,https://github.com/naemon/naemon-core,188,2017-06-12T20:24:26Z,2018-04-04T12:41:40Z,2018-04-04T12:41:40Z,CLOSED,False,59,4,6,https://github.com/modulatix,Support for GCC 7,1,[],https://github.com/naemon/naemon-core/pull/188,https://github.com/tvestelind,6,https://github.com/naemon/naemon-core/pull/188#issuecomment-309976719,Done via preprocessor macros so users of pre-7 GCC will notice no changes,That sounds great! 👍,True,{}
naemon/naemon-core,https://github.com/naemon/naemon-core,188,2017-06-12T20:24:26Z,2018-04-04T12:41:40Z,2018-04-04T12:41:40Z,CLOSED,False,59,4,6,https://github.com/modulatix,Support for GCC 7,1,[],https://github.com/naemon/naemon-core/pull/188,https://github.com/tvestelind,7,https://github.com/naemon/naemon-core/pull/188#issuecomment-314439554,Done via preprocessor macros so users of pre-7 GCC will notice no changes,If you need help please let me know and I'll see what I can do :),True,{}
naemon/naemon-core,https://github.com/naemon/naemon-core,188,2017-06-12T20:24:26Z,2018-04-04T12:41:40Z,2018-04-04T12:41:40Z,CLOSED,False,59,4,6,https://github.com/modulatix,Support for GCC 7,1,[],https://github.com/naemon/naemon-core/pull/188,https://github.com/jacobbaungard,8,https://github.com/naemon/naemon-core/pull/188#issuecomment-373646658,Done via preprocessor macros so users of pre-7 GCC will notice no changes,"Hi,
I have added an alternative solution at #217 . I started working on the issue (#184) before I noticed there was already a pull request up. The alternate solution uses comments instead of GCC attributes for fall through, and relies on the follow property of snprintf:

Thus, a return value of size or more means that the output was truncated
https://linux.die.net/man/3/snprintf",True,{}
naemon/naemon-core,https://github.com/naemon/naemon-core,188,2017-06-12T20:24:26Z,2018-04-04T12:41:40Z,2018-04-04T12:41:40Z,CLOSED,False,59,4,6,https://github.com/modulatix,Support for GCC 7,1,[],https://github.com/naemon/naemon-core/pull/188,https://github.com/roengstrom,9,https://github.com/naemon/naemon-core/pull/188#issuecomment-378585969,Done via preprocessor macros so users of pre-7 GCC will notice no changes,A fix for this without preprocessor was merged with PR #217,True,{}
naemon/naemon-core,https://github.com/naemon/naemon-core,190,2017-06-16T08:24:13Z,2017-06-26T06:37:29Z,2017-06-26T06:37:29Z,MERGED,True,2,2,1,https://github.com/eythori-sensa,Documentation fix for Sticky Acknowledgements,1,[],https://github.com/naemon/naemon-core/pull/190,https://github.com/eythori-sensa,1,https://github.com/naemon/naemon-core/pull/190,,,True,{}
naemon/naemon-core,https://github.com/naemon/naemon-core,190,2017-06-16T08:24:13Z,2017-06-26T06:37:29Z,2017-06-26T06:37:29Z,MERGED,True,2,2,1,https://github.com/eythori-sensa,Documentation fix for Sticky Acknowledgements,1,[],https://github.com/naemon/naemon-core/pull/190,https://github.com/tvestelind,2,https://github.com/naemon/naemon-core/pull/190#issuecomment-309660185,,"Thank you for the PR! Is this (http://www.naemon.org/documentation/developer/externalcommands/acknowledge_svc_problem.html) also wrong then? If so, how can we update it @sni ?",True,{}
naemon/naemon-core,https://github.com/naemon/naemon-core,190,2017-06-16T08:24:13Z,2017-06-26T06:37:29Z,2017-06-26T06:37:29Z,MERGED,True,2,2,1,https://github.com/eythori-sensa,Documentation fix for Sticky Acknowledgements,1,[],https://github.com/naemon/naemon-core/pull/190,https://github.com/sni,3,https://github.com/naemon/naemon-core/pull/190#issuecomment-309670529,,"There is a manual update script here:
https://github.com/naemon/naemon.github.io/blob/master/documentation/developer/externalcommands/update
Which need to be run after the PR got merged.",True,{}
naemon/naemon-core,https://github.com/naemon/naemon-core,191,2017-06-19T20:43:28Z,2017-06-26T06:37:17Z,2017-06-26T06:37:17Z,MERGED,True,4,3,1,https://github.com/dirtyren,Fixes memory leak for DATE/TIME Macros,3,[],https://github.com/naemon/naemon-core/pull/191,https://github.com/dirtyren,1,https://github.com/naemon/naemon-core/pull/191,"The DATE / TIME macros were not freeing the allocated pointer.
I had to swap mkstr for asprintf in order to be able to free the pointer outside the function.","The DATE / TIME macros were not freeing the allocated pointer.
I had to swap mkstr for asprintf in order to be able to free the pointer outside the function.",True,{}
naemon/naemon-core,https://github.com/naemon/naemon-core,191,2017-06-19T20:43:28Z,2017-06-26T06:37:17Z,2017-06-26T06:37:17Z,MERGED,True,4,3,1,https://github.com/dirtyren,Fixes memory leak for DATE/TIME Macros,3,[],https://github.com/naemon/naemon-core/pull/191,https://github.com/sni,2,https://github.com/naemon/naemon-core/pull/191#issuecomment-309671539,"The DATE / TIME macros were not freeing the allocated pointer.
I had to swap mkstr for asprintf in order to be able to free the pointer outside the function.",Also please use the nm_asprintf macro which does automatically return code checking.,True,{}
naemon/naemon-core,https://github.com/naemon/naemon-core,191,2017-06-19T20:43:28Z,2017-06-26T06:37:17Z,2017-06-26T06:37:17Z,MERGED,True,4,3,1,https://github.com/dirtyren,Fixes memory leak for DATE/TIME Macros,3,[],https://github.com/naemon/naemon-core/pull/191,https://github.com/dirtyren,3,https://github.com/naemon/naemon-core/pull/191#issuecomment-309742166,"The DATE / TIME macros were not freeing the allocated pointer.
I had to swap mkstr for asprintf in order to be able to free the pointer outside the function.","Done, changes made.",True,{}
naemon/naemon-core,https://github.com/naemon/naemon-core,194,2017-07-06T13:38:04Z,2017-07-12T08:44:47Z,2017-07-12T08:44:47Z,MERGED,True,2,2278,5,https://github.com/sni,remove shadownaemon,1,[],https://github.com/naemon/naemon-core/pull/194,https://github.com/sni,1,https://github.com/naemon/naemon-core/pull/194,"shadownaemon was meant to shadow a live copy of a real naemon process over livestatus. This has several unsolvable flaws, like timeperiods not working, etc...
meanwhile there is the LMD project which solves most of the issues, so deprecate shadownaemon here and save some code.","shadownaemon was meant to shadow a live copy of a real naemon process over livestatus. This has several unsolvable flaws, like timeperiods not working, etc...
meanwhile there is the LMD project which solves most of the issues, so deprecate shadownaemon here and save some code.",True,{}
naemon/naemon-core,https://github.com/naemon/naemon-core,194,2017-07-06T13:38:04Z,2017-07-12T08:44:47Z,2017-07-12T08:44:47Z,MERGED,True,2,2278,5,https://github.com/sni,remove shadownaemon,1,[],https://github.com/naemon/naemon-core/pull/194,https://github.com/tvestelind,2,https://github.com/naemon/naemon-core/pull/194#issuecomment-313401286,"shadownaemon was meant to shadow a live copy of a real naemon process over livestatus. This has several unsolvable flaws, like timeperiods not working, etc...
meanwhile there is the LMD project which solves most of the issues, so deprecate shadownaemon here and save some code.",👏 👍,True,{}
naemon/naemon-core,https://github.com/naemon/naemon-core,195,2017-07-06T14:18:19Z,2017-09-08T08:03:27Z,2017-09-08T08:03:52Z,MERGED,True,25,11,4,https://github.com/sni,fix newline escaping for check results,2,[],https://github.com/naemon/naemon-core/pull/195,https://github.com/sni,1,https://github.com/naemon/naemon-core/pull/195,"parse_check_outputs() escape_newlines_please argument has been ignored but it
is explicitly requested from the worker.  This leads to issues in csv output of
livestatus, since livestatus assumes the longout put to be escaped while it was
not. So it returned newlines just as is instead of escaped ones.","parse_check_outputs() escape_newlines_please argument has been ignored but it
is explicitly requested from the worker.  This leads to issues in csv output of
livestatus, since livestatus assumes the longout put to be escaped while it was
not. So it returned newlines just as is instead of escaped ones.",True,{}
naemon/naemon-core,https://github.com/naemon/naemon-core,195,2017-07-06T14:18:19Z,2017-09-08T08:03:27Z,2017-09-08T08:03:52Z,MERGED,True,25,11,4,https://github.com/sni,fix newline escaping for check results,2,[],https://github.com/naemon/naemon-core/pull/195,https://github.com/tvestelind,2,https://github.com/naemon/naemon-core/pull/195#issuecomment-314437566,"parse_check_outputs() escape_newlines_please argument has been ignored but it
is explicitly requested from the worker.  This leads to issues in csv output of
livestatus, since livestatus assumes the longout put to be escaped while it was
not. So it returned newlines just as is instead of escaped ones.","Looks good and really nice with a test for it!
Now there's only the issue with the two failing tests.",True,{}
naemon/naemon-core,https://github.com/naemon/naemon-core,195,2017-07-06T14:18:19Z,2017-09-08T08:03:27Z,2017-09-08T08:03:52Z,MERGED,True,25,11,4,https://github.com/sni,fix newline escaping for check results,2,[],https://github.com/naemon/naemon-core/pull/195,https://github.com/sni,3,https://github.com/naemon/naemon-core/pull/195#issuecomment-314438394,"parse_check_outputs() escape_newlines_please argument has been ignored but it
is explicitly requested from the worker.  This leads to issues in csv output of
livestatus, since livestatus assumes the longout put to be escaped while it was
not. So it returned newlines just as is instead of escaped ones.","Oh thanks for the pointer, didn't notice the retention tests failing :-) Will fix them...",True,{}
naemon/naemon-core,https://github.com/naemon/naemon-core,195,2017-07-06T14:18:19Z,2017-09-08T08:03:27Z,2017-09-08T08:03:52Z,MERGED,True,25,11,4,https://github.com/sni,fix newline escaping for check results,2,[],https://github.com/naemon/naemon-core/pull/195,https://github.com/sni,4,https://github.com/naemon/naemon-core/pull/195#issuecomment-314448483,"parse_check_outputs() escape_newlines_please argument has been ignored but it
is explicitly requested from the worker.  This leads to issues in csv output of
livestatus, since livestatus assumes the longout put to be escaped while it was
not. So it returned newlines just as is instead of escaped ones.",Alright,True,{}
naemon/naemon-core,https://github.com/naemon/naemon-core,195,2017-07-06T14:18:19Z,2017-09-08T08:03:27Z,2017-09-08T08:03:52Z,MERGED,True,25,11,4,https://github.com/sni,fix newline escaping for check results,2,[],https://github.com/naemon/naemon-core/pull/195,https://github.com/sni,5,https://github.com/naemon/naemon-core/pull/195#issuecomment-327421701,"parse_check_outputs() escape_newlines_please argument has been ignored but it
is explicitly requested from the worker.  This leads to issues in csv output of
livestatus, since livestatus assumes the longout put to be escaped while it was
not. So it returned newlines just as is instead of escaped ones.",Any thoughts on this?,True,{}
naemon/naemon-core,https://github.com/naemon/naemon-core,195,2017-07-06T14:18:19Z,2017-09-08T08:03:27Z,2017-09-08T08:03:52Z,MERGED,True,25,11,4,https://github.com/sni,fix newline escaping for check results,2,[],https://github.com/naemon/naemon-core/pull/195,https://github.com/tvestelind,6,https://github.com/naemon/naemon-core/pull/195#issuecomment-327422656,"parse_check_outputs() escape_newlines_please argument has been ignored but it
is explicitly requested from the worker.  This leads to issues in csv output of
livestatus, since livestatus assumes the longout put to be escaped while it was
not. So it returned newlines just as is instead of escaped ones.","Copying from our internal code review, I guess @roengstrom forgot to add it here:
The documentation for g_strescape() says

    Escapes the special characters '\b', '\f', '\n', '\r', '\t', '\v', '' and '""' in the string source by inserting a '' before them.

Should we really escape everything if escape_newlines_please is TRUE? Intuitively It seems like we should only escape newlines.

Sorry :/",True,{}
naemon/naemon-core,https://github.com/naemon/naemon-core,195,2017-07-06T14:18:19Z,2017-09-08T08:03:27Z,2017-09-08T08:03:52Z,MERGED,True,25,11,4,https://github.com/sni,fix newline escaping for check results,2,[],https://github.com/naemon/naemon-core/pull/195,https://github.com/sni,7,https://github.com/naemon/naemon-core/pull/195#issuecomment-327424185,"parse_check_outputs() escape_newlines_please argument has been ignored but it
is explicitly requested from the worker.  This leads to issues in csv output of
livestatus, since livestatus assumes the longout put to be escaped while it was
not. So it returned newlines just as is instead of escaped ones.","Maybe we can rename that function later on, for now it doesn't matter as long as we use
g_strcompress() to reverse escaping. Its also only used to safely store the state into the retention file. So i'd say it doesn't matter here.",True,{}
naemon/naemon-core,https://github.com/naemon/naemon-core,195,2017-07-06T14:18:19Z,2017-09-08T08:03:27Z,2017-09-08T08:03:52Z,MERGED,True,25,11,4,https://github.com/sni,fix newline escaping for check results,2,[],https://github.com/naemon/naemon-core/pull/195,https://github.com/roengstrom,8,https://github.com/naemon/naemon-core/pull/195#issuecomment-327428006,"parse_check_outputs() escape_newlines_please argument has been ignored but it
is explicitly requested from the worker.  This leads to issues in csv output of
livestatus, since livestatus assumes the longout put to be escaped while it was
not. So it returned newlines just as is instead of escaped ones.",I think we could rename escape_newlines_please with something like escape_output. But if you @sni and @tvestelind feel like it's fine as it is I'm okay with merging. :),True,{}
naemon/naemon-core,https://github.com/naemon/naemon-core,195,2017-07-06T14:18:19Z,2017-09-08T08:03:27Z,2017-09-08T08:03:52Z,MERGED,True,25,11,4,https://github.com/sni,fix newline escaping for check results,2,[],https://github.com/naemon/naemon-core/pull/195,https://github.com/tvestelind,9,https://github.com/naemon/naemon-core/pull/195#issuecomment-327428703,"parse_check_outputs() escape_newlines_please argument has been ignored but it
is explicitly requested from the worker.  This leads to issues in csv output of
livestatus, since livestatus assumes the longout put to be escaped while it was
not. So it returned newlines just as is instead of escaped ones.",I'm fine with whatever 👍,True,{}
naemon/naemon-core,https://github.com/naemon/naemon-core,195,2017-07-06T14:18:19Z,2017-09-08T08:03:27Z,2017-09-08T08:03:52Z,MERGED,True,25,11,4,https://github.com/sni,fix newline escaping for check results,2,[],https://github.com/naemon/naemon-core/pull/195,https://github.com/sni,10,https://github.com/naemon/naemon-core/pull/195#issuecomment-327429795,"parse_check_outputs() escape_newlines_please argument has been ignored but it
is explicitly requested from the worker.  This leads to issues in csv output of
livestatus, since livestatus assumes the longout put to be escaped while it was
not. So it returned newlines just as is instead of escaped ones.","i am fine with that as well, we can/should refactor parse_check_output later. For example the last argument newlines_are_escaped isn't used at all anywhere in the code.",True,{}
naemon/naemon-core,https://github.com/naemon/naemon-core,195,2017-07-06T14:18:19Z,2017-09-08T08:03:27Z,2017-09-08T08:03:52Z,MERGED,True,25,11,4,https://github.com/sni,fix newline escaping for check results,2,[],https://github.com/naemon/naemon-core/pull/195,https://github.com/roengstrom,11,https://github.com/naemon/naemon-core/pull/195#issuecomment-328032247,"parse_check_outputs() escape_newlines_please argument has been ignored but it
is explicitly requested from the worker.  This leads to issues in csv output of
livestatus, since livestatus assumes the longout put to be escaped while it was
not. So it returned newlines just as is instead of escaped ones.",Nice work @sni,True,{}
naemon/naemon-core,https://github.com/naemon/naemon-core,196,2017-07-24T08:59:39Z,2018-05-30T12:48:15Z,2018-05-30T12:48:16Z,CLOSED,False,20,1,6,https://github.com/sni,cleanup minor memory leaks,3,[],https://github.com/naemon/naemon-core/pull/196,https://github.com/sni,1,https://github.com/naemon/naemon-core/pull/196,to make valgrind output a bit cleaner.,to make valgrind output a bit cleaner.,True,{}
naemon/naemon-core,https://github.com/naemon/naemon-core,196,2017-07-24T08:59:39Z,2018-05-30T12:48:15Z,2018-05-30T12:48:16Z,CLOSED,False,20,1,6,https://github.com/sni,cleanup minor memory leaks,3,[],https://github.com/naemon/naemon-core/pull/196,https://github.com/sni,2,https://github.com/naemon/naemon-core/pull/196#issuecomment-393148433,to make valgrind output a bit cleaner.,already merged.,True,{}
naemon/naemon-core,https://github.com/naemon/naemon-core,199,2017-08-24T06:48:32Z,2018-05-16T12:55:45Z,2018-05-16T12:55:45Z,CLOSED,False,29,1,6,https://github.com/tvestelind,Allow circular dependencies for host graph,5,['enhancement'],https://github.com/naemon/naemon-core/pull/199,https://github.com/tvestelind,1,https://github.com/naemon/naemon-core/pull/199,"This PR enables circular dependencies for naemon's host graph.
The feature is has been manually tested but all the consequences of enabling this are not known. So this must be considered experimental.
Some are know and propagation of the following will be disabled:

Scheduling downtime
Enabling notifications
Disabling notifications","This PR enables circular dependencies for naemon's host graph.
The feature is has been manually tested but all the consequences of enabling this are not known. So this must be considered experimental.
Some are know and propagation of the following will be disabled:

Scheduling downtime
Enabling notifications
Disabling notifications",True,{}
naemon/naemon-core,https://github.com/naemon/naemon-core,199,2017-08-24T06:48:32Z,2018-05-16T12:55:45Z,2018-05-16T12:55:45Z,CLOSED,False,29,1,6,https://github.com/tvestelind,Allow circular dependencies for host graph,5,['enhancement'],https://github.com/naemon/naemon-core/pull/199,https://github.com/tvestelind,2,https://github.com/naemon/naemon-core/pull/199#issuecomment-338594743,"This PR enables circular dependencies for naemon's host graph.
The feature is has been manually tested but all the consequences of enabling this are not known. So this must be considered experimental.
Some are know and propagation of the following will be disabled:

Scheduling downtime
Enabling notifications
Disabling notifications",Any opinions or can I go ahead and merge this?,True,{}
naemon/naemon-core,https://github.com/naemon/naemon-core,199,2017-08-24T06:48:32Z,2018-05-16T12:55:45Z,2018-05-16T12:55:45Z,CLOSED,False,29,1,6,https://github.com/tvestelind,Allow circular dependencies for host graph,5,['enhancement'],https://github.com/naemon/naemon-core/pull/199,https://github.com/roengstrom,3,https://github.com/naemon/naemon-core/pull/199#issuecomment-339312169,"This PR enables circular dependencies for naemon's host graph.
The feature is has been manually tested but all the consequences of enabling this are not known. So this must be considered experimental.
Some are know and propagation of the following will be disabled:

Scheduling downtime
Enabling notifications
Disabling notifications",I'm okay with merging this. Since you need to explicitly enable it and you're given a warning in the configuration file. However we should update the documentation as well on http://www.naemon.org/documentation/usersguide/configmain.html I think,True,{}
naemon/naemon-core,https://github.com/naemon/naemon-core,199,2017-08-24T06:48:32Z,2018-05-16T12:55:45Z,2018-05-16T12:55:45Z,CLOSED,False,29,1,6,https://github.com/tvestelind,Allow circular dependencies for host graph,5,['enhancement'],https://github.com/naemon/naemon-core/pull/199,https://github.com/mattiasr,4,https://github.com/naemon/naemon-core/pull/199#issuecomment-339650815,"This PR enables circular dependencies for naemon's host graph.
The feature is has been manually tested but all the consequences of enabling this are not known. So this must be considered experimental.
Some are know and propagation of the following will be disabled:

Scheduling downtime
Enabling notifications
Disabling notifications","I don't know why, but this is now merged but the PR didn't change status to Merged as they use to.",True,{}
naemon/naemon-core,https://github.com/naemon/naemon-core,199,2017-08-24T06:48:32Z,2018-05-16T12:55:45Z,2018-05-16T12:55:45Z,CLOSED,False,29,1,6,https://github.com/tvestelind,Allow circular dependencies for host graph,5,['enhancement'],https://github.com/naemon/naemon-core/pull/199,https://github.com/jacobbaungard,5,https://github.com/naemon/naemon-core/pull/199#issuecomment-389508302,"This PR enables circular dependencies for naemon's host graph.
The feature is has been manually tested but all the consequences of enabling this are not known. So this must be considered experimental.
Some are know and propagation of the following will be disabled:

Scheduling downtime
Enabling notifications
Disabling notifications",Closing this as it has been merged: 1688701,True,{}
naemon/naemon-core,https://github.com/naemon/naemon-core,201,2017-09-08T15:08:56Z,2018-05-30T12:47:27Z,2018-05-30T12:47:28Z,CLOSED,False,1,1,1,https://github.com/nycaleksey,Make systemd startup cleaner,1,[],https://github.com/naemon/naemon-core/pull/201,https://github.com/nycaleksey,1,https://github.com/naemon/naemon-core/pull/201,Prevent naemon systemd service from complaining about existing /var/run/naemon directory.,Prevent naemon systemd service from complaining about existing /var/run/naemon directory.,True,{}
naemon/naemon-core,https://github.com/naemon/naemon-core,201,2017-09-08T15:08:56Z,2018-05-30T12:47:27Z,2018-05-30T12:47:28Z,CLOSED,False,1,1,1,https://github.com/nycaleksey,Make systemd startup cleaner,1,[],https://github.com/naemon/naemon-core/pull/201,https://github.com/sni,2,https://github.com/naemon/naemon-core/pull/201#issuecomment-328829736,Prevent naemon systemd service from complaining about existing /var/run/naemon directory.,"Thanks, looks good to me.",True,{}
naemon/naemon-core,https://github.com/naemon/naemon-core,201,2017-09-08T15:08:56Z,2018-05-30T12:47:27Z,2018-05-30T12:47:28Z,CLOSED,False,1,1,1,https://github.com/nycaleksey,Make systemd startup cleaner,1,[],https://github.com/naemon/naemon-core/pull/201,https://github.com/sni,3,https://github.com/naemon/naemon-core/pull/201#issuecomment-393148178,Prevent naemon systemd service from complaining about existing /var/run/naemon directory.,"thanks, this has been merged.",True,{}
naemon/naemon-core,https://github.com/naemon/naemon-core,202,2017-09-09T22:44:22Z,2017-09-26T15:45:56Z,2017-09-26T15:46:57Z,CLOSED,False,1,0,1,https://github.com/jframeau,cleanup memory leak,1,[],https://github.com/naemon/naemon-core/pull/202,https://github.com/jframeau,1,https://github.com/naemon/naemon-core/pull/202,to make valgrind output a bit cleaner.,to make valgrind output a bit cleaner.,True,{}
naemon/naemon-core,https://github.com/naemon/naemon-core,202,2017-09-09T22:44:22Z,2017-09-26T15:45:56Z,2017-09-26T15:46:57Z,CLOSED,False,1,0,1,https://github.com/jframeau,cleanup memory leak,1,[],https://github.com/naemon/naemon-core/pull/202,https://github.com/sni,2,https://github.com/naemon/naemon-core/pull/202#issuecomment-331928070,to make valgrind output a bit cleaner.,This patch does have some side effects at least. Retrieving the check_source via livestatus doesn't work anymore when applying this patch.,True,{}
naemon/naemon-core,https://github.com/naemon/naemon-core,202,2017-09-09T22:44:22Z,2017-09-26T15:45:56Z,2017-09-26T15:46:57Z,CLOSED,False,1,0,1,https://github.com/jframeau,cleanup memory leak,1,[],https://github.com/naemon/naemon-core/pull/202,https://github.com/sni,3,https://github.com/naemon/naemon-core/pull/202#issuecomment-332242310,to make valgrind output a bit cleaner.,"This has been fixed in Mod-Gearman. The check_source is explicitly a ""non-freeable"" const char* and Mod-Gearman used a allocated char*.",True,{}
naemon/naemon-core,https://github.com/naemon/naemon-core,206,2017-09-19T12:55:50Z,2018-05-25T07:23:25Z,2018-05-25T07:23:25Z,CLOSED,False,1106,233,39,https://github.com/sni,"decouple core, livestatus and thruk",14,[],https://github.com/naemon/naemon-core/pull/206,https://github.com/sni,1,https://github.com/naemon/naemon-core/pull/206,"Having packaging files for livestatus and core in one folder is a bad idea
because each component has different release cycles. After moving thruk into
its own packages, this is the next consequent step and decouples core and
livestatus.
This allows us to build naemon-core packages without having to rebuild livestatus.
It also makes space for a clean naemon meta package.
While cleaning up, the tools subpackage has been included in core now since
it only contains one single small binary only.
In order to make rpmbuild work, the op5build folder has to go, but vendor
specific files don't belong here anyway.
This PR also add 2 new make targets:

make rpm
make deb

which will build the corresponding packages.
Signed-off-by: Sven Nierlein sven@nierlein.de","Having packaging files for livestatus and core in one folder is a bad idea
because each component has different release cycles. After moving thruk into
its own packages, this is the next consequent step and decouples core and
livestatus.
This allows us to build naemon-core packages without having to rebuild livestatus.
It also makes space for a clean naemon meta package.
While cleaning up, the tools subpackage has been included in core now since
it only contains one single small binary only.
In order to make rpmbuild work, the op5build folder has to go, but vendor
specific files don't belong here anyway.
This PR also add 2 new make targets:

make rpm
make deb

which will build the corresponding packages.
Signed-off-by: Sven Nierlein sven@nierlein.de",True,{}
naemon/naemon-core,https://github.com/naemon/naemon-core,206,2017-09-19T12:55:50Z,2018-05-25T07:23:25Z,2018-05-25T07:23:25Z,CLOSED,False,1106,233,39,https://github.com/sni,"decouple core, livestatus and thruk",14,[],https://github.com/naemon/naemon-core/pull/206,https://github.com/sni,2,https://github.com/naemon/naemon-core/pull/206#issuecomment-339258974,"Having packaging files for livestatus and core in one folder is a bad idea
because each component has different release cycles. After moving thruk into
its own packages, this is the next consequent step and decouples core and
livestatus.
This allows us to build naemon-core packages without having to rebuild livestatus.
It also makes space for a clean naemon meta package.
While cleaning up, the tools subpackage has been included in core now since
it only contains one single small binary only.
In order to make rpmbuild work, the op5build folder has to go, but vendor
specific files don't belong here anyway.
This PR also add 2 new make targets:

make rpm
make deb

which will build the corresponding packages.
Signed-off-by: Sven Nierlein sven@nierlein.de",Any chance to have a look at this?,True,{}
naemon/naemon-core,https://github.com/naemon/naemon-core,206,2017-09-19T12:55:50Z,2018-05-25T07:23:25Z,2018-05-25T07:23:25Z,CLOSED,False,1106,233,39,https://github.com/sni,"decouple core, livestatus and thruk",14,[],https://github.com/naemon/naemon-core/pull/206,https://github.com/sni,3,https://github.com/naemon/naemon-core/pull/206#issuecomment-391965750,"Having packaging files for livestatus and core in one folder is a bad idea
because each component has different release cycles. After moving thruk into
its own packages, this is the next consequent step and decouples core and
livestatus.
This allows us to build naemon-core packages without having to rebuild livestatus.
It also makes space for a clean naemon meta package.
While cleaning up, the tools subpackage has been included in core now since
it only contains one single small binary only.
In order to make rpmbuild work, the op5build folder has to go, but vendor
specific files don't belong here anyway.
This PR also add 2 new make targets:

make rpm
make deb

which will build the corresponding packages.
Signed-off-by: Sven Nierlein sven@nierlein.de",obsolete with #230,True,{}
naemon/naemon-core,https://github.com/naemon/naemon-core,207,2017-09-20T07:44:00Z,2017-09-20T08:23:59Z,2018-12-03T13:13:21Z,MERGED,True,11,25,4,https://github.com/roengstrom,"Revert ""Merge pull request #73 in MONITOR/naemon from bugfix/fix_newl…",1,[],https://github.com/naemon/naemon-core/pull/207,https://github.com/roengstrom,1,https://github.com/naemon/naemon-core/pull/207,"…ine_escaping to master""
After the merge of this patch other naemon modules started failing, such as Merlin.
We shouldn't adapt the naemon api after another module. But instead adapt
the module in question according to the api.
This reverts commit c722025, reversing
changes made to 9ac4d20.
Signed-off-by: Robin Engström robin.engstrom@op5.com","…ine_escaping to master""
After the merge of this patch other naemon modules started failing, such as Merlin.
We shouldn't adapt the naemon api after another module. But instead adapt
the module in question according to the api.
This reverts commit c722025, reversing
changes made to 9ac4d20.
Signed-off-by: Robin Engström robin.engstrom@op5.com",True,{}
naemon/naemon-core,https://github.com/naemon/naemon-core,207,2017-09-20T07:44:00Z,2017-09-20T08:23:59Z,2018-12-03T13:13:21Z,MERGED,True,11,25,4,https://github.com/roengstrom,"Revert ""Merge pull request #73 in MONITOR/naemon from bugfix/fix_newl…",1,[],https://github.com/naemon/naemon-core/pull/207,https://github.com/rhagman,2,https://github.com/naemon/naemon-core/pull/207#issuecomment-330774477,"…ine_escaping to master""
After the merge of this patch other naemon modules started failing, such as Merlin.
We shouldn't adapt the naemon api after another module. But instead adapt
the module in question according to the api.
This reverts commit c722025, reversing
changes made to 9ac4d20.
Signed-off-by: Robin Engström robin.engstrom@op5.com","Looks good to me! The formulation of the commit sounds like we will make changes in merlin, but the affected module that we have in mind here is livestatus that we know of. But might be other modules that can be affected as well.",True,{}
naemon/naemon-core,https://github.com/naemon/naemon-core,207,2017-09-20T07:44:00Z,2017-09-20T08:23:59Z,2018-12-03T13:13:21Z,MERGED,True,11,25,4,https://github.com/roengstrom,"Revert ""Merge pull request #73 in MONITOR/naemon from bugfix/fix_newl…",1,[],https://github.com/naemon/naemon-core/pull/207,https://github.com/sni,3,https://github.com/naemon/naemon-core/pull/207#issuecomment-330776800,"…ine_escaping to master""
After the merge of this patch other naemon modules started failing, such as Merlin.
We shouldn't adapt the naemon api after another module. But instead adapt
the module in question according to the api.
This reverts commit c722025, reversing
changes made to 9ac4d20.
Signed-off-by: Robin Engström robin.engstrom@op5.com",I'd assume other modules expect naemon to behave closely like nagios which we reverted here again. I'd say Merlin should be fixed and soon and then we should fix this again.,True,{}
naemon/naemon-core,https://github.com/naemon/naemon-core,207,2017-09-20T07:44:00Z,2017-09-20T08:23:59Z,2018-12-03T13:13:21Z,MERGED,True,11,25,4,https://github.com/roengstrom,"Revert ""Merge pull request #73 in MONITOR/naemon from bugfix/fix_newl…",1,[],https://github.com/naemon/naemon-core/pull/207,https://github.com/sni,4,https://github.com/naemon/naemon-core/pull/207#issuecomment-330778209,"…ine_escaping to master""
After the merge of this patch other naemon modules started failing, such as Merlin.
We shouldn't adapt the naemon api after another module. But instead adapt
the module in question according to the api.
This reverts commit c722025, reversing
changes made to 9ac4d20.
Signed-off-by: Robin Engström robin.engstrom@op5.com","Oh, and the comment id bugfix got reverted as well? Is this on purpose?",True,{}
naemon/naemon-core,https://github.com/naemon/naemon-core,207,2017-09-20T07:44:00Z,2017-09-20T08:23:59Z,2018-12-03T13:13:21Z,MERGED,True,11,25,4,https://github.com/roengstrom,"Revert ""Merge pull request #73 in MONITOR/naemon from bugfix/fix_newl…",1,[],https://github.com/naemon/naemon-core/pull/207,https://github.com/roengstrom,5,https://github.com/naemon/naemon-core/pull/207#issuecomment-330779468,"…ine_escaping to master""
After the merge of this patch other naemon modules started failing, such as Merlin.
We shouldn't adapt the naemon api after another module. But instead adapt
the module in question according to the api.
This reverts commit c722025, reversing
changes made to 9ac4d20.
Signed-off-by: Robin Engström robin.engstrom@op5.com","I'm fine with reverting for now, fixing merlin as soon as possible and then apply this patch again. Did I understand you correctly?
Don't see the comment id bugfix being included here, am I missing it?",True,{}
naemon/naemon-core,https://github.com/naemon/naemon-core,207,2017-09-20T07:44:00Z,2017-09-20T08:23:59Z,2018-12-03T13:13:21Z,MERGED,True,11,25,4,https://github.com/roengstrom,"Revert ""Merge pull request #73 in MONITOR/naemon from bugfix/fix_newl…",1,[],https://github.com/naemon/naemon-core/pull/207,https://github.com/sni,6,https://github.com/naemon/naemon-core/pull/207#issuecomment-330780036,"…ine_escaping to master""
After the merge of this patch other naemon modules started failing, such as Merlin.
We shouldn't adapt the naemon api after another module. But instead adapt
the module in question according to the api.
This reverts commit c722025, reversing
changes made to 9ac4d20.
Signed-off-by: Robin Engström robin.engstrom@op5.com","You are right, the comment id commit was mentioned in the commit text.
And also right, please fix Merlin and the reapply the changes. I really would like to do a release soon with all those fixes.",True,{}
naemon/naemon-core,https://github.com/naemon/naemon-core,207,2017-09-20T07:44:00Z,2017-09-20T08:23:59Z,2018-12-03T13:13:21Z,MERGED,True,11,25,4,https://github.com/roengstrom,"Revert ""Merge pull request #73 in MONITOR/naemon from bugfix/fix_newl…",1,[],https://github.com/naemon/naemon-core/pull/207,https://github.com/roengstrom,7,https://github.com/naemon/naemon-core/pull/207#issuecomment-330780958,"…ine_escaping to master""
After the merge of this patch other naemon modules started failing, such as Merlin.
We shouldn't adapt the naemon api after another module. But instead adapt
the module in question according to the api.
This reverts commit c722025, reversing
changes made to 9ac4d20.
Signed-off-by: Robin Engström robin.engstrom@op5.com","Oh yeah, sorry for the confusion. Thanks, will fix Merlin asap.",True,{}
naemon/naemon-core,https://github.com/naemon/naemon-core,207,2017-09-20T07:44:00Z,2017-09-20T08:23:59Z,2018-12-03T13:13:21Z,MERGED,True,11,25,4,https://github.com/roengstrom,"Revert ""Merge pull request #73 in MONITOR/naemon from bugfix/fix_newl…",1,[],https://github.com/naemon/naemon-core/pull/207,https://github.com/nook24,8,https://github.com/naemon/naemon-core/pull/207#issuecomment-330784531,"…ine_escaping to master""
After the merge of this patch other naemon modules started failing, such as Merlin.
We shouldn't adapt the naemon api after another module. But instead adapt
the module in question according to the api.
This reverts commit c722025, reversing
changes made to 9ac4d20.
Signed-off-by: Robin Engström robin.engstrom@op5.com","Is there some documentation about API changes available?
At the moment module maintainers always need to track all commits done to Naemon, or just try and error...
But as long as g_strescape will not break json, I'm fine with this :)",True,{}
naemon/naemon-core,https://github.com/naemon/naemon-core,207,2017-09-20T07:44:00Z,2017-09-20T08:23:59Z,2018-12-03T13:13:21Z,MERGED,True,11,25,4,https://github.com/roengstrom,"Revert ""Merge pull request #73 in MONITOR/naemon from bugfix/fix_newl…",1,[],https://github.com/naemon/naemon-core/pull/207,https://github.com/sni,9,https://github.com/naemon/naemon-core/pull/207#issuecomment-330785292,"…ine_escaping to master""
After the merge of this patch other naemon modules started failing, such as Merlin.
We shouldn't adapt the naemon api after another module. But instead adapt
the module in question according to the api.
This reverts commit c722025, reversing
changes made to 9ac4d20.
Signed-off-by: Robin Engström robin.engstrom@op5.com","g_strescape won't break json, but it will change how long plugin output is stored internally (and served by livestatus). With that change, newlines will be exported as newlines, just like with the original mk-livestatus. Before that change you had to unescape the literal '\n' by yourself.",True,{}
naemon/naemon-core,https://github.com/naemon/naemon-core,207,2017-09-20T07:44:00Z,2017-09-20T08:23:59Z,2018-12-03T13:13:21Z,MERGED,True,11,25,4,https://github.com/roengstrom,"Revert ""Merge pull request #73 in MONITOR/naemon from bugfix/fix_newl…",1,[],https://github.com/naemon/naemon-core/pull/207,https://github.com/nook24,10,https://github.com/naemon/naemon-core/pull/207#issuecomment-330785884,"…ine_escaping to master""
After the merge of this patch other naemon modules started failing, such as Merlin.
We shouldn't adapt the naemon api after another module. But instead adapt
the module in question according to the api.
This reverts commit c722025, reversing
changes made to 9ac4d20.
Signed-off-by: Robin Engström robin.engstrom@op5.com","Ok good, many thanks for the information @sni",True,{}
naemon/naemon-core,https://github.com/naemon/naemon-core,207,2017-09-20T07:44:00Z,2017-09-20T08:23:59Z,2018-12-03T13:13:21Z,MERGED,True,11,25,4,https://github.com/roengstrom,"Revert ""Merge pull request #73 in MONITOR/naemon from bugfix/fix_newl…",1,[],https://github.com/naemon/naemon-core/pull/207,https://github.com/sni,11,https://github.com/naemon/naemon-core/pull/207#issuecomment-332138468,"…ine_escaping to master""
After the merge of this patch other naemon modules started failing, such as Merlin.
We shouldn't adapt the naemon api after another module. But instead adapt
the module in question according to the api.
This reverts commit c722025, reversing
changes made to 9ac4d20.
Signed-off-by: Robin Engström robin.engstrom@op5.com",Any news on this? Were you able to address this issue in merlin?,True,{}
naemon/naemon-core,https://github.com/naemon/naemon-core,207,2017-09-20T07:44:00Z,2017-09-20T08:23:59Z,2018-12-03T13:13:21Z,MERGED,True,11,25,4,https://github.com/roengstrom,"Revert ""Merge pull request #73 in MONITOR/naemon from bugfix/fix_newl…",1,[],https://github.com/naemon/naemon-core/pull/207,https://github.com/roengstrom,12,https://github.com/naemon/naemon-core/pull/207#issuecomment-332199943,"…ine_escaping to master""
After the merge of this patch other naemon modules started failing, such as Merlin.
We shouldn't adapt the naemon api after another module. But instead adapt
the module in question according to the api.
This reverts commit c722025, reversing
changes made to 9ac4d20.
Signed-off-by: Robin Engström robin.engstrom@op5.com","Have been able to work around the patch but it became a lot harder to test. So we haven't been able to write a fix with tests so nothing is merged. Will have another look at the tests for Merlin today and tomorrow so hopefully we can remerge the patch if that's ok?
By the way, did you verify the long output in notifications with your patch?",True,{}
naemon/naemon-core,https://github.com/naemon/naemon-core,207,2017-09-20T07:44:00Z,2017-09-20T08:23:59Z,2018-12-03T13:13:21Z,MERGED,True,11,25,4,https://github.com/roengstrom,"Revert ""Merge pull request #73 in MONITOR/naemon from bugfix/fix_newl…",1,[],https://github.com/naemon/naemon-core/pull/207,https://github.com/sni,13,https://github.com/naemon/naemon-core/pull/207#issuecomment-332242814,"…ine_escaping to master""
After the merge of this patch other naemon modules started failing, such as Merlin.
We shouldn't adapt the naemon api after another module. But instead adapt
the module in question according to the api.
This reverts commit c722025, reversing
changes made to 9ac4d20.
Signed-off-by: Robin Engström robin.engstrom@op5.com","By the way, did you verify the long output in notifications with your patch?

I will do that...",True,{}
naemon/naemon-core,https://github.com/naemon/naemon-core,207,2017-09-20T07:44:00Z,2017-09-20T08:23:59Z,2018-12-03T13:13:21Z,MERGED,True,11,25,4,https://github.com/roengstrom,"Revert ""Merge pull request #73 in MONITOR/naemon from bugfix/fix_newl…",1,[],https://github.com/naemon/naemon-core/pull/207,https://github.com/roengstrom,14,https://github.com/naemon/naemon-core/pull/207#issuecomment-333759843,"…ine_escaping to master""
After the merge of this patch other naemon modules started failing, such as Merlin.
We shouldn't adapt the naemon api after another module. But instead adapt
the module in question according to the api.
This reverts commit c722025, reversing
changes made to 9ac4d20.
Signed-off-by: Robin Engström robin.engstrom@op5.com",Sorry that this is taking time. We spent a couple of days during our last sprint trying to fix our tests but it has shown to be a complicated matter. We're up to our necks in urgent issues that we're working on this week and it doesn't look like we're going to be able to spend any more time on it this week. :(,True,{}
naemon/naemon-core,https://github.com/naemon/naemon-core,209,2017-09-26T09:03:46Z,2017-10-04T08:12:36Z,2017-10-04T08:12:36Z,CLOSED,False,39,12,2,https://github.com/roengstrom,Naemon starts SIGKILLing the wrong processes if the PID wrap-around,3,[],https://github.com/naemon/naemon-core/pull/209,https://github.com/roengstrom,1,https://github.com/naemon/naemon-core/pull/209,"This reverts commit 615ca32.
MON-10495
Naemon starts SIGKILLing the wrong processes if the PID wrap-around is too short.
This bug is a regression caused by fix of MON-8090.","This reverts commit 615ca32.
MON-10495
Naemon starts SIGKILLing the wrong processes if the PID wrap-around is too short.
This bug is a regression caused by fix of MON-8090.",True,{}
naemon/naemon-core,https://github.com/naemon/naemon-core,209,2017-09-26T09:03:46Z,2017-10-04T08:12:36Z,2017-10-04T08:12:36Z,CLOSED,False,39,12,2,https://github.com/roengstrom,Naemon starts SIGKILLing the wrong processes if the PID wrap-around,3,[],https://github.com/naemon/naemon-core/pull/209,https://github.com/sni,2,https://github.com/naemon/naemon-core/pull/209#issuecomment-332136005,"This reverts commit 615ca32.
MON-10495
Naemon starts SIGKILLing the wrong processes if the PID wrap-around is too short.
This bug is a regression caused by fix of MON-8090.","Why not remove killing grandchildren completly if it has sideeffects either way.
Killing them immediatly has very bad sideeffects when sending mails.
Killing them later has appearantly also sideeffects.
So why not remove the code completly. It is not naemons job to take care of badly written
scripts.",True,{}
naemon/naemon-core,https://github.com/naemon/naemon-core,209,2017-09-26T09:03:46Z,2017-10-04T08:12:36Z,2017-10-04T08:12:36Z,CLOSED,False,39,12,2,https://github.com/roengstrom,Naemon starts SIGKILLing the wrong processes if the PID wrap-around,3,[],https://github.com/naemon/naemon-core/pull/209,https://github.com/roengstrom,3,https://github.com/naemon/naemon-core/pull/209#issuecomment-332202123,"This reverts commit 615ca32.
MON-10495
Naemon starts SIGKILLing the wrong processes if the PID wrap-around is too short.
This bug is a regression caused by fix of MON-8090.","I agree with the fact that naemon shouldn't be responsible for badly written plugins. However since naemon is responsible for spawning the processes I think it is responsible if its children are going haywire.
Another approach could perhaps be to verify that the PID we're trying to kill is actually a child of the worker.",True,{}
naemon/naemon-core,https://github.com/naemon/naemon-core,209,2017-09-26T09:03:46Z,2017-10-04T08:12:36Z,2017-10-04T08:12:36Z,CLOSED,False,39,12,2,https://github.com/roengstrom,Naemon starts SIGKILLing the wrong processes if the PID wrap-around,3,[],https://github.com/naemon/naemon-core/pull/209,https://github.com/sni,4,https://github.com/naemon/naemon-core/pull/209#issuecomment-332430465,"This reverts commit 615ca32.
MON-10495
Naemon starts SIGKILLing the wrong processes if the PID wrap-around is too short.
This bug is a regression caused by fix of MON-8090.","I'd agree if this is easily possible. But it seems more complicated than expected. Sure, naemon spawns the processes, but if you have a plugin that swipes your harddrive, its not naemons fault either. The
plugins are not in control of naemon and i don't think we can reliable kill anything except the direct child.
If a plugin goes wild, it has to be fixed. You should not rely on naemon to kill it.",True,{}
naemon/naemon-core,https://github.com/naemon/naemon-core,209,2017-09-26T09:03:46Z,2017-10-04T08:12:36Z,2017-10-04T08:12:36Z,CLOSED,False,39,12,2,https://github.com/roengstrom,Naemon starts SIGKILLing the wrong processes if the PID wrap-around,3,[],https://github.com/naemon/naemon-core/pull/209,https://github.com/roengstrom,5,https://github.com/naemon/naemon-core/pull/209#issuecomment-333761214,"This reverts commit 615ca32.
MON-10495
Naemon starts SIGKILLing the wrong processes if the PID wrap-around is too short.
This bug is a regression caused by fix of MON-8090.","But it is in naemons interest to not fill up its workers and to stay alive. It's a failsafe for any arbitrary failure that might occur, not only badly written plugins, but perhaps some os related issue or something else unforeseeable.",True,{}
naemon/naemon-core,https://github.com/naemon/naemon-core,209,2017-09-26T09:03:46Z,2017-10-04T08:12:36Z,2017-10-04T08:12:36Z,CLOSED,False,39,12,2,https://github.com/roengstrom,Naemon starts SIGKILLing the wrong processes if the PID wrap-around,3,[],https://github.com/naemon/naemon-core/pull/209,https://github.com/sni,6,https://github.com/naemon/naemon-core/pull/209#issuecomment-333762410,"This reverts commit 615ca32.
MON-10495
Naemon starts SIGKILLing the wrong processes if the PID wrap-around is too short.
This bug is a regression caused by fix of MON-8090.",I am ok with this fix.,True,{}
naemon/naemon-core,https://github.com/naemon/naemon-core,209,2017-09-26T09:03:46Z,2017-10-04T08:12:36Z,2017-10-04T08:12:36Z,CLOSED,False,39,12,2,https://github.com/roengstrom,Naemon starts SIGKILLing the wrong processes if the PID wrap-around,3,[],https://github.com/naemon/naemon-core/pull/209,https://github.com/roengstrom,7,https://github.com/naemon/naemon-core/pull/209#issuecomment-334081546,"This reverts commit 615ca32.
MON-10495
Naemon starts SIGKILLing the wrong processes if the PID wrap-around is too short.
This bug is a regression caused by fix of MON-8090.",Will close this PR as a proper patch PR for the same issue is found in #211,True,{}
naemon/naemon-core,https://github.com/naemon/naemon-core,210,2017-09-26T09:46:25Z,2017-10-18T10:36:11Z,2017-10-18T10:36:11Z,MERGED,True,6,0,1,https://github.com/sni,fix memory leak when collecting performance data,1,[],https://github.com/naemon/naemon-core/pull/210,https://github.com/sni,1,https://github.com/naemon/naemon-core/pull/210,"We need to check the service_perfdata_fd/host_perfdata_fd as well when pushing
performance data onto our buffer. Checking the template does not work because
it is never NULL as it will be set to a default if its unset.
This should fix issue #200.","We need to check the service_perfdata_fd/host_perfdata_fd as well when pushing
performance data onto our buffer. Checking the template does not work because
it is never NULL as it will be set to a default if its unset.
This should fix issue #200.",True,{}
naemon/naemon-core,https://github.com/naemon/naemon-core,210,2017-09-26T09:46:25Z,2017-10-18T10:36:11Z,2017-10-18T10:36:11Z,MERGED,True,6,0,1,https://github.com/sni,fix memory leak when collecting performance data,1,[],https://github.com/naemon/naemon-core/pull/210,https://github.com/tvestelind,2,https://github.com/naemon/naemon-core/pull/210#issuecomment-335171100,"We need to check the service_perfdata_fd/host_perfdata_fd as well when pushing
performance data onto our buffer. Checking the template does not work because
it is never NULL as it will be set to a default if its unset.
This should fix issue #200.","Would it be possible to return faster from update_{service,host}_performance_data instead?
Edit: that way we can try keep the amount of logic to a minimum in each function and do the pre-checks like if the file exists, if the correct flags are set and so on in one function",True,{}
naemon/naemon-core,https://github.com/naemon/naemon-core,210,2017-09-26T09:46:25Z,2017-10-18T10:36:11Z,2017-10-18T10:36:11Z,MERGED,True,6,0,1,https://github.com/sni,fix memory leak when collecting performance data,1,[],https://github.com/naemon/naemon-core/pull/210,https://github.com/sni,3,https://github.com/naemon/naemon-core/pull/210#issuecomment-335177014,"We need to check the service_perfdata_fd/host_perfdata_fd as well when pushing
performance data onto our buffer. Checking the template does not work because
it is never NULL as it will be set to a default if its unset.
This should fix issue #200.","I thought about that as well, but i guess we need to check the filehandle in xpddefault_update_service|host_performance_data_file anyway and then we would have the same if clause twice with probably almost no benefit.
Also this is out of the scope of this bugfix, this one just plugs a memory leak :-)",True,"{'THUMBS_UP': ['https://github.com/tvestelind', 'https://github.com/roengstrom']}"
naemon/naemon-core,https://github.com/naemon/naemon-core,211,2017-10-04T08:10:40Z,2017-10-11T15:12:31Z,2017-10-11T15:12:31Z,CLOSED,False,51,0,1,https://github.com/sidhartha-sankar,Naemon starts SIGKILLing the wrong processes if the PID wrap-around is too short,6,[],https://github.com/naemon/naemon-core/pull/211,https://github.com/sidhartha-sankar,1,https://github.com/naemon/naemon-core/pull/211,"MON-10495
Naemon starts SIGKILLing the wrong processes if the PID wrap-around is too short","MON-10495
Naemon starts SIGKILLing the wrong processes if the PID wrap-around is too short",True,{}
naemon/naemon-core,https://github.com/naemon/naemon-core,211,2017-10-04T08:10:40Z,2017-10-11T15:12:31Z,2017-10-11T15:12:31Z,CLOSED,False,51,0,1,https://github.com/sidhartha-sankar,Naemon starts SIGKILLing the wrong processes if the PID wrap-around is too short,6,[],https://github.com/naemon/naemon-core/pull/211,https://github.com/sni,2,https://github.com/naemon/naemon-core/pull/211#issuecomment-334098968,"MON-10495
Naemon starts SIGKILLing the wrong processes if the PID wrap-around is too short","Thats what i mentioned already. There are probably a lot of corner cases were we will still kill the wrong process. I think its better to have 10 processes still running than killing 1 wrong one.
Also, less code, less complicated.",True,{}
naemon/naemon-core,https://github.com/naemon/naemon-core,211,2017-10-04T08:10:40Z,2017-10-11T15:12:31Z,2017-10-11T15:12:31Z,CLOSED,False,51,0,1,https://github.com/sidhartha-sankar,Naemon starts SIGKILLing the wrong processes if the PID wrap-around is too short,6,[],https://github.com/naemon/naemon-core/pull/211,https://github.com/sidhartha-sankar,3,https://github.com/naemon/naemon-core/pull/211#issuecomment-334101848,"MON-10495
Naemon starts SIGKILLing the wrong processes if the PID wrap-around is too short","@sni Currently (existing naemon code) we are killing so many wrong processes.
I think this fix would restrict that to a large extend.",True,{}
naemon/naemon-core,https://github.com/naemon/naemon-core,211,2017-10-04T08:10:40Z,2017-10-11T15:12:31Z,2017-10-11T15:12:31Z,CLOSED,False,51,0,1,https://github.com/sidhartha-sankar,Naemon starts SIGKILLing the wrong processes if the PID wrap-around is too short,6,[],https://github.com/naemon/naemon-core/pull/211,https://github.com/roengstrom,4,https://github.com/naemon/naemon-core/pull/211#issuecomment-334105248,"MON-10495
Naemon starts SIGKILLing the wrong processes if the PID wrap-around is too short",My opinion is that we need this fail safe as a self preservation mechanism. I also believe a lot of users depend on having a timeout for their plugins. But I don't think we should be sending SIGKILL to a PID which isn't in the process list upon check. Since some users may be running several hundred checks per second the PID count will be increasing very fast and the PID may in fact exist upon sending the SIGKILL after the PID check.,True,{}
naemon/naemon-core,https://github.com/naemon/naemon-core,211,2017-10-04T08:10:40Z,2017-10-11T15:12:31Z,2017-10-11T15:12:31Z,CLOSED,False,51,0,1,https://github.com/sidhartha-sankar,Naemon starts SIGKILLing the wrong processes if the PID wrap-around is too short,6,[],https://github.com/naemon/naemon-core/pull/211,https://github.com/sni,5,https://github.com/naemon/naemon-core/pull/211#issuecomment-334108327,"MON-10495
Naemon starts SIGKILLing the wrong processes if the PID wrap-around is too short","Sure, we should definitly kill the direct child of a worker (the check plugin) when hitting the timeout.",True,{}
naemon/naemon-core,https://github.com/naemon/naemon-core,211,2017-10-04T08:10:40Z,2017-10-11T15:12:31Z,2017-10-11T15:12:31Z,CLOSED,False,51,0,1,https://github.com/sidhartha-sankar,Naemon starts SIGKILLing the wrong processes if the PID wrap-around is too short,6,[],https://github.com/naemon/naemon-core/pull/211,https://github.com/tvestelind,6,https://github.com/naemon/naemon-core/pull/211#issuecomment-335423385,"MON-10495
Naemon starts SIGKILLing the wrong processes if the PID wrap-around is too short",Looks good to me 👍,True,{}
naemon/naemon-core,https://github.com/naemon/naemon-core,214,2017-12-07T08:11:42Z,2018-12-07T16:04:11Z,2018-12-07T16:04:11Z,CLOSED,False,18,7,3,https://github.com/felixmarch,Perform nudging to reschedule at 1 second next interval,2,[],https://github.com/naemon/naemon-core/pull/214,https://github.com/felixmarch,1,https://github.com/naemon/naemon-core/pull/214,"When the max_concurrent_checks is set, currently the naemon just simply drops the check and reschedules it to next interval when the retry_interval is not defined (src/naemon/checks_service.c:145)
As a result, if the next interval is 1 minute, the next check will be scheduled to next 1 minute slot, causing the system to be idling for at most 1 minute.
If we have 200 service checks, then the delay will be very long, up to 200 minutes. With this, critical service check will not be able to run immediately when the system is back to idle and has to lose 1 minute interval
So, we propose to make use of ""nudge_seconds"" variable to perform reschedule in one second interval attempt.
Easy test would be to create 200 service checks and set max_concurrent_checks to 1 second. With max_concurrent_checks 1 second, the 200 service checks has to be able to run 200 checks in serial mode without making the system idle for 1 minute during that serial execution","When the max_concurrent_checks is set, currently the naemon just simply drops the check and reschedules it to next interval when the retry_interval is not defined (src/naemon/checks_service.c:145)
As a result, if the next interval is 1 minute, the next check will be scheduled to next 1 minute slot, causing the system to be idling for at most 1 minute.
If we have 200 service checks, then the delay will be very long, up to 200 minutes. With this, critical service check will not be able to run immediately when the system is back to idle and has to lose 1 minute interval
So, we propose to make use of ""nudge_seconds"" variable to perform reschedule in one second interval attempt.
Easy test would be to create 200 service checks and set max_concurrent_checks to 1 second. With max_concurrent_checks 1 second, the 200 service checks has to be able to run 200 checks in serial mode without making the system idle for 1 minute during that serial execution",True,{}
naemon/naemon-core,https://github.com/naemon/naemon-core,214,2017-12-07T08:11:42Z,2018-12-07T16:04:11Z,2018-12-07T16:04:11Z,CLOSED,False,18,7,3,https://github.com/felixmarch,Perform nudging to reschedule at 1 second next interval,2,[],https://github.com/naemon/naemon-core/pull/214,https://github.com/sni,2,https://github.com/naemon/naemon-core/pull/214#issuecomment-350066648,"When the max_concurrent_checks is set, currently the naemon just simply drops the check and reschedules it to next interval when the retry_interval is not defined (src/naemon/checks_service.c:145)
As a result, if the next interval is 1 minute, the next check will be scheduled to next 1 minute slot, causing the system to be idling for at most 1 minute.
If we have 200 service checks, then the delay will be very long, up to 200 minutes. With this, critical service check will not be able to run immediately when the system is back to idle and has to lose 1 minute interval
So, we propose to make use of ""nudge_seconds"" variable to perform reschedule in one second interval attempt.
Easy test would be to create 200 service checks and set max_concurrent_checks to 1 second. With max_concurrent_checks 1 second, the 200 service checks has to be able to run 200 checks in serial mode without making the system idle for 1 minute during that serial execution","I like it, till now, max_concurrent_checks was quite useless. I haven't looked deeper into your changes yet, but i would lower the loglevel to debug.",True,{}
naemon/naemon-core,https://github.com/naemon/naemon-core,214,2017-12-07T08:11:42Z,2018-12-07T16:04:11Z,2018-12-07T16:04:11Z,CLOSED,False,18,7,3,https://github.com/felixmarch,Perform nudging to reschedule at 1 second next interval,2,[],https://github.com/naemon/naemon-core/pull/214,https://github.com/ageric,3,https://github.com/naemon/naemon-core/pull/214#issuecomment-351142316,"When the max_concurrent_checks is set, currently the naemon just simply drops the check and reschedules it to next interval when the retry_interval is not defined (src/naemon/checks_service.c:145)
As a result, if the next interval is 1 minute, the next check will be scheduled to next 1 minute slot, causing the system to be idling for at most 1 minute.
If we have 200 service checks, then the delay will be very long, up to 200 minutes. With this, critical service check will not be able to run immediately when the system is back to idle and has to lose 1 minute interval
So, we propose to make use of ""nudge_seconds"" variable to perform reschedule in one second interval attempt.
Easy test would be to create 200 service checks and set max_concurrent_checks to 1 second. With max_concurrent_checks 1 second, the 200 service checks has to be able to run 200 checks in serial mode without making the system idle for 1 minute during that serial execution","It might be a while since I looked at this, but max_concurrent_checks doesn't track seconds (and never has). It's simply the max number of checks that can be run at any given time. If you set it to one, you're effectively configuring your system to only ever run one check at any given time. If any of your checks then take a long time to complete, that will cause long latencies for all other checks in your system.
You should almost always leave max_concurrent_checks at 0, which is taken to mean ""spawn checks as required to meet the scheduling requirements"".",True,{}
naemon/naemon-core,https://github.com/naemon/naemon-core,214,2017-12-07T08:11:42Z,2018-12-07T16:04:11Z,2018-12-07T16:04:11Z,CLOSED,False,18,7,3,https://github.com/felixmarch,Perform nudging to reschedule at 1 second next interval,2,[],https://github.com/naemon/naemon-core/pull/214,https://github.com/ageric,4,https://github.com/naemon/naemon-core/pull/214#issuecomment-351143496,"When the max_concurrent_checks is set, currently the naemon just simply drops the check and reschedules it to next interval when the retry_interval is not defined (src/naemon/checks_service.c:145)
As a result, if the next interval is 1 minute, the next check will be scheduled to next 1 minute slot, causing the system to be idling for at most 1 minute.
If we have 200 service checks, then the delay will be very long, up to 200 minutes. With this, critical service check will not be able to run immediately when the system is back to idle and has to lose 1 minute interval
So, we propose to make use of ""nudge_seconds"" variable to perform reschedule in one second interval attempt.
Easy test would be to create 200 service checks and set max_concurrent_checks to 1 second. With max_concurrent_checks 1 second, the 200 service checks has to be able to run 200 checks in serial mode without making the system idle for 1 minute during that serial execution","On a secondary note, adding runtime tracking variables for ""engine state"" to objects breaks the ABI, and so requires a major release, as all modules (livestatus, merlin, etc) need to be updated and/or recompiled in order to not break. A compatible way of doing it would be to introduce a data segment located hash table (ie, one declared ""static"" in the file where it's used) which tracks if the check is being nudged or not.",True,{}
naemon/naemon-core,https://github.com/naemon/naemon-core,214,2017-12-07T08:11:42Z,2018-12-07T16:04:11Z,2018-12-07T16:04:11Z,CLOSED,False,18,7,3,https://github.com/felixmarch,Perform nudging to reschedule at 1 second next interval,2,[],https://github.com/naemon/naemon-core/pull/214,https://github.com/nook24,5,https://github.com/naemon/naemon-core/pull/214#issuecomment-351188383,"When the max_concurrent_checks is set, currently the naemon just simply drops the check and reschedules it to next interval when the retry_interval is not defined (src/naemon/checks_service.c:145)
As a result, if the next interval is 1 minute, the next check will be scheduled to next 1 minute slot, causing the system to be idling for at most 1 minute.
If we have 200 service checks, then the delay will be very long, up to 200 minutes. With this, critical service check will not be able to run immediately when the system is back to idle and has to lose 1 minute interval
So, we propose to make use of ""nudge_seconds"" variable to perform reschedule in one second interval attempt.
Easy test would be to create 200 service checks and set max_concurrent_checks to 1 second. With max_concurrent_checks 1 second, the 200 service checks has to be able to run 200 checks in serial mode without making the system idle for 1 minute during that serial execution","Could you please document the required changes for NEB modules? It's very hard for module developers that there is no change log and documentation. We need to track all source code changes, to bring our modules back to live.",True,{}
naemon/naemon-core,https://github.com/naemon/naemon-core,214,2017-12-07T08:11:42Z,2018-12-07T16:04:11Z,2018-12-07T16:04:11Z,CLOSED,False,18,7,3,https://github.com/felixmarch,Perform nudging to reschedule at 1 second next interval,2,[],https://github.com/naemon/naemon-core/pull/214,https://github.com/felixmarch,6,https://github.com/naemon/naemon-core/pull/214#issuecomment-353233657,"When the max_concurrent_checks is set, currently the naemon just simply drops the check and reschedules it to next interval when the retry_interval is not defined (src/naemon/checks_service.c:145)
As a result, if the next interval is 1 minute, the next check will be scheduled to next 1 minute slot, causing the system to be idling for at most 1 minute.
If we have 200 service checks, then the delay will be very long, up to 200 minutes. With this, critical service check will not be able to run immediately when the system is back to idle and has to lose 1 minute interval
So, we propose to make use of ""nudge_seconds"" variable to perform reschedule in one second interval attempt.
Easy test would be to create 200 service checks and set max_concurrent_checks to 1 second. With max_concurrent_checks 1 second, the 200 service checks has to be able to run 200 checks in serial mode without making the system idle for 1 minute during that serial execution","Hello @ageric . Thanks for the highlight and comment.
I am not clear on what you mean on introducing a data segment located hash table.
Is your highlight about the new variable nudging_in_progress in the objects_service.h?
If so, would it be fine to move it to checks_service.c like this?
diff --git a/src/naemon/checks_service.c b/src/naemon/checks_service.c
index 41d0a28..0ce417b 100644
--- a/src/naemon/checks_service.c
+++ b/src/naemon/checks_service.c
@@ -40,6 +40,8 @@ static void check_for_orphaned_services_eventhandler(struct nm_event_execution_p
 /* Status functions, immutable */
 static int is_service_result_fresh(service *, time_t, int);

+/* nudging status */
+static int nudging_in_progress = 0;

 /******************************************************************************
  *******************************  INIT METHODS  *******************************
diff --git a/src/naemon/objects_service.c b/src/naemon/objects_service.c
index f9c396f..d15d66d 100644
--- a/src/naemon/objects_service.c
+++ b/src/naemon/objects_service.c
@@ -13,7 +13,6 @@
 #include ""lib/libnaemon.h""

 static GHashTable *service_hash_table;
-int nudging_in_progress = 0;
 service *service_list = NULL;
 service **service_ary = NULL;

diff --git a/src/naemon/objects_service.h b/src/naemon/objects_service.h
index 1159f05..3e33280 100644
--- a/src/naemon/objects_service.h
+++ b/src/naemon/objects_service.h
@@ -19,7 +19,6 @@ typedef struct service service;
 struct servicemember;
 typedef struct servicesmember servicesmember;

-extern int nudging_in_progress;
 extern struct service *service_list;
 extern struct service **service_ary;

The reason we set max_concurrent_checks not to 0, is because we are getting high load average issue caused by the concurrent checks.
Your colleague Jonatan Sundeen found limiting the max_concurrent_checks to 10 could resolve the high load average issue.
However, after applying max_concurrent_checks, we noticed new issue that many critical checks were missing due to this long latency issue.
So, we'd like to have this max_concurrent_checks issue to be fixed as soon as possible because we rely on this max_concurrent_checks to prevent high load average in our environment.
Thanks.",True,{}
naemon/naemon-core,https://github.com/naemon/naemon-core,214,2017-12-07T08:11:42Z,2018-12-07T16:04:11Z,2018-12-07T16:04:11Z,CLOSED,False,18,7,3,https://github.com/felixmarch,Perform nudging to reschedule at 1 second next interval,2,[],https://github.com/naemon/naemon-core/pull/214,https://github.com/ageric,7,https://github.com/naemon/naemon-core/pull/214#issuecomment-354300295,"When the max_concurrent_checks is set, currently the naemon just simply drops the check and reschedules it to next interval when the retry_interval is not defined (src/naemon/checks_service.c:145)
As a result, if the next interval is 1 minute, the next check will be scheduled to next 1 minute slot, causing the system to be idling for at most 1 minute.
If we have 200 service checks, then the delay will be very long, up to 200 minutes. With this, critical service check will not be able to run immediately when the system is back to idle and has to lose 1 minute interval
So, we propose to make use of ""nudge_seconds"" variable to perform reschedule in one second interval attempt.
Easy test would be to create 200 service checks and set max_concurrent_checks to 1 second. With max_concurrent_checks 1 second, the 200 service checks has to be able to run 200 checks in serial mode without making the system idle for 1 minute during that serial execution","What I mean is that adding variables to the object structures causes modules to no longer work, so adding a variable to the service object means we have to bump the object ABI version, which is troublesome. Also, I don't quite see the point behind this patch. Checks should be automagically nudged if the max_concurrent_checks limit is reached anyway. If they're not, then it should be fixed globally, and not on a per-service level.
That aside though; If you're getting high load average issues with max_concurrent_checks set to 0, your hardware is underdimensioned for the network you want to monitor and the frequency with which you want to run the checks. Try increasing check_interval instead.
You should also be aware that a system with 16 cpu's (for example) can safely have a high load average and still not be running at high capacity.
My thinking is therefore that this patch has come about as a fix to a bad solution for a different problem. If I'm correct, that means that fixing the original problem the correct way makes this patch unneeded.",True,{}
naemon/naemon-core,https://github.com/naemon/naemon-core,214,2017-12-07T08:11:42Z,2018-12-07T16:04:11Z,2018-12-07T16:04:11Z,CLOSED,False,18,7,3,https://github.com/felixmarch,Perform nudging to reschedule at 1 second next interval,2,[],https://github.com/naemon/naemon-core/pull/214,https://github.com/felixmarch,8,https://github.com/naemon/naemon-core/pull/214#issuecomment-354428297,"When the max_concurrent_checks is set, currently the naemon just simply drops the check and reschedules it to next interval when the retry_interval is not defined (src/naemon/checks_service.c:145)
As a result, if the next interval is 1 minute, the next check will be scheduled to next 1 minute slot, causing the system to be idling for at most 1 minute.
If we have 200 service checks, then the delay will be very long, up to 200 minutes. With this, critical service check will not be able to run immediately when the system is back to idle and has to lose 1 minute interval
So, we propose to make use of ""nudge_seconds"" variable to perform reschedule in one second interval attempt.
Easy test would be to create 200 service checks and set max_concurrent_checks to 1 second. With max_concurrent_checks 1 second, the 200 service checks has to be able to run 200 checks in serial mode without making the system idle for 1 minute during that serial execution","Hello @ageric
Can you please propose how to fix it globally, and not on a per-service level? Does adding static variable in checks_service.c also need to bump the object ABI version?
We hope we can fix it on our side, but currently not able to. There are some critical check our customers need to perform and do not allow us to increase check_interval. Moreover, the script is just simple check (using generic snmpwalk/snmpget command), running over 100+ devices. We are not able to control hardware/infrastructure where network can sometimes slow down the check.",True,{}
naemon/naemon-core,https://github.com/naemon/naemon-core,214,2017-12-07T08:11:42Z,2018-12-07T16:04:11Z,2018-12-07T16:04:11Z,CLOSED,False,18,7,3,https://github.com/felixmarch,Perform nudging to reschedule at 1 second next interval,2,[],https://github.com/naemon/naemon-core/pull/214,https://github.com/sni,9,https://github.com/naemon/naemon-core/pull/214#issuecomment-354429610,"When the max_concurrent_checks is set, currently the naemon just simply drops the check and reschedules it to next interval when the retry_interval is not defined (src/naemon/checks_service.c:145)
As a result, if the next interval is 1 minute, the next check will be scheduled to next 1 minute slot, causing the system to be idling for at most 1 minute.
If we have 200 service checks, then the delay will be very long, up to 200 minutes. With this, critical service check will not be able to run immediately when the system is back to idle and has to lose 1 minute interval
So, we propose to make use of ""nudge_seconds"" variable to perform reschedule in one second interval attempt.
Easy test would be to create 200 service checks and set max_concurrent_checks to 1 second. With max_concurrent_checks 1 second, the 200 service checks has to be able to run 200 checks in serial mode without making the system idle for 1 minute during that serial execution","For the time being, you could have a look at Mod-Gearman (https://mod-gearman.org/) addon which makes it possible to serially execute checks either globally or on a servicegroup basis.",True,{}
naemon/naemon-core,https://github.com/naemon/naemon-core,214,2017-12-07T08:11:42Z,2018-12-07T16:04:11Z,2018-12-07T16:04:11Z,CLOSED,False,18,7,3,https://github.com/felixmarch,Perform nudging to reschedule at 1 second next interval,2,[],https://github.com/naemon/naemon-core/pull/214,https://github.com/ageric,10,https://github.com/naemon/naemon-core/pull/214#issuecomment-355025934,"When the max_concurrent_checks is set, currently the naemon just simply drops the check and reschedules it to next interval when the retry_interval is not defined (src/naemon/checks_service.c:145)
As a result, if the next interval is 1 minute, the next check will be scheduled to next 1 minute slot, causing the system to be idling for at most 1 minute.
If we have 200 service checks, then the delay will be very long, up to 200 minutes. With this, critical service check will not be able to run immediately when the system is back to idle and has to lose 1 minute interval
So, we propose to make use of ""nudge_seconds"" variable to perform reschedule in one second interval attempt.
Easy test would be to create 200 service checks and set max_concurrent_checks to 1 second. With max_concurrent_checks 1 second, the 200 service checks has to be able to run 200 checks in serial mode without making the system idle for 1 minute during that serial execution","The most likely culprit for the high load is inefficient plugins though. It's one of the few drawbacks of the Naemon/Nagios way of running checks, that resourcehungry plugins can make the entire machine sluggish. SNMP plugins written in perl are notorious for this, so if that's what you're using you should probably switch to the ones found in https://github.com/ageric/plugins-op5-check_snmp instead.
It's an SNMP shim written in C and a bunch of plugins coded around it that I wrote while I was working at op5, simply because all script languages we tried were absolutely horrible with regards to performance.
If you're using plugins that read and write files as part of their execution (like checking traffic on switches etc) you should also make sure you disable the 'modify on read' flag for the mounted disk. That will cut your disk write accesses essentially in half.
So swapping from inefficient plugins to efficient ones would be step one. Once that's done, setting max_concurrent_checks back to 0 (ie, unlimited) should be doable. If it's not, we should start looking into modifying the core rescheduling logic.
You must understand that keeping the check schedule when the machine is underdimensioned for it is an absolute impossibility though. You just can't run 1,000,000 real checks per minute on a raspberry pi for instance, so if max_concurrent_checks=0 isn't working for you now, it's not gonna work in the future with just minor patches to the core. Me and Sven (@sni) both put a lot of effort into maximizing the efficiency of the core a few years back, so I'm pretty sure that whatever's hogging resources on your system lies somewhere else, and nudging checks one second at a time isn't going to solve your problem.",True,{}
naemon/naemon-core,https://github.com/naemon/naemon-core,214,2017-12-07T08:11:42Z,2018-12-07T16:04:11Z,2018-12-07T16:04:11Z,CLOSED,False,18,7,3,https://github.com/felixmarch,Perform nudging to reschedule at 1 second next interval,2,[],https://github.com/naemon/naemon-core/pull/214,https://github.com/felixmarch,11,https://github.com/naemon/naemon-core/pull/214#issuecomment-356477751,"When the max_concurrent_checks is set, currently the naemon just simply drops the check and reschedules it to next interval when the retry_interval is not defined (src/naemon/checks_service.c:145)
As a result, if the next interval is 1 minute, the next check will be scheduled to next 1 minute slot, causing the system to be idling for at most 1 minute.
If we have 200 service checks, then the delay will be very long, up to 200 minutes. With this, critical service check will not be able to run immediately when the system is back to idle and has to lose 1 minute interval
So, we propose to make use of ""nudge_seconds"" variable to perform reschedule in one second interval attempt.
Easy test would be to create 200 service checks and set max_concurrent_checks to 1 second. With max_concurrent_checks 1 second, the 200 service checks has to be able to run 200 checks in serial mode without making the system idle for 1 minute during that serial execution","Thank you @ageric and @sni
Will try your plugins-op5-check_snmp and mod_gearman from @sni
It would be good if op5 can include this mod_gearman as well for its customers
Last week I did try to install mod_gearman but getting ""unrecognized service"" when starting the service. Was having some time to figure it out and learn similar issue reported at the forum (https://groups.google.com/forum/#!topic/mod_gearman/rueEiWpK8e8)
Eventually I realized I needed to run ""yum install gearmand-server"".",True,{}
naemon/naemon-core,https://github.com/naemon/naemon-core,214,2017-12-07T08:11:42Z,2018-12-07T16:04:11Z,2018-12-07T16:04:11Z,CLOSED,False,18,7,3,https://github.com/felixmarch,Perform nudging to reschedule at 1 second next interval,2,[],https://github.com/naemon/naemon-core/pull/214,https://github.com/ageric,12,https://github.com/naemon/naemon-core/pull/214#issuecomment-358462462,"When the max_concurrent_checks is set, currently the naemon just simply drops the check and reschedules it to next interval when the retry_interval is not defined (src/naemon/checks_service.c:145)
As a result, if the next interval is 1 minute, the next check will be scheduled to next 1 minute slot, causing the system to be idling for at most 1 minute.
If we have 200 service checks, then the delay will be very long, up to 200 minutes. With this, critical service check will not be able to run immediately when the system is back to idle and has to lose 1 minute interval
So, we propose to make use of ""nudge_seconds"" variable to perform reschedule in one second interval attempt.
Easy test would be to create 200 service checks and set max_concurrent_checks to 1 second. With max_concurrent_checks 1 second, the 200 service checks has to be able to run 200 checks in serial mode without making the system idle for 1 minute during that serial execution",Please let us know how it pans out. It will also help whoever's looking to solve similar problems in the future,True,{}
naemon/naemon-core,https://github.com/naemon/naemon-core,214,2017-12-07T08:11:42Z,2018-12-07T16:04:11Z,2018-12-07T16:04:11Z,CLOSED,False,18,7,3,https://github.com/felixmarch,Perform nudging to reschedule at 1 second next interval,2,[],https://github.com/naemon/naemon-core/pull/214,https://github.com/jacobbaungard,13,https://github.com/naemon/naemon-core/pull/214#issuecomment-444499610,"When the max_concurrent_checks is set, currently the naemon just simply drops the check and reschedules it to next interval when the retry_interval is not defined (src/naemon/checks_service.c:145)
As a result, if the next interval is 1 minute, the next check will be scheduled to next 1 minute slot, causing the system to be idling for at most 1 minute.
If we have 200 service checks, then the delay will be very long, up to 200 minutes. With this, critical service check will not be able to run immediately when the system is back to idle and has to lose 1 minute interval
So, we propose to make use of ""nudge_seconds"" variable to perform reschedule in one second interval attempt.
Easy test would be to create 200 service checks and set max_concurrent_checks to 1 second. With max_concurrent_checks 1 second, the 200 service checks has to be able to run 200 checks in serial mode without making the system idle for 1 minute during that serial execution",I wonder if we should close this one?,True,{}
naemon/naemon-core,https://github.com/naemon/naemon-core,215,2018-02-10T21:19:17Z,2018-05-30T12:47:42Z,2018-05-30T12:47:42Z,CLOSED,False,3,3,2,https://github.com/sni,do not escape double quotes,1,[],https://github.com/naemon/naemon-core/pull/215,https://github.com/sni,1,https://github.com/naemon/naemon-core/pull/215,"they would end up escaped in the pluginoutput and break html output for example.
Signed-off-by: Sven Nierlein sven@nierlein.de","they would end up escaped in the pluginoutput and break html output for example.
Signed-off-by: Sven Nierlein sven@nierlein.de",True,{}
naemon/naemon-core,https://github.com/naemon/naemon-core,215,2018-02-10T21:19:17Z,2018-05-30T12:47:42Z,2018-05-30T12:47:42Z,CLOSED,False,3,3,2,https://github.com/sni,do not escape double quotes,1,[],https://github.com/naemon/naemon-core/pull/215,https://github.com/sni,2,https://github.com/naemon/naemon-core/pull/215#issuecomment-364693992,"they would end up escaped in the pluginoutput and break html output for example.
Signed-off-by: Sven Nierlein sven@nierlein.de","ah, the original #195 has been reverted. This patch will apply if #195 gets merged again.",True,{}
naemon/naemon-core,https://github.com/naemon/naemon-core,215,2018-02-10T21:19:17Z,2018-05-30T12:47:42Z,2018-05-30T12:47:42Z,CLOSED,False,3,3,2,https://github.com/sni,do not escape double quotes,1,[],https://github.com/naemon/naemon-core/pull/215,https://github.com/sni,3,https://github.com/naemon/naemon-core/pull/215#issuecomment-393148263,"they would end up escaped in the pluginoutput and break html output for example.
Signed-off-by: Sven Nierlein sven@nierlein.de",already merged.,True,{}
naemon/naemon-core,https://github.com/naemon/naemon-core,216,2018-03-10T00:38:41Z,2018-05-30T12:49:33Z,2018-05-30T12:49:33Z,CLOSED,False,6,1,1,https://github.com/dirtyren,Reload : Fix defunct,1,[],https://github.com/naemon/naemon-core/pull/216,https://github.com/dirtyren,1,https://github.com/naemon/naemon-core/pull/216,Fixes issue #150.,Fixes issue #150.,True,{}
naemon/naemon-core,https://github.com/naemon/naemon-core,216,2018-03-10T00:38:41Z,2018-05-30T12:49:33Z,2018-05-30T12:49:33Z,CLOSED,False,6,1,1,https://github.com/dirtyren,Reload : Fix defunct,1,[],https://github.com/naemon/naemon-core/pull/216,https://github.com/sni,2,https://github.com/naemon/naemon-core/pull/216#issuecomment-393148832,Fixes issue #150.,"thanks, this PR has been merged.",True,{}
naemon/naemon-core,https://github.com/naemon/naemon-core,217,2018-03-16T08:55:33Z,2018-04-04T12:39:47Z,2018-04-04T12:39:47Z,MERGED,True,59,6,6,https://github.com/jacobbaungard,Allow compilation with GCC 7,2,[],https://github.com/naemon/naemon-core/pull/217,https://github.com/jacobbaungard,1,https://github.com/naemon/naemon-core/pull/217,"GCC 7 introduced new warnings which, as we compile Naemon with -Werror, meant that the builds failed.
The warnings are trigged when building naemon are:
-Wimplicit-fallthrough which warns if there are any fallthroughs in switch/case statements.
-Wformat-truncation which warns if output from a formatting operating could be truncated (for example when running snprintf)
These two patches ensures that Naemon can be correctly built with GCC 7. See the individual commit messages for further information.
This should fix #184","GCC 7 introduced new warnings which, as we compile Naemon with -Werror, meant that the builds failed.
The warnings are trigged when building naemon are:
-Wimplicit-fallthrough which warns if there are any fallthroughs in switch/case statements.
-Wformat-truncation which warns if output from a formatting operating could be truncated (for example when running snprintf)
These two patches ensures that Naemon can be correctly built with GCC 7. See the individual commit messages for further information.
This should fix #184",True,{}
naemon/naemon-core,https://github.com/naemon/naemon-core,217,2018-03-16T08:55:33Z,2018-04-04T12:39:47Z,2018-04-04T12:39:47Z,MERGED,True,59,6,6,https://github.com/jacobbaungard,Allow compilation with GCC 7,2,[],https://github.com/naemon/naemon-core/pull/217,https://github.com/roengstrom,2,https://github.com/naemon/naemon-core/pull/217#issuecomment-378585156,"GCC 7 introduced new warnings which, as we compile Naemon with -Werror, meant that the builds failed.
The warnings are trigged when building naemon are:
-Wimplicit-fallthrough which warns if there are any fallthroughs in switch/case statements.
-Wformat-truncation which warns if output from a formatting operating could be truncated (for example when running snprintf)
These two patches ensures that Naemon can be correctly built with GCC 7. See the individual commit messages for further information.
This should fix #184",Looks good to me,True,{}
naemon/naemon-core,https://github.com/naemon/naemon-core,218,2018-04-09T09:56:07Z,2018-04-20T13:32:04Z,2018-04-20T13:32:04Z,CLOSED,False,0,0,0,https://github.com/tnsetting,Triggered Scheduled downtime,2,[],https://github.com/naemon/naemon-core/pull/218,https://github.com/tnsetting,1,https://github.com/naemon/naemon-core/pull/218,"The triggered scheduled downtime is deleted when the triggering downtime is starting.
This issue is due to that flex downtime start variable is feeded with right value. It will add a wrong time (negative) for the downtime stop event on the event queue and the downtime stop event for the triggered downtime will start evecution right after it is started.
The solution is to initialize the flex downtime start variable of the triggered downtime with the value of the flex downtime start value of the triggering one.
Signed-off-by: Nian Tang ntang@op5.com","The triggered scheduled downtime is deleted when the triggering downtime is starting.
This issue is due to that flex downtime start variable is feeded with right value. It will add a wrong time (negative) for the downtime stop event on the event queue and the downtime stop event for the triggered downtime will start evecution right after it is started.
The solution is to initialize the flex downtime start variable of the triggered downtime with the value of the flex downtime start value of the triggering one.
Signed-off-by: Nian Tang ntang@op5.com",True,{}
naemon/naemon-core,https://github.com/naemon/naemon-core,218,2018-04-09T09:56:07Z,2018-04-20T13:32:04Z,2018-04-20T13:32:04Z,CLOSED,False,0,0,0,https://github.com/tnsetting,Triggered Scheduled downtime,2,[],https://github.com/naemon/naemon-core/pull/218,https://github.com/roengstrom,2,https://github.com/naemon/naemon-core/pull/218#issuecomment-382353714,"The triggered scheduled downtime is deleted when the triggering downtime is starting.
This issue is due to that flex downtime start variable is feeded with right value. It will add a wrong time (negative) for the downtime stop event on the event queue and the downtime stop event for the triggered downtime will start evecution right after it is started.
The solution is to initialize the flex downtime start variable of the triggered downtime with the value of the flex downtime start value of the triggering one.
Signed-off-by: Nian Tang ntang@op5.com",Looks good to me.,True,{}
naemon/naemon-core,https://github.com/naemon/naemon-core,218,2018-04-09T09:56:07Z,2018-04-20T13:32:04Z,2018-04-20T13:32:04Z,CLOSED,False,0,0,0,https://github.com/tnsetting,Triggered Scheduled downtime,2,[],https://github.com/naemon/naemon-core/pull/218,https://github.com/jacobbaungard,3,https://github.com/naemon/naemon-core/pull/218#issuecomment-383096825,"The triggered scheduled downtime is deleted when the triggering downtime is starting.
This issue is due to that flex downtime start variable is feeded with right value. It will add a wrong time (negative) for the downtime stop event on the event queue and the downtime stop event for the triggered downtime will start evecution right after it is started.
The solution is to initialize the flex downtime start variable of the triggered downtime with the value of the flex downtime start value of the triggering one.
Signed-off-by: Nian Tang ntang@op5.com",This has been merged into master with: 2a4d594 and I am therefore closing this PR.,True,{}
naemon/naemon-core,https://github.com/naemon/naemon-core,219,2018-04-09T12:24:03Z,2018-04-20T13:32:29Z,2018-04-20T13:32:29Z,CLOSED,False,191,10,4,https://github.com/tnsetting,Notification is sent even if a service is in a scheduled flexible downtime,1,[],https://github.com/naemon/naemon-core/pull/219,https://github.com/tnsetting,1,https://github.com/naemon/naemon-core/pull/219,"When a service is set in flexible downtime, the first notification is sent out before the downtime schedule takes effect.
Since service result check logic is just add the downtime start event to the event queue and then start notification right after,
this bug is natural with this flow.
The solution is checking whether there is any flexible downtime started and if not we will send out the normal notifciation.
Signed-off-by: Nian Tang ntang@op5.com","When a service is set in flexible downtime, the first notification is sent out before the downtime schedule takes effect.
Since service result check logic is just add the downtime start event to the event queue and then start notification right after,
this bug is natural with this flow.
The solution is checking whether there is any flexible downtime started and if not we will send out the normal notifciation.
Signed-off-by: Nian Tang ntang@op5.com",True,{}
naemon/naemon-core,https://github.com/naemon/naemon-core,219,2018-04-09T12:24:03Z,2018-04-20T13:32:29Z,2018-04-20T13:32:29Z,CLOSED,False,191,10,4,https://github.com/tnsetting,Notification is sent even if a service is in a scheduled flexible downtime,1,[],https://github.com/naemon/naemon-core/pull/219,https://github.com/jacobbaungard,2,https://github.com/naemon/naemon-core/pull/219#issuecomment-383096932,"When a service is set in flexible downtime, the first notification is sent out before the downtime schedule takes effect.
Since service result check logic is just add the downtime start event to the event queue and then start notification right after,
this bug is natural with this flow.
The solution is checking whether there is any flexible downtime started and if not we will send out the normal notifciation.
Signed-off-by: Nian Tang ntang@op5.com",This has been merged into master with: 2a4d594 and I am therefore closing this PR.,True,{}
naemon/naemon-core,https://github.com/naemon/naemon-core,220,2018-04-12T12:16:28Z,2018-04-24T11:07:18Z,2018-04-24T11:07:18Z,CLOSED,False,311,10,3,https://github.com/tnsetting,Switching service states for host down is confusing and ruins reports,1,[],https://github.com/naemon/naemon-core/pull/220,https://github.com/tnsetting,1,https://github.com/naemon/naemon-core/pull/220,"When there is a service check failure and the host is down, the service will be set to hard state
even though the service has not reach the max attempts. This behavior is corrected and notification is not send while the host is down as it is supposed to be.
Signed-off-by: Nian Tang ntang@op5.com","When there is a service check failure and the host is down, the service will be set to hard state
even though the service has not reach the max attempts. This behavior is corrected and notification is not send while the host is down as it is supposed to be.
Signed-off-by: Nian Tang ntang@op5.com",True,{}
naemon/naemon-core,https://github.com/naemon/naemon-core,220,2018-04-12T12:16:28Z,2018-04-24T11:07:18Z,2018-04-24T11:07:18Z,CLOSED,False,311,10,3,https://github.com/tnsetting,Switching service states for host down is confusing and ruins reports,1,[],https://github.com/naemon/naemon-core/pull/220,https://github.com/sni,2,https://github.com/naemon/naemon-core/pull/220#issuecomment-380785425,"When there is a service check failure and the host is down, the service will be set to hard state
even though the service has not reach the max attempts. This behavior is corrected and notification is not send while the host is down as it is supposed to be.
Signed-off-by: Nian Tang ntang@op5.com","How does this ruin reports? If the host is down, a service should not stay in a soft something?",True,{}
naemon/naemon-core,https://github.com/naemon/naemon-core,220,2018-04-12T12:16:28Z,2018-04-24T11:07:18Z,2018-04-24T11:07:18Z,CLOSED,False,311,10,3,https://github.com/tnsetting,Switching service states for host down is confusing and ruins reports,1,[],https://github.com/naemon/naemon-core/pull/220,https://github.com/tnsetting,3,https://github.com/naemon/naemon-core/pull/220#issuecomment-380787416,"When there is a service check failure and the host is down, the service will be set to hard state
even though the service has not reach the max attempts. This behavior is corrected and notification is not send while the host is down as it is supposed to be.
Signed-off-by: Nian Tang ntang@op5.com","Hi Sven,

Currently when there is a service check result with critical, the service
check function will check whether the host is down, if it is, it will make the
service to a hard state even though the service has not reached its max
attempts. So in the report you will see that the service switch states when
the current_attempt is smaller than the max_attempt.

Regards
Nian
…
On Thu, Apr 12, 2018 at 2:20 PM, Sven Nierlein ***@***.***> wrote:
 How does this ruin reports? If the host is down, a service should not stay
 in a soft something?

 —
 You are receiving this because you authored the thread.
 Reply to this email directly, view it on GitHub
 <#220 (comment)>,
 or mute the thread
 <https://github.com/notifications/unsubscribe-auth/Akb-6O8ja-zCgQbfMyh7dbooi8lEOdC9ks5tn0Z0gaJpZM4TRrdI>
 .


-- 
Nian",True,{}
naemon/naemon-core,https://github.com/naemon/naemon-core,220,2018-04-12T12:16:28Z,2018-04-24T11:07:18Z,2018-04-24T11:07:18Z,CLOSED,False,311,10,3,https://github.com/tnsetting,Switching service states for host down is confusing and ruins reports,1,[],https://github.com/naemon/naemon-core/pull/220,https://github.com/sni,4,https://github.com/naemon/naemon-core/pull/220#issuecomment-380792241,"When there is a service check failure and the host is down, the service will be set to hard state
even though the service has not reach the max attempts. This behavior is corrected and notification is not send while the host is down as it is supposed to be.
Signed-off-by: Nian Tang ntang@op5.com","If i recall correctly, naemon will stop checking services if the host is down, so the service will stay in a soft state until the host recovers?",True,{}
naemon/naemon-core,https://github.com/naemon/naemon-core,220,2018-04-12T12:16:28Z,2018-04-24T11:07:18Z,2018-04-24T11:07:18Z,CLOSED,False,311,10,3,https://github.com/tnsetting,Switching service states for host down is confusing and ruins reports,1,[],https://github.com/naemon/naemon-core/pull/220,https://github.com/tnsetting,5,https://github.com/naemon/naemon-core/pull/220#issuecomment-380794116,"When there is a service check failure and the host is down, the service will be set to hard state
even though the service has not reach the max attempts. This behavior is corrected and notification is not send while the host is down as it is supposed to be.
Signed-off-by: Nian Tang ntang@op5.com","No. The service check is not stopped if the host is down. The only way to
stop a service check if to run external command disable_service_check.

Nian
…
On Thu, Apr 12, 2018 at 2:45 PM, Sven Nierlein ***@***.***> wrote:
 If i recall correctly, naemon will stop checking services if the host is
 down, so the service will stay in a soft state until the host recovers?

 —
 You are receiving this because you authored the thread.
 Reply to this email directly, view it on GitHub
 <#220 (comment)>,
 or mute the thread
 <https://github.com/notifications/unsubscribe-auth/Akb-6OUi_omgKnG0Qh_vvfQI11errEm4ks5tn0x0gaJpZM4TRrdI>
 .


-- 
Nian",True,{}
naemon/naemon-core,https://github.com/naemon/naemon-core,220,2018-04-12T12:16:28Z,2018-04-24T11:07:18Z,2018-04-24T11:07:18Z,CLOSED,False,311,10,3,https://github.com/tnsetting,Switching service states for host down is confusing and ruins reports,1,[],https://github.com/naemon/naemon-core/pull/220,https://github.com/nook24,6,https://github.com/naemon/naemon-core/pull/220#issuecomment-380928734,"When there is a service check failure and the host is down, the service will be set to hard state
even though the service has not reach the max attempts. This behavior is corrected and notification is not send while the host is down as it is supposed to be.
Signed-off-by: Nian Tang ntang@op5.com","I don't remember exactly but I have in mind that Nagios/Naemon execute a host check on every state change of a service or something like this...
How ever, most of the time, the host check is just check_icmp. So if your host can't respond to an icmp request, your service will definitely be hard critical^^
Removing this behavior will ruin my reports, because most of the time I filter soft stats and only consider hard states. This feature should get a config option in naemon.cmd which is off by default i think.",True,{}
naemon/naemon-core,https://github.com/naemon/naemon-core,220,2018-04-12T12:16:28Z,2018-04-24T11:07:18Z,2018-04-24T11:07:18Z,CLOSED,False,311,10,3,https://github.com/tnsetting,Switching service states for host down is confusing and ruins reports,1,[],https://github.com/naemon/naemon-core/pull/220,https://github.com/tnsetting,7,https://github.com/naemon/naemon-core/pull/220#issuecomment-381050446,"When there is a service check failure and the host is down, the service will be set to hard state
even though the service has not reach the max attempts. This behavior is corrected and notification is not send while the host is down as it is supposed to be.
Signed-off-by: Nian Tang ntang@op5.com","In case of receiving a critical service check, if the host is in down state, then the service will schedule a  host check but not every state change will cause a host check.
If you just consider hard states, the service will still go to hard state if it reaches the max_attempts.
The fake logic has some negative effect since it will set the service in a strange state and make the check handling logic complicated. I would suggest to remove this anyway since this also breaks other functionality such flexible scheduled downtime for service, handle_service_event.",True,{}
naemon/naemon-core,https://github.com/naemon/naemon-core,220,2018-04-12T12:16:28Z,2018-04-24T11:07:18Z,2018-04-24T11:07:18Z,CLOSED,False,311,10,3,https://github.com/tnsetting,Switching service states for host down is confusing and ruins reports,1,[],https://github.com/naemon/naemon-core/pull/220,https://github.com/jacobbaungard,8,https://github.com/naemon/naemon-core/pull/220#issuecomment-383892420,"When there is a service check failure and the host is down, the service will be set to hard state
even though the service has not reach the max attempts. This behavior is corrected and notification is not send while the host is down as it is supposed to be.
Signed-off-by: Nian Tang ntang@op5.com",Closing this due to alternative approach chosen at PR: #221,True,{}
naemon/naemon-core,https://github.com/naemon/naemon-core,221,2018-04-17T14:00:36Z,2018-04-25T12:47:43Z,2018-04-25T12:47:56Z,MERGED,True,3,1,1,https://github.com/tnsetting,Logging for switching service states for host down ,1,[],https://github.com/naemon/naemon-core/pull/221,https://github.com/tnsetting,1,https://github.com/naemon/naemon-core/pull/221,"Adding a log message to explain why the hard states switching happened.
Signed-off-by: Nian Tang ntang@op5.com","Adding a log message to explain why the hard states switching happened.
Signed-off-by: Nian Tang ntang@op5.com",True,{}
naemon/naemon-core,https://github.com/naemon/naemon-core,221,2018-04-17T14:00:36Z,2018-04-25T12:47:43Z,2018-04-25T12:47:56Z,MERGED,True,3,1,1,https://github.com/tnsetting,Logging for switching service states for host down ,1,[],https://github.com/naemon/naemon-core/pull/221,https://github.com/jacobbaungard,2,https://github.com/naemon/naemon-core/pull/221#issuecomment-382015343,"Adding a log message to explain why the hard states switching happened.
Signed-off-by: Nian Tang ntang@op5.com","As I understand it, this change will add a log message for every single service which is moved to a hard down state due to the host being down.  This might spam the log quite a bit if a host have many services.
Is it possible to add something to the existing service alert if it is moved to a hard down state instead? I.e something like:
[2016-04-05 14:49:01] SERVICE ALERT: host1;svc1;CRITICAL;HARD;1;CRITICAL - Hard state switch due to host down - Socket timeout after 10 seconds
I am however not sure if the above format, will cause problems for other tools which parses the service alerts?",True,{}
naemon/naemon-core,https://github.com/naemon/naemon-core,221,2018-04-17T14:00:36Z,2018-04-25T12:47:43Z,2018-04-25T12:47:56Z,MERGED,True,3,1,1,https://github.com/tnsetting,Logging for switching service states for host down ,1,[],https://github.com/naemon/naemon-core/pull/221,https://github.com/tnsetting,3,https://github.com/naemon/naemon-core/pull/221#issuecomment-382296189,"Adding a log message to explain why the hard states switching happened.
Signed-off-by: Nian Tang ntang@op5.com","@jacobbaungard  Currently the logging logic for service alert is a generic function which is printing states switching basically. The last field for the alert is the output from the check plugin.  I am not sure it is a good idea to alter this information.
I think it will span the log a bit but this scenario does not happen that regularly.",True,{}
naemon/naemon-core,https://github.com/naemon/naemon-core,221,2018-04-17T14:00:36Z,2018-04-25T12:47:43Z,2018-04-25T12:47:56Z,MERGED,True,3,1,1,https://github.com/tnsetting,Logging for switching service states for host down ,1,[],https://github.com/naemon/naemon-core/pull/221,https://github.com/jacobbaungard,4,https://github.com/naemon/naemon-core/pull/221#issuecomment-382380472,"Adding a log message to explain why the hard states switching happened.
Signed-off-by: Nian Tang ntang@op5.com","Okay, if changing the current output log-line is infeasible, I am OK with adding a new log line as is done here.
I am however a little bit concerned that this does not have the same format as the other SERVICE ALERT. That might be problematic for tools parsing the logs.
I think we should probably take one of these two approaches:


Follow the existing service alert format ( check objects_service.c ) - and add a ""NOTE: .."" as the message or something similar. But then perhaps if something parses the logs, it will see two service alerts with the same status (expect for the message), not sure if that will be confusing/cause problems.


Do not use the service alert format at all, but add a log entry that does not contain ""SERVICE ALERT"". Maybe it could be ""SERVICE NOTE"" or something similar. The disadvantage of this, is if there are tools the parses the logs and look for events related to a specific service, then this message might not show up. Those tools could be updated to support this new log entry though.


I am would be leaning towards approach 2). Any additional comments is welcome though @roengstrom .",True,{}
naemon/naemon-core,https://github.com/naemon/naemon-core,221,2018-04-17T14:00:36Z,2018-04-25T12:47:43Z,2018-04-25T12:47:56Z,MERGED,True,3,1,1,https://github.com/tnsetting,Logging for switching service states for host down ,1,[],https://github.com/naemon/naemon-core/pull/221,https://github.com/tnsetting,5,https://github.com/naemon/naemon-core/pull/221#issuecomment-382641089,"Adding a log message to explain why the hard states switching happened.
Signed-off-by: Nian Tang ntang@op5.com",Currently the log type is set to NSLOG_INFO_MESSAGE instead other service related type NSLOG_SERVICE*. I agree with the second option that the message heading should be something like SERVICE NOTE or SERVICE INFO since it is an additional message to the SERVICE ALERT message and it will not confuse the existing parsing tools.,True,{}
naemon/naemon-core,https://github.com/naemon/naemon-core,225,2018-05-09T12:54:19Z,2018-05-15T09:42:43Z,2018-05-15T09:42:43Z,CLOSED,False,3,4,2,https://github.com/tnsetting,Double naemon process,1,[],https://github.com/naemon/naemon-core/pull/225,https://github.com/tnsetting,1,https://github.com/naemon/naemon-core/pull/225,"Double naemon process situtation is created if other process is monitoring naemon process and try to restart it.
Naemon relies on the pid file locking to ensure other instance of naemon could not be started. But the advisory locking breaks if the
pid file is remove. Bascially you can always create a new naemon process if you just remove the pidfile.
In the commit, the pidfile is not removed after naemon process exiting the main function and the modified the stop function in the
init.d file so that the pidfile is not removed after killing the naemon process.
Signed-off-by: Nian Tang ntang@op5.com","Double naemon process situtation is created if other process is monitoring naemon process and try to restart it.
Naemon relies on the pid file locking to ensure other instance of naemon could not be started. But the advisory locking breaks if the
pid file is remove. Bascially you can always create a new naemon process if you just remove the pidfile.
In the commit, the pidfile is not removed after naemon process exiting the main function and the modified the stop function in the
init.d file so that the pidfile is not removed after killing the naemon process.
Signed-off-by: Nian Tang ntang@op5.com",True,{}
naemon/naemon-core,https://github.com/naemon/naemon-core,225,2018-05-09T12:54:19Z,2018-05-15T09:42:43Z,2018-05-15T09:42:43Z,CLOSED,False,3,4,2,https://github.com/tnsetting,Double naemon process,1,[],https://github.com/naemon/naemon-core/pull/225,https://github.com/jacobbaungard,2,https://github.com/naemon/naemon-core/pull/225#issuecomment-387755162,"Double naemon process situtation is created if other process is monitoring naemon process and try to restart it.
Naemon relies on the pid file locking to ensure other instance of naemon could not be started. But the advisory locking breaks if the
pid file is remove. Bascially you can always create a new naemon process if you just remove the pidfile.
In the commit, the pidfile is not removed after naemon process exiting the main function and the modified the stop function in the
init.d file so that the pidfile is not removed after killing the naemon process.
Signed-off-by: Nian Tang ntang@op5.com","I am not 100% sure I understand everything correctly so perhaps you could confirm whether my assumptions here are correct ?

At startup we check whether a pid file exists
If it exists and the process noted in the pid file is alive, then we wont start Naemon. If the pid in the pidfile is no longer active, we can start Naemon
At shutdown we leave the pidfile, but because the process of the pid file is no longer alive, it is possible to start a new process",True,{}
naemon/naemon-core,https://github.com/naemon/naemon-core,225,2018-05-09T12:54:19Z,2018-05-15T09:42:43Z,2018-05-15T09:42:43Z,CLOSED,False,3,4,2,https://github.com/tnsetting,Double naemon process,1,[],https://github.com/naemon/naemon-core/pull/225,https://github.com/tnsetting,3,https://github.com/naemon/naemon-core/pull/225#issuecomment-387758424,"Double naemon process situtation is created if other process is monitoring naemon process and try to restart it.
Naemon relies on the pid file locking to ensure other instance of naemon could not be started. But the advisory locking breaks if the
pid file is remove. Bascially you can always create a new naemon process if you just remove the pidfile.
In the commit, the pidfile is not removed after naemon process exiting the main function and the modified the stop function in the
init.d file so that the pidfile is not removed after killing the naemon process.
Signed-off-by: Nian Tang ntang@op5.com","@jacobbaungard
No really.
At start up, naemon opens the pidfile. If not exist, it will create the pidfile.
Then naemon check if anyone is locking the pidfile. If someone locking, it will exit immediately.
Fork.
Naemon tries to lock the pidfile using advisory locking. If locking ok, naemon continues. If not, naemon exits. This is the place no other naemon can lock the pidfile.
Naemon shutdown and other naemon can start and lock the pidfile.
The problem. If the pidfile is moved, removed or renamed, you can start another naemon instance and the existing naemon will still be running. And you can repeat the process and create multiple naemon instance.",True,{}
naemon/naemon-core,https://github.com/naemon/naemon-core,225,2018-05-09T12:54:19Z,2018-05-15T09:42:43Z,2018-05-15T09:42:43Z,CLOSED,False,3,4,2,https://github.com/tnsetting,Double naemon process,1,[],https://github.com/naemon/naemon-core/pull/225,https://github.com/sni,4,https://github.com/naemon/naemon-core/pull/225#issuecomment-387846871,"Double naemon process situtation is created if other process is monitoring naemon process and try to restart it.
Naemon relies on the pid file locking to ensure other instance of naemon could not be started. But the advisory locking breaks if the
pid file is remove. Bascially you can always create a new naemon process if you just remove the pidfile.
In the commit, the pidfile is not removed after naemon process exiting the main function and the modified the stop function in the
init.d file so that the pidfile is not removed after killing the naemon process.
Signed-off-by: Nian Tang ntang@op5.com","If naemon creates the pidfile, naemon should remove the pidfile as well. Maybe it would be better to make the pidfile optional and move it to a command line argument. Then you could choose if you want naemon handle the pidfile or do it yourself, for example like systemd would do it.",True,{}
naemon/naemon-core,https://github.com/naemon/naemon-core,226,2018-05-15T13:27:21Z,2018-05-30T12:47:56Z,2018-05-30T12:47:56Z,CLOSED,False,12,4,1,https://github.com/sni,first notification delay should start on hard state,1,[],https://github.com/naemon/naemon-core/pull/226,https://github.com/sni,1,https://github.com/naemon/naemon-core/pull/226,"first_notification_delay should start on the last hard change instead of using
the last time ok. Hard state changes could also occur if a service changes from
warning to critical.
Example with max_check_attempts 5
soft 0: OK
soft 1: Warning
soft 2: Warning
soft 3: Warning
soft 4: Critical
hard 5: Critical
Naemon would now use the time from the last ok (soft 0) to calculate the first
notification delay instead of the actual time of the usual first notification (hard 5)
references:

NagiosEnterprises/nagioscore#483
NagiosEnterprises/nagioscore#482

Signed-off-by: Sven Nierlein sven@nierlein.de","first_notification_delay should start on the last hard change instead of using
the last time ok. Hard state changes could also occur if a service changes from
warning to critical.
Example with max_check_attempts 5
soft 0: OK
soft 1: Warning
soft 2: Warning
soft 3: Warning
soft 4: Critical
hard 5: Critical
Naemon would now use the time from the last ok (soft 0) to calculate the first
notification delay instead of the actual time of the usual first notification (hard 5)
references:

NagiosEnterprises/nagioscore#483
NagiosEnterprises/nagioscore#482

Signed-off-by: Sven Nierlein sven@nierlein.de",True,{}
naemon/naemon-core,https://github.com/naemon/naemon-core,226,2018-05-15T13:27:21Z,2018-05-30T12:47:56Z,2018-05-30T12:47:56Z,CLOSED,False,12,4,1,https://github.com/sni,first notification delay should start on hard state,1,[],https://github.com/naemon/naemon-core/pull/226,https://github.com/sni,2,https://github.com/naemon/naemon-core/pull/226#issuecomment-393148326,"first_notification_delay should start on the last hard change instead of using
the last time ok. Hard state changes could also occur if a service changes from
warning to critical.
Example with max_check_attempts 5
soft 0: OK
soft 1: Warning
soft 2: Warning
soft 3: Warning
soft 4: Critical
hard 5: Critical
Naemon would now use the time from the last ok (soft 0) to calculate the first
notification delay instead of the actual time of the usual first notification (hard 5)
references:

NagiosEnterprises/nagioscore#483
NagiosEnterprises/nagioscore#482

Signed-off-by: Sven Nierlein sven@nierlein.de",already merged.,True,{}
naemon/naemon-core,https://github.com/naemon/naemon-core,230,2018-05-25T07:22:55Z,2018-05-30T12:41:31Z,2018-05-30T12:41:31Z,MERGED,True,1152,273,51,https://github.com/sni,Prepare next release,18,[],https://github.com/naemon/naemon-core/pull/230,https://github.com/sni,1,https://github.com/naemon/naemon-core/pull/230,this pull request contains everything required for the next release except the release itself.,this pull request contains everything required for the next release except the release itself.,True,{}
naemon/naemon-core,https://github.com/naemon/naemon-core,230,2018-05-25T07:22:55Z,2018-05-30T12:41:31Z,2018-05-30T12:41:31Z,MERGED,True,1152,273,51,https://github.com/sni,Prepare next release,18,[],https://github.com/naemon/naemon-core/pull/230,https://github.com/jacobbaungard,2,https://github.com/naemon/naemon-core/pull/230#issuecomment-392073931,this pull request contains everything required for the next release except the release itself.,"Is it possible to change the commit author on ""Reload : Fix defunct"" so it is not ""root"" ?",True,{}
naemon/naemon-core,https://github.com/naemon/naemon-core,230,2018-05-25T07:22:55Z,2018-05-30T12:41:31Z,2018-05-30T12:41:31Z,MERGED,True,1152,273,51,https://github.com/sni,Prepare next release,18,[],https://github.com/naemon/naemon-core/pull/230,https://github.com/sni,3,https://github.com/naemon/naemon-core/pull/230#issuecomment-392075814,this pull request contains everything required for the next release except the release itself.,"sure, i faked the author and rebased the commits.",True,{}
naemon/naemon-core,https://github.com/naemon/naemon-core,230,2018-05-25T07:22:55Z,2018-05-30T12:41:31Z,2018-05-30T12:41:31Z,MERGED,True,1152,273,51,https://github.com/sni,Prepare next release,18,[],https://github.com/naemon/naemon-core/pull/230,https://github.com/sni,4,https://github.com/naemon/naemon-core/pull/230#issuecomment-392548177,this pull request contains everything required for the next release except the release itself.,The commit also changes the location in the debian and rpm package. I could amend those changes to the initial pkg commit or reword the commit description if you like.,True,{}
naemon/naemon-core,https://github.com/naemon/naemon-core,230,2018-05-25T07:22:55Z,2018-05-30T12:41:31Z,2018-05-30T12:41:31Z,MERGED,True,1152,273,51,https://github.com/sni,Prepare next release,18,[],https://github.com/naemon/naemon-core/pull/230,https://github.com/jacobbaungard,5,https://github.com/naemon/naemon-core/pull/230#issuecomment-392549870,this pull request contains everything required for the next release except the release itself.,"It is not really a major issue, but I feel it would be cleaner to have them in the initial pkg commit instead.",True,{}
naemon/naemon-core,https://github.com/naemon/naemon-core,230,2018-05-25T07:22:55Z,2018-05-30T12:41:31Z,2018-05-30T12:41:31Z,MERGED,True,1152,273,51,https://github.com/sni,Prepare next release,18,[],https://github.com/naemon/naemon-core/pull/230,https://github.com/sni,6,https://github.com/naemon/naemon-core/pull/230#issuecomment-392669917,this pull request contains everything required for the next release except the release itself.,"Ok, i moved the spec and debian/ changes to the first commit and rebased everything.",True,{}
naemon/naemon-core,https://github.com/naemon/naemon-core,230,2018-05-25T07:22:55Z,2018-05-30T12:41:31Z,2018-05-30T12:41:31Z,MERGED,True,1152,273,51,https://github.com/sni,Prepare next release,18,[],https://github.com/naemon/naemon-core/pull/230,https://github.com/sni,7,https://github.com/naemon/naemon-core/pull/230#issuecomment-392696907,this pull request contains everything required for the next release except the release itself.,"yeah, i thought that as well but i was too lazy :-) its fixed now",True,{}
naemon/naemon-core,https://github.com/naemon/naemon-core,231,2018-05-28T11:25:14Z,2018-05-28T13:57:37Z,2018-05-28T14:04:20Z,MERGED,True,0,310,6,https://github.com/jacobbaungard,Remove op5build,1,[],https://github.com/naemon/naemon-core/pull/231,https://github.com/jacobbaungard,1,https://github.com/naemon/naemon-core/pull/231,"Vendor specific files should be in a downstream repository instead.
Signed-off-by: Jacob Hansen jhansen@op5.com","Vendor specific files should be in a downstream repository instead.
Signed-off-by: Jacob Hansen jhansen@op5.com",True,{}
naemon/naemon-core,https://github.com/naemon/naemon-core,231,2018-05-28T11:25:14Z,2018-05-28T13:57:37Z,2018-05-28T14:04:20Z,MERGED,True,0,310,6,https://github.com/jacobbaungard,Remove op5build,1,[],https://github.com/naemon/naemon-core/pull/231,https://github.com/sni,2,https://github.com/naemon/naemon-core/pull/231#issuecomment-392501644,"Vendor specific files should be in a downstream repository instead.
Signed-off-by: Jacob Hansen jhansen@op5.com","yeah, thanks.",True,{}
naemon/naemon-core,https://github.com/naemon/naemon-core,232,2018-05-31T15:17:35Z,2018-06-01T08:47:55Z,2018-06-01T08:47:55Z,MERGED,True,47,1,2,https://github.com/jacobbaungard,Added changelog for 1.0.7 release,1,[],https://github.com/naemon/naemon-core/pull/232,https://github.com/jacobbaungard,1,https://github.com/naemon/naemon-core/pull/232,"An attempt to create a change log for the 1.0.7 release.
Signed-off-by: Jacob Hansen jhansen@op5.com","An attempt to create a change log for the 1.0.7 release.
Signed-off-by: Jacob Hansen jhansen@op5.com",True,{}
naemon/naemon-core,https://github.com/naemon/naemon-core,232,2018-05-31T15:17:35Z,2018-06-01T08:47:55Z,2018-06-01T08:47:55Z,MERGED,True,47,1,2,https://github.com/jacobbaungard,Added changelog for 1.0.7 release,1,[],https://github.com/naemon/naemon-core/pull/232,https://github.com/jacobbaungard,2,https://github.com/naemon/naemon-core/pull/232#issuecomment-393566222,"An attempt to create a change log for the 1.0.7 release.
Signed-off-by: Jacob Hansen jhansen@op5.com","I moved the NEWS file to the CHANGELOG and removed the ChangeLog file from before. I am not sure whether this is the best practice though. Maybe it is better to keep it in the NEWS file. Not sure.
Or perhaps we should follow something like: https://keepachangelog.com/en/1.0.0/ instead.
I left out some commits, such as those that only touches testing (for example 8720d6d), or things that did minor code cleanup (for example: 0335087).
Hopefully everything else should be included.",True,{}
naemon/naemon-core,https://github.com/naemon/naemon-core,232,2018-05-31T15:17:35Z,2018-06-01T08:47:55Z,2018-06-01T08:47:55Z,MERGED,True,47,1,2,https://github.com/jacobbaungard,Added changelog for 1.0.7 release,1,[],https://github.com/naemon/naemon-core/pull/232,https://github.com/sni,3,https://github.com/naemon/naemon-core/pull/232#issuecomment-393572764,"An attempt to create a change log for the 1.0.7 release.
Signed-off-by: Jacob Hansen jhansen@op5.com","The changelog itself looks good, thanks for your work. The news and changelog file must be changed in the spec file as well and in the Makefile.am.",True,{}
naemon/naemon-core,https://github.com/naemon/naemon-core,233,2018-05-31T19:44:58Z,2018-06-01T07:19:35Z,2018-06-01T07:19:35Z,MERGED,True,0,1,1,https://github.com/sni,rpm: remove duplicate listing of naemonstats manpage,1,[],https://github.com/naemon/naemon-core/pull/233,https://github.com/sni,1,https://github.com/naemon/naemon-core/pull/233,"This file has been listed twice, under naemon-core and libnaemon. It should be listed only once.","This file has been listed twice, under naemon-core and libnaemon. It should be listed only once.",True,{}
naemon/naemon-core,https://github.com/naemon/naemon-core,234,2018-06-01T08:43:46Z,2018-06-01T09:07:18Z,2018-06-01T09:07:18Z,MERGED,True,23,2,5,https://github.com/sni,Fix packaging,2,[],https://github.com/naemon/naemon-core/pull/234,https://github.com/sni,1,https://github.com/naemon/naemon-core/pull/234,Some final changes to get osb builds working.,Some final changes to get osb builds working.,True,{}
naemon/naemon-core,https://github.com/naemon/naemon-core,234,2018-06-01T08:43:46Z,2018-06-01T09:07:18Z,2018-06-01T09:07:18Z,MERGED,True,23,2,5,https://github.com/sni,Fix packaging,2,[],https://github.com/naemon/naemon-core/pull/234,https://github.com/sni,2,https://github.com/naemon/naemon-core/pull/234#issuecomment-393817671,Some final changes to get osb builds working.,"you are right, added a comment.",True,{}
naemon/naemon-core,https://github.com/naemon/naemon-core,235,2018-06-01T09:41:35Z,2018-06-08T09:55:14Z,2018-06-08T09:55:14Z,CLOSED,False,18,18,5,https://github.com/sni,Remove unused parameter,2,[],https://github.com/naemon/naemon-core/pull/235,https://github.com/sni,1,https://github.com/naemon/naemon-core/pull/235,,,True,{}
naemon/naemon-core,https://github.com/naemon/naemon-core,235,2018-06-01T09:41:35Z,2018-06-08T09:55:14Z,2018-06-08T09:55:14Z,CLOSED,False,18,18,5,https://github.com/sni,Remove unused parameter,2,[],https://github.com/naemon/naemon-core/pull/235,https://github.com/sni,2,https://github.com/naemon/naemon-core/pull/235#issuecomment-395712431,,"The only reason for this PR was to support broken gcc versions on sles 11 on obs. Since there are more issues on sles 11, we just drop support for sles 11 obs builds for now.",True,{}
naemon/naemon-core,https://github.com/naemon/naemon-core,236,2018-06-01T12:15:05Z,2018-06-01T12:25:17Z,2018-06-01T12:25:18Z,MERGED,True,6,3,1,https://github.com/sni,Release 1 0 7,2,[],https://github.com/naemon/naemon-core/pull/236,https://github.com/sni,1,https://github.com/naemon/naemon-core/pull/236,,,True,{}
naemon/naemon-core,https://github.com/naemon/naemon-core,237,2018-06-01T13:12:18Z,2018-06-01T13:40:13Z,2018-06-01T13:40:13Z,MERGED,True,10,2,2,https://github.com/sni,bump version in spec and debian files,1,[],https://github.com/naemon/naemon-core/pull/237,https://github.com/sni,1,https://github.com/naemon/naemon-core/pull/237,bump versions in the spec file and the debian changelog as well. Otherwise the release packages would have the old version. Only daily builds get increased automatically.,bump versions in the spec file and the debian changelog as well. Otherwise the release packages would have the old version. Only daily builds get increased automatically.,True,{}
naemon/naemon-core,https://github.com/naemon/naemon-core,240,2018-06-05T11:19:27Z,2018-06-05T11:23:14Z,2018-12-03T13:13:32Z,MERGED,True,1,1,1,https://github.com/sni,Fix reload command on el6 (fixes #239),1,[],https://github.com/naemon/naemon-core/pull/240,https://github.com/sni,1,https://github.com/naemon/naemon-core/pull/240,"The backticks are missing, otherwise it trys to execute the pidfile.
This fixes #239","The backticks are missing, otherwise it trys to execute the pidfile.
This fixes #239",True,{}
naemon/naemon-core,https://github.com/naemon/naemon-core,241,2018-06-05T11:32:44Z,2018-06-05T12:28:31Z,2018-06-05T12:28:31Z,MERGED,True,4,0,1,https://github.com/wAmpIre,"daemon-init.in: Source ""/etc/default/naemon"" if exists",1,[],https://github.com/naemon/naemon-core/pull/241,https://github.com/wAmpIre,1,https://github.com/naemon/naemon-core/pull/241,Set variables for naemon process (e.g. LANG and LC_ALL) to work around some (plugin threshold) issues,Set variables for naemon process (e.g. LANG and LC_ALL) to work around some (plugin threshold) issues,True,{}
naemon/naemon-core,https://github.com/naemon/naemon-core,242,2018-06-05T13:35:26Z,2018-06-05T19:41:57Z,2018-06-05T19:41:57Z,MERGED,True,1,0,1,https://github.com/sni,add obsoletes for naemon-tools,1,[],https://github.com/naemon/naemon-core/pull/242,https://github.com/sni,1,https://github.com/naemon/naemon-core/pull/242,"naemon-tools are now part of the naemon-core project. Adding an obsoletes should
make updates smoother.
Signed-off-by: Sven Nierlein sven@nierlein.de","naemon-tools are now part of the naemon-core project. Adding an obsoletes should
make updates smoother.
Signed-off-by: Sven Nierlein sven@nierlein.de",True,{'THUMBS_UP': ['https://github.com/rephlex']}
naemon/naemon-core,https://github.com/naemon/naemon-core,242,2018-06-05T13:35:26Z,2018-06-05T19:41:57Z,2018-06-05T19:41:57Z,MERGED,True,1,0,1,https://github.com/sni,add obsoletes for naemon-tools,1,[],https://github.com/naemon/naemon-core/pull/242,https://github.com/rephlex,2,https://github.com/naemon/naemon-core/pull/242#issuecomment-394716744,"naemon-tools are now part of the naemon-core project. Adding an obsoletes should
make updates smoother.
Signed-off-by: Sven Nierlein sven@nierlein.de",sweet!,True,{}
naemon/naemon-core,https://github.com/naemon/naemon-core,245,2018-07-09T09:32:29Z,2018-07-16T14:03:01Z,2018-07-16T14:29:50Z,MERGED,True,20,2,4,https://github.com/sni,release 1.0.8,1,[],https://github.com/naemon/naemon-core/pull/245,https://github.com/sni,1,https://github.com/naemon/naemon-core/pull/245,,,True,{}
naemon/naemon-core,https://github.com/naemon/naemon-core,245,2018-07-09T09:32:29Z,2018-07-16T14:03:01Z,2018-07-16T14:29:50Z,MERGED,True,20,2,4,https://github.com/sni,release 1.0.8,1,[],https://github.com/naemon/naemon-core/pull/245,https://github.com/sni,2,https://github.com/naemon/naemon-core/pull/245#issuecomment-405253016,,i don't think its worth to mention that?,True,{}
naemon/naemon-core,https://github.com/naemon/naemon-core,245,2018-07-09T09:32:29Z,2018-07-16T14:03:01Z,2018-07-16T14:29:50Z,MERGED,True,20,2,4,https://github.com/sni,release 1.0.8,1,[],https://github.com/naemon/naemon-core/pull/245,https://github.com/jacobbaungard,3,https://github.com/naemon/naemon-core/pull/245#issuecomment-405266663,,"Given it is such a small release log, I thought we might as well, but its OK for me if you think it is not worth to mention.",True,{}
naemon/naemon-core,https://github.com/naemon/naemon-core,248,2018-07-13T19:35:57Z,2018-07-16T13:44:20Z,2018-07-16T13:44:20Z,MERGED,True,19,19,6,https://github.com/sni,add missing newlines to log_debug_info (fixes #247),1,[],https://github.com/naemon/naemon-core/pull/248,https://github.com/sni,1,https://github.com/naemon/naemon-core/pull/248,"newlines are not automatically added and must be manually appended to each log entry.
Signed-off-by: Sven Nierlein sven@nierlein.de","newlines are not automatically added and must be manually appended to each log entry.
Signed-off-by: Sven Nierlein sven@nierlein.de",True,{}
naemon/naemon-core,https://github.com/naemon/naemon-core,250,2018-07-17T15:43:57Z,2018-07-17T19:50:01Z,2018-07-17T19:50:10Z,MERGED,True,1,0,1,https://github.com/RikaDenia,Make naemon-core depend on libnaemon,1,[],https://github.com/naemon/naemon-core/pull/250,https://github.com/RikaDenia,1,https://github.com/naemon/naemon-core/pull/250,"Resolves upgrade issues when updating from 1.0.6 to 1.0.8.
Upgrading from Naemon 1.0.6 / 1.0.7 on CentOS 7 may, depending on the order yum decides to install packages, fail with:
file /usr/share/man/man8/naemonstats.8 from install of naemon-core-1.0.8-0.x86_64 conflicts with file from package libnaemon-1.0.6-1.el7.centos.x86_64
This lets naemon-core depend on libnaemon, ensuring updating always takes place in the correct order.
Fixes #249.","Resolves upgrade issues when updating from 1.0.6 to 1.0.8.
Upgrading from Naemon 1.0.6 / 1.0.7 on CentOS 7 may, depending on the order yum decides to install packages, fail with:
file /usr/share/man/man8/naemonstats.8 from install of naemon-core-1.0.8-0.x86_64 conflicts with file from package libnaemon-1.0.6-1.el7.centos.x86_64
This lets naemon-core depend on libnaemon, ensuring updating always takes place in the correct order.
Fixes #249.",True,{}
naemon/naemon-core,https://github.com/naemon/naemon-core,250,2018-07-17T15:43:57Z,2018-07-17T19:50:01Z,2018-07-17T19:50:10Z,MERGED,True,1,0,1,https://github.com/RikaDenia,Make naemon-core depend on libnaemon,1,[],https://github.com/naemon/naemon-core/pull/250,https://github.com/sni,2,https://github.com/naemon/naemon-core/pull/250#issuecomment-405705955,"Resolves upgrade issues when updating from 1.0.6 to 1.0.8.
Upgrading from Naemon 1.0.6 / 1.0.7 on CentOS 7 may, depending on the order yum decides to install packages, fail with:
file /usr/share/man/man8/naemonstats.8 from install of naemon-core-1.0.8-0.x86_64 conflicts with file from package libnaemon-1.0.6-1.el7.centos.x86_64
This lets naemon-core depend on libnaemon, ensuring updating always takes place in the correct order.
Fixes #249.","thanks, looks good.",True,{}
naemon/naemon-core,https://github.com/naemon/naemon-core,251,2018-08-01T08:17:05Z,2018-08-03T15:02:29Z,2018-12-03T13:14:02Z,MERGED,True,2,0,1,https://github.com/sni,Update control,1,[],https://github.com/naemon/naemon-core/pull/251,https://github.com/sni,1,https://github.com/naemon/naemon-core/pull/251,naemonstats has been moved to naemon-core and naemon-tools does no longer exist. Add replace/conflict tags for smooth updates.,naemonstats has been moved to naemon-core and naemon-tools does no longer exist. Add replace/conflict tags for smooth updates.,True,{}
naemon/naemon-core,https://github.com/naemon/naemon-core,254,2018-08-24T11:47:41Z,2018-08-29T07:02:38Z,2018-09-03T09:04:47Z,MERGED,True,276,2,2,https://github.com/jacobbaungard,No on-demand host check on service hard critical,1,[],https://github.com/naemon/naemon-core/pull/254,https://github.com/jacobbaungard,1,https://github.com/naemon/naemon-core/pull/254,"The current Naemon documentation states that on-demand host checks are performed when a service associated with the host changes state. This is not actually what happens. Currently on-demand host checks are scheduled on any service checks which results in a non-ok state for the service (even if there is no state change)
This commit changes that logic slightly, so that on-demand host checks are performed when the service changes state OR if the service is in a soft state. That is, we no longer do on-demand host checks on non-ok service checks, if the service is already in hard critical.
This ensures that we do not do a high amount of host checks when a host and all its services are in hard critical. For example, assume that we have a host with 20 services, and that the host and services have the same check_interval. In the situation were both the host and services are in hard critical, we would now do at least 20 host checks in every check_interval period. One host check for every service check. With this change we only schedule as host check if the service recovers again.
The time for a host to recover from an outage, can be slightly longer now if services goes up at a later time than the host itself. This could happen on a server reboot/startup as the host check(ping) might be responding marginally quicker than some of the services that are being checked.
We still do many on-demand host checks, that should ensure that the host goes into hard critical before the services, thus avoiding notification storms. This is true unless the max_attemps for the host is significantly higher than its services (and the host has few services).
More context here: https://jira.op5.com/browse/MON-5625","The current Naemon documentation states that on-demand host checks are performed when a service associated with the host changes state. This is not actually what happens. Currently on-demand host checks are scheduled on any service checks which results in a non-ok state for the service (even if there is no state change)
This commit changes that logic slightly, so that on-demand host checks are performed when the service changes state OR if the service is in a soft state. That is, we no longer do on-demand host checks on non-ok service checks, if the service is already in hard critical.
This ensures that we do not do a high amount of host checks when a host and all its services are in hard critical. For example, assume that we have a host with 20 services, and that the host and services have the same check_interval. In the situation were both the host and services are in hard critical, we would now do at least 20 host checks in every check_interval period. One host check for every service check. With this change we only schedule as host check if the service recovers again.
The time for a host to recover from an outage, can be slightly longer now if services goes up at a later time than the host itself. This could happen on a server reboot/startup as the host check(ping) might be responding marginally quicker than some of the services that are being checked.
We still do many on-demand host checks, that should ensure that the host goes into hard critical before the services, thus avoiding notification storms. This is true unless the max_attemps for the host is significantly higher than its services (and the host has few services).
More context here: https://jira.op5.com/browse/MON-5625",True,{}
naemon/naemon-core,https://github.com/naemon/naemon-core,255,2018-08-28T13:11:37Z,2018-08-29T08:32:40Z,2018-09-03T09:05:05Z,MERGED,True,15,1,2,https://github.com/jacobbaungard,parse_check: Don't escape already escaped newlines,1,[],https://github.com/naemon/naemon-core/pull/255,https://github.com/jacobbaungard,1,https://github.com/naemon/naemon-core/pull/255,"This patch ensures that newlines which are already escaped in a check results long_output is not escaped again.
When running Merlin to have a distributed Naemon setup, we had a problem where a check result coming from another node, would have double escaped new lines. Merlin sends check results to different nodes, after the check_result has been parsed. The receiving node would in turn also run it through Naemons check result parsing mechanism. This resulted in newlines being escaped twice, once on the remote node, and once on the master (for example).
This fix works by adding ""\n"" to the exception list of g_strescape, which means already escaped newlines won't be escaped again.
This fixes MON-11112","This patch ensures that newlines which are already escaped in a check results long_output is not escaped again.
When running Merlin to have a distributed Naemon setup, we had a problem where a check result coming from another node, would have double escaped new lines. Merlin sends check results to different nodes, after the check_result has been parsed. The receiving node would in turn also run it through Naemons check result parsing mechanism. This resulted in newlines being escaped twice, once on the remote node, and once on the master (for example).
This fix works by adding ""\n"" to the exception list of g_strescape, which means already escaped newlines won't be escaped again.
This fixes MON-11112",True,{}
naemon/naemon-core,https://github.com/naemon/naemon-core,255,2018-08-28T13:11:37Z,2018-08-29T08:32:40Z,2018-09-03T09:05:05Z,MERGED,True,15,1,2,https://github.com/jacobbaungard,parse_check: Don't escape already escaped newlines,1,[],https://github.com/naemon/naemon-core/pull/255,https://github.com/roengstrom,2,https://github.com/naemon/naemon-core/pull/255#issuecomment-416870038,"This patch ensures that newlines which are already escaped in a check results long_output is not escaped again.
When running Merlin to have a distributed Naemon setup, we had a problem where a check result coming from another node, would have double escaped new lines. Merlin sends check results to different nodes, after the check_result has been parsed. The receiving node would in turn also run it through Naemons check result parsing mechanism. This resulted in newlines being escaped twice, once on the remote node, and once on the master (for example).
This fix works by adding ""\n"" to the exception list of g_strescape, which means already escaped newlines won't be escaped again.
This fixes MON-11112","Nice fix, looks good to me.",True,{}
naemon/naemon-core,https://github.com/naemon/naemon-core,256,2018-08-28T13:35:42Z,2018-08-29T07:01:28Z,2018-08-29T07:01:28Z,MERGED,True,6,1,1,https://github.com/pvdputte,Reload : Fix defunct,2,[],https://github.com/naemon/naemon-core/pull/256,https://github.com/pvdputte,1,https://github.com/naemon/naemon-core/pull/256,Fixes issue #150 for debian/ubuntu packaging as well.,Fixes issue #150 for debian/ubuntu packaging as well.,True,{}
naemon/naemon-core,https://github.com/naemon/naemon-core,257,2018-09-04T12:42:09Z,2018-10-08T12:03:54Z,2018-10-08T12:03:54Z,MERGED,True,49,8,5,https://github.com/jacobbaungard,Init: Increase delay between SIGTERM and SIGKILL,2,[],https://github.com/naemon/naemon-core/pull/257,https://github.com/jacobbaungard,1,https://github.com/naemon/naemon-core/pull/257,"When using killproc to kill a process, first a SIGTERM signal is sent, and after a default of 3 seconds (5 seconds on SUSE), a SIGKILL is sent.
On larger setups we often see that 3 seconds, is not sufficient for Naemon to shutdown, and as a result retention data might not be correctly saved.
This commit increase the timeout to 90 seconds on redhat based releases and on SUSE. We only change this one these systems, as killproc seems to differ between different distributions, and might not offer the same arguments. 90 seconds timeout is the default for systemd, and the therefore the rational for choosing that value.
In addition added a log message on save of retention data at shutdown to aid debugging.
More context: MON-10565.","When using killproc to kill a process, first a SIGTERM signal is sent, and after a default of 3 seconds (5 seconds on SUSE), a SIGKILL is sent.
On larger setups we often see that 3 seconds, is not sufficient for Naemon to shutdown, and as a result retention data might not be correctly saved.
This commit increase the timeout to 90 seconds on redhat based releases and on SUSE. We only change this one these systems, as killproc seems to differ between different distributions, and might not offer the same arguments. 90 seconds timeout is the default for systemd, and the therefore the rational for choosing that value.
In addition added a log message on save of retention data at shutdown to aid debugging.
More context: MON-10565.",True,{}
naemon/naemon-core,https://github.com/naemon/naemon-core,257,2018-09-04T12:42:09Z,2018-10-08T12:03:54Z,2018-10-08T12:03:54Z,MERGED,True,49,8,5,https://github.com/jacobbaungard,Init: Increase delay between SIGTERM and SIGKILL,2,[],https://github.com/naemon/naemon-core/pull/257,https://github.com/sni,2,https://github.com/naemon/naemon-core/pull/257#issuecomment-421471316,"When using killproc to kill a process, first a SIGTERM signal is sent, and after a default of 3 seconds (5 seconds on SUSE), a SIGKILL is sent.
On larger setups we often see that 3 seconds, is not sufficient for Naemon to shutdown, and as a result retention data might not be correctly saved.
This commit increase the timeout to 90 seconds on redhat based releases and on SUSE. We only change this one these systems, as killproc seems to differ between different distributions, and might not offer the same arguments. 90 seconds timeout is the default for systemd, and the therefore the rational for choosing that value.
In addition added a log message on save of retention data at shutdown to aid debugging.
More context: MON-10565.",we should at least wait till there is a solution for #258,True,{}
naemon/naemon-core,https://github.com/naemon/naemon-core,257,2018-09-04T12:42:09Z,2018-10-08T12:03:54Z,2018-10-08T12:03:54Z,MERGED,True,49,8,5,https://github.com/jacobbaungard,Init: Increase delay between SIGTERM and SIGKILL,2,[],https://github.com/naemon/naemon-core/pull/257,https://github.com/jacobbaungard,3,https://github.com/naemon/naemon-core/pull/257#issuecomment-421919822,"When using killproc to kill a process, first a SIGTERM signal is sent, and after a default of 3 seconds (5 seconds on SUSE), a SIGKILL is sent.
On larger setups we often see that 3 seconds, is not sufficient for Naemon to shutdown, and as a result retention data might not be correctly saved.
This commit increase the timeout to 90 seconds on redhat based releases and on SUSE. We only change this one these systems, as killproc seems to differ between different distributions, and might not offer the same arguments. 90 seconds timeout is the default for systemd, and the therefore the rational for choosing that value.
In addition added a log message on save of retention data at shutdown to aid debugging.
More context: MON-10565.","For us, this is quite an important issue (i.e retention data not saved correctly), but we can just merge it downstream until #258 is fixed, if you prefer.
Note that this commit, does not impact reload, only stopping/restarting Naemon. The issue described in #258 is not as significant during stopping/restarting due to the fact that the system will kill the process if it hangs during shutdown (after 90 seconds with this commit).
As an alternative we can decrease the timeout to for example 15 seconds instead. 90 Seconds was chosen so that the behavior is the same on init and systemd based systems, but that is not so important perhaps.",True,{}
naemon/naemon-core,https://github.com/naemon/naemon-core,257,2018-09-04T12:42:09Z,2018-10-08T12:03:54Z,2018-10-08T12:03:54Z,MERGED,True,49,8,5,https://github.com/jacobbaungard,Init: Increase delay between SIGTERM and SIGKILL,2,[],https://github.com/naemon/naemon-core/pull/257,https://github.com/roengstrom,4,https://github.com/naemon/naemon-core/pull/257#issuecomment-422287682,"When using killproc to kill a process, first a SIGTERM signal is sent, and after a default of 3 seconds (5 seconds on SUSE), a SIGKILL is sent.
On larger setups we often see that 3 seconds, is not sufficient for Naemon to shutdown, and as a result retention data might not be correctly saved.
This commit increase the timeout to 90 seconds on redhat based releases and on SUSE. We only change this one these systems, as killproc seems to differ between different distributions, and might not offer the same arguments. 90 seconds timeout is the default for systemd, and the therefore the rational for choosing that value.
In addition added a log message on save of retention data at shutdown to aid debugging.
More context: MON-10565.","Like @jacobbaungard says, this only applies when restarting/shutting down naemon, so it shouldn't have any affect on #258. And I think that this is an important fix because this means that we get broken retention files on large configurations, which is very bad.
Also, I realize that livestatus is a popular module for naemon, but should fixes in the core be dependent on the behaviour of a module, just as a principle I mean?",True,{}
naemon/naemon-core,https://github.com/naemon/naemon-core,259,2018-09-10T11:07:20Z,2018-10-01T08:17:48Z,2018-10-01T08:17:55Z,MERGED,True,250,2,3,https://github.com/jacobbaungard,"Retain next_check schedule on restart (#224, #156)",2,[],https://github.com/naemon/naemon-core/pull/259,https://github.com/jacobbaungard,1,https://github.com/naemon/naemon-core/pull/259,"This commit ensures that the next_check schedule for hosts and services
are retained on Naemon restart, given that use_retained_scheduling_info
is enabled.
The logic is as follows:

If use_retained_scheduling_info is disabled, set a random time (as
before)
If use_retained_schedule_info is enabled:

If we didn't miss the check during the restart, retain the old
next_check time
If we missed one check, schedule the service/host within the next
interval_length (usually 60 seconds)
If we missed more than one check, schedule the next check randomly.



We schedule missed checks within 60 seconds, rather than immediately in
order to do some load balacing. This is also the rationale for
scheduling the check randomly, in case we missed more than one check
(this indicates Naemon has been down for a longer period of time).
This fixes:

#224
#156
MON-10720

Signed-off-by: Jacob Hansen jhansen@op5.com","This commit ensures that the next_check schedule for hosts and services
are retained on Naemon restart, given that use_retained_scheduling_info
is enabled.
The logic is as follows:

If use_retained_scheduling_info is disabled, set a random time (as
before)
If use_retained_schedule_info is enabled:

If we didn't miss the check during the restart, retain the old
next_check time
If we missed one check, schedule the service/host within the next
interval_length (usually 60 seconds)
If we missed more than one check, schedule the next check randomly.



We schedule missed checks within 60 seconds, rather than immediately in
order to do some load balacing. This is also the rationale for
scheduling the check randomly, in case we missed more than one check
(this indicates Naemon has been down for a longer period of time).
This fixes:

#224
#156
MON-10720

Signed-off-by: Jacob Hansen jhansen@op5.com",True,{'THUMBS_UP': ['https://github.com/nook24']}
naemon/naemon-core,https://github.com/naemon/naemon-core,259,2018-09-10T11:07:20Z,2018-10-01T08:17:48Z,2018-10-01T08:17:55Z,MERGED,True,250,2,3,https://github.com/jacobbaungard,"Retain next_check schedule on restart (#224, #156)",2,[],https://github.com/naemon/naemon-core/pull/259,https://github.com/sni,2,https://github.com/naemon/naemon-core/pull/259#issuecomment-419877256,"This commit ensures that the next_check schedule for hosts and services
are retained on Naemon restart, given that use_retained_scheduling_info
is enabled.
The logic is as follows:

If use_retained_scheduling_info is disabled, set a random time (as
before)
If use_retained_schedule_info is enabled:

If we didn't miss the check during the restart, retain the old
next_check time
If we missed one check, schedule the service/host within the next
interval_length (usually 60 seconds)
If we missed more than one check, schedule the next check randomly.



We schedule missed checks within 60 seconds, rather than immediately in
order to do some load balacing. This is also the rationale for
scheduling the check randomly, in case we missed more than one check
(this indicates Naemon has been down for a longer period of time).
This fixes:

#224
#156
MON-10720

Signed-off-by: Jacob Hansen jhansen@op5.com","thats great news, thanks",True,{}
naemon/naemon-core,https://github.com/naemon/naemon-core,259,2018-09-10T11:07:20Z,2018-10-01T08:17:48Z,2018-10-01T08:17:55Z,MERGED,True,250,2,3,https://github.com/jacobbaungard,"Retain next_check schedule on restart (#224, #156)",2,[],https://github.com/naemon/naemon-core/pull/259,https://github.com/jacobbaungard,3,https://github.com/naemon/naemon-core/pull/259#issuecomment-420218252,"This commit ensures that the next_check schedule for hosts and services
are retained on Naemon restart, given that use_retained_scheduling_info
is enabled.
The logic is as follows:

If use_retained_scheduling_info is disabled, set a random time (as
before)
If use_retained_schedule_info is enabled:

If we didn't miss the check during the restart, retain the old
next_check time
If we missed one check, schedule the service/host within the next
interval_length (usually 60 seconds)
If we missed more than one check, schedule the next check randomly.



We schedule missed checks within 60 seconds, rather than immediately in
order to do some load balacing. This is also the rationale for
scheduling the check randomly, in case we missed more than one check
(this indicates Naemon has been down for a longer period of time).
This fixes:

#224
#156
MON-10720

Signed-off-by: Jacob Hansen jhansen@op5.com","I think this is ready now.

Added tests
Fixed an issue from the previous commit (check_interval was in minutes instead of seconds as it should be).",True,{}
naemon/naemon-core,https://github.com/naemon/naemon-core,259,2018-09-10T11:07:20Z,2018-10-01T08:17:48Z,2018-10-01T08:17:55Z,MERGED,True,250,2,3,https://github.com/jacobbaungard,"Retain next_check schedule on restart (#224, #156)",2,[],https://github.com/naemon/naemon-core/pull/259,https://github.com/jacobbaungard,4,https://github.com/naemon/naemon-core/pull/259#issuecomment-423187904,"This commit ensures that the next_check schedule for hosts and services
are retained on Naemon restart, given that use_retained_scheduling_info
is enabled.
The logic is as follows:

If use_retained_scheduling_info is disabled, set a random time (as
before)
If use_retained_schedule_info is enabled:

If we didn't miss the check during the restart, retain the old
next_check time
If we missed one check, schedule the service/host within the next
interval_length (usually 60 seconds)
If we missed more than one check, schedule the next check randomly.



We schedule missed checks within 60 seconds, rather than immediately in
order to do some load balacing. This is also the rationale for
scheduling the check randomly, in case we missed more than one check
(this indicates Naemon has been down for a longer period of time).
This fixes:

#224
#156
MON-10720

Signed-off-by: Jacob Hansen jhansen@op5.com","@nook24, @jvigna, how does the logic in this PR sound for you guys? You think it would solve the problems you have reported?",True,{}
naemon/naemon-core,https://github.com/naemon/naemon-core,259,2018-09-10T11:07:20Z,2018-10-01T08:17:48Z,2018-10-01T08:17:55Z,MERGED,True,250,2,3,https://github.com/jacobbaungard,"Retain next_check schedule on restart (#224, #156)",2,[],https://github.com/naemon/naemon-core/pull/259,https://github.com/jvigna,5,https://github.com/naemon/naemon-core/pull/259#issuecomment-424683636,"This commit ensures that the next_check schedule for hosts and services
are retained on Naemon restart, given that use_retained_scheduling_info
is enabled.
The logic is as follows:

If use_retained_scheduling_info is disabled, set a random time (as
before)
If use_retained_schedule_info is enabled:

If we didn't miss the check during the restart, retain the old
next_check time
If we missed one check, schedule the service/host within the next
interval_length (usually 60 seconds)
If we missed more than one check, schedule the next check randomly.



We schedule missed checks within 60 seconds, rather than immediately in
order to do some load balacing. This is also the rationale for
scheduling the check randomly, in case we missed more than one check
(this indicates Naemon has been down for a longer period of time).
This fixes:

#224
#156
MON-10720

Signed-off-by: Jacob Hansen jhansen@op5.com",Look very good to me!,True,{}
naemon/naemon-core,https://github.com/naemon/naemon-core,259,2018-09-10T11:07:20Z,2018-10-01T08:17:48Z,2018-10-01T08:17:55Z,MERGED,True,250,2,3,https://github.com/jacobbaungard,"Retain next_check schedule on restart (#224, #156)",2,[],https://github.com/naemon/naemon-core/pull/259,https://github.com/jacobbaungard,6,https://github.com/naemon/naemon-core/pull/259#issuecomment-424984942,"This commit ensures that the next_check schedule for hosts and services
are retained on Naemon restart, given that use_retained_scheduling_info
is enabled.
The logic is as follows:

If use_retained_scheduling_info is disabled, set a random time (as
before)
If use_retained_schedule_info is enabled:

If we didn't miss the check during the restart, retain the old
next_check time
If we missed one check, schedule the service/host within the next
interval_length (usually 60 seconds)
If we missed more than one check, schedule the next check randomly.



We schedule missed checks within 60 seconds, rather than immediately in
order to do some load balacing. This is also the rationale for
scheduling the check randomly, in case we missed more than one check
(this indicates Naemon has been down for a longer period of time).
This fixes:

#224
#156
MON-10720

Signed-off-by: Jacob Hansen jhansen@op5.com","I will go ahead and merge this, this afternoon unless there are any objections. (Edit: Okay didn't manage that, will do so next week. Any comments are still appreciated).",True,{}
naemon/naemon-core,https://github.com/naemon/naemon-core,259,2018-09-10T11:07:20Z,2018-10-01T08:17:48Z,2018-10-01T08:17:55Z,MERGED,True,250,2,3,https://github.com/jacobbaungard,"Retain next_check schedule on restart (#224, #156)",2,[],https://github.com/naemon/naemon-core/pull/259,https://github.com/sni,7,https://github.com/naemon/naemon-core/pull/259#issuecomment-425438368,"This commit ensures that the next_check schedule for hosts and services
are retained on Naemon restart, given that use_retained_scheduling_info
is enabled.
The logic is as follows:

If use_retained_scheduling_info is disabled, set a random time (as
before)
If use_retained_schedule_info is enabled:

If we didn't miss the check during the restart, retain the old
next_check time
If we missed one check, schedule the service/host within the next
interval_length (usually 60 seconds)
If we missed more than one check, schedule the next check randomly.



We schedule missed checks within 60 seconds, rather than immediately in
order to do some load balacing. This is also the rationale for
scheduling the check randomly, in case we missed more than one check
(this indicates Naemon has been down for a longer period of time).
This fixes:

#224
#156
MON-10720

Signed-off-by: Jacob Hansen jhansen@op5.com",i was on vacation... :-) no objections.,True,{}
naemon/naemon-core,https://github.com/naemon/naemon-core,262,2018-10-01T09:03:39Z,2018-10-02T08:19:32Z,2018-10-02T08:19:50Z,MERGED,True,5,0,1,https://github.com/sni,el6: use correct logrotate script,1,[],https://github.com/naemon/naemon-core/pull/262,https://github.com/sni,1,https://github.com/naemon/naemon-core/pull/262,"Since we install the el7 logrotate in our Makefile.am without further OS
detection we need to replace the logrotate file for el6 later. Otherwise we
would end up with the el7 file and no logrotation.","Since we install the el7 logrotate in our Makefile.am without further OS
detection we need to replace the logrotate file for el6 later. Otherwise we
would end up with the el7 file and no logrotation.",True,{}
naemon/naemon-core,https://github.com/naemon/naemon-core,262,2018-10-01T09:03:39Z,2018-10-02T08:19:32Z,2018-10-02T08:19:50Z,MERGED,True,5,0,1,https://github.com/sni,el6: use correct logrotate script,1,[],https://github.com/naemon/naemon-core/pull/262,https://github.com/sni,2,https://github.com/naemon/naemon-core/pull/262#issuecomment-426189293,"Since we install the el7 logrotate in our Makefile.am without further OS
detection we need to replace the logrotate file for el6 later. Otherwise we
would end up with the el7 file and no logrotation.","thanks for reviewing. Yes, that seems to be the same thing.",True,{}
naemon/naemon-core,https://github.com/naemon/naemon-core,263,2018-10-04T20:28:12Z,2018-12-10T13:52:20Z,2018-12-10T13:52:20Z,CLOSED,False,28,0,6,https://github.com/sni,Add host_down_disable_service_checks config option,1,[],https://github.com/naemon/naemon-core/pull/263,https://github.com/sni,1,https://github.com/naemon/naemon-core/pull/263,"Add option to not check services if their host is down.
references:
- NagiosEnterprises/nagioscore@05e1dda","Add option to not check services if their host is down.
references:
- NagiosEnterprises/nagioscore@05e1dda",True,{}
naemon/naemon-core,https://github.com/naemon/naemon-core,263,2018-10-04T20:28:12Z,2018-12-10T13:52:20Z,2018-12-10T13:52:20Z,CLOSED,False,28,0,6,https://github.com/sni,Add host_down_disable_service_checks config option,1,[],https://github.com/naemon/naemon-core/pull/263,https://github.com/jacobbaungard,2,https://github.com/naemon/naemon-core/pull/263#issuecomment-427267675,"Add option to not check services if their host is down.
references:
- NagiosEnterprises/nagioscore@05e1dda",Looks interesting. Perhaps we should have a unit test for this?,True,{}
naemon/naemon-core,https://github.com/naemon/naemon-core,263,2018-10-04T20:28:12Z,2018-12-10T13:52:20Z,2018-12-10T13:52:20Z,CLOSED,False,28,0,6,https://github.com/sni,Add host_down_disable_service_checks config option,1,[],https://github.com/naemon/naemon-core/pull/263,https://github.com/jacobbaungard,3,https://github.com/naemon/naemon-core/pull/263#issuecomment-430583174,"Add option to not check services if their host is down.
references:
- NagiosEnterprises/nagioscore@05e1dda","I think this change is good, but would really like to see a short test for it as well. I think we have a reasonable test setup, so we might as well use it (useful for guarding against future regressions etc etc).",True,{}
naemon/naemon-core,https://github.com/naemon/naemon-core,263,2018-10-04T20:28:12Z,2018-12-10T13:52:20Z,2018-12-10T13:52:20Z,CLOSED,False,28,0,6,https://github.com/sni,Add host_down_disable_service_checks config option,1,[],https://github.com/naemon/naemon-core/pull/263,https://github.com/sni,4,https://github.com/naemon/naemon-core/pull/263#issuecomment-430584841,"Add option to not check services if their host is down.
references:
- NagiosEnterprises/nagioscore@05e1dda",i just didn't have time to look further into this. I will add a simple test case.,True,{}
naemon/naemon-core,https://github.com/naemon/naemon-core,263,2018-10-04T20:28:12Z,2018-12-10T13:52:20Z,2018-12-10T13:52:20Z,CLOSED,False,28,0,6,https://github.com/sni,Add host_down_disable_service_checks config option,1,[],https://github.com/naemon/naemon-core/pull/263,https://github.com/jacobbaungard,5,https://github.com/naemon/naemon-core/pull/263#issuecomment-445817205,"Add option to not check services if their host is down.
references:
- NagiosEnterprises/nagioscore@05e1dda","This should do, I think.
0001-Test-host_down_disable_service_checks.txt",True,{}
naemon/naemon-core,https://github.com/naemon/naemon-core,263,2018-10-04T20:28:12Z,2018-12-10T13:52:20Z,2018-12-10T13:52:20Z,CLOSED,False,28,0,6,https://github.com/sni,Add host_down_disable_service_checks config option,1,[],https://github.com/naemon/naemon-core/pull/263,https://github.com/sni,6,https://github.com/naemon/naemon-core/pull/263#issuecomment-445817665,"Add option to not check services if their host is down.
references:
- NagiosEnterprises/nagioscore@05e1dda","thanks, i will add that one",True,{}
naemon/naemon-core,https://github.com/naemon/naemon-core,264,2018-10-05T09:47:18Z,2018-10-05T13:43:43Z,2018-10-05T13:43:43Z,MERGED,True,190,1,4,https://github.com/sni,fix pending dependencies,1,[],https://github.com/naemon/naemon-core/pull/264,https://github.com/sni,1,https://github.com/naemon/naemon-core/pull/264,"right now we did only check the state during dependency checks. But for pending hosts
and services the state is usually OK/UP so the check passed. For pending flag checks
we have to look at the has_been_checked flag as well. This leads to the situation where
services checks will be run if the master service is in pending state even if the service
has pending service execution failure flag set.","right now we did only check the state during dependency checks. But for pending hosts
and services the state is usually OK/UP so the check passed. For pending flag checks
we have to look at the has_been_checked flag as well. This leads to the situation where
services checks will be run if the master service is in pending state even if the service
has pending service execution failure flag set.",True,{}
naemon/naemon-core,https://github.com/naemon/naemon-core,265,2018-10-10T09:14:22Z,2018-10-10T12:25:23Z,2018-10-10T12:25:26Z,MERGED,True,64,4,3,https://github.com/jacobbaungard,Always schedule next_check within check_interval,1,[],https://github.com/naemon/naemon-core/pull/265,https://github.com/jacobbaungard,1,https://github.com/naemon/naemon-core/pull/265,"After #259 we now keep the
next_check schedule over restarts if use_retained_schedule_info is
enabled. However after this patch, if one would lower the check_interval
it was possible that after the restart, the next check of an object
would be more than one check_interval away.
This commit ensures that if the next_check is more than one
check_interval away, then we randomly schedule the next check, instead
of using the retention data.
This fixed MON-11295 (https://jira.op5.com/browse/MON-11295)
Signed-off-by: Jacob Hansen jhansen@op5.com","After #259 we now keep the
next_check schedule over restarts if use_retained_schedule_info is
enabled. However after this patch, if one would lower the check_interval
it was possible that after the restart, the next check of an object
would be more than one check_interval away.
This commit ensures that if the next_check is more than one
check_interval away, then we randomly schedule the next check, instead
of using the retention data.
This fixed MON-11295 (https://jira.op5.com/browse/MON-11295)
Signed-off-by: Jacob Hansen jhansen@op5.com",True,{}
naemon/naemon-core,https://github.com/naemon/naemon-core,266,2018-10-10T10:08:19Z,2018-10-16T07:28:48Z,2018-10-16T07:28:53Z,MERGED,True,339,0,1,https://github.com/jacobbaungard,Re-add COPYING file with license,1,[],https://github.com/naemon/naemon-core/pull/266,https://github.com/jacobbaungard,1,https://github.com/naemon/naemon-core/pull/266,"Commit: 88e3b60 removed the COPYING file that contained the license for the Naemon project.
The rationale given for this, was that this is auto-generated anyway, so there is no reason to have it in the git repository. I think that it is bad practice to not include a license with a project (even if it should be auto-generated), especially on GitHub, where licenses for projects are detected.
Furthermore the auto-generated COPYING file, uses GPLv3 while the the rest of the project references to GPLv2 as the license.
I therefore propose that we re-add the COPYING file with the license to the project again.
Automake requires a COPYING file, so we use that rather than LICENSE.","Commit: 88e3b60 removed the COPYING file that contained the license for the Naemon project.
The rationale given for this, was that this is auto-generated anyway, so there is no reason to have it in the git repository. I think that it is bad practice to not include a license with a project (even if it should be auto-generated), especially on GitHub, where licenses for projects are detected.
Furthermore the auto-generated COPYING file, uses GPLv3 while the the rest of the project references to GPLv2 as the license.
I therefore propose that we re-add the COPYING file with the license to the project again.
Automake requires a COPYING file, so we use that rather than LICENSE.",True,{}
naemon/naemon-core,https://github.com/naemon/naemon-core,269,2018-11-30T14:05:37Z,2018-12-03T12:51:02Z,2018-12-03T12:51:02Z,MERGED,True,14,4,2,https://github.com/sni,fix orphaned checks logic,1,[],https://github.com/naemon/naemon-core/pull/269,https://github.com/sni,1,https://github.com/naemon/naemon-core/pull/269,"the orphaned check eventhandler checks the next_check against the expected next
check. But normal service/host check events simply run schedule_next_... so
then orphan check will never match.
right now its like this:
handle_host_check_event()
->  run_async_host_check()
->  sets is_executing true
->  check never comes back
->  next check scheduled
handle_host_check_event()
->  run_async_host_check()
->  returns an error because is_executing is still set
->  next check scheduled
So since next_check is always pushed forward, the orphan check will never
match, even if the host/service has the is_executing flag for days. To fix
this, we only reschedule the next check if the is_executing flag is false.
Now when the check takes longer than the check interval, this can lead to
situations that there is no event scheduled. So make sure we scheduled a event
when receiving a check result and there is no event yet.
Signed-off-by: Sven Nierlein sven@nierlein.de","the orphaned check eventhandler checks the next_check against the expected next
check. But normal service/host check events simply run schedule_next_... so
then orphan check will never match.
right now its like this:
handle_host_check_event()
->  run_async_host_check()
->  sets is_executing true
->  check never comes back
->  next check scheduled
handle_host_check_event()
->  run_async_host_check()
->  returns an error because is_executing is still set
->  next check scheduled
So since next_check is always pushed forward, the orphan check will never
match, even if the host/service has the is_executing flag for days. To fix
this, we only reschedule the next check if the is_executing flag is false.
Now when the check takes longer than the check interval, this can lead to
situations that there is no event scheduled. So make sure we scheduled a event
when receiving a check result and there is no event yet.
Signed-off-by: Sven Nierlein sven@nierlein.de",True,{}
naemon/naemon-core,https://github.com/naemon/naemon-core,270,2018-11-30T15:52:35Z,2018-12-03T12:51:14Z,2018-12-03T12:51:14Z,MERGED,True,2,0,1,https://github.com/sni,reset is_executing flag when processing active host check result (fix…,1,[],https://github.com/naemon/naemon-core/pull/270,https://github.com/sni,1,https://github.com/naemon/naemon-core/pull/270,"…es #154)
right now, we have to reset the flag in mod-gearman but hosts should just
behave like services here when processing check results and reset the flag on
processing an active check result.
Signed-off-by: Sven Nierlein sven@nierlein.de","…es #154)
right now, we have to reset the flag in mod-gearman but hosts should just
behave like services here when processing check results and reset the flag on
processing an active check result.
Signed-off-by: Sven Nierlein sven@nierlein.de",True,{}
naemon/naemon-core,https://github.com/naemon/naemon-core,271,2018-12-03T08:26:28Z,2018-12-03T12:51:25Z,2018-12-03T12:51:26Z,MERGED,True,3,3,3,https://github.com/sni,set flag if check is scheduled from the orphan event handler,1,[],https://github.com/naemon/naemon-core/pull/271,https://github.com/sni,1,https://github.com/naemon/naemon-core/pull/271,"this flag was used by mod-gearman to detect orphaned checks for example
from misconfiguration to submit a critical check result with a useful
message.","this flag was used by mod-gearman to detect orphaned checks for example
from misconfiguration to submit a critical check result with a useful
message.",True,{}
naemon/naemon-core,https://github.com/naemon/naemon-core,272,2018-12-03T21:44:55Z,2018-12-04T13:06:51Z,2018-12-04T13:06:51Z,MERGED,True,4,2,2,https://github.com/sni,fix memory leak when overriding checks,1,[],https://github.com/naemon/naemon-core/pull/272,https://github.com/sni,1,https://github.com/naemon/naemon-core/pull/272,"Overriding checks during the host/service_initiate stage leads to a memory leak.
Freeing the check_result pointer helps.","Overriding checks during the host/service_initiate stage leads to a memory leak.
Freeing the check_result pointer helps.",True,{}
naemon/naemon-core,https://github.com/naemon/naemon-core,274,2018-12-10T13:52:06Z,2018-12-10T14:29:51Z,2018-12-10T14:29:59Z,MERGED,True,70,0,7,https://github.com/sni,Add host down disable service checks,2,[],https://github.com/naemon/naemon-core/pull/274,https://github.com/sni,1,https://github.com/naemon/naemon-core/pull/274,"seems like the original upstream PR #263 branch has been removed meanwhile, here is a new one including the test case. thanks @jacobbaungard","seems like the original upstream PR #263 branch has been removed meanwhile, here is a new one including the test case. thanks @jacobbaungard",True,{}
naemon/naemon-core,https://github.com/naemon/naemon-core,274,2018-12-10T13:52:06Z,2018-12-10T14:29:51Z,2018-12-10T14:29:59Z,MERGED,True,70,0,7,https://github.com/sni,Add host down disable service checks,2,[],https://github.com/naemon/naemon-core/pull/274,https://github.com/sni,2,https://github.com/naemon/naemon-core/pull/274#issuecomment-445834545,"seems like the original upstream PR #263 branch has been removed meanwhile, here is a new one including the test case. thanks @jacobbaungard",thanks,True,{}
naemon/naemon-core,https://github.com/naemon/naemon-core,275,2018-12-10T16:48:29Z,2018-12-11T10:08:31Z,2018-12-11T10:08:31Z,MERGED,True,99,1,3,https://github.com/sni,fix query handler not returning command response,1,[],https://github.com/naemon/naemon-core/pull/275,https://github.com/sni,1,https://github.com/naemon/naemon-core/pull/275,"there was a missing newline which prevented the query handler
from returning the errors for commands.
add a test case to ensure this does not fail again.","there was a missing newline which prevented the query handler
from returning the errors for commands.
add a test case to ensure this does not fail again.",True,{}
naemon/naemon-core,https://github.com/naemon/naemon-core,276,2018-12-12T12:39:28Z,2018-12-13T14:10:35Z,2018-12-18T12:57:20Z,MERGED,True,6,3,1,https://github.com/jacobbaungard,Fix heap corruption when callback dereigsters itself,1,[],https://github.com/naemon/naemon-core/pull/276,https://github.com/jacobbaungard,1,https://github.com/naemon/naemon-core/pull/276,"A neb callback deregistering itself in a callback currently causes a heap
corruption. This is due to getting the naming information for the module
after the callback has been run.
This patch gets the naming information of the module before the
callback, ensuring that we will not try to access the callback pointer
after it is potentially freed.
This fixes: MON-11365 & #268
Signed-off-by: Jacob Hansen jhansen@op5.com","A neb callback deregistering itself in a callback currently causes a heap
corruption. This is due to getting the naming information for the module
after the callback has been run.
This patch gets the naming information of the module before the
callback, ensuring that we will not try to access the callback pointer
after it is potentially freed.
This fixes: MON-11365 & #268
Signed-off-by: Jacob Hansen jhansen@op5.com",True,{}
naemon/naemon-core,https://github.com/naemon/naemon-core,277,2018-12-13T10:48:55Z,2018-12-18T11:56:30Z,2018-12-18T12:57:16Z,MERGED,True,99,8,8,https://github.com/jacobbaungard,Introduce retained_scheduling_randomize_window,3,[],https://github.com/naemon/naemon-core/pull/277,https://github.com/jacobbaungard,1,https://github.com/naemon/naemon-core/pull/277,"With use_retained_scheduling_info enabled, we would schedule checks
which was missed with less than one check_interval, within one
interval_lenght.
This commit introduces a new setting
retained_scheduling_randomize_window which allows users to configure
the window in which checks that were missed over a restart is rescheduled.
This can be useful in order to increase the load balacing done after a
restart, and might be able to help fixing CPU load spikes, due to checks
being unevenly scheduled.
This part of MON-11418
Signed-off-by: Jacob Hansen jhansen@op5.com","With use_retained_scheduling_info enabled, we would schedule checks
which was missed with less than one check_interval, within one
interval_lenght.
This commit introduces a new setting
retained_scheduling_randomize_window which allows users to configure
the window in which checks that were missed over a restart is rescheduled.
This can be useful in order to increase the load balacing done after a
restart, and might be able to help fixing CPU load spikes, due to checks
being unevenly scheduled.
This part of MON-11418
Signed-off-by: Jacob Hansen jhansen@op5.com",True,{}
naemon/naemon-core,https://github.com/naemon/naemon-core,277,2018-12-13T10:48:55Z,2018-12-18T11:56:30Z,2018-12-18T12:57:16Z,MERGED,True,99,8,8,https://github.com/jacobbaungard,Introduce retained_scheduling_randomize_window,3,[],https://github.com/naemon/naemon-core/pull/277,https://github.com/sni,2,https://github.com/naemon/naemon-core/pull/277#issuecomment-446964789,"With use_retained_scheduling_info enabled, we would schedule checks
which was missed with less than one check_interval, within one
interval_lenght.
This commit introduces a new setting
retained_scheduling_randomize_window which allows users to configure
the window in which checks that were missed over a restart is rescheduled.
This can be useful in order to increase the load balacing done after a
restart, and might be able to help fixing CPU load spikes, due to checks
being unevenly scheduled.
This part of MON-11418
Signed-off-by: Jacob Hansen jhansen@op5.com","i don't think this is a good idea. With this, basically all missed checks would be randomly rescheduled within their check_interval which could be several hours which basically is the oposite of use_retained_scheduling. Maybe disabling retaining scheduling would solve the initial issue here?",True,{}
naemon/naemon-core,https://github.com/naemon/naemon-core,277,2018-12-13T10:48:55Z,2018-12-18T11:56:30Z,2018-12-18T12:57:16Z,MERGED,True,99,8,8,https://github.com/jacobbaungard,Introduce retained_scheduling_randomize_window,3,[],https://github.com/naemon/naemon-core/pull/277,https://github.com/jacobbaungard,3,https://github.com/naemon/naemon-core/pull/277#issuecomment-446981235,"With use_retained_scheduling_info enabled, we would schedule checks
which was missed with less than one check_interval, within one
interval_lenght.
This commit introduces a new setting
retained_scheduling_randomize_window which allows users to configure
the window in which checks that were missed over a restart is rescheduled.
This can be useful in order to increase the load balacing done after a
restart, and might be able to help fixing CPU load spikes, due to checks
being unevenly scheduled.
This part of MON-11418
Signed-off-by: Jacob Hansen jhansen@op5.com","Yes, disabling use_retained_scheduling more or less guarantees that Naemon will schedule the load balance evenly (given all checks take an equal amount of resources), and is a workaround for the problem encountered here.
But with the current implementation I think there is a quite high possibility of uneven load manifesting over time, which is not good. This patch should ensure the load is more or less optimal as before use_retained_scheduling info was fixed, while we still keep most check scheduling over restarts.",True,{}
naemon/naemon-core,https://github.com/naemon/naemon-core,277,2018-12-13T10:48:55Z,2018-12-18T11:56:30Z,2018-12-18T12:57:16Z,MERGED,True,99,8,8,https://github.com/jacobbaungard,Introduce retained_scheduling_randomize_window,3,[],https://github.com/naemon/naemon-core/pull/277,https://github.com/sni,4,https://github.com/naemon/naemon-core/pull/277#issuecomment-447002110,"With use_retained_scheduling_info enabled, we would schedule checks
which was missed with less than one check_interval, within one
interval_lenght.
This commit introduces a new setting
retained_scheduling_randomize_window which allows users to configure
the window in which checks that were missed over a restart is rescheduled.
This can be useful in order to increase the load balacing done after a
restart, and might be able to help fixing CPU load spikes, due to checks
being unevenly scheduled.
This part of MON-11418
Signed-off-by: Jacob Hansen jhansen@op5.com","the current implementation is not that bad I'd say. It randomly distributes load over one minute already.
I am unsure which problem the new implementation would solve. It has problems with long check intervals, because a check which only runs every couple of hours would be delayed by a long time which is better with the current implemention. And for checks with a very short interval the current implementation would randomize them anyway over a minute, so nothing would change here.",True,{}
naemon/naemon-core,https://github.com/naemon/naemon-core,277,2018-12-13T10:48:55Z,2018-12-18T11:56:30Z,2018-12-18T12:57:16Z,MERGED,True,99,8,8,https://github.com/jacobbaungard,Introduce retained_scheduling_randomize_window,3,[],https://github.com/naemon/naemon-core/pull/277,https://github.com/nook24,5,https://github.com/naemon/naemon-core/pull/277#issuecomment-447076934,"With use_retained_scheduling_info enabled, we would schedule checks
which was missed with less than one check_interval, within one
interval_lenght.
This commit introduces a new setting
retained_scheduling_randomize_window which allows users to configure
the window in which checks that were missed over a restart is rescheduled.
This can be useful in order to increase the load balacing done after a
restart, and might be able to help fixing CPU load spikes, due to checks
being unevenly scheduled.
This part of MON-11418
Signed-off-by: Jacob Hansen jhansen@op5.com","Sorry guys, I totally missed that you mentioned me in #259 (Unfortunately I haven't tested it now)
I think to make use of interval_length is not the best idea. I use an interval_length of 1. So the ""load balancing"" will be like:
delay = ranged_urand(0, 1);

in the current version.
What is the value of temp_service->next_check for new created hosts and services? 0 or current_time?",True,{}
naemon/naemon-core,https://github.com/naemon/naemon-core,277,2018-12-13T10:48:55Z,2018-12-18T11:56:30Z,2018-12-18T12:57:16Z,MERGED,True,99,8,8,https://github.com/jacobbaungard,Introduce retained_scheduling_randomize_window,3,[],https://github.com/naemon/naemon-core/pull/277,https://github.com/sni,6,https://github.com/naemon/naemon-core/pull/277#issuecomment-447131789,"With use_retained_scheduling_info enabled, we would schedule checks
which was missed with less than one check_interval, within one
interval_lenght.
This commit introduces a new setting
retained_scheduling_randomize_window which allows users to configure
the window in which checks that were missed over a restart is rescheduled.
This can be useful in order to increase the load balacing done after a
restart, and might be able to help fixing CPU load spikes, due to checks
being unevenly scheduled.
This part of MON-11418
Signed-off-by: Jacob Hansen jhansen@op5.com","valid point, so my favorite would be the current implementation with random delay up to 1 minute.",True,{}
naemon/naemon-core,https://github.com/naemon/naemon-core,277,2018-12-13T10:48:55Z,2018-12-18T11:56:30Z,2018-12-18T12:57:16Z,MERGED,True,99,8,8,https://github.com/jacobbaungard,Introduce retained_scheduling_randomize_window,3,[],https://github.com/naemon/naemon-core/pull/277,https://github.com/jacobbaungard,7,https://github.com/naemon/naemon-core/pull/277#issuecomment-447802159,"With use_retained_scheduling_info enabled, we would schedule checks
which was missed with less than one check_interval, within one
interval_lenght.
This commit introduces a new setting
retained_scheduling_randomize_window which allows users to configure
the window in which checks that were missed over a restart is rescheduled.
This can be useful in order to increase the load balacing done after a
restart, and might be able to help fixing CPU load spikes, due to checks
being unevenly scheduled.
This part of MON-11418
Signed-off-by: Jacob Hansen jhansen@op5.com","I am OK with hard-coding the value instead of using the interval_length.
However I still think this is rather likely to cause issues with load balancing. On our side, 5 minutes is the default (which I think is probably common ?). So with the current implementation that is already a very significant unbalance if you happen to restart twice in roughly the same time within that 5 minute interval (if you have a large setup where restart takes 10+ seconds)
With the solution in this PR, there is indeed a chance to miss a check with a long check_period during a restart, and then there could be significant wait time until the next check. However, the chance of this happening more than once in a row is very low, I would think. Missing one check is probably not the end of the world, none the less not very nice.
Could an alternative be to have a limit of when we schedule within 1 minute, or when we randomly reschedule? I.e. all checks missed with an check_interval less than 5, or perhaps 10 minutes are randomly scheduled, and all missed checks with longer check intervals are scheduled within 1 minute. That way we still get a good load balancing of checks with short check intervals and missed checks with long intervals are executed shortly after a restart.",True,{}
naemon/naemon-core,https://github.com/naemon/naemon-core,277,2018-12-13T10:48:55Z,2018-12-18T11:56:30Z,2018-12-18T12:57:16Z,MERGED,True,99,8,8,https://github.com/jacobbaungard,Introduce retained_scheduling_randomize_window,3,[],https://github.com/naemon/naemon-core/pull/277,https://github.com/jacobbaungard,8,https://github.com/naemon/naemon-core/pull/277#issuecomment-447895179,"With use_retained_scheduling_info enabled, we would schedule checks
which was missed with less than one check_interval, within one
interval_lenght.
This commit introduces a new setting
retained_scheduling_randomize_window which allows users to configure
the window in which checks that were missed over a restart is rescheduled.
This can be useful in order to increase the load balacing done after a
restart, and might be able to help fixing CPU load spikes, due to checks
being unevenly scheduled.
This part of MON-11418
Signed-off-by: Jacob Hansen jhansen@op5.com","Updated PR, see edited PR description.
Hope this is OK with everyone.",True,{}
naemon/naemon-core,https://github.com/naemon/naemon-core,277,2018-12-13T10:48:55Z,2018-12-18T11:56:30Z,2018-12-18T12:57:16Z,MERGED,True,99,8,8,https://github.com/jacobbaungard,Introduce retained_scheduling_randomize_window,3,[],https://github.com/naemon/naemon-core/pull/277,https://github.com/nook24,9,https://github.com/naemon/naemon-core/pull/277#issuecomment-447895932,"With use_retained_scheduling_info enabled, we would schedule checks
which was missed with less than one check_interval, within one
interval_lenght.
This commit introduces a new setting
retained_scheduling_randomize_window which allows users to configure
the window in which checks that were missed over a restart is rescheduled.
This can be useful in order to increase the load balacing done after a
restart, and might be able to help fixing CPU load spikes, due to checks
being unevenly scheduled.
This part of MON-11418
Signed-off-by: Jacob Hansen jhansen@op5.com",I will take a look,True,{}
naemon/naemon-core,https://github.com/naemon/naemon-core,277,2018-12-13T10:48:55Z,2018-12-18T11:56:30Z,2018-12-18T12:57:16Z,MERGED,True,99,8,8,https://github.com/jacobbaungard,Introduce retained_scheduling_randomize_window,3,[],https://github.com/naemon/naemon-core/pull/277,https://github.com/nook24,10,https://github.com/naemon/naemon-core/pull/277#issuecomment-447901819,"With use_retained_scheduling_info enabled, we would schedule checks
which was missed with less than one check_interval, within one
interval_lenght.
This commit introduces a new setting
retained_scheduling_randomize_window which allows users to configure
the window in which checks that were missed over a restart is rescheduled.
This can be useful in order to increase the load balacing done after a
restart, and might be able to help fixing CPU load spikes, due to checks
being unevenly scheduled.
This part of MON-11418
Signed-off-by: Jacob Hansen jhansen@op5.com",@jacobbaungard What is the value of temp_service->next_check for new created objects?,True,{}
naemon/naemon-core,https://github.com/naemon/naemon-core,277,2018-12-13T10:48:55Z,2018-12-18T11:56:30Z,2018-12-18T12:57:16Z,MERGED,True,99,8,8,https://github.com/jacobbaungard,Introduce retained_scheduling_randomize_window,3,[],https://github.com/naemon/naemon-core/pull/277,https://github.com/jacobbaungard,11,https://github.com/naemon/naemon-core/pull/277#issuecomment-447904159,"With use_retained_scheduling_info enabled, we would schedule checks
which was missed with less than one check_interval, within one
interval_lenght.
This commit introduces a new setting
retained_scheduling_randomize_window which allows users to configure
the window in which checks that were missed over a restart is rescheduled.
This can be useful in order to increase the load balacing done after a
restart, and might be able to help fixing CPU load spikes, due to checks
being unevenly scheduled.
This part of MON-11418
Signed-off-by: Jacob Hansen jhansen@op5.com","Jeez, sorry for all the random force-pushes. Couldn't get an alignment right, and then couldn't spell properly apparently. Should work again now.
@nook24 afaik it is 0, and hence the logic in place will randomly select the first check to happen within the first check_interval after the restart.
One thing to note about this approach, which is less optimal, is that if you have a check_interval < retained_scheduling_randomize_window then it might happen that the next check after restart is longer than one check_interval away. Perhaps it should be the maximum window size, but if any checks has a check interval less than the window, the check_interval is used instead?",True,{}
naemon/naemon-core,https://github.com/naemon/naemon-core,277,2018-12-13T10:48:55Z,2018-12-18T11:56:30Z,2018-12-18T12:57:16Z,MERGED,True,99,8,8,https://github.com/jacobbaungard,Introduce retained_scheduling_randomize_window,3,[],https://github.com/naemon/naemon-core/pull/277,https://github.com/nook24,12,https://github.com/naemon/naemon-core/pull/277#issuecomment-447917501,"With use_retained_scheduling_info enabled, we would schedule checks
which was missed with less than one check_interval, within one
interval_lenght.
This commit introduces a new setting
retained_scheduling_randomize_window which allows users to configure
the window in which checks that were missed over a restart is rescheduled.
This can be useful in order to increase the load balacing done after a
restart, and might be able to help fixing CPU load spikes, due to checks
being unevenly scheduled.
This part of MON-11418
Signed-off-by: Jacob Hansen jhansen@op5.com",Looks good for me too.,True,{}
naemon/naemon-core,https://github.com/naemon/naemon-core,277,2018-12-13T10:48:55Z,2018-12-18T11:56:30Z,2018-12-18T12:57:16Z,MERGED,True,99,8,8,https://github.com/jacobbaungard,Introduce retained_scheduling_randomize_window,3,[],https://github.com/naemon/naemon-core/pull/277,https://github.com/jacobbaungard,13,https://github.com/naemon/naemon-core/pull/277#issuecomment-448154617,"With use_retained_scheduling_info enabled, we would schedule checks
which was missed with less than one check_interval, within one
interval_lenght.
This commit introduces a new setting
retained_scheduling_randomize_window which allows users to configure
the window in which checks that were missed over a restart is rescheduled.
This can be useful in order to increase the load balacing done after a
restart, and might be able to help fixing CPU load spikes, due to checks
being unevenly scheduled.
This part of MON-11418
Signed-off-by: Jacob Hansen jhansen@op5.com","Just added a new commit to ensure that if the retained_scheduling_randomize_window is larger than the objects check_interval, then we use the check_interval instead.",True,{'THUMBS_UP': ['https://github.com/nook24']}
naemon/naemon-core,https://github.com/naemon/naemon-core,277,2018-12-13T10:48:55Z,2018-12-18T11:56:30Z,2018-12-18T12:57:16Z,MERGED,True,99,8,8,https://github.com/jacobbaungard,Introduce retained_scheduling_randomize_window,3,[],https://github.com/naemon/naemon-core/pull/277,https://github.com/sni,14,https://github.com/naemon/naemon-core/pull/277#issuecomment-448155001,"With use_retained_scheduling_info enabled, we would schedule checks
which was missed with less than one check_interval, within one
interval_lenght.
This commit introduces a new setting
retained_scheduling_randomize_window which allows users to configure
the window in which checks that were missed over a restart is rescheduled.
This can be useful in order to increase the load balacing done after a
restart, and might be able to help fixing CPU load spikes, due to checks
being unevenly scheduled.
This part of MON-11418
Signed-off-by: Jacob Hansen jhansen@op5.com","agreed, that sounds like a good idea.",True,{}
naemon/naemon-core,https://github.com/naemon/naemon-core,279,2018-12-18T12:18:44Z,2018-12-18T12:31:15Z,2018-12-18T12:56:54Z,MERGED,True,34,2,4,https://github.com/jacobbaungard,Release 1.0.9,1,[],https://github.com/naemon/naemon-core/pull/279,https://github.com/jacobbaungard,1,https://github.com/naemon/naemon-core/pull/279,,,True,{}
naemon/naemon-core,https://github.com/naemon/naemon-core,280,2019-01-17T21:32:28Z,2019-01-21T08:26:14Z,2019-01-21T08:26:14Z,MERGED,True,172,2,5,https://github.com/sni,fix newline handling in spoolfile results,1,[],https://github.com/naemon/naemon-core/pull/280,https://github.com/sni,1,https://github.com/naemon/naemon-core/pull/280,"newlines from spoolfiles need to be unescaped, otherwise they remain
as \n in the plugin output und multiline output parser does not
parse the output correctly.
Signed-off-by: Sven Nierlein sven@nierlein.de","newlines from spoolfiles need to be unescaped, otherwise they remain
as \n in the plugin output und multiline output parser does not
parse the output correctly.
Signed-off-by: Sven Nierlein sven@nierlein.de",True,{}
naemon/naemon-core,https://github.com/naemon/naemon-core,282,2019-03-05T11:52:21Z,2019-03-05T11:52:28Z,2019-03-19T09:50:27Z,MERGED,True,4,4,2,https://github.com/sni,allow NEBERROR_CALLBACKCANCEL from NEBTYPE_*CHECK_INITIATE,1,[],https://github.com/naemon/naemon-core/pull/282,https://github.com/sni,1,https://github.com/naemon/naemon-core/pull/282,"returning NEBERROR_CALLBACKCANCEL  from a NEBTYPE_HOSTCHECK_INITIATE or
NEBTYPE_SERVICECHECK_INITIATE neb callback resulted in naemon running the check
itself. Instead naemon should just skip the check and reschedule it.
Signed-off-by: Sven Nierlein sven@nierlein.de","returning NEBERROR_CALLBACKCANCEL  from a NEBTYPE_HOSTCHECK_INITIATE or
NEBTYPE_SERVICECHECK_INITIATE neb callback resulted in naemon running the check
itself. Instead naemon should just skip the check and reschedule it.
Signed-off-by: Sven Nierlein sven@nierlein.de",True,{}
naemon/naemon-core,https://github.com/naemon/naemon-core,282,2019-03-05T11:52:21Z,2019-03-05T11:52:28Z,2019-03-19T09:50:27Z,MERGED,True,4,4,2,https://github.com/sni,allow NEBERROR_CALLBACKCANCEL from NEBTYPE_*CHECK_INITIATE,1,[],https://github.com/naemon/naemon-core/pull/282,https://github.com/jacobbaungard,2,https://github.com/naemon/naemon-core/pull/282#issuecomment-474273304,"returning NEBERROR_CALLBACKCANCEL  from a NEBTYPE_HOSTCHECK_INITIATE or
NEBTYPE_SERVICECHECK_INITIATE neb callback resulted in naemon running the check
itself. Instead naemon should just skip the check and reschedule it.
Signed-off-by: Sven Nierlein sven@nierlein.de",Finally had time to look at this. Looks fine.,True,{}
naemon/naemon-core,https://github.com/naemon/naemon-core,283,2019-03-12T15:10:07Z,2019-03-19T09:58:39Z,2019-03-19T09:58:39Z,MERGED,True,20,2,4,https://github.com/sni,release 1.0.10,1,[],https://github.com/naemon/naemon-core/pull/283,https://github.com/sni,1,https://github.com/naemon/naemon-core/pull/283,Signed-off-by: Sven Nierlein sven@nierlein.de,Signed-off-by: Sven Nierlein sven@nierlein.de,True,{}
naemon/naemon-core,https://github.com/naemon/naemon-core,284,2019-03-22T09:47:04Z,2019-03-22T10:01:43Z,2019-03-22T10:01:43Z,MERGED,True,1,1,1,https://github.com/sni,add missing documentation,1,[],https://github.com/naemon/naemon-core/pull/284,https://github.com/sni,1,https://github.com/naemon/naemon-core/pull/284,Signed-off-by: Sven Nierlein sven@nierlein.de,Signed-off-by: Sven Nierlein sven@nierlein.de,True,{}
naemon/naemon-core,https://github.com/naemon/naemon-core,289,2019-04-09T12:29:59Z,2019-04-12T14:18:52Z,2019-04-12T14:18:52Z,MERGED,True,1,1,1,https://github.com/sni,fix loading neb module multiple times,1,[],https://github.com/naemon/naemon-core/pull/289,https://github.com/sni,1,https://github.com/naemon/naemon-core/pull/289,"loading a neb module multiple times (with different configuration), even from
different files, leads to unwanted results. For example callbacks always
targeted the first loaded module instead of the one which registered the
callback.
This has worked before already, probably because the default used lt_dlopen
without any flags which uses RTLD_LOCAL in fact.
The only scenarion when RTLD_GLOBAL would make sense would be modules which
rely on each other and i am not aware of any module doing something like that.
Signed-off-by: Sven Nierlein sven@nierlein.de","loading a neb module multiple times (with different configuration), even from
different files, leads to unwanted results. For example callbacks always
targeted the first loaded module instead of the one which registered the
callback.
This has worked before already, probably because the default used lt_dlopen
without any flags which uses RTLD_LOCAL in fact.
The only scenarion when RTLD_GLOBAL would make sense would be modules which
rely on each other and i am not aware of any module doing something like that.
Signed-off-by: Sven Nierlein sven@nierlein.de",True,{}
naemon/naemon-core,https://github.com/naemon/naemon-core,290,2019-04-10T07:34:52Z,2019-04-10T08:56:55Z,2019-04-10T08:56:55Z,MERGED,True,1,1,1,https://github.com/sni,fix format overflow,1,[],https://github.com/naemon/naemon-core/pull/290,https://github.com/sni,1,https://github.com/naemon/naemon-core/pull/290,"cmd_name may be null.
Signed-off-by: Sven Nierlein sven@nierlein.de","cmd_name may be null.
Signed-off-by: Sven Nierlein sven@nierlein.de",True,{}
naemon/naemon-core,https://github.com/naemon/naemon-core,291,2019-04-10T14:32:56Z,2019-04-15T11:09:21Z,2019-04-15T11:09:33Z,MERGED,True,4,2,1,https://github.com/nook24,Issue 287,3,[],https://github.com/naemon/naemon-core/pull/291,https://github.com/nook24,1,https://github.com/naemon/naemon-core/pull/291,"This should resolve issue #287 without breaking the Naemon logic.
Current behavior:

With this patch:

Also some more state changes:","This should resolve issue #287 without breaking the Naemon logic.
Current behavior:

With this patch:

Also some more state changes:",True,{}
naemon/naemon-core,https://github.com/naemon/naemon-core,291,2019-04-10T14:32:56Z,2019-04-15T11:09:21Z,2019-04-15T11:09:33Z,MERGED,True,4,2,1,https://github.com/nook24,Issue 287,3,[],https://github.com/naemon/naemon-core/pull/291,https://github.com/jacobbaungard,2,https://github.com/naemon/naemon-core/pull/291#issuecomment-481728840,"This should resolve issue #287 without breaking the Naemon logic.
Current behavior:

With this patch:

Also some more state changes:","If I understand it correctly this patch changes when last_hard_state is set, so that it is set after the NEB event handler callback that I assume you use to capture the results?
I see a couple of problems with this.


The meaning of last_hard_state is now inconsistent. That is for a period of time last_hard_state != current_state when state_type is hard


The meaning of last_hard_state has now changed for other modules which are using the event handler callback


I would much prefer implementing this behavior as a separate parameter - although obviously that is a lot more work.",True,{}
naemon/naemon-core,https://github.com/naemon/naemon-core,291,2019-04-10T14:32:56Z,2019-04-15T11:09:21Z,2019-04-15T11:09:33Z,MERGED,True,4,2,1,https://github.com/nook24,Issue 287,3,[],https://github.com/naemon/naemon-core/pull/291,https://github.com/ibering,3,https://github.com/naemon/naemon-core/pull/291#issuecomment-481734992,"This should resolve issue #287 without breaking the Naemon logic.
Current behavior:

With this patch:

Also some more state changes:","Isn't it now the same behavior like if the host is up?

  
    
      naemon-core/src/naemon/checks_service.c
    
    
        Lines 988 to 993
      in
      72aa5cb
    
  
  
    

        
          
           /* run the service event handler if we changed state from the last hard state or if this service is flagged as being volatile */ 
        

        
          
           if (hard_state_change == TRUE || temp_service->is_volatile == TRUE) 
        

        
          
           	handle_service_event(temp_service); 
        

        
          
            
        

        
          
           /* save the last hard state */ 
        

        
          
           temp_service->last_hard_state = temp_service->current_state; 
        
    
  


Host is Up:
Call handle_service_event() and change last_hard_state
Host is down (without patch)
Change last_hard_state

  
    
      naemon-core/src/naemon/checks_service.c
    
    
        Lines 836 to 857
      in
      72aa5cb
    
  
  
    

        
          
           if (route_result != STATE_UP) { 
        

        
          
            
        

        
          
           	log_debug_info(DEBUGL_CHECKS, 2, ""Host is not UP, so we mark state changes if appropriate\n""); 
        

        
          
            
        

        
          
           	/* ""fake"" a hard state change for the service - well, its not really fake, but it didn't get caught earlier... */ 
        

        
          
           	if (temp_service->last_hard_state != temp_service->current_state) { 
        

        
          
           		hard_state_change = TRUE; 
        

        
          
           		nm_log(NSLOG_INFO_MESSAGE, ""SERVICE INFO: %s;%s; Service switch to hard down state due to host down.\n"", temp_service->host_name, temp_service->description); 
        

        
          
           	} 
        

        
          
            
        

        
          
           	/* update last state change times */ 
        

        
          
           	if (state_change == TRUE || hard_state_change == TRUE) 
        

        
          
           		temp_service->last_state_change = temp_service->last_check; 
        

        
          
           	if (hard_state_change == TRUE) { 
        

        
          
           		temp_service->last_hard_state_change = temp_service->last_check; 
        

        
          
           		temp_service->state_type = HARD_STATE; 
        

        
          
           		temp_service->last_hard_state = temp_service->current_state; 
        

        
          
           	} 
        

        
          
            
        

        
          
           	/* put service into a hard state without attempting check retries and don't send out notifications about it */ 
        

        
          
           	temp_service->host_problem_at_last_check = TRUE; 
        

        
          
           } 
        
    
  


and call handle_service_event()

  
    
      naemon-core/src/naemon/checks_service.c
    
    
        Lines 881 to 893
      in
      72aa5cb
    
  
  
    

        
          
           if (route_result != STATE_UP) { 
        

        
          
            
        

        
          
           	log_debug_info(DEBUGL_CHECKS, 1, ""Host isn't UP, so we won't retry the service check...\n""); 
        

        
          
            
        

        
          
           	/* log the problem as a hard state if the host just went down */ 
        

        
          
           	if (hard_state_change == TRUE) { 
        

        
          
           		log_service_event(temp_service); 
        

        
          
           		alert_recorded = NEBATTR_CHECK_ALERT; 
        

        
          
            
        

        
          
           		/* run the service event handler to handle the hard state */ 
        

        
          
           		handle_service_event(temp_service); 
        

        
          
           	} 
        

        
          
           } 
        
    
  


so handle_service_event() gets called to late I think.

With the patch the behavior of Naemon is the same doesn't matters if the host is up or not up. In the current implementation, there are to different behaviors.",True,{}
naemon/naemon-core,https://github.com/naemon/naemon-core,291,2019-04-10T14:32:56Z,2019-04-15T11:09:21Z,2019-04-15T11:09:33Z,MERGED,True,4,2,1,https://github.com/nook24,Issue 287,3,[],https://github.com/naemon/naemon-core/pull/291,https://github.com/sni,4,https://github.com/naemon/naemon-core/pull/291#issuecomment-481792369,"This should resolve issue #287 without breaking the Naemon logic.
Current behavior:

With this patch:

Also some more state changes:","without having looked into details, i agree with @jacobbaungard. Also setting last_hard_state_change but not the actual state definitly results in an inconsistant wrong state where last_hard_state_change points to the current state, but the state itself is the old one.",True,{}
naemon/naemon-core,https://github.com/naemon/naemon-core,291,2019-04-10T14:32:56Z,2019-04-15T11:09:21Z,2019-04-15T11:09:33Z,MERGED,True,4,2,1,https://github.com/nook24,Issue 287,3,[],https://github.com/naemon/naemon-core/pull/291,https://github.com/ibering,5,https://github.com/naemon/naemon-core/pull/291#issuecomment-482030754,"This should resolve issue #287 without breaking the Naemon logic.
Current behavior:

With this patch:

Also some more state changes:","But it is already inconsistent now. Why is there different behavior between host UP and host NOT UP? Please see screenshots with ""Current behavior"" :/",True,{}
naemon/naemon-core,https://github.com/naemon/naemon-core,291,2019-04-10T14:32:56Z,2019-04-15T11:09:21Z,2019-04-15T11:09:33Z,MERGED,True,4,2,1,https://github.com/nook24,Issue 287,3,[],https://github.com/naemon/naemon-core/pull/291,https://github.com/sni,6,https://github.com/naemon/naemon-core/pull/291#issuecomment-482332886,"This should resolve issue #287 without breaking the Naemon logic.
Current behavior:

With this patch:

Also some more state changes:","don't get me wrong, all i am saying is, that if you move setting last_hard_state, move last_hard_state_change along with it.",True,{}
naemon/naemon-core,https://github.com/naemon/naemon-core,291,2019-04-10T14:32:56Z,2019-04-15T11:09:21Z,2019-04-15T11:09:33Z,MERGED,True,4,2,1,https://github.com/nook24,Issue 287,3,[],https://github.com/naemon/naemon-core/pull/291,https://github.com/nook24,7,https://github.com/naemon/naemon-core/pull/291#issuecomment-482452017,"This should resolve issue #287 without breaking the Naemon logic.
Current behavior:

With this patch:

Also some more state changes:","don't get me wrong, all i am saying is, that if you move setting last_hard_state, move last_hard_state_change along with it.

Yes that makes sense. I've fixed this",True,{}
naemon/naemon-core,https://github.com/naemon/naemon-core,291,2019-04-10T14:32:56Z,2019-04-15T11:09:21Z,2019-04-15T11:09:33Z,MERGED,True,4,2,1,https://github.com/nook24,Issue 287,3,[],https://github.com/naemon/naemon-core/pull/291,https://github.com/jacobbaungard,8,https://github.com/naemon/naemon-core/pull/291#issuecomment-482557212,"This should resolve issue #287 without breaking the Naemon logic.
Current behavior:

With this patch:

Also some more state changes:","I missed that there is a different logic when the host is OK - that is not good.
I guess we need to decide what last_hard_state should mean. I.e. should it be the previous hard state or the latest hard state (that is including the current_state if state_type is hard), and then implement a behavior which is consistent across all states.
I always thought it as as the latest hard state.
With the latest commit it is better, but I find it strange to pass some values to the event handler for them then to be immediately changed afterwards.",True,{}
naemon/naemon-core,https://github.com/naemon/naemon-core,291,2019-04-10T14:32:56Z,2019-04-15T11:09:21Z,2019-04-15T11:09:33Z,MERGED,True,4,2,1,https://github.com/nook24,Issue 287,3,[],https://github.com/naemon/naemon-core/pull/291,https://github.com/jacobbaungard,9,https://github.com/naemon/naemon-core/pull/291#issuecomment-482559462,"This should resolve issue #287 without breaking the Naemon logic.
Current behavior:

With this patch:

Also some more state changes:","But now, when there is a hard state change to OK, we don't actually adjust last_hard_state at all. So that should probably be fixed as well?",True,{}
naemon/naemon-core,https://github.com/naemon/naemon-core,291,2019-04-10T14:32:56Z,2019-04-15T11:09:21Z,2019-04-15T11:09:33Z,MERGED,True,4,2,1,https://github.com/nook24,Issue 287,3,[],https://github.com/naemon/naemon-core/pull/291,https://github.com/sni,10,https://github.com/naemon/naemon-core/pull/291#issuecomment-482559731,"This should resolve issue #287 without breaking the Naemon logic.
Current behavior:

With this patch:

Also some more state changes:","if thats the case, this should be fixed of course :-)",True,{}
naemon/naemon-core,https://github.com/naemon/naemon-core,291,2019-04-10T14:32:56Z,2019-04-15T11:09:21Z,2019-04-15T11:09:33Z,MERGED,True,4,2,1,https://github.com/nook24,Issue 287,3,[],https://github.com/naemon/naemon-core/pull/291,https://github.com/nook24,11,https://github.com/naemon/naemon-core/pull/291#issuecomment-482568230,"This should resolve issue #287 without breaking the Naemon logic.
Current behavior:

With this patch:

Also some more state changes:","In our case we use the data provided by broker_statechange_data to build up a state history for all hosts and services.
If a host is up last_hard_state always holds the previous hard state when broker_statechange_data gets called by Naemon.
So for me, I thought last_hard_state == previous_hard_state. Which also caused some misunderstandings in the conversation above and #291 :)
If a host is down, last_hard_state behaves slidely different when broker_statechange_data gets called it holds the current hard state. So for me this was a bug. It also makes sense for me that if the system give me a statechange event, that this event contains the previous state.
I know last_hard_state is also used in service status events. I never checked if it holds there the ""current hard state"" or the ""previous hard state""...
Enjoy the weekend :)",True,{}
naemon/naemon-core,https://github.com/naemon/naemon-core,291,2019-04-10T14:32:56Z,2019-04-15T11:09:21Z,2019-04-15T11:09:33Z,MERGED,True,4,2,1,https://github.com/nook24,Issue 287,3,[],https://github.com/naemon/naemon-core/pull/291,https://github.com/jacobbaungard,12,https://github.com/naemon/naemon-core/pull/291#issuecomment-482585404,"This should resolve issue #287 without breaking the Naemon logic.
Current behavior:

With this patch:

Also some more state changes:","While I think it is a bit odd to do the change after the after the event handler is called, I am ok with this as long as the behavior for the UP state is the same. So perhaps you could add the same logic for when there is a hard state change to UP (around L732 I believe it should be) ?",True,{}
naemon/naemon-core,https://github.com/naemon/naemon-core,291,2019-04-10T14:32:56Z,2019-04-15T11:09:21Z,2019-04-15T11:09:33Z,MERGED,True,4,2,1,https://github.com/nook24,Issue 287,3,[],https://github.com/naemon/naemon-core/pull/291,https://github.com/nook24,13,https://github.com/naemon/naemon-core/pull/291#issuecomment-483155207,"This should resolve issue #287 without breaking the Naemon logic.
Current behavior:

With this patch:

Also some more state changes:","@jacobbaungard
Unfortunately I'm not sure what I should do^^
Case ""Services goes from HARD Critical to HARD Ok (Host is UP)"":

Call to handle_service_event() in L732
Set last_hard_state to Ok in L764

Case ""Service goes from HARD Ok to HARD Critical (Host is UP)""

Call to handle_service_event() in L910
Set last_hard_state in L995

Or should I move L650 around?",True,{}
naemon/naemon-core,https://github.com/naemon/naemon-core,291,2019-04-10T14:32:56Z,2019-04-15T11:09:21Z,2019-04-15T11:09:33Z,MERGED,True,4,2,1,https://github.com/nook24,Issue 287,3,[],https://github.com/naemon/naemon-core/pull/291,https://github.com/jacobbaungard,14,https://github.com/naemon/naemon-core/pull/291#issuecomment-483208646,"This should resolve issue #287 without breaking the Naemon logic.
Current behavior:

With this patch:

Also some more state changes:","Merged this, thanks!",True,{}
naemon/naemon-core,https://github.com/naemon/naemon-core,293,2019-05-08T13:49:07Z,2019-05-11T06:07:02Z,2019-05-11T06:07:02Z,MERGED,True,71,42,2,https://github.com/sni,avoid iterating all services when expanding service lists,1,[],https://github.com/naemon/naemon-core/pull/293,https://github.com/sni,1,https://github.com/naemon/naemon-core/pull/293,"before this change, expanding wildcards or regular expressions in service lists
resulted in iterating over all services, even for static hostnames. For example
when expanding same host service dependencies still all services will be
checked.
With this patch, we create a shortlist for each hosts services and use that
if possible. Using a example configuration with 80k same host service dependencies
brings down the configuration parsing duration from 20 seconds down to 0.3 seconds.
Signed-off-by: Sven Nierlein sven@nierlein.de","before this change, expanding wildcards or regular expressions in service lists
resulted in iterating over all services, even for static hostnames. For example
when expanding same host service dependencies still all services will be
checked.
With this patch, we create a shortlist for each hosts services and use that
if possible. Using a example configuration with 80k same host service dependencies
brings down the configuration parsing duration from 20 seconds down to 0.3 seconds.
Signed-off-by: Sven Nierlein sven@nierlein.de",True,{}
naemon/naemon-core,https://github.com/naemon/naemon-core,294,2019-05-08T14:37:49Z,2019-05-13T14:05:46Z,2019-05-13T14:05:46Z,MERGED,True,626,1,6,https://github.com/sni,fix expanding service dependencies,1,[],https://github.com/naemon/naemon-core/pull/294,https://github.com/sni,1,https://github.com/naemon/naemon-core/pull/294,"when iterating over parents and childs during service dependency expansion, we
need to reset the children map for each parent. Otherwise dependencies are
only added for the first parent while it should be added for all parents.
Signed-off-by: Sven Nierlein sven@nierlein.de","when iterating over parents and childs during service dependency expansion, we
need to reset the children map for each parent. Otherwise dependencies are
only added for the first parent while it should be added for all parents.
Signed-off-by: Sven Nierlein sven@nierlein.de",True,{}
naemon/naemon-core,https://github.com/naemon/naemon-core,294,2019-05-08T14:37:49Z,2019-05-13T14:05:46Z,2019-05-13T14:05:46Z,MERGED,True,626,1,6,https://github.com/sni,fix expanding service dependencies,1,[],https://github.com/naemon/naemon-core/pull/294,https://github.com/sni,2,https://github.com/naemon/naemon-core/pull/294#issuecomment-491831678,"when iterating over parents and childs during service dependency expansion, we
need to reset the children map for each parent. Otherwise dependencies are
only added for the first parent while it should be added for all parents.
Signed-off-by: Sven Nierlein sven@nierlein.de",i added a test case and also move the bitmap_clear a bit further.,True,{}
naemon/naemon-core,https://github.com/naemon/naemon-core,295,2019-07-03T16:42:36Z,2019-07-03T18:29:55Z,2019-07-03T18:30:01Z,MERGED,True,17,17,1,https://github.com/immae,Fix format overflow,1,[],https://github.com/naemon/naemon-core/pull/295,https://github.com/immae,1,https://github.com/naemon/naemon-core/pull/295,"The compiler is complaining about some pointers or strings that may be
null.
NB: some pointers are actually null (for instance temp_ptr which has
some == NULL checks), so the message could probably be rephrased.","The compiler is complaining about some pointers or strings that may be
null.
NB: some pointers are actually null (for instance temp_ptr which has
some == NULL checks), so the message could probably be rephrased.",True,{}
naemon/naemon-core,https://github.com/naemon/naemon-core,295,2019-07-03T16:42:36Z,2019-07-03T18:29:55Z,2019-07-03T18:30:01Z,MERGED,True,17,17,1,https://github.com/immae,Fix format overflow,1,[],https://github.com/naemon/naemon-core/pull/295,https://github.com/sni,2,https://github.com/naemon/naemon-core/pull/295#issuecomment-508207325,"The compiler is complaining about some pointers or strings that may be
null.
NB: some pointers are actually null (for instance temp_ptr which has
some == NULL checks), so the message could probably be rephrased.","looks good, thanks",True,{}
naemon/naemon-core,https://github.com/naemon/naemon-core,296,2019-07-09T09:44:48Z,2019-07-11T12:10:53Z,2019-07-11T12:10:53Z,MERGED,True,37,6,9,https://github.com/sni,add last_update timestamp to track host/service changes,1,[],https://github.com/naemon/naemon-core/pull/296,https://github.com/sni,1,https://github.com/naemon/naemon-core/pull/296,"This PR adds a new attribute last_update to hosts and services. This timestamp
will be updated before and after servicechecks, on notifications and on
external commands. Once this timestamp is accessible via livestatus, it will
make synchronisation (ex. by LMD) easier.
Signed-off-by: Sven Nierlein sven@nierlein.de","This PR adds a new attribute last_update to hosts and services. This timestamp
will be updated before and after servicechecks, on notifications and on
external commands. Once this timestamp is accessible via livestatus, it will
make synchronisation (ex. by LMD) easier.
Signed-off-by: Sven Nierlein sven@nierlein.de",True,{}
naemon/naemon-core,https://github.com/naemon/naemon-core,296,2019-07-09T09:44:48Z,2019-07-11T12:10:53Z,2019-07-11T12:10:53Z,MERGED,True,37,6,9,https://github.com/sni,add last_update timestamp to track host/service changes,1,[],https://github.com/naemon/naemon-core/pull/296,https://github.com/sni,2,https://github.com/naemon/naemon-core/pull/296#issuecomment-510027191,"This PR adds a new attribute last_update to hosts and services. This timestamp
will be updated before and after servicechecks, on notifications and on
external commands. Once this timestamp is accessible via livestatus, it will
make synchronisation (ex. by LMD) easier.
Signed-off-by: Sven Nierlein sven@nierlein.de","thanks for pointing this out. last_update should only be reset to zero on restarts. That was by mistake. I moved the gettimeofday again. Should be fine again now.
Let me double check this, but my observation is that last_update and last_check should not have the same value for any check that takes more than 1 seconds. It is set to the start time of the check in
https://github.com/naemon/naemon-core/blob/master/src/naemon/checks_service.c#L460
The reason to update last_update when the check starts is that i want to catch the update of the is_executing flag.",True,{}
naemon/naemon-core,https://github.com/naemon/naemon-core,296,2019-07-09T09:44:48Z,2019-07-11T12:10:53Z,2019-07-11T12:10:53Z,MERGED,True,37,6,9,https://github.com/sni,add last_update timestamp to track host/service changes,1,[],https://github.com/naemon/naemon-core/pull/296,https://github.com/jacobbaungard,3,https://github.com/naemon/naemon-core/pull/296#issuecomment-510027657,"This PR adds a new attribute last_update to hosts and services. This timestamp
will be updated before and after servicechecks, on notifications and on
external commands. Once this timestamp is accessible via livestatus, it will
make synchronisation (ex. by LMD) easier.
Signed-off-by: Sven Nierlein sven@nierlein.de",Why do you want to reset last_update to 0 on restarts? The value is still valid after restarts if general state retention over restarts is enabled?,True,{}
naemon/naemon-core,https://github.com/naemon/naemon-core,296,2019-07-09T09:44:48Z,2019-07-11T12:10:53Z,2019-07-11T12:10:53Z,MERGED,True,37,6,9,https://github.com/sni,add last_update timestamp to track host/service changes,1,[],https://github.com/naemon/naemon-core/pull/296,https://github.com/sni,4,https://github.com/naemon/naemon-core/pull/296#issuecomment-510028420,"This PR adds a new attribute last_update to hosts and services. This timestamp
will be updated before and after servicechecks, on notifications and on
external commands. Once this timestamp is accessible via livestatus, it will
make synchronisation (ex. by LMD) easier.
Signed-off-by: Sven Nierlein sven@nierlein.de","tbh, i don't care if it is reset to zero or not since LMD will refetch everything after a reload anyway. But if there is a use case for keeping the value, i can add this as well.",True,{}
naemon/naemon-core,https://github.com/naemon/naemon-core,296,2019-07-09T09:44:48Z,2019-07-11T12:10:53Z,2019-07-11T12:10:53Z,MERGED,True,37,6,9,https://github.com/sni,add last_update timestamp to track host/service changes,1,[],https://github.com/naemon/naemon-core/pull/296,https://github.com/jacobbaungard,5,https://github.com/naemon/naemon-core/pull/296#issuecomment-510030674,"This PR adds a new attribute last_update to hosts and services. This timestamp
will be updated before and after servicechecks, on notifications and on
external commands. Once this timestamp is accessible via livestatus, it will
make synchronisation (ex. by LMD) easier.
Signed-off-by: Sven Nierlein sven@nierlein.de","I think we should keep it over restarts if state retention is enabled. I don't think we have any other fields that are not retained, if the retain settings are enabled.",True,{}
naemon/naemon-core,https://github.com/naemon/naemon-core,296,2019-07-09T09:44:48Z,2019-07-11T12:10:53Z,2019-07-11T12:10:53Z,MERGED,True,37,6,9,https://github.com/sni,add last_update timestamp to track host/service changes,1,[],https://github.com/naemon/naemon-core/pull/296,https://github.com/sni,6,https://github.com/naemon/naemon-core/pull/296#issuecomment-510033402,"This PR adds a new attribute last_update to hosts and services. This timestamp
will be updated before and after servicechecks, on notifications and on
external commands. Once this timestamp is accessible via livestatus, it will
make synchronisation (ex. by LMD) easier.
Signed-off-by: Sven Nierlein sven@nierlein.de","ok, i added retention handling.",True,{}
naemon/naemon-core,https://github.com/naemon/naemon-core,296,2019-07-09T09:44:48Z,2019-07-11T12:10:53Z,2019-07-11T12:10:53Z,MERGED,True,37,6,9,https://github.com/sni,add last_update timestamp to track host/service changes,1,[],https://github.com/naemon/naemon-core/pull/296,https://github.com/jacobbaungard,7,https://github.com/naemon/naemon-core/pull/296#issuecomment-510065635,"This PR adds a new attribute last_update to hosts and services. This timestamp
will be updated before and after servicechecks, on notifications and on
external commands. Once this timestamp is accessible via livestatus, it will
make synchronisation (ex. by LMD) easier.
Signed-off-by: Sven Nierlein sven@nierlein.de","Thanks.
Looks better. But I still get that last_update and last_check are the same after execution of the check. I.e:
# echo ""GET services\nColumns: host_name description last_check last_update execution_time\nFilter: description = slow"" | unixcat /opt/monitor/var/rw/live_tmp
monitor;slow;1562765853;1562765853;5.0039770000e+00
# printf ""[%lu] SCHEDULE_SVC_CHECK;monitor;slow;%lu\n"" `date +%s` > /opt/monitor/var/rw/nagios.cmd
# echo ""GET services\nColumns: host_name description last_check last_update execution_time\nFilter: description = slow"" | unixcat /opt/monitor/var/rw/live_tmp
monitor;slow;1562765853;1562765966;5.0039770000e+00
# echo ""GET services\nColumns: host_name description last_check last_update execution_time\nFilter: description = slow"" | unixcat /opt/monitor/var/rw/live_tmp
monitor;slow;1562765971;1562765971;5.0040860000e+00

Slow is running the following check plugin:
#!/bin/bash
sleep 5
echo ""OK""
exit 0",True,{}
naemon/naemon-core,https://github.com/naemon/naemon-core,296,2019-07-09T09:44:48Z,2019-07-11T12:10:53Z,2019-07-11T12:10:53Z,MERGED,True,37,6,9,https://github.com/sni,add last_update timestamp to track host/service changes,1,[],https://github.com/naemon/naemon-core/pull/296,https://github.com/sni,8,https://github.com/naemon/naemon-core/pull/296#issuecomment-510397768,"This PR adds a new attribute last_update to hosts and services. This timestamp
will be updated before and after servicechecks, on notifications and on
external commands. Once this timestamp is accessible via livestatus, it will
make synchronisation (ex. by LMD) easier.
Signed-off-by: Sven Nierlein sven@nierlein.de","strange, i cannot reproduce that. last_update and last_check are different here. Do you have any modules loaded (besides livestatus)?
Doing this query while a sleep 3 check is running:
GET services\nColumns: state has_been_checked is_executing last_check last_update\nFilter: description = slow\n""
results in:
0;0;0;0;0
0;0;1;0;1562834995
0;0;1;0;1562834995
0;0;1;0;1562834995
0;1;0;1562834995;1562834998
0;1;0;1562834995;1562834998

So the last_check is set to the start date of the check immediatly when the check starts. When finished, last_update is set to the end time of the check. Thats exactly what i would expect.",True,{}
naemon/naemon-core,https://github.com/naemon/naemon-core,296,2019-07-09T09:44:48Z,2019-07-11T12:10:53Z,2019-07-11T12:10:53Z,MERGED,True,37,6,9,https://github.com/sni,add last_update timestamp to track host/service changes,1,[],https://github.com/naemon/naemon-core/pull/296,https://github.com/jacobbaungard,9,https://github.com/naemon/naemon-core/pull/296#issuecomment-510447414,"This PR adds a new attribute last_update to hosts and services. This timestamp
will be updated before and after servicechecks, on notifications and on
external commands. Once this timestamp is accessible via livestatus, it will
make synchronisation (ex. by LMD) easier.
Signed-off-by: Sven Nierlein sven@nierlein.de",Seems like merlin is disrupting this somehow. Investigating.,True,{}
naemon/naemon-core,https://github.com/naemon/naemon-core,296,2019-07-09T09:44:48Z,2019-07-11T12:10:53Z,2019-07-11T12:10:53Z,MERGED,True,37,6,9,https://github.com/sni,add last_update timestamp to track host/service changes,1,[],https://github.com/naemon/naemon-core/pull/296,https://github.com/jacobbaungard,10,https://github.com/naemon/naemon-core/pull/296#issuecomment-510456980,"This PR adds a new attribute last_update to hosts and services. This timestamp
will be updated before and after servicechecks, on notifications and on
external commands. Once this timestamp is accessible via livestatus, it will
make synchronisation (ex. by LMD) easier.
Signed-off-by: Sven Nierlein sven@nierlein.de","Interesting. Merlin actually changes the last_check so that it update when the check is being processed: https://github.com/ITRS-Group/monitor-merlin/blob/d7ba3a5ac2f6e92db46dc2917b8403b8b092ddf3/module/hooks.c#L356
As a result the last_check and last_update is the same post check.",True,{}
naemon/naemon-core,https://github.com/naemon/naemon-core,296,2019-07-09T09:44:48Z,2019-07-11T12:10:53Z,2019-07-11T12:10:53Z,MERGED,True,37,6,9,https://github.com/sni,add last_update timestamp to track host/service changes,1,[],https://github.com/naemon/naemon-core/pull/296,https://github.com/sni,11,https://github.com/naemon/naemon-core/pull/296#issuecomment-510459476,"This PR adds a new attribute last_update to hosts and services. This timestamp
will be updated before and after servicechecks, on notifications and on
external commands. Once this timestamp is accessible via livestatus, it will
make synchronisation (ex. by LMD) easier.
Signed-off-by: Sven Nierlein sven@nierlein.de",might be interesting to see that behaviour in vanilla naemon as well.,True,{}
naemon/naemon-core,https://github.com/naemon/naemon-core,298,2019-07-18T07:56:29Z,2019-08-08T12:08:02Z,2019-08-08T12:08:02Z,MERGED,True,13,4,2,https://github.com/sni,fix segfault in on demand group macros,1,[],https://github.com/naemon/naemon-core/pull/298,https://github.com/sni,1,https://github.com/naemon/naemon-core/pull/298,"using on demand hostgroup macros like $HOSTSTATEID:hostgroupname:,$""  made naemon crash
Signed-off-by: Sven Nierlein sven@nierlein.de","using on demand hostgroup macros like $HOSTSTATEID:hostgroupname:,$""  made naemon crash
Signed-off-by: Sven Nierlein sven@nierlein.de",True,{}
naemon/naemon-core,https://github.com/naemon/naemon-core,301,2019-08-19T11:51:03Z,2019-08-22T07:46:59Z,2019-08-22T07:46:59Z,MERGED,True,13,25,4,https://github.com/sni,bail out if members cannot be expaned (fixes #300),5,[],https://github.com/naemon/naemon-core/pull/301,https://github.com/sni,1,https://github.com/naemon/naemon-core/pull/301,"Configuration validation should bail out if host members cannot be found. Right now it would
print an error but continue. Problem here is that the rest of the list is not parsed, so the
first unknown host cuts off the members list.
Same applies to servicegroup members.
Signed-off-by: Sven Nierlein sven@nierlein.de","Configuration validation should bail out if host members cannot be found. Right now it would
print an error but continue. Problem here is that the rest of the list is not parsed, so the
first unknown host cuts off the members list.
Same applies to servicegroup members.
Signed-off-by: Sven Nierlein sven@nierlein.de",True,{}
naemon/naemon-core,https://github.com/naemon/naemon-core,301,2019-08-19T11:51:03Z,2019-08-22T07:46:59Z,2019-08-22T07:46:59Z,MERGED,True,13,25,4,https://github.com/sni,bail out if members cannot be expaned (fixes #300),5,[],https://github.com/naemon/naemon-core/pull/301,https://github.com/jacobbaungard,2,https://github.com/naemon/naemon-core/pull/301#issuecomment-523785563,"Configuration validation should bail out if host members cannot be found. Right now it would
print an error but continue. Problem here is that the rest of the list is not parsed, so the
first unknown host cuts off the members list.
Same applies to servicegroup members.
Signed-off-by: Sven Nierlein sven@nierlein.de","Looks good, but should be rebased (hand't noticed you had included all the testcase fixes to this PR, too)",True,{}
naemon/naemon-core,https://github.com/naemon/naemon-core,312,2019-08-20T15:37:28Z,2019-08-21T16:25:26Z,2019-08-21T16:25:26Z,MERGED,True,1,14,2,https://github.com/sni,fix downtime tests,2,[],https://github.com/naemon/naemon-core/pull/312,https://github.com/sni,1,https://github.com/naemon/naemon-core/pull/312,"it is unsafe to access the downtimes after they have been removed because this
frees the underlaying memory. This might work sometimes if the memory has not
been reused meanwhile but is undefined behaviour.
The effect of has been asserted indirectly through the affected hosts/service already.
Signed-off-by: Sven Nierlein sven@nierlein.de","it is unsafe to access the downtimes after they have been removed because this
frees the underlaying memory. This might work sometimes if the memory has not
been reused meanwhile but is undefined behaviour.
The effect of has been asserted indirectly through the affected hosts/service already.
Signed-off-by: Sven Nierlein sven@nierlein.de",True,{}
naemon/naemon-core,https://github.com/naemon/naemon-core,313,2019-09-02T15:02:52Z,2019-09-04T11:07:09Z,2019-09-04T11:07:09Z,MERGED,True,26,3,4,https://github.com/sni,release 1.1.0,1,[],https://github.com/naemon/naemon-core/pull/313,https://github.com/sni,1,https://github.com/naemon/naemon-core/pull/313,Signed-off-by: Sven Nierlein sven@nierlein.de,Signed-off-by: Sven Nierlein sven@nierlein.de,True,{}
naemon/naemon-core,https://github.com/naemon/naemon-core,313,2019-09-02T15:02:52Z,2019-09-04T11:07:09Z,2019-09-04T11:07:09Z,MERGED,True,26,3,4,https://github.com/sni,release 1.1.0,1,[],https://github.com/naemon/naemon-core/pull/313,https://github.com/nook24,2,https://github.com/naemon/naemon-core/pull/313#issuecomment-527179600,Signed-off-by: Sven Nierlein sven@nierlein.de,"* add internal last_update timestamp to track host/service changes

Could this be used to resolve #162?",True,{}
naemon/naemon-core,https://github.com/naemon/naemon-core,313,2019-09-02T15:02:52Z,2019-09-04T11:07:09Z,2019-09-04T11:07:09Z,MERGED,True,26,3,4,https://github.com/sni,release 1.1.0,1,[],https://github.com/naemon/naemon-core/pull/313,https://github.com/sni,3,https://github.com/naemon/naemon-core/pull/313#issuecomment-527180045,Signed-off-by: Sven Nierlein sven@nierlein.de,"i don't think so. The last_update timestamp is useful when you want to fetch data, the issue above is when you want to push data.",True,{}
naemon/naemon-core,https://github.com/naemon/naemon-core,315,2019-10-02T11:01:12Z,2019-10-02T12:03:24Z,2019-10-02T12:55:01Z,MERGED,True,1,1,1,https://github.com/jabdr,Fix sprintf format error with clang,1,[],https://github.com/naemon/naemon-core/pull/315,https://github.com/jabdr,1,https://github.com/naemon/naemon-core/pull/315,Fixes #314,Fixes #314,True,{}
naemon/naemon-core,https://github.com/naemon/naemon-core,315,2019-10-02T11:01:12Z,2019-10-02T12:03:24Z,2019-10-02T12:55:01Z,MERGED,True,1,1,1,https://github.com/jabdr,Fix sprintf format error with clang,1,[],https://github.com/naemon/naemon-core/pull/315,https://github.com/sni,2,https://github.com/naemon/naemon-core/pull/315#issuecomment-537464580,Fixes #314,"seems like newer gccs don't like that change:
copied from https://build.opensuse.org/package/live_build_log/home:naemon:daily/naemon-core/Debian_10/i586
[  250s] src/naemon/objects_timeperiod.c: In function 'timerange2str.part.0':
[  250s] src/naemon/objects_timeperiod.c:289:26: error: '%02d' directive writing between 2 and 7 bytes into a region of size between 1 and 6 [-Werror=format-overflow=]
[  250s]   sprintf(str, ""%02d:%02d-%02d:%02d"", sh, sm, eh, em);
[  250s]                           ^~~~
[  250s] src/naemon/objects_timeperiod.c:289:15: note: directive argument in the range [0, 1193046]
[  250s]   sprintf(str, ""%02d:%02d-%02d:%02d"", sh, sm, eh, em);
[  250s]                ^~~~~~~~~~~~~~~~~~~~~
[  250s] src/naemon/objects_timeperiod.c:289:15: note: directive argument in the range [0, 59]
[  250s] In file included from /usr/include/stdio.h:873,
[  250s]                  from src/naemon/objects_common.h:10,
[  250s]                  from src/naemon/objects_timeperiod.h:11,
[  250s]                  from src/naemon/objects_timeperiod.c:1:
[  250s] /usr/include/i386-linux-gnu/bits/stdio2.h:36:10: note: '__builtin___sprintf_chk' output between 12 and 22 bytes into a destination of size 12
[  250s]    return __builtin___sprintf_chk (__s, __USE_FORTIFY_LEVEL - 1,
[  250s]           ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
[  250s]        __bos (__s), __fmt, __va_arg_pack ());
[  250s]        ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
[  251s] cc1: all warnings being treated as errors
[  251s] make[2]: *** [Makefile:2013: src/naemon/objects_timeperiod.lo] Error 1
[  251s] make[2]: Leaving directory '/usr/src/packages/BUILD'
[  251s] make[1]: *** [Makefile:1340: all] Error 2
[  251s] make[1]: Leaving directory '/usr/src/packages/BUILD'
[  251s] dh_auto_build: make -j1 returned exit code 2
[  251s] make: *** [debian/rules:80: build] Error 2
[  251s] dpkg-buildpackage: error: debian/rules build subprocess returned exit status 2",True,{}
naemon/naemon-core,https://github.com/naemon/naemon-core,315,2019-10-02T11:01:12Z,2019-10-02T12:03:24Z,2019-10-02T12:55:01Z,MERGED,True,1,1,1,https://github.com/jabdr,Fix sprintf format error with clang,1,[],https://github.com/naemon/naemon-core/pull/315,https://github.com/jabdr,3,https://github.com/naemon/naemon-core/pull/315#issuecomment-537469393,Fixes #314,"@sni I think we could also change the type of sh, sm, eh, em to short and keep the format string %02hd. clang doesn't want to compile because %02hd expects a short.",True,{}
naemon/naemon-core,https://github.com/naemon/naemon-core,315,2019-10-02T11:01:12Z,2019-10-02T12:03:24Z,2019-10-02T12:55:01Z,MERGED,True,1,1,1,https://github.com/jabdr,Fix sprintf format error with clang,1,[],https://github.com/naemon/naemon-core/pull/315,https://github.com/sni,4,https://github.com/naemon/naemon-core/pull/315#issuecomment-537471006,Fixes #314,"that would be better, yes",True,{}
naemon/naemon-core,https://github.com/naemon/naemon-core,315,2019-10-02T11:01:12Z,2019-10-02T12:03:24Z,2019-10-02T12:55:01Z,MERGED,True,1,1,1,https://github.com/jabdr,Fix sprintf format error with clang,1,[],https://github.com/naemon/naemon-core/pull/315,https://github.com/jacobbaungard,5,https://github.com/naemon/naemon-core/pull/315#issuecomment-537474031,Fixes #314,"Ah, sigh. Perhaps we should make travis build on the same platforms as OBS.
Changing it to a short make sense.",True,{}
naemon/naemon-core,https://github.com/naemon/naemon-core,315,2019-10-02T11:01:12Z,2019-10-02T12:03:24Z,2019-10-02T12:55:01Z,MERGED,True,1,1,1,https://github.com/jabdr,Fix sprintf format error with clang,1,[],https://github.com/naemon/naemon-core/pull/315,https://github.com/sni,6,https://github.com/naemon/naemon-core/pull/315#issuecomment-537476203,Fixes #314,"i just checked, travis does not offer any 32bit machines",True,{}
naemon/naemon-core,https://github.com/naemon/naemon-core,315,2019-10-02T11:01:12Z,2019-10-02T12:03:24Z,2019-10-02T12:55:01Z,MERGED,True,1,1,1,https://github.com/jabdr,Fix sprintf format error with clang,1,[],https://github.com/naemon/naemon-core/pull/315,https://github.com/jabdr,7,https://github.com/naemon/naemon-core/pull/315#issuecomment-537479005,Fixes #314,I created another pull request for that: #316,True,{}
naemon/naemon-core,https://github.com/naemon/naemon-core,316,2019-10-02T12:54:14Z,2019-10-02T15:34:46Z,2019-10-02T15:55:34Z,MERGED,True,2,2,1,https://github.com/jabdr,Change timerange2str to use short instead of int,1,[],https://github.com/naemon/naemon-core/pull/316,https://github.com/jabdr,1,https://github.com/naemon/naemon-core/pull/316,Fixes #314,Fixes #314,True,{}
naemon/naemon-core,https://github.com/naemon/naemon-core,316,2019-10-02T12:54:14Z,2019-10-02T15:34:46Z,2019-10-02T15:55:34Z,MERGED,True,2,2,1,https://github.com/jabdr,Change timerange2str to use short instead of int,1,[],https://github.com/naemon/naemon-core/pull/316,https://github.com/jacobbaungard,2,https://github.com/naemon/naemon-core/pull/316#issuecomment-537484681,Fixes #314,"Looks good to me. @sni, you think this is OK to merge, or do you want to double check with the exact config that failed before?
I checked with GCC 9.0 (on a 64bit system), and it was OK.",True,{}
naemon/naemon-core,https://github.com/naemon/naemon-core,316,2019-10-02T12:54:14Z,2019-10-02T15:34:46Z,2019-10-02T15:55:34Z,MERGED,True,2,2,1,https://github.com/jabdr,Change timerange2str to use short instead of int,1,[],https://github.com/naemon/naemon-core/pull/316,https://github.com/sni,3,https://github.com/naemon/naemon-core/pull/316#issuecomment-537549977,Fixes #314,the daily builds will start automatically once this PR is merged into master.,True,{}
naemon/naemon-core,https://github.com/naemon/naemon-core,317,2019-10-02T16:00:40Z,2019-10-03T09:54:21Z,2019-10-03T09:54:21Z,MERGED,True,1,1,1,https://github.com/sni,fix E: invalid-license,1,[],https://github.com/naemon/naemon-core/pull/317,https://github.com/sni,1,https://github.com/naemon/naemon-core/pull/317,"according to https://spdx.org/licenses/ the official license tag is ""GPL-2.0-or-later"". Otherwise
rpmlint complains about ""E: invalid-license""","according to https://spdx.org/licenses/ the official license tag is ""GPL-2.0-or-later"". Otherwise
rpmlint complains about ""E: invalid-license""",True,{}
naemon/naemon-core,https://github.com/naemon/naemon-core,317,2019-10-02T16:00:40Z,2019-10-03T09:54:21Z,2019-10-03T09:54:21Z,MERGED,True,1,1,1,https://github.com/sni,fix E: invalid-license,1,[],https://github.com/naemon/naemon-core/pull/317,https://github.com/jacobbaungard,2,https://github.com/naemon/naemon-core/pull/317#issuecomment-537821267,"according to https://spdx.org/licenses/ the official license tag is ""GPL-2.0-or-later"". Otherwise
rpmlint complains about ""E: invalid-license""",Shouldn't it be GPL-2.0-only then? Or do we allow GPLv3 ? (I don't have any strong preferences).,True,{}
naemon/naemon-core,https://github.com/naemon/naemon-core,317,2019-10-02T16:00:40Z,2019-10-03T09:54:21Z,2019-10-03T09:54:21Z,MERGED,True,1,1,1,https://github.com/sni,fix E: invalid-license,1,[],https://github.com/naemon/naemon-core/pull/317,https://github.com/sni,3,https://github.com/naemon/naemon-core/pull/317#issuecomment-537824370,"according to https://spdx.org/licenses/ the official license tag is ""GPL-2.0-or-later"". Otherwise
rpmlint complains about ""E: invalid-license""","i have no strong feelings either, i always understood gplv2 as gplv2 or later but i really don't care :-)",True,{}
naemon/naemon-core,https://github.com/naemon/naemon-core,317,2019-10-02T16:00:40Z,2019-10-03T09:54:21Z,2019-10-03T09:54:21Z,MERGED,True,1,1,1,https://github.com/sni,fix E: invalid-license,1,[],https://github.com/naemon/naemon-core/pull/317,https://github.com/jacobbaungard,4,https://github.com/naemon/naemon-core/pull/317#issuecomment-537832676,"according to https://spdx.org/licenses/ the official license tag is ""GPL-2.0-or-later"". Otherwise
rpmlint complains about ""E: invalid-license""","I think that everyone would need to explicitly agree to ""later"", so might be safer to keep it at v2.",True,{}
naemon/naemon-core,https://github.com/naemon/naemon-core,317,2019-10-02T16:00:40Z,2019-10-03T09:54:21Z,2019-10-03T09:54:21Z,MERGED,True,1,1,1,https://github.com/sni,fix E: invalid-license,1,[],https://github.com/naemon/naemon-core/pull/317,https://github.com/sni,5,https://github.com/naemon/naemon-core/pull/317#issuecomment-537837366,"according to https://spdx.org/licenses/ the official license tag is ""GPL-2.0-or-later"". Otherwise
rpmlint complains about ""E: invalid-license""","ok, changed it.",True,{}
naemon/naemon-core,https://github.com/naemon/naemon-core,318,2019-10-10T09:35:48Z,2019-10-10T09:50:37Z,2019-10-10T09:50:37Z,MERGED,True,4,1,1,https://github.com/sni,add build requirement for centos 8,1,[],https://github.com/naemon/naemon-core/pull/318,https://github.com/sni,1,https://github.com/naemon/naemon-core/pull/318,"creating debug packages requires gdb-add-index which is in package gdb-headless. Results in
ERROR: GDB index requested, but no gdb-add-index installed

otherwise.","creating debug packages requires gdb-add-index which is in package gdb-headless. Results in
ERROR: GDB index requested, but no gdb-add-index installed

otherwise.",True,{}
naemon/naemon-core,https://github.com/naemon/naemon-core,322,2020-01-21T15:26:55Z,2020-01-21T18:56:07Z,2020-01-21T18:56:13Z,MERGED,True,9,1,1,https://github.com/jacobbaungard,Retain flap detection option over restarts,1,[],https://github.com/naemon/naemon-core/pull/322,https://github.com/jacobbaungard,1,https://github.com/naemon/naemon-core/pull/322,"This commit ensures the flap detection of a host/service is retained
over restarts, when changing the flap detection option using an external
command. This is in line with other options such as passive checks,
notifications etc.","This commit ensures the flap detection of a host/service is retained
over restarts, when changing the flap detection option using an external
command. This is in line with other options such as passive checks,
notifications etc.",True,{}
naemon/naemon-core,https://github.com/naemon/naemon-core,322,2020-01-21T15:26:55Z,2020-01-21T18:56:07Z,2020-01-21T18:56:13Z,MERGED,True,9,1,1,https://github.com/jacobbaungard,Retain flap detection option over restarts,1,[],https://github.com/naemon/naemon-core/pull/322,https://github.com/sni,2,https://github.com/naemon/naemon-core/pull/322#issuecomment-576827359,"This commit ensures the flap detection of a host/service is retained
over restarts, when changing the flap detection option using an external
command. This is in line with other options such as passive checks,
notifications etc.",thanks,True,{}
naemon/naemon-core,https://github.com/naemon/naemon-core,323,2020-02-17T10:15:49Z,2020-02-17T12:57:10Z,2020-02-17T12:57:10Z,MERGED,True,80,0,7,https://github.com/sni,Feature skip check options,3,[],https://github.com/naemon/naemon-core/pull/323,https://github.com/sni,1,https://github.com/naemon/naemon-core/pull/323,,,True,{}
naemon/naemon-core,https://github.com/naemon/naemon-core,324,2020-02-17T10:41:52Z,2020-02-17T10:56:37Z,2020-02-17T10:56:42Z,MERGED,True,6,0,1,https://github.com/jacobbaungard,Don't process perfdata file if perfdata is disabled,1,[],https://github.com/naemon/naemon-core/pull/324,https://github.com/jacobbaungard,1,https://github.com/naemon/naemon-core/pull/324,"Currently, even if process_performance_data is set to 0, the commands
set by host/service_perfdata_file_processing_command are executed.
Often these file processing commands move the current perfdata files to
a spool directory. These files will be empty if
process_performance_data is set to 0. Given the files are empty
anyway, it does not make a lot of sense to process the perfdata files.
Furthermore, if no software is cleaning up the empty files from the spool
directory, the system will eventually be flooded with empty files.
This commit fixes the above behaviour by not executing the file
processing commands if performance data processing is disabled.","Currently, even if process_performance_data is set to 0, the commands
set by host/service_perfdata_file_processing_command are executed.
Often these file processing commands move the current perfdata files to
a spool directory. These files will be empty if
process_performance_data is set to 0. Given the files are empty
anyway, it does not make a lot of sense to process the perfdata files.
Furthermore, if no software is cleaning up the empty files from the spool
directory, the system will eventually be flooded with empty files.
This commit fixes the above behaviour by not executing the file
processing commands if performance data processing is disabled.",True,{}
naemon/naemon-core,https://github.com/naemon/naemon-core,325,2020-02-17T13:10:58Z,2020-02-17T13:19:19Z,2020-02-17T13:19:19Z,MERGED,True,18,2,4,https://github.com/sni,release 1.2.0,1,[],https://github.com/naemon/naemon-core/pull/325,https://github.com/sni,1,https://github.com/naemon/naemon-core/pull/325,"Add new skip check options to set a particular state on skipped checks


Retain flap detection option over restarts


Don't process perfdata file if perfdata is disabled","Add new skip check options to set a particular state on skipped checks


Retain flap detection option over restarts


Don't process perfdata file if perfdata is disabled",True,{}
naemon/naemon-core,https://github.com/naemon/naemon-core,326,2020-02-17T15:17:49Z,2020-02-19T11:34:11Z,2020-02-19T11:34:11Z,MERGED,True,3,0,1,https://github.com/sni,fix using pipes in commands (fixes #319),1,[],https://github.com/naemon/naemon-core/pull/326,https://github.com/sni,1,https://github.com/naemon/naemon-core/pull/326,"using a simple shell script like this runs into timeout when run
by naemon because SIGPIPE is beeing ignored for unknown reasons.

random-string()
{
  /bin/cat /dev/urandom | /bin/tr -dc 'a-zA-Z0-9' | /bin/fold -w ${1:-32} | /bin/head -n 1
}

echo ""OK - rand: $(random-string)""
exit 0

using the default signal handling right before we run child commands fixes this.","using a simple shell script like this runs into timeout when run
by naemon because SIGPIPE is beeing ignored for unknown reasons.

random-string()
{
  /bin/cat /dev/urandom | /bin/tr -dc 'a-zA-Z0-9' | /bin/fold -w ${1:-32} | /bin/head -n 1
}

echo ""OK - rand: $(random-string)""
exit 0

using the default signal handling right before we run child commands fixes this.",True,{}
naemon/naemon-core,https://github.com/naemon/naemon-core,328,2020-03-13T13:45:48Z,2020-03-13T13:55:26Z,2020-03-13T13:55:30Z,MERGED,True,6,3,1,https://github.com/jacobbaungard,Downtimes: don't send NEB callback if add failed,1,[],https://github.com/naemon/naemon-core/pull/328,https://github.com/jacobbaungard,1,https://github.com/naemon/naemon-core/pull/328,"Today, even if we fail to add a host or service downtime we still send a
NEB callback. This results in Livestatus adding a downtime which was
never actually added in Naemon.
This commit ensures we only send the NEB callback with
NEBTYPE_DOWNTIME_ADD if we succeeded in adding the downtime.","Today, even if we fail to add a host or service downtime we still send a
NEB callback. This results in Livestatus adding a downtime which was
never actually added in Naemon.
This commit ensures we only send the NEB callback with
NEBTYPE_DOWNTIME_ADD if we succeeded in adding the downtime.",True,{}
naemon/naemon-core,https://github.com/naemon/naemon-core,329,2020-03-22T22:45:09Z,2020-03-25T10:06:42Z,2020-03-25T10:13:31Z,MERGED,True,4,4,1,https://github.com/kjetilho,fix crash in DEL_DOWNTIME_BY_HOSTGROUP_NAME,2,[],https://github.com/naemon/naemon-core/pull/329,https://github.com/kjetilho,1,https://github.com/naemon/naemon-core/pull/329,"uninitialised struct causes dereference to random addresses.
I first thought this was due to supplying too few parameters (Icinga 1 only requires one, the hostgroup), but this command would never work, I think.","uninitialised struct causes dereference to random addresses.
I first thought this was due to supplying too few parameters (Icinga 1 only requires one, the hostgroup), but this command would never work, I think.",True,{}
naemon/naemon-core,https://github.com/naemon/naemon-core,329,2020-03-22T22:45:09Z,2020-03-25T10:06:42Z,2020-03-25T10:13:31Z,MERGED,True,4,4,1,https://github.com/kjetilho,fix crash in DEL_DOWNTIME_BY_HOSTGROUP_NAME,2,[],https://github.com/naemon/naemon-core/pull/329,https://github.com/jacobbaungard,2,https://github.com/naemon/naemon-core/pull/329#issuecomment-602519407,"uninitialised struct causes dereference to random addresses.
I first thought this was due to supplying too few parameters (Icinga 1 only requires one, the hostgroup), but this command would never work, I think.","Thanks a lot for sending this in. I tested this and it definitely fixes the crash which is great, but I still seem to be unable to remove any downtimes with this command.
Did you manage to get it to work as is?
I think we need some testing for these functions to make sure they work correctly.",True,{}
naemon/naemon-core,https://github.com/naemon/naemon-core,329,2020-03-22T22:45:09Z,2020-03-25T10:06:42Z,2020-03-25T10:13:31Z,MERGED,True,4,4,1,https://github.com/kjetilho,fix crash in DEL_DOWNTIME_BY_HOSTGROUP_NAME,2,[],https://github.com/naemon/naemon-core/pull/329,https://github.com/kjetilho,3,https://github.com/naemon/naemon-core/pull/329#issuecomment-602575582,"uninitialised struct causes dereference to random addresses.
I first thought this was due to supplying too few parameters (Icinga 1 only requires one, the hostgroup), but this command would never work, I think.","you are absolutely right!  haha.
unfortunately, this is sort of on purpose.  in src/naemon/commands.c:1101
	/* Do not allow deletion of everything - must have at least 1 filter on */
	if (hostname == NULL && service_description == NULL && start_time == 0 && cmnt == NULL)
		return deleted;
looks like the function needs an override for this case?",True,{}
naemon/naemon-core,https://github.com/naemon/naemon-core,329,2020-03-22T22:45:09Z,2020-03-25T10:06:42Z,2020-03-25T10:13:31Z,MERGED,True,4,4,1,https://github.com/kjetilho,fix crash in DEL_DOWNTIME_BY_HOSTGROUP_NAME,2,[],https://github.com/naemon/naemon-core/pull/329,https://github.com/kjetilho,4,https://github.com/naemon/naemon-core/pull/329#issuecomment-602641693,"uninitialised struct causes dereference to random addresses.
I first thought this was due to supplying too few parameters (Icinga 1 only requires one, the hostgroup), but this command would never work, I think.",I think I found a better fix (it is better when it is local).  it now works to delete all downtimes for a hostgroup.  It is also backwards compatible with Icinga 1 since the missing parameters are filled with defaults which the filter understand as unset.,True,{}
naemon/naemon-core,https://github.com/naemon/naemon-core,329,2020-03-22T22:45:09Z,2020-03-25T10:06:42Z,2020-03-25T10:13:31Z,MERGED,True,4,4,1,https://github.com/kjetilho,fix crash in DEL_DOWNTIME_BY_HOSTGROUP_NAME,2,[],https://github.com/naemon/naemon-core/pull/329,https://github.com/jacobbaungard,5,https://github.com/naemon/naemon-core/pull/329#issuecomment-602705270,"uninitialised struct causes dereference to random addresses.
I first thought this was due to supplying too few parameters (Icinga 1 only requires one, the hostgroup), but this command would never work, I think.","When I tried, I didn't manage to delete any downtimes with DEL_DOWNTIME_BY_HOSTGROUP_NAME even if I supplied all the variables.
I am not sure the dummy hostname will work correctly. When supplying a dummy host name to delete_downtime_by_hostname_service_description_start_time_comment I would assume it will fail to find any downtimes, because there is no host with the same name as the dummy name ?",True,{}
naemon/naemon-core,https://github.com/naemon/naemon-core,329,2020-03-22T22:45:09Z,2020-03-25T10:06:42Z,2020-03-25T10:13:31Z,MERGED,True,4,4,1,https://github.com/kjetilho,fix crash in DEL_DOWNTIME_BY_HOSTGROUP_NAME,2,[],https://github.com/naemon/naemon-core/pull/329,https://github.com/kjetilho,6,https://github.com/naemon/naemon-core/pull/329#issuecomment-602756450,"uninitialised struct causes dereference to random addresses.
I first thought this was due to supplying too few parameters (Icinga 1 only requires one, the hostgroup), but this command would never work, I think.","the hostname is not really a dummy, it is the hostname for each member host in the hostgroup, so it is guaranteed to match.
it is still possible to supply a hostname filter in the livestatus command, and this will be respected.  this does not make much sense, since you could use DEL_DOWNTIME_BY_HOST_NAME instead.  at best it is a failsafe, since both hostgroup and hostname must match for the command to have effect.",True,{}
naemon/naemon-core,https://github.com/naemon/naemon-core,330,2020-05-05T10:17:02Z,2020-05-05T10:57:16Z,2020-05-05T10:57:16Z,MERGED,True,1,1,1,https://github.com/sni,correct signal handler return type,1,[],https://github.com/naemon/naemon-core/pull/330,https://github.com/sni,1,https://github.com/naemon/naemon-core/pull/330,"fixes:
lib/test-iobroker.c: In function ‘conn_spam’:
lib/test-iobroker.c:125:18: error: cast between incompatible function types from ‘int (*)(int)’ to ‘void (*)(int)’ [-Werror=cast-function-type]
  125 |  signal(SIGALRM, (void (*)(int))sighandler);
      |                  ^
lib/test-iobroker.c:126:17: error: cast between incompatible function types from ‘int (*)(int)’ to ‘void (*)(int)’ [-Werror=cast-function-type]
  126 |  signal(SIGINT, (void (*)(int))sighandler);
      |                 ^

Signed-off-by: Sven Nierlein sven@nierlein.de","fixes:
lib/test-iobroker.c: In function ‘conn_spam’:
lib/test-iobroker.c:125:18: error: cast between incompatible function types from ‘int (*)(int)’ to ‘void (*)(int)’ [-Werror=cast-function-type]
  125 |  signal(SIGALRM, (void (*)(int))sighandler);
      |                  ^
lib/test-iobroker.c:126:17: error: cast between incompatible function types from ‘int (*)(int)’ to ‘void (*)(int)’ [-Werror=cast-function-type]
  126 |  signal(SIGINT, (void (*)(int))sighandler);
      |                 ^

Signed-off-by: Sven Nierlein sven@nierlein.de",True,{}
naemon/naemon-core,https://github.com/naemon/naemon-core,332,2020-05-17T21:20:35Z,2020-05-18T09:57:34Z,2020-05-18T09:57:36Z,CLOSED,False,42,13,1,https://github.com/chifac08,fixed various memory leaks in naemonstats,3,[],https://github.com/naemon/naemon-core/pull/332,https://github.com/chifac08,1,https://github.com/naemon/naemon-core/pull/332,"Hi guys,
i fell over some memory leaks in ""neamonstats"" and fixed it. Please be so kind and review my request.
i tried nearly all variations of parameters from ""naemonstats"" with valgrind. This is the result:
Call 1:
/usr/local/bin/naemonstats -s /var/lib/naemon/status.dat
/usr/local/bin/naemonstats -c /etc/naemon/naemon.cfg

Memory Leak main_config_file:
==7514== 27 bytes in 1 blocks are still reachable in loss record 3 of 9
==7514==    at 0x4C2FB0F: malloc (in /usr/lib/valgrind/vgpreload_memcheck-amd64-linux.so)
==7514==    by 0x53859B9: strdup (strdup.c:42)
==7514==    by 0x108FD1: main (naemonstats.c:235)

Memory Leaks status_file:
==7514== 33 bytes in 1 blocks are still reachable in loss record 5 of 9
==7514==    at 0x4C2FB0F: malloc (in /usr/lib/valgrind/vgpreload_memcheck-amd64-linux.so)
==7514==    by 0x53859B9: strdup (strdup.c:42)
==7514==    by 0x108EFA: main (naemonstats.c:207)

==7514== 6 bytes in 1 blocks are still reachable in loss record 2 of 9
==7514==    at 0x4C2FB0F: malloc (in /usr/lib/valgrind/vgpreload_memcheck-amd64-linux.so)
==7514==    by 0x53859B9: strdup (strdup.c:42)
==7514==    by 0x10BF9C: read_status_file (naemonstats.c:1161)
==7514==    by 0x10901F: main (naemonstats.c:378)

Memory Leaks status_version:
==13487== 6 bytes in 1 blocks are still reachable in loss record 2 of 7
==13487==    at 0x4C2FB0F: malloc (in /usr/lib/valgrind/vgpreload_memcheck-amd64-linux.so)
==13487==    by 0x574C9B9: strdup (strdup.c:42)
==13487==    by 0x10D100: read_status_file (naemonstats.c:1174)
==13487==    by 0x109667: main (naemonstats.c:380)

Call 1 Re-Check:
==29509== Memcheck, a memory error detector
==29509== Copyright (C) 2002-2017, and GNU GPL'd, by Julian Seward et al.
==29509== Using Valgrind-3.13.0 and LibVEX; rerun with -h for copyright info
==29509== Command: /usr/local/bin/naemonstats -s /var/lib/naemon/status.dat
==29509== Parent PID: 6961
==29509== 
==29509== 
==29509== HEAP SUMMARY:
==29509==     in use at exit: 18,604 bytes in 6 blocks
==29509==   total heap usage: 12 allocs, 6 frees, 24,342 bytes allocated
==29509== 
==29509== LEAK SUMMARY:
==29509==    definitely lost: 0 bytes in 0 blocks
==29509==    indirectly lost: 0 bytes in 0 blocks
==29509==      possibly lost: 0 bytes in 0 blocks
==29509==    still reachable: 18,604 bytes in 6 blocks
==29509==         suppressed: 0 bytes in 0 blocks
==29509== Rerun with --leak-check=full to see details of leaked memory
==29509== 
==29509== For counts of detected and suppressed errors, rerun with: -v
==29509== ERROR SUMMARY: 0 errors from 0 contexts (suppressed: 0 from 0)



Call 2:
/usr/local/bin/naemonstats
Memory Leak main_cfg_dir:
Method ""pcomp_construct"" allocates memory and returns a pointer. So the caller has to free the memory on purpose.
==9990== 35 bytes in 1 blocks are definitely lost in loss record 3 of 7
==9990==    at 0x4C2FB0F: malloc (in /usr/lib/valgrind/vgpreload_memcheck-amd64-linux.so)
==9990==    by 0x4ECDCE8: pcomp_construct (nspath.c:48)
==9990==    by 0x4ECE0BC: nspath_normalize (nspath.c:130)
==9990==    by 0x4ECE142: nspath_absolute (nspath.c:146)
==9990==    by 0x10BECC: read_config_file (naemonstats.c:837)
==9990==    by 0x10960F: main (naemonstats.c:370)

Call 2 fixed:

==12501== Memcheck, a memory error detector
==12501== Copyright (C) 2002-2017, and GNU GPL'd, by Julian Seward et al.
==12501== Using Valgrind-3.13.0 and LibVEX; rerun with -h for copyright info
==12501== Command: /usr/local/bin/naemonstats
==12501== 


Naemon Stats 1.2.0.g9308955f.20200511.source
Copyright (c) 2013-present Naemon Development Team (www.naemon.org)
Copyright (c) 2003-2008 Ethan Galstad (www.nagios.org)
License: GPL

Error reading status file '/usr/local/var/status.dat': No such file or directory
==12501== 
==12501== HEAP SUMMARY:
==12501==     in use at exit: 18,604 bytes in 6 blocks
==12501==   total heap usage: 17 allocs, 11 frees, 25,159 bytes allocated
==12501== 
==12501== LEAK SUMMARY:
==12501==    definitely lost: 0 bytes in 0 blocks
==12501==    indirectly lost: 0 bytes in 0 blocks
==12501==      possibly lost: 0 bytes in 0 blocks
==12501==    still reachable: 18,604 bytes in 6 blocks
==12501==         suppressed: 0 bytes in 0 blocks
==12501== Reachable blocks (those to which a pointer was found) are not shown.
==12501== To see them, rerun with: --leak-check=full --show-leak-kinds=all
==12501== 
==12501== For counts of detected and suppressed errors, rerun with: -v
==12501== ERROR SUMMARY: 0 errors from 0 contexts (suppressed: 0 from 0)


Call 3:
 /usr/local/bin/naemonstats -h
Memory Leak main_cfg_file:
==12604== 33 bytes in 1 blocks are still reachable in loss record 3 of 7
==12604==    at 0x4C2FB0F: malloc (in /usr/lib/valgrind/vgpreload_memcheck-amd64-linux.so)
==12604==    by 0x574C9B9: strdup (strdup.c:42)
==12604==    by 0x108FE0: main (naemonstats.c:207)


Call 4:
 /usr/local/bin/naemonstats -s /var/lib/naemon/status.dat -d PROGRUNTIME
Memory Leak:
==17882== 12 bytes in 1 blocks are still reachable in loss record 2 of 7
==17882==    at 0x4C2FB0F: malloc (in /usr/lib/valgrind/vgpreload_memcheck-amd64-linux.so)
==17882==    by 0x574C9B9: strdup (strdup.c:42)
==17882==    by 0x1090EC: main (naemonstats.c:241)


Fixed!

i do not post the valgrind output of every fix as this won't be necessary and will only expand the message. furthermore, i adjusted some pointer and char array initialization.
let me know if i can provide you with any further information.
thanks!","Hi guys,
i fell over some memory leaks in ""neamonstats"" and fixed it. Please be so kind and review my request.
i tried nearly all variations of parameters from ""naemonstats"" with valgrind. This is the result:
Call 1:
/usr/local/bin/naemonstats -s /var/lib/naemon/status.dat
/usr/local/bin/naemonstats -c /etc/naemon/naemon.cfg

Memory Leak main_config_file:
==7514== 27 bytes in 1 blocks are still reachable in loss record 3 of 9
==7514==    at 0x4C2FB0F: malloc (in /usr/lib/valgrind/vgpreload_memcheck-amd64-linux.so)
==7514==    by 0x53859B9: strdup (strdup.c:42)
==7514==    by 0x108FD1: main (naemonstats.c:235)

Memory Leaks status_file:
==7514== 33 bytes in 1 blocks are still reachable in loss record 5 of 9
==7514==    at 0x4C2FB0F: malloc (in /usr/lib/valgrind/vgpreload_memcheck-amd64-linux.so)
==7514==    by 0x53859B9: strdup (strdup.c:42)
==7514==    by 0x108EFA: main (naemonstats.c:207)

==7514== 6 bytes in 1 blocks are still reachable in loss record 2 of 9
==7514==    at 0x4C2FB0F: malloc (in /usr/lib/valgrind/vgpreload_memcheck-amd64-linux.so)
==7514==    by 0x53859B9: strdup (strdup.c:42)
==7514==    by 0x10BF9C: read_status_file (naemonstats.c:1161)
==7514==    by 0x10901F: main (naemonstats.c:378)

Memory Leaks status_version:
==13487== 6 bytes in 1 blocks are still reachable in loss record 2 of 7
==13487==    at 0x4C2FB0F: malloc (in /usr/lib/valgrind/vgpreload_memcheck-amd64-linux.so)
==13487==    by 0x574C9B9: strdup (strdup.c:42)
==13487==    by 0x10D100: read_status_file (naemonstats.c:1174)
==13487==    by 0x109667: main (naemonstats.c:380)

Call 1 Re-Check:
==29509== Memcheck, a memory error detector
==29509== Copyright (C) 2002-2017, and GNU GPL'd, by Julian Seward et al.
==29509== Using Valgrind-3.13.0 and LibVEX; rerun with -h for copyright info
==29509== Command: /usr/local/bin/naemonstats -s /var/lib/naemon/status.dat
==29509== Parent PID: 6961
==29509== 
==29509== 
==29509== HEAP SUMMARY:
==29509==     in use at exit: 18,604 bytes in 6 blocks
==29509==   total heap usage: 12 allocs, 6 frees, 24,342 bytes allocated
==29509== 
==29509== LEAK SUMMARY:
==29509==    definitely lost: 0 bytes in 0 blocks
==29509==    indirectly lost: 0 bytes in 0 blocks
==29509==      possibly lost: 0 bytes in 0 blocks
==29509==    still reachable: 18,604 bytes in 6 blocks
==29509==         suppressed: 0 bytes in 0 blocks
==29509== Rerun with --leak-check=full to see details of leaked memory
==29509== 
==29509== For counts of detected and suppressed errors, rerun with: -v
==29509== ERROR SUMMARY: 0 errors from 0 contexts (suppressed: 0 from 0)



Call 2:
/usr/local/bin/naemonstats
Memory Leak main_cfg_dir:
Method ""pcomp_construct"" allocates memory and returns a pointer. So the caller has to free the memory on purpose.
==9990== 35 bytes in 1 blocks are definitely lost in loss record 3 of 7
==9990==    at 0x4C2FB0F: malloc (in /usr/lib/valgrind/vgpreload_memcheck-amd64-linux.so)
==9990==    by 0x4ECDCE8: pcomp_construct (nspath.c:48)
==9990==    by 0x4ECE0BC: nspath_normalize (nspath.c:130)
==9990==    by 0x4ECE142: nspath_absolute (nspath.c:146)
==9990==    by 0x10BECC: read_config_file (naemonstats.c:837)
==9990==    by 0x10960F: main (naemonstats.c:370)

Call 2 fixed:

==12501== Memcheck, a memory error detector
==12501== Copyright (C) 2002-2017, and GNU GPL'd, by Julian Seward et al.
==12501== Using Valgrind-3.13.0 and LibVEX; rerun with -h for copyright info
==12501== Command: /usr/local/bin/naemonstats
==12501== 


Naemon Stats 1.2.0.g9308955f.20200511.source
Copyright (c) 2013-present Naemon Development Team (www.naemon.org)
Copyright (c) 2003-2008 Ethan Galstad (www.nagios.org)
License: GPL

Error reading status file '/usr/local/var/status.dat': No such file or directory
==12501== 
==12501== HEAP SUMMARY:
==12501==     in use at exit: 18,604 bytes in 6 blocks
==12501==   total heap usage: 17 allocs, 11 frees, 25,159 bytes allocated
==12501== 
==12501== LEAK SUMMARY:
==12501==    definitely lost: 0 bytes in 0 blocks
==12501==    indirectly lost: 0 bytes in 0 blocks
==12501==      possibly lost: 0 bytes in 0 blocks
==12501==    still reachable: 18,604 bytes in 6 blocks
==12501==         suppressed: 0 bytes in 0 blocks
==12501== Reachable blocks (those to which a pointer was found) are not shown.
==12501== To see them, rerun with: --leak-check=full --show-leak-kinds=all
==12501== 
==12501== For counts of detected and suppressed errors, rerun with: -v
==12501== ERROR SUMMARY: 0 errors from 0 contexts (suppressed: 0 from 0)


Call 3:
 /usr/local/bin/naemonstats -h
Memory Leak main_cfg_file:
==12604== 33 bytes in 1 blocks are still reachable in loss record 3 of 7
==12604==    at 0x4C2FB0F: malloc (in /usr/lib/valgrind/vgpreload_memcheck-amd64-linux.so)
==12604==    by 0x574C9B9: strdup (strdup.c:42)
==12604==    by 0x108FE0: main (naemonstats.c:207)


Call 4:
 /usr/local/bin/naemonstats -s /var/lib/naemon/status.dat -d PROGRUNTIME
Memory Leak:
==17882== 12 bytes in 1 blocks are still reachable in loss record 2 of 7
==17882==    at 0x4C2FB0F: malloc (in /usr/lib/valgrind/vgpreload_memcheck-amd64-linux.so)
==17882==    by 0x574C9B9: strdup (strdup.c:42)
==17882==    by 0x1090EC: main (naemonstats.c:241)


Fixed!

i do not post the valgrind output of every fix as this won't be necessary and will only expand the message. furthermore, i adjusted some pointer and char array initialization.
let me know if i can provide you with any further information.
thanks!",True,{}
naemon/naemon-core,https://github.com/naemon/naemon-core,332,2020-05-17T21:20:35Z,2020-05-18T09:57:34Z,2020-05-18T09:57:36Z,CLOSED,False,42,13,1,https://github.com/chifac08,fixed various memory leaks in naemonstats,3,[],https://github.com/naemon/naemon-core/pull/332,https://github.com/sni,2,https://github.com/naemon/naemon-core/pull/332#issuecomment-630051941,"Hi guys,
i fell over some memory leaks in ""neamonstats"" and fixed it. Please be so kind and review my request.
i tried nearly all variations of parameters from ""naemonstats"" with valgrind. This is the result:
Call 1:
/usr/local/bin/naemonstats -s /var/lib/naemon/status.dat
/usr/local/bin/naemonstats -c /etc/naemon/naemon.cfg

Memory Leak main_config_file:
==7514== 27 bytes in 1 blocks are still reachable in loss record 3 of 9
==7514==    at 0x4C2FB0F: malloc (in /usr/lib/valgrind/vgpreload_memcheck-amd64-linux.so)
==7514==    by 0x53859B9: strdup (strdup.c:42)
==7514==    by 0x108FD1: main (naemonstats.c:235)

Memory Leaks status_file:
==7514== 33 bytes in 1 blocks are still reachable in loss record 5 of 9
==7514==    at 0x4C2FB0F: malloc (in /usr/lib/valgrind/vgpreload_memcheck-amd64-linux.so)
==7514==    by 0x53859B9: strdup (strdup.c:42)
==7514==    by 0x108EFA: main (naemonstats.c:207)

==7514== 6 bytes in 1 blocks are still reachable in loss record 2 of 9
==7514==    at 0x4C2FB0F: malloc (in /usr/lib/valgrind/vgpreload_memcheck-amd64-linux.so)
==7514==    by 0x53859B9: strdup (strdup.c:42)
==7514==    by 0x10BF9C: read_status_file (naemonstats.c:1161)
==7514==    by 0x10901F: main (naemonstats.c:378)

Memory Leaks status_version:
==13487== 6 bytes in 1 blocks are still reachable in loss record 2 of 7
==13487==    at 0x4C2FB0F: malloc (in /usr/lib/valgrind/vgpreload_memcheck-amd64-linux.so)
==13487==    by 0x574C9B9: strdup (strdup.c:42)
==13487==    by 0x10D100: read_status_file (naemonstats.c:1174)
==13487==    by 0x109667: main (naemonstats.c:380)

Call 1 Re-Check:
==29509== Memcheck, a memory error detector
==29509== Copyright (C) 2002-2017, and GNU GPL'd, by Julian Seward et al.
==29509== Using Valgrind-3.13.0 and LibVEX; rerun with -h for copyright info
==29509== Command: /usr/local/bin/naemonstats -s /var/lib/naemon/status.dat
==29509== Parent PID: 6961
==29509== 
==29509== 
==29509== HEAP SUMMARY:
==29509==     in use at exit: 18,604 bytes in 6 blocks
==29509==   total heap usage: 12 allocs, 6 frees, 24,342 bytes allocated
==29509== 
==29509== LEAK SUMMARY:
==29509==    definitely lost: 0 bytes in 0 blocks
==29509==    indirectly lost: 0 bytes in 0 blocks
==29509==      possibly lost: 0 bytes in 0 blocks
==29509==    still reachable: 18,604 bytes in 6 blocks
==29509==         suppressed: 0 bytes in 0 blocks
==29509== Rerun with --leak-check=full to see details of leaked memory
==29509== 
==29509== For counts of detected and suppressed errors, rerun with: -v
==29509== ERROR SUMMARY: 0 errors from 0 contexts (suppressed: 0 from 0)



Call 2:
/usr/local/bin/naemonstats
Memory Leak main_cfg_dir:
Method ""pcomp_construct"" allocates memory and returns a pointer. So the caller has to free the memory on purpose.
==9990== 35 bytes in 1 blocks are definitely lost in loss record 3 of 7
==9990==    at 0x4C2FB0F: malloc (in /usr/lib/valgrind/vgpreload_memcheck-amd64-linux.so)
==9990==    by 0x4ECDCE8: pcomp_construct (nspath.c:48)
==9990==    by 0x4ECE0BC: nspath_normalize (nspath.c:130)
==9990==    by 0x4ECE142: nspath_absolute (nspath.c:146)
==9990==    by 0x10BECC: read_config_file (naemonstats.c:837)
==9990==    by 0x10960F: main (naemonstats.c:370)

Call 2 fixed:

==12501== Memcheck, a memory error detector
==12501== Copyright (C) 2002-2017, and GNU GPL'd, by Julian Seward et al.
==12501== Using Valgrind-3.13.0 and LibVEX; rerun with -h for copyright info
==12501== Command: /usr/local/bin/naemonstats
==12501== 


Naemon Stats 1.2.0.g9308955f.20200511.source
Copyright (c) 2013-present Naemon Development Team (www.naemon.org)
Copyright (c) 2003-2008 Ethan Galstad (www.nagios.org)
License: GPL

Error reading status file '/usr/local/var/status.dat': No such file or directory
==12501== 
==12501== HEAP SUMMARY:
==12501==     in use at exit: 18,604 bytes in 6 blocks
==12501==   total heap usage: 17 allocs, 11 frees, 25,159 bytes allocated
==12501== 
==12501== LEAK SUMMARY:
==12501==    definitely lost: 0 bytes in 0 blocks
==12501==    indirectly lost: 0 bytes in 0 blocks
==12501==      possibly lost: 0 bytes in 0 blocks
==12501==    still reachable: 18,604 bytes in 6 blocks
==12501==         suppressed: 0 bytes in 0 blocks
==12501== Reachable blocks (those to which a pointer was found) are not shown.
==12501== To see them, rerun with: --leak-check=full --show-leak-kinds=all
==12501== 
==12501== For counts of detected and suppressed errors, rerun with: -v
==12501== ERROR SUMMARY: 0 errors from 0 contexts (suppressed: 0 from 0)


Call 3:
 /usr/local/bin/naemonstats -h
Memory Leak main_cfg_file:
==12604== 33 bytes in 1 blocks are still reachable in loss record 3 of 7
==12604==    at 0x4C2FB0F: malloc (in /usr/lib/valgrind/vgpreload_memcheck-amd64-linux.so)
==12604==    by 0x574C9B9: strdup (strdup.c:42)
==12604==    by 0x108FE0: main (naemonstats.c:207)


Call 4:
 /usr/local/bin/naemonstats -s /var/lib/naemon/status.dat -d PROGRUNTIME
Memory Leak:
==17882== 12 bytes in 1 blocks are still reachable in loss record 2 of 7
==17882==    at 0x4C2FB0F: malloc (in /usr/lib/valgrind/vgpreload_memcheck-amd64-linux.so)
==17882==    by 0x574C9B9: strdup (strdup.c:42)
==17882==    by 0x1090EC: main (naemonstats.c:241)


Fixed!

i do not post the valgrind output of every fix as this won't be necessary and will only expand the message. furthermore, i adjusted some pointer and char array initialization.
let me know if i can provide you with any further information.
thanks!","i guess we never really cared about leaks in naemonstats since it isn't a long running daemon and usually done in a few seconds.
However, thanks for your patches. Keeping valgrind happy is always nice.",True,{'LAUGH': ['https://github.com/chifac08']}
naemon/naemon-core,https://github.com/naemon/naemon-core,332,2020-05-17T21:20:35Z,2020-05-18T09:57:34Z,2020-05-18T09:57:36Z,CLOSED,False,42,13,1,https://github.com/chifac08,fixed various memory leaks in naemonstats,3,[],https://github.com/naemon/naemon-core/pull/332,https://github.com/chifac08,3,https://github.com/naemon/naemon-core/pull/332#issuecomment-630054661,"Hi guys,
i fell over some memory leaks in ""neamonstats"" and fixed it. Please be so kind and review my request.
i tried nearly all variations of parameters from ""naemonstats"" with valgrind. This is the result:
Call 1:
/usr/local/bin/naemonstats -s /var/lib/naemon/status.dat
/usr/local/bin/naemonstats -c /etc/naemon/naemon.cfg

Memory Leak main_config_file:
==7514== 27 bytes in 1 blocks are still reachable in loss record 3 of 9
==7514==    at 0x4C2FB0F: malloc (in /usr/lib/valgrind/vgpreload_memcheck-amd64-linux.so)
==7514==    by 0x53859B9: strdup (strdup.c:42)
==7514==    by 0x108FD1: main (naemonstats.c:235)

Memory Leaks status_file:
==7514== 33 bytes in 1 blocks are still reachable in loss record 5 of 9
==7514==    at 0x4C2FB0F: malloc (in /usr/lib/valgrind/vgpreload_memcheck-amd64-linux.so)
==7514==    by 0x53859B9: strdup (strdup.c:42)
==7514==    by 0x108EFA: main (naemonstats.c:207)

==7514== 6 bytes in 1 blocks are still reachable in loss record 2 of 9
==7514==    at 0x4C2FB0F: malloc (in /usr/lib/valgrind/vgpreload_memcheck-amd64-linux.so)
==7514==    by 0x53859B9: strdup (strdup.c:42)
==7514==    by 0x10BF9C: read_status_file (naemonstats.c:1161)
==7514==    by 0x10901F: main (naemonstats.c:378)

Memory Leaks status_version:
==13487== 6 bytes in 1 blocks are still reachable in loss record 2 of 7
==13487==    at 0x4C2FB0F: malloc (in /usr/lib/valgrind/vgpreload_memcheck-amd64-linux.so)
==13487==    by 0x574C9B9: strdup (strdup.c:42)
==13487==    by 0x10D100: read_status_file (naemonstats.c:1174)
==13487==    by 0x109667: main (naemonstats.c:380)

Call 1 Re-Check:
==29509== Memcheck, a memory error detector
==29509== Copyright (C) 2002-2017, and GNU GPL'd, by Julian Seward et al.
==29509== Using Valgrind-3.13.0 and LibVEX; rerun with -h for copyright info
==29509== Command: /usr/local/bin/naemonstats -s /var/lib/naemon/status.dat
==29509== Parent PID: 6961
==29509== 
==29509== 
==29509== HEAP SUMMARY:
==29509==     in use at exit: 18,604 bytes in 6 blocks
==29509==   total heap usage: 12 allocs, 6 frees, 24,342 bytes allocated
==29509== 
==29509== LEAK SUMMARY:
==29509==    definitely lost: 0 bytes in 0 blocks
==29509==    indirectly lost: 0 bytes in 0 blocks
==29509==      possibly lost: 0 bytes in 0 blocks
==29509==    still reachable: 18,604 bytes in 6 blocks
==29509==         suppressed: 0 bytes in 0 blocks
==29509== Rerun with --leak-check=full to see details of leaked memory
==29509== 
==29509== For counts of detected and suppressed errors, rerun with: -v
==29509== ERROR SUMMARY: 0 errors from 0 contexts (suppressed: 0 from 0)



Call 2:
/usr/local/bin/naemonstats
Memory Leak main_cfg_dir:
Method ""pcomp_construct"" allocates memory and returns a pointer. So the caller has to free the memory on purpose.
==9990== 35 bytes in 1 blocks are definitely lost in loss record 3 of 7
==9990==    at 0x4C2FB0F: malloc (in /usr/lib/valgrind/vgpreload_memcheck-amd64-linux.so)
==9990==    by 0x4ECDCE8: pcomp_construct (nspath.c:48)
==9990==    by 0x4ECE0BC: nspath_normalize (nspath.c:130)
==9990==    by 0x4ECE142: nspath_absolute (nspath.c:146)
==9990==    by 0x10BECC: read_config_file (naemonstats.c:837)
==9990==    by 0x10960F: main (naemonstats.c:370)

Call 2 fixed:

==12501== Memcheck, a memory error detector
==12501== Copyright (C) 2002-2017, and GNU GPL'd, by Julian Seward et al.
==12501== Using Valgrind-3.13.0 and LibVEX; rerun with -h for copyright info
==12501== Command: /usr/local/bin/naemonstats
==12501== 


Naemon Stats 1.2.0.g9308955f.20200511.source
Copyright (c) 2013-present Naemon Development Team (www.naemon.org)
Copyright (c) 2003-2008 Ethan Galstad (www.nagios.org)
License: GPL

Error reading status file '/usr/local/var/status.dat': No such file or directory
==12501== 
==12501== HEAP SUMMARY:
==12501==     in use at exit: 18,604 bytes in 6 blocks
==12501==   total heap usage: 17 allocs, 11 frees, 25,159 bytes allocated
==12501== 
==12501== LEAK SUMMARY:
==12501==    definitely lost: 0 bytes in 0 blocks
==12501==    indirectly lost: 0 bytes in 0 blocks
==12501==      possibly lost: 0 bytes in 0 blocks
==12501==    still reachable: 18,604 bytes in 6 blocks
==12501==         suppressed: 0 bytes in 0 blocks
==12501== Reachable blocks (those to which a pointer was found) are not shown.
==12501== To see them, rerun with: --leak-check=full --show-leak-kinds=all
==12501== 
==12501== For counts of detected and suppressed errors, rerun with: -v
==12501== ERROR SUMMARY: 0 errors from 0 contexts (suppressed: 0 from 0)


Call 3:
 /usr/local/bin/naemonstats -h
Memory Leak main_cfg_file:
==12604== 33 bytes in 1 blocks are still reachable in loss record 3 of 7
==12604==    at 0x4C2FB0F: malloc (in /usr/lib/valgrind/vgpreload_memcheck-amd64-linux.so)
==12604==    by 0x574C9B9: strdup (strdup.c:42)
==12604==    by 0x108FE0: main (naemonstats.c:207)


Call 4:
 /usr/local/bin/naemonstats -s /var/lib/naemon/status.dat -d PROGRUNTIME
Memory Leak:
==17882== 12 bytes in 1 blocks are still reachable in loss record 2 of 7
==17882==    at 0x4C2FB0F: malloc (in /usr/lib/valgrind/vgpreload_memcheck-amd64-linux.so)
==17882==    by 0x574C9B9: strdup (strdup.c:42)
==17882==    by 0x1090EC: main (naemonstats.c:241)


Fixed!

i do not post the valgrind output of every fix as this won't be necessary and will only expand the message. furthermore, i adjusted some pointer and char array initialization.
let me know if i can provide you with any further information.
thanks!","Please do not get me wrong, it's not an evil intention from my side. As you said, i just want to satisfy valgrind and had the time to fix the leaks. Thats all.
Thank you :)",True,{'THUMBS_UP': ['https://github.com/sni']}
naemon/naemon-core,https://github.com/naemon/naemon-core,332,2020-05-17T21:20:35Z,2020-05-18T09:57:34Z,2020-05-18T09:57:36Z,CLOSED,False,42,13,1,https://github.com/chifac08,fixed various memory leaks in naemonstats,3,[],https://github.com/naemon/naemon-core/pull/332,https://github.com/chifac08,4,https://github.com/naemon/naemon-core/pull/332#issuecomment-630075715,"Hi guys,
i fell over some memory leaks in ""neamonstats"" and fixed it. Please be so kind and review my request.
i tried nearly all variations of parameters from ""naemonstats"" with valgrind. This is the result:
Call 1:
/usr/local/bin/naemonstats -s /var/lib/naemon/status.dat
/usr/local/bin/naemonstats -c /etc/naemon/naemon.cfg

Memory Leak main_config_file:
==7514== 27 bytes in 1 blocks are still reachable in loss record 3 of 9
==7514==    at 0x4C2FB0F: malloc (in /usr/lib/valgrind/vgpreload_memcheck-amd64-linux.so)
==7514==    by 0x53859B9: strdup (strdup.c:42)
==7514==    by 0x108FD1: main (naemonstats.c:235)

Memory Leaks status_file:
==7514== 33 bytes in 1 blocks are still reachable in loss record 5 of 9
==7514==    at 0x4C2FB0F: malloc (in /usr/lib/valgrind/vgpreload_memcheck-amd64-linux.so)
==7514==    by 0x53859B9: strdup (strdup.c:42)
==7514==    by 0x108EFA: main (naemonstats.c:207)

==7514== 6 bytes in 1 blocks are still reachable in loss record 2 of 9
==7514==    at 0x4C2FB0F: malloc (in /usr/lib/valgrind/vgpreload_memcheck-amd64-linux.so)
==7514==    by 0x53859B9: strdup (strdup.c:42)
==7514==    by 0x10BF9C: read_status_file (naemonstats.c:1161)
==7514==    by 0x10901F: main (naemonstats.c:378)

Memory Leaks status_version:
==13487== 6 bytes in 1 blocks are still reachable in loss record 2 of 7
==13487==    at 0x4C2FB0F: malloc (in /usr/lib/valgrind/vgpreload_memcheck-amd64-linux.so)
==13487==    by 0x574C9B9: strdup (strdup.c:42)
==13487==    by 0x10D100: read_status_file (naemonstats.c:1174)
==13487==    by 0x109667: main (naemonstats.c:380)

Call 1 Re-Check:
==29509== Memcheck, a memory error detector
==29509== Copyright (C) 2002-2017, and GNU GPL'd, by Julian Seward et al.
==29509== Using Valgrind-3.13.0 and LibVEX; rerun with -h for copyright info
==29509== Command: /usr/local/bin/naemonstats -s /var/lib/naemon/status.dat
==29509== Parent PID: 6961
==29509== 
==29509== 
==29509== HEAP SUMMARY:
==29509==     in use at exit: 18,604 bytes in 6 blocks
==29509==   total heap usage: 12 allocs, 6 frees, 24,342 bytes allocated
==29509== 
==29509== LEAK SUMMARY:
==29509==    definitely lost: 0 bytes in 0 blocks
==29509==    indirectly lost: 0 bytes in 0 blocks
==29509==      possibly lost: 0 bytes in 0 blocks
==29509==    still reachable: 18,604 bytes in 6 blocks
==29509==         suppressed: 0 bytes in 0 blocks
==29509== Rerun with --leak-check=full to see details of leaked memory
==29509== 
==29509== For counts of detected and suppressed errors, rerun with: -v
==29509== ERROR SUMMARY: 0 errors from 0 contexts (suppressed: 0 from 0)



Call 2:
/usr/local/bin/naemonstats
Memory Leak main_cfg_dir:
Method ""pcomp_construct"" allocates memory and returns a pointer. So the caller has to free the memory on purpose.
==9990== 35 bytes in 1 blocks are definitely lost in loss record 3 of 7
==9990==    at 0x4C2FB0F: malloc (in /usr/lib/valgrind/vgpreload_memcheck-amd64-linux.so)
==9990==    by 0x4ECDCE8: pcomp_construct (nspath.c:48)
==9990==    by 0x4ECE0BC: nspath_normalize (nspath.c:130)
==9990==    by 0x4ECE142: nspath_absolute (nspath.c:146)
==9990==    by 0x10BECC: read_config_file (naemonstats.c:837)
==9990==    by 0x10960F: main (naemonstats.c:370)

Call 2 fixed:

==12501== Memcheck, a memory error detector
==12501== Copyright (C) 2002-2017, and GNU GPL'd, by Julian Seward et al.
==12501== Using Valgrind-3.13.0 and LibVEX; rerun with -h for copyright info
==12501== Command: /usr/local/bin/naemonstats
==12501== 


Naemon Stats 1.2.0.g9308955f.20200511.source
Copyright (c) 2013-present Naemon Development Team (www.naemon.org)
Copyright (c) 2003-2008 Ethan Galstad (www.nagios.org)
License: GPL

Error reading status file '/usr/local/var/status.dat': No such file or directory
==12501== 
==12501== HEAP SUMMARY:
==12501==     in use at exit: 18,604 bytes in 6 blocks
==12501==   total heap usage: 17 allocs, 11 frees, 25,159 bytes allocated
==12501== 
==12501== LEAK SUMMARY:
==12501==    definitely lost: 0 bytes in 0 blocks
==12501==    indirectly lost: 0 bytes in 0 blocks
==12501==      possibly lost: 0 bytes in 0 blocks
==12501==    still reachable: 18,604 bytes in 6 blocks
==12501==         suppressed: 0 bytes in 0 blocks
==12501== Reachable blocks (those to which a pointer was found) are not shown.
==12501== To see them, rerun with: --leak-check=full --show-leak-kinds=all
==12501== 
==12501== For counts of detected and suppressed errors, rerun with: -v
==12501== ERROR SUMMARY: 0 errors from 0 contexts (suppressed: 0 from 0)


Call 3:
 /usr/local/bin/naemonstats -h
Memory Leak main_cfg_file:
==12604== 33 bytes in 1 blocks are still reachable in loss record 3 of 7
==12604==    at 0x4C2FB0F: malloc (in /usr/lib/valgrind/vgpreload_memcheck-amd64-linux.so)
==12604==    by 0x574C9B9: strdup (strdup.c:42)
==12604==    by 0x108FE0: main (naemonstats.c:207)


Call 4:
 /usr/local/bin/naemonstats -s /var/lib/naemon/status.dat -d PROGRUNTIME
Memory Leak:
==17882== 12 bytes in 1 blocks are still reachable in loss record 2 of 7
==17882==    at 0x4C2FB0F: malloc (in /usr/lib/valgrind/vgpreload_memcheck-amd64-linux.so)
==17882==    by 0x574C9B9: strdup (strdup.c:42)
==17882==    by 0x1090EC: main (naemonstats.c:241)


Fixed!

i do not post the valgrind output of every fix as this won't be necessary and will only expand the message. furthermore, i adjusted some pointer and char array initialization.
let me know if i can provide you with any further information.
thanks!",thanks for your suggestions. I implemented it already :),True,{}
naemon/naemon-core,https://github.com/naemon/naemon-core,332,2020-05-17T21:20:35Z,2020-05-18T09:57:34Z,2020-05-18T09:57:36Z,CLOSED,False,42,13,1,https://github.com/chifac08,fixed various memory leaks in naemonstats,3,[],https://github.com/naemon/naemon-core/pull/332,https://github.com/sni,5,https://github.com/naemon/naemon-core/pull/332#issuecomment-630077826,"Hi guys,
i fell over some memory leaks in ""neamonstats"" and fixed it. Please be so kind and review my request.
i tried nearly all variations of parameters from ""naemonstats"" with valgrind. This is the result:
Call 1:
/usr/local/bin/naemonstats -s /var/lib/naemon/status.dat
/usr/local/bin/naemonstats -c /etc/naemon/naemon.cfg

Memory Leak main_config_file:
==7514== 27 bytes in 1 blocks are still reachable in loss record 3 of 9
==7514==    at 0x4C2FB0F: malloc (in /usr/lib/valgrind/vgpreload_memcheck-amd64-linux.so)
==7514==    by 0x53859B9: strdup (strdup.c:42)
==7514==    by 0x108FD1: main (naemonstats.c:235)

Memory Leaks status_file:
==7514== 33 bytes in 1 blocks are still reachable in loss record 5 of 9
==7514==    at 0x4C2FB0F: malloc (in /usr/lib/valgrind/vgpreload_memcheck-amd64-linux.so)
==7514==    by 0x53859B9: strdup (strdup.c:42)
==7514==    by 0x108EFA: main (naemonstats.c:207)

==7514== 6 bytes in 1 blocks are still reachable in loss record 2 of 9
==7514==    at 0x4C2FB0F: malloc (in /usr/lib/valgrind/vgpreload_memcheck-amd64-linux.so)
==7514==    by 0x53859B9: strdup (strdup.c:42)
==7514==    by 0x10BF9C: read_status_file (naemonstats.c:1161)
==7514==    by 0x10901F: main (naemonstats.c:378)

Memory Leaks status_version:
==13487== 6 bytes in 1 blocks are still reachable in loss record 2 of 7
==13487==    at 0x4C2FB0F: malloc (in /usr/lib/valgrind/vgpreload_memcheck-amd64-linux.so)
==13487==    by 0x574C9B9: strdup (strdup.c:42)
==13487==    by 0x10D100: read_status_file (naemonstats.c:1174)
==13487==    by 0x109667: main (naemonstats.c:380)

Call 1 Re-Check:
==29509== Memcheck, a memory error detector
==29509== Copyright (C) 2002-2017, and GNU GPL'd, by Julian Seward et al.
==29509== Using Valgrind-3.13.0 and LibVEX; rerun with -h for copyright info
==29509== Command: /usr/local/bin/naemonstats -s /var/lib/naemon/status.dat
==29509== Parent PID: 6961
==29509== 
==29509== 
==29509== HEAP SUMMARY:
==29509==     in use at exit: 18,604 bytes in 6 blocks
==29509==   total heap usage: 12 allocs, 6 frees, 24,342 bytes allocated
==29509== 
==29509== LEAK SUMMARY:
==29509==    definitely lost: 0 bytes in 0 blocks
==29509==    indirectly lost: 0 bytes in 0 blocks
==29509==      possibly lost: 0 bytes in 0 blocks
==29509==    still reachable: 18,604 bytes in 6 blocks
==29509==         suppressed: 0 bytes in 0 blocks
==29509== Rerun with --leak-check=full to see details of leaked memory
==29509== 
==29509== For counts of detected and suppressed errors, rerun with: -v
==29509== ERROR SUMMARY: 0 errors from 0 contexts (suppressed: 0 from 0)



Call 2:
/usr/local/bin/naemonstats
Memory Leak main_cfg_dir:
Method ""pcomp_construct"" allocates memory and returns a pointer. So the caller has to free the memory on purpose.
==9990== 35 bytes in 1 blocks are definitely lost in loss record 3 of 7
==9990==    at 0x4C2FB0F: malloc (in /usr/lib/valgrind/vgpreload_memcheck-amd64-linux.so)
==9990==    by 0x4ECDCE8: pcomp_construct (nspath.c:48)
==9990==    by 0x4ECE0BC: nspath_normalize (nspath.c:130)
==9990==    by 0x4ECE142: nspath_absolute (nspath.c:146)
==9990==    by 0x10BECC: read_config_file (naemonstats.c:837)
==9990==    by 0x10960F: main (naemonstats.c:370)

Call 2 fixed:

==12501== Memcheck, a memory error detector
==12501== Copyright (C) 2002-2017, and GNU GPL'd, by Julian Seward et al.
==12501== Using Valgrind-3.13.0 and LibVEX; rerun with -h for copyright info
==12501== Command: /usr/local/bin/naemonstats
==12501== 


Naemon Stats 1.2.0.g9308955f.20200511.source
Copyright (c) 2013-present Naemon Development Team (www.naemon.org)
Copyright (c) 2003-2008 Ethan Galstad (www.nagios.org)
License: GPL

Error reading status file '/usr/local/var/status.dat': No such file or directory
==12501== 
==12501== HEAP SUMMARY:
==12501==     in use at exit: 18,604 bytes in 6 blocks
==12501==   total heap usage: 17 allocs, 11 frees, 25,159 bytes allocated
==12501== 
==12501== LEAK SUMMARY:
==12501==    definitely lost: 0 bytes in 0 blocks
==12501==    indirectly lost: 0 bytes in 0 blocks
==12501==      possibly lost: 0 bytes in 0 blocks
==12501==    still reachable: 18,604 bytes in 6 blocks
==12501==         suppressed: 0 bytes in 0 blocks
==12501== Reachable blocks (those to which a pointer was found) are not shown.
==12501== To see them, rerun with: --leak-check=full --show-leak-kinds=all
==12501== 
==12501== For counts of detected and suppressed errors, rerun with: -v
==12501== ERROR SUMMARY: 0 errors from 0 contexts (suppressed: 0 from 0)


Call 3:
 /usr/local/bin/naemonstats -h
Memory Leak main_cfg_file:
==12604== 33 bytes in 1 blocks are still reachable in loss record 3 of 7
==12604==    at 0x4C2FB0F: malloc (in /usr/lib/valgrind/vgpreload_memcheck-amd64-linux.so)
==12604==    by 0x574C9B9: strdup (strdup.c:42)
==12604==    by 0x108FE0: main (naemonstats.c:207)


Call 4:
 /usr/local/bin/naemonstats -s /var/lib/naemon/status.dat -d PROGRUNTIME
Memory Leak:
==17882== 12 bytes in 1 blocks are still reachable in loss record 2 of 7
==17882==    at 0x4C2FB0F: malloc (in /usr/lib/valgrind/vgpreload_memcheck-amd64-linux.so)
==17882==    by 0x574C9B9: strdup (strdup.c:42)
==17882==    by 0x1090EC: main (naemonstats.c:241)


Fixed!

i do not post the valgrind output of every fix as this won't be necessary and will only expand the message. furthermore, i adjusted some pointer and char array initialization.
let me know if i can provide you with any further information.
thanks!","merged and rebase with d7ac1c8.
Thanks",True,{}
naemon/naemon-core,https://github.com/naemon/naemon-core,333,2020-07-13T10:37:00Z,2020-07-13T11:33:51Z,2020-07-13T11:33:51Z,MERGED,True,3,7,3,https://github.com/sni,update debian compat level,1,[],https://github.com/naemon/naemon-core/pull/333,https://github.com/sni,1,https://github.com/naemon/naemon-core/pull/333,Compatibility levels before 10 are deprecated with current debian testing.,Compatibility levels before 10 are deprecated with current debian testing.,True,{}
naemon/naemon-core,https://github.com/naemon/naemon-core,333,2020-07-13T10:37:00Z,2020-07-13T11:33:51Z,2020-07-13T11:33:51Z,MERGED,True,3,7,3,https://github.com/sni,update debian compat level,1,[],https://github.com/naemon/naemon-core/pull/333,https://github.com/sni,2,https://github.com/naemon/naemon-core/pull/333#issuecomment-657497979,Compatibility levels before 10 are deprecated with current debian testing.,"in case you are wondering, i removed the if clause from the version.sh, since debian runs a autoreconfigure now and would tag all builds as source build anyway.",True,{}
naemon/naemon-core,https://github.com/naemon/naemon-core,334,2020-07-13T13:03:02Z,2020-07-13T13:20:29Z,2020-07-13T13:20:29Z,MERGED,True,44,2,5,https://github.com/sni,Release 1 2 1,2,[],https://github.com/naemon/naemon-core/pull/334,https://github.com/sni,1,https://github.com/naemon/naemon-core/pull/334,"Changed:

nothing

Bugfixes:

fix executing commands with pipes (#319)
fix external command to remove downtimes by filter","Changed:

nothing

Bugfixes:

fix executing commands with pipes (#319)
fix external command to remove downtimes by filter",True,{}
naemon/naemon-core,https://github.com/naemon/naemon-core,335,2020-07-13T13:31:37Z,2020-07-13T13:39:24Z,2020-07-13T13:39:24Z,MERGED,True,2,2,2,https://github.com/sni,fix typo in release files,1,[],https://github.com/naemon/naemon-core/pull/335,https://github.com/sni,1,https://github.com/naemon/naemon-core/pull/335,fixing copy&paste error. This is the core pkg.,fixing copy&paste error. This is the core pkg.,True,{}
naemon/naemon-core,https://github.com/naemon/naemon-core,336,2020-07-25T06:37:36Z,2020-07-27T10:26:29Z,2020-07-27T10:26:29Z,MERGED,True,8,8,1,https://github.com/ckujau,Move include_dir= to the very end of the (sample) configuration file.,1,[],https://github.com/naemon/naemon-core/pull/336,https://github.com/ckujau,1,https://github.com/naemon/naemon-core/pull/336,"This directive needs to be last, otherwise custom configuration parameters
in that directory may get reset again by whatever comes after the
include_dir= directive in the naemon.cfg file.
Example: maybe we set use_regexp_matching=1 in module-conf.d/local.cfg.
And indeed, include_dir=module-conf.d will parse and honor this setting,
but then use_regexp_matching=0 is set in naemon.cfg again and our
local settings from module-conf.d are gone :(","This directive needs to be last, otherwise custom configuration parameters
in that directory may get reset again by whatever comes after the
include_dir= directive in the naemon.cfg file.
Example: maybe we set use_regexp_matching=1 in module-conf.d/local.cfg.
And indeed, include_dir=module-conf.d will parse and honor this setting,
but then use_regexp_matching=0 is set in naemon.cfg again and our
local settings from module-conf.d are gone :(",True,{}
naemon/naemon-core,https://github.com/naemon/naemon-core,337,2020-08-11T11:51:41Z,2020-08-12T12:34:44Z,2020-08-12T12:35:51Z,MERGED,True,6,1,1,https://github.com/jacobbaungard,Worker: Null pointer sanity check before wlog call,1,[],https://github.com/naemon/naemon-core/pull/337,https://github.com/jacobbaungard,1,https://github.com/naemon/naemon-core/pull/337,"This commit implements a couple of sanity checks on pointers in the
worker process. This is the result of analyzing core-dumps from a crash.","This commit implements a couple of sanity checks on pointers in the
worker process. This is the result of analyzing core-dumps from a crash.",True,{}
naemon/naemon-core,https://github.com/naemon/naemon-core,337,2020-08-11T11:51:41Z,2020-08-12T12:34:44Z,2020-08-12T12:35:51Z,MERGED,True,6,1,1,https://github.com/jacobbaungard,Worker: Null pointer sanity check before wlog call,1,[],https://github.com/naemon/naemon-core/pull/337,https://github.com/sni,2,https://github.com/naemon/naemon-core/pull/337#issuecomment-672776804,"This commit implements a couple of sanity checks on pointers in the
worker process. This is the result of analyzing core-dumps from a crash.","Would it makes sense to still print/log the error, even if cp is not set, but without those details?",True,{}
naemon/naemon-core,https://github.com/naemon/naemon-core,337,2020-08-11T11:51:41Z,2020-08-12T12:34:44Z,2020-08-12T12:35:51Z,MERGED,True,6,1,1,https://github.com/jacobbaungard,Worker: Null pointer sanity check before wlog call,1,[],https://github.com/naemon/naemon-core/pull/337,https://github.com/jacobbaungard,3,https://github.com/naemon/naemon-core/pull/337#issuecomment-672817348,"This commit implements a couple of sanity checks on pointers in the
worker process. This is the result of analyzing core-dumps from a crash.",Yes makes sense. Amended.,True,{}
naemon/naemon-core,https://github.com/naemon/naemon-core,337,2020-08-11T11:51:41Z,2020-08-12T12:34:44Z,2020-08-12T12:35:51Z,MERGED,True,6,1,1,https://github.com/jacobbaungard,Worker: Null pointer sanity check before wlog call,1,[],https://github.com/naemon/naemon-core/pull/337,https://github.com/sni,4,https://github.com/naemon/naemon-core/pull/337#issuecomment-672844021,"This commit implements a couple of sanity checks on pointers in the
worker process. This is the result of analyzing core-dumps from a crash.","thanks, merged",True,{}
naemon/naemon-core,https://github.com/naemon/naemon-core,338,2020-08-14T14:52:40Z,2020-08-17T06:58:19Z,2020-08-17T06:58:19Z,MERGED,True,2,2,1,https://github.com/sni,correct acknowledgement external command help,1,[],https://github.com/naemon/naemon-core/pull/338,https://github.com/sni,1,https://github.com/naemon/naemon-core/pull/338,"the current documentation is plain wrong here. The related comment for acknowledgements always survive restarts of the core.
The peristent flag just sets if the comment survices removal of the acknowledgement. The flag is checked in
delete_host_acknowledgement_comments() and delete_service_acknowledgement_comments()","the current documentation is plain wrong here. The related comment for acknowledgements always survive restarts of the core.
The peristent flag just sets if the comment survices removal of the acknowledgement. The flag is checked in
delete_host_acknowledgement_comments() and delete_service_acknowledgement_comments()",True,{}
naemon/naemon-core,https://github.com/naemon/naemon-core,338,2020-08-14T14:52:40Z,2020-08-17T06:58:19Z,2020-08-17T06:58:19Z,MERGED,True,2,2,1,https://github.com/sni,correct acknowledgement external command help,1,[],https://github.com/naemon/naemon-core/pull/338,https://github.com/sni,2,https://github.com/naemon/naemon-core/pull/338#issuecomment-674113344,"the current documentation is plain wrong here. The related comment for acknowledgements always survive restarts of the core.
The peristent flag just sets if the comment survices removal of the acknowledgement. The flag is checked in
delete_host_acknowledgement_comments() and delete_service_acknowledgement_comments()","Once this is merged, we need to run the update script from the naemon.org page as well.",True,{}
naemon/naemon-core,https://github.com/naemon/naemon-core,339,2020-08-18T13:51:42Z,2020-08-19T07:38:21Z,2020-08-19T07:38:21Z,MERGED,True,32,3,4,https://github.com/sni,debian: install systemd service,2,[],https://github.com/naemon/naemon-core/pull/339,https://github.com/sni,1,https://github.com/naemon/naemon-core/pull/339,"debian uses systemd and should therefor use systemd service as well.
Signed-off-by: Sven Nierlein Sven.Nierlein@consol.de","debian uses systemd and should therefor use systemd service as well.
Signed-off-by: Sven Nierlein Sven.Nierlein@consol.de",True,{}
naemon/naemon-core,https://github.com/naemon/naemon-core,340,2020-08-21T10:42:07Z,2020-08-21T14:40:04Z,2020-08-21T14:40:04Z,MERGED,True,1,0,1,https://github.com/sni,set debian source format,1,[],https://github.com/naemon/naemon-core/pull/340,https://github.com/sni,1,https://github.com/naemon/naemon-core/pull/340,obs builds require a debian release otherwise.,obs builds require a debian release otherwise.,True,{}
naemon/naemon-core,https://github.com/naemon/naemon-core,341,2020-10-08T13:39:50Z,2020-10-13T11:44:11Z,2020-10-14T05:59:46Z,MERGED,True,2459,2411,67,https://github.com/sjoegren,Fix indentation,2,[],https://github.com/naemon/naemon-core/pull/341,https://github.com/sjoegren,1,https://github.com/naemon/naemon-core/pull/341,"This PR fixes paths to sources in the indent-all.sh script and reindent the entire tree.
Since paths wasn't correct in indent-all.sh, indentation has been lacking behind the rules since sourced were moved around in 7344116. Therefore this is a large change, re-indenting everything according to the project rules.","This PR fixes paths to sources in the indent-all.sh script and reindent the entire tree.
Since paths wasn't correct in indent-all.sh, indentation has been lacking behind the rules since sourced were moved around in 7344116. Therefore this is a large change, re-indenting everything according to the project rules.",True,"{'THUMBS_UP': ['https://github.com/jacobbaungard', 'https://github.com/sni']}"
naemon/naemon-core,https://github.com/naemon/naemon-core,341,2020-10-08T13:39:50Z,2020-10-13T11:44:11Z,2020-10-14T05:59:46Z,MERGED,True,2459,2411,67,https://github.com/sjoegren,Fix indentation,2,[],https://github.com/naemon/naemon-core/pull/341,https://github.com/sni,2,https://github.com/naemon/naemon-core/pull/341#issuecomment-705580654,"This PR fixes paths to sources in the indent-all.sh script and reindent the entire tree.
Since paths wasn't correct in indent-all.sh, indentation has been lacking behind the rules since sourced were moved around in 7344116. Therefore this is a large change, re-indenting everything according to the project rules.",Looks good. Can we add a test case on travis to check if new commits are formated correctly?,True,{}
naemon/naemon-core,https://github.com/naemon/naemon-core,341,2020-10-08T13:39:50Z,2020-10-13T11:44:11Z,2020-10-14T05:59:46Z,MERGED,True,2459,2411,67,https://github.com/sjoegren,Fix indentation,2,[],https://github.com/naemon/naemon-core/pull/341,https://github.com/sjoegren,3,https://github.com/naemon/naemon-core/pull/341#issuecomment-706187987,"This PR fixes paths to sources in the indent-all.sh script and reindent the entire tree.
Since paths wasn't correct in indent-all.sh, indentation has been lacking behind the rules since sourced were moved around in 7344116. Therefore this is a large change, re-indenting everything according to the project rules.","That sounds like a good idea.
I made changed to indent-all.sh to support that. Unfortunately the astyle version available on CI (ubuntu xenial) is old (2.x) and complains on a few things that astyle 3.1 has formatted nicely on my machine. It looks like bugs in astyle, cause it doesn't make sense.
I tried to change travis to ubuntu bionic instead, which has the latest astyle (3.1). But then a timeperiod testcase failed :(
It it this test: 
  
    
      naemon-core/t-tap/test_timeperiods.c
    
    
         Line 380
      in
      70944db
    
  
  
    

        
          
           ok(chosen_valid_time == test_time, ""There should be no next valid time, was %s"", ctime(&chosen_valid_time)); 
        
    
  


From test-suite.log:
ok 93 - Testing exclude always                                                                                                                                                                                                                                 
ok 94 - 12 Jul 2010 15:00:00 should not be valid                                                                                                                                                                                                               
not ok 95 - There should be no next valid time, was Mon Nov  1 00:00:00 2038                                                                                                                                                                                   
                                                                                                                                                                                                                                                               
#     Failed test (t-tap/test_timeperiods.c:main() at line 380)                                                                                                                                                                                                
ok 96 - Testing Sunday 00:00-01:15,03:15-22:00

I tried to understand why it happens, but couldn't. It's a newer gcc on bionic which might have something to do with it? I tried with gcc 7.3 on CentOS 7, but there it works.",True,{}
naemon/naemon-core,https://github.com/naemon/naemon-core,341,2020-10-08T13:39:50Z,2020-10-13T11:44:11Z,2020-10-14T05:59:46Z,MERGED,True,2459,2411,67,https://github.com/sjoegren,Fix indentation,2,[],https://github.com/naemon/naemon-core/pull/341,https://github.com/sni,4,https://github.com/naemon/naemon-core/pull/341#issuecomment-706194248,"This PR fixes paths to sources in the indent-all.sh script and reindent the entire tree.
Since paths wasn't correct in indent-all.sh, indentation has been lacking behind the rules since sourced were moved around in 7344116. Therefore this is a large change, re-indenting everything according to the project rules.",Let me have a look on a bionic test machine. Maybe its some 32bit thing.,True,{'THUMBS_UP': ['https://github.com/sjoegren']}
naemon/naemon-core,https://github.com/naemon/naemon-core,341,2020-10-08T13:39:50Z,2020-10-13T11:44:11Z,2020-10-14T05:59:46Z,MERGED,True,2459,2411,67,https://github.com/sjoegren,Fix indentation,2,[],https://github.com/naemon/naemon-core/pull/341,https://github.com/sjoegren,5,https://github.com/naemon/naemon-core/pull/341#issuecomment-707631209,"This PR fixes paths to sources in the indent-all.sh script and reindent the entire tree.
Since paths wasn't correct in indent-all.sh, indentation has been lacking behind the rules since sourced were moved around in 7344116. Therefore this is a large change, re-indenting everything according to the project rules.","Removed running the indent check script in CI for now.
Rebased branch.

All changes in the big commit are made by astyle when running ./indent-all.sh.",True,{}
naemon/naemon-core,https://github.com/naemon/naemon-core,342,2020-10-13T05:52:07Z,2020-10-13T09:08:06Z,2020-10-13T09:10:58Z,MERGED,True,76,3,2,https://github.com/sjoegren,Fix scheduling flexible service downtime,1,[],https://github.com/naemon/naemon-core/pull/342,https://github.com/sjoegren,1,https://github.com/naemon/naemon-core/pull/342,"Fixes the issue that a flexible (fixed=0) service downtime doesn't honor
the ""duration"" argument, but sets the duration to (end_time -
start_time) just like a fixed downtime.
Add test cases for fixed/flexible host/service downtimes.
Signed-off-by: Aksel Sjögren asjogren@itrsgroup.com","Fixes the issue that a flexible (fixed=0) service downtime doesn't honor
the ""duration"" argument, but sets the duration to (end_time -
start_time) just like a fixed downtime.
Add test cases for fixed/flexible host/service downtimes.
Signed-off-by: Aksel Sjögren asjogren@itrsgroup.com",True,{}
naemon/naemon-core,https://github.com/naemon/naemon-core,342,2020-10-13T05:52:07Z,2020-10-13T09:08:06Z,2020-10-13T09:10:58Z,MERGED,True,76,3,2,https://github.com/sjoegren,Fix scheduling flexible service downtime,1,[],https://github.com/naemon/naemon-core/pull/342,https://github.com/sjoegren,2,https://github.com/naemon/naemon-core/pull/342#issuecomment-707511905,"Fixes the issue that a flexible (fixed=0) service downtime doesn't honor
the ""duration"" argument, but sets the duration to (end_time -
start_time) just like a fixed downtime.
Add test cases for fixed/flexible host/service downtimes.
Signed-off-by: Aksel Sjögren asjogren@itrsgroup.com","Hm, why isn't Travis running it's stuff for the PR?",True,{}
naemon/naemon-core,https://github.com/naemon/naemon-core,342,2020-10-13T05:52:07Z,2020-10-13T09:08:06Z,2020-10-13T09:10:58Z,MERGED,True,76,3,2,https://github.com/sjoegren,Fix scheduling flexible service downtime,1,[],https://github.com/naemon/naemon-core/pull/342,https://github.com/jacobbaungard,3,https://github.com/naemon/naemon-core/pull/342#issuecomment-707536768,"Fixes the issue that a flexible (fixed=0) service downtime doesn't honor
the ""duration"" argument, but sets the duration to (end_time -
start_time) just like a fixed downtime.
Add test cases for fixed/flexible host/service downtimes.
Signed-off-by: Aksel Sjögren asjogren@itrsgroup.com","Looks like travis built, but didn't report to github? https://travis-ci.org/github/naemon/naemon-core/builds/735266156",True,{}
naemon/naemon-core,https://github.com/naemon/naemon-core,342,2020-10-13T05:52:07Z,2020-10-13T09:08:06Z,2020-10-13T09:10:58Z,MERGED,True,76,3,2,https://github.com/sjoegren,Fix scheduling flexible service downtime,1,[],https://github.com/naemon/naemon-core/pull/342,https://github.com/sjoegren,4,https://github.com/naemon/naemon-core/pull/342#issuecomment-707576208,"Fixes the issue that a flexible (fixed=0) service downtime doesn't honor
the ""duration"" argument, but sets the duration to (end_time -
start_time) just like a fixed downtime.
Add test cases for fixed/flexible host/service downtimes.
Signed-off-by: Aksel Sjögren asjogren@itrsgroup.com",Others have that issue as well apparently: https://travis-ci.community/t/travis-build-status-not-show-on-github/7990,True,{}
naemon/naemon-core,https://github.com/naemon/naemon-core,342,2020-10-13T05:52:07Z,2020-10-13T09:08:06Z,2020-10-13T09:10:58Z,MERGED,True,76,3,2,https://github.com/sjoegren,Fix scheduling flexible service downtime,1,[],https://github.com/naemon/naemon-core/pull/342,https://github.com/sni,5,https://github.com/naemon/naemon-core/pull/342#issuecomment-707596447,"Fixes the issue that a flexible (fixed=0) service downtime doesn't honor
the ""duration"" argument, but sets the duration to (end_time -
start_time) just like a fixed downtime.
Add test cases for fixed/flexible host/service downtimes.
Signed-off-by: Aksel Sjögren asjogren@itrsgroup.com","i switched to travis-ci.com,should be better next time.",True,{'THUMBS_UP': ['https://github.com/sjoegren']}
naemon/naemon-core,https://github.com/naemon/naemon-core,343,2020-10-13T09:51:28Z,2020-10-13T09:56:00Z,2020-10-13T09:56:00Z,MERGED,True,0,2,1,https://github.com/ozamosi,Remove my email from travis config,1,[],https://github.com/naemon/naemon-core/pull/343,https://github.com/ozamosi,1,https://github.com/naemon/naemon-core/pull/343,I have no particular need for these emails any longer.,I have no particular need for these emails any longer.,True,{}
naemon/naemon-core,https://github.com/naemon/naemon-core,343,2020-10-13T09:51:28Z,2020-10-13T09:56:00Z,2020-10-13T09:56:00Z,MERGED,True,0,2,1,https://github.com/ozamosi,Remove my email from travis config,1,[],https://github.com/naemon/naemon-core/pull/343,https://github.com/sni,2,https://github.com/naemon/naemon-core/pull/343#issuecomment-707629408,I have no particular need for these emails any longer.,are you sure? :-),True,{}
naemon/naemon-core,https://github.com/naemon/naemon-core,344,2020-10-13T10:40:22Z,2020-10-13T10:48:59Z,2020-10-13T10:49:09Z,MERGED,True,17,16,1,https://github.com/jacobbaungard,Travis: Add slack notifications,1,[],https://github.com/naemon/naemon-core/pull/344,https://github.com/jacobbaungard,1,https://github.com/naemon/naemon-core/pull/344,"With this commit slack notification is posted from travis to #naemon-travis at the Openmonitoring slack channel.
The travis CLI tools also reformatted the .travis file slightly - I assume with the ""correct"" formatting.","With this commit slack notification is posted from travis to #naemon-travis at the Openmonitoring slack channel.
The travis CLI tools also reformatted the .travis file slightly - I assume with the ""correct"" formatting.",True,{}
naemon/naemon-core,https://github.com/naemon/naemon-core,345,2020-11-11T09:50:25Z,2020-11-11T10:37:49Z,2020-11-11T10:39:49Z,MERGED,True,2,0,2,https://github.com/sni,fix next_check synchronisation,1,[],https://github.com/naemon/naemon-core/pull/345,https://github.com/sni,1,https://github.com/naemon/naemon-core/pull/345,"last_update wasn't updated in some cases when only the next_check advances. For example for
pending and disabled hosts/services.
This patch ensures, that whenever next_check is updated, also last_update is set.","last_update wasn't updated in some cases when only the next_check advances. For example for
pending and disabled hosts/services.
This patch ensures, that whenever next_check is updated, also last_update is set.",True,{}
naemon/naemon-core,https://github.com/naemon/naemon-core,345,2020-11-11T09:50:25Z,2020-11-11T10:37:49Z,2020-11-11T10:39:49Z,MERGED,True,2,0,2,https://github.com/sni,fix next_check synchronisation,1,[],https://github.com/naemon/naemon-core/pull/345,https://github.com/sni,2,https://github.com/naemon/naemon-core/pull/345#issuecomment-725340085,"last_update wasn't updated in some cases when only the next_check advances. For example for
pending and disabled hosts/services.
This patch ensures, that whenever next_check is updated, also last_update is set.","There is a short sentence about the attribute in livestatus:
Time of the last update of this host (Unix timestamp)
The last_update should basically be updated whenever any attribute of the host/service changes. We set it for external commands already, we just missed some cornercases.",True,{'THUMBS_UP': ['https://github.com/jacobbaungard']}
naemon/naemon-core,https://github.com/naemon/naemon-core,345,2020-11-11T09:50:25Z,2020-11-11T10:37:49Z,2020-11-11T10:39:49Z,MERGED,True,2,0,2,https://github.com/sni,fix next_check synchronisation,1,[],https://github.com/naemon/naemon-core/pull/345,https://github.com/sni,3,https://github.com/naemon/naemon-core/pull/345#issuecomment-725345537,"last_update wasn't updated in some cases when only the next_check advances. For example for
pending and disabled hosts/services.
This patch ensures, that whenever next_check is updated, also last_update is set.",Here is a PR to update the tables descriptions to match the ones on naemon.org naemon/naemon-livestatus#82,True,{}
naemon/naemon-core,https://github.com/naemon/naemon-core,346,2020-11-13T10:38:08Z,2020-11-13T16:35:25Z,2020-11-13T16:35:25Z,MERGED,True,25,2,4,https://github.com/sni,release 1.2.2,1,[],https://github.com/naemon/naemon-core/pull/346,https://github.com/sni,1,https://github.com/naemon/naemon-core/pull/346,"1.2.2 - Nov 13 2020
Features:

none

Changed:

improved sample configuration
improved external command help
debian package source format changed
debian packages changed to systemd

Bugfixes:

fix scheduling flexible service downtime
fix setting last_update whenever next_check changes
fix worker crashing

Signed-off-by: Sven Nierlein sven@nierlein.de","1.2.2 - Nov 13 2020
Features:

none

Changed:

improved sample configuration
improved external command help
debian package source format changed
debian packages changed to systemd

Bugfixes:

fix scheduling flexible service downtime
fix setting last_update whenever next_check changes
fix worker crashing

Signed-off-by: Sven Nierlein sven@nierlein.de",True,{}
naemon/naemon-core,https://github.com/naemon/naemon-core,347,2020-11-27T10:58:07Z,2020-11-30T08:01:19Z,2020-11-30T08:01:22Z,MERGED,True,8,12,4,https://github.com/sni,debian: move to systemd,1,[],https://github.com/naemon/naemon-core/pull/347,https://github.com/sni,1,https://github.com/naemon/naemon-core/pull/347,"The oldest supported system is ubuntu14.04 on obs right now and even that supports systemd already. So
simply move to systemd completely. Detection of system failed on obs on some systems and the packages
end up with half complete systemd integration.
dh_installinit install service anyway, even if we do not add the other systemd files.
[  144s] dh_installinit --name=naemon
[  144s] 	install -d debian/naemon-core/lib/systemd/system
[  144s] 	install -p -m0644 debian/naemon-core.naemon.service debian/naemon-core/lib/systemd/system/naemon.service","The oldest supported system is ubuntu14.04 on obs right now and even that supports systemd already. So
simply move to systemd completely. Detection of system failed on obs on some systems and the packages
end up with half complete systemd integration.
dh_installinit install service anyway, even if we do not add the other systemd files.
[  144s] dh_installinit --name=naemon
[  144s] 	install -d debian/naemon-core/lib/systemd/system
[  144s] 	install -p -m0644 debian/naemon-core.naemon.service debian/naemon-core/lib/systemd/system/naemon.service",True,{}
naemon/naemon-core,https://github.com/naemon/naemon-core,348,2020-12-07T11:23:36Z,2020-12-07T11:55:05Z,2020-12-07T11:55:05Z,MERGED,True,14,2,4,https://github.com/sni,release 1.2.3,1,[],https://github.com/naemon/naemon-core/pull/348,https://github.com/sni,1,https://github.com/naemon/naemon-core/pull/348,,,True,{}
naemon/naemon-core,https://github.com/naemon/naemon-core,350,2021-01-07T16:40:05Z,2021-01-07T16:46:50Z,2021-01-07T19:53:01Z,MERGED,True,2,2,2,https://github.com/atj,Fix header file location in deb/rpm packages,1,[],https://github.com/naemon/naemon-core/pull/350,https://github.com/atj,1,https://github.com/naemon/naemon-core/pull/350,"Automake automatically adds a package name suffix to includedir so it
should be specified as ""/usr/include"" not ""/usr/include/naemon"".
Fixes #299.","Automake automatically adds a package name suffix to includedir so it
should be specified as ""/usr/include"" not ""/usr/include/naemon"".
Fixes #299.",True,{}
naemon/naemon-core,https://github.com/naemon/naemon-core,350,2021-01-07T16:40:05Z,2021-01-07T16:46:50Z,2021-01-07T19:53:01Z,MERGED,True,2,2,2,https://github.com/atj,Fix header file location in deb/rpm packages,1,[],https://github.com/naemon/naemon-core/pull/350,https://github.com/sni,2,https://github.com/naemon/naemon-core/pull/350#issuecomment-756235159,"Automake automatically adds a package name suffix to includedir so it
should be specified as ""/usr/include"" not ""/usr/include/naemon"".
Fixes #299.","great, thanks.",True,{}
naemon/naemon-core,https://github.com/naemon/naemon-core,350,2021-01-07T16:40:05Z,2021-01-07T16:46:50Z,2021-01-07T19:53:01Z,MERGED,True,2,2,2,https://github.com/atj,Fix header file location in deb/rpm packages,1,[],https://github.com/naemon/naemon-core/pull/350,https://github.com/atj,3,https://github.com/naemon/naemon-core/pull/350#issuecomment-756260270,"Automake automatically adds a package name suffix to includedir so it
should be specified as ""/usr/include"" not ""/usr/include/naemon"".
Fixes #299.",Thanks. Could you trigger a rebuild of the packages on OBS?,True,{}
naemon/naemon-core,https://github.com/naemon/naemon-core,350,2021-01-07T16:40:05Z,2021-01-07T16:46:50Z,2021-01-07T19:53:01Z,MERGED,True,2,2,2,https://github.com/atj,Fix header file location in deb/rpm packages,1,[],https://github.com/naemon/naemon-core/pull/350,https://github.com/sni,4,https://github.com/naemon/naemon-core/pull/350#issuecomment-756345515,"Automake automatically adds a package name suffix to includedir so it
should be specified as ""/usr/include"" not ""/usr/include/naemon"".
Fixes #299.","https://build.opensuse.org/project/show/home:naemon:daily are triggered automatically, but it might take a few moments. Stable builds are triggered manually upon release.",True,{}
naemon/naemon-core,https://github.com/naemon/naemon-core,351,2021-01-08T08:43:03Z,2021-01-08T10:18:33Z,2021-01-08T10:18:34Z,MERGED,True,2,2,2,https://github.com/sni,dh-systemd has been merged into debhelper,1,[],https://github.com/naemon/naemon-core/pull/351,https://github.com/sni,1,https://github.com/naemon/naemon-core/pull/351,it is obsolete since stretch and removed in debian 11.,it is obsolete since stretch and removed in debian 11.,True,{}
naemon/naemon-core,https://github.com/naemon/naemon-core,352,2021-01-27T08:03:56Z,2021-01-27T09:42:39Z,2021-01-27T09:42:39Z,MERGED,True,31,32,4,https://github.com/sjoegren,Run behave feature tests with Python 3,1,[],https://github.com/naemon/naemon-core/pull/352,https://github.com/sjoegren,1,https://github.com/naemon/naemon-core/pull/352,"Elevate behave feature tests to Python 3 compatible code and install/run
Python 3 in Travis CI.
Signed-off-by: Aksel Sjögren asjogren@itrsgroup.com","Elevate behave feature tests to Python 3 compatible code and install/run
Python 3 in Travis CI.
Signed-off-by: Aksel Sjögren asjogren@itrsgroup.com",True,{}
naemon/naemon-core,https://github.com/naemon/naemon-core,352,2021-01-27T08:03:56Z,2021-01-27T09:42:39Z,2021-01-27T09:42:39Z,MERGED,True,31,32,4,https://github.com/sjoegren,Run behave feature tests with Python 3,1,[],https://github.com/naemon/naemon-core/pull/352,https://github.com/apps/lgtm-com,2,https://github.com/naemon/naemon-core/pull/352#issuecomment-768123845,"Elevate behave feature tests to Python 3 compatible code and install/run
Python 3 in Travis CI.
Signed-off-by: Aksel Sjögren asjogren@itrsgroup.com","This pull request fixes 2 alerts when merging 3392daa into c049dbd - view on LGTM.com
fixed alerts:

2 for Unnecessary pass",True,{}
naemon/naemon-core,https://github.com/naemon/naemon-core,352,2021-01-27T08:03:56Z,2021-01-27T09:42:39Z,2021-01-27T09:42:39Z,MERGED,True,31,32,4,https://github.com/sjoegren,Run behave feature tests with Python 3,1,[],https://github.com/naemon/naemon-core/pull/352,https://github.com/apps/lgtm-com,3,https://github.com/naemon/naemon-core/pull/352#issuecomment-768154884,"Elevate behave feature tests to Python 3 compatible code and install/run
Python 3 in Travis CI.
Signed-off-by: Aksel Sjögren asjogren@itrsgroup.com","This pull request fixes 2 alerts when merging 8178089 into c049dbd - view on LGTM.com
fixed alerts:

2 for Unnecessary pass",True,{}
naemon/naemon-core,https://github.com/naemon/naemon-core,353,2021-02-09T12:41:17Z,2021-02-09T13:17:35Z,2021-02-09T13:17:35Z,MERGED,True,43,8,6,https://github.com/sjoegren,Fix stopping Naemon in logging feature test,1,[],https://github.com/naemon/naemon-core/pull/353,https://github.com/sjoegren,1,https://github.com/naemon/naemon-core/pull/353,"We noticed that logging tests failed when building in a new environment, it seemed to be a timing issue where the test case wants to SIGTERM naemon, but some investigation gave that naemon didn't completely start yet, so its signal handlers weren't installed and thus didn't stop as intended. This is an attempt to improve the timing issue.


Fix test that failed if Naemon haven't yet started correctly and got
it's signal handlers setup at the stage a test case executes the ""When
I stop naemon"" step.
Add a step that starts Naemon and wait until it finds a line in the
log file that indicates that it should be ready.
It's not perfect, but it seems to make the test case more stable.


Close opened files in feature tests.


Signed-off-by: Aksel Sjögren asjogren@itrsgroup.com","We noticed that logging tests failed when building in a new environment, it seemed to be a timing issue where the test case wants to SIGTERM naemon, but some investigation gave that naemon didn't completely start yet, so its signal handlers weren't installed and thus didn't stop as intended. This is an attempt to improve the timing issue.


Fix test that failed if Naemon haven't yet started correctly and got
it's signal handlers setup at the stage a test case executes the ""When
I stop naemon"" step.
Add a step that starts Naemon and wait until it finds a line in the
log file that indicates that it should be ready.
It's not perfect, but it seems to make the test case more stable.


Close opened files in feature tests.


Signed-off-by: Aksel Sjögren asjogren@itrsgroup.com",True,{}
naemon/naemon-core,https://github.com/naemon/naemon-core,353,2021-02-09T12:41:17Z,2021-02-09T13:17:35Z,2021-02-09T13:17:35Z,MERGED,True,43,8,6,https://github.com/sjoegren,Fix stopping Naemon in logging feature test,1,[],https://github.com/naemon/naemon-core/pull/353,https://github.com/sni,2,https://github.com/naemon/naemon-core/pull/353#issuecomment-775909669,"We noticed that logging tests failed when building in a new environment, it seemed to be a timing issue where the test case wants to SIGTERM naemon, but some investigation gave that naemon didn't completely start yet, so its signal handlers weren't installed and thus didn't stop as intended. This is an attempt to improve the timing issue.


Fix test that failed if Naemon haven't yet started correctly and got
it's signal handlers setup at the stage a test case executes the ""When
I stop naemon"" step.
Add a step that starts Naemon and wait until it finds a line in the
log file that indicates that it should be ready.
It's not perfect, but it seems to make the test case more stable.


Close opened files in feature tests.


Signed-off-by: Aksel Sjögren asjogren@itrsgroup.com",Is this the reason why the travis tests fail occasionally? For example this one: https://travis-ci.com/github/naemon/naemon-core/builds/213749004,True,{}
naemon/naemon-core,https://github.com/naemon/naemon-core,353,2021-02-09T12:41:17Z,2021-02-09T13:17:35Z,2021-02-09T13:17:35Z,MERGED,True,43,8,6,https://github.com/sjoegren,Fix stopping Naemon in logging feature test,1,[],https://github.com/naemon/naemon-core/pull/353,https://github.com/apps/lgtm-com,3,https://github.com/naemon/naemon-core/pull/353#issuecomment-775910227,"We noticed that logging tests failed when building in a new environment, it seemed to be a timing issue where the test case wants to SIGTERM naemon, but some investigation gave that naemon didn't completely start yet, so its signal handlers weren't installed and thus didn't stop as intended. This is an attempt to improve the timing issue.


Fix test that failed if Naemon haven't yet started correctly and got
it's signal handlers setup at the stage a test case executes the ""When
I stop naemon"" step.
Add a step that starts Naemon and wait until it finds a line in the
log file that indicates that it should be ready.
It's not perfect, but it seems to make the test case more stable.


Close opened files in feature tests.


Signed-off-by: Aksel Sjögren asjogren@itrsgroup.com","This pull request fixes 2 alerts when merging 406bc62 into 3013b53 - view on LGTM.com
fixed alerts:

2 for An assert statement has a side-effect",True,{}
naemon/naemon-core,https://github.com/naemon/naemon-core,353,2021-02-09T12:41:17Z,2021-02-09T13:17:35Z,2021-02-09T13:17:35Z,MERGED,True,43,8,6,https://github.com/sjoegren,Fix stopping Naemon in logging feature test,1,[],https://github.com/naemon/naemon-core/pull/353,https://github.com/sjoegren,4,https://github.com/naemon/naemon-core/pull/353#issuecomment-775910437,"We noticed that logging tests failed when building in a new environment, it seemed to be a timing issue where the test case wants to SIGTERM naemon, but some investigation gave that naemon didn't completely start yet, so its signal handlers weren't installed and thus didn't stop as intended. This is an attempt to improve the timing issue.


Fix test that failed if Naemon haven't yet started correctly and got
it's signal handlers setup at the stage a test case executes the ""When
I stop naemon"" step.
Add a step that starts Naemon and wait until it finds a line in the
log file that indicates that it should be ready.
It's not perfect, but it seems to make the test case more stable.


Close opened files in feature tests.


Signed-off-by: Aksel Sjögren asjogren@itrsgroup.com","Is this the reason why the travis tests fail occasionally? For example this one: https://travis-ci.com/github/naemon/naemon-core/builds/213749004

That is the exact same failure I had.",True,{}
naemon/naemon-core,https://github.com/naemon/naemon-core,354,2021-02-23T16:00:02Z,2021-02-26T13:22:09Z,2021-02-26T13:22:09Z,MERGED,True,51,24,14,https://github.com/sni,speed up cleanup() when having servicegroups,1,[],https://github.com/naemon/naemon-core/pull/354,https://github.com/sni,1,https://github.com/naemon/naemon-core/pull/354,"while clean up we iterate over all services and each service is removed from
all its servicegroups by iterating over the servicegroup members over and over
to remove each service one by one.
Adding a new flag truncate_lists which can be set on teardown to simply iterate once
over each list and remove/free all items.
This reduces the duration of a config check for an example config with 100k services from
30seconds to less than 3seconds.
Signed-off-by: Sven Nierlein sven@nierlein.de","while clean up we iterate over all services and each service is removed from
all its servicegroups by iterating over the servicegroup members over and over
to remove each service one by one.
Adding a new flag truncate_lists which can be set on teardown to simply iterate once
over each list and remove/free all items.
This reduces the duration of a config check for an example config with 100k services from
30seconds to less than 3seconds.
Signed-off-by: Sven Nierlein sven@nierlein.de",True,{}
naemon/naemon-core,https://github.com/naemon/naemon-core,355,2021-02-24T14:22:42Z,2021-02-26T08:05:13Z,2021-02-26T08:05:17Z,MERGED,True,1,1,1,https://github.com/jacobbaungard,Correclty set svc check_command for broker call,1,[],https://github.com/naemon/naemon-core/pull/355,https://github.com/jacobbaungard,1,https://github.com/naemon/naemon-core/pull/355,"This commit fixes a bug where the check_command argument was not
correctly set for the calls to broker_service_check with the
NEBTYPE_SERVICECHECK_PROCESSED type.
This in turn meant that the command_ members of the
nebstruct_service_check_data struct was never populated.
This is similar to the behaviour for host checks.
Signed-off-by: Jacob Hansen jhansen@itrsgroup.com
Co-authored-by: Robert Wikman rwikman@itrsgroup.com","This commit fixes a bug where the check_command argument was not
correctly set for the calls to broker_service_check with the
NEBTYPE_SERVICECHECK_PROCESSED type.
This in turn meant that the command_ members of the
nebstruct_service_check_data struct was never populated.
This is similar to the behaviour for host checks.
Signed-off-by: Jacob Hansen jhansen@itrsgroup.com
Co-authored-by: Robert Wikman rwikman@itrsgroup.com",True,{}
naemon/naemon-core,https://github.com/naemon/naemon-core,356,2021-03-02T10:58:49Z,2021-03-02T14:59:32Z,2021-03-02T14:59:32Z,MERGED,True,37,29,2,https://github.com/sni,migrate from travis to github actions,1,[],https://github.com/naemon/naemon-core/pull/356,https://github.com/sni,1,https://github.com/naemon/naemon-core/pull/356,Signed-off-by: Sven Nierlein sven@nierlein.de,Signed-off-by: Sven Nierlein sven@nierlein.de,True,{}
naemon/naemon-core,https://github.com/naemon/naemon-core,357,2021-03-03T09:59:20Z,2021-03-03T11:03:02Z,2021-03-03T11:03:02Z,MERGED,True,21,2,4,https://github.com/sni,release 1.2.4,1,[],https://github.com/naemon/naemon-core/pull/357,https://github.com/sni,1,https://github.com/naemon/naemon-core/pull/357,"Features:

improve config check when having lots of servicegroups

Changed:

migrate ci tests to github actions
change header file location in deb/rpm packages

Bugfixes:

fix svc in check_command broker call

Signed-off-by: Sven Nierlein sven@nierlein.de","Features:

improve config check when having lots of servicegroups

Changed:

migrate ci tests to github actions
change header file location in deb/rpm packages

Bugfixes:

fix svc in check_command broker call

Signed-off-by: Sven Nierlein sven@nierlein.de",True,{}
naemon/naemon-core,https://github.com/naemon/naemon-core,358,2021-03-03T11:40:44Z,2021-03-03T11:59:39Z,2021-03-03T11:59:47Z,MERGED,True,1,1,1,https://github.com/sni,fix news entry wording,1,[],https://github.com/naemon/naemon-core/pull/358,https://github.com/sni,1,https://github.com/naemon/naemon-core/pull/358,,,True,{}
naemon/naemon-core,https://github.com/naemon/naemon-core,358,2021-03-03T11:40:44Z,2021-03-03T11:59:39Z,2021-03-03T11:59:47Z,MERGED,True,1,1,1,https://github.com/sni,fix news entry wording,1,[],https://github.com/naemon/naemon-core/pull/358,https://github.com/sni,2,https://github.com/naemon/naemon-core/pull/358#issuecomment-789663794,,no worries :),True,{}
naemon/naemon-core,https://github.com/naemon/naemon-core,366,2021-06-12T12:34:23Z,2021-10-01T14:31:26Z,2021-10-01T14:31:27Z,MERGED,True,69,12,7,https://github.com/sni,feature vault macros and broker,2,[],https://github.com/naemon/naemon-core/pull/366,https://github.com/sni,1,https://github.com/naemon/naemon-core/pull/366,"this PR implements dynamic macro expansion from neb modules in order to
implement password vault broker neb modules. From naemons point of view, we
simply define a new macro prefix $VAULT...$ and add broker callbacks to fill
the value.
In order to make password vault work, we need enter a master password before
naemon starts, thats why closing stdin moved to after the neb module
initialization. So the neb module may implement reading something from stdin.
Advantage of this vault macros would be:

dynamic 3rd party macro expansion is not limited to passwords
password storage can be implemented in any way you like, ex. simply encrypted file or advanced remote vaults
vault macros are not limited to numbers like $VAULT1$ but can be anything ex.: $VAULTSNMPCOMMUNITY$

some context might be interesting during macro expansion, something like hostname, etc...
thats why the macros *mac struct is passed to the broker which might contain host/service/contact pointer.

Things to be done:

 master password is lost during reloads and stdin is closed at that point,
so need to find a way to save the master password over reloads.
 systemd integration needs to be tested (can stdin be used here)
 add simple example neb module

Signed-off-by: Sven Nierlein sven@nierlein.de","this PR implements dynamic macro expansion from neb modules in order to
implement password vault broker neb modules. From naemons point of view, we
simply define a new macro prefix $VAULT...$ and add broker callbacks to fill
the value.
In order to make password vault work, we need enter a master password before
naemon starts, thats why closing stdin moved to after the neb module
initialization. So the neb module may implement reading something from stdin.
Advantage of this vault macros would be:

dynamic 3rd party macro expansion is not limited to passwords
password storage can be implemented in any way you like, ex. simply encrypted file or advanced remote vaults
vault macros are not limited to numbers like $VAULT1$ but can be anything ex.: $VAULTSNMPCOMMUNITY$

some context might be interesting during macro expansion, something like hostname, etc...
thats why the macros *mac struct is passed to the broker which might contain host/service/contact pointer.

Things to be done:

 master password is lost during reloads and stdin is closed at that point,
so need to find a way to save the master password over reloads.
 systemd integration needs to be tested (can stdin be used here)
 add simple example neb module

Signed-off-by: Sven Nierlein sven@nierlein.de",True,{}
naemon/naemon-core,https://github.com/naemon/naemon-core,366,2021-06-12T12:34:23Z,2021-10-01T14:31:26Z,2021-10-01T14:31:27Z,MERGED,True,69,12,7,https://github.com/sni,feature vault macros and broker,2,[],https://github.com/naemon/naemon-core/pull/366,https://github.com/sni,2,https://github.com/naemon/naemon-core/pull/366#issuecomment-860049197,"this PR implements dynamic macro expansion from neb modules in order to
implement password vault broker neb modules. From naemons point of view, we
simply define a new macro prefix $VAULT...$ and add broker callbacks to fill
the value.
In order to make password vault work, we need enter a master password before
naemon starts, thats why closing stdin moved to after the neb module
initialization. So the neb module may implement reading something from stdin.
Advantage of this vault macros would be:

dynamic 3rd party macro expansion is not limited to passwords
password storage can be implemented in any way you like, ex. simply encrypted file or advanced remote vaults
vault macros are not limited to numbers like $VAULT1$ but can be anything ex.: $VAULTSNMPCOMMUNITY$

some context might be interesting during macro expansion, something like hostname, etc...
thats why the macros *mac struct is passed to the broker which might contain host/service/contact pointer.

Things to be done:

 master password is lost during reloads and stdin is closed at that point,
so need to find a way to save the master password over reloads.
 systemd integration needs to be tested (can stdin be used here)
 add simple example neb module

Signed-off-by: Sven Nierlein sven@nierlein.de",there is a POC module written in go here: https://github.com/sni/naemon-vault-example/blob/master/main.go,True,{}
naemon/naemon-core,https://github.com/naemon/naemon-core,366,2021-06-12T12:34:23Z,2021-10-01T14:31:26Z,2021-10-01T14:31:27Z,MERGED,True,69,12,7,https://github.com/sni,feature vault macros and broker,2,[],https://github.com/naemon/naemon-core/pull/366,https://github.com/jacobbaungard,3,https://github.com/naemon/naemon-core/pull/366#issuecomment-860705079,"this PR implements dynamic macro expansion from neb modules in order to
implement password vault broker neb modules. From naemons point of view, we
simply define a new macro prefix $VAULT...$ and add broker callbacks to fill
the value.
In order to make password vault work, we need enter a master password before
naemon starts, thats why closing stdin moved to after the neb module
initialization. So the neb module may implement reading something from stdin.
Advantage of this vault macros would be:

dynamic 3rd party macro expansion is not limited to passwords
password storage can be implemented in any way you like, ex. simply encrypted file or advanced remote vaults
vault macros are not limited to numbers like $VAULT1$ but can be anything ex.: $VAULTSNMPCOMMUNITY$

some context might be interesting during macro expansion, something like hostname, etc...
thats why the macros *mac struct is passed to the broker which might contain host/service/contact pointer.

Things to be done:

 master password is lost during reloads and stdin is closed at that point,
so need to find a way to save the master password over reloads.
 systemd integration needs to be tested (can stdin be used here)
 add simple example neb module

Signed-off-by: Sven Nierlein sven@nierlein.de","Nice!
Some thoughts:

Maybe it should be named something else than vault, given it can be used for anything (not just passwords)
How to handle naming collisions if multiple NEB modules implements this (or limit to only have one NEB module allowed with that capability)
When we do a reload we don't (have to) clear all memory, so we could probably implement some method for NEB modules to initialize memory on first load which isn't reset. Will probably be a bit messy and that wouldn't work on full restarts of course.
Another solution could be to store the vault secret in a root-only readable file on disk, I believe one could then make use of StandardInput=file:/path/to/credentials in systemd but I have not tested this. Slightly less secure than using user input directly, but saves one from having to do things manually.",True,{}
naemon/naemon-core,https://github.com/naemon/naemon-core,366,2021-06-12T12:34:23Z,2021-10-01T14:31:26Z,2021-10-01T14:31:27Z,MERGED,True,69,12,7,https://github.com/sni,feature vault macros and broker,2,[],https://github.com/naemon/naemon-core/pull/366,https://github.com/sni,4,https://github.com/naemon/naemon-core/pull/366#issuecomment-860716941,"this PR implements dynamic macro expansion from neb modules in order to
implement password vault broker neb modules. From naemons point of view, we
simply define a new macro prefix $VAULT...$ and add broker callbacks to fill
the value.
In order to make password vault work, we need enter a master password before
naemon starts, thats why closing stdin moved to after the neb module
initialization. So the neb module may implement reading something from stdin.
Advantage of this vault macros would be:

dynamic 3rd party macro expansion is not limited to passwords
password storage can be implemented in any way you like, ex. simply encrypted file or advanced remote vaults
vault macros are not limited to numbers like $VAULT1$ but can be anything ex.: $VAULTSNMPCOMMUNITY$

some context might be interesting during macro expansion, something like hostname, etc...
thats why the macros *mac struct is passed to the broker which might contain host/service/contact pointer.

Things to be done:

 master password is lost during reloads and stdin is closed at that point,
so need to find a way to save the master password over reloads.
 systemd integration needs to be tested (can stdin be used here)
 add simple example neb module

Signed-off-by: Sven Nierlein sven@nierlein.de","Thanks for having a look.

* Maybe it should be named something else than vault, given it can be used for anything (not just passwords)


Any name will do. I chose VAULT because that was the main purpose for now.

* How to handle naming collisions if multiple NEB modules implements this (or limit to only have one NEB module allowed with that capability)


Basically the last NEB wins, but since the same datastructure is passed to all NEBs in the order of load. A NEB could check if the value is already filled and then decide wether to overwrite, extend or leave it.

* When we do a reload we don't (have to) clear all memory, so we could probably implement some method for NEB modules to initialize memory on first load which isn't reset. Will probably be a bit messy and that wouldn't work on full restarts of course.


Right, maybe some kind of persistent key/value store would be nice. There is a key/value vector library in naemon already, i will check if its suiteable here.

* Another solution could be to store the vault secret in a root-only readable file on disk, I believe one could then make use of `StandardInput=file:/path/to/credentials` in systemd but I have not tested this. Slightly less secure than using user input directly, but saves one from having to do things manually.


That sounds promissing.",True,{}
naemon/naemon-core,https://github.com/naemon/naemon-core,366,2021-06-12T12:34:23Z,2021-10-01T14:31:26Z,2021-10-01T14:31:27Z,MERGED,True,69,12,7,https://github.com/sni,feature vault macros and broker,2,[],https://github.com/naemon/naemon-core/pull/366,https://github.com/nook24,5,https://github.com/naemon/naemon-core/pull/366#issuecomment-861378337,"this PR implements dynamic macro expansion from neb modules in order to
implement password vault broker neb modules. From naemons point of view, we
simply define a new macro prefix $VAULT...$ and add broker callbacks to fill
the value.
In order to make password vault work, we need enter a master password before
naemon starts, thats why closing stdin moved to after the neb module
initialization. So the neb module may implement reading something from stdin.
Advantage of this vault macros would be:

dynamic 3rd party macro expansion is not limited to passwords
password storage can be implemented in any way you like, ex. simply encrypted file or advanced remote vaults
vault macros are not limited to numbers like $VAULT1$ but can be anything ex.: $VAULTSNMPCOMMUNITY$

some context might be interesting during macro expansion, something like hostname, etc...
thats why the macros *mac struct is passed to the broker which might contain host/service/contact pointer.

Things to be done:

 master password is lost during reloads and stdin is closed at that point,
so need to find a way to save the master password over reloads.
 systemd integration needs to be tested (can stdin be used here)
 add simple example neb module

Signed-off-by: Sven Nierlein sven@nierlein.de","This sounds like I could use it for #362
I will definitely take a closer look at this after my vacation :)",True,{}
naemon/naemon-core,https://github.com/naemon/naemon-core,366,2021-06-12T12:34:23Z,2021-10-01T14:31:26Z,2021-10-01T14:31:27Z,MERGED,True,69,12,7,https://github.com/sni,feature vault macros and broker,2,[],https://github.com/naemon/naemon-core/pull/366,https://github.com/sni,6,https://github.com/naemon/naemon-core/pull/366#issuecomment-861383204,"this PR implements dynamic macro expansion from neb modules in order to
implement password vault broker neb modules. From naemons point of view, we
simply define a new macro prefix $VAULT...$ and add broker callbacks to fill
the value.
In order to make password vault work, we need enter a master password before
naemon starts, thats why closing stdin moved to after the neb module
initialization. So the neb module may implement reading something from stdin.
Advantage of this vault macros would be:

dynamic 3rd party macro expansion is not limited to passwords
password storage can be implemented in any way you like, ex. simply encrypted file or advanced remote vaults
vault macros are not limited to numbers like $VAULT1$ but can be anything ex.: $VAULTSNMPCOMMUNITY$

some context might be interesting during macro expansion, something like hostname, etc...
thats why the macros *mac struct is passed to the broker which might contain host/service/contact pointer.

Things to be done:

 master password is lost during reloads and stdin is closed at that point,
so need to find a way to save the master password over reloads.
 systemd integration needs to be tested (can stdin be used here)
 add simple example neb module

Signed-off-by: Sven Nierlein sven@nierlein.de","right, i am pretty sure you could implement those uuids with this change.",True,{}
naemon/naemon-core,https://github.com/naemon/naemon-core,366,2021-06-12T12:34:23Z,2021-10-01T14:31:26Z,2021-10-01T14:31:27Z,MERGED,True,69,12,7,https://github.com/sni,feature vault macros and broker,2,[],https://github.com/naemon/naemon-core/pull/366,https://github.com/sni,7,https://github.com/naemon/naemon-core/pull/366#issuecomment-863839386,"this PR implements dynamic macro expansion from neb modules in order to
implement password vault broker neb modules. From naemons point of view, we
simply define a new macro prefix $VAULT...$ and add broker callbacks to fill
the value.
In order to make password vault work, we need enter a master password before
naemon starts, thats why closing stdin moved to after the neb module
initialization. So the neb module may implement reading something from stdin.
Advantage of this vault macros would be:

dynamic 3rd party macro expansion is not limited to passwords
password storage can be implemented in any way you like, ex. simply encrypted file or advanced remote vaults
vault macros are not limited to numbers like $VAULT1$ but can be anything ex.: $VAULTSNMPCOMMUNITY$

some context might be interesting during macro expansion, something like hostname, etc...
thats why the macros *mac struct is passed to the broker which might contain host/service/contact pointer.

Things to be done:

 master password is lost during reloads and stdin is closed at that point,
so need to find a way to save the master password over reloads.
 systemd integration needs to be tested (can stdin be used here)
 add simple example neb module

Signed-off-by: Sven Nierlein sven@nierlein.de",here is another example module using this https://github.com/naemon/naemon-vimcrypt-vault-broker,True,{}
naemon/naemon-core,https://github.com/naemon/naemon-core,366,2021-06-12T12:34:23Z,2021-10-01T14:31:26Z,2021-10-01T14:31:27Z,MERGED,True,69,12,7,https://github.com/sni,feature vault macros and broker,2,[],https://github.com/naemon/naemon-core/pull/366,https://github.com/nook24,8,https://github.com/naemon/naemon-core/pull/366#issuecomment-872094655,"this PR implements dynamic macro expansion from neb modules in order to
implement password vault broker neb modules. From naemons point of view, we
simply define a new macro prefix $VAULT...$ and add broker callbacks to fill
the value.
In order to make password vault work, we need enter a master password before
naemon starts, thats why closing stdin moved to after the neb module
initialization. So the neb module may implement reading something from stdin.
Advantage of this vault macros would be:

dynamic 3rd party macro expansion is not limited to passwords
password storage can be implemented in any way you like, ex. simply encrypted file or advanced remote vaults
vault macros are not limited to numbers like $VAULT1$ but can be anything ex.: $VAULTSNMPCOMMUNITY$

some context might be interesting during macro expansion, something like hostname, etc...
thats why the macros *mac struct is passed to the broker which might contain host/service/contact pointer.

Things to be done:

 master password is lost during reloads and stdin is closed at that point,
so need to find a way to save the master password over reloads.
 systemd integration needs to be tested (can stdin be used here)
 add simple example neb module

Signed-off-by: Sven Nierlein sven@nierlein.de",Is there a timeline when this will be merged into the main branch?,True,{'THUMBS_UP': ['https://github.com/trevrobwhite']}
naemon/naemon-core,https://github.com/naemon/naemon-core,366,2021-06-12T12:34:23Z,2021-10-01T14:31:26Z,2021-10-01T14:31:27Z,MERGED,True,69,12,7,https://github.com/sni,feature vault macros and broker,2,[],https://github.com/naemon/naemon-core/pull/366,https://github.com/sni,9,https://github.com/naemon/naemon-core/pull/366#issuecomment-872358802,"this PR implements dynamic macro expansion from neb modules in order to
implement password vault broker neb modules. From naemons point of view, we
simply define a new macro prefix $VAULT...$ and add broker callbacks to fill
the value.
In order to make password vault work, we need enter a master password before
naemon starts, thats why closing stdin moved to after the neb module
initialization. So the neb module may implement reading something from stdin.
Advantage of this vault macros would be:

dynamic 3rd party macro expansion is not limited to passwords
password storage can be implemented in any way you like, ex. simply encrypted file or advanced remote vaults
vault macros are not limited to numbers like $VAULT1$ but can be anything ex.: $VAULTSNMPCOMMUNITY$

some context might be interesting during macro expansion, something like hostname, etc...
thats why the macros *mac struct is passed to the broker which might contain host/service/contact pointer.

Things to be done:

 master password is lost during reloads and stdin is closed at that point,
so need to find a way to save the master password over reloads.
 systemd integration needs to be tested (can stdin be used here)
 add simple example neb module

Signed-off-by: Sven Nierlein sven@nierlein.de","there is no timeline, its ready when its ready. There are two things open i'd like to test as mentioned in the first post.",True,{}
naemon/naemon-core,https://github.com/naemon/naemon-core,366,2021-06-12T12:34:23Z,2021-10-01T14:31:26Z,2021-10-01T14:31:27Z,MERGED,True,69,12,7,https://github.com/sni,feature vault macros and broker,2,[],https://github.com/naemon/naemon-core/pull/366,https://github.com/sni,10,https://github.com/naemon/naemon-core/pull/366#issuecomment-932204212,"this PR implements dynamic macro expansion from neb modules in order to
implement password vault broker neb modules. From naemons point of view, we
simply define a new macro prefix $VAULT...$ and add broker callbacks to fill
the value.
In order to make password vault work, we need enter a master password before
naemon starts, thats why closing stdin moved to after the neb module
initialization. So the neb module may implement reading something from stdin.
Advantage of this vault macros would be:

dynamic 3rd party macro expansion is not limited to passwords
password storage can be implemented in any way you like, ex. simply encrypted file or advanced remote vaults
vault macros are not limited to numbers like $VAULT1$ but can be anything ex.: $VAULTSNMPCOMMUNITY$

some context might be interesting during macro expansion, something like hostname, etc...
thats why the macros *mac struct is passed to the broker which might contain host/service/contact pointer.

Things to be done:

 master password is lost during reloads and stdin is closed at that point,
so need to find a way to save the master password over reloads.
 systemd integration needs to be tested (can stdin be used here)
 add simple example neb module

Signed-off-by: Sven Nierlein sven@nierlein.de",I'd say this is ready. Anything else is left to be implemented in the broker modules.,True,{}
naemon/naemon-core,https://github.com/naemon/naemon-core,367,2021-07-12T08:22:34Z,2021-07-12T08:35:00Z,2021-07-12T08:35:00Z,MERGED,True,7,2,1,https://github.com/nook24,Update description of date_format in default naemon.cfg ,3,[],https://github.com/naemon/naemon-core/pull/367,https://github.com/nook24,1,https://github.com/naemon/naemon-core/pull/367,"As the title already says, this is just a little change to update the description of the date_format option in the default naemon.cfg","As the title already says, this is just a little change to update the description of the date_format option in the default naemon.cfg",True,{}
naemon/naemon-core,https://github.com/naemon/naemon-core,367,2021-07-12T08:22:34Z,2021-07-12T08:35:00Z,2021-07-12T08:35:00Z,MERGED,True,7,2,1,https://github.com/nook24,Update description of date_format in default naemon.cfg ,3,[],https://github.com/naemon/naemon-core/pull/367,https://github.com/sni,2,https://github.com/naemon/naemon-core/pull/367#issuecomment-878085001,"As the title already says, this is just a little change to update the description of the date_format option in the default naemon.cfg",thanks,True,{}
naemon/naemon-core,https://github.com/naemon/naemon-core,369,2021-10-31T21:35:44Z,2021-11-03T20:33:03Z,2021-11-03T20:33:03Z,MERGED,True,159,29,4,https://github.com/sni,extract environment from command_line and set it directly,3,[],https://github.com/naemon/naemon-core/pull/369,https://github.com/sni,1,https://github.com/naemon/naemon-core/pull/369,"This PR removes exposing environment variables to the process list. The
example command line: TEST=1 FOO=BAR /bin/true
would be executed like: /bin/sh -c 'TEST=1 FOO=BAR /bin/true'
and the environement variables would be visible in ps output.
Instead we extend command line parsing and remove environment variable
definitions and put them into the environment before forking and running
the command.
Signed-off-by: Sven Nierlein sven@nierlein.de","This PR removes exposing environment variables to the process list. The
example command line: TEST=1 FOO=BAR /bin/true
would be executed like: /bin/sh -c 'TEST=1 FOO=BAR /bin/true'
and the environement variables would be visible in ps output.
Instead we extend command line parsing and remove environment variable
definitions and put them into the environment before forking and running
the command.
Signed-off-by: Sven Nierlein sven@nierlein.de",True,{}
naemon/naemon-core,https://github.com/naemon/naemon-core,369,2021-10-31T21:35:44Z,2021-11-03T20:33:03Z,2021-11-03T20:33:03Z,MERGED,True,159,29,4,https://github.com/sni,extract environment from command_line and set it directly,3,[],https://github.com/naemon/naemon-core/pull/369,https://github.com/sni,2,https://github.com/naemon/naemon-core/pull/369#issuecomment-956198422,"This PR removes exposing environment variables to the process list. The
example command line: TEST=1 FOO=BAR /bin/true
would be executed like: /bin/sh -c 'TEST=1 FOO=BAR /bin/true'
and the environement variables would be visible in ps output.
Instead we extend command line parsing and remove environment variable
definitions and put them into the environment before forking and running
the command.
Signed-off-by: Sven Nierlein sven@nierlein.de","* The env variable parsing logic is a little confusing to me. Is it possible to add some additional comments that explains the logic a bit clearer?


Sure, i will add a few more comments. It took me a while to figure it out myself :-)

* Do we support setting env variables to marcros here (i.e `ENV1=$VAULT_MYSQLPW$` ). Would be good with a testcase that includes a macro.


Naemon macros are already expanded at this point, so this is the final command line already with no more macros in it.",True,{}
naemon/naemon-core,https://github.com/naemon/naemon-core,370,2021-11-02T13:26:14Z,,2021-11-03T12:24:08Z,OPEN,False,31,5,3,https://github.com/ccztux,Implemented: Adding configuration for start and reload (#364),1,[],https://github.com/naemon/naemon-core/pull/370,https://github.com/ccztux,1,https://github.com/naemon/naemon-core/pull/370,"This pull request adds a config check before reloading/starting naemon. The implementation is done in the systemd unit and the init script. If the config check throws an error, reloading/starting will be aborted.","This pull request adds a config check before reloading/starting naemon. The implementation is done in the systemd unit and the init script. If the config check throws an error, reloading/starting will be aborted.",True,{}
naemon/naemon-core,https://github.com/naemon/naemon-core,370,2021-11-02T13:26:14Z,,2021-11-03T12:24:08Z,OPEN,False,31,5,3,https://github.com/ccztux,Implemented: Adding configuration for start and reload (#364),1,[],https://github.com/naemon/naemon-core/pull/370,https://github.com/jacobbaungard,2,https://github.com/naemon/naemon-core/pull/370#issuecomment-957664036,"This pull request adds a config check before reloading/starting naemon. The implementation is done in the systemd unit and the init script. If the config check throws an error, reloading/starting will be aborted.","Thanks for sending this is. We used to have something similar in the past although for restarts, however it was removed (see references below). The past is messy though, and it seems it was never removed for systemd, even though that was probably intended.
The reason at that time, is that for very large systems, reload/restarts/verify-config can take a significant amount of time. Bundling verify-config with the systemd/service commands makes it hard for a user/other software to make a choice what you want to prioritize. I.e. there is nothing stopping you today from running a verify config yourselves before doing the systemctl/service call, if that's what you prefer.
I personally don't have a strong opinion on the subject (and we ship our own systemd files anyway), but it's not entirely clear to me what the ""correct"" / ""best"" approach is.
References:
#170
#175
#177",True,{}
naemon/naemon-core,https://github.com/naemon/naemon-core,370,2021-11-02T13:26:14Z,,2021-11-03T12:24:08Z,OPEN,False,31,5,3,https://github.com/ccztux,Implemented: Adding configuration for start and reload (#364),1,[],https://github.com/naemon/naemon-core/pull/370,https://github.com/tomaszdubiel18,3,https://github.com/naemon/naemon-core/pull/370#issuecomment-958739783,"This pull request adds a config check before reloading/starting naemon. The implementation is done in the systemd unit and the init script. If the config check throws an error, reloading/starting will be aborted.","What's the problem to check the scale, the size of naemon installation and to perform the check at the restart when number of services/hosts/etc is not greater than X?
For me it is very expected feature, missing in Naemon, existing in Nagios. For my installation (2388 services, 169 hosts) config check takes miliseconds.
People make mistakes, forget about something and restarting Naemon with existing errors sometimes can take time to find the cause of it.",True,{}
naemon/naemon-core,https://github.com/naemon/naemon-core,370,2021-11-02T13:26:14Z,,2021-11-03T12:24:08Z,OPEN,False,31,5,3,https://github.com/ccztux,Implemented: Adding configuration for start and reload (#364),1,[],https://github.com/naemon/naemon-core/pull/370,https://github.com/sni,4,https://github.com/naemon/naemon-core/pull/370#issuecomment-958774108,"This pull request adds a config check before reloading/starting naemon. The implementation is done in the systemd unit and the init script. If the config check throws an error, reloading/starting will be aborted.","The problem is, this will add a random and arbitrary boundary at which reloads/restart behave differently.
The reason why we removed the config check is, that its super easy to do the check whenever you need it and its
quite hard to remove the check once its there and you don't want it.
I would consider this a standard workflow, you change something, you run a config check and if thats sucessful, do a reload.
In OMD we have a special environment variable CORE_NOVERIFY=yes which controls the behaviour if config checks before reload/restart/start commands. Maybe something like that would be a good idea for the standard rc files here as well.",True,{}
naemon/naemon-core,https://github.com/naemon/naemon-core,370,2021-11-02T13:26:14Z,,2021-11-03T12:24:08Z,OPEN,False,31,5,3,https://github.com/ccztux,Implemented: Adding configuration for start and reload (#364),1,[],https://github.com/naemon/naemon-core/pull/370,https://github.com/tomaszdubiel18,5,https://github.com/naemon/naemon-core/pull/370#issuecomment-958777323,"This pull request adds a config check before reloading/starting naemon. The implementation is done in the systemd unit and the init script. If the config check throws an error, reloading/starting will be aborted.","Maybe just add the parameter in Naemon configuration?
""Check Naemon configuration before the restart = true""
with default ""No""",True,{}
naemon/naemon-core,https://github.com/naemon/naemon-core,370,2021-11-02T13:26:14Z,,2021-11-03T12:24:08Z,OPEN,False,31,5,3,https://github.com/ccztux,Implemented: Adding configuration for start and reload (#364),1,[],https://github.com/naemon/naemon-core/pull/370,https://github.com/jacobbaungard,6,https://github.com/naemon/naemon-core/pull/370#issuecomment-958777720,"This pull request adds a config check before reloading/starting naemon. The implementation is done in the systemd unit and the init script. If the config check throws an error, reloading/starting will be aborted.","In OMD we have a special environment variable CORE_NOVERIFY=yes which controls the behaviour if config checks before reload/restart/start commands. Maybe something like that would be a good idea for the standard rc files here as well.

I think that sounds like a great solution 👍",True,{}
naemon/naemon-core,https://github.com/naemon/naemon-core,370,2021-11-02T13:26:14Z,,2021-11-03T12:24:08Z,OPEN,False,31,5,3,https://github.com/ccztux,Implemented: Adding configuration for start and reload (#364),1,[],https://github.com/naemon/naemon-core/pull/370,https://github.com/jacobbaungard,7,https://github.com/naemon/naemon-core/pull/370#issuecomment-958781521,"This pull request adds a config check before reloading/starting naemon. The implementation is done in the systemd unit and the init script. If the config check throws an error, reloading/starting will be aborted.","Maybe just add the parameter in Naemon configuration?
""Check Naemon configuration before the restart = true""
with default ""No""

I think that's not so easy to implement as we'd need to parse the config file somehow then (i.e in the systemd/service files). It would be much simpler to implement with an environment variable.",True,{}
naemon/naemon-core,https://github.com/naemon/naemon-core,370,2021-11-02T13:26:14Z,,2021-11-03T12:24:08Z,OPEN,False,31,5,3,https://github.com/ccztux,Implemented: Adding configuration for start and reload (#364),1,[],https://github.com/naemon/naemon-core/pull/370,https://github.com/nook24,8,https://github.com/naemon/naemon-core/pull/370#issuecomment-958796388,"This pull request adds a config check before reloading/starting naemon. The implementation is done in the systemd unit and the init script. If the config check throws an error, reloading/starting will be aborted.","I think this would be useful for users that are not working with Naemon all day long :)
To be fair, I guess most of us are using some tool to generate the Naemon config files for us. I don't think that @jacobbaungard or @sni are messing around with config files anymore.
I also assume that the power users are using own systemd services anyway. Mine for example does a config check:
[Service]
Type=forking
PIDFile=/opt/openitc/nagios/var/nagios.lock
ExecStartPre=/opt/openitc/nagios/bin/naemon -v /opt/openitc/etc/nagios/nagios.cfg
ExecStart=/opt/openitc/nagios/bin/naemon -d /opt/openitc/etc/nagios/nagios.cfg
ExecReload=/bin/kill -HUP $MAINPID

Also the config generator is running a validation before reloading Naemon:

On my test system with 5024 hosts and 93363 services the verification takes 2.5 seconds.
In my opinion a config validation should be enabled by default. Users with such big setups where the verification takes too long have enough experience to disable the verification, or are already using own systemd service configurations anyway.",True,{}
naemon/naemon-core,https://github.com/naemon/naemon-core,370,2021-11-02T13:26:14Z,,2021-11-03T12:24:08Z,OPEN,False,31,5,3,https://github.com/ccztux,Implemented: Adding configuration for start and reload (#364),1,[],https://github.com/naemon/naemon-core/pull/370,https://github.com/jacobbaungard,9,https://github.com/naemon/naemon-core/pull/370#issuecomment-958959748,"This pull request adds a config check before reloading/starting naemon. The implementation is done in the systemd unit and the init script. If the config check throws an error, reloading/starting will be aborted.","In my opinion a config validation should be enabled by default. Users with such big setups where the verification takes too long have enough experience to disable the verification, or are already using own systemd service configurations anyway.

OK for me as long as it's easy to disable with i.e an env var. You have any preference for default behaviour @sni ?",True,{}
naemon/naemon-core,https://github.com/naemon/naemon-core,370,2021-11-02T13:26:14Z,,2021-11-03T12:24:08Z,OPEN,False,31,5,3,https://github.com/ccztux,Implemented: Adding configuration for start and reload (#364),1,[],https://github.com/naemon/naemon-core/pull/370,https://github.com/sni,10,https://github.com/naemon/naemon-core/pull/370#issuecomment-958977991,"This pull request adds a config check before reloading/starting naemon. The implementation is done in the systemd unit and the init script. If the config check throws an error, reloading/starting will be aborted.",its fine for me if the default is enabled. As long as its easy to disable.,True,{}
naemon/naemon-core,https://github.com/naemon/naemon-core,370,2021-11-02T13:26:14Z,,2021-11-03T12:24:08Z,OPEN,False,31,5,3,https://github.com/ccztux,Implemented: Adding configuration for start and reload (#364),1,[],https://github.com/naemon/naemon-core/pull/370,https://github.com/ccztux,11,https://github.com/naemon/naemon-core/pull/370#issuecomment-958981380,"This pull request adds a config check before reloading/starting naemon. The implementation is done in the systemd unit and the init script. If the config check throws an error, reloading/starting will be aborted.","In OMD we have a special environment variable CORE_NOVERIFY=yes which controls the behaviour if config checks before reload/restart/start commands. Maybe something like that would be a good idea for the standard rc files here as well.

I think that sounds like a great solution 👍

That sounds great.",True,{}
naemon/naemon-core,https://github.com/naemon/naemon-core,371,2021-11-03T20:44:51Z,2021-11-05T07:45:02Z,2021-11-05T07:45:02Z,MERGED,True,21,2,4,https://github.com/sni,release 1.3.0,1,[],https://github.com/naemon/naemon-core/pull/371,https://github.com/sni,1,https://github.com/naemon/naemon-core/pull/371,"1.3.0 - Nov 03 2021
Features:

new vault broker api
secure environment variables

Changed:

none

Bugfixes:

none","1.3.0 - Nov 03 2021
Features:

new vault broker api
secure environment variables

Changed:

none

Bugfixes:

none",True,{}
naemon/naemon-core,https://github.com/naemon/naemon-core,371,2021-11-03T20:44:51Z,2021-11-05T07:45:02Z,2021-11-05T07:45:02Z,MERGED,True,21,2,4,https://github.com/sni,release 1.3.0,1,[],https://github.com/naemon/naemon-core/pull/371,https://github.com/jacobbaungard,2,https://github.com/naemon/naemon-core/pull/371#issuecomment-961191403,"1.3.0 - Nov 03 2021
Features:

new vault broker api
secure environment variables

Changed:

none

Bugfixes:

none","Although it's the 4th of November already. Perhaps should be set to 5th (i.e tomorrow, Friday) ?",True,{}
naemon/naemon-core,https://github.com/naemon/naemon-core,372,2021-11-08T10:56:23Z,2021-11-08T12:30:27Z,2021-11-08T12:30:27Z,MERGED,True,32,10,1,https://github.com/sni,naemon-core must depend on libnaemon,1,[],https://github.com/naemon/naemon-core/pull/372,https://github.com/sni,1,https://github.com/naemon/naemon-core/pull/372,"which is the case for rpms already. And while on it, line up dependencies.","which is the case for rpms already. And while on it, line up dependencies.",True,{'HOORAY': ['https://github.com/lamaral']}
naemon/naemon-core,https://github.com/naemon/naemon-core,375,2021-11-24T14:49:28Z,,2021-11-25T08:39:15Z,OPEN,False,25,12,3,https://github.com/nook24,Use hasmap to find comment data #374,1,[],https://github.com/naemon/naemon-core/pull/375,https://github.com/nook24,1,https://github.com/naemon/naemon-core/pull/375,"This is my attempt to patch #374. Would be good if you could review this.
Signed-off-by: nook24 info@nook24.eu","This is my attempt to patch #374. Would be good if you could review this.
Signed-off-by: nook24 info@nook24.eu",True,{}
naemon/naemon-core,https://github.com/naemon/naemon-core,375,2021-11-24T14:49:28Z,,2021-11-25T08:39:15Z,OPEN,False,25,12,3,https://github.com/nook24,Use hasmap to find comment data #374,1,[],https://github.com/naemon/naemon-core/pull/375,https://github.com/sni,2,https://github.com/naemon/naemon-core/pull/375#issuecomment-978015427,"This is my attempt to patch #374. Would be good if you could review this.
Signed-off-by: nook24 info@nook24.eu",Now there is a comment_hashtable and a comment_hashlist. Have you checked if we need both by any chance?,True,{}
naemon/naemon-core,https://github.com/naemon/naemon-core,375,2021-11-24T14:49:28Z,,2021-11-25T08:39:15Z,OPEN,False,25,12,3,https://github.com/nook24,Use hasmap to find comment data #374,1,[],https://github.com/naemon/naemon-core/pull/375,https://github.com/nook24,3,https://github.com/naemon/naemon-core/pull/375#issuecomment-978955224,"This is my attempt to patch #374. Would be good if you could review this.
Signed-off-by: nook24 info@nook24.eu","Tbh, I'm not sure if I understand whats happening with comment_hashlist by 100%. It is only used by comment *get_next_comment_by_host(char *host_name, comment *start). I think it is possible to get rid of it but I tried to touched as less code as possible.
How ever, by removing comment_hashlist we could also delete this:

  
    
      naemon-core/src/naemon/shared.c
    
    
        Lines 416 to 437
      in
      4700219
    
  
  
    

        
          
           /************************************************** 
        

        
          
            *************** HASH FUNCTIONS ******************* 
        

        
          
            **************************************************/ 
        

        
          
           /* dual hash function */ 
        

        
          
           int hashfunc(const char *name1, const char *name2, int hashslots) 
        

        
          
           { 
        

        
          
           	unsigned int i, result; 
        

        
          
            
        

        
          
           	result = 0; 
        

        
          
            
        

        
          
           	if (name1) 
        

        
          
           		for (i = 0; i < strlen(name1); i++) 
        

        
          
           			result += name1[i]; 
        

        
          
            
        

        
          
           	if (name2) 
        

        
          
           		for (i = 0; i < strlen(name2); i++) 
        

        
          
           			result += name2[i]; 
        

        
          
            
        

        
          
           	result = result % hashslots; 
        

        
          
            
        

        
          
           	return result; 
        

        
          
           }",True,{}
naemon/naemon-core,https://github.com/naemon/naemon-core,376,2021-11-29T15:58:04Z,2021-12-02T16:02:33Z,2021-12-02T16:02:33Z,MERGED,True,6,0,2,https://github.com/sni,pass commands containing tilde to shell,1,[],https://github.com/naemon/naemon-core/pull/376,https://github.com/sni,1,https://github.com/naemon/naemon-core/pull/376,"Commands containing a tilde ~ should be passed to the shell. execvp does not expand it.
Signed-off-by: Sven Nierlein sven@nierlein.de","Commands containing a tilde ~ should be passed to the shell. execvp does not expand it.
Signed-off-by: Sven Nierlein sven@nierlein.de",True,{}
naemon/naemon-core,https://github.com/naemon/naemon-core,379,2022-01-28T16:36:54Z,2022-01-31T10:53:00Z,2022-01-31T10:53:08Z,MERGED,True,13,13,2,https://github.com/lamaral,Cast time_t variables so tests run on Debian,1,[],https://github.com/naemon/naemon-core/pull/379,https://github.com/lamaral,1,https://github.com/naemon/naemon-core/pull/379,"While running the tests on Debian, they were failing due to an incompatibility on the size of time_t and the format that was being used when printing the messages in the tests.
Casting the variables fixed the tests on Debian. After the changes, I ran the tests inside a Ubuntu 20.04 container to make sure they didn't break and everything was fine.","While running the tests on Debian, they were failing due to an incompatibility on the size of time_t and the format that was being used when printing the messages in the tests.
Casting the variables fixed the tests on Debian. After the changes, I ran the tests inside a Ubuntu 20.04 container to make sure they didn't break and everything was fine.",True,{}
naemon/naemon-core,https://github.com/naemon/naemon-core,379,2022-01-28T16:36:54Z,2022-01-31T10:53:00Z,2022-01-31T10:53:08Z,MERGED,True,13,13,2,https://github.com/lamaral,Cast time_t variables so tests run on Debian,1,[],https://github.com/naemon/naemon-core/pull/379,https://github.com/sni,2,https://github.com/naemon/naemon-core/pull/379#issuecomment-1025605047,"While running the tests on Debian, they were failing due to an incompatibility on the size of time_t and the format that was being used when printing the messages in the tests.
Casting the variables fixed the tests on Debian. After the changes, I ran the tests inside a Ubuntu 20.04 container to make sure they didn't break and everything was fine.",i think i ran into this as well some time ago,True,{}
naemon/naemon-core,https://github.com/naemon/naemon-core,379,2022-01-28T16:36:54Z,2022-01-31T10:53:00Z,2022-01-31T10:53:08Z,MERGED,True,13,13,2,https://github.com/lamaral,Cast time_t variables so tests run on Debian,1,[],https://github.com/naemon/naemon-core/pull/379,https://github.com/sni,3,https://github.com/naemon/naemon-core/pull/379#issuecomment-1025609682,"While running the tests on Debian, they were failing due to an incompatibility on the size of time_t and the format that was being used when printing the messages in the tests.
Casting the variables fixed the tests on Debian. After the changes, I ran the tests inside a Ubuntu 20.04 container to make sure they didn't break and everything was fine.",thanks,True,{'THUMBS_UP': ['https://github.com/lamaral']}
naemon/naemon-core,https://github.com/naemon/naemon-core,381,2022-02-07T17:17:33Z,2022-02-07T20:12:28Z,2022-02-07T20:12:33Z,MERGED,True,1,0,1,https://github.com/ezbik,configtest_exit_code_fix,1,[],https://github.com/naemon/naemon-core/pull/381,https://github.com/ezbik,1,https://github.com/naemon/naemon-core/pull/381,Sets nonzero exit code status when config test fails.,Sets nonzero exit code status when config test fails.,True,{}
naemon/naemon-core,https://github.com/naemon/naemon-core,381,2022-02-07T17:17:33Z,2022-02-07T20:12:28Z,2022-02-07T20:12:33Z,MERGED,True,1,0,1,https://github.com/ezbik,configtest_exit_code_fix,1,[],https://github.com/naemon/naemon-core/pull/381,https://github.com/sni,2,https://github.com/naemon/naemon-core/pull/381#issuecomment-1031875878,Sets nonzero exit code status when config test fails.,thanks,True,{}
naemon/naemon-core,https://github.com/naemon/naemon-core,382,2022-03-10T14:01:18Z,2022-03-10T14:48:02Z,2022-03-23T08:05:01Z,MERGED,True,11,11,9,https://github.com/sni,let all links point to https://www.naemon.io,1,[],https://github.com/naemon/naemon-core/pull/382,https://github.com/sni,1,https://github.com/naemon/naemon-core/pull/382,,,True,{}
naemon/naemon-core,https://github.com/naemon/naemon-core,382,2022-03-10T14:01:18Z,2022-03-10T14:48:02Z,2022-03-23T08:05:01Z,MERGED,True,11,11,9,https://github.com/sni,let all links point to https://www.naemon.io,1,[],https://github.com/naemon/naemon-core/pull/382,https://github.com/nook24,2,https://github.com/naemon/naemon-core/pull/382#issuecomment-1064146329,,Will be naemon.org still available or should we all adjust our links?,True,{}
naemon/naemon-core,https://github.com/naemon/naemon-core,382,2022-03-10T14:01:18Z,2022-03-10T14:48:02Z,2022-03-23T08:05:01Z,MERGED,True,11,11,9,https://github.com/sni,let all links point to https://www.naemon.io,1,[],https://github.com/naemon/naemon-core/pull/382,https://github.com/sni,3,https://github.com/naemon/naemon-core/pull/382#issuecomment-1064149574,,"naemon.io is the new address. The old one will continue to work, but it is recommended to update all links if possible.",True,{}
naemon/naemon-core,https://github.com/naemon/naemon-core,382,2022-03-10T14:01:18Z,2022-03-10T14:48:02Z,2022-03-23T08:05:01Z,MERGED,True,11,11,9,https://github.com/sni,let all links point to https://www.naemon.io,1,[],https://github.com/naemon/naemon-core/pull/382,https://github.com/nook24,4,https://github.com/naemon/naemon-core/pull/382#issuecomment-1076044128,,Looks like naemon.org is dead now. This was quite fast. The Naemon's GitHub profile is also still using the .org domain:,True,{}
naemon/naemon-core,https://github.com/naemon/naemon-core,382,2022-03-10T14:01:18Z,2022-03-10T14:48:02Z,2022-03-23T08:05:01Z,MERGED,True,11,11,9,https://github.com/sni,let all links point to https://www.naemon.io,1,[],https://github.com/naemon/naemon-core/pull/382,https://github.com/sni,5,https://github.com/naemon/naemon-core/pull/382#issuecomment-1076056322,,"thanks, i fixed that link. naemon.org is dead, that's right and that's the reason for this move. What's still working is www.naemon.org.",True,{}
naemon/naemon-core,https://github.com/naemon/naemon-core,383,2022-03-16T12:22:40Z,2022-03-16T14:20:34Z,2022-03-16T14:20:35Z,MERGED,True,2,0,1,https://github.com/sni,set last_update broker_*_status,1,[],https://github.com/naemon/naemon-core/pull/383,https://github.com/sni,1,https://github.com/naemon/naemon-core/pull/383,"the broker status is usually called when the object changes, so lets update
the last_update attribute as well.
This addresses an issue where last_update does not get updated when a downtime ends.","the broker status is usually called when the object changes, so lets update
the last_update attribute as well.
This addresses an issue where last_update does not get updated when a downtime ends.",True,{}
naemon/naemon-core,https://github.com/naemon/naemon-core,384,2022-04-26T19:56:11Z,2022-04-27T09:14:13Z,2022-04-27T09:14:13Z,MERGED,True,18,2,4,https://github.com/sni,release v1.3.1,1,[],https://github.com/naemon/naemon-core/pull/384,https://github.com/sni,1,https://github.com/naemon/naemon-core/pull/384,,,True,{}
naemon/naemon-core,https://github.com/naemon/naemon-core,386,2022-05-24T06:44:54Z,2022-05-24T13:02:24Z,2022-05-24T13:02:24Z,MERGED,True,4,2,1,https://github.com/nook24,Update description of use_aggressive_host_checking #278,6,[],https://github.com/naemon/naemon-core/pull/386,https://github.com/nook24,1,https://github.com/naemon/naemon-core/pull/386,,,True,{}
naemon/naemon-core,https://github.com/naemon/naemon-core,386,2022-05-24T06:44:54Z,2022-05-24T13:02:24Z,2022-05-24T13:02:24Z,MERGED,True,4,2,1,https://github.com/nook24,Update description of use_aggressive_host_checking #278,6,[],https://github.com/naemon/naemon-core/pull/386,https://github.com/nook24,2,https://github.com/naemon/naemon-core/pull/386#issuecomment-1135569696,,You are right! I have fixed this.,True,{}
