edenhill/librdkafka,https://github.com/edenhill/librdkafka,572,2016-03-04T01:37:47Z,2016-03-10T15:15:53Z,2016-03-10T15:15:53Z,CLOSED,False,17,46,1,https://github.com/bruth,Represent feature listing as table,1,[],https://github.com/edenhill/librdkafka/pull/572,https://github.com/bruth,1,https://github.com/edenhill/librdkafka/pull/572,Signed-off-by: Byron Ruth b@devel.io,Signed-off-by: Byron Ruth b@devel.io,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,572,2016-03-04T01:37:47Z,2016-03-10T15:15:53Z,2016-03-10T15:15:53Z,CLOSED,False,17,46,1,https://github.com/bruth,Represent feature listing as table,1,[],https://github.com/edenhill/librdkafka/pull/572,https://github.com/edenhill,2,https://github.com/edenhill/librdkafka/pull/572#issuecomment-194862732,Signed-off-by: Byron Ruth b@devel.io,"Thanks, this looks good on github (markdown) but is very hard to read in text format (the README file is included in the Debian package, for example).",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,572,2016-03-04T01:37:47Z,2016-03-10T15:15:53Z,2016-03-10T15:15:53Z,CLOSED,False,17,46,1,https://github.com/bruth,Represent feature listing as table,1,[],https://github.com/edenhill/librdkafka/pull/572,https://github.com/bruth,3,https://github.com/edenhill/librdkafka/pull/572#issuecomment-194899065,Signed-off-by: Byron Ruth b@devel.io,Ah good call.,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,597,2016-04-04T09:11:01Z,2016-08-16T11:57:18Z,2016-08-16T11:57:18Z,CLOSED,False,17,2,3,https://github.com/takano-akio,Dynamically increase the max fetch size when needed,1,[],https://github.com/edenhill/librdkafka/pull/597,https://github.com/takano-akio,1,https://github.com/edenhill/librdkafka/pull/597,"This patch allows the max fetch size (currently fixed at fetch.msg.max.bytes) to be increased dynamically when a larger message has to be consumed.
Motivation: I have an application where a client have to seek around in a topic and consume a small amount of data after each seek. I'd like to set fetch.msg.max.bytes to a small value to minimize bandwidth usage. However, currently this has a danger that a consumer may become permanently stuck when there is a message whose size exceeds this value. I'd like a way to use a small transfer sizes while avoiding the danger.
Regarding the implementation, it's possible that I have made mistakes because I'm not very familiar with the code. In particular I'm not sure that I correctly implemented the check to identify a case where the next message is too large to consume.
Related issues: #320 and #482.","This patch allows the max fetch size (currently fixed at fetch.msg.max.bytes) to be increased dynamically when a larger message has to be consumed.
Motivation: I have an application where a client have to seek around in a topic and consume a small amount of data after each seek. I'd like to set fetch.msg.max.bytes to a small value to minimize bandwidth usage. However, currently this has a danger that a consumer may become permanently stuck when there is a message whose size exceeds this value. I'd like a way to use a small transfer sizes while avoiding the danger.
Regarding the implementation, it's possible that I have made mistakes because I'm not very familiar with the code. In particular I'm not sure that I correctly implemented the check to identify a case where the next message is too large to consume.
Related issues: #320 and #482.",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,597,2016-04-04T09:11:01Z,2016-08-16T11:57:18Z,2016-08-16T11:57:18Z,CLOSED,False,17,2,3,https://github.com/takano-akio,Dynamically increase the max fetch size when needed,1,[],https://github.com/edenhill/librdkafka/pull/597,https://github.com/takano-akio,2,https://github.com/edenhill/librdkafka/pull/597#issuecomment-209845137,"This patch allows the max fetch size (currently fixed at fetch.msg.max.bytes) to be increased dynamically when a larger message has to be consumed.
Motivation: I have an application where a client have to seek around in a topic and consume a small amount of data after each seek. I'd like to set fetch.msg.max.bytes to a small value to minimize bandwidth usage. However, currently this has a danger that a consumer may become permanently stuck when there is a message whose size exceeds this value. I'd like a way to use a small transfer sizes while avoiding the danger.
Regarding the implementation, it's possible that I have made mistakes because I'm not very familiar with the code. In particular I'm not sure that I correctly implemented the check to identify a case where the next message is too large to consume.
Related issues: #320 and #482.","Any comments on this? If this implementation is unsound and/or unacceptable, is there an alternative approach that may be taken?",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,597,2016-04-04T09:11:01Z,2016-08-16T11:57:18Z,2016-08-16T11:57:18Z,CLOSED,False,17,2,3,https://github.com/takano-akio,Dynamically increase the max fetch size when needed,1,[],https://github.com/edenhill/librdkafka/pull/597,https://github.com/asayers,3,https://github.com/edenhill/librdkafka/pull/597#issuecomment-236487496,"This patch allows the max fetch size (currently fixed at fetch.msg.max.bytes) to be increased dynamically when a larger message has to be consumed.
Motivation: I have an application where a client have to seek around in a topic and consume a small amount of data after each seek. I'd like to set fetch.msg.max.bytes to a small value to minimize bandwidth usage. However, currently this has a danger that a consumer may become permanently stuck when there is a message whose size exceeds this value. I'd like a way to use a small transfer sizes while avoiding the danger.
Regarding the implementation, it's possible that I have made mistakes because I'm not very familiar with the code. In particular I'm not sure that I correctly implemented the check to identify a case where the next message is too large to consume.
Related issues: #320 and #482.","I'd also like to have this functionality. @edenhill, do you have any thoughts on this patch?",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,597,2016-04-04T09:11:01Z,2016-08-16T11:57:18Z,2016-08-16T11:57:18Z,CLOSED,False,17,2,3,https://github.com/takano-akio,Dynamically increase the max fetch size when needed,1,[],https://github.com/edenhill/librdkafka/pull/597,https://github.com/edenhill,4,https://github.com/edenhill/librdkafka/pull/597#issuecomment-240080873,"This patch allows the max fetch size (currently fixed at fetch.msg.max.bytes) to be increased dynamically when a larger message has to be consumed.
Motivation: I have an application where a client have to seek around in a topic and consume a small amount of data after each seek. I'd like to set fetch.msg.max.bytes to a small value to minimize bandwidth usage. However, currently this has a danger that a consumer may become permanently stuck when there is a message whose size exceeds this value. I'd like a way to use a small transfer sizes while avoiding the danger.
Regarding the implementation, it's possible that I have made mistakes because I'm not very familiar with the code. In particular I'm not sure that I correctly implemented the check to identify a case where the next message is too large to consume.
Related issues: #320 and #482.","Thanks for your great patch @takano-akio.
It will be included in the next release.",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,605,2016-04-09T11:00:25Z,2016-04-10T06:53:34Z,2016-04-10T06:53:39Z,MERGED,True,26,8,1,https://github.com/danny794,added virtual destructors to all classes that have a virtual function,1,[],https://github.com/edenhill/librdkafka/pull/605,https://github.com/danny794,1,https://github.com/edenhill/librdkafka/pull/605,This ensures that the objects are properly cleaned up and also solves compilation issue when flag -Werror=non-virtual-dtor is present.,This ensures that the objects are properly cleaned up and also solves compilation issue when flag -Werror=non-virtual-dtor is present.,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,605,2016-04-09T11:00:25Z,2016-04-10T06:53:34Z,2016-04-10T06:53:39Z,MERGED,True,26,8,1,https://github.com/danny794,added virtual destructors to all classes that have a virtual function,1,[],https://github.com/edenhill/librdkafka/pull/605,https://github.com/edenhill,2,https://github.com/edenhill/librdkafka/pull/605#issuecomment-207935233,This ensures that the objects are properly cleaned up and also solves compilation issue when flag -Werror=non-virtual-dtor is present.,"Thanks @danny794 , looks good!",True,{'HOORAY': ['https://github.com/danny794']}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,612,2016-04-15T19:44:23Z,2016-05-19T08:29:09Z,2016-05-19T08:29:09Z,MERGED,True,1,0,1,https://github.com/rthalley,Initialize rktp_stored_offset after loading from a file.,1,[],https://github.com/edenhill/librdkafka/pull/612,https://github.com/rthalley,1,https://github.com/edenhill/librdkafka/pull/612,"This seemed to be needed, but alas I don't have good notes on the exact problem that not doing it caused.  (I'm trying to catch you up on outstanding patches I have that might be of general use.)","This seemed to be needed, but alas I don't have good notes on the exact problem that not doing it caused.  (I'm trying to catch you up on outstanding patches I have that might be of general use.)",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,613,2016-04-15T19:48:03Z,2016-05-12T09:14:31Z,2016-05-12T09:14:39Z,MERGED,True,4,4,2,https://github.com/rthalley,Fix a potential signed integer overflow that gcc 5.1 complains about,1,[],https://github.com/edenhill/librdkafka/pull/613,https://github.com/rthalley,1,https://github.com/edenhill/librdkafka/pull/613,"gcc 5.1 complains
rdlog.c:39:6: error: assuming signed overflow does not occur when
assuming that (X + c) >= X is always true [-Werror=strict-overflow]","gcc 5.1 complains
rdlog.c:39:6: error: assuming signed overflow does not occur when
assuming that (X + c) >= X is always true [-Werror=strict-overflow]",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,613,2016-04-15T19:48:03Z,2016-05-12T09:14:31Z,2016-05-12T09:14:39Z,MERGED,True,4,4,2,https://github.com/rthalley,Fix a potential signed integer overflow that gcc 5.1 complains about,1,[],https://github.com/edenhill/librdkafka/pull/613,https://github.com/xkrt,2,https://github.com/edenhill/librdkafka/pull/613#issuecomment-218702587,"gcc 5.1 complains
rdlog.c:39:6: error: assuming signed overflow does not occur when
assuming that (X + c) >= X is always true [-Werror=strict-overflow]","Please merge this, I cant build librdkafka in fresh Ubuntu Xenial 16.04 LTS",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,613,2016-04-15T19:48:03Z,2016-05-12T09:14:31Z,2016-05-12T09:14:39Z,MERGED,True,4,4,2,https://github.com/rthalley,Fix a potential signed integer overflow that gcc 5.1 complains about,1,[],https://github.com/edenhill/librdkafka/pull/613,https://github.com/edenhill,3,https://github.com/edenhill/librdkafka/pull/613#issuecomment-218702775,"gcc 5.1 complains
rdlog.c:39:6: error: assuming signed overflow does not occur when
assuming that (X + c) >= X is always true [-Werror=strict-overflow]",Thanks @rthalley !,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,615,2016-04-18T09:32:29Z,2016-04-18T10:29:23Z,2016-04-18T10:29:23Z,MERGED,True,3,1,2,https://github.com/vincentbernat,Misc fixes for Debian packaging,2,[],https://github.com/edenhill/librdkafka/pull/615,https://github.com/vincentbernat,1,https://github.com/edenhill/librdkafka/pull/615,Cc @paravoid,Cc @paravoid,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,615,2016-04-18T09:32:29Z,2016-04-18T10:29:23Z,2016-04-18T10:29:23Z,MERGED,True,3,1,2,https://github.com/vincentbernat,Misc fixes for Debian packaging,2,[],https://github.com/edenhill/librdkafka/pull/615,https://github.com/edenhill,2,https://github.com/edenhill/librdkafka/pull/615#issuecomment-211295382,Cc @paravoid,LGTM,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,628,2016-04-23T20:46:34Z,2017-01-03T13:56:20Z,2017-01-03T13:56:20Z,CLOSED,False,112,3,7,https://github.com/dwieland,issue #539 - support redirect of partition queue,4,[],https://github.com/edenhill/librdkafka/pull/628,https://github.com/dwieland,1,https://github.com/edenhill/librdkafka/pull/628,#539,#539,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,628,2016-04-23T20:46:34Z,2017-01-03T13:56:20Z,2017-01-03T13:56:20Z,CLOSED,False,112,3,7,https://github.com/dwieland,issue #539 - support redirect of partition queue,4,[],https://github.com/edenhill/librdkafka/pull/628,https://github.com/edenhill,2,https://github.com/edenhill/librdkafka/pull/628#issuecomment-213995573,#539,"Some remaining things:

You will need to add the symbol to tests/0006-symbol.c  (with all arguments being 0 or NULL)
Add a C++ interface (optional)
Add a test case (optional)",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,628,2016-04-23T20:46:34Z,2017-01-03T13:56:20Z,2017-01-03T13:56:20Z,CLOSED,False,112,3,7,https://github.com/dwieland,issue #539 - support redirect of partition queue,4,[],https://github.com/edenhill/librdkafka/pull/628,https://github.com/dwieland,3,https://github.com/edenhill/librdkafka/pull/628#issuecomment-214022453,#539,I will take a look at the tests later on.,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,628,2016-04-23T20:46:34Z,2017-01-03T13:56:20Z,2017-01-03T13:56:20Z,CLOSED,False,112,3,7,https://github.com/dwieland,issue #539 - support redirect of partition queue,4,[],https://github.com/edenhill/librdkafka/pull/628,https://github.com/edenhill,4,https://github.com/edenhill/librdkafka/pull/628#issuecomment-270119055,#539,work is being done in #953,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,643,2016-05-04T13:16:56Z,2016-05-04T13:38:23Z,2016-05-04T13:38:26Z,MERGED,True,7,0,4,https://github.com/janmejay,test-suite-fix: tests that expect more than one partition now create topic ahead-of-time,1,[],https://github.com/edenhill/librdkafka/pull/643,https://github.com/janmejay,1,https://github.com/edenhill/librdkafka/pull/643,These tests were failing because they were trying to publish to partitions that didn't exist (topics were being created with 1 partition (default)).,These tests were failing because they were trying to publish to partitions that didn't exist (topics were being created with 1 partition (default)).,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,643,2016-05-04T13:16:56Z,2016-05-04T13:38:23Z,2016-05-04T13:38:26Z,MERGED,True,7,0,4,https://github.com/janmejay,test-suite-fix: tests that expect more than one partition now create topic ahead-of-time,1,[],https://github.com/edenhill/librdkafka/pull/643,https://github.com/janmejay,2,https://github.com/edenhill/librdkafka/pull/643#issuecomment-216862909,These tests were failing because they were trying to publish to partitions that didn't exist (topics were being created with 1 partition (default)).,"Not sure about this, but appveyor run seems to have failed due to an unrelated problem.",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,643,2016-05-04T13:16:56Z,2016-05-04T13:38:23Z,2016-05-04T13:38:26Z,MERGED,True,7,0,4,https://github.com/janmejay,test-suite-fix: tests that expect more than one partition now create topic ahead-of-time,1,[],https://github.com/edenhill/librdkafka/pull/643,https://github.com/edenhill,3,https://github.com/edenhill/librdkafka/pull/643#issuecomment-216867474,These tests were failing because they were trying to publish to partitions that didn't exist (topics were being created with 1 partition (default)).,Thanks!,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,650,2016-05-10T13:59:36Z,2016-05-10T14:10:45Z,2016-05-10T14:10:45Z,MERGED,True,26,0,4,https://github.com/vincentbernat,Add ability to specify a CRL for checking remote certificates,1,[],https://github.com/edenhill/librdkafka/pull/650,https://github.com/vincentbernat,1,https://github.com/edenhill/librdkafka/pull/650,"By default, OpenSSL will ignore any CRL provided in the CA. With this
option, a user can provide a specific CRL file and, in this case, we
also instruct OpenSSL to do a CRL check when verifying a remote
certificate. If a user wants OpenSSL to check the CRL embedded in a CA,
they can just provide the CA instead of the CRL.","By default, OpenSSL will ignore any CRL provided in the CA. With this
option, a user can provide a specific CRL file and, in this case, we
also instruct OpenSSL to do a CRL check when verifying a remote
certificate. If a user wants OpenSSL to check the CRL embedded in a CA,
they can just provide the CA instead of the CRL.",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,652,2016-05-11T00:16:57Z,2016-05-11T08:17:39Z,2016-05-11T08:17:39Z,CLOSED,False,1,1,1,https://github.com/hzhxxx,Silence 'migrated to unknown broker' log (issue #394),1,[],https://github.com/edenhill/librdkafka/pull/652,https://github.com/hzhxxx,1,https://github.com/edenhill/librdkafka/pull/652,what time to support Consumer group and Consumer offset?,what time to support Consumer group and Consumer offset?,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,652,2016-05-11T00:16:57Z,2016-05-11T08:17:39Z,2016-05-11T08:17:39Z,CLOSED,False,1,1,1,https://github.com/hzhxxx,Silence 'migrated to unknown broker' log (issue #394),1,[],https://github.com/edenhill/librdkafka/pull/652,https://github.com/edenhill,2,https://github.com/edenhill/librdkafka/pull/652#issuecomment-218392577,what time to support Consumer group and Consumer offset?,"You should be using master branch for 0.8 brokers too, the 0.8 branch shouldnt be used.",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,664,2016-05-20T15:00:51Z,2016-05-20T15:03:42Z,2016-05-20T15:03:42Z,MERGED,True,16,1,2,https://github.com/ewencp,"Revert ""Migrate from our own -dbg package to the automatic -dbgsym package.""",1,[],https://github.com/edenhill/librdkafka/pull/664,https://github.com/ewencp,1,https://github.com/edenhill/librdkafka/pull/664,This reverts commit 003ba22.,This reverts commit 003ba22.,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,665,2016-05-23T11:40:19Z,2016-05-23T11:46:46Z,2016-10-13T11:15:37Z,MERGED,True,274,32,9,https://github.com/edenhill,SASL kerberos kinit command templating (issue #635),3,[],https://github.com/edenhill/librdkafka/pull/665,https://github.com/edenhill,1,https://github.com/edenhill/librdkafka/pull/665,,,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,679,2016-06-01T12:59:38Z,2019-07-12T12:48:05Z,2019-07-12T15:21:45Z,CLOSED,False,400,308,10,https://github.com/janmejay,m broker <-> n threads,2,[],https://github.com/edenhill/librdkafka/pull/679,https://github.com/janmejay,1,https://github.com/edenhill/librdkafka/pull/679,"create pre-configured number of broker threads (these threads do everything that broker threads used to do in 1:1 broker to thread model). The way they pace computation is different though, because they are multiplexing many brokers now.
new entity called rd_kafka_broker_thread_t (rkbt), which gets brokers assigned in a way that tries to balances number of brokers assigned per-thread.
termination and freeing of rkb is performed on rkbt
rkbt has a separate rkb array owned by the thread which is re-populated every time a broker is added or removed. Here addition or removal use rkbt broker-assignment lock, but this array is exclusively owned by rkbt. The rkb pointers are copied over this array and then used lock-lessly.
this introduces a new config parameter called 'broker.thread.count' (defaults to 1) and identifies number of threads user intends to run for all the broker related work, which allows user to configure n (thread count) independent of m (broker count)

Note: have tried to fix windows support in transport (WSApoll call), but not sure if that is the right thing to do. That area needs a very close review.","create pre-configured number of broker threads (these threads do everything that broker threads used to do in 1:1 broker to thread model). The way they pace computation is different though, because they are multiplexing many brokers now.
new entity called rd_kafka_broker_thread_t (rkbt), which gets brokers assigned in a way that tries to balances number of brokers assigned per-thread.
termination and freeing of rkb is performed on rkbt
rkbt has a separate rkb array owned by the thread which is re-populated every time a broker is added or removed. Here addition or removal use rkbt broker-assignment lock, but this array is exclusively owned by rkbt. The rkb pointers are copied over this array and then used lock-lessly.
this introduces a new config parameter called 'broker.thread.count' (defaults to 1) and identifies number of threads user intends to run for all the broker related work, which allows user to configure n (thread count) independent of m (broker count)

Note: have tried to fix windows support in transport (WSApoll call), but not sure if that is the right thing to do. That area needs a very close review.",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,679,2016-06-01T12:59:38Z,2019-07-12T12:48:05Z,2019-07-12T15:21:45Z,CLOSED,False,400,308,10,https://github.com/janmejay,m broker <-> n threads,2,[],https://github.com/edenhill/librdkafka/pull/679,https://github.com/janmejay,2,https://github.com/edenhill/librdkafka/pull/679#issuecomment-223013312,"create pre-configured number of broker threads (these threads do everything that broker threads used to do in 1:1 broker to thread model). The way they pace computation is different though, because they are multiplexing many brokers now.
new entity called rd_kafka_broker_thread_t (rkbt), which gets brokers assigned in a way that tries to balances number of brokers assigned per-thread.
termination and freeing of rkb is performed on rkbt
rkbt has a separate rkb array owned by the thread which is re-populated every time a broker is added or removed. Here addition or removal use rkbt broker-assignment lock, but this array is exclusively owned by rkbt. The rkb pointers are copied over this array and then used lock-lessly.
this introduces a new config parameter called 'broker.thread.count' (defaults to 1) and identifies number of threads user intends to run for all the broker related work, which allows user to configure n (thread count) independent of m (broker count)

Note: have tried to fix windows support in transport (WSApoll call), but not sure if that is the right thing to do. That area needs a very close review.","@edenhill here is the status of tests with this change:

all tests pass (bare)
all valgrind tests except 0014 pass (0014 is failing on master, haven't checked why yet, it fails in PR too)
all valgrind tests pass with this patch over HEAD as of Apr 30 (https://github.com/janmejay/librdkafka/commits/multiple_brokers_per_thread)
all helgrind tests pass this patch over HEAD as of Apr 30 (same branch)
some helgrind tests fail as of now on this rebased patch (they same set of tests fail on master too, without this patch)

I haven't tested it on windows and because I have changed WSApoll call, this requires a deeper review of that area.
Also found a problem in trivup (the kafka_path is hardcoded for your dev-env, I think). I had to change it to match my dev-env. Other than that, it worked good.",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,679,2016-06-01T12:59:38Z,2019-07-12T12:48:05Z,2019-07-12T15:21:45Z,CLOSED,False,400,308,10,https://github.com/janmejay,m broker <-> n threads,2,[],https://github.com/edenhill/librdkafka/pull/679,https://github.com/janmejay,3,https://github.com/edenhill/librdkafka/pull/679#issuecomment-223026787,"create pre-configured number of broker threads (these threads do everything that broker threads used to do in 1:1 broker to thread model). The way they pace computation is different though, because they are multiplexing many brokers now.
new entity called rd_kafka_broker_thread_t (rkbt), which gets brokers assigned in a way that tries to balances number of brokers assigned per-thread.
termination and freeing of rkb is performed on rkbt
rkbt has a separate rkb array owned by the thread which is re-populated every time a broker is added or removed. Here addition or removal use rkbt broker-assignment lock, but this array is exclusively owned by rkbt. The rkb pointers are copied over this array and then used lock-lessly.
this introduces a new config parameter called 'broker.thread.count' (defaults to 1) and identifies number of threads user intends to run for all the broker related work, which allows user to configure n (thread count) independent of m (broker count)

Note: have tried to fix windows support in transport (WSApoll call), but not sure if that is the right thing to do. That area needs a very close review.",TEST 0038 takes the same time across master and this patch. Both take 9 seconds. This is with default of broker-threads = 1 when using this patch.,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,679,2016-06-01T12:59:38Z,2019-07-12T12:48:05Z,2019-07-12T15:21:45Z,CLOSED,False,400,308,10,https://github.com/janmejay,m broker <-> n threads,2,[],https://github.com/edenhill/librdkafka/pull/679,https://github.com/janmejay,4,https://github.com/edenhill/librdkafka/pull/679#issuecomment-223028594,"create pre-configured number of broker threads (these threads do everything that broker threads used to do in 1:1 broker to thread model). The way they pace computation is different though, because they are multiplexing many brokers now.
new entity called rd_kafka_broker_thread_t (rkbt), which gets brokers assigned in a way that tries to balances number of brokers assigned per-thread.
termination and freeing of rkb is performed on rkbt
rkbt has a separate rkb array owned by the thread which is re-populated every time a broker is added or removed. Here addition or removal use rkbt broker-assignment lock, but this array is exclusively owned by rkbt. The rkb pointers are copied over this array and then used lock-lessly.
this introduces a new config parameter called 'broker.thread.count' (defaults to 1) and identifies number of threads user intends to run for all the broker related work, which allows user to configure n (thread count) independent of m (broker count)

Note: have tried to fix windows support in transport (WSApoll call), but not sure if that is the right thing to do. That area needs a very close review.","BTW, this is with trivup pointed to 0.9.0.1. I'll do another run with 0.10 and post the results here.",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,679,2016-06-01T12:59:38Z,2019-07-12T12:48:05Z,2019-07-12T15:21:45Z,CLOSED,False,400,308,10,https://github.com/janmejay,m broker <-> n threads,2,[],https://github.com/edenhill/librdkafka/pull/679,https://github.com/janmejay,5,https://github.com/edenhill/librdkafka/pull/679#issuecomment-223032626,"create pre-configured number of broker threads (these threads do everything that broker threads used to do in 1:1 broker to thread model). The way they pace computation is different though, because they are multiplexing many brokers now.
new entity called rd_kafka_broker_thread_t (rkbt), which gets brokers assigned in a way that tries to balances number of brokers assigned per-thread.
termination and freeing of rkb is performed on rkbt
rkbt has a separate rkb array owned by the thread which is re-populated every time a broker is added or removed. Here addition or removal use rkbt broker-assignment lock, but this array is exclusively owned by rkbt. The rkb pointers are copied over this array and then used lock-lessly.
this introduces a new config parameter called 'broker.thread.count' (defaults to 1) and identifies number of threads user intends to run for all the broker related work, which allows user to configure n (thread count) independent of m (broker count)

Note: have tried to fix windows support in transport (WSApoll call), but not sure if that is the right thing to do. That area needs a very close review.",0.10.0.0 comes out clean too (both bare and valgrind),True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,679,2016-06-01T12:59:38Z,2019-07-12T12:48:05Z,2019-07-12T15:21:45Z,CLOSED,False,400,308,10,https://github.com/janmejay,m broker <-> n threads,2,[],https://github.com/edenhill/librdkafka/pull/679,https://github.com/janmejay,6,https://github.com/edenhill/librdkafka/pull/679#issuecomment-223258056,"create pre-configured number of broker threads (these threads do everything that broker threads used to do in 1:1 broker to thread model). The way they pace computation is different though, because they are multiplexing many brokers now.
new entity called rd_kafka_broker_thread_t (rkbt), which gets brokers assigned in a way that tries to balances number of brokers assigned per-thread.
termination and freeing of rkb is performed on rkbt
rkbt has a separate rkb array owned by the thread which is re-populated every time a broker is added or removed. Here addition or removal use rkbt broker-assignment lock, but this array is exclusively owned by rkbt. The rkb pointers are copied over this array and then used lock-lessly.
this introduces a new config parameter called 'broker.thread.count' (defaults to 1) and identifies number of threads user intends to run for all the broker related work, which allows user to configure n (thread count) independent of m (broker count)

Note: have tried to fix windows support in transport (WSApoll call), but not sure if that is the right thing to do. That area needs a very close review.","Everything comes clean on 0.8.2.1 as well (haven't tested other 0.8.x.y versions).
BTW, 0014 with valgrind fails intermittently. Haven't had a chance to dig more yet.
I'll roll this out in a test cluster and plan to perform large cluster benchmarks with it after validation run.
Will keep you posted.",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,679,2016-06-01T12:59:38Z,2019-07-12T12:48:05Z,2019-07-12T15:21:45Z,CLOSED,False,400,308,10,https://github.com/janmejay,m broker <-> n threads,2,[],https://github.com/edenhill/librdkafka/pull/679,https://github.com/janmejay,7,https://github.com/edenhill/librdkafka/pull/679#issuecomment-225149530,"create pre-configured number of broker threads (these threads do everything that broker threads used to do in 1:1 broker to thread model). The way they pace computation is different though, because they are multiplexing many brokers now.
new entity called rd_kafka_broker_thread_t (rkbt), which gets brokers assigned in a way that tries to balances number of brokers assigned per-thread.
termination and freeing of rkb is performed on rkbt
rkbt has a separate rkb array owned by the thread which is re-populated every time a broker is added or removed. Here addition or removal use rkbt broker-assignment lock, but this array is exclusively owned by rkbt. The rkb pointers are copied over this array and then used lock-lessly.
this introduces a new config parameter called 'broker.thread.count' (defaults to 1) and identifies number of threads user intends to run for all the broker related work, which allows user to configure n (thread count) independent of m (broker count)

Note: have tried to fix windows support in transport (WSApoll call), but not sure if that is the right thing to do. That area needs a very close review.",This has been deployed (0.9.1 + patch) to a production env and has been working well for a few days now. In this scenario it is being used as a publisher that auto-partitions across 49 brokers (each broker having 10s of partitions). This is with default broker.thread.count (which is 1). Each producer node is pushing at ~100 Mbps and there are 66 producer nodes.,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,679,2016-06-01T12:59:38Z,2019-07-12T12:48:05Z,2019-07-12T15:21:45Z,CLOSED,False,400,308,10,https://github.com/janmejay,m broker <-> n threads,2,[],https://github.com/edenhill/librdkafka/pull/679,https://github.com/janmejay,8,https://github.com/edenhill/librdkafka/pull/679#issuecomment-227112253,"create pre-configured number of broker threads (these threads do everything that broker threads used to do in 1:1 broker to thread model). The way they pace computation is different though, because they are multiplexing many brokers now.
new entity called rd_kafka_broker_thread_t (rkbt), which gets brokers assigned in a way that tries to balances number of brokers assigned per-thread.
termination and freeing of rkb is performed on rkbt
rkbt has a separate rkb array owned by the thread which is re-populated every time a broker is added or removed. Here addition or removal use rkbt broker-assignment lock, but this array is exclusively owned by rkbt. The rkb pointers are copied over this array and then used lock-lessly.
this introduces a new config parameter called 'broker.thread.count' (defaults to 1) and identifies number of threads user intends to run for all the broker related work, which allows user to configure n (thread count) independent of m (broker count)

Note: have tried to fix windows support in transport (WSApoll call), but not sure if that is the right thing to do. That area needs a very close review.","So benchmark with 1 or 2 broker thread does extremely good (no CPU burn, no run-queue load visible). This is with 300 brokers.
Here is a profile with the change (2 broker-threads, 300 brokers):

Here is the old one for comparison (this was using the old thread-per-broker model) that I shared with you over mail when we started talking about this.",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,679,2016-06-01T12:59:38Z,2019-07-12T12:48:05Z,2019-07-12T15:21:45Z,CLOSED,False,400,308,10,https://github.com/janmejay,m broker <-> n threads,2,[],https://github.com/edenhill/librdkafka/pull/679,https://github.com/janmejay,9,https://github.com/edenhill/librdkafka/pull/679#issuecomment-227113092,"create pre-configured number of broker threads (these threads do everything that broker threads used to do in 1:1 broker to thread model). The way they pace computation is different though, because they are multiplexing many brokers now.
new entity called rd_kafka_broker_thread_t (rkbt), which gets brokers assigned in a way that tries to balances number of brokers assigned per-thread.
termination and freeing of rkb is performed on rkbt
rkbt has a separate rkb array owned by the thread which is re-populated every time a broker is added or removed. Here addition or removal use rkbt broker-assignment lock, but this array is exclusively owned by rkbt. The rkb pointers are copied over this array and then used lock-lessly.
this introduces a new config parameter called 'broker.thread.count' (defaults to 1) and identifies number of threads user intends to run for all the broker related work, which allows user to configure n (thread count) independent of m (broker count)

Note: have tried to fix windows support in transport (WSApoll call), but not sure if that is the right thing to do. That area needs a very close review.","In the second screenshot (old), section marked ""Production"" was running 49 brokers (which is why the problems does not show up). The one marked ""Benchmark"" was running 300 brokers (afair), which clearly shows the problem.",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,679,2016-06-01T12:59:38Z,2019-07-12T12:48:05Z,2019-07-12T15:21:45Z,CLOSED,False,400,308,10,https://github.com/janmejay,m broker <-> n threads,2,[],https://github.com/edenhill/librdkafka/pull/679,https://github.com/edenhill,10,https://github.com/edenhill/librdkafka/pull/679#issuecomment-230444442,"create pre-configured number of broker threads (these threads do everything that broker threads used to do in 1:1 broker to thread model). The way they pace computation is different though, because they are multiplexing many brokers now.
new entity called rd_kafka_broker_thread_t (rkbt), which gets brokers assigned in a way that tries to balances number of brokers assigned per-thread.
termination and freeing of rkb is performed on rkbt
rkbt has a separate rkb array owned by the thread which is re-populated every time a broker is added or removed. Here addition or removal use rkbt broker-assignment lock, but this array is exclusively owned by rkbt. The rkb pointers are copied over this array and then used lock-lessly.
this introduces a new config parameter called 'broker.thread.count' (defaults to 1) and identifies number of threads user intends to run for all the broker related work, which allows user to configure n (thread count) independent of m (broker count)

Note: have tried to fix windows support in transport (WSApoll call), but not sure if that is the right thing to do. That area needs a very close review.","Sorry for not getting back to you sooner, this is a substantial change and I need time to go through it.
I have some questions:

How are brokers balanced among the available threads?
When are brokers rebalanced?
How is throughput affected? (e.g., using rdkafka_performance)",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,679,2016-06-01T12:59:38Z,2019-07-12T12:48:05Z,2019-07-12T15:21:45Z,CLOSED,False,400,308,10,https://github.com/janmejay,m broker <-> n threads,2,[],https://github.com/edenhill/librdkafka/pull/679,https://github.com/janmejay,11,https://github.com/edenhill/librdkafka/pull/679#issuecomment-230451787,"create pre-configured number of broker threads (these threads do everything that broker threads used to do in 1:1 broker to thread model). The way they pace computation is different though, because they are multiplexing many brokers now.
new entity called rd_kafka_broker_thread_t (rkbt), which gets brokers assigned in a way that tries to balances number of brokers assigned per-thread.
termination and freeing of rkb is performed on rkbt
rkbt has a separate rkb array owned by the thread which is re-populated every time a broker is added or removed. Here addition or removal use rkbt broker-assignment lock, but this array is exclusively owned by rkbt. The rkb pointers are copied over this array and then used lock-lessly.
this introduces a new config parameter called 'broker.thread.count' (defaults to 1) and identifies number of threads user intends to run for all the broker related work, which allows user to configure n (thread count) independent of m (broker count)

Note: have tried to fix windows support in transport (WSApoll call), but not sure if that is the right thing to do. That area needs a very close review.","Hi Magnus, no problem. Answers:
Q: How are brokers balanced among the available threads?
A: All broker threads start with 0 brokers assigned. When adding a new broker it picks the broker threads with lowest number of assigned brokers (a per-broker-thread counter).
Q: When are brokers rebalanced?
A: As of now, never. But this is fairly simple to do. Intuitively it requires the same level of exclusive access as broker-addition. We can basically take all brokers out and add them back in according to the balancing criterion.
Q: How is throughput affected? (e.g., using rdkafka_performance)
A: Data from separate load-tests:

With 2 broker-threads, 300 brokers, 150 producers and aggregate throughput of 6M messages per second with each message between 1.5 to 1.8 KB: This was with snappy. Aggregate TP per producer: 64 Mbps (compressed).
With 1 broker-thread, 100 Mbps per producer, compressed(snappy), in a cluster of 49 brokers receiving from 66 producers.

Intuitively, because work that was dispatched on the broker thread hasn't changed, I don't anticipate any drop in performance in small clusters (small clusters can afford higher thread:broker ratio too). Large clusters such as the first-test above show remarkable improvement in screenshots). With 1:1 model we used to grind-halt in terms of throughput due to CPU burn. Even on 49 node kafka cluster (49 brokers) it used to burn atleast > 30% more CPU with 1:1 model, so lower compute overhead should lead to better throughput.
Every load-testing environment is different, so I think it may be worth running a perf-test in controlled environment. Because number of broker threads is configurable, smaller clusters can choose 1:1 model (balancing criterion will ensure 1:1 if number of broker threads is same as number of brokers).",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,679,2016-06-01T12:59:38Z,2019-07-12T12:48:05Z,2019-07-12T15:21:45Z,CLOSED,False,400,308,10,https://github.com/janmejay,m broker <-> n threads,2,[],https://github.com/edenhill/librdkafka/pull/679,https://github.com/janmejay,12,https://github.com/edenhill/librdkafka/pull/679#issuecomment-242194603,"create pre-configured number of broker threads (these threads do everything that broker threads used to do in 1:1 broker to thread model). The way they pace computation is different though, because they are multiplexing many brokers now.
new entity called rd_kafka_broker_thread_t (rkbt), which gets brokers assigned in a way that tries to balances number of brokers assigned per-thread.
termination and freeing of rkb is performed on rkbt
rkbt has a separate rkb array owned by the thread which is re-populated every time a broker is added or removed. Here addition or removal use rkbt broker-assignment lock, but this array is exclusively owned by rkbt. The rkb pointers are copied over this array and then used lock-lessly.
this introduces a new config parameter called 'broker.thread.count' (defaults to 1) and identifies number of threads user intends to run for all the broker related work, which allows user to configure n (thread count) independent of m (broker count)

Note: have tried to fix windows support in transport (WSApoll call), but not sure if that is the right thing to do. That area needs a very close review.","Fixed a bug (race condition between connect and poll) that left brokers in RD_KAFKA_BROKER_STATE_CONNECT state if acquiring the connection took long enough.
This is not related to m broker <-> n threads patch, this was an existing unrelated bug.",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,679,2016-06-01T12:59:38Z,2019-07-12T12:48:05Z,2019-07-12T15:21:45Z,CLOSED,False,400,308,10,https://github.com/janmejay,m broker <-> n threads,2,[],https://github.com/edenhill/librdkafka/pull/679,None,13,https://github.com/edenhill/librdkafka/pull/679#issuecomment-257864829,"create pre-configured number of broker threads (these threads do everything that broker threads used to do in 1:1 broker to thread model). The way they pace computation is different though, because they are multiplexing many brokers now.
new entity called rd_kafka_broker_thread_t (rkbt), which gets brokers assigned in a way that tries to balances number of brokers assigned per-thread.
termination and freeing of rkb is performed on rkbt
rkbt has a separate rkb array owned by the thread which is re-populated every time a broker is added or removed. Here addition or removal use rkbt broker-assignment lock, but this array is exclusively owned by rkbt. The rkb pointers are copied over this array and then used lock-lessly.
this introduces a new config parameter called 'broker.thread.count' (defaults to 1) and identifies number of threads user intends to run for all the broker related work, which allows user to configure n (thread count) independent of m (broker count)

Note: have tried to fix windows support in transport (WSApoll call), but not sure if that is the right thing to do. That area needs a very close review.","@edenhill Hi Magnus, can this patch getting merged into trunk? The Kafka Java client already does this and this would help us run librdkafka with a large number of broker machines.",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,679,2016-06-01T12:59:38Z,2019-07-12T12:48:05Z,2019-07-12T15:21:45Z,CLOSED,False,400,308,10,https://github.com/janmejay,m broker <-> n threads,2,[],https://github.com/edenhill/librdkafka/pull/679,https://github.com/ljackson,14,https://github.com/edenhill/librdkafka/pull/679#issuecomment-278089981,"create pre-configured number of broker threads (these threads do everything that broker threads used to do in 1:1 broker to thread model). The way they pace computation is different though, because they are multiplexing many brokers now.
new entity called rd_kafka_broker_thread_t (rkbt), which gets brokers assigned in a way that tries to balances number of brokers assigned per-thread.
termination and freeing of rkb is performed on rkbt
rkbt has a separate rkb array owned by the thread which is re-populated every time a broker is added or removed. Here addition or removal use rkbt broker-assignment lock, but this array is exclusively owned by rkbt. The rkb pointers are copied over this array and then used lock-lessly.
this introduces a new config parameter called 'broker.thread.count' (defaults to 1) and identifies number of threads user intends to run for all the broker related work, which allows user to configure n (thread count) independent of m (broker count)

Note: have tried to fix windows support in transport (WSApoll call), but not sure if that is the right thing to do. That area needs a very close review.",Bump? Whats the status of this change wrt master we may need this optimization soon?,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,679,2016-06-01T12:59:38Z,2019-07-12T12:48:05Z,2019-07-12T15:21:45Z,CLOSED,False,400,308,10,https://github.com/janmejay,m broker <-> n threads,2,[],https://github.com/edenhill/librdkafka/pull/679,https://github.com/edenhill,15,https://github.com/edenhill/librdkafka/pull/679#issuecomment-278122679,"create pre-configured number of broker threads (these threads do everything that broker threads used to do in 1:1 broker to thread model). The way they pace computation is different though, because they are multiplexing many brokers now.
new entity called rd_kafka_broker_thread_t (rkbt), which gets brokers assigned in a way that tries to balances number of brokers assigned per-thread.
termination and freeing of rkb is performed on rkbt
rkbt has a separate rkb array owned by the thread which is re-populated every time a broker is added or removed. Here addition or removal use rkbt broker-assignment lock, but this array is exclusively owned by rkbt. The rkb pointers are copied over this array and then used lock-lessly.
this introduces a new config parameter called 'broker.thread.count' (defaults to 1) and identifies number of threads user intends to run for all the broker related work, which allows user to configure n (thread count) independent of m (broker count)

Note: have tried to fix windows support in transport (WSApoll call), but not sure if that is the right thing to do. That area needs a very close review.","I really appreciate the effort in this PR, and I'll make use of some parts of it, but the way forward is to split up broker threads into IO threads (low-latency) and partition threads (high concurrency).
The target is the June release.
@ljackson Can you tell me what problems you are seeing?",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,679,2016-06-01T12:59:38Z,2019-07-12T12:48:05Z,2019-07-12T15:21:45Z,CLOSED,False,400,308,10,https://github.com/janmejay,m broker <-> n threads,2,[],https://github.com/edenhill/librdkafka/pull/679,https://github.com/ljackson,16,https://github.com/edenhill/librdkafka/pull/679#issuecomment-278148515,"create pre-configured number of broker threads (these threads do everything that broker threads used to do in 1:1 broker to thread model). The way they pace computation is different though, because they are multiplexing many brokers now.
new entity called rd_kafka_broker_thread_t (rkbt), which gets brokers assigned in a way that tries to balances number of brokers assigned per-thread.
termination and freeing of rkb is performed on rkbt
rkbt has a separate rkb array owned by the thread which is re-populated every time a broker is added or removed. Here addition or removal use rkbt broker-assignment lock, but this array is exclusively owned by rkbt. The rkb pointers are copied over this array and then used lock-lessly.
this introduces a new config parameter called 'broker.thread.count' (defaults to 1) and identifies number of threads user intends to run for all the broker related work, which allows user to configure n (thread count) independent of m (broker count)

Note: have tried to fix windows support in transport (WSApoll call), but not sure if that is the right thing to do. That area needs a very close review.",Nothing as of yet but we are just ramping up the replacement of Sarama with librdkafka/golang wrapper and we have large Kafka clusters and will be adding more. Mainly I wanted to understand how you were addressing the one thread per broker potential issues. Thx,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,679,2016-06-01T12:59:38Z,2019-07-12T12:48:05Z,2019-07-12T15:21:45Z,CLOSED,False,400,308,10,https://github.com/janmejay,m broker <-> n threads,2,[],https://github.com/edenhill/librdkafka/pull/679,https://github.com/edenhill,17,https://github.com/edenhill/librdkafka/pull/679#issuecomment-278152394,"create pre-configured number of broker threads (these threads do everything that broker threads used to do in 1:1 broker to thread model). The way they pace computation is different though, because they are multiplexing many brokers now.
new entity called rd_kafka_broker_thread_t (rkbt), which gets brokers assigned in a way that tries to balances number of brokers assigned per-thread.
termination and freeing of rkb is performed on rkbt
rkbt has a separate rkb array owned by the thread which is re-populated every time a broker is added or removed. Here addition or removal use rkbt broker-assignment lock, but this array is exclusively owned by rkbt. The rkb pointers are copied over this array and then used lock-lessly.
this introduces a new config parameter called 'broker.thread.count' (defaults to 1) and identifies number of threads user intends to run for all the broker related work, which allows user to configure n (thread count) independent of m (broker count)

Note: have tried to fix windows support in transport (WSApoll call), but not sure if that is the right thing to do. That area needs a very close review.","Happy to hear that.
What number of brokers, topics and typical partition counts do you reckon you'll see the coming 12 months?",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,679,2016-06-01T12:59:38Z,2019-07-12T12:48:05Z,2019-07-12T15:21:45Z,CLOSED,False,400,308,10,https://github.com/janmejay,m broker <-> n threads,2,[],https://github.com/edenhill/librdkafka/pull/679,https://github.com/janmejay,18,https://github.com/edenhill/librdkafka/pull/679#issuecomment-278251402,"create pre-configured number of broker threads (these threads do everything that broker threads used to do in 1:1 broker to thread model). The way they pace computation is different though, because they are multiplexing many brokers now.
new entity called rd_kafka_broker_thread_t (rkbt), which gets brokers assigned in a way that tries to balances number of brokers assigned per-thread.
termination and freeing of rkb is performed on rkbt
rkbt has a separate rkb array owned by the thread which is re-populated every time a broker is added or removed. Here addition or removal use rkbt broker-assignment lock, but this array is exclusively owned by rkbt. The rkb pointers are copied over this array and then used lock-lessly.
this introduces a new config parameter called 'broker.thread.count' (defaults to 1) and identifies number of threads user intends to run for all the broker related work, which allows user to configure n (thread count) independent of m (broker count)

Note: have tried to fix windows support in transport (WSApoll call), but not sure if that is the right thing to do. That area needs a very close review.",@edenhill can you describe the design you have in mind in more detail?,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,679,2016-06-01T12:59:38Z,2019-07-12T12:48:05Z,2019-07-12T15:21:45Z,CLOSED,False,400,308,10,https://github.com/janmejay,m broker <-> n threads,2,[],https://github.com/edenhill/librdkafka/pull/679,https://github.com/edenhill,19,https://github.com/edenhill/librdkafka/pull/679#issuecomment-278254912,"create pre-configured number of broker threads (these threads do everything that broker threads used to do in 1:1 broker to thread model). The way they pace computation is different though, because they are multiplexing many brokers now.
new entity called rd_kafka_broker_thread_t (rkbt), which gets brokers assigned in a way that tries to balances number of brokers assigned per-thread.
termination and freeing of rkb is performed on rkbt
rkbt has a separate rkb array owned by the thread which is re-populated every time a broker is added or removed. Here addition or removal use rkbt broker-assignment lock, but this array is exclusively owned by rkbt. The rkb pointers are copied over this array and then used lock-lessly.
this introduces a new config parameter called 'broker.thread.count' (defaults to 1) and identifies number of threads user intends to run for all the broker related work, which allows user to configure n (thread count) independent of m (broker count)

Note: have tried to fix windows support in transport (WSApoll call), but not sure if that is the right thing to do. That area needs a very close review.","@janmejay This design stems from solving the latency issue of waiting for condvars and IO simultaneously, which is not possible on most platforms:

have one (or more) IO threads that use the most appropriate IO event mechanism for the platform (epoll on linux, kqueues on osx, ...). They only wait on IO, not buffer queues, and it is up to the other threads to enable POLLOUT on fds when they have something to send. E.g., IO-based wakeups rather than condvar-based.  This solves the latency issue. Transmits will be immediate. One IO thread should be enough.
have a set of partition worker threads that does batching, (de)compression, protocol parsing, etc. This concurrency is for performance. How this thread pool is scaled (automatically, statically, weighed, ..) is not decided on.
the broker control plane is moved to the main rdkafka thread, with some parts (fetch decisions) moved to partition threads.",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,679,2016-06-01T12:59:38Z,2019-07-12T12:48:05Z,2019-07-12T15:21:45Z,CLOSED,False,400,308,10,https://github.com/janmejay,m broker <-> n threads,2,[],https://github.com/edenhill/librdkafka/pull/679,https://github.com/rgerhards,20,https://github.com/edenhill/librdkafka/pull/679#issuecomment-355220889,"create pre-configured number of broker threads (these threads do everything that broker threads used to do in 1:1 broker to thread model). The way they pace computation is different though, because they are multiplexing many brokers now.
new entity called rd_kafka_broker_thread_t (rkbt), which gets brokers assigned in a way that tries to balances number of brokers assigned per-thread.
termination and freeing of rkb is performed on rkbt
rkbt has a separate rkb array owned by the thread which is re-populated every time a broker is added or removed. Here addition or removal use rkbt broker-assignment lock, but this array is exclusively owned by rkbt. The rkb pointers are copied over this array and then used lock-lessly.
this introduces a new config parameter called 'broker.thread.count' (defaults to 1) and identifies number of threads user intends to run for all the broker related work, which allows user to configure n (thread count) independent of m (broker count)

Note: have tried to fix windows support in transport (WSApoll call), but not sure if that is the right thing to do. That area needs a very close review.",Just for the records: we are also getting user questions on kafka tuning from  high-performance  rsyslog users.,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,679,2016-06-01T12:59:38Z,2019-07-12T12:48:05Z,2019-07-12T15:21:45Z,CLOSED,False,400,308,10,https://github.com/janmejay,m broker <-> n threads,2,[],https://github.com/edenhill/librdkafka/pull/679,https://github.com/edenhill,21,https://github.com/edenhill/librdkafka/pull/679#issuecomment-355307618,"create pre-configured number of broker threads (these threads do everything that broker threads used to do in 1:1 broker to thread model). The way they pace computation is different though, because they are multiplexing many brokers now.
new entity called rd_kafka_broker_thread_t (rkbt), which gets brokers assigned in a way that tries to balances number of brokers assigned per-thread.
termination and freeing of rkb is performed on rkbt
rkbt has a separate rkb array owned by the thread which is re-populated every time a broker is added or removed. Here addition or removal use rkbt broker-assignment lock, but this array is exclusively owned by rkbt. The rkb pointers are copied over this array and then used lock-lessly.
this introduces a new config parameter called 'broker.thread.count' (defaults to 1) and identifies number of threads user intends to run for all the broker related work, which allows user to configure n (thread count) independent of m (broker count)

Note: have tried to fix windows support in transport (WSApoll call), but not sure if that is the right thing to do. That area needs a very close review.",We're looking to address this issue in Q2.,True,{'THUMBS_UP': ['https://github.com/rgerhards']}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,679,2016-06-01T12:59:38Z,2019-07-12T12:48:05Z,2019-07-12T15:21:45Z,CLOSED,False,400,308,10,https://github.com/janmejay,m broker <-> n threads,2,[],https://github.com/edenhill/librdkafka/pull/679,https://github.com/edenhill,22,https://github.com/edenhill/librdkafka/pull/679#issuecomment-355307729,"create pre-configured number of broker threads (these threads do everything that broker threads used to do in 1:1 broker to thread model). The way they pace computation is different though, because they are multiplexing many brokers now.
new entity called rd_kafka_broker_thread_t (rkbt), which gets brokers assigned in a way that tries to balances number of brokers assigned per-thread.
termination and freeing of rkb is performed on rkbt
rkbt has a separate rkb array owned by the thread which is re-populated every time a broker is added or removed. Here addition or removal use rkbt broker-assignment lock, but this array is exclusively owned by rkbt. The rkb pointers are copied over this array and then used lock-lessly.
this introduces a new config parameter called 'broker.thread.count' (defaults to 1) and identifies number of threads user intends to run for all the broker related work, which allows user to configure n (thread count) independent of m (broker count)

Note: have tried to fix windows support in transport (WSApoll call), but not sure if that is the right thing to do. That area needs a very close review.",#825,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,679,2016-06-01T12:59:38Z,2019-07-12T12:48:05Z,2019-07-12T15:21:45Z,CLOSED,False,400,308,10,https://github.com/janmejay,m broker <-> n threads,2,[],https://github.com/edenhill/librdkafka/pull/679,https://github.com/qduyang,23,https://github.com/edenhill/librdkafka/pull/679#issuecomment-409804380,"create pre-configured number of broker threads (these threads do everything that broker threads used to do in 1:1 broker to thread model). The way they pace computation is different though, because they are multiplexing many brokers now.
new entity called rd_kafka_broker_thread_t (rkbt), which gets brokers assigned in a way that tries to balances number of brokers assigned per-thread.
termination and freeing of rkb is performed on rkbt
rkbt has a separate rkb array owned by the thread which is re-populated every time a broker is added or removed. Here addition or removal use rkbt broker-assignment lock, but this array is exclusively owned by rkbt. The rkb pointers are copied over this array and then used lock-lessly.
this introduces a new config parameter called 'broker.thread.count' (defaults to 1) and identifies number of threads user intends to run for all the broker related work, which allows user to configure n (thread count) independent of m (broker count)

Note: have tried to fix windows support in transport (WSApoll call), but not sure if that is the right thing to do. That area needs a very close review.","Hi, there are too many threads if i have multiple consumers, and it will  cause performance issue if there are thread context switching.
May I know when we could have the feature to reduce thread count?",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,679,2016-06-01T12:59:38Z,2019-07-12T12:48:05Z,2019-07-12T15:21:45Z,CLOSED,False,400,308,10,https://github.com/janmejay,m broker <-> n threads,2,[],https://github.com/edenhill/librdkafka/pull/679,https://github.com/rnpridgeon,24,https://github.com/edenhill/librdkafka/pull/679#issuecomment-409912292,"create pre-configured number of broker threads (these threads do everything that broker threads used to do in 1:1 broker to thread model). The way they pace computation is different though, because they are multiplexing many brokers now.
new entity called rd_kafka_broker_thread_t (rkbt), which gets brokers assigned in a way that tries to balances number of brokers assigned per-thread.
termination and freeing of rkb is performed on rkbt
rkbt has a separate rkb array owned by the thread which is re-populated every time a broker is added or removed. Here addition or removal use rkbt broker-assignment lock, but this array is exclusively owned by rkbt. The rkb pointers are copied over this array and then used lock-lessly.
this introduces a new config parameter called 'broker.thread.count' (defaults to 1) and identifies number of threads user intends to run for all the broker related work, which allows user to configure n (thread count) independent of m (broker count)

Note: have tried to fix windows support in transport (WSApoll call), but not sure if that is the right thing to do. That area needs a very close review.","@qduyang
#1659",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,679,2016-06-01T12:59:38Z,2019-07-12T12:48:05Z,2019-07-12T15:21:45Z,CLOSED,False,400,308,10,https://github.com/janmejay,m broker <-> n threads,2,[],https://github.com/edenhill/librdkafka/pull/679,https://github.com/qduyang,25,https://github.com/edenhill/librdkafka/pull/679#issuecomment-410135025,"create pre-configured number of broker threads (these threads do everything that broker threads used to do in 1:1 broker to thread model). The way they pace computation is different though, because they are multiplexing many brokers now.
new entity called rd_kafka_broker_thread_t (rkbt), which gets brokers assigned in a way that tries to balances number of brokers assigned per-thread.
termination and freeing of rkb is performed on rkbt
rkbt has a separate rkb array owned by the thread which is re-populated every time a broker is added or removed. Here addition or removal use rkbt broker-assignment lock, but this array is exclusively owned by rkbt. The rkb pointers are copied over this array and then used lock-lessly.
this introduces a new config parameter called 'broker.thread.count' (defaults to 1) and identifies number of threads user intends to run for all the broker related work, which allows user to configure n (thread count) independent of m (broker count)

Note: have tried to fix windows support in transport (WSApoll call), but not sure if that is the right thing to do. That area needs a very close review.","@rnpridgeon Thanks for your information. But the solution isn't going to solve my problem, because I have to consume messages from multiple sources in parallel. For example I have 10 separate Kafka message sources (with each source has 3 brokers), in this case there would have about ~40 threads in background and which would cause heavy CPU contention.",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,679,2016-06-01T12:59:38Z,2019-07-12T12:48:05Z,2019-07-12T15:21:45Z,CLOSED,False,400,308,10,https://github.com/janmejay,m broker <-> n threads,2,[],https://github.com/edenhill/librdkafka/pull/679,https://github.com/rgerhards,26,https://github.com/edenhill/librdkafka/pull/679#issuecomment-510880378,"create pre-configured number of broker threads (these threads do everything that broker threads used to do in 1:1 broker to thread model). The way they pace computation is different though, because they are multiplexing many brokers now.
new entity called rd_kafka_broker_thread_t (rkbt), which gets brokers assigned in a way that tries to balances number of brokers assigned per-thread.
termination and freeing of rkb is performed on rkbt
rkbt has a separate rkb array owned by the thread which is re-populated every time a broker is added or removed. Here addition or removal use rkbt broker-assignment lock, but this array is exclusively owned by rkbt. The rkb pointers are copied over this array and then used lock-lessly.
this introduces a new config parameter called 'broker.thread.count' (defaults to 1) and identifies number of threads user intends to run for all the broker related work, which allows user to configure n (thread count) independent of m (broker count)

Note: have tried to fix windows support in transport (WSApoll call), but not sure if that is the right thing to do. That area needs a very close review.","@edenhill does the ""close"" mean this issue has now been addressed?",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,679,2016-06-01T12:59:38Z,2019-07-12T12:48:05Z,2019-07-12T15:21:45Z,CLOSED,False,400,308,10,https://github.com/janmejay,m broker <-> n threads,2,[],https://github.com/edenhill/librdkafka/pull/679,https://github.com/edenhill,27,https://github.com/edenhill/librdkafka/pull/679#issuecomment-510927208,"create pre-configured number of broker threads (these threads do everything that broker threads used to do in 1:1 broker to thread model). The way they pace computation is different though, because they are multiplexing many brokers now.
new entity called rd_kafka_broker_thread_t (rkbt), which gets brokers assigned in a way that tries to balances number of brokers assigned per-thread.
termination and freeing of rkb is performed on rkbt
rkbt has a separate rkb array owned by the thread which is re-populated every time a broker is added or removed. Here addition or removal use rkbt broker-assignment lock, but this array is exclusively owned by rkbt. The rkb pointers are copied over this array and then used lock-lessly.
this introduces a new config parameter called 'broker.thread.count' (defaults to 1) and identifies number of threads user intends to run for all the broker related work, which allows user to configure n (thread count) independent of m (broker count)

Note: have tried to fix windows support in transport (WSApoll call), but not sure if that is the right thing to do. That area needs a very close review.","@rgerhards No, we're still spawning one thread per broker.
We'll eventually look into fixing this, but not in the near term.",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,683,2016-06-02T18:22:50Z,2017-02-10T12:15:18Z,2017-02-10T12:15:18Z,CLOSED,False,167,32,4,https://github.com/twaters,Metadata Updates,3,[],https://github.com/edenhill/librdkafka/pull/683,https://github.com/twaters,1,https://github.com/edenhill/librdkafka/pull/683,"Adds ability to cache the latest metadata.  Cached metadata is saved to the broker struct and protected with the broker's mutex.
Adds ability to retrieve the cached metadata by passing a < 0 timeout to the metadata call.
Fixes a reference message request ""leak"":


Metadata requests were not actually timing out correctly (the rkbuf timeout wasn't set to the configuration value).
Metadata requests were always set to retry.  So even when shutting down the broker the message queue was populated and processing.  Set to not retry, relying instead on the periodic request of the metadata, allowing a clean shutdown.","Adds ability to cache the latest metadata.  Cached metadata is saved to the broker struct and protected with the broker's mutex.
Adds ability to retrieve the cached metadata by passing a < 0 timeout to the metadata call.
Fixes a reference message request ""leak"":


Metadata requests were not actually timing out correctly (the rkbuf timeout wasn't set to the configuration value).
Metadata requests were always set to retry.  So even when shutting down the broker the message queue was populated and processing.  Set to not retry, relying instead on the periodic request of the metadata, allowing a clean shutdown.",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,683,2016-06-02T18:22:50Z,2017-02-10T12:15:18Z,2017-02-10T12:15:18Z,CLOSED,False,167,32,4,https://github.com/twaters,Metadata Updates,3,[],https://github.com/edenhill/librdkafka/pull/683,https://github.com/edenhill,2,https://github.com/edenhill/librdkafka/pull/683#issuecomment-278929106,"Adds ability to cache the latest metadata.  Cached metadata is saved to the broker struct and protected with the broker's mutex.
Adds ability to retrieve the cached metadata by passing a < 0 timeout to the metadata call.
Fixes a reference message request ""leak"":


Metadata requests were not actually timing out correctly (the rkbuf timeout wasn't set to the configuration value).
Metadata requests were always set to retry.  So even when shutting down the broker the message queue was populated and processing.  Set to not retry, relying instead on the periodic request of the metadata, allowing a clean shutdown.","The metadata layer has been rewritten.
Will make sure to add a get-cached-metadata variant.
Thanks for this though!",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,690,2016-06-13T17:56:21Z,2016-08-16T11:58:30Z,2016-08-16T18:17:22Z,CLOSED,False,4,7,2,https://github.com/senior7515,Makefile: add enable/disable lz4 builds,1,[],https://github.com/edenhill/librdkafka/pull/690,https://github.com/senior7515,1,https://github.com/edenhill/librdkafka/pull/690,"Added ability to add or remove -llz4 builds.

This is to get it to compile on ubuntu 14.04 which doesn't support the latest lz4 lib avail on 16.04
The header lz4frame.h is missing on liblz4-dev
Style is compatible with the other enable/disable makefile flags.

Signed-off-by: Alexander Gallego gallego.alexx@gmail.com","Added ability to add or remove -llz4 builds.

This is to get it to compile on ubuntu 14.04 which doesn't support the latest lz4 lib avail on 16.04
The header lz4frame.h is missing on liblz4-dev
Style is compatible with the other enable/disable makefile flags.

Signed-off-by: Alexander Gallego gallego.alexx@gmail.com",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,690,2016-06-13T17:56:21Z,2016-08-16T11:58:30Z,2016-08-16T18:17:22Z,CLOSED,False,4,7,2,https://github.com/senior7515,Makefile: add enable/disable lz4 builds,1,[],https://github.com/edenhill/librdkafka/pull/690,https://github.com/edenhill,2,https://github.com/edenhill/librdkafka/pull/690#issuecomment-225669899,"Added ability to add or remove -llz4 builds.

This is to get it to compile on ubuntu 14.04 which doesn't support the latest lz4 lib avail on 16.04
The header lz4frame.h is missing on liblz4-dev
Style is compatible with the other enable/disable makefile flags.

Signed-off-by: Alexander Gallego gallego.alexx@gmail.com","I think it might be better if the check verified that the expected version or functionality is in place.
I.e., make it include lz4frame.h
Otherwise people will still get the failure and need to figure out that they should disable it manually, which is counterproductive to what configure is supposed to do :)",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,690,2016-06-13T17:56:21Z,2016-08-16T11:58:30Z,2016-08-16T18:17:22Z,CLOSED,False,4,7,2,https://github.com/senior7515,Makefile: add enable/disable lz4 builds,1,[],https://github.com/edenhill/librdkafka/pull/690,https://github.com/senior7515,3,https://github.com/edenhill/librdkafka/pull/690#issuecomment-225675056,"Added ability to add or remove -llz4 builds.

This is to get it to compile on ubuntu 14.04 which doesn't support the latest lz4 lib avail on 16.04
The header lz4frame.h is missing on liblz4-dev
Style is compatible with the other enable/disable makefile flags.

Signed-off-by: Alexander Gallego gallego.alexx@gmail.com","Sure, do you have a similar thing on the configure? let me poke around.",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,690,2016-06-13T17:56:21Z,2016-08-16T11:58:30Z,2016-08-16T18:17:22Z,CLOSED,False,4,7,2,https://github.com/senior7515,Makefile: add enable/disable lz4 builds,1,[],https://github.com/edenhill/librdkafka/pull/690,https://github.com/senior7515,4,https://github.com/edenhill/librdkafka/pull/690#issuecomment-225675261,"Added ability to add or remove -llz4 builds.

This is to get it to compile on ubuntu 14.04 which doesn't support the latest lz4 lib avail on 16.04
The header lz4frame.h is missing on liblz4-dev
Style is compatible with the other enable/disable makefile flags.

Signed-off-by: Alexander Gallego gallego.alexx@gmail.com",mkl_compile_check right?,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,690,2016-06-13T17:56:21Z,2016-08-16T11:58:30Z,2016-08-16T18:17:22Z,CLOSED,False,4,7,2,https://github.com/senior7515,Makefile: add enable/disable lz4 builds,1,[],https://github.com/edenhill/librdkafka/pull/690,https://github.com/senior7515,5,https://github.com/edenhill/librdkafka/pull/690#issuecomment-225677630,"Added ability to add or remove -llz4 builds.

This is to get it to compile on ubuntu 14.04 which doesn't support the latest lz4 lib avail on 16.04
The header lz4frame.h is missing on liblz4-dev
Style is compatible with the other enable/disable makefile flags.

Signed-off-by: Alexander Gallego gallego.alexx@gmail.com",@edenhill hard to check w/out a ubuntu 16.04 box. Will try to get a cloud box to test. I think i patched it.,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,690,2016-06-13T17:56:21Z,2016-08-16T11:58:30Z,2016-08-16T18:17:22Z,CLOSED,False,4,7,2,https://github.com/senior7515,Makefile: add enable/disable lz4 builds,1,[],https://github.com/edenhill/librdkafka/pull/690,https://github.com/senior7515,6,https://github.com/edenhill/librdkafka/pull/690#issuecomment-225689937,"Added ability to add or remove -llz4 builds.

This is to get it to compile on ubuntu 14.04 which doesn't support the latest lz4 lib avail on 16.04
The header lz4frame.h is missing on liblz4-dev
Style is compatible with the other enable/disable makefile flags.

Signed-off-by: Alexander Gallego gallego.alexx@gmail.com","Ok, fixed the llz4 checks.
Tested on ubuntu 14.04 - works - skips lz4
Configuration summary:
  prefix                   /usr/local
  ARCH                     x86_64
  CPU                      generic
  GEN_PKG_CONFIG           y
  ENABLE_DEVEL             n
  ENABLE_VALGRIND          n
  ENABLE_REFCNT_DEBUG      n
  ENABLE_SHAREDPTR_DEBUG   n
  ENABLE_SSL               y
  ENABLE_SASL              y
  MKL_APP_NAME             librdkafka
  MKL_APP_DESC_ONELINE     The Apache Kafka C/C++ library
  MKL_DISTRO               Ubuntu
  CC                       gcc
  CXX                      g++
  LD                       ld
  NM                       nm
  OBJDUMP                  objdump
  STRIP                    strip
  CPPFLAGS                 -g -O2 -fPIC -Wall -Wsign-compare -Wfloat-equal -Wpointer-arith
  PKG_CONFIG               pkg-config
  INSTALL                  install
  LIB_LDFLAGS              -shared -Wl,-soname,$(LIBFILENAME)
  LDFLAG_LINKERSCRIPT      -Wl,--version-script=
  RDKAFKA_VERSION_STR      0.9.1
  MKL_APP_VERSION          0.9.1
  LIBS                     -lpthread -lz   -lcrypto   -lssl -lcrypto   -lsasl2 -lrt
  CFLAGS                        
  CXXFLAGS                 -Wno-non-virtual-dtor
  SYMDUMPER                $(NM) -D
  exec_prefix              /usr/local
  bindir                   /usr/local/bin
  sbindir                  /usr/local/sbin
  libexecdir               /usr/local/libexec
  datadir                  /usr/local/share
  sysconfdir               /usr/local/etc
  sharedstatedir           /usr/local/com
  localstatedir            /usr/local/var
  libdir                   /usr/local/lib
  includedir               /usr/local/include
  infodir                  /usr/local/info
  mandir                   /usr/local/man
Generated config.cache

Now type 'make' to build
(.bolt) agallego@agallego:~/workspace/librdkafka$ lsb_release -a
No LSB modules are available.
Distributor ID: Ubuntu
Description:    Ubuntu 14.04.4 LTS
Release:        14.04
Codename:       trusty


Tested on ubuntu 16.04 - works - includes lz4
Configuration summary:
  prefix                   /usr/local
  ARCH                     x86_64
  CPU                      generic
  GEN_PKG_CONFIG           y
  ENABLE_DEVEL             n
  ENABLE_VALGRIND          n
  ENABLE_REFCNT_DEBUG      n
  ENABLE_SHAREDPTR_DEBUG   n
  ENABLE_SSL               y
  ENABLE_SASL              y
  MKL_APP_NAME             librdkafka
  MKL_APP_DESC_ONELINE     The Apache Kafka C/C++ library
  MKL_DISTRO               Ubuntu
  CC                       gcc
  CXX                      g++
  LD                       ld
  NM                       nm
  OBJDUMP                  objdump
  STRIP                    strip
  CPPFLAGS                 -g -O2 -fPIC -Wall -Wsign-compare -Wfloat-equal -Wpointer-arith
  PKG_CONFIG               pkg-config
  INSTALL                  install
  LIB_LDFLAGS              -shared -Wl,-soname,$(LIBFILENAME)
  LDFLAG_LINKERSCRIPT      -Wl,--version-script=
  RDKAFKA_VERSION_STR      0.9.1
  MKL_APP_VERSION          0.9.1
  LIBS                     -lpthread -lz -lcrypto -llz4 -lssl -lsasl2 -lrt
  CXXFLAGS                 -Wno-non-virtual-dtor
  SYMDUMPER                $(NM) -D
  exec_prefix              /usr/local
  bindir                   /usr/local/bin
  sbindir                  /usr/local/sbin
  libexecdir               /usr/local/libexec
  datadir                  /usr/local/share
  sysconfdir               /usr/local/etc
  sharedstatedir           /usr/local/com
  localstatedir            /usr/local/var
  libdir                   /usr/local/lib
  includedir               /usr/local/include
  infodir                  /usr/local/info
  mandir                   /usr/local/man
Generated config.cache

Now type 'make' to build
agallego@agallego2:~/workspace/librdkafka$ lsb_release -a
No LSB modules are available.
Distributor ID: Ubuntu
Description:    Ubuntu 16.04 LTS
Release:    16.04
Codename:   xenial",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,690,2016-06-13T17:56:21Z,2016-08-16T11:58:30Z,2016-08-16T18:17:22Z,CLOSED,False,4,7,2,https://github.com/senior7515,Makefile: add enable/disable lz4 builds,1,[],https://github.com/edenhill/librdkafka/pull/690,https://github.com/senior7515,7,https://github.com/edenhill/librdkafka/pull/690#issuecomment-240190335,"Added ability to add or remove -llz4 builds.

This is to get it to compile on ubuntu 14.04 which doesn't support the latest lz4 lib avail on 16.04
The header lz4frame.h is missing on liblz4-dev
Style is compatible with the other enable/disable makefile flags.

Signed-off-by: Alexander Gallego gallego.alexx@gmail.com","hi @edenhill is the pkging working w/ ubuntu 14.04 I haven't tested the latest changes. Would you like me to change the patch in a specific way?
Sorry, I was uncertain what the next steps were on the previous comment. Happy to help out.
Alex",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,694,2016-06-16T15:38:48Z,2016-06-18T20:51:01Z,2016-06-18T20:51:01Z,CLOSED,False,3,0,1,https://github.com/JohanAR,Add rd_kafka_query_watermark_offsets call to rd_kafka_topic_leader_query,1,[],https://github.com/edenhill/librdkafka/pull/694,https://github.com/JohanAR,1,https://github.com/edenhill/librdkafka/pull/694,"Query watermark offsets uses rd_kafka_toppar_get2 to create a topic which
in turn calls rd_kafka_topic_new0, which unlike rd_kafka_topic_new does
not call rd_kafka_topic_leader_query.
This had the effect that rd_kafka_toppar_leader would return NULL until
rd_kafka_topic_leader_query was called by rd_kafka_topic_scan_tmr_cb,
which is run every 1000ms by a timer. Because of this query watermark
offsets would occasionally take 1000ms longer to finish, or return
WAIT_COORD error if timeout was shorter than that.","Query watermark offsets uses rd_kafka_toppar_get2 to create a topic which
in turn calls rd_kafka_topic_new0, which unlike rd_kafka_topic_new does
not call rd_kafka_topic_leader_query.
This had the effect that rd_kafka_toppar_leader would return NULL until
rd_kafka_topic_leader_query was called by rd_kafka_topic_scan_tmr_cb,
which is run every 1000ms by a timer. Because of this query watermark
offsets would occasionally take 1000ms longer to finish, or return
WAIT_COORD error if timeout was shorter than that.",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,698,2016-06-20T08:28:37Z,2016-07-05T10:38:02Z,2016-07-05T10:38:02Z,CLOSED,False,69,34,2,https://github.com/jerryyyq,Add a export api: rd_kafka_brokers_are_all_down,1,[],https://github.com/edenhill/librdkafka/pull/698,https://github.com/jerryyyq,1,https://github.com/edenhill/librdkafka/pull/698,The api can help the caller to judge whether refuse client datas,The api can help the caller to judge whether refuse client datas,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,698,2016-06-20T08:28:37Z,2016-07-05T10:38:02Z,2016-07-05T10:38:02Z,CLOSED,False,69,34,2,https://github.com/jerryyyq,Add a export api: rd_kafka_brokers_are_all_down,1,[],https://github.com/edenhill/librdkafka/pull/698,https://github.com/edenhill,2,https://github.com/edenhill/librdkafka/pull/698#issuecomment-230444873,The api can help the caller to judge whether refuse client datas,"Thanks for your effort but I belive this should be handled more generally by events that tells the application when brokers, topics and partitions are available or not. This saves checking these two atomics for each produce() call.",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,705,2016-06-27T23:46:54Z,2016-06-28T08:20:22Z,2016-06-28T08:20:23Z,MERGED,True,1,0,1,https://github.com/ah-,Add rdkafka-dotnet to README,1,[],https://github.com/edenhill/librdkafka/pull/705,https://github.com/ah-,1,https://github.com/edenhill/librdkafka/pull/705,,,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,726,2016-07-08T18:18:58Z,2016-07-08T18:33:18Z,2016-07-08T18:33:18Z,CLOSED,False,3,3,1,https://github.com/ottomata,Correct reference to enable.auto.commit in topic configuration doc fo,1,[],https://github.com/edenhill/librdkafka/pull/726,https://github.com/ottomata,1,https://github.com/edenhill/librdkafka/pull/726,r auto.commit.enable,r auto.commit.enable,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,726,2016-07-08T18:18:58Z,2016-07-08T18:33:18Z,2016-07-08T18:33:18Z,CLOSED,False,3,3,1,https://github.com/ottomata,Correct reference to enable.auto.commit in topic configuration doc fo,1,[],https://github.com/edenhill/librdkafka/pull/726,https://github.com/edenhill,2,https://github.com/edenhill/librdkafka/pull/726#issuecomment-231434620,r auto.commit.enable,"That file is automatically generated, you need to edit rdkafka_conf.c and run make",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,727,2016-07-08T18:34:37Z,2016-10-01T17:50:04Z,2016-10-01T17:50:08Z,MERGED,True,1,1,1,https://github.com/ottomata,Correct reference to global enable.auto.commit in rdkafka_conf,1,[],https://github.com/edenhill/librdkafka/pull/727,https://github.com/ottomata,1,https://github.com/edenhill/librdkafka/pull/727,make changed a bunch of stuff in CONFIGURATION.md other than this one thing.  I'll let you commit a new CONFIGURATION.md at your leisure. :),make changed a bunch of stuff in CONFIGURATION.md other than this one thing.  I'll let you commit a new CONFIGURATION.md at your leisure. :),True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,727,2016-07-08T18:34:37Z,2016-10-01T17:50:04Z,2016-10-01T17:50:08Z,MERGED,True,1,1,1,https://github.com/ottomata,Correct reference to global enable.auto.commit in rdkafka_conf,1,[],https://github.com/edenhill/librdkafka/pull/727,https://github.com/edenhill,2,https://github.com/edenhill/librdkafka/pull/727#issuecomment-250926442,make changed a bunch of stuff in CONFIGURATION.md other than this one thing.  I'll let you commit a new CONFIGURATION.md at your leisure. :),Thanks! :),True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,741,2016-07-27T17:31:29Z,2016-11-14T21:49:49Z,2016-11-14T21:50:08Z,MERGED,True,49,7,4,https://github.com/lambdaknight,"Add support for setting ""consume_cb"" in the C++ wrapper of librdkafka",1,[],https://github.com/edenhill/librdkafka/pull/741,https://github.com/lambdaknight,1,https://github.com/edenhill/librdkafka/pull/741,,,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,741,2016-07-27T17:31:29Z,2016-11-14T21:49:49Z,2016-11-14T21:50:08Z,MERGED,True,49,7,4,https://github.com/lambdaknight,"Add support for setting ""consume_cb"" in the C++ wrapper of librdkafka",1,[],https://github.com/edenhill/librdkafka/pull/741,https://github.com/lambdaknight,2,https://github.com/edenhill/librdkafka/pull/741#issuecomment-235661229,,"I opened up issue #740 and found I had a little bit of time, so I banged out this patch really quick. It worked when I tested it with the rdkafka_consumer_example_cpp example (which I appropriately modified), but I'm not super familiar with the library yet, so I don't know what I may be missing. I don't have a lot of extra time to devote to it, so if my patch is complete, yay. But if not, feel free to use whatever parts of my patch are useful.",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,741,2016-07-27T17:31:29Z,2016-11-14T21:49:49Z,2016-11-14T21:50:08Z,MERGED,True,49,7,4,https://github.com/lambdaknight,"Add support for setting ""consume_cb"" in the C++ wrapper of librdkafka",1,[],https://github.com/edenhill/librdkafka/pull/741,https://github.com/edenhill,3,https://github.com/edenhill/librdkafka/pull/741#issuecomment-260473995,,"Thanks!
(sorry for the loooong delay)",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,742,2016-07-27T19:37:58Z,2016-10-24T18:48:10Z,2016-10-26T18:38:07Z,MERGED,True,0,3,1,https://github.com/yunjing,Only store initial offset when fetcher was first started,1,"['bug', 'consumer']",https://github.com/edenhill/librdkafka/pull/742,https://github.com/yunjing,1,https://github.com/edenhill/librdkafka/pull/742,There is a race condition for offset store. rd_kafka_cgrp_partitions_fetch_start should not overwrite the stored offset except when it was first started.,There is a race condition for offset store. rd_kafka_cgrp_partitions_fetch_start should not overwrite the stored offset except when it was first started.,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,742,2016-07-27T19:37:58Z,2016-10-24T18:48:10Z,2016-10-26T18:38:07Z,MERGED,True,0,3,1,https://github.com/yunjing,Only store initial offset when fetcher was first started,1,"['bug', 'consumer']",https://github.com/edenhill/librdkafka/pull/742,https://github.com/edenhill,2,https://github.com/edenhill/librdkafka/pull/742#issuecomment-250926990,There is a race condition for offset store. rd_kafka_cgrp_partitions_fetch_start should not overwrite the stored offset except when it was first started.,"Hi @yunjing,
can you elaborate on this fix?
You move offset_store0() down below the async op_fetch_start() call, but rktpar->offset doesnt change  during that time so why do you see the need for the move?
Also op_fetch_start() or op_seek() are asynchronous so  nothing will really have happened by the time they return",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,742,2016-07-27T19:37:58Z,2016-10-24T18:48:10Z,2016-10-26T18:38:07Z,MERGED,True,0,3,1,https://github.com/yunjing,Only store initial offset when fetcher was first started,1,"['bug', 'consumer']",https://github.com/edenhill/librdkafka/pull/742,https://github.com/yunjing,3,https://github.com/edenhill/librdkafka/pull/742#issuecomment-250945190,There is a race condition for offset store. rd_kafka_cgrp_partitions_fetch_start should not overwrite the stored offset except when it was first started.,"Hi @edenhill,
The problem I saw was that rd_kafka_cgrp_partitions_fetch_start may be invoked after the fetcher has started (as you commented in the else branch). As a result, without the fix, there is a possibility that the following scenario could happen:

A topic has three messages in the broker (index 0, 1, 2)
A consumer fetched all three from that topic
The consumer stores the current offset 3
The stored offset is overridden with 0 asynchronously

Because it only affects the stored offset, if there are new incoming messages to this topic, the consumer continues with the right offset and eventually stored the correct, current offset. However, if you kill the consumer and restart before any new messages arrived, it will incorrectly re-consume the three messages again.",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,742,2016-07-27T19:37:58Z,2016-10-24T18:48:10Z,2016-10-26T18:38:07Z,MERGED,True,0,3,1,https://github.com/yunjing,Only store initial offset when fetcher was first started,1,"['bug', 'consumer']",https://github.com/edenhill/librdkafka/pull/742,https://github.com/meox,4,https://github.com/edenhill/librdkafka/pull/742#issuecomment-254016609,There is a race condition for offset store. rd_kafka_cgrp_partitions_fetch_start should not overwrite the stored offset except when it was first started.,any update on this issue?,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,742,2016-07-27T19:37:58Z,2016-10-24T18:48:10Z,2016-10-26T18:38:07Z,MERGED,True,0,3,1,https://github.com/yunjing,Only store initial offset when fetcher was first started,1,"['bug', 'consumer']",https://github.com/edenhill/librdkafka/pull/742,https://github.com/yunjing,5,https://github.com/edenhill/librdkafka/pull/742#issuecomment-254951746,There is a race condition for offset store. rd_kafka_cgrp_partitions_fetch_start should not overwrite the stored offset except when it was first started.,@meox would love to have this reviewed.,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,742,2016-07-27T19:37:58Z,2016-10-24T18:48:10Z,2016-10-26T18:38:07Z,MERGED,True,0,3,1,https://github.com/yunjing,Only store initial offset when fetcher was first started,1,"['bug', 'consumer']",https://github.com/edenhill/librdkafka/pull/742,https://github.com/edenhill,6,https://github.com/edenhill/librdkafka/pull/742#issuecomment-255608086,There is a race condition for offset store. rd_kafka_cgrp_partitions_fetch_start should not overwrite the stored offset except when it was first started.,"Huhm, I dont think it should store the offset at all in that situation.
Offsets should only be stored (for future commits) when a message is delivered to the application (if enable.auto.offset.store=true) or explicitly by the application itself.
Doing it here, and not even checking the conf property, might very well commit offsets prematurely.",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,742,2016-07-27T19:37:58Z,2016-10-24T18:48:10Z,2016-10-26T18:38:07Z,MERGED,True,0,3,1,https://github.com/yunjing,Only store initial offset when fetcher was first started,1,"['bug', 'consumer']",https://github.com/edenhill/librdkafka/pull/742,https://github.com/yunjing,7,https://github.com/edenhill/librdkafka/pull/742#issuecomment-255658249,There is a race condition for offset store. rd_kafka_cgrp_partitions_fetch_start should not overwrite the stored offset except when it was first started.,"I agree with the assessment. Before we come up with a permanent solution, do you think this PR is a strict improvement of the existing logic?",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,742,2016-07-27T19:37:58Z,2016-10-24T18:48:10Z,2016-10-26T18:38:07Z,MERGED,True,0,3,1,https://github.com/yunjing,Only store initial offset when fetcher was first started,1,"['bug', 'consumer']",https://github.com/edenhill/librdkafka/pull/742,https://github.com/edenhill,8,https://github.com/edenhill/librdkafka/pull/742#issuecomment-255668252,There is a race condition for offset store. rd_kafka_cgrp_partitions_fetch_start should not overwrite the stored offset except when it was first started.,I dont think we want to keep the store0() at all.,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,742,2016-07-27T19:37:58Z,2016-10-24T18:48:10Z,2016-10-26T18:38:07Z,MERGED,True,0,3,1,https://github.com/yunjing,Only store initial offset when fetcher was first started,1,"['bug', 'consumer']",https://github.com/edenhill/librdkafka/pull/742,https://github.com/yunjing,9,https://github.com/edenhill/librdkafka/pull/742#issuecomment-255668801,There is a race condition for offset store. rd_kafka_cgrp_partitions_fetch_start should not overwrite the stored offset except when it was first started.,"Okay, I see. I am not that familiar with the code, which is why I moved the two lines. So removing them won't have other side effects?",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,742,2016-07-27T19:37:58Z,2016-10-24T18:48:10Z,2016-10-26T18:38:07Z,MERGED,True,0,3,1,https://github.com/yunjing,Only store initial offset when fetcher was first started,1,"['bug', 'consumer']",https://github.com/edenhill/librdkafka/pull/742,https://github.com/edenhill,10,https://github.com/edenhill/librdkafka/pull/742#issuecomment-255669286,There is a race condition for offset store. rd_kafka_cgrp_partitions_fetch_start should not overwrite the stored offset except when it was first started.,"I did some quick analysis of the code yesterday and couldnt find a reason for why that store would be there, so I'm sure it should be removed.",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,742,2016-07-27T19:37:58Z,2016-10-24T18:48:10Z,2016-10-26T18:38:07Z,MERGED,True,0,3,1,https://github.com/yunjing,Only store initial offset when fetcher was first started,1,"['bug', 'consumer']",https://github.com/edenhill/librdkafka/pull/742,https://github.com/yunjing,11,https://github.com/edenhill/librdkafka/pull/742#issuecomment-255804703,There is a race condition for offset store. rd_kafka_cgrp_partitions_fetch_start should not overwrite the stored offset except when it was first started.,OK. I updated the PR.,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,742,2016-07-27T19:37:58Z,2016-10-24T18:48:10Z,2016-10-26T18:38:07Z,MERGED,True,0,3,1,https://github.com/yunjing,Only store initial offset when fetcher was first started,1,"['bug', 'consumer']",https://github.com/edenhill/librdkafka/pull/742,https://github.com/edenhill,12,https://github.com/edenhill/librdkafka/pull/742#issuecomment-255830132,There is a race condition for offset store. rd_kafka_cgrp_partitions_fetch_start should not overwrite the stored offset except when it was first started.,Thanks for locating this bug!,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,753,2016-08-10T19:27:11Z,2016-08-10T19:29:35Z,2016-08-10T19:33:55Z,CLOSED,False,1,1,1,https://github.com/QuintenJohnson,silence warning about hidden constructor,1,[],https://github.com/edenhill/librdkafka/pull/753,https://github.com/QuintenJohnson,1,https://github.com/edenhill/librdkafka/pull/753,"Compiling with the rdkafka.h header in my environment gives the following warning:
/rdkafka.h:2247:34: warning: rd_kafka_resp_err_t rd_kafka_metadata(rd_kafka_t_, int, rd_kafka_topic_t_, const rd_kafka_metadata**, int) hides constructor for struct rd_kafka_metadata [-Wshadow]
int timeout_ms);
The struct in question is typedef'd directly above this, so we should be able to safely use it's type, rd_kafka_metadata_t, instead.","Compiling with the rdkafka.h header in my environment gives the following warning:
/rdkafka.h:2247:34: warning: rd_kafka_resp_err_t rd_kafka_metadata(rd_kafka_t_, int, rd_kafka_topic_t_, const rd_kafka_metadata**, int) hides constructor for struct rd_kafka_metadata [-Wshadow]
int timeout_ms);
The struct in question is typedef'd directly above this, so we should be able to safely use it's type, rd_kafka_metadata_t, instead.",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,753,2016-08-10T19:27:11Z,2016-08-10T19:29:35Z,2016-08-10T19:33:55Z,CLOSED,False,1,1,1,https://github.com/QuintenJohnson,silence warning about hidden constructor,1,[],https://github.com/edenhill/librdkafka/pull/753,https://github.com/QuintenJohnson,2,https://github.com/edenhill/librdkafka/pull/753#issuecomment-238977565,"Compiling with the rdkafka.h header in my environment gives the following warning:
/rdkafka.h:2247:34: warning: rd_kafka_resp_err_t rd_kafka_metadata(rd_kafka_t_, int, rd_kafka_topic_t_, const rd_kafka_metadata**, int) hides constructor for struct rd_kafka_metadata [-Wshadow]
int timeout_ms);
The struct in question is typedef'd directly above this, so we should be able to safely use it's type, rd_kafka_metadata_t, instead.","I should have rebuilt, which shows that this actually doesn't fix the issue.",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,753,2016-08-10T19:27:11Z,2016-08-10T19:29:35Z,2016-08-10T19:33:55Z,CLOSED,False,1,1,1,https://github.com/QuintenJohnson,silence warning about hidden constructor,1,[],https://github.com/edenhill/librdkafka/pull/753,https://github.com/QuintenJohnson,3,https://github.com/edenhill/librdkafka/pull/753#issuecomment-238978595,"Compiling with the rdkafka.h header in my environment gives the following warning:
/rdkafka.h:2247:34: warning: rd_kafka_resp_err_t rd_kafka_metadata(rd_kafka_t_, int, rd_kafka_topic_t_, const rd_kafka_metadata**, int) hides constructor for struct rd_kafka_metadata [-Wshadow]
int timeout_ms);
The struct in question is typedef'd directly above this, so we should be able to safely use it's type, rd_kafka_metadata_t, instead.",It looks like the function declaration of the same name is shadowing the struct.,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,769,2016-08-29T17:04:53Z,2016-08-29T21:20:50Z,2017-04-22T11:13:42Z,MERGED,True,40,23,7,https://github.com/PKRoma,Fix tiny WIN32 problems building 32-bit debug and release projects.,1,[],https://github.com/edenhill/librdkafka/pull/769,https://github.com/PKRoma,1,https://github.com/edenhill/librdkafka/pull/769,"Thanks for a great product.
In building the WIN32 debug and release versions of all projects, a few inconsistencies were found (library paths and libraries missing, VS2015 toolset being referenced, etc.). These have all been fixed and now all 32-bit projects build. The resultant changes to the .sln and .vcxproj files are in the pull request.
Also, the README.win32 has been updated to state that Microsoft Visual Studio 2013 should be used, and that the latest v1.0.2 version of OpenSSL should be used, because the actual latest version v1.1.0 has a changed API and this breaks compilation.","Thanks for a great product.
In building the WIN32 debug and release versions of all projects, a few inconsistencies were found (library paths and libraries missing, VS2015 toolset being referenced, etc.). These have all been fixed and now all 32-bit projects build. The resultant changes to the .sln and .vcxproj files are in the pull request.
Also, the README.win32 has been updated to state that Microsoft Visual Studio 2013 should be used, and that the latest v1.0.2 version of OpenSSL should be used, because the actual latest version v1.1.0 has a changed API and this breaks compilation.",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,769,2016-08-29T17:04:53Z,2016-08-29T21:20:50Z,2017-04-22T11:13:42Z,MERGED,True,40,23,7,https://github.com/PKRoma,Fix tiny WIN32 problems building 32-bit debug and release projects.,1,[],https://github.com/edenhill/librdkafka/pull/769,https://github.com/edenhill,2,https://github.com/edenhill/librdkafka/pull/769#issuecomment-243260278,"Thanks for a great product.
In building the WIN32 debug and release versions of all projects, a few inconsistencies were found (library paths and libraries missing, VS2015 toolset being referenced, etc.). These have all been fixed and now all 32-bit projects build. The resultant changes to the .sln and .vcxproj files are in the pull request.
Also, the README.win32 has been updated to state that Microsoft Visual Studio 2013 should be used, and that the latest v1.0.2 version of OpenSSL should be used, because the actual latest version v1.1.0 has a changed API and this breaks compilation.",Thanks a bunch!,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,770,2016-08-30T00:26:06Z,2016-09-27T22:44:35Z,2017-04-22T11:12:50Z,CLOSED,False,1,1,1,https://github.com/PKRoma,Fix 0025 test to run as RD_KAFKA_PRODUCER,1,[],https://github.com/edenhill/librdkafka/pull/770,https://github.com/PKRoma,1,https://github.com/edenhill/librdkafka/pull/770,"Running test 0025-timers.c as RD_KAFKA_CONSUMER causes the rk_curr_msgs.lock to not be initialised in rd_kafka_new before an attempt is made to lock it in rd_kafka_curr_msgs_get, while running the test as RD_KAFKA_PRODUCER does initialise the lock and the test succeeds.
While this bugfix does fix the symptom, it is not clear that the test should be run as an RD_KAFKA_PRODUCER. Please confirm that this should be the case, and that testing timers on an RD_KAFKA_CONSUMER does not make sense.
This bugfix also begs the question as to why the tests are succeeding on Linux.","Running test 0025-timers.c as RD_KAFKA_CONSUMER causes the rk_curr_msgs.lock to not be initialised in rd_kafka_new before an attempt is made to lock it in rd_kafka_curr_msgs_get, while running the test as RD_KAFKA_PRODUCER does initialise the lock and the test succeeds.
While this bugfix does fix the symptom, it is not clear that the test should be run as an RD_KAFKA_PRODUCER. Please confirm that this should be the case, and that testing timers on an RD_KAFKA_CONSUMER does not make sense.
This bugfix also begs the question as to why the tests are succeeding on Linux.",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,789,2016-09-09T14:27:56Z,2016-09-22T19:13:00Z,2016-09-22T20:40:16Z,MERGED,True,2,0,2,https://github.com/julien-lecomte,Examples require getopt header file to avoid 'optind' undeclared error,1,[],https://github.com/edenhill/librdkafka/pull/789,https://github.com/julien-lecomte,1,https://github.com/edenhill/librdkafka/pull/789,"Configuration on Linux 3.19.0-49-generic #55~14.04.1-Ubuntu x86_64 fails on C files in the examples directory because of missing getopt.h include.
CFLAGS=""--std=c99 -g -O2"" CXXFLAGS=""--std=c++11 -g -O2"" ./configure --no-download --disable-ssl --disable-sasl --prefix=/usr/local
(...)
make[1]: Entering directory `/home/julien/dev/github/librdkafka/examples'
gcc -g -O2 -fPIC -Wall -Wsign-compare -Wfloat-equal -Wpointer-arith --std=c99 -g -O2     -I../src rdkafka_example.c -o rdkafka_example  \
                ../src/librdkafka.a -lpthread -lz   -lcrypto   -lrt
rdkafka_example.c: In function main:
rdkafka_example.c:303:2: warning: implicit declaration of function getopt [-Wimplicit-function-declaration]
  while ((opt = getopt(argc, argv, ""PCLt:p:b:z:qd:o:eX:As:"")) != -1) {
  ^
rdkafka_example.c:311:12: error: optarg undeclared (first use in this function)
    topic = optarg;
            ^
rdkafka_example.c:311:12: note: each undeclared identifier is reported only once for each function it appears in
rdkafka_example.c:478:6: error: optind undeclared (first use in this function)
  if (optind != argc || (mode != 'L' && !topic)) {
      ^
make[1]: *** [rdkafka_example] Error 1","Configuration on Linux 3.19.0-49-generic #55~14.04.1-Ubuntu x86_64 fails on C files in the examples directory because of missing getopt.h include.
CFLAGS=""--std=c99 -g -O2"" CXXFLAGS=""--std=c++11 -g -O2"" ./configure --no-download --disable-ssl --disable-sasl --prefix=/usr/local
(...)
make[1]: Entering directory `/home/julien/dev/github/librdkafka/examples'
gcc -g -O2 -fPIC -Wall -Wsign-compare -Wfloat-equal -Wpointer-arith --std=c99 -g -O2     -I../src rdkafka_example.c -o rdkafka_example  \
                ../src/librdkafka.a -lpthread -lz   -lcrypto   -lrt
rdkafka_example.c: In function main:
rdkafka_example.c:303:2: warning: implicit declaration of function getopt [-Wimplicit-function-declaration]
  while ((opt = getopt(argc, argv, ""PCLt:p:b:z:qd:o:eX:As:"")) != -1) {
  ^
rdkafka_example.c:311:12: error: optarg undeclared (first use in this function)
    topic = optarg;
            ^
rdkafka_example.c:311:12: note: each undeclared identifier is reported only once for each function it appears in
rdkafka_example.c:478:6: error: optind undeclared (first use in this function)
  if (optind != argc || (mode != 'L' && !topic)) {
      ^
make[1]: *** [rdkafka_example] Error 1",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,789,2016-09-09T14:27:56Z,2016-09-22T19:13:00Z,2016-09-22T20:40:16Z,MERGED,True,2,0,2,https://github.com/julien-lecomte,Examples require getopt header file to avoid 'optind' undeclared error,1,[],https://github.com/edenhill/librdkafka/pull/789,https://github.com/edenhill,2,https://github.com/edenhill/librdkafka/pull/789#issuecomment-249000229,"Configuration on Linux 3.19.0-49-generic #55~14.04.1-Ubuntu x86_64 fails on C files in the examples directory because of missing getopt.h include.
CFLAGS=""--std=c99 -g -O2"" CXXFLAGS=""--std=c++11 -g -O2"" ./configure --no-download --disable-ssl --disable-sasl --prefix=/usr/local
(...)
make[1]: Entering directory `/home/julien/dev/github/librdkafka/examples'
gcc -g -O2 -fPIC -Wall -Wsign-compare -Wfloat-equal -Wpointer-arith --std=c99 -g -O2     -I../src rdkafka_example.c -o rdkafka_example  \
                ../src/librdkafka.a -lpthread -lz   -lcrypto   -lrt
rdkafka_example.c: In function main:
rdkafka_example.c:303:2: warning: implicit declaration of function getopt [-Wimplicit-function-declaration]
  while ((opt = getopt(argc, argv, ""PCLt:p:b:z:qd:o:eX:As:"")) != -1) {
  ^
rdkafka_example.c:311:12: error: optarg undeclared (first use in this function)
    topic = optarg;
            ^
rdkafka_example.c:311:12: note: each undeclared identifier is reported only once for each function it appears in
rdkafka_example.c:478:6: error: optind undeclared (first use in this function)
  if (optind != argc || (mode != 'L' && !topic)) {
      ^
make[1]: *** [rdkafka_example] Error 1",Thanks!,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,809,2016-09-28T17:27:52Z,2016-10-01T17:49:02Z,2017-04-22T11:12:35Z,MERGED,True,1,0,1,https://github.com/PKRoma,Add missing return in rd_kafka_curr_msgs_get if RD_KAFKA_PRODUCER.,1,[],https://github.com/edenhill/librdkafka/pull/809,https://github.com/PKRoma,1,https://github.com/edenhill/librdkafka/pull/809,"There is a pattern in the rd_kafka_curr_msgs_* functions in rdkafka_int.h to avoid referencing the rk_curr_msgs structure if the application is not a RD_KAFKA_PRODUCER.
However, for the rd_kafka_curr_msgs_get function, while there exists a check for RD_KAFKA_PRODUCER and return values are set to 0, there is no subsequent return statement and the function falls through to manipulating the rk_curr_msgs structure even when the application is not a RD_KAFKA_PRODUCER, causing exceptions on trying to lock the structure on WIN32.
This patch adds the missing return statement.","There is a pattern in the rd_kafka_curr_msgs_* functions in rdkafka_int.h to avoid referencing the rk_curr_msgs structure if the application is not a RD_KAFKA_PRODUCER.
However, for the rd_kafka_curr_msgs_get function, while there exists a check for RD_KAFKA_PRODUCER and return values are set to 0, there is no subsequent return statement and the function falls through to manipulating the rk_curr_msgs structure even when the application is not a RD_KAFKA_PRODUCER, causing exceptions on trying to lock the structure on WIN32.
This patch adds the missing return statement.",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,809,2016-09-28T17:27:52Z,2016-10-01T17:49:02Z,2017-04-22T11:12:35Z,MERGED,True,1,0,1,https://github.com/PKRoma,Add missing return in rd_kafka_curr_msgs_get if RD_KAFKA_PRODUCER.,1,[],https://github.com/edenhill/librdkafka/pull/809,https://github.com/edenhill,2,https://github.com/edenhill/librdkafka/pull/809#issuecomment-250926392,"There is a pattern in the rd_kafka_curr_msgs_* functions in rdkafka_int.h to avoid referencing the rk_curr_msgs structure if the application is not a RD_KAFKA_PRODUCER.
However, for the rd_kafka_curr_msgs_get function, while there exists a check for RD_KAFKA_PRODUCER and return values are set to 0, there is no subsequent return statement and the function falls through to manipulating the rk_curr_msgs structure even when the application is not a RD_KAFKA_PRODUCER, causing exceptions on trying to lock the structure on WIN32.
This patch adds the missing return statement.",Thank you for finding and fixing this issue!,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,810,2016-09-29T02:28:03Z,2016-09-29T07:30:51Z,2016-09-29T07:30:59Z,MERGED,True,1,2,1,https://github.com/rustycodemonkey,Change the order of parameter explanation in intro for rd_kafka_produce fn,1,[],https://github.com/edenhill/librdkafka/pull/810,https://github.com/rustycodemonkey,1,https://github.com/edenhill/librdkafka/pull/810,"I just changed the order of your explanation in the introduction file so that it reflect the actual function.
Sorry for being pedantic.","I just changed the order of your explanation in the introduction file so that it reflect the actual function.
Sorry for being pedantic.",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,810,2016-09-29T02:28:03Z,2016-09-29T07:30:51Z,2016-09-29T07:30:59Z,MERGED,True,1,2,1,https://github.com/rustycodemonkey,Change the order of parameter explanation in intro for rd_kafka_produce fn,1,[],https://github.com/edenhill/librdkafka/pull/810,https://github.com/edenhill,2,https://github.com/edenhill/librdkafka/pull/810#issuecomment-250392011,"I just changed the order of your explanation in the introduction file so that it reflect the actual function.
Sorry for being pedantic.",Thanks :),True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,812,2016-09-29T09:06:15Z,2016-09-29T09:10:08Z,2016-09-29T09:10:08Z,CLOSED,False,6,5,3,https://github.com/yanzhiqiang,slove consumer hang when it close,1,[],https://github.com/edenhill/librdkafka/pull/812,https://github.com/yanzhiqiang,1,https://github.com/edenhill/librdkafka/pull/812,"1I  just changed the rdkafka close broker thread so that  broker thread can be closed.
2I encounted one problem consumer close will be hanged because rd_kafka_broker_terminating(rb) not true","1I  just changed the rdkafka close broker thread so that  broker thread can be closed.
2I encounted one problem consumer close will be hanged because rd_kafka_broker_terminating(rb) not true",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,824,2016-10-07T02:26:32Z,2016-10-07T05:47:33Z,2016-10-08T19:33:41Z,MERGED,True,1,1,1,https://github.com/ewencp,Remove logo setting from Doxygen so recent versions won't complain about it being missing.,1,[],https://github.com/edenhill/librdkafka/pull/824,https://github.com/ewencp,1,https://github.com/edenhill/librdkafka/pull/824,,,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,824,2016-10-07T02:26:32Z,2016-10-07T05:47:33Z,2016-10-08T19:33:41Z,MERGED,True,1,1,1,https://github.com/ewencp,Remove logo setting from Doxygen so recent versions won't complain about it being missing.,1,[],https://github.com/edenhill/librdkafka/pull/824,https://github.com/ewencp,2,https://github.com/edenhill/librdkafka/pull/824#issuecomment-252138913,,@edenhill Not sure if this is the ideal solution or not. Apache can be picky about the use of project logos so I didn't go with adding the file in this case and instead just removed it so we'll be logo-free for now.,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,833,2016-10-12T05:30:54Z,2016-10-13T10:15:15Z,2016-10-18T03:05:05Z,MERGED,True,14,0,1,https://github.com/ciasom,add 4 more steps for generating client java key and certificate,1,[],https://github.com/edenhill/librdkafka/pull/833,https://github.com/ciasom,1,https://github.com/edenhill/librdkafka/pull/833,#823,#823,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,833,2016-10-12T05:30:54Z,2016-10-13T10:15:15Z,2016-10-18T03:05:05Z,MERGED,True,14,0,1,https://github.com/ciasom,add 4 more steps for generating client java key and certificate,1,[],https://github.com/edenhill/librdkafka/pull/833,https://github.com/edenhill,2,https://github.com/edenhill/librdkafka/pull/833#issuecomment-253473372,#823,Thank you!,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,836,2016-10-13T12:38:54Z,2016-10-13T13:26:50Z,2016-10-13T13:32:59Z,MERGED,True,14,9,2,https://github.com/julien-lecomte,rd_kafka_new: delete conf only on success,1,[],https://github.com/edenhill/librdkafka/pull/836,https://github.com/julien-lecomte,1,https://github.com/edenhill/librdkafka/pull/836,"This patch allows the user to be sure the rd_kafka_conf_t* is only deleted on success when calling rd_kafka_new.
It works by first duplicating the user conf if supplied, or creating a new conf. This means that this duplicate conf may be deleted from the same places in rd_kafka_new as before.
At the end of rd_kafka_new: delete the user supplied conf to keep current behavior.
If we were to change behavior and have user create and delete conf, it's as easy as just deleting the last lines of function rd_kafka_new.","This patch allows the user to be sure the rd_kafka_conf_t* is only deleted on success when calling rd_kafka_new.
It works by first duplicating the user conf if supplied, or creating a new conf. This means that this duplicate conf may be deleted from the same places in rd_kafka_new as before.
At the end of rd_kafka_new: delete the user supplied conf to keep current behavior.
If we were to change behavior and have user create and delete conf, it's as easy as just deleting the last lines of function rd_kafka_new.",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,836,2016-10-13T12:38:54Z,2016-10-13T13:26:50Z,2016-10-13T13:32:59Z,MERGED,True,14,9,2,https://github.com/julien-lecomte,rd_kafka_new: delete conf only on success,1,[],https://github.com/edenhill/librdkafka/pull/836,https://github.com/edenhill,2,https://github.com/edenhill/librdkafka/pull/836#issuecomment-253512436,"This patch allows the user to be sure the rd_kafka_conf_t* is only deleted on success when calling rd_kafka_new.
It works by first duplicating the user conf if supplied, or creating a new conf. This means that this duplicate conf may be deleted from the same places in rd_kafka_new as before.
At the end of rd_kafka_new: delete the user supplied conf to keep current behavior.
If we were to change behavior and have user create and delete conf, it's as easy as just deleting the last lines of function rd_kafka_new.",Thanks!,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,840,2016-10-17T15:17:12Z,2016-10-31T20:44:19Z,2016-10-31T20:44:19Z,CLOSED,False,59,59,3,https://github.com/hustlijian,add snappy function header to be compatible with google snappy,3,[],https://github.com/edenhill/librdkafka/pull/840,https://github.com/hustlijian,1,https://github.com/edenhill/librdkafka/pull/840,"sed -i 's/snappy_init_env/rdkafka_snappy_init_env/g' snappy.h  snappy.c rdkafka_broker.c
     sed -i 's/snappy_free_env/rdkafka_snappy_free_env/g' rdkafka_broker.c snappy.h snappy.c
     sed -i 's/snappy_uncompress/rdkafka_snappy_uncompress/g' rdkafka_broker.c snappy.h snappy.c
     sed -i 's/snappy_compress/rdkafka_snappy_compress/g' rdkafka_broker.c snappy.h snappy.c
     sed -i 's/snappy_max_compressed_length/rdkafka_snappy_max_compressed_length/g' rdkafka_broker.c snappy.h snappy.c","sed -i 's/snappy_init_env/rdkafka_snappy_init_env/g' snappy.h  snappy.c rdkafka_broker.c
     sed -i 's/snappy_free_env/rdkafka_snappy_free_env/g' rdkafka_broker.c snappy.h snappy.c
     sed -i 's/snappy_uncompress/rdkafka_snappy_uncompress/g' rdkafka_broker.c snappy.h snappy.c
     sed -i 's/snappy_compress/rdkafka_snappy_compress/g' rdkafka_broker.c snappy.h snappy.c
     sed -i 's/snappy_max_compressed_length/rdkafka_snappy_max_compressed_length/g' rdkafka_broker.c snappy.h snappy.c",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,850,2016-10-21T11:27:10Z,2016-10-25T19:06:13Z,2016-10-25T19:06:13Z,CLOSED,False,2,2,1,https://github.com/wukezhan,fix crash in rd_kafka_new() when conf is NULL,1,[],https://github.com/edenhill/librdkafka/pull/850,https://github.com/wukezhan,1,https://github.com/edenhill/librdkafka/pull/850,"when we use the latest code of https://github.com/edenhill/librdkafka and https://pecl.php.net/package/rdkafka, the php core dumped, so i fixed it.
please review if it's ok, thanks:)
the php code:
$rk = new RdKafka\Producer();
$rk->setLogLevel(LOG_DEBUG);
the core dump backtrace:
Program received signal SIGSEGV, Segmentation fault.
rd_kafka_new (type=RD_KAFKA_CONSUMER, conf=0x0, errstr=0x7fffffffa9a0 ""\316\006"", errstr_size=512) at rdkafka.c:1113
1113            if (!conf->socket_cb) {
Missing separate debuginfos, use: debuginfo-install audit-libs-2.3.7-5.el6.x86_64 cyrus-sasl-lib-2.1.23-15.el6_6.2.x86_64 freetype-2.3.11-15.el6_6.1.x86_64 glibc-2.12-1.149.el6_6.9.x86_64 keyutils-libs-1.4-5.el6.x86_64 krb5-libs-1.10.3-37.el6_6.x86_64 libcom_err-1.41.12-21.el6.x86_64 libcurl-7.19.7-46.el6.x86_64 libgcc-4.4.7-11.el6.x86_64 libgomp-4.4.7-11.el6.x86_64 libidn-1.18-2.el6.x86_64 libjpeg-turbo-1.2.1-3.el6_5.x86_64 libselinux-2.0.94-5.8.el6.x86_64 libssh2-1.4.2-1.el6_6.1.x86_64 libxml2-2.7.6-17.el6_6.1.x86_64 libyaml-0.1.6-1.el6.x86_64 nspr-4.10.8-1.el6_6.x86_64 nss-3.19.1-3.el6_6.x86_64 nss-softokn-freebl-3.14.3-22.el6_6.x86_64 nss-util-3.19.1-1.el6_6.x86_64 openldap-2.4.39-8.el6.x86_64 openssl-1.0.1e-42.el6.x86_64 pam-1.1.1-20.el6.x86_64 postgresql-libs-8.4.20-3.el6_6.x86_64 sqlite-3.6.20-1.el6.x86_64 zlib-1.2.3-29.el6.x86_64
(gdb) backtrace
#0  rd_kafka_new (type=RD_KAFKA_CONSUMER, conf=0x0, errstr=0x7fffffffa9a0 ""\316\006"", errstr_size=512) at rdkafka.c:1113
#1  0x00007fffe802ab6d in kafka_init (this_ptr=0x7ffff1e130f0, type=RD_KAFKA_CONSUMER, zconf=<value optimized out>)
    at /tmp/source/rdkafka-2.0.0/rdkafka.c:98
#2  0x00007fffe802acbc in zim_RdKafka__Consumer___construct (execute_data=0x7ffff1e130d0, return_value=<value optimized out>)
    at /tmp/source/rdkafka-2.0.0/rdkafka.c:169
#3  0x0000000000785628 in ZEND_DO_FCALL_SPEC_HANDLER (execute_data=0x7ffff1e13030)
    at /tmp/opdir/php-7.0.11/Zend/zend_vm_execute.h:842
#4  0x000000000074d420 in execute_ex (ex=<value optimized out>) at /tmp/opdir/php-7.0.11/Zend/zend_vm_execute.h:417
#5  0x00000000007a06cb in zend_execute (op_array=0x7ffff1e7e000, return_value=<value optimized out>)
    at /tmp/opdir/php-7.0.11/Zend/zend_vm_execute.h:458
#6  0x000000000070dde3 in zend_execute_scripts (type=8, retval=0x0, file_count=3)
    at /tmp/opdir/php-7.0.11/Zend/zend.c:1427
#7  0x00000000006b0290 in php_execute_script (primary_file=0x7fffffffe120) at /tmp/opdir/php-7.0.11/main/main.c:2494
#8  0x00000000007a49fa in do_cli (argc=2, argv=0xd31be0) at /tmp/opdir/php-7.0.11/sapi/cli/php_cli.c:974
#9  0x00000000007a51fa in main (argc=2, argv=0xd31be0) at /tmp/opdir/php-7.0.11/sapi/cli/php_cli.c:1344","when we use the latest code of https://github.com/edenhill/librdkafka and https://pecl.php.net/package/rdkafka, the php core dumped, so i fixed it.
please review if it's ok, thanks:)
the php code:
$rk = new RdKafka\Producer();
$rk->setLogLevel(LOG_DEBUG);
the core dump backtrace:
Program received signal SIGSEGV, Segmentation fault.
rd_kafka_new (type=RD_KAFKA_CONSUMER, conf=0x0, errstr=0x7fffffffa9a0 ""\316\006"", errstr_size=512) at rdkafka.c:1113
1113            if (!conf->socket_cb) {
Missing separate debuginfos, use: debuginfo-install audit-libs-2.3.7-5.el6.x86_64 cyrus-sasl-lib-2.1.23-15.el6_6.2.x86_64 freetype-2.3.11-15.el6_6.1.x86_64 glibc-2.12-1.149.el6_6.9.x86_64 keyutils-libs-1.4-5.el6.x86_64 krb5-libs-1.10.3-37.el6_6.x86_64 libcom_err-1.41.12-21.el6.x86_64 libcurl-7.19.7-46.el6.x86_64 libgcc-4.4.7-11.el6.x86_64 libgomp-4.4.7-11.el6.x86_64 libidn-1.18-2.el6.x86_64 libjpeg-turbo-1.2.1-3.el6_5.x86_64 libselinux-2.0.94-5.8.el6.x86_64 libssh2-1.4.2-1.el6_6.1.x86_64 libxml2-2.7.6-17.el6_6.1.x86_64 libyaml-0.1.6-1.el6.x86_64 nspr-4.10.8-1.el6_6.x86_64 nss-3.19.1-3.el6_6.x86_64 nss-softokn-freebl-3.14.3-22.el6_6.x86_64 nss-util-3.19.1-1.el6_6.x86_64 openldap-2.4.39-8.el6.x86_64 openssl-1.0.1e-42.el6.x86_64 pam-1.1.1-20.el6.x86_64 postgresql-libs-8.4.20-3.el6_6.x86_64 sqlite-3.6.20-1.el6.x86_64 zlib-1.2.3-29.el6.x86_64
(gdb) backtrace
#0  rd_kafka_new (type=RD_KAFKA_CONSUMER, conf=0x0, errstr=0x7fffffffa9a0 ""\316\006"", errstr_size=512) at rdkafka.c:1113
#1  0x00007fffe802ab6d in kafka_init (this_ptr=0x7ffff1e130f0, type=RD_KAFKA_CONSUMER, zconf=<value optimized out>)
    at /tmp/source/rdkafka-2.0.0/rdkafka.c:98
#2  0x00007fffe802acbc in zim_RdKafka__Consumer___construct (execute_data=0x7ffff1e130d0, return_value=<value optimized out>)
    at /tmp/source/rdkafka-2.0.0/rdkafka.c:169
#3  0x0000000000785628 in ZEND_DO_FCALL_SPEC_HANDLER (execute_data=0x7ffff1e13030)
    at /tmp/opdir/php-7.0.11/Zend/zend_vm_execute.h:842
#4  0x000000000074d420 in execute_ex (ex=<value optimized out>) at /tmp/opdir/php-7.0.11/Zend/zend_vm_execute.h:417
#5  0x00000000007a06cb in zend_execute (op_array=0x7ffff1e7e000, return_value=<value optimized out>)
    at /tmp/opdir/php-7.0.11/Zend/zend_vm_execute.h:458
#6  0x000000000070dde3 in zend_execute_scripts (type=8, retval=0x0, file_count=3)
    at /tmp/opdir/php-7.0.11/Zend/zend.c:1427
#7  0x00000000006b0290 in php_execute_script (primary_file=0x7fffffffe120) at /tmp/opdir/php-7.0.11/main/main.c:2494
#8  0x00000000007a49fa in do_cli (argc=2, argv=0xd31be0) at /tmp/opdir/php-7.0.11/sapi/cli/php_cli.c:974
#9  0x00000000007a51fa in main (argc=2, argv=0xd31be0) at /tmp/opdir/php-7.0.11/sapi/cli/php_cli.c:1344",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,850,2016-10-21T11:27:10Z,2016-10-25T19:06:13Z,2016-10-25T19:06:13Z,CLOSED,False,2,2,1,https://github.com/wukezhan,fix crash in rd_kafka_new() when conf is NULL,1,[],https://github.com/edenhill/librdkafka/pull/850,https://github.com/edenhill,2,https://github.com/edenhill/librdkafka/pull/850#issuecomment-256144353,"when we use the latest code of https://github.com/edenhill/librdkafka and https://pecl.php.net/package/rdkafka, the php core dumped, so i fixed it.
please review if it's ok, thanks:)
the php code:
$rk = new RdKafka\Producer();
$rk->setLogLevel(LOG_DEBUG);
the core dump backtrace:
Program received signal SIGSEGV, Segmentation fault.
rd_kafka_new (type=RD_KAFKA_CONSUMER, conf=0x0, errstr=0x7fffffffa9a0 ""\316\006"", errstr_size=512) at rdkafka.c:1113
1113            if (!conf->socket_cb) {
Missing separate debuginfos, use: debuginfo-install audit-libs-2.3.7-5.el6.x86_64 cyrus-sasl-lib-2.1.23-15.el6_6.2.x86_64 freetype-2.3.11-15.el6_6.1.x86_64 glibc-2.12-1.149.el6_6.9.x86_64 keyutils-libs-1.4-5.el6.x86_64 krb5-libs-1.10.3-37.el6_6.x86_64 libcom_err-1.41.12-21.el6.x86_64 libcurl-7.19.7-46.el6.x86_64 libgcc-4.4.7-11.el6.x86_64 libgomp-4.4.7-11.el6.x86_64 libidn-1.18-2.el6.x86_64 libjpeg-turbo-1.2.1-3.el6_5.x86_64 libselinux-2.0.94-5.8.el6.x86_64 libssh2-1.4.2-1.el6_6.1.x86_64 libxml2-2.7.6-17.el6_6.1.x86_64 libyaml-0.1.6-1.el6.x86_64 nspr-4.10.8-1.el6_6.x86_64 nss-3.19.1-3.el6_6.x86_64 nss-softokn-freebl-3.14.3-22.el6_6.x86_64 nss-util-3.19.1-1.el6_6.x86_64 openldap-2.4.39-8.el6.x86_64 openssl-1.0.1e-42.el6.x86_64 pam-1.1.1-20.el6.x86_64 postgresql-libs-8.4.20-3.el6_6.x86_64 sqlite-3.6.20-1.el6.x86_64 zlib-1.2.3-29.el6.x86_64
(gdb) backtrace
#0  rd_kafka_new (type=RD_KAFKA_CONSUMER, conf=0x0, errstr=0x7fffffffa9a0 ""\316\006"", errstr_size=512) at rdkafka.c:1113
#1  0x00007fffe802ab6d in kafka_init (this_ptr=0x7ffff1e130f0, type=RD_KAFKA_CONSUMER, zconf=<value optimized out>)
    at /tmp/source/rdkafka-2.0.0/rdkafka.c:98
#2  0x00007fffe802acbc in zim_RdKafka__Consumer___construct (execute_data=0x7ffff1e130d0, return_value=<value optimized out>)
    at /tmp/source/rdkafka-2.0.0/rdkafka.c:169
#3  0x0000000000785628 in ZEND_DO_FCALL_SPEC_HANDLER (execute_data=0x7ffff1e13030)
    at /tmp/opdir/php-7.0.11/Zend/zend_vm_execute.h:842
#4  0x000000000074d420 in execute_ex (ex=<value optimized out>) at /tmp/opdir/php-7.0.11/Zend/zend_vm_execute.h:417
#5  0x00000000007a06cb in zend_execute (op_array=0x7ffff1e7e000, return_value=<value optimized out>)
    at /tmp/opdir/php-7.0.11/Zend/zend_vm_execute.h:458
#6  0x000000000070dde3 in zend_execute_scripts (type=8, retval=0x0, file_count=3)
    at /tmp/opdir/php-7.0.11/Zend/zend.c:1427
#7  0x00000000006b0290 in php_execute_script (primary_file=0x7fffffffe120) at /tmp/opdir/php-7.0.11/main/main.c:2494
#8  0x00000000007a49fa in do_cli (argc=2, argv=0xd31be0) at /tmp/opdir/php-7.0.11/sapi/cli/php_cli.c:974
#9  0x00000000007a51fa in main (argc=2, argv=0xd31be0) at /tmp/opdir/php-7.0.11/sapi/cli/php_cli.c:1344",Fixed in #863,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,851,2016-10-21T12:32:19Z,2016-10-21T12:48:11Z,2016-10-21T12:48:11Z,CLOSED,False,2,2,1,https://github.com/eugpermar,FIX SIGSEGV if rd_kafka_new called with NULL conf,1,[],https://github.com/edenhill/librdkafka/pull/851,https://github.com/eugpermar,1,https://github.com/edenhill/librdkafka/pull/851,,,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,851,2016-10-21T12:32:19Z,2016-10-21T12:48:11Z,2016-10-21T12:48:11Z,CLOSED,False,2,2,1,https://github.com/eugpermar,FIX SIGSEGV if rd_kafka_new called with NULL conf,1,[],https://github.com/edenhill/librdkafka/pull/851,https://github.com/eugpermar,2,https://github.com/edenhill/librdkafka/pull/851#issuecomment-255367470,,"Nobody can use this feature at this moment, so another valid option could be to only allow to call rd_kafka_new with a valid conf, never a NULL one.",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,851,2016-10-21T12:32:19Z,2016-10-21T12:48:11Z,2016-10-21T12:48:11Z,CLOSED,False,2,2,1,https://github.com/eugpermar,FIX SIGSEGV if rd_kafka_new called with NULL conf,1,[],https://github.com/edenhill/librdkafka/pull/851,https://github.com/eugpermar,3,https://github.com/edenhill/librdkafka/pull/851#issuecomment-255369553,,I didn't see the provided solution in PR #850 minutes ago. I think that one is better since it has no sense to check if default config has these cb. I close this one.,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,852,2016-10-21T19:15:39Z,2016-10-31T22:14:51Z,2016-10-31T22:15:01Z,MERGED,True,12,8,2,https://github.com/asharma339,Fix for async offset commit issue on AIX,1,[],https://github.com/edenhill/librdkafka/pull/852,https://github.com/asharma339,1,https://github.com/edenhill/librdkafka/pull/852,"While running tests on IBM-AIX, I experienced a crash while running 0030-offset_commit for  manual async commit. What's happening is that, it is evaluating both expressions and repq being NULL for async case, wreaks havoc when NULL is being passed to rd_kafka_q_keep().","While running tests on IBM-AIX, I experienced a crash while running 0030-offset_commit for  manual async commit. What's happening is that, it is evaluating both expressions and repq being NULL for async case, wreaks havoc when NULL is being passed to rd_kafka_q_keep().",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,852,2016-10-21T19:15:39Z,2016-10-31T22:14:51Z,2016-10-31T22:15:01Z,MERGED,True,12,8,2,https://github.com/asharma339,Fix for async offset commit issue on AIX,1,[],https://github.com/edenhill/librdkafka/pull/852,https://github.com/asharma339,2,https://github.com/edenhill/librdkafka/pull/852#issuecomment-255761893,"While running tests on IBM-AIX, I experienced a crash while running 0030-offset_commit for  manual async commit. What's happening is that, it is evaluating both expressions and repq being NULL for async case, wreaks havoc when NULL is being passed to rd_kafka_q_keep().","It is correct that lot of places use ternary operator in the code, but this place is unique for couple of reasons. Most of the places have scalars as the second and third operands for the ternary operator. This place uses a macro and object construction with member passed in curly brackets. In my opinion, this combination is resulting in arguments to object constructor being evaluated and resulting in a call to rd_kafka_q_keep(), which I verified by print statements. I checked xlc documentation, their claim is that either second or third operand is evaluated based on first operand to ternary operator. But silent on when macros are involved.  In my testing this is the only place failing. My suggestion is to keep an eye on any such issues if I come across on IBM and also follow up on research on this unique issue for IBM.  Until then, we should keep this simple fix.",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,852,2016-10-21T19:15:39Z,2016-10-31T22:14:51Z,2016-10-31T22:15:01Z,MERGED,True,12,8,2,https://github.com/asharma339,Fix for async offset commit issue on AIX,1,[],https://github.com/edenhill/librdkafka/pull/852,https://github.com/asharma339,3,https://github.com/edenhill/librdkafka/pull/852#issuecomment-257292449,"While running tests on IBM-AIX, I experienced a crash while running 0030-offset_commit for  manual async commit. What's happening is that, it is evaluating both expressions and repq being NULL for async case, wreaks havoc when NULL is being passed to rd_kafka_q_keep().",Done.,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,852,2016-10-21T19:15:39Z,2016-10-31T22:14:51Z,2016-10-31T22:15:01Z,MERGED,True,12,8,2,https://github.com/asharma339,Fix for async offset commit issue on AIX,1,[],https://github.com/edenhill/librdkafka/pull/852,https://github.com/asharma339,4,https://github.com/edenhill/librdkafka/pull/852#issuecomment-257427981,"While running tests on IBM-AIX, I experienced a crash while running 0030-offset_commit for  manual async commit. What's happening is that, it is evaluating both expressions and repq being NULL for async case, wreaks havoc when NULL is being passed to rd_kafka_q_keep().",Done,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,852,2016-10-21T19:15:39Z,2016-10-31T22:14:51Z,2016-10-31T22:15:01Z,MERGED,True,12,8,2,https://github.com/asharma339,Fix for async offset commit issue on AIX,1,[],https://github.com/edenhill/librdkafka/pull/852,https://github.com/edenhill,5,https://github.com/edenhill/librdkafka/pull/852#issuecomment-257436640,"While running tests on IBM-AIX, I experienced a crash while running 0030-offset_commit for  manual async commit. What's happening is that, it is evaluating both expressions and repq being NULL for async case, wreaks havoc when NULL is being passed to rd_kafka_q_keep().",Thanks!,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,855,2016-10-24T08:11:24Z,2016-10-25T08:53:06Z,2016-10-25T08:53:06Z,CLOSED,False,75,33,8,https://github.com/joseAndresGomezTovar,cosmetic changes,4,[],https://github.com/edenhill/librdkafka/pull/855,https://github.com/joseAndresGomezTovar,1,https://github.com/edenhill/librdkafka/pull/855,"add support to vs2013 and vs2015 (using
Condition=""'$(VisualStudioVersion)'=='12.0')
remove openssl dependency (using #pragma comment(lib)) with WITH_SSL=0
definition","add support to vs2013 and vs2015 (using
Condition=""'$(VisualStudioVersion)'=='12.0')
remove openssl dependency (using #pragma comment(lib)) with WITH_SSL=0
definition",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,855,2016-10-24T08:11:24Z,2016-10-25T08:53:06Z,2016-10-25T08:53:06Z,CLOSED,False,75,33,8,https://github.com/joseAndresGomezTovar,cosmetic changes,4,[],https://github.com/edenhill/librdkafka/pull/855,https://github.com/edenhill,2,https://github.com/edenhill/librdkafka/pull/855#issuecomment-255683802,"add support to vs2013 and vs2015 (using
Condition=""'$(VisualStudioVersion)'=='12.0')
remove openssl dependency (using #pragma comment(lib)) with WITH_SSL=0
definition",AppVeyor build fails: https://ci.appveyor.com/project/edenhill/librdkafka/build/0.9.1-post437#L1273,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,855,2016-10-24T08:11:24Z,2016-10-25T08:53:06Z,2016-10-25T08:53:06Z,CLOSED,False,75,33,8,https://github.com/joseAndresGomezTovar,cosmetic changes,4,[],https://github.com/edenhill/librdkafka/pull/855,https://github.com/joseAndresGomezTovar,3,https://github.com/edenhill/librdkafka/pull/855#issuecomment-255687586,"add support to vs2013 and vs2015 (using
Condition=""'$(VisualStudioVersion)'=='12.0')
remove openssl dependency (using #pragma comment(lib)) with WITH_SSL=0
definition","let me check it this night (UTC+1) (searching in google appears a ""working directory issue...""[https://github.com/dantarion/SF5DInput/pull/16/files])",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,855,2016-10-24T08:11:24Z,2016-10-25T08:53:06Z,2016-10-25T08:53:06Z,CLOSED,False,75,33,8,https://github.com/joseAndresGomezTovar,cosmetic changes,4,[],https://github.com/edenhill/librdkafka/pull/855,https://github.com/joseAndresGomezTovar,4,https://github.com/edenhill/librdkafka/pull/855#issuecomment-255833170,"add support to vs2013 and vs2015 (using
Condition=""'$(VisualStudioVersion)'=='12.0')
remove openssl dependency (using #pragma comment(lib)) with WITH_SSL=0
definition","do you know if you are using libraries.v140 appveyor?
Its the first time I see appveyor but  at line 10
MSBuild auto-detection: using msbuild version '14.0' from 'C:\Program Files (x86)\MSBuild\14.0\bin'.
apparently is using vs2015 isnt it?",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,855,2016-10-24T08:11:24Z,2016-10-25T08:53:06Z,2016-10-25T08:53:06Z,CLOSED,False,75,33,8,https://github.com/joseAndresGomezTovar,cosmetic changes,4,[],https://github.com/edenhill/librdkafka/pull/855,https://github.com/edenhill,5,https://github.com/edenhill/librdkafka/pull/855#issuecomment-255834590,"add support to vs2013 and vs2015 (using
Condition=""'$(VisualStudioVersion)'=='12.0')
remove openssl dependency (using #pragma comment(lib)) with WITH_SSL=0
definition","The toolchain wasn't defined so I set it to build both VS 13 and 15, let's see what happens.",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,855,2016-10-24T08:11:24Z,2016-10-25T08:53:06Z,2016-10-25T08:53:06Z,CLOSED,False,75,33,8,https://github.com/joseAndresGomezTovar,cosmetic changes,4,[],https://github.com/edenhill/librdkafka/pull/855,https://github.com/edenhill,6,https://github.com/edenhill/librdkafka/pull/855#issuecomment-255835744,"add support to vs2013 and vs2015 (using
Condition=""'$(VisualStudioVersion)'=='12.0')
remove openssl dependency (using #pragma comment(lib)) with WITH_SSL=0
definition","Fails for both of them
https://ci.appveyor.com/project/edenhill/librdkafka/build/0.9.1-post442",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,855,2016-10-24T08:11:24Z,2016-10-25T08:53:06Z,2016-10-25T08:53:06Z,CLOSED,False,75,33,8,https://github.com/joseAndresGomezTovar,cosmetic changes,4,[],https://github.com/edenhill/librdkafka/pull/855,https://github.com/joseAndresGomezTovar,7,https://github.com/edenhill/librdkafka/pull/855#issuecomment-255836468,"add support to vs2013 and vs2015 (using
Condition=""'$(VisualStudioVersion)'=='12.0')
remove openssl dependency (using #pragma comment(lib)) with WITH_SSL=0
definition","works well locally, I keep searching (I use Win32OpenSSL-1_0_2j of
https://slproweb.com/products/Win32OpenSSL.html",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,855,2016-10-24T08:11:24Z,2016-10-25T08:53:06Z,2016-10-25T08:53:06Z,CLOSED,False,75,33,8,https://github.com/joseAndresGomezTovar,cosmetic changes,4,[],https://github.com/edenhill/librdkafka/pull/855,https://github.com/joseAndresGomezTovar,8,https://github.com/edenhill/librdkafka/pull/855#issuecomment-255839422,"add support to vs2013 and vs2015 (using
Condition=""'$(VisualStudioVersion)'=='12.0')
remove openssl dependency (using #pragma comment(lib)) with WITH_SSL=0
definition",There is an error with the library directory ... that all folks,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,855,2016-10-24T08:11:24Z,2016-10-25T08:53:06Z,2016-10-25T08:53:06Z,CLOSED,False,75,33,8,https://github.com/joseAndresGomezTovar,cosmetic changes,4,[],https://github.com/edenhill/librdkafka/pull/855,https://github.com/mhowlett,9,https://github.com/edenhill/librdkafka/pull/855#issuecomment-255853228,"add support to vs2013 and vs2015 (using
Condition=""'$(VisualStudioVersion)'=='12.0')
remove openssl dependency (using #pragma comment(lib)) with WITH_SSL=0
definition","To the extent that I know what I'm looking at here, this looks fine to me except i'm wondering about the absolute paths in win32/tests/tests.vcxproj.",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,857,2016-10-24T16:43:53Z,2016-10-28T15:44:37Z,2016-10-28T15:44:37Z,MERGED,True,1,1,1,https://github.com/asharma339,Fix for AIX-- exclude endian.h on AIX,1,[],https://github.com/edenhill/librdkafka/pull/857,https://github.com/asharma339,1,https://github.com/edenhill/librdkafka/pull/857,"A few more changes for AIX:

malloc/calloc return NULL for 0 size memory request.
As described in issue  #837, need to use mutexes for atomic ops.","A few more changes for AIX:

malloc/calloc return NULL for 0 size memory request.
As described in issue  #837, need to use mutexes for atomic ops.",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,857,2016-10-24T16:43:53Z,2016-10-28T15:44:37Z,2016-10-28T15:44:37Z,MERGED,True,1,1,1,https://github.com/asharma339,Fix for AIX-- exclude endian.h on AIX,1,[],https://github.com/edenhill/librdkafka/pull/857,https://github.com/asharma339,2,https://github.com/edenhill/librdkafka/pull/857#issuecomment-256474401,"A few more changes for AIX:

malloc/calloc return NULL for 0 size memory request.
As described in issue  #837, need to use mutexes for atomic ops.",Cleaned up. Now the change is just for excluding endian.h,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,857,2016-10-24T16:43:53Z,2016-10-28T15:44:37Z,2016-10-28T15:44:37Z,MERGED,True,1,1,1,https://github.com/asharma339,Fix for AIX-- exclude endian.h on AIX,1,[],https://github.com/edenhill/librdkafka/pull/857,https://github.com/edenhill,3,https://github.com/edenhill/librdkafka/pull/857#issuecomment-256475357,"A few more changes for AIX:

malloc/calloc return NULL for 0 size memory request.
As described in issue  #837, need to use mutexes for atomic ops.","Thanks.
Can you rebase the PR so that the fixes-after-review are combined with the original commit?
Something like git rebase -i origin/master then set the fixes-on-fixes as 'fixup'",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,857,2016-10-24T16:43:53Z,2016-10-28T15:44:37Z,2016-10-28T15:44:37Z,MERGED,True,1,1,1,https://github.com/asharma339,Fix for AIX-- exclude endian.h on AIX,1,[],https://github.com/edenhill/librdkafka/pull/857,https://github.com/asharma339,4,https://github.com/edenhill/librdkafka/pull/857#issuecomment-256493921,"A few more changes for AIX:

malloc/calloc return NULL for 0 size memory request.
As described in issue  #837, need to use mutexes for atomic ops.",Done,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,858,2016-10-25T02:06:19Z,2016-11-16T22:28:58Z,2016-11-18T00:36:37Z,CLOSED,False,24,12,1,https://github.com/yunjing,Fix timestamp extraction for compressed messages,1,[],https://github.com/edenhill/librdkafka/pull/858,https://github.com/yunjing,1,https://github.com/edenhill/librdkafka/pull/858,Currently timestamp extraction fails for compressed messages. It would report -1 instead of the actual timestamp because the timestamps for compressed messages are stored at the block level. This PR passes down the header for the block to each of the uncompressed messages.,Currently timestamp extraction fails for compressed messages. It would report -1 instead of the actual timestamp because the timestamps for compressed messages are stored at the block level. This PR passes down the header for the block to each of the uncompressed messages.,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,858,2016-10-25T02:06:19Z,2016-11-16T22:28:58Z,2016-11-18T00:36:37Z,CLOSED,False,24,12,1,https://github.com/yunjing,Fix timestamp extraction for compressed messages,1,[],https://github.com/edenhill/librdkafka/pull/858,https://github.com/yunjing,2,https://github.com/edenhill/librdkafka/pull/858#issuecomment-256992897,Currently timestamp extraction fails for compressed messages. It would report -1 instead of the actual timestamp because the timestamps for compressed messages are stored at the block level. This PR passes down the header for the block to each of the uncompressed messages.,"@edenhill any suggestion on this? We are trying to open source a project in a week, but this bug is blocking us.",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,858,2016-10-25T02:06:19Z,2016-11-16T22:28:58Z,2016-11-18T00:36:37Z,CLOSED,False,24,12,1,https://github.com/yunjing,Fix timestamp extraction for compressed messages,1,[],https://github.com/edenhill/librdkafka/pull/858,https://github.com/edenhill,3,https://github.com/edenhill/librdkafka/pull/858#issuecomment-256993837,Currently timestamp extraction fails for compressed messages. It would report -1 instead of the actual timestamp because the timestamps for compressed messages are stored at the block level. This PR passes down the header for the block to each of the uncompressed messages.,"Hey, I'll look at this during the weekend and get back to you.",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,858,2016-10-25T02:06:19Z,2016-11-16T22:28:58Z,2016-11-18T00:36:37Z,CLOSED,False,24,12,1,https://github.com/yunjing,Fix timestamp extraction for compressed messages,1,[],https://github.com/edenhill/librdkafka/pull/858,https://github.com/yunjing,4,https://github.com/edenhill/librdkafka/pull/858#issuecomment-257006941,Currently timestamp extraction fails for compressed messages. It would report -1 instead of the actual timestamp because the timestamps for compressed messages are stored at the block level. This PR passes down the header for the block to each of the uncompressed messages.,Thanks @edenhill. Really appreciated!,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,858,2016-10-25T02:06:19Z,2016-11-16T22:28:58Z,2016-11-18T00:36:37Z,CLOSED,False,24,12,1,https://github.com/yunjing,Fix timestamp extraction for compressed messages,1,[],https://github.com/edenhill/librdkafka/pull/858,https://github.com/qix,5,https://github.com/edenhill/librdkafka/pull/858#issuecomment-259215417,Currently timestamp extraction fails for compressed messages. It would report -1 instead of the actual timestamp because the timestamps for compressed messages are stored at the block level. This PR passes down the header for the block to each of the uncompressed messages.,"Hi, we've just pushed an open source project for which we require these changes in order to run in production: https://github.com/smyte/kafka_store
For now we're recommending that users install our fork, which we'll revert as soon as this makes it into a release.",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,858,2016-10-25T02:06:19Z,2016-11-16T22:28:58Z,2016-11-18T00:36:37Z,CLOSED,False,24,12,1,https://github.com/yunjing,Fix timestamp extraction for compressed messages,1,[],https://github.com/edenhill/librdkafka/pull/858,https://github.com/edenhill,6,https://github.com/edenhill/librdkafka/pull/858#issuecomment-261093215,Currently timestamp extraction fails for compressed messages. It would report -1 instead of the actual timestamp because the timestamps for compressed messages are stored at the block level. This PR passes down the header for the block to each of the uncompressed messages.,Thanks!,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,858,2016-10-25T02:06:19Z,2016-11-16T22:28:58Z,2016-11-18T00:36:37Z,CLOSED,False,24,12,1,https://github.com/yunjing,Fix timestamp extraction for compressed messages,1,[],https://github.com/edenhill/librdkafka/pull/858,https://github.com/edenhill,7,https://github.com/edenhill/librdkafka/pull/858#issuecomment-261361735,Currently timestamp extraction fails for compressed messages. It would report -1 instead of the actual timestamp because the timestamps for compressed messages are stored at the block level. This PR passes down the header for the block to each of the uncompressed messages.,"Can you verify the fix on your end, thanks?",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,858,2016-10-25T02:06:19Z,2016-11-16T22:28:58Z,2016-11-18T00:36:37Z,CLOSED,False,24,12,1,https://github.com/yunjing,Fix timestamp extraction for compressed messages,1,[],https://github.com/edenhill/librdkafka/pull/858,https://github.com/yunjing,8,https://github.com/edenhill/librdkafka/pull/858#issuecomment-261414803,Currently timestamp extraction fails for compressed messages. It would report -1 instead of the actual timestamp because the timestamps for compressed messages are stored at the block level. This PR passes down the header for the block to each of the uncompressed messages.,"Verified that it worked by compiling kafkacat with librdkafka master and consuming from several topics in production.
We have not yet incorporated the latest librdkafka in our production services.",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,859,2016-10-25T06:22:01Z,2016-10-25T19:23:16Z,2016-10-26T18:37:54Z,MERGED,True,2,2,1,https://github.com/yunjing,Fix relative offset calculation,2,"['bug', 'consumer']",https://github.com/edenhill/librdkafka/pull/859,https://github.com/yunjing,1,https://github.com/edenhill/librdkafka/pull/859,rd_kafka_q_fix_offsets purges messages by comparing a relative offset to the absolute min offset. It only works when the absolute min offset is 0.,rd_kafka_q_fix_offsets purges messages by comparing a relative offset to the absolute min offset. It only works when the absolute min offset is 0.,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,859,2016-10-25T06:22:01Z,2016-10-25T19:23:16Z,2016-10-26T18:37:54Z,MERGED,True,2,2,1,https://github.com/yunjing,Fix relative offset calculation,2,"['bug', 'consumer']",https://github.com/edenhill/librdkafka/pull/859,https://github.com/yunjing,2,https://github.com/edenhill/librdkafka/pull/859#issuecomment-256148579,rd_kafka_q_fix_offsets purges messages by comparing a relative offset to the absolute min offset. It only works when the absolute min offset is 0.,Fixed.,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,859,2016-10-25T06:22:01Z,2016-10-25T19:23:16Z,2016-10-26T18:37:54Z,MERGED,True,2,2,1,https://github.com/yunjing,Fix relative offset calculation,2,"['bug', 'consumer']",https://github.com/edenhill/librdkafka/pull/859,https://github.com/edenhill,3,https://github.com/edenhill/librdkafka/pull/859#issuecomment-256149009,rd_kafka_q_fix_offsets purges messages by comparing a relative offset to the absolute min offset. It only works when the absolute min offset is 0.,Thanks for locating this issue!,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,861,2016-10-25T10:03:50Z,2016-11-02T16:05:06Z,2016-11-02T19:48:15Z,CLOSED,False,113,67,25,https://github.com/joseAndresGomezTovar,changes after checking with cppcheck ,12,[],https://github.com/edenhill/librdkafka/pull/861,https://github.com/joseAndresGomezTovar,1,https://github.com/edenhill/librdkafka/pull/861,"A tool for static C/C++ code analysis: more info here
(http://cppcheck.sourceforge.net/)

Summary: Prefer prefix ++/-- operators for non-primitive types.
Message: Prefix ++/-- operators should be preferred for non-primitive
types. Pre-increment/decrement can be more efficient than
post-increment/decrement. Post-increment/decrement usually involves
keeping a copy of the previous value around and adds a little extra
code.

Summary: Variable 'topic_' is assigned in constructor body. Consider
performing initialization in initialization list.
Message: When an object of a class is created, the constructors of all
member variables are called consecutively in the order the variables are
declared, even if you don't explicitly write them to the initialization
list. You could avoid assigning 'topic_' a value by passing the value to
the constructor in the initialization list.
First included by: src-cpp\ConfImpl.cpp

Summary: %zd in format string (no. 2) requires 'ssize_t' but the
argument type is 'size_t {aka unsigned long long}'.
Message: %zd in format string (no. 2) requires 'ssize_t' but the
argument type is 'size_t {aka unsigned long long}'.

Summary: Either the condition 'if(rko_orig)' is redundant or there is
possible null pointer dereference: rko_orig.
Message: Either the condition 'if(rko_orig)' is redundant or there is
possible null pointer dereference: rko_orig.

there are advise/errors but I dont know if it is an error or not... I
am noob with kafka



file rdkafka_broker.c
Summary: Dead pointer usage. Pointer 'msg' is dead if it has been
assigned '&msg2' at line 1760.
Message: Dead pointer usage. Pointer 'msg' is dead if it has been
assigned '&msg2' at line 1760.



file
rdkafka_cgrp.c
Summary: Conversion of string literal ""RD_KAFKA_OP_OFFSET_COMMIT"" to
bool always evaluates to true.
Message: Conversion of string literal ""RD_KAFKA_OP_OFFSET_COMMIT"" to
bool always evaluates to true.



file
rdrand.c
Summary: Obsolete function 'alloca' called. In C99 and later it is
recommended to use a variable length array instead.
Message: The obsolete function 'alloca' is called. In C99 and later it
is recommended to use a variable length array or a dynamically allocated
array instead. The function 'alloca' is dangerous for many reasons
(http://stackoverflow.com/questions/1018853/why-is-alloca-not-considered-good-practice
and http://linux.die.net/man/3/alloca).","A tool for static C/C++ code analysis: more info here
(http://cppcheck.sourceforge.net/)

Summary: Prefer prefix ++/-- operators for non-primitive types.
Message: Prefix ++/-- operators should be preferred for non-primitive
types. Pre-increment/decrement can be more efficient than
post-increment/decrement. Post-increment/decrement usually involves
keeping a copy of the previous value around and adds a little extra
code.

Summary: Variable 'topic_' is assigned in constructor body. Consider
performing initialization in initialization list.
Message: When an object of a class is created, the constructors of all
member variables are called consecutively in the order the variables are
declared, even if you don't explicitly write them to the initialization
list. You could avoid assigning 'topic_' a value by passing the value to
the constructor in the initialization list.
First included by: src-cpp\ConfImpl.cpp

Summary: %zd in format string (no. 2) requires 'ssize_t' but the
argument type is 'size_t {aka unsigned long long}'.
Message: %zd in format string (no. 2) requires 'ssize_t' but the
argument type is 'size_t {aka unsigned long long}'.

Summary: Either the condition 'if(rko_orig)' is redundant or there is
possible null pointer dereference: rko_orig.
Message: Either the condition 'if(rko_orig)' is redundant or there is
possible null pointer dereference: rko_orig.

there are advise/errors but I dont know if it is an error or not... I
am noob with kafka



file rdkafka_broker.c
Summary: Dead pointer usage. Pointer 'msg' is dead if it has been
assigned '&msg2' at line 1760.
Message: Dead pointer usage. Pointer 'msg' is dead if it has been
assigned '&msg2' at line 1760.



file
rdkafka_cgrp.c
Summary: Conversion of string literal ""RD_KAFKA_OP_OFFSET_COMMIT"" to
bool always evaluates to true.
Message: Conversion of string literal ""RD_KAFKA_OP_OFFSET_COMMIT"" to
bool always evaluates to true.



file
rdrand.c
Summary: Obsolete function 'alloca' called. In C99 and later it is
recommended to use a variable length array instead.
Message: The obsolete function 'alloca' is called. In C99 and later it
is recommended to use a variable length array or a dynamically allocated
array instead. The function 'alloca' is dangerous for many reasons
(http://stackoverflow.com/questions/1018853/why-is-alloca-not-considered-good-practice
and http://linux.die.net/man/3/alloca).",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,861,2016-10-25T10:03:50Z,2016-11-02T16:05:06Z,2016-11-02T19:48:15Z,CLOSED,False,113,67,25,https://github.com/joseAndresGomezTovar,changes after checking with cppcheck ,12,[],https://github.com/edenhill/librdkafka/pull/861,https://github.com/edenhill,2,https://github.com/edenhill/librdkafka/pull/861#issuecomment-257978252,"A tool for static C/C++ code analysis: more info here
(http://cppcheck.sourceforge.net/)

Summary: Prefer prefix ++/-- operators for non-primitive types.
Message: Prefix ++/-- operators should be preferred for non-primitive
types. Pre-increment/decrement can be more efficient than
post-increment/decrement. Post-increment/decrement usually involves
keeping a copy of the previous value around and adds a little extra
code.

Summary: Variable 'topic_' is assigned in constructor body. Consider
performing initialization in initialization list.
Message: When an object of a class is created, the constructors of all
member variables are called consecutively in the order the variables are
declared, even if you don't explicitly write them to the initialization
list. You could avoid assigning 'topic_' a value by passing the value to
the constructor in the initialization list.
First included by: src-cpp\ConfImpl.cpp

Summary: %zd in format string (no. 2) requires 'ssize_t' but the
argument type is 'size_t {aka unsigned long long}'.
Message: %zd in format string (no. 2) requires 'ssize_t' but the
argument type is 'size_t {aka unsigned long long}'.

Summary: Either the condition 'if(rko_orig)' is redundant or there is
possible null pointer dereference: rko_orig.
Message: Either the condition 'if(rko_orig)' is redundant or there is
possible null pointer dereference: rko_orig.

there are advise/errors but I dont know if it is an error or not... I
am noob with kafka



file rdkafka_broker.c
Summary: Dead pointer usage. Pointer 'msg' is dead if it has been
assigned '&msg2' at line 1760.
Message: Dead pointer usage. Pointer 'msg' is dead if it has been
assigned '&msg2' at line 1760.



file
rdkafka_cgrp.c
Summary: Conversion of string literal ""RD_KAFKA_OP_OFFSET_COMMIT"" to
bool always evaluates to true.
Message: Conversion of string literal ""RD_KAFKA_OP_OFFSET_COMMIT"" to
bool always evaluates to true.



file
rdrand.c
Summary: Obsolete function 'alloca' called. In C99 and later it is
recommended to use a variable length array instead.
Message: The obsolete function 'alloca' is called. In C99 and later it
is recommended to use a variable length array or a dynamically allocated
array instead. The function 'alloca' is dangerous for many reasons
(http://stackoverflow.com/questions/1018853/why-is-alloca-not-considered-good-practice
and http://linux.die.net/man/3/alloca).","I havent had time to look at this yet.
What is the reason you close it?",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,863,2016-10-25T12:21:40Z,2016-10-25T19:06:45Z,2016-10-25T19:06:50Z,MERGED,True,2,2,1,https://github.com/wadee,fix func rd_kafka_new declare use_conf but not use cause conf->socket,1,[],https://github.com/edenhill/librdkafka/pull/863,https://github.com/wadee,1,https://github.com/edenhill/librdkafka/pull/863,fix func rd_kafka_new declare use_conf but not use cause conf->socket_cb segmentfault,fix func rd_kafka_new declare use_conf but not use cause conf->socket_cb segmentfault,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,863,2016-10-25T12:21:40Z,2016-10-25T19:06:45Z,2016-10-25T19:06:50Z,MERGED,True,2,2,1,https://github.com/wadee,fix func rd_kafka_new declare use_conf but not use cause conf->socket,1,[],https://github.com/edenhill/librdkafka/pull/863,https://github.com/edenhill,2,https://github.com/edenhill/librdkafka/pull/863#issuecomment-256144534,fix func rd_kafka_new declare use_conf but not use cause conf->socket_cb segmentfault,Thanks!,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,865,2016-10-26T15:58:34Z,2016-10-31T20:54:52Z,2016-10-31T20:54:56Z,MERGED,True,52,1,1,https://github.com/asharma339,Replacement for builtins for sun and ibm,1,[],https://github.com/edenhill/librdkafka/pull/865,https://github.com/asharma339,1,https://github.com/edenhill/librdkafka/pull/865,"Since  __builtin_clz, __builtin_ctz, __builtin_ctzll are not available for  AIX and Sun, provided the replacements","Since  __builtin_clz, __builtin_ctz, __builtin_ctzll are not available for  AIX and Sun, provided the replacements",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,865,2016-10-26T15:58:34Z,2016-10-31T20:54:52Z,2016-10-31T20:54:56Z,MERGED,True,52,1,1,https://github.com/asharma339,Replacement for builtins for sun and ibm,1,[],https://github.com/edenhill/librdkafka/pull/865,https://github.com/asharma339,2,https://github.com/edenhill/librdkafka/pull/865#issuecomment-257297776,"Since  __builtin_clz, __builtin_ctz, __builtin_ctzll are not available for  AIX and Sun, provided the replacements",Done. Used toolchain based checks.,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,865,2016-10-26T15:58:34Z,2016-10-31T20:54:52Z,2016-10-31T20:54:56Z,MERGED,True,52,1,1,https://github.com/asharma339,Replacement for builtins for sun and ibm,1,[],https://github.com/edenhill/librdkafka/pull/865,https://github.com/asharma339,3,https://github.com/edenhill/librdkafka/pull/865#issuecomment-257417707,"Since  __builtin_clz, __builtin_ctz, __builtin_ctzll are not available for  AIX and Sun, provided the replacements",Done,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,865,2016-10-26T15:58:34Z,2016-10-31T20:54:52Z,2016-10-31T20:54:56Z,MERGED,True,52,1,1,https://github.com/asharma339,Replacement for builtins for sun and ibm,1,[],https://github.com/edenhill/librdkafka/pull/865,https://github.com/edenhill,4,https://github.com/edenhill/librdkafka/pull/865#issuecomment-257417859,"Since  __builtin_clz, __builtin_ctz, __builtin_ctzll are not available for  AIX and Sun, provided the replacements",Thank you!,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,869,2016-10-27T20:47:15Z,2016-10-28T07:33:04Z,2016-10-28T07:33:09Z,MERGED,True,14,6,1,https://github.com/asharma339,Use compiler specific atomic ops for sun studio compiler,1,[],https://github.com/edenhill/librdkafka/pull/869,https://github.com/asharma339,1,https://github.com/edenhill/librdkafka/pull/869,This is to make atomic functions work  using native sun solaris compiler. I could not find any good way to do it thru config.,This is to make atomic functions work  using native sun solaris compiler. I could not find any good way to do it thru config.,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,869,2016-10-27T20:47:15Z,2016-10-28T07:33:04Z,2016-10-28T07:33:09Z,MERGED,True,14,6,1,https://github.com/asharma339,Use compiler specific atomic ops for sun studio compiler,1,[],https://github.com/edenhill/librdkafka/pull/869,https://github.com/edenhill,2,https://github.com/edenhill/librdkafka/pull/869#issuecomment-256855648,This is to make atomic functions work  using native sun solaris compiler. I could not find any good way to do it thru config.,"Looks great, thanks!",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,870,2016-10-28T06:49:03Z,2016-10-28T07:31:44Z,2016-10-28T07:31:51Z,MERGED,True,14,14,8,https://github.com/lvxv,fix bug rd_kafka_timer_t microsecond interval overflow.,2,[],https://github.com/edenhill/librdkafka/pull/870,https://github.com/lvxv,1,https://github.com/edenhill/librdkafka/pull/870,"./rdkafka_performance -P -b xxxx:xx -t test_topic -i 3600000
% Configuration property ""statistics.interval.ms"" value -694967 is outside allowed range 0..86400000
statistics.interval.ms 86400000
int rtmr_interval max :  2147483647(2^31)
86400000 * 1000 > 2147483647(2^31)","./rdkafka_performance -P -b xxxx:xx -t test_topic -i 3600000
% Configuration property ""statistics.interval.ms"" value -694967 is outside allowed range 0..86400000
statistics.interval.ms 86400000
int rtmr_interval max :  2147483647(2^31)
86400000 * 1000 > 2147483647(2^31)",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,870,2016-10-28T06:49:03Z,2016-10-28T07:31:44Z,2016-10-28T07:31:51Z,MERGED,True,14,14,8,https://github.com/lvxv,fix bug rd_kafka_timer_t microsecond interval overflow.,2,[],https://github.com/edenhill/librdkafka/pull/870,https://github.com/edenhill,2,https://github.com/edenhill/librdkafka/pull/870#issuecomment-256855425,"./rdkafka_performance -P -b xxxx:xx -t test_topic -i 3600000
% Configuration property ""statistics.interval.ms"" value -694967 is outside allowed range 0..86400000
statistics.interval.ms 86400000
int rtmr_interval max :  2147483647(2^31)
86400000 * 1000 > 2147483647(2^31)",Thanks!,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,876,2016-10-31T09:51:24Z,2016-10-31T20:19:53Z,2016-10-31T20:19:53Z,CLOSED,False,59,59,3,https://github.com/hustlijian,add snappy function header to be compatible with google snappy,1,[],https://github.com/edenhill/librdkafka/pull/876,https://github.com/hustlijian,1,https://github.com/edenhill/librdkafka/pull/876,use rd_kafka_ prefix,use rd_kafka_ prefix,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,880,2016-10-31T14:38:55Z,2016-10-31T20:18:52Z,2016-10-31T20:18:56Z,MERGED,True,59,59,3,https://github.com/hustlijian,add snappy function header to be compatible with google snappy,1,[],https://github.com/edenhill/librdkafka/pull/880,https://github.com/hustlijian,1,https://github.com/edenhill/librdkafka/pull/880,add snappy function header to be compatible with google snappy,add snappy function header to be compatible with google snappy,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,880,2016-10-31T14:38:55Z,2016-10-31T20:18:52Z,2016-10-31T20:18:56Z,MERGED,True,59,59,3,https://github.com/hustlijian,add snappy function header to be compatible with google snappy,1,[],https://github.com/edenhill/librdkafka/pull/880,https://github.com/edenhill,2,https://github.com/edenhill/librdkafka/pull/880#issuecomment-257408548,add snappy function header to be compatible with google snappy,Thank you!,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,883,2016-11-02T13:59:32Z,2016-11-17T20:37:29Z,2016-11-17T20:37:32Z,MERGED,True,143,25,2,https://github.com/vin-d,Enrichment of Conf getter for callbacks.,7,[],https://github.com/edenhill/librdkafka/pull/883,https://github.com/vin-d,1,https://github.com/edenhill/librdkafka/pull/883,"for issue #878
As the (C++) Conf setters do not forward callbacks in the C struct like with other parameters, special getter should be used to read the callbacks from the Conf.
This update also blocks the original Getter(String&, String&) to get the callbacks from the C struct.","for issue #878
As the (C++) Conf setters do not forward callbacks in the C struct like with other parameters, special getter should be used to read the callbacks from the Conf.
This update also blocks the original Getter(String&, String&) to get the callbacks from the C struct.",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,883,2016-11-02T13:59:32Z,2016-11-17T20:37:29Z,2016-11-17T20:37:32Z,MERGED,True,143,25,2,https://github.com/vin-d,Enrichment of Conf getter for callbacks.,7,[],https://github.com/edenhill/librdkafka/pull/883,https://github.com/edenhill,2,https://github.com/edenhill/librdkafka/pull/883#issuecomment-257886934,"for issue #878
As the (C++) Conf setters do not forward callbacks in the C struct like with other parameters, special getter should be used to read the callbacks from the Conf.
This update also blocks the original Getter(String&, String&) to get the callbacks from the C struct.",What are you using the get() interface for?,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,883,2016-11-02T13:59:32Z,2016-11-17T20:37:29Z,2016-11-17T20:37:32Z,MERGED,True,143,25,2,https://github.com/vin-d,Enrichment of Conf getter for callbacks.,7,[],https://github.com/edenhill/librdkafka/pull/883,https://github.com/vin-d,3,https://github.com/edenhill/librdkafka/pull/883#issuecomment-257889764,"for issue #878
As the (C++) Conf setters do not forward callbacks in the C struct like with other parameters, special getter should be used to read the callbacks from the Conf.
This update also blocks the original Getter(String&, String&) to get the callbacks from the C struct.","I started using the get() only for a mock KafkaConsumer object and, while
only looking at the doxygen docs I wasn't aware that Conf didn't forward
callbacks to the c version of librdkafka, and that the get(string&,
string&) wasn't the way to do that.
Reading the source files I figured out that a KafkaConsumer directly reads
the callback variable from the Conf without using a getter. I thought that
a getter would still be a good thing to have a complete Conf interface and
richer Doxygen docs.
On Wed, Nov 2, 2016 at 3:48 PM, Magnus Edenhill notifications@github.com
wrote:

What are you using the get() interface for?

You are receiving this because you authored the thread.
Reply to this email directly, view it on GitHub
#883 (comment),
or mute the thread
https://github.com/notifications/unsubscribe-auth/ATQE25o8bmgELJx4JBPEN8D6R5uVYVqNks5q6KLBgaJpZM4KnRD-
.


The information transmitted is intended only for the person or entity to
which it is addressed and may contain confidential and/or privileged
material. Any review, retransmission, dissemination or other use of, or
taking of any action in reliance upon, this information by persons or
entities other than the intended recipient is prohibited. If you received
this in error, please contact the sender and delete the material from any
computer.",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,883,2016-11-02T13:59:32Z,2016-11-17T20:37:29Z,2016-11-17T20:37:32Z,MERGED,True,143,25,2,https://github.com/vin-d,Enrichment of Conf getter for callbacks.,7,[],https://github.com/edenhill/librdkafka/pull/883,https://github.com/edenhill,4,https://github.com/edenhill/librdkafka/pull/883#issuecomment-257891244,"for issue #878
As the (C++) Conf setters do not forward callbacks in the C struct like with other parameters, special getter should be used to read the callbacks from the Conf.
This update also blocks the original Getter(String&, String&) to get the callbacks from the C struct.",My only concern is that this becomes a bit verbose and somewhat dreary to maintain for very little gain.,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,883,2016-11-02T13:59:32Z,2016-11-17T20:37:29Z,2016-11-17T20:37:32Z,MERGED,True,143,25,2,https://github.com/vin-d,Enrichment of Conf getter for callbacks.,7,[],https://github.com/edenhill/librdkafka/pull/883,https://github.com/vin-d,5,https://github.com/edenhill/librdkafka/pull/883#issuecomment-257894237,"for issue #878
As the (C++) Conf setters do not forward callbacks in the C struct like with other parameters, special getter should be used to read the callbacks from the Conf.
This update also blocks the original Getter(String&, String&) to get the callbacks from the C struct.","Can't we do the same but with void* instead of the callback object specific
pointer ?
Then there would be just one method to maintain and the caller has to do
the type conversion.
The caller would then do
RdKafka::DeliveryReportCb* myDr_cbPtr;
get(""dr_cb"", (void *) myDr_cbPtr )
I did this commit mainly to have a more complete generated doc, but at the
end it's up to you.
On Wed, Nov 2, 2016 at 4:02 PM, Magnus Edenhill notifications@github.com
wrote:

My only concern is that this becomes a bit verbose and somewhat dreary to
maintain for very little gain.

You are receiving this because you authored the thread.
Reply to this email directly, view it on GitHub
#883 (comment),
or mute the thread
https://github.com/notifications/unsubscribe-auth/ATQE20Guo7ppg6ZprYs7v-ZWoguxsgFkks5q6KXsgaJpZM4KnRD-
.


The information transmitted is intended only for the person or entity to
which it is addressed and may contain confidential and/or privileged
material. Any review, retransmission, dissemination or other use of, or
taking of any action in reliance upon, this information by persons or
entities other than the intended recipient is prohibited. If you received
this in error, please contact the sender and delete the material from any
computer.",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,883,2016-11-02T13:59:32Z,2016-11-17T20:37:29Z,2016-11-17T20:37:32Z,MERGED,True,143,25,2,https://github.com/vin-d,Enrichment of Conf getter for callbacks.,7,[],https://github.com/edenhill/librdkafka/pull/883,https://github.com/vin-d,6,https://github.com/edenhill/librdkafka/pull/883#issuecomment-257936039,"for issue #878
As the (C++) Conf setters do not forward callbacks in the C struct like with other parameters, special getter should be used to read the callbacks from the Conf.
This update also blocks the original Getter(String&, String&) to get the callbacks from the C struct.","I thought this pull request was not a necessity as I thought I could also get the callbacks directly from the conf::impl (as a KafkaConsumer does).
But in fact I can't access ConfImpl as  it's not visible from rdkafkacpp.h
So if you accept the PR or the other proposal that'd be great.",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,883,2016-11-02T13:59:32Z,2016-11-17T20:37:29Z,2016-11-17T20:37:32Z,MERGED,True,143,25,2,https://github.com/vin-d,Enrichment of Conf getter for callbacks.,7,[],https://github.com/edenhill/librdkafka/pull/883,https://github.com/vin-d,7,https://github.com/edenhill/librdkafka/pull/883#issuecomment-260688605,"for issue #878
As the (C++) Conf setters do not forward callbacks in the C struct like with other parameters, special getter should be used to read the callbacks from the Conf.
This update also blocks the original Getter(String&, String&) to get the callbacks from the C struct.","Hi  @edenhill,
I'd like to know if you would finally consider pulling this change.
I understand it increases the code base but the Conf object  will then have the same functionality as its C counterpart that we can't use from C++.
In addition this doesn't add any coupling or drawback to the existing code.
Please let me know, thank you.",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,883,2016-11-02T13:59:32Z,2016-11-17T20:37:29Z,2016-11-17T20:37:32Z,MERGED,True,143,25,2,https://github.com/vin-d,Enrichment of Conf getter for callbacks.,7,[],https://github.com/edenhill/librdkafka/pull/883,https://github.com/edenhill,8,https://github.com/edenhill/librdkafka/pull/883#issuecomment-261362355,"for issue #878
As the (C++) Conf setters do not forward callbacks in the C struct like with other parameters, special getter should be used to read the callbacks from the Conf.
This update also blocks the original Getter(String&, String&) to get the callbacks from the C struct.",Thanks!,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,884,2016-11-02T16:36:16Z,2016-11-02T16:50:58Z,2016-11-02T16:50:58Z,CLOSED,False,14,8,1,https://github.com/vin-d,#882,2,[],https://github.com/edenhill/librdkafka/pull/884,https://github.com/vin-d,1,https://github.com/edenhill/librdkafka/pull/884,"Bugfix a char* to string conversion in Conf::get(string&, string&)","Bugfix a char* to string conversion in Conf::get(string&, string&)",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,884,2016-11-02T16:36:16Z,2016-11-02T16:50:58Z,2016-11-02T16:50:58Z,CLOSED,False,14,8,1,https://github.com/vin-d,#882,2,[],https://github.com/edenhill/librdkafka/pull/884,https://github.com/vin-d,2,https://github.com/edenhill/librdkafka/pull/884#issuecomment-257925834,"Bugfix a char* to string conversion in Conf::get(string&, string&)",bugged commit.,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,885,2016-11-02T16:53:11Z,2016-11-07T08:21:50Z,2016-11-07T08:21:50Z,CLOSED,False,14,8,1,https://github.com/vin-d,minor bug in RdKafka::Conf::get() string update of parameter value,2,[],https://github.com/edenhill/librdkafka/pull/885,https://github.com/vin-d,1,https://github.com/edenhill/librdkafka/pull/885,minor bug in RdKafka::Conf::get() string update of parameter value,minor bug in RdKafka::Conf::get() string update of parameter value,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,885,2016-11-02T16:53:11Z,2016-11-07T08:21:50Z,2016-11-07T08:21:50Z,CLOSED,False,14,8,1,https://github.com/vin-d,minor bug in RdKafka::Conf::get() string update of parameter value,2,[],https://github.com/edenhill/librdkafka/pull/885,https://github.com/edenhill,2,https://github.com/edenhill/librdkafka/pull/885#issuecomment-257963236,minor bug in RdKafka::Conf::get() string update of parameter value,https://ci.appveyor.com/project/edenhill/librdkafka/build/0.9.1-post506/job/q99wub89jbox9jia#L1109,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,885,2016-11-02T16:53:11Z,2016-11-07T08:21:50Z,2016-11-07T08:21:50Z,CLOSED,False,14,8,1,https://github.com/vin-d,minor bug in RdKafka::Conf::get() string update of parameter value,2,[],https://github.com/edenhill/librdkafka/pull/885,https://github.com/vin-d,3,https://github.com/edenhill/librdkafka/pull/885#issuecomment-258087943,minor bug in RdKafka::Conf::get() string update of parameter value,"Yes I've seen that.
Unfortunately I don't know how to have that test on the commits I do on my
fork of librdkafka.
https://ci.appveyor.com/project/vin-d/librdkafka/build/1.0.2
and I can test it doing PR but then I don't want to spam you just to
perform some CI tests.
Could you help me to set the test on my forked project ?
On Wed, Nov 2, 2016 at 7:51 PM, Magnus Edenhill notifications@github.com
wrote:

https://ci.appveyor.com/project/edenhill/librdkafka/
build/0.9.1-post506/job/q99wub89jbox9jia#L1109

You are receiving this because you authored the thread.
Reply to this email directly, view it on GitHub
#885 (comment),
or mute the thread
https://github.com/notifications/unsubscribe-auth/ATQE2zrZLKwBakA44i4YSoCvb2uIxLTrks5q6NuUgaJpZM4KnevY
.


The information transmitted is intended only for the person or entity to
which it is addressed and may contain confidential and/or privileged
material. Any review, retransmission, dissemination or other use of, or
taking of any action in reliance upon, this information by persons or
entities other than the intended recipient is prohibited. If you received
this in error, please contact the sender and delete the material from any
computer.",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,885,2016-11-02T16:53:11Z,2016-11-07T08:21:50Z,2016-11-07T08:21:50Z,CLOSED,False,14,8,1,https://github.com/vin-d,minor bug in RdKafka::Conf::get() string update of parameter value,2,[],https://github.com/edenhill/librdkafka/pull/885,https://github.com/vin-d,4,https://github.com/edenhill/librdkafka/pull/885#issuecomment-258460701,minor bug in RdKafka::Conf::get() string update of parameter value,If I understood clearly there should be a appveyor.yml config file in the git repository (as there is a .travis.yml) but I don't see any I don't have the environment to test commits on my fork then.,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,885,2016-11-02T16:53:11Z,2016-11-07T08:21:50Z,2016-11-07T08:21:50Z,CLOSED,False,14,8,1,https://github.com/vin-d,minor bug in RdKafka::Conf::get() string update of parameter value,2,[],https://github.com/edenhill/librdkafka/pull/885,https://github.com/vin-d,5,https://github.com/edenhill/librdkafka/pull/885#issuecomment-258772646,minor bug in RdKafka::Conf::get() string update of parameter value,close because of appveyor failure.,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,903,2016-11-14T10:23:55Z,2016-11-14T10:29:29Z,2016-11-14T10:29:29Z,CLOSED,False,8,0,2,https://github.com/skarlsson,adds check of defined LIBKAFKARD_STATICLIB to enable static library b,1,[],https://github.com/edenhill/librdkafka/pull/903,https://github.com/skarlsson,1,https://github.com/edenhill/librdkafka/pull/903,I currently build libkafkard out of source with cmake but it would be convenient to make it possible to build  static libs. This pull request only enables that - it does not change anything for existing builds.,I currently build libkafkard out of source with cmake but it would be convenient to make it possible to build  static libs. This pull request only enables that - it does not change anything for existing builds.,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,903,2016-11-14T10:23:55Z,2016-11-14T10:29:29Z,2016-11-14T10:29:29Z,CLOSED,False,8,0,2,https://github.com/skarlsson,adds check of defined LIBKAFKARD_STATICLIB to enable static library b,1,[],https://github.com/edenhill/librdkafka/pull/903,https://github.com/skarlsson,2,https://github.com/edenhill/librdkafka/pull/903#issuecomment-260300487,I currently build libkafkard out of source with cmake but it would be convenient to make it possible to build  static libs. This pull request only enables that - it does not change anything for existing builds.,oops typo should be LIBRDKAFKA_STATICLIB closing and fixing..,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,905,2016-11-14T10:40:01Z,2016-11-14T22:09:14Z,2016-11-14T22:09:26Z,MERGED,True,12,0,3,https://github.com/skarlsson,adds check of defined LIBRDKAFKA_STATICLIB to enable static library,3,[],https://github.com/edenhill/librdkafka/pull/905,https://github.com/skarlsson,1,https://github.com/edenhill/librdkafka/pull/905,I currently build librdkafka out of source with cmake but it would be convenient to make it possible to build static libs. This pull request only enables that - it does not change anything for existing builds.,I currently build librdkafka out of source with cmake but it would be convenient to make it possible to build static libs. This pull request only enables that - it does not change anything for existing builds.,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,905,2016-11-14T10:40:01Z,2016-11-14T22:09:14Z,2016-11-14T22:09:26Z,MERGED,True,12,0,3,https://github.com/skarlsson,adds check of defined LIBRDKAFKA_STATICLIB to enable static library,3,[],https://github.com/edenhill/librdkafka/pull/905,https://github.com/edenhill,2,https://github.com/edenhill/librdkafka/pull/905#issuecomment-260369017,I currently build librdkafka out of source with cmake but it would be convenient to make it possible to build static libs. This pull request only enables that - it does not change anything for existing builds.,"Looks good, can you add a small snippet to README.win32 on when (and how) to use LIBRDKAFKA_STATICLIB?",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,905,2016-11-14T10:40:01Z,2016-11-14T22:09:14Z,2016-11-14T22:09:26Z,MERGED,True,12,0,3,https://github.com/skarlsson,adds check of defined LIBRDKAFKA_STATICLIB to enable static library,3,[],https://github.com/edenhill/librdkafka/pull/905,https://github.com/skarlsson,3,https://github.com/edenhill/librdkafka/pull/905#issuecomment-260377756,I currently build librdkafka out of source with cmake but it would be convenient to make it possible to build static libs. This pull request only enables that - it does not change anything for existing builds.,"sure, but as I said I needed to build outside of librdkafka source tree so i ended up just globbing all files except rdkafka_sasl.c
this is what i did in my CmakeLists.txt
...
file(GLOB librdkafka_files
${CMAKE_CURRENT_SOURCE_DIR}/../librdkafka/src/.c
${CMAKE_CURRENT_SOURCE_DIR}/../librdkafka/src/.h
)
file(GLOB librdkafka_cpp_files
${CMAKE_CURRENT_SOURCE_DIR}/../librdkafka/src-cpp/.cpp
${CMAKE_CURRENT_SOURCE_DIR}/../librdkafka/src-cpp/.h
)
list(REMOVE_ITEM librdkafka_files ${CMAKE_CURRENT_SOURCE_DIR}/../librdkafka/src/rdkafka_sasl.c)
add_definitions(-DLIBRDKAFKA_STATICLIB)
add_library(librdkafka STATIC ${librdkafka_files})
add_library(librdkafkacpp STATIC ${librdkafka_cpp_files})
Do you want this alternative build explained in the README.win32?
And BTW I also noticed that that openssl 1.1.0+ changed the lib names",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,905,2016-11-14T10:40:01Z,2016-11-14T22:09:14Z,2016-11-14T22:09:26Z,MERGED,True,12,0,3,https://github.com/skarlsson,adds check of defined LIBRDKAFKA_STATICLIB to enable static library,3,[],https://github.com/edenhill/librdkafka/pull/905,https://github.com/edenhill,4,https://github.com/edenhill/librdkafka/pull/905#issuecomment-260378782,I currently build librdkafka out of source with cmake but it would be convenient to make it possible to build static libs. This pull request only enables that - it does not change anything for existing builds.,"There's ongoing work by @olibre to add proper cmake support to librdkafka, as an alternative to VS and mklove: olibre/cmake-for-rdkafka#1
I dont think you need to describe the cmake process, just mention the define and why it is needed.
And librdkafka only supports OpenSSL 1.0 at this point.",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,905,2016-11-14T10:40:01Z,2016-11-14T22:09:14Z,2016-11-14T22:09:26Z,MERGED,True,12,0,3,https://github.com/skarlsson,adds check of defined LIBRDKAFKA_STATICLIB to enable static library,3,[],https://github.com/edenhill/librdkafka/pull/905,https://github.com/skarlsson,5,https://github.com/edenhill/librdkafka/pull/905#issuecomment-260380662,I currently build librdkafka out of source with cmake but it would be convenient to make it possible to build static libs. This pull request only enables that - it does not change anything for existing builds.,"Sure, I'll add a comment.
I did not test running anything against a ssl enabled kafka broker but everything compiles with 1.1.0c anv visual studio 2015 update 2 (and runs fine against a normal kafka)",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,905,2016-11-14T10:40:01Z,2016-11-14T22:09:14Z,2016-11-14T22:09:26Z,MERGED,True,12,0,3,https://github.com/skarlsson,adds check of defined LIBRDKAFKA_STATICLIB to enable static library,3,[],https://github.com/edenhill/librdkafka/pull/905,https://github.com/edenhill,6,https://github.com/edenhill/librdkafka/pull/905#issuecomment-260479375,I currently build librdkafka out of source with cmake but it would be convenient to make it possible to build static libs. This pull request only enables that - it does not change anything for existing builds.,Thans Svante!,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,906,2016-11-14T16:02:31Z,2016-11-14T16:10:18Z,2016-11-14T16:10:40Z,MERGED,True,2,0,1,https://github.com/9il,Add note about Dlang bindings,1,[],https://github.com/edenhill/librdkafka/pull/906,https://github.com/9il,1,https://github.com/edenhill/librdkafka/pull/906,,,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,906,2016-11-14T16:02:31Z,2016-11-14T16:10:18Z,2016-11-14T16:10:40Z,MERGED,True,2,0,1,https://github.com/9il,Add note about Dlang bindings,1,[],https://github.com/edenhill/librdkafka/pull/906,https://github.com/edenhill,2,https://github.com/edenhill/librdkafka/pull/906#issuecomment-260379082,,Thank you!,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,921,2016-11-25T02:32:56Z,2016-11-25T07:05:28Z,2016-11-25T07:05:31Z,MERGED,True,1,1,1,https://github.com/HeChuanXUPT,update comment,1,[],https://github.com/edenhill/librdkafka/pull/921,https://github.com/HeChuanXUPT,1,https://github.com/edenhill/librdkafka/pull/921,update comment,update comment,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,921,2016-11-25T02:32:56Z,2016-11-25T07:05:28Z,2016-11-25T07:05:31Z,MERGED,True,1,1,1,https://github.com/HeChuanXUPT,update comment,1,[],https://github.com/edenhill/librdkafka/pull/921,https://github.com/edenhill,2,https://github.com/edenhill/librdkafka/pull/921#issuecomment-262893100,update comment,Thanks!,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,924,2016-11-28T12:53:01Z,2016-11-28T13:03:17Z,2016-11-28T13:03:20Z,MERGED,True,1,1,1,https://github.com/HeChuanXUPT,Update INTRODUCTION.md,1,[],https://github.com/edenhill/librdkafka/pull/924,https://github.com/HeChuanXUPT,1,https://github.com/edenhill/librdkafka/pull/924,fix wrong batch.num.messages default value,fix wrong batch.num.messages default value,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,924,2016-11-28T12:53:01Z,2016-11-28T13:03:17Z,2016-11-28T13:03:20Z,MERGED,True,1,1,1,https://github.com/HeChuanXUPT,Update INTRODUCTION.md,1,[],https://github.com/edenhill/librdkafka/pull/924,https://github.com/edenhill,2,https://github.com/edenhill/librdkafka/pull/924#issuecomment-263265945,fix wrong batch.num.messages default value,Thanks,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,933,2016-12-02T17:39:17Z,2016-12-02T18:29:58Z,2016-12-02T18:29:58Z,MERGED,True,1,0,1,https://github.com/fede1024,Add link to Rust bindings,1,[],https://github.com/edenhill/librdkafka/pull/933,https://github.com/fede1024,1,https://github.com/edenhill/librdkafka/pull/933,Add link to Rust bindings.,Add link to Rust bindings.,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,933,2016-12-02T17:39:17Z,2016-12-02T18:29:58Z,2016-12-02T18:29:58Z,MERGED,True,1,0,1,https://github.com/fede1024,Add link to Rust bindings,1,[],https://github.com/edenhill/librdkafka/pull/933,https://github.com/edenhill,2,https://github.com/edenhill/librdkafka/pull/933#issuecomment-264526517,Add link to Rust bindings.,Awesome!,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,936,2016-12-05T08:05:19Z,2016-12-06T13:50:48Z,2016-12-06T13:51:19Z,MERGED,True,4,1,1,https://github.com/9il,fix issue #934,3,[],https://github.com/edenhill/librdkafka/pull/936,https://github.com/9il,1,https://github.com/edenhill/librdkafka/pull/936,,,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,936,2016-12-05T08:05:19Z,2016-12-06T13:50:48Z,2016-12-06T13:51:19Z,MERGED,True,4,1,1,https://github.com/9il,fix issue #934,3,[],https://github.com/edenhill/librdkafka/pull/936,https://github.com/9il,2,https://github.com/edenhill/librdkafka/pull/936#issuecomment-265152850,,Done. Tabs was replaced in the second commit.,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,936,2016-12-05T08:05:19Z,2016-12-06T13:50:48Z,2016-12-06T13:51:19Z,MERGED,True,4,1,1,https://github.com/9il,fix issue #934,3,[],https://github.com/edenhill/librdkafka/pull/936,https://github.com/edenhill,3,https://github.com/edenhill/librdkafka/pull/936#issuecomment-265154046,,Thanks!,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,953,2016-12-14T21:57:01Z,2017-01-04T15:29:55Z,2017-01-11T13:23:57Z,MERGED,True,455,9,13,https://github.com/lambdaknight,Multithread partition read support,2,[],https://github.com/edenhill/librdkafka/pull/953,https://github.com/lambdaknight,1,https://github.com/edenhill/librdkafka/pull/953,Add some functionality to support for consuming from partitions using multiple threads.,Add some functionality to support for consuming from partitions using multiple threads.,True,{'HEART': ['https://github.com/jeffwidman']}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,953,2016-12-14T21:57:01Z,2017-01-04T15:29:55Z,2017-01-11T13:23:57Z,MERGED,True,455,9,13,https://github.com/lambdaknight,Multithread partition read support,2,[],https://github.com/edenhill/librdkafka/pull/953,https://github.com/edenhill,2,https://github.com/edenhill/librdkafka/pull/953#issuecomment-267750277,Add some functionality to support for consuming from partitions using multiple threads.,"This doesnt compile cleanly:
https://travis-ci.org/edenhill/librdkafka/jobs/184577261#L287
Are you sure this passes the full test suite?",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,956,2016-12-16T14:07:12Z,2016-12-16T14:10:51Z,2016-12-16T14:20:00Z,MERGED,True,2,1,1,https://github.com/andoma,Add doozer build badge,1,[],https://github.com/edenhill/librdkafka/pull/956,https://github.com/andoma,1,https://github.com/edenhill/librdkafka/pull/956,,,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,956,2016-12-16T14:07:12Z,2016-12-16T14:10:51Z,2016-12-16T14:20:00Z,MERGED,True,2,1,1,https://github.com/andoma,Add doozer build badge,1,[],https://github.com/edenhill/librdkafka/pull/956,https://github.com/edenhill,2,https://github.com/edenhill/librdkafka/pull/956#issuecomment-267601831,,Thanks so much for this!,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,956,2016-12-16T14:07:12Z,2016-12-16T14:10:51Z,2016-12-16T14:20:00Z,MERGED,True,2,1,1,https://github.com/andoma,Add doozer build badge,1,[],https://github.com/edenhill/librdkafka/pull/956,https://github.com/andoma,3,https://github.com/edenhill/librdkafka/pull/956#issuecomment-267603584,,What can I say?,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,962,2016-12-20T15:35:10Z,2016-12-21T15:36:28Z,2016-12-21T16:17:31Z,MERGED,True,41,27,2,https://github.com/bbfgelman1,Fix undefined references to static const ints (961),1,[],https://github.com/edenhill/librdkafka/pull/962,https://github.com/bbfgelman1,1,https://github.com/edenhill/librdkafka/pull/962,"This patch fixes an issue when compiling code with librdkafka
with all optimizations disabled. The library declares a number
of static constant integer values in headers, but not in
compiled files. This causes any code that passes these constants
directly to a function accepting a const reference to result in
undefined reference errors when linking. The error is fixed by
also declaring the variables in their corresponding .cpp files.
This issue affects GCC when compiling with optimizations disabled.","This patch fixes an issue when compiling code with librdkafka
with all optimizations disabled. The library declares a number
of static constant integer values in headers, but not in
compiled files. This causes any code that passes these constants
directly to a function accepting a const reference to result in
undefined reference errors when linking. The error is fixed by
also declaring the variables in their corresponding .cpp files.
This issue affects GCC when compiling with optimizations disabled.",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,962,2016-12-20T15:35:10Z,2016-12-21T15:36:28Z,2016-12-21T16:17:31Z,MERGED,True,41,27,2,https://github.com/bbfgelman1,Fix undefined references to static const ints (961),1,[],https://github.com/edenhill/librdkafka/pull/962,https://github.com/bbfgelman1,2,https://github.com/edenhill/librdkafka/pull/962#issuecomment-268277668,"This patch fixes an issue when compiling code with librdkafka
with all optimizations disabled. The library declares a number
of static constant integer values in headers, but not in
compiled files. This causes any code that passes these constants
directly to a function accepting a const reference to result in
undefined reference errors when linking. The error is fixed by
also declaring the variables in their corresponding .cpp files.
This issue affects GCC when compiling with optimizations disabled.",This fixes the issue #961,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,962,2016-12-20T15:35:10Z,2016-12-21T15:36:28Z,2016-12-21T16:17:31Z,MERGED,True,41,27,2,https://github.com/bbfgelman1,Fix undefined references to static const ints (961),1,[],https://github.com/edenhill/librdkafka/pull/962,https://github.com/bbfgelman1,3,https://github.com/edenhill/librdkafka/pull/962#issuecomment-268296982,"This patch fixes an issue when compiling code with librdkafka
with all optimizations disabled. The library declares a number
of static constant integer values in headers, but not in
compiled files. This causes any code that passes these constants
directly to a function accepting a const reference to result in
undefined reference errors when linking. The error is fixed by
also declaring the variables in their corresponding .cpp files.
This issue affects GCC when compiling with optimizations disabled.","I changed the constants in RdKafka::Producer from using static const int to using an anonymous enum. They can be referred to in the same way as before (RdKafka::Producer::RK_MSG_FREE), but the names now behave like literal values rather than static member variables.
I did not apply the same change to the constants in RdKafka::Topic so that their explicit types are preserved.",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,962,2016-12-20T15:35:10Z,2016-12-21T15:36:28Z,2016-12-21T16:17:31Z,MERGED,True,41,27,2,https://github.com/bbfgelman1,Fix undefined references to static const ints (961),1,[],https://github.com/edenhill/librdkafka/pull/962,https://github.com/edenhill,4,https://github.com/edenhill/librdkafka/pull/962#issuecomment-268299984,"This patch fixes an issue when compiling code with librdkafka
with all optimizations disabled. The library declares a number
of static constant integer values in headers, but not in
compiled files. This causes any code that passes these constants
directly to a function accepting a const reference to result in
undefined reference errors when linking. The error is fixed by
also declaring the variables in their corresponding .cpp files.
This issue affects GCC when compiling with optimizations disabled.","Fails on VS 2013:
https://ci.appveyor.com/project/edenhill/librdkafka/build/0.9.3-pre-54-ucuvrpyw/job/o9k2domaj75thde1#L189",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,962,2016-12-20T15:35:10Z,2016-12-21T15:36:28Z,2016-12-21T16:17:31Z,MERGED,True,41,27,2,https://github.com/bbfgelman1,Fix undefined references to static const ints (961),1,[],https://github.com/edenhill/librdkafka/pull/962,https://github.com/bbfgelman1,5,https://github.com/edenhill/librdkafka/pull/962#issuecomment-268319450,"This patch fixes an issue when compiling code with librdkafka
with all optimizations disabled. The library declares a number
of static constant integer values in headers, but not in
compiled files. This causes any code that passes these constants
directly to a function accepting a const reference to result in
undefined reference errors when linking. The error is fixed by
also declaring the variables in their corresponding .cpp files.
This issue affects GCC when compiling with optimizations disabled.","C++17 will introduce inline variables to address exactly this problem. Until that is widely supported, please consider the solution implemented in this PR.",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,962,2016-12-20T15:35:10Z,2016-12-21T15:36:28Z,2016-12-21T16:17:31Z,MERGED,True,41,27,2,https://github.com/bbfgelman1,Fix undefined references to static const ints (961),1,[],https://github.com/edenhill/librdkafka/pull/962,https://github.com/bbfgelman1,6,https://github.com/edenhill/librdkafka/pull/962#issuecomment-268529161,"This patch fixes an issue when compiling code with librdkafka
with all optimizations disabled. The library declares a number
of static constant integer values in headers, but not in
compiled files. This causes any code that passes these constants
directly to a function accepting a const reference to result in
undefined reference errors when linking. The error is fixed by
also declaring the variables in their corresponding .cpp files.
This issue affects GCC when compiling with optimizations disabled.",Would you like me to squash all the commits in this PR?,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,962,2016-12-20T15:35:10Z,2016-12-21T15:36:28Z,2016-12-21T16:17:31Z,MERGED,True,41,27,2,https://github.com/bbfgelman1,Fix undefined references to static const ints (961),1,[],https://github.com/edenhill/librdkafka/pull/962,https://github.com/edenhill,7,https://github.com/edenhill/librdkafka/pull/962#issuecomment-268540824,"This patch fixes an issue when compiling code with librdkafka
with all optimizations disabled. The library declares a number
of static constant integer values in headers, but not in
compiled files. This causes any code that passes these constants
directly to a function accepting a const reference to result in
undefined reference errors when linking. The error is fixed by
also declaring the variables in their corresponding .cpp files.
This issue affects GCC when compiling with optimizations disabled.",Yes please!,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,962,2016-12-20T15:35:10Z,2016-12-21T15:36:28Z,2016-12-21T16:17:31Z,MERGED,True,41,27,2,https://github.com/bbfgelman1,Fix undefined references to static const ints (961),1,[],https://github.com/edenhill/librdkafka/pull/962,https://github.com/edenhill,8,https://github.com/edenhill/librdkafka/pull/962#issuecomment-268552953,"This patch fixes an issue when compiling code with librdkafka
with all optimizations disabled. The library declares a number
of static constant integer values in headers, but not in
compiled files. This causes any code that passes these constants
directly to a function accepting a const reference to result in
undefined reference errors when linking. The error is fixed by
also declaring the variables in their corresponding .cpp files.
This issue affects GCC when compiling with optimizations disabled.",Thanks alot!,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,962,2016-12-20T15:35:10Z,2016-12-21T15:36:28Z,2016-12-21T16:17:31Z,MERGED,True,41,27,2,https://github.com/bbfgelman1,Fix undefined references to static const ints (961),1,[],https://github.com/edenhill/librdkafka/pull/962,https://github.com/bbfgelman1,9,https://github.com/edenhill/librdkafka/pull/962#issuecomment-268563936,"This patch fixes an issue when compiling code with librdkafka
with all optimizations disabled. The library declares a number
of static constant integer values in headers, but not in
compiled files. This causes any code that passes these constants
directly to a function accepting a const reference to result in
undefined reference errors when linking. The error is fixed by
also declaring the variables in their corresponding .cpp files.
This issue affects GCC when compiling with optimizations disabled.",Thanks for merging!,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,974,2016-12-29T13:40:49Z,2017-01-03T10:18:33Z,2017-01-03T10:18:37Z,MERGED,True,1,1,1,https://github.com/HeChuanXUPT,fix: use out of scope variable,1,[],https://github.com/edenhill/librdkafka/pull/974,https://github.com/HeChuanXUPT,1,https://github.com/edenhill/librdkafka/pull/974,"rd_kafka_broker_send may use the address of msg2, out of  scope","rd_kafka_broker_send may use the address of msg2, out of  scope",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,974,2016-12-29T13:40:49Z,2017-01-03T10:18:33Z,2017-01-03T10:18:37Z,MERGED,True,1,1,1,https://github.com/HeChuanXUPT,fix: use out of scope variable,1,[],https://github.com/edenhill/librdkafka/pull/974,https://github.com/HeChuanXUPT,2,https://github.com/edenhill/librdkafka/pull/974#issuecomment-269636924,"rd_kafka_broker_send may use the address of msg2, out of  scope","#861
has mention it too",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,974,2016-12-29T13:40:49Z,2017-01-03T10:18:33Z,2017-01-03T10:18:37Z,MERGED,True,1,1,1,https://github.com/HeChuanXUPT,fix: use out of scope variable,1,[],https://github.com/edenhill/librdkafka/pull/974,https://github.com/edenhill,3,https://github.com/edenhill/librdkafka/pull/974#issuecomment-270084508,"rd_kafka_broker_send may use the address of msg2, out of  scope",Thanks!,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,979,2017-01-04T04:15:16Z,2017-01-04T08:40:56Z,2017-01-04T08:40:56Z,CLOSED,False,19,3,1,https://github.com/HeChuanXUPT,validate topic name when add a new one,1,[],https://github.com/edenhill/librdkafka/pull/979,https://github.com/HeChuanXUPT,1,https://github.com/edenhill/librdkafka/pull/979,"topic name length is not greater then 255 , and in the Kafka 0.10 is 249
when add a new topic name should check validity","topic name length is not greater then 255 , and in the Kafka 0.10 is 249
when add a new topic name should check validity",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,979,2017-01-04T04:15:16Z,2017-01-04T08:40:56Z,2017-01-04T08:40:56Z,CLOSED,False,19,3,1,https://github.com/HeChuanXUPT,validate topic name when add a new one,1,[],https://github.com/edenhill/librdkafka/pull/979,https://github.com/edenhill,2,https://github.com/edenhill/librdkafka/pull/979#issuecomment-270320597,"topic name length is not greater then 255 , and in the Kafka 0.10 is 249
when add a new topic name should check validity",I dont want the topic name to be enforced in the client since an old client must work with a newer broker that might have less strict topic name requirements.,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,996,2017-01-13T17:15:21Z,2017-01-16T17:23:12Z,2017-01-16T20:10:14Z,CLOSED,False,0,0,0,https://github.com/zyzil,Win32 sspi,0,[],https://github.com/edenhill/librdkafka/pull/996,https://github.com/zyzil,1,https://github.com/edenhill/librdkafka/pull/996,"I need to be able to use rdkafka-dotnet with a Kerberos enabled Kafka system and found the work you had started on using SSPI on Windows.  I have continued that work and with these changes can now authenticate via Kerberos/SASL and produce to our Kafka system.  The win32_sspi branch was quite a bit behind master, so I merged master into the branch before making the changes.  I was hoping that this would make it easier to get these changes back to master; however, please let me know if this approach is not favorable and I can do whatever is necessary to correct it.
Thanks!
Kevin","I need to be able to use rdkafka-dotnet with a Kerberos enabled Kafka system and found the work you had started on using SSPI on Windows.  I have continued that work and with these changes can now authenticate via Kerberos/SASL and produce to our Kafka system.  The win32_sspi branch was quite a bit behind master, so I merged master into the branch before making the changes.  I was hoping that this would make it easier to get these changes back to master; however, please let me know if this approach is not favorable and I can do whatever is necessary to correct it.
Thanks!
Kevin",True,{'HOORAY': ['https://github.com/ah-']}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,996,2017-01-13T17:15:21Z,2017-01-16T17:23:12Z,2017-01-16T20:10:14Z,CLOSED,False,0,0,0,https://github.com/zyzil,Win32 sspi,0,[],https://github.com/edenhill/librdkafka/pull/996,https://github.com/edenhill,2,https://github.com/edenhill/librdkafka/pull/996#issuecomment-272496901,"I need to be able to use rdkafka-dotnet with a Kerberos enabled Kafka system and found the work you had started on using SSPI on Windows.  I have continued that work and with these changes can now authenticate via Kerberos/SASL and produce to our Kafka system.  The win32_sspi branch was quite a bit behind master, so I merged master into the branch before making the changes.  I was hoping that this would make it easier to get these changes back to master; however, please let me know if this approach is not favorable and I can do whatever is necessary to correct it.
Thanks!
Kevin","Wow, interesting!
Rebase
First a nitpick: can you rebase from master instead of merging it, to avoid all the extra commits?
Do this:
git checkout master
git pull origin master
git checkout sspi_branch..whatever
git rebase master
git push --force origin sspi_branch_whatever  # overwrites previous history in branch

What, how, when, why, who
I never got SSPI to work so I'm very excited about this.
Please tell me about your setup:

are you using Windows AD to authenticate, or Kerberos KDC?
are your browsers running on Windows aswell?
what does your librdkafka client config look like?",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,996,2017-01-13T17:15:21Z,2017-01-16T17:23:12Z,2017-01-16T20:10:14Z,CLOSED,False,0,0,0,https://github.com/zyzil,Win32 sspi,0,[],https://github.com/edenhill/librdkafka/pull/996,https://github.com/zyzil,3,https://github.com/edenhill/librdkafka/pull/996#issuecomment-272508013,"I need to be able to use rdkafka-dotnet with a Kerberos enabled Kafka system and found the work you had started on using SSPI on Windows.  I have continued that work and with these changes can now authenticate via Kerberos/SASL and produce to our Kafka system.  The win32_sspi branch was quite a bit behind master, so I merged master into the branch before making the changes.  I was hoping that this would make it easier to get these changes back to master; however, please let me know if this approach is not favorable and I can do whatever is necessary to correct it.
Thanks!
Kevin","Sorry, I'm still working out the rebase.
To answer your questions:
I am  using Windows Active Directory to authenticate.
Kafka is running on a kerberos enabled Cloudera cluster running on Oracle Linux 6.8.
I am using the rdkafka_example to produce message.  The command line arguments look like this:
-P -t librdkafka -b kafka.mydomain.net:9092 -X security.protocol=SASL_PLAINTEXT -X sasl.kerberos.service.name=kafka/kafka.mydomain.net@MYDOMAIN.NET -X sasl.kerberos.principal=kafka -d security,protocol,broker.",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,996,2017-01-13T17:15:21Z,2017-01-16T17:23:12Z,2017-01-16T20:10:14Z,CLOSED,False,0,0,0,https://github.com/zyzil,Win32 sspi,0,[],https://github.com/edenhill/librdkafka/pull/996,https://github.com/edenhill,4,https://github.com/edenhill/librdkafka/pull/996#issuecomment-272887937,"I need to be able to use rdkafka-dotnet with a Kerberos enabled Kafka system and found the work you had started on using SSPI on Windows.  I have continued that work and with these changes can now authenticate via Kerberos/SASL and produce to our Kafka system.  The win32_sspi branch was quite a bit behind master, so I merged master into the branch before making the changes.  I was hoping that this would make it easier to get these changes back to master; however, please let me know if this approach is not favorable and I can do whatever is necessary to correct it.
Thanks!
Kevin","I'm not really sure what you did there, but now all commits (your own and the branch diff) is squashed up in one single commit :/.
Let me rebase the win32_sspi branch for you, then you fetch that and apply your changes (and your changes only) cleanly on that and push that to the PR.",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,996,2017-01-13T17:15:21Z,2017-01-16T17:23:12Z,2017-01-16T20:10:14Z,CLOSED,False,0,0,0,https://github.com/zyzil,Win32 sspi,0,[],https://github.com/edenhill/librdkafka/pull/996,https://github.com/zyzil,5,https://github.com/edenhill/librdkafka/pull/996#issuecomment-272916136,"I need to be able to use rdkafka-dotnet with a Kerberos enabled Kafka system and found the work you had started on using SSPI on Windows.  I have continued that work and with these changes can now authenticate via Kerberos/SASL and produce to our Kafka system.  The win32_sspi branch was quite a bit behind master, so I merged master into the branch before making the changes.  I was hoping that this would make it easier to get these changes back to master; however, please let me know if this approach is not favorable and I can do whatever is necessary to correct it.
Thanks!
Kevin","I'm still relatively new to Git.  I tried rebasing with your suggestions but it still wanted to have all the individual commits.  Sorry for the inconvenience.
Kevin",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,996,2017-01-13T17:15:21Z,2017-01-16T17:23:12Z,2017-01-16T20:10:14Z,CLOSED,False,0,0,0,https://github.com/zyzil,Win32 sspi,0,[],https://github.com/edenhill/librdkafka/pull/996,https://github.com/edenhill,6,https://github.com/edenhill/librdkafka/pull/996#issuecomment-272951934,"I need to be able to use rdkafka-dotnet with a Kerberos enabled Kafka system and found the work you had started on using SSPI on Windows.  I have continued that work and with these changes can now authenticate via Kerberos/SASL and produce to our Kafka system.  The win32_sspi branch was quite a bit behind master, so I merged master into the branch before making the changes.  I was hoping that this would make it easier to get these changes back to master; however, please let me know if this approach is not favorable and I can do whatever is necessary to correct it.
Thanks!
Kevin","@zyzil Git is hard, no need to apoligize.
I've rebased the win32_sspi branch (force-pushed) now.
The easiest approach for you now is probably to make a clean git clone of librdkafka, check out the updated win32_sspi branch, and the copy your stuff over to that repo and commit it. This should result in a diff only containing your fixes. Then you create a PR of that.
Let me know if you need assistance.",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,1003,2017-01-16T22:12:22Z,2017-01-23T09:13:42Z,2017-01-31T14:19:55Z,MERGED,True,205,22,1,https://github.com/zyzil,SASL: Windows SASL implementation via SSPI (#888),1,[],https://github.com/edenhill/librdkafka/pull/1003,https://github.com/zyzil,1,https://github.com/edenhill/librdkafka/pull/1003,"Additional changes to support Kerberos authentication from Windows
using SSPI.","Additional changes to support Kerberos authentication from Windows
using SSPI.",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,1003,2017-01-16T22:12:22Z,2017-01-23T09:13:42Z,2017-01-31T14:19:55Z,MERGED,True,205,22,1,https://github.com/zyzil,SASL: Windows SASL implementation via SSPI (#888),1,[],https://github.com/edenhill/librdkafka/pull/1003,https://github.com/zyzil,2,https://github.com/edenhill/librdkafka/pull/1003#issuecomment-272975330,"Additional changes to support Kerberos authentication from Windows
using SSPI.","With this changes I am able to successfully authenticate with Kafka from a Windows machine.  Here are some details about the environment:
I am using Windows Active Directory to authenticate.
Kafka is running on a kerberos enabled Cloudera cluster running on Oracle Linux 6.8.
I am using the rdkafka_example to produce message. The command line arguments look like this:
-P -t librdkafka -b kafka.mydomain.net:9092 -X security.protocol=SASL_PLAINTEXT -X sasl.kerberos.service.name=kafka/kafka.mydomain.net@MYDOMAIN.NET -X sasl.kerberos.principal=kafka -d security,protocol,broker.",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,1003,2017-01-16T22:12:22Z,2017-01-23T09:13:42Z,2017-01-31T14:19:55Z,MERGED,True,205,22,1,https://github.com/zyzil,SASL: Windows SASL implementation via SSPI (#888),1,[],https://github.com/edenhill/librdkafka/pull/1003,https://github.com/edenhill,3,https://github.com/edenhill/librdkafka/pull/1003#issuecomment-273058050,"Additional changes to support Kerberos authentication from Windows
using SSPI.","You know what would be really great? If you could write a blog post outlining the steps needed to deploy this in practice, including broker config, any AD specific stuff, etc.
I think there are alot of people that are looking into this setup but are thrown off due to lack of information and guides.",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,1003,2017-01-16T22:12:22Z,2017-01-23T09:13:42Z,2017-01-31T14:19:55Z,MERGED,True,205,22,1,https://github.com/zyzil,SASL: Windows SASL implementation via SSPI (#888),1,[],https://github.com/edenhill/librdkafka/pull/1003,https://github.com/zyzil,4,https://github.com/edenhill/librdkafka/pull/1003#issuecomment-273152049,"Additional changes to support Kerberos authentication from Windows
using SSPI.","I can definitely understand.  Anything Kerberos related ends up being a daunting task and throw AD into the mix and you have even more fun.  I don't have a full picture of the steps currently because others helped setup the servers, but I wouldn't mind learning more about it.  I can create a small test domain and integrate ZooKeeper/Kafka into it via Kerberos/AD.  That should be easy enough to document.  I'll make it a little pet project and let you know when I have some documentation.",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,1003,2017-01-16T22:12:22Z,2017-01-23T09:13:42Z,2017-01-31T14:19:55Z,MERGED,True,205,22,1,https://github.com/zyzil,SASL: Windows SASL implementation via SSPI (#888),1,[],https://github.com/edenhill/librdkafka/pull/1003,https://github.com/edenhill,5,https://github.com/edenhill/librdkafka/pull/1003#issuecomment-273174907,"Additional changes to support Kerberos authentication from Windows
using SSPI.",@MaximGurschi Do you want to try this PR out in your setup?,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,1003,2017-01-16T22:12:22Z,2017-01-23T09:13:42Z,2017-01-31T14:19:55Z,MERGED,True,205,22,1,https://github.com/zyzil,SASL: Windows SASL implementation via SSPI (#888),1,[],https://github.com/edenhill/librdkafka/pull/1003,https://github.com/MaximGurschi,6,https://github.com/edenhill/librdkafka/pull/1003#issuecomment-273212735,"Additional changes to support Kerberos authentication from Windows
using SSPI.","It works! It lists the topics now. Had to use ""sasl.kerberos.service.name=kafka/kafka.mydomain.net@MYDOMAIN.NET -X sasl.kerberos.principal=kafka"" as zyzil pointed out.
So what next Eden? Props to zyzil for adding the missing piece!",True,"{'HEART': ['https://github.com/edenhill'], 'HOORAY': ['https://github.com/ah-']}"
edenhill/librdkafka,https://github.com/edenhill/librdkafka,1003,2017-01-16T22:12:22Z,2017-01-23T09:13:42Z,2017-01-31T14:19:55Z,MERGED,True,205,22,1,https://github.com/zyzil,SASL: Windows SASL implementation via SSPI (#888),1,[],https://github.com/edenhill/librdkafka/pull/1003,https://github.com/gwenshap,7,https://github.com/edenhill/librdkafka/pull/1003#issuecomment-273951423,"Additional changes to support Kerberos authentication from Windows
using SSPI.",@edenhill Can't wait to see this merged - and even more excited about a possible @zyzil blog.,True,{'THUMBS_UP': ['https://github.com/edenhill']}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,1003,2017-01-16T22:12:22Z,2017-01-23T09:13:42Z,2017-01-31T14:19:55Z,MERGED,True,205,22,1,https://github.com/zyzil,SASL: Windows SASL implementation via SSPI (#888),1,[],https://github.com/edenhill/librdkafka/pull/1003,https://github.com/edenhill,8,https://github.com/edenhill/librdkafka/pull/1003#issuecomment-274434830,"Additional changes to support Kerberos authentication from Windows
using SSPI.",Thanks for this!,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,1003,2017-01-16T22:12:22Z,2017-01-23T09:13:42Z,2017-01-31T14:19:55Z,MERGED,True,205,22,1,https://github.com/zyzil,SASL: Windows SASL implementation via SSPI (#888),1,[],https://github.com/edenhill/librdkafka/pull/1003,https://github.com/edenhill,9,https://github.com/edenhill/librdkafka/pull/1003#issuecomment-274436184,"Additional changes to support Kerberos authentication from Windows
using SSPI.","@zyzil and @MaximGurschi  Would you like to add any additional steps needed on the client-side for using SASL with Win32 on the librdkafka wiki-page here?:
https://github.com/edenhill/librdkafka/wiki/Using-SASL-with-librdkafka",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,1003,2017-01-16T22:12:22Z,2017-01-23T09:13:42Z,2017-01-31T14:19:55Z,MERGED,True,205,22,1,https://github.com/zyzil,SASL: Windows SASL implementation via SSPI (#888),1,[],https://github.com/edenhill/librdkafka/pull/1003,https://github.com/edenhill,10,https://github.com/edenhill/librdkafka/pull/1003#issuecomment-274436229,"Additional changes to support Kerberos authentication from Windows
using SSPI.","And remember, you are both heroes.",True,"{'THUMBS_UP': ['https://github.com/gwenshap'], 'HEART': ['https://github.com/gwenshap']}"
edenhill/librdkafka,https://github.com/edenhill/librdkafka,1003,2017-01-16T22:12:22Z,2017-01-23T09:13:42Z,2017-01-31T14:19:55Z,MERGED,True,205,22,1,https://github.com/zyzil,SASL: Windows SASL implementation via SSPI (#888),1,[],https://github.com/edenhill/librdkafka/pull/1003,https://github.com/MaximGurschi,11,https://github.com/edenhill/librdkafka/pull/1003#issuecomment-274496605,"Additional changes to support Kerberos authentication from Windows
using SSPI.","Once I setup everything again from start to finish will update with the steps. I'll allocate some time towards the end of the week/early next week.
Thanks all again, good job!",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,1003,2017-01-16T22:12:22Z,2017-01-23T09:13:42Z,2017-01-31T14:19:55Z,MERGED,True,205,22,1,https://github.com/zyzil,SASL: Windows SASL implementation via SSPI (#888),1,[],https://github.com/edenhill/librdkafka/pull/1003,https://github.com/MaximGurschi,12,https://github.com/edenhill/librdkafka/pull/1003#issuecomment-276329374,"Additional changes to support Kerberos authentication from Windows
using SSPI.","Hi Magnus, I am ready to put the Windows manual on the WIKI. Should it go on the same page (https://github.com/edenhill/librdkafka/wiki/Using-SASL-with-librdkafka) or should there be a dedicated Windows SSPI page?",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,1003,2017-01-16T22:12:22Z,2017-01-23T09:13:42Z,2017-01-31T14:19:55Z,MERGED,True,205,22,1,https://github.com/zyzil,SASL: Windows SASL implementation via SSPI (#888),1,[],https://github.com/edenhill/librdkafka/pull/1003,https://github.com/edenhill,13,https://github.com/edenhill/librdkafka/pull/1003#issuecomment-276330388,"Additional changes to support Kerberos authentication from Windows
using SSPI.","@MaximGurschi Awesome! That's up to you, if it is too meaty it probably makes sense to have  its own wiki page (reference it from the original page), otherwise integrate into the existing one.",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,1003,2017-01-16T22:12:22Z,2017-01-23T09:13:42Z,2017-01-31T14:19:55Z,MERGED,True,205,22,1,https://github.com/zyzil,SASL: Windows SASL implementation via SSPI (#888),1,[],https://github.com/edenhill/librdkafka/pull/1003,https://github.com/MaximGurschi,14,https://github.com/edenhill/librdkafka/pull/1003#issuecomment-276346969,"Additional changes to support Kerberos authentication from Windows
using SSPI.","I've created the page here:
https://github.com/edenhill/librdkafka/wiki/Using-SASL-with-librdkafka-on-Windows
Please let me know if you have any suggestions/remarks.",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,1003,2017-01-16T22:12:22Z,2017-01-23T09:13:42Z,2017-01-31T14:19:55Z,MERGED,True,205,22,1,https://github.com/zyzil,SASL: Windows SASL implementation via SSPI (#888),1,[],https://github.com/edenhill/librdkafka/pull/1003,https://github.com/edenhill,15,https://github.com/edenhill/librdkafka/pull/1003#issuecomment-276348449,"Additional changes to support Kerberos authentication from Windows
using SSPI.","@MaximGurschi This is really great!
Some minor things:

use ALLUPPERCASE for all variable fields, e.g. ""domain\Zookeeper_.."" should probably be ""DOMAIN\Zookeeper_.."",
The **_HOST.. notation doesn't work in code sections
what is dp0? %~dp0../..
HOST.fully_qualified should probable be HOST_FQDN
Describe all variable fields (DOMAIN, HOST, etc..) at the start of the wiki post so people know what they need to set up/define.
Make a note that zookeeper auth is optional and mark those chapters as optional, I'm guessing librdkafka users will mostly care about authentication the client.
It would be great if you provided some example output from rdkafka_example.exe with the -d security option so that people can see what a succesful auth looks like.

Big thanks!",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,1003,2017-01-16T22:12:22Z,2017-01-23T09:13:42Z,2017-01-31T14:19:55Z,MERGED,True,205,22,1,https://github.com/zyzil,SASL: Windows SASL implementation via SSPI (#888),1,[],https://github.com/edenhill/librdkafka/pull/1003,https://github.com/MaximGurschi,16,https://github.com/edenhill/librdkafka/pull/1003#issuecomment-276367110,"Additional changes to support Kerberos authentication from Windows
using SSPI.","Cheers Magnus, great feedback! What do you say to the update?",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,1003,2017-01-16T22:12:22Z,2017-01-23T09:13:42Z,2017-01-31T14:19:55Z,MERGED,True,205,22,1,https://github.com/zyzil,SASL: Windows SASL implementation via SSPI (#888),1,[],https://github.com/edenhill/librdkafka/pull/1003,https://github.com/edenhill,17,https://github.com/edenhill/librdkafka/pull/1003#issuecomment-276367718,"Additional changes to support Kerberos authentication from Windows
using SSPI.","Looks great!
I'm still curious, what is %~dp0..?",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,1003,2017-01-16T22:12:22Z,2017-01-23T09:13:42Z,2017-01-31T14:19:55Z,MERGED,True,205,22,1,https://github.com/zyzil,SASL: Windows SASL implementation via SSPI (#888),1,[],https://github.com/edenhill/librdkafka/pull/1003,https://github.com/MaximGurschi,18,https://github.com/edenhill/librdkafka/pull/1003#issuecomment-276374583,"Additional changes to support Kerberos authentication from Windows
using SSPI.",":)
I placed it in the WIKI as well:
The %~dp0 (thats a zero) variable will expand to the drive letter and path of the batch file.",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,1003,2017-01-16T22:12:22Z,2017-01-23T09:13:42Z,2017-01-31T14:19:55Z,MERGED,True,205,22,1,https://github.com/zyzil,SASL: Windows SASL implementation via SSPI (#888),1,[],https://github.com/edenhill/librdkafka/pull/1003,https://github.com/edenhill,19,https://github.com/edenhill/librdkafka/pull/1003#issuecomment-276374736,"Additional changes to support Kerberos authentication from Windows
using SSPI.",Windows magic :),True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,1003,2017-01-16T22:12:22Z,2017-01-23T09:13:42Z,2017-01-31T14:19:55Z,MERGED,True,205,22,1,https://github.com/zyzil,SASL: Windows SASL implementation via SSPI (#888),1,[],https://github.com/edenhill/librdkafka/pull/1003,https://github.com/edenhill,20,https://github.com/edenhill/librdkafka/pull/1003#issuecomment-276374865,"Additional changes to support Kerberos authentication from Windows
using SSPI.","Thanks to both of you for all your work with SASL on Windows, this is open source at its finest!",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,1014,2017-01-19T12:10:53Z,2017-01-27T13:54:50Z,2017-01-30T05:08:41Z,MERGED,True,477,0,14,https://github.com/ruslo,Add CMake,16,[],https://github.com/edenhill/librdkafka/pull/1014,https://github.com/ruslo,1,https://github.com/edenhill/librdkafka/pull/1014,,,True,{'HOORAY': ['https://github.com/yannick']}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,1014,2017-01-19T12:10:53Z,2017-01-27T13:54:50Z,2017-01-30T05:08:41Z,MERGED,True,477,0,14,https://github.com/ruslo,Add CMake,16,[],https://github.com/edenhill/librdkafka/pull/1014,https://github.com/ruslo,2,https://github.com/edenhill/librdkafka/pull/1014#issuecomment-273761641,,"This is basic CMake configuration. Currently I have problems with test, even ./configure && make && cd tests && make doesn't work for me. Builds fine on OSX and Linux, on Windows I got few problems. The first one is the way how to integrate lz4 into project.",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,1014,2017-01-19T12:10:53Z,2017-01-27T13:54:50Z,2017-01-30T05:08:41Z,MERGED,True,477,0,14,https://github.com/ruslo,Add CMake,16,[],https://github.com/edenhill/librdkafka/pull/1014,https://github.com/edenhill,3,https://github.com/edenhill/librdkafka/pull/1014#issuecomment-274456376,,"On Windows we make use of nuget for dependencies, so having msbuild automatically restore nuget packages should work.",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,1014,2017-01-19T12:10:53Z,2017-01-27T13:54:50Z,2017-01-30T05:08:41Z,MERGED,True,477,0,14,https://github.com/ruslo,Add CMake,16,[],https://github.com/edenhill/librdkafka/pull/1014,https://github.com/ruslo,4,https://github.com/edenhill/librdkafka/pull/1014#issuecomment-274506506,,"having msbuild automatically restore nuget packages should work

As far as I know CMake doesn't work well with nuget. Let me know if I'm missing something.",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,1014,2017-01-19T12:10:53Z,2017-01-27T13:54:50Z,2017-01-30T05:08:41Z,MERGED,True,477,0,14,https://github.com/ruslo,Add CMake,16,[],https://github.com/edenhill/librdkafka/pull/1014,https://github.com/edenhill,5,https://github.com/edenhill/librdkafka/pull/1014#issuecomment-274565947,,"Could you also add a doozer target for cmake so it is verified on CI?
It only needs to be run for one platform - xenial-amd64",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,1014,2017-01-19T12:10:53Z,2017-01-27T13:54:50Z,2017-01-30T05:08:41Z,MERGED,True,477,0,14,https://github.com/ruslo,Add CMake,16,[],https://github.com/edenhill/librdkafka/pull/1014,https://github.com/ruslo,6,https://github.com/edenhill/librdkafka/pull/1014#issuecomment-274708049,,"Could you also add a doozer target for cmake so it is verified on CI?

Will take a look",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,1014,2017-01-19T12:10:53Z,2017-01-27T13:54:50Z,2017-01-30T05:08:41Z,MERGED,True,477,0,14,https://github.com/ruslo,Add CMake,16,[],https://github.com/edenhill/librdkafka/pull/1014,https://github.com/ruslo,7,https://github.com/edenhill/librdkafka/pull/1014#issuecomment-274830683,,"Will take a look

Target cmake-xenial-amd64 added: https://doozer.io/edenhill/librdkafka/build/205",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,1014,2017-01-19T12:10:53Z,2017-01-27T13:54:50Z,2017-01-30T05:08:41Z,MERGED,True,477,0,14,https://github.com/ruslo,Add CMake,16,[],https://github.com/edenhill/librdkafka/pull/1014,https://github.com/Whissi,8,https://github.com/edenhill/librdkafka/pull/1014#issuecomment-275662934,,"Is librdkafa switching from mklove to CMake?
Please add an CMake option to disable optional usage of lz4, see #1025, if you haven't done this yet.",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,1014,2017-01-19T12:10:53Z,2017-01-27T13:54:50Z,2017-01-30T05:08:41Z,MERGED,True,477,0,14,https://github.com/ruslo,Add CMake,16,[],https://github.com/edenhill/librdkafka/pull/1014,https://github.com/edenhill,9,https://github.com/edenhill/librdkafka/pull/1014#issuecomment-275668183,,"@Whissi No, CMake was contributed to provide an alternative, but mklove will remain as the primary build system.",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,1014,2017-01-19T12:10:53Z,2017-01-27T13:54:50Z,2017-01-30T05:08:41Z,MERGED,True,477,0,14,https://github.com/ruslo,Add CMake,16,[],https://github.com/edenhill/librdkafka/pull/1014,https://github.com/edenhill,10,https://github.com/edenhill/librdkafka/pull/1014#issuecomment-275670064,,Thanks for this!,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,1017,2017-01-20T16:34:50Z,2017-01-20T18:06:26Z,2017-01-20T18:06:33Z,MERGED,True,1,0,1,https://github.com/edoardocomar,Updated README.md mentioning node-rdkafka,1,[],https://github.com/edenhill/librdkafka/pull/1017,https://github.com/edoardocomar,1,https://github.com/edenhill/librdkafka/pull/1017,it's AFAIK the most active of the 3 Node.js projects listed,it's AFAIK the most active of the 3 Node.js projects listed,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,1017,2017-01-20T16:34:50Z,2017-01-20T18:06:26Z,2017-01-20T18:06:33Z,MERGED,True,1,0,1,https://github.com/edoardocomar,Updated README.md mentioning node-rdkafka,1,[],https://github.com/edenhill/librdkafka/pull/1017,https://github.com/edenhill,2,https://github.com/edenhill/librdkafka/pull/1017#issuecomment-274138839,it's AFAIK the most active of the 3 Node.js projects listed,,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,1025,2017-01-27T13:14:03Z,2017-01-27T13:46:28Z,2017-01-27T13:46:32Z,MERGED,True,7,2,1,https://github.com/Whissi,configure: Add option to disable automagic dependency on liblz4,1,[],https://github.com/edenhill/librdkafka/pull/1025,https://github.com/Whissi,1,https://github.com/edenhill/librdkafka/pull/1025,"Previously, mklove activated lz4 support when lz4 was found. This added
a so called ""automagic"" dependency on liblz4 which is a problem from
distributions.
This commit will add an option which will allow you to explicit disable
lz4 usage.","Previously, mklove activated lz4 support when lz4 was found. This added
a so called ""automagic"" dependency on liblz4 which is a problem from
distributions.
This commit will add an option which will allow you to explicit disable
lz4 usage.",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,1025,2017-01-27T13:14:03Z,2017-01-27T13:46:28Z,2017-01-27T13:46:32Z,MERGED,True,7,2,1,https://github.com/Whissi,configure: Add option to disable automagic dependency on liblz4,1,[],https://github.com/edenhill/librdkafka/pull/1025,https://github.com/edenhill,2,https://github.com/edenhill/librdkafka/pull/1025#issuecomment-275668367,"Previously, mklove activated lz4 support when lz4 was found. This added
a so called ""automagic"" dependency on liblz4 which is a problem from
distributions.
This commit will add an option which will allow you to explicit disable
lz4 usage.",Thanks!,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,1027,2017-01-30T08:33:43Z,2017-02-10T11:41:29Z,2017-02-10T11:51:45Z,MERGED,True,89,0,4,https://github.com/ruslo,Add install target,4,[],https://github.com/edenhill/librdkafka/pull/1027,https://github.com/ruslo,1,https://github.com/edenhill/librdkafka/pull/1027,"After install package can be found by find_package. Example:
find_package(RdKafka CONFIG REQUIRED)

add_executable(rdkafka_example rdkafka_example.c)
target_link_libraries(rdkafka_example PUBLIC RdKafka::rdkafka)

add_executable(rdkafka_example_cpp rdkafka_example.cpp)
target_link_libraries(rdkafka_example_cpp PUBLIC RdKafka::rdkafka++)","After install package can be found by find_package. Example:
find_package(RdKafka CONFIG REQUIRED)

add_executable(rdkafka_example rdkafka_example.c)
target_link_libraries(rdkafka_example PUBLIC RdKafka::rdkafka)

add_executable(rdkafka_example_cpp rdkafka_example.cpp)
target_link_libraries(rdkafka_example_cpp PUBLIC RdKafka::rdkafka++)",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,1027,2017-01-30T08:33:43Z,2017-02-10T11:41:29Z,2017-02-10T11:51:45Z,MERGED,True,89,0,4,https://github.com/ruslo,Add install target,4,[],https://github.com/edenhill/librdkafka/pull/1027,https://github.com/yannick,2,https://github.com/edenhill/librdkafka/pull/1027#issuecomment-276006950,"After install package can be found by find_package. Example:
find_package(RdKafka CONFIG REQUIRED)

add_executable(rdkafka_example rdkafka_example.c)
target_link_libraries(rdkafka_example PUBLIC RdKafka::rdkafka)

add_executable(rdkafka_example_cpp rdkafka_example.cpp)
target_link_libraries(rdkafka_example_cpp PUBLIC RdKafka::rdkafka++)","i think the tree that is generated in install dir for gnu systems should look like:
include/
include/librdkafka/
include/librdkafka/rdkafka.h
include/librdkafka/rdkafkacpp.h
lib/
lib/librdkafka++.so
lib/librdkafka++.so.1
lib/librdkafka.so
lib/librdkafka.so.1
lib/pkgconfig/
lib/pkgconfig/rdkafka++.pc
lib/pkgconfig/rdkafka.pc",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,1027,2017-01-30T08:33:43Z,2017-02-10T11:41:29Z,2017-02-10T11:51:45Z,MERGED,True,89,0,4,https://github.com/ruslo,Add install target,4,[],https://github.com/edenhill/librdkafka/pull/1027,https://github.com/ruslo,3,https://github.com/edenhill/librdkafka/pull/1027#issuecomment-276008612,"After install package can be found by find_package. Example:
find_package(RdKafka CONFIG REQUIRED)

add_executable(rdkafka_example rdkafka_example.c)
target_link_libraries(rdkafka_example PUBLIC RdKafka::rdkafka)

add_executable(rdkafka_example_cpp rdkafka_example.cpp)
target_link_libraries(rdkafka_example_cpp PUBLIC RdKafka::rdkafka++)","lib/pkgconfig/rdkafka++.pc
lib/pkgconfig/rdkafka.pc

#1027 (comment)

share/licenses/librdkafka-git/LICENSE

Why *-git? I can add it though.",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,1027,2017-01-30T08:33:43Z,2017-02-10T11:41:29Z,2017-02-10T11:51:45Z,MERGED,True,89,0,4,https://github.com/ruslo,Add install target,4,[],https://github.com/edenhill/librdkafka/pull/1027,https://github.com/yannick,4,https://github.com/edenhill/librdkafka/pull/1027#issuecomment-276009086,"After install package can be found by find_package. Example:
find_package(RdKafka CONFIG REQUIRED)

add_executable(rdkafka_example rdkafka_example.c)
target_link_libraries(rdkafka_example PUBLIC RdKafka::rdkafka)

add_executable(rdkafka_example_cpp rdkafka_example.cpp)
target_link_libraries(rdkafka_example_cpp PUBLIC RdKafka::rdkafka++)","Why *-git? I can add it though.
i copied that from a call to pacman -Ql librdkafka-git and misse that. removed it.",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,1027,2017-01-30T08:33:43Z,2017-02-10T11:41:29Z,2017-02-10T11:51:45Z,MERGED,True,89,0,4,https://github.com/ruslo,Add install target,4,[],https://github.com/edenhill/librdkafka/pull/1027,https://github.com/ruslo,5,https://github.com/edenhill/librdkafka/pull/1027#issuecomment-276009212,"After install package can be found by find_package. Example:
find_package(RdKafka CONFIG REQUIRED)

add_executable(rdkafka_example rdkafka_example.c)
target_link_libraries(rdkafka_example PUBLIC RdKafka::rdkafka)

add_executable(rdkafka_example_cpp rdkafka_example.cpp)
target_link_libraries(rdkafka_example_cpp PUBLIC RdKafka::rdkafka++)","Why *-git? I can add it though.

c389f77",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,1027,2017-01-30T08:33:43Z,2017-02-10T11:41:29Z,2017-02-10T11:51:45Z,MERGED,True,89,0,4,https://github.com/ruslo,Add install target,4,[],https://github.com/edenhill/librdkafka/pull/1027,https://github.com/ruslo,6,https://github.com/edenhill/librdkafka/pull/1027#issuecomment-276009514,"After install package can be found by find_package. Example:
find_package(RdKafka CONFIG REQUIRED)

add_executable(rdkafka_example rdkafka_example.c)
target_link_libraries(rdkafka_example PUBLIC RdKafka::rdkafka)

add_executable(rdkafka_example_cpp rdkafka_example.cpp)
target_link_libraries(rdkafka_example_cpp PUBLIC RdKafka::rdkafka++)","i copied that from a call to pacman -Ql librdkafka-git and misse that. removed it

130d830",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,1027,2017-01-30T08:33:43Z,2017-02-10T11:41:29Z,2017-02-10T11:51:45Z,MERGED,True,89,0,4,https://github.com/ruslo,Add install target,4,[],https://github.com/edenhill/librdkafka/pull/1027,https://github.com/yannick,7,https://github.com/edenhill/librdkafka/pull/1027#issuecomment-276010102,"After install package can be found by find_package. Example:
find_package(RdKafka CONFIG REQUIRED)

add_executable(rdkafka_example rdkafka_example.c)
target_link_libraries(rdkafka_example PUBLIC RdKafka::rdkafka)

add_executable(rdkafka_example_cpp rdkafka_example.cpp)
target_link_libraries(rdkafka_example_cpp PUBLIC RdKafka::rdkafka++)","as discussed in skype, i vote for using  the GNUInstallDirs module for unix since it's more or less standard",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,1027,2017-01-30T08:33:43Z,2017-02-10T11:41:29Z,2017-02-10T11:51:45Z,MERGED,True,89,0,4,https://github.com/ruslo,Add install target,4,[],https://github.com/edenhill/librdkafka/pull/1027,https://github.com/ruslo,8,https://github.com/edenhill/librdkafka/pull/1027#issuecomment-276012307,"After install package can be found by find_package. Example:
find_package(RdKafka CONFIG REQUIRED)

add_executable(rdkafka_example rdkafka_example.c)
target_link_libraries(rdkafka_example PUBLIC RdKafka::rdkafka)

add_executable(rdkafka_example_cpp rdkafka_example.cpp)
target_link_libraries(rdkafka_example_cpp PUBLIC RdKafka::rdkafka++)","i vote for using the GNUInstallDirs module for unix since it's more or less standard

I'll take a look",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,1027,2017-01-30T08:33:43Z,2017-02-10T11:41:29Z,2017-02-10T11:51:45Z,MERGED,True,89,0,4,https://github.com/ruslo,Add install target,4,[],https://github.com/edenhill/librdkafka/pull/1027,https://github.com/ruslo,9,https://github.com/edenhill/librdkafka/pull/1027#issuecomment-276017112,"After install package can be found by find_package. Example:
find_package(RdKafka CONFIG REQUIRED)

add_executable(rdkafka_example rdkafka_example.c)
target_link_libraries(rdkafka_example PUBLIC RdKafka::rdkafka)

add_executable(rdkafka_example_cpp rdkafka_example.cpp)
target_link_libraries(rdkafka_example_cpp PUBLIC RdKafka::rdkafka++)","I'll take a look

Done 0947ab0
I don't see variables for CMake configs and license file so I have to set it myself anyway.",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,1027,2017-01-30T08:33:43Z,2017-02-10T11:41:29Z,2017-02-10T11:51:45Z,MERGED,True,89,0,4,https://github.com/ruslo,Add install target,4,[],https://github.com/edenhill/librdkafka/pull/1027,https://github.com/edenhill,10,https://github.com/edenhill/librdkafka/pull/1027#issuecomment-276165916,"After install package can be found by find_package. Example:
find_package(RdKafka CONFIG REQUIRED)

add_executable(rdkafka_example rdkafka_example.c)
target_link_libraries(rdkafka_example PUBLIC RdKafka::rdkafka)

add_executable(rdkafka_example_cpp rdkafka_example.cpp)
target_link_libraries(rdkafka_example_cpp PUBLIC RdKafka::rdkafka++)",Is everyone happy with this?,True,{'THUMBS_UP': ['https://github.com/ruslo']}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,1038,2017-02-06T15:46:21Z,2017-02-10T11:43:30Z,2017-02-10T11:43:34Z,MERGED,True,1,1,1,https://github.com/zakaluka,Updated link to rdkafka-dotnet,2,[],https://github.com/edenhill/librdkafka/pull/1038,https://github.com/zakaluka,1,https://github.com/edenhill/librdkafka/pull/1038,Based on note at old website.,Based on note at old website.,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,1038,2017-02-06T15:46:21Z,2017-02-10T11:43:30Z,2017-02-10T11:43:34Z,MERGED,True,1,1,1,https://github.com/zakaluka,Updated link to rdkafka-dotnet,2,[],https://github.com/edenhill/librdkafka/pull/1038,https://github.com/edenhill,2,https://github.com/edenhill/librdkafka/pull/1038#issuecomment-278923830,Based on note at old website.,Thanks,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,1050,2017-02-10T16:14:53Z,2017-02-10T16:15:39Z,2017-02-10T16:15:39Z,CLOSED,False,143,25,2,https://github.com/vin-d,update to latest edenhill/librdkafka - master,7,[],https://github.com/edenhill/librdkafka/pull/1050,https://github.com/vin-d,1,https://github.com/edenhill/librdkafka/pull/1050,,,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,1050,2017-02-10T16:14:53Z,2017-02-10T16:15:39Z,2017-02-10T16:15:39Z,CLOSED,False,143,25,2,https://github.com/vin-d,update to latest edenhill/librdkafka - master,7,[],https://github.com/edenhill/librdkafka/pull/1050,https://github.com/vin-d,2,https://github.com/edenhill/librdkafka/pull/1050#issuecomment-278987421,,wrong operation !,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,1053,2017-02-13T09:57:34Z,2017-02-13T10:07:51Z,2017-02-13T10:07:51Z,CLOSED,False,4,4,1,https://github.com/vin-d,Compatibility bugfix of a link issue due to change of RdKafka::Conf method declaration order. ,9,[],https://github.com/edenhill/librdkafka/pull/1053,https://github.com/vin-d,1,https://github.com/edenhill/librdkafka/pull/1053,"Method declaration issue initially introduced  by  c09efc1
introduction of

virtual Conf::ConfResult set (const std::string &name, ConsumeCb *consume_cb,  std::string &errstr) = 0;

in the middle of existing methods of class definition, makes software using librdkafka that are compiled against previous version of librdkafka (pre c09efc1), link wrong methods of the class when used with the library which has this modified interface. (...yes this sentence is a bit confusing -_-)","Method declaration issue initially introduced  by  c09efc1
introduction of

virtual Conf::ConfResult set (const std::string &name, ConsumeCb *consume_cb,  std::string &errstr) = 0;

in the middle of existing methods of class definition, makes software using librdkafka that are compiled against previous version of librdkafka (pre c09efc1), link wrong methods of the class when used with the library which has this modified interface. (...yes this sentence is a bit confusing -_-)",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,1053,2017-02-13T09:57:34Z,2017-02-13T10:07:51Z,2017-02-13T10:07:51Z,CLOSED,False,4,4,1,https://github.com/vin-d,Compatibility bugfix of a link issue due to change of RdKafka::Conf method declaration order. ,9,[],https://github.com/edenhill/librdkafka/pull/1053,https://github.com/edenhill,2,https://github.com/edenhill/librdkafka/pull/1053#issuecomment-279341789,"Method declaration issue initially introduced  by  c09efc1
introduction of

virtual Conf::ConfResult set (const std::string &name, ConsumeCb *consume_cb,  std::string &errstr) = 0;

in the middle of existing methods of class definition, makes software using librdkafka that are compiled against previous version of librdkafka (pre c09efc1), link wrong methods of the class when used with the library which has this modified interface. (...yes this sentence is a bit confusing -_-)",The diff shows up empty?,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,1053,2017-02-13T09:57:34Z,2017-02-13T10:07:51Z,2017-02-13T10:07:51Z,CLOSED,False,4,4,1,https://github.com/vin-d,Compatibility bugfix of a link issue due to change of RdKafka::Conf method declaration order. ,9,[],https://github.com/edenhill/librdkafka/pull/1053,https://github.com/vin-d,3,https://github.com/edenhill/librdkafka/pull/1053#issuecomment-279342104,"Method declaration issue initially introduced  by  c09efc1
introduction of

virtual Conf::ConfResult set (const std::string &name, ConsumeCb *consume_cb,  std::string &errstr) = 0;

in the middle of existing methods of class definition, makes software using librdkafka that are compiled against previous version of librdkafka (pre c09efc1), link wrong methods of the class when used with the library which has this modified interface. (...yes this sentence is a bit confusing -_-)","yes, i'm trying to findout what I did wrong.
i'm really not git-fluent.",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,1054,2017-02-13T10:09:06Z,2017-02-13T11:36:17Z,2017-02-13T11:36:22Z,MERGED,True,4,4,1,https://github.com/vin-d,Compatibility bugfix of a link issue due to change of RdKafka::Conf method declaration order.,9,[],https://github.com/edenhill/librdkafka/pull/1054,https://github.com/vin-d,1,https://github.com/edenhill/librdkafka/pull/1054,"Method declaration issue initially introduced by c09efc1
introduction of

virtual Conf::ConfResult set (const std::string &name, ConsumeCb *consume_cb, std::string &errstr) = 0;

in the middle of existing methods of class definition, makes software using librdkafka that are compiled against previous version of librdkafka (pre c09efc1), link wrong methods of the class when used with the library which has this modified interface. (...yes this sentence is a bit confusing -_-)","Method declaration issue initially introduced by c09efc1
introduction of

virtual Conf::ConfResult set (const std::string &name, ConsumeCb *consume_cb, std::string &errstr) = 0;

in the middle of existing methods of class definition, makes software using librdkafka that are compiled against previous version of librdkafka (pre c09efc1), link wrong methods of the class when used with the library which has this modified interface. (...yes this sentence is a bit confusing -_-)",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,1054,2017-02-13T10:09:06Z,2017-02-13T11:36:17Z,2017-02-13T11:36:22Z,MERGED,True,4,4,1,https://github.com/vin-d,Compatibility bugfix of a link issue due to change of RdKafka::Conf method declaration order.,9,[],https://github.com/edenhill/librdkafka/pull/1054,https://github.com/vin-d,2,https://github.com/edenhill/librdkafka/pull/1054#issuecomment-279344435,"Method declaration issue initially introduced by c09efc1
introduction of

virtual Conf::ConfResult set (const std::string &name, ConsumeCb *consume_cb, std::string &errstr) = 0;

in the middle of existing methods of class definition, makes software using librdkafka that are compiled against previous version of librdkafka (pre c09efc1), link wrong methods of the class when used with the library which has this modified interface. (...yes this sentence is a bit confusing -_-)","ok @edenhill , should be better like that.
i'm not sure why it shows all my differents old commits from november in the list. of the PR, but at least d6fc13b is there.",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,1054,2017-02-13T10:09:06Z,2017-02-13T11:36:17Z,2017-02-13T11:36:22Z,MERGED,True,4,4,1,https://github.com/vin-d,Compatibility bugfix of a link issue due to change of RdKafka::Conf method declaration order.,9,[],https://github.com/edenhill/librdkafka/pull/1054,https://github.com/vin-d,3,https://github.com/edenhill/librdkafka/pull/1054#issuecomment-279344704,"Method declaration issue initially introduced by c09efc1
introduction of

virtual Conf::ConfResult set (const std::string &name, ConsumeCb *consume_cb, std::string &errstr) = 0;

in the middle of existing methods of class definition, makes software using librdkafka that are compiled against previous version of librdkafka (pre c09efc1), link wrong methods of the class when used with the library which has this modified interface. (...yes this sentence is a bit confusing -_-)","I tested before/after, and it's really this part involved in d6fc13b that created this ""wrong link"" issue.",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,1054,2017-02-13T10:09:06Z,2017-02-13T11:36:17Z,2017-02-13T11:36:22Z,MERGED,True,4,4,1,https://github.com/vin-d,Compatibility bugfix of a link issue due to change of RdKafka::Conf method declaration order.,9,[],https://github.com/edenhill/librdkafka/pull/1054,https://github.com/edenhill,4,https://github.com/edenhill/librdkafka/pull/1054#issuecomment-279362795,"Method declaration issue initially introduced by c09efc1
introduction of

virtual Conf::ConfResult set (const std::string &name, ConsumeCb *consume_cb, std::string &errstr) = 0;

in the middle of existing methods of class definition, makes software using librdkafka that are compiled against previous version of librdkafka (pre c09efc1), link wrong methods of the class when used with the library which has this modified interface. (...yes this sentence is a bit confusing -_-)",Thanks!,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,1083,2017-02-28T17:23:28Z,2017-03-10T14:59:09Z,2017-04-19T14:53:13Z,MERGED,True,30,12,1,https://github.com/zyzil,sasl: protection level and message size not sent,1,[],https://github.com/edenhill/librdkafka/pull/1083,https://github.com/zyzil,1,https://github.com/edenhill/librdkafka/pull/1083,"The protection level and message size was not being sent back to
the server during the authentication handshake.  This was causing
the first characters of the user principal to be misinterpreted
and could cause authentication errors due to an unknown protection
level if the first character of the username did not have bit 1 set","The protection level and message size was not being sent back to
the server during the authentication handshake.  This was causing
the first characters of the user principal to be misinterpreted
and could cause authentication errors due to an unknown protection
level if the first character of the username did not have bit 1 set",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,1083,2017-02-28T17:23:28Z,2017-03-10T14:59:09Z,2017-04-19T14:53:13Z,MERGED,True,30,12,1,https://github.com/zyzil,sasl: protection level and message size not sent,1,[],https://github.com/edenhill/librdkafka/pull/1083,https://github.com/edenhill,2,https://github.com/edenhill/librdkafka/pull/1083#issuecomment-284858130,"The protection level and message size was not being sent back to
the server during the authentication handshake.  This was causing
the first characters of the user principal to be misinterpreted
and could cause authentication errors due to an unknown protection
level if the first character of the username did not have bit 1 set","Thanks a lot for this, will review it next week.",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,1083,2017-02-28T17:23:28Z,2017-03-10T14:59:09Z,2017-04-19T14:53:13Z,MERGED,True,30,12,1,https://github.com/zyzil,sasl: protection level and message size not sent,1,[],https://github.com/edenhill/librdkafka/pull/1083,https://github.com/edenhill,3,https://github.com/edenhill/librdkafka/pull/1083#issuecomment-285690048,"The protection level and message size was not being sent back to
the server during the authentication handshake.  This was causing
the first characters of the user principal to be misinterpreted
and could cause authentication errors due to an unknown protection
level if the first character of the username did not have bit 1 set",Big thanks for this @zyzil,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,1086,2017-03-02T21:12:31Z,2017-03-08T20:51:06Z,2017-03-17T21:20:06Z,MERGED,True,41,2,4,https://github.com/edenhill,Add CRC checking support to consumer (#1056),1,[],https://github.com/edenhill/librdkafka/pull/1086,https://github.com/edenhill,1,https://github.com/edenhill/librdkafka/pull/1086,,,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,1086,2017-03-02T21:12:31Z,2017-03-08T20:51:06Z,2017-03-17T21:20:06Z,MERGED,True,41,2,4,https://github.com/edenhill,Add CRC checking support to consumer (#1056),1,[],https://github.com/edenhill/librdkafka/pull/1086,https://github.com/ewencp,2,https://github.com/edenhill/librdkafka/pull/1086#issuecomment-283886233,,@edenhill How about requesting reviews from @hqin and @mhowlett too?,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,1086,2017-03-02T21:12:31Z,2017-03-08T20:51:06Z,2017-03-17T21:20:06Z,MERGED,True,41,2,4,https://github.com/edenhill,Add CRC checking support to consumer (#1056),1,[],https://github.com/edenhill/librdkafka/pull/1086,https://github.com/edenhill,3,https://github.com/edenhill/librdkafka/pull/1086#issuecomment-283890591,,"They are not yet on this repo so they can't be assigned as reviewers, but now they know what to do, right @hqin and @mhowlett? :)",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,1086,2017-03-02T21:12:31Z,2017-03-08T20:51:06Z,2017-03-17T21:20:06Z,MERGED,True,41,2,4,https://github.com/edenhill,Add CRC checking support to consumer (#1056),1,[],https://github.com/edenhill/librdkafka/pull/1086,https://github.com/edenhill,4,https://github.com/edenhill/librdkafka/pull/1086#issuecomment-283890987,,"@ewencp Regarding tests: none of this code is easy to unit test unfortunately.
I couldn't find any tests for the java client either so I was thinking of adding a generic test to kafkatest.",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,1086,2017-03-02T21:12:31Z,2017-03-08T20:51:06Z,2017-03-17T21:20:06Z,MERGED,True,41,2,4,https://github.com/edenhill,Add CRC checking support to consumer (#1056),1,[],https://github.com/edenhill/librdkafka/pull/1086,https://github.com/mhowlett,5,https://github.com/edenhill/librdkafka/pull/1086#issuecomment-284301287,,"@ewencp very happy to review C code ( particularly written by @edenhill )
Logic looks good to me at face value. Will look deeper / verify through executing a test app if no LGTM by someone else in near future (just cautious 'cause this code is still new to me).",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,1095,2017-03-07T09:17:56Z,2017-03-07T09:18:12Z,2017-03-07T09:18:12Z,MERGED,True,197,30,5,https://github.com/edenhill,Win32: add rdkafka_performance,2,[],https://github.com/edenhill/librdkafka/pull/1095,https://github.com/edenhill,1,https://github.com/edenhill/librdkafka/pull/1095,,,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,1105,2017-03-08T22:49:32Z,2017-03-08T22:54:25Z,2017-03-08T22:54:30Z,MERGED,True,1,0,1,https://github.com/RockfordWei,Adding Swift to Language Bindings,1,[],https://github.com/edenhill/librdkafka/pull/1105,https://github.com/RockfordWei,1,https://github.com/edenhill/librdkafka/pull/1105,"Perfect is the most popular Server Side Swift API library and HTTP
server framework and librdkafka was successfully imported to Swift on March, 2017.","Perfect is the most popular Server Side Swift API library and HTTP
server framework and librdkafka was successfully imported to Swift on March, 2017.",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,1105,2017-03-08T22:49:32Z,2017-03-08T22:54:25Z,2017-03-08T22:54:30Z,MERGED,True,1,0,1,https://github.com/RockfordWei,Adding Swift to Language Bindings,1,[],https://github.com/edenhill/librdkafka/pull/1105,https://github.com/edenhill,2,https://github.com/edenhill/librdkafka/pull/1105#issuecomment-285196942,"Perfect is the most popular Server Side Swift API library and HTTP
server framework and librdkafka was successfully imported to Swift on March, 2017.",Awesome! Thanks,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,1116,2017-03-14T21:21:17Z,2017-03-20T13:38:35Z,2017-03-20T13:38:35Z,CLOSED,False,31,31,1,https://github.com/mcandre,Fix minor conf leak,3,[],https://github.com/edenhill/librdkafka/pull/1116,https://github.com/mcandre,1,https://github.com/edenhill/librdkafka/pull/1116,"Clean up after heap allocation of Kafka configuration struct pointer, so that developers have a more solid example code to work from.","Clean up after heap allocation of Kafka configuration struct pointer, so that developers have a more solid example code to work from.",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,1116,2017-03-14T21:21:17Z,2017-03-20T13:38:35Z,2017-03-20T13:38:35Z,CLOSED,False,31,31,1,https://github.com/mcandre,Fix minor conf leak,3,[],https://github.com/edenhill/librdkafka/pull/1116,https://github.com/edenhill,2,https://github.com/edenhill/librdkafka/pull/1116#issuecomment-287761168,"Clean up after heap allocation of Kafka configuration struct pointer, so that developers have a more solid example code to work from.","I don't see the need to clutter the example code with cleanup on error, it will just exit(1) immediately anyway.
This change also has some problems with it, namely destroying things in the wrong order.
Thank you for your effort but I don't think this one will pay off, the example code is bloated as-is.",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,1119,2017-03-16T00:00:37Z,2017-03-21T13:13:47Z,2017-03-21T13:13:47Z,CLOSED,False,20,6,1,https://github.com/llinsky,Feature/add error messages,2,[],https://github.com/edenhill/librdkafka/pull/1119,https://github.com/llinsky,1,https://github.com/edenhill/librdkafka/pull/1119,"This adds error messages for some ssl configuration errors which previously returned ""No error""","This adds error messages for some ssl configuration errors which previously returned ""No error""",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,1119,2017-03-16T00:00:37Z,2017-03-21T13:13:47Z,2017-03-21T13:13:47Z,CLOSED,False,20,6,1,https://github.com/llinsky,Feature/add error messages,2,[],https://github.com/edenhill/librdkafka/pull/1119,https://github.com/edenhill,2,https://github.com/edenhill/librdkafka/pull/1119#issuecomment-288073949,"This adds error messages for some ssl configuration errors which previously returned ""No error""",Thanks for this!,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,1121,2017-03-16T12:33:38Z,2017-03-19T21:25:19Z,2017-03-19T21:25:19Z,CLOSED,False,31,22,2,https://github.com/orthrus,Fixed high level consumer CPU usage issue,1,[],https://github.com/edenhill/librdkafka/pull/1121,https://github.com/orthrus,1,https://github.com/edenhill/librdkafka/pull/1121,Fixed an issue that caused the high level consumer to use a lot of CPU due to a queue being continuously signaled while there are no new queue entries available.,Fixed an issue that caused the high level consumer to use a lot of CPU due to a queue being continuously signaled while there are no new queue entries available.,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,1121,2017-03-16T12:33:38Z,2017-03-19T21:25:19Z,2017-03-19T21:25:19Z,CLOSED,False,31,22,2,https://github.com/orthrus,Fixed high level consumer CPU usage issue,1,[],https://github.com/edenhill/librdkafka/pull/1121,https://github.com/edenhill,2,https://github.com/edenhill/librdkafka/pull/1121#issuecomment-287047539,Fixed an issue that caused the high level consumer to use a lot of CPU due to a queue being continuously signaled while there are no new queue entries available.,Good find!,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,1121,2017-03-16T12:33:38Z,2017-03-19T21:25:19Z,2017-03-19T21:25:19Z,CLOSED,False,31,22,2,https://github.com/orthrus,Fixed high level consumer CPU usage issue,1,[],https://github.com/edenhill/librdkafka/pull/1121,https://github.com/orthrus,3,https://github.com/edenhill/librdkafka/pull/1121#issuecomment-287080672,Fixed an issue that caused the high level consumer to use a lot of CPU due to a queue being continuously signaled while there are no new queue entries available.,"I've updated the fix. I couldn't use rd_kafka_q_len since it tries to lock the queue, which causes a simulated deadlock.",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,1121,2017-03-16T12:33:38Z,2017-03-19T21:25:19Z,2017-03-19T21:25:19Z,CLOSED,False,31,22,2,https://github.com/orthrus,Fixed high level consumer CPU usage issue,1,[],https://github.com/edenhill/librdkafka/pull/1121,https://github.com/edenhill,4,https://github.com/edenhill/librdkafka/pull/1121#issuecomment-287103795,Fixed an issue that caused the high level consumer to use a lot of CPU due to a queue being continuously signaled while there are no new queue entries available.,"The srcq is not locked in q_concat() (it is assumed to already be locked, or not needed to be locked).
So I think you should just check srcq->rkq_len the first thing you do in q_concat, if it is zero you return immediately, before doing anything else.",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,1121,2017-03-16T12:33:38Z,2017-03-19T21:25:19Z,2017-03-19T21:25:19Z,CLOSED,False,31,22,2,https://github.com/orthrus,Fixed high level consumer CPU usage issue,1,[],https://github.com/edenhill/librdkafka/pull/1121,https://github.com/orthrus,5,https://github.com/edenhill/librdkafka/pull/1121#issuecomment-287293789,Fixed an issue that caused the high level consumer to use a lot of CPU due to a queue being continuously signaled while there are no new queue entries available.,Would that also work for forwarded queues?,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,1121,2017-03-16T12:33:38Z,2017-03-19T21:25:19Z,2017-03-19T21:25:19Z,CLOSED,False,31,22,2,https://github.com/orthrus,Fixed high level consumer CPU usage issue,1,[],https://github.com/edenhill/librdkafka/pull/1121,https://github.com/edenhill,6,https://github.com/edenhill/librdkafka/pull/1121#issuecomment-287394160,Fixed an issue that caused the high level consumer to use a lot of CPU due to a queue being continuously signaled while there are no new queue entries available.,"You are absolutely right, it needs to be checked when srcq->rkq_fwdq is NULL",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,1121,2017-03-16T12:33:38Z,2017-03-19T21:25:19Z,2017-03-19T21:25:19Z,CLOSED,False,31,22,2,https://github.com/orthrus,Fixed high level consumer CPU usage issue,1,[],https://github.com/edenhill/librdkafka/pull/1121,https://github.com/raskolnikoov,7,https://github.com/edenhill/librdkafka/pull/1121#issuecomment-287553906,Fixed an issue that caused the high level consumer to use a lot of CPU due to a queue being continuously signaled while there are no new queue entries available.,Lets release this fix!,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,1121,2017-03-16T12:33:38Z,2017-03-19T21:25:19Z,2017-03-19T21:25:19Z,CLOSED,False,31,22,2,https://github.com/orthrus,Fixed high level consumer CPU usage issue,1,[],https://github.com/edenhill/librdkafka/pull/1121,https://github.com/edenhill,8,https://github.com/edenhill/librdkafka/pull/1121#issuecomment-287648870,Fixed an issue that caused the high level consumer to use a lot of CPU due to a queue being continuously signaled while there are no new queue entries available.,"Thank's a lot of for identifying and fixing this issue @orthrus !
Since this PR would need another round or two of review (whitespace diffs, remove some srcq checks, etc) I took the liberty to push a commit with your fix, giving you proper attribution of course:
5eb6f17",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,1134,2017-03-22T16:59:51Z,2017-03-31T16:17:58Z,2017-03-31T16:18:03Z,MERGED,True,14,0,1,https://github.com/vincentbernat,Use SNI when connecting through SSL,1,[],https://github.com/edenhill/librdkafka/pull/1134,https://github.com/vincentbernat,1,https://github.com/edenhill/librdkafka/pull/1134,"Add TLS SNI (serve name) when connecting with SSL. If the broker is
behind a load balancer, the load balancer can use this information to
locate the right server.
I didn't find an easy way to just get the hostname. When I started, I thought I would just have to use rd_kafka_broker_name(), so the patch would have been quite minimal. Since some parsing is needed, maybe the patch is not worth it. As I have other non-SNI client (Scala client), I don't really need the patch yet. If you have doubts, feel free to drop it.","Add TLS SNI (serve name) when connecting with SSL. If the broker is
behind a load balancer, the load balancer can use this information to
locate the right server.
I didn't find an easy way to just get the hostname. When I started, I thought I would just have to use rd_kafka_broker_name(), so the patch would have been quite minimal. Since some parsing is needed, maybe the patch is not worth it. As I have other non-SNI client (Scala client), I don't really need the patch yet. If you have doubts, feel free to drop it.",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,1134,2017-03-22T16:59:51Z,2017-03-31T16:17:58Z,2017-03-31T16:18:03Z,MERGED,True,14,0,1,https://github.com/vincentbernat,Use SNI when connecting through SSL,1,[],https://github.com/edenhill/librdkafka/pull/1134,https://github.com/edenhill,2,https://github.com/edenhill/librdkafka/pull/1134#issuecomment-290674451,"Add TLS SNI (serve name) when connecting with SSL. If the broker is
behind a load balancer, the load balancer can use this information to
locate the right server.
I didn't find an easy way to just get the hostname. When I started, I thought I would just have to use rd_kafka_broker_name(), so the patch would have been quite minimal. Since some parsing is needed, maybe the patch is not worth it. As I have other non-SNI client (Scala client), I don't really need the patch yet. If you have doubts, feel free to drop it.",Are we good here?,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,1134,2017-03-22T16:59:51Z,2017-03-31T16:17:58Z,2017-03-31T16:18:03Z,MERGED,True,14,0,1,https://github.com/vincentbernat,Use SNI when connecting through SSL,1,[],https://github.com/edenhill/librdkafka/pull/1134,https://github.com/vincentbernat,3,https://github.com/edenhill/librdkafka/pull/1134#issuecomment-290742464,"Add TLS SNI (serve name) when connecting with SSL. If the broker is
behind a load balancer, the load balancer can use this information to
locate the right server.
I didn't find an easy way to just get the hostname. When I started, I thought I would just have to use rd_kafka_broker_name(), so the patch would have been quite minimal. Since some parsing is needed, maybe the patch is not worth it. As I have other non-SNI client (Scala client), I don't really need the patch yet. If you have doubts, feel free to drop it.","Yep, OK for me!",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,1134,2017-03-22T16:59:51Z,2017-03-31T16:17:58Z,2017-03-31T16:18:03Z,MERGED,True,14,0,1,https://github.com/vincentbernat,Use SNI when connecting through SSL,1,[],https://github.com/edenhill/librdkafka/pull/1134,https://github.com/edenhill,4,https://github.com/edenhill/librdkafka/pull/1134#issuecomment-290758664,"Add TLS SNI (serve name) when connecting with SSL. If the broker is
behind a load balancer, the load balancer can use this information to
locate the right server.
I didn't find an easy way to just get the hostname. When I started, I thought I would just have to use rd_kafka_broker_name(), so the patch would have been quite minimal. Since some parsing is needed, maybe the patch is not worth it. As I have other non-SNI client (Scala client), I don't really need the patch yet. If you have doubts, feel free to drop it.",Big thanks!,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,1152,2017-03-31T16:02:12Z,2017-04-05T18:27:28Z,2017-04-05T18:27:28Z,CLOSED,False,10,2,1,https://github.com/rthalley,metadata: align partitions in metadata cache entry [#1150],1,[],https://github.com/edenhill/librdkafka/pull/1152,https://github.com/rthalley,1,https://github.com/edenhill/librdkafka/pull/1152,"This is a possible fix for [#1150].
I didn't make a test case as that was not easy, but I did test it by hand.
The pointer used for rkmce->rkmce_mtopic.partitions was not aligned,
which would cause the code to program to be terminated on platforms
where alignment is required (e.g. SPARC).","This is a possible fix for [#1150].
I didn't make a test case as that was not easy, but I did test it by hand.
The pointer used for rkmce->rkmce_mtopic.partitions was not aligned,
which would cause the code to program to be terminated on platforms
where alignment is required (e.g. SPARC).",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,1157,2017-04-05T18:26:59Z,2017-04-05T18:27:12Z,2017-11-15T08:24:50Z,MERGED,True,1422,195,30,https://github.com/edenhill,Builtin SASL PLAIN and SCRAM support,15,[],https://github.com/edenhill/librdkafka/pull/1157,https://github.com/edenhill,1,https://github.com/edenhill/librdkafka/pull/1157,,,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,1159,2017-04-06T16:36:26Z,2017-05-02T17:33:41Z,2017-05-02T17:33:41Z,CLOSED,False,186,0,2,https://github.com/jbfuzier,Add a Decribe Group Method,2,[],https://github.com/edenhill/librdkafka/pull/1159,https://github.com/jbfuzier,1,https://github.com/edenhill/librdkafka/pull/1159,Expose a function that issue a DescribeGroupsRequest and generate a json string from it.,Expose a function that issue a DescribeGroupsRequest and generate a json string from it.,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,1159,2017-04-06T16:36:26Z,2017-05-02T17:33:41Z,2017-05-02T17:33:41Z,CLOSED,False,186,0,2,https://github.com/jbfuzier,Add a Decribe Group Method,2,[],https://github.com/edenhill/librdkafka/pull/1159,https://github.com/edenhill,2,https://github.com/edenhill/librdkafka/pull/1159#issuecomment-292297577,Expose a function that issue a DescribeGroupsRequest and generate a json string from it.,"Thanks for your contribution.
Some concerns I have:

I dont think librdkafka should provide JSON output for structured data, that's up to the application or binding to construct (correctly and safely) from the rich C structs.
You mention that ListGroups is not available for older brokers, which is true, but in that case neither is DescribeGroups, right? They were both added in the same commit.
There is probably not a real need for an additional API, we should instead fix list_groups() to be a bit smarter with the desired group id and skip the ListGroups step and go directly to DescribeGroup if a groupid is specified, this should cover the use-case that this PR seems to implement.",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,1159,2017-04-06T16:36:26Z,2017-05-02T17:33:41Z,2017-05-02T17:33:41Z,CLOSED,False,186,0,2,https://github.com/jbfuzier,Add a Decribe Group Method,2,[],https://github.com/edenhill/librdkafka/pull/1159,https://github.com/jbfuzier,3,https://github.com/edenhill/librdkafka/pull/1159#issuecomment-292329424,Expose a function that issue a DescribeGroupsRequest and generate a json string from it.,"Thanks for tour reply,


Regarding the JSON I thought it was ok to do this here because that is what is done for the stats ? For the python binding is it ok to build the JSON in the python c module and pass a JSON string to python ?


When I asked my colleague to get listgroups and describegroup rights, he told me that there is no listgroups right available in the version we are using. I will double check with him.


As both APIs where added at the same time there is no need for this, list_groups already has all the data needed.


Sorry for the trouble, I should have double checked before sending a PR",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,1159,2017-04-06T16:36:26Z,2017-05-02T17:33:41Z,2017-05-02T17:33:41Z,CLOSED,False,186,0,2,https://github.com/jbfuzier,Add a Decribe Group Method,2,[],https://github.com/edenhill/librdkafka/pull/1159,https://github.com/jbfuzier,4,https://github.com/edenhill/librdkafka/pull/1159#issuecomment-292586960,Expose a function that issue a DescribeGroupsRequest and generate a json string from it.,"You were right, our cluster does indeed support ListGroup.
The problem is in the java client that allows to change the ACL on the cluster, Describe option is missing for the cluster. If we set the cluster ACL to All it does work, so the issue is in the java client.
I have added a describe_groups method in the python client based on the example in librdkafka : jbfuzier/confluent-kafka-python@68c9bde
Does it seems ok to you ?",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,1164,2017-04-11T05:26:33Z,2017-04-11T06:10:22Z,2017-04-11T06:14:51Z,MERGED,True,1,0,1,https://github.com/nmred,add weiboad/aidp project links,1,[],https://github.com/edenhill/librdkafka/pull/1164,https://github.com/nmred,1,https://github.com/edenhill/librdkafka/pull/1164,"We use librdkafka to develop an embedded Lua script to consume Kafka data process service
Now we have open source it, we hope you add our project
Our project : https://github.com/weiboad/aidp
Thanks","We use librdkafka to develop an embedded Lua script to consume Kafka data process service
Now we have open source it, we hope you add our project
Our project : https://github.com/weiboad/aidp
Thanks",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,1164,2017-04-11T05:26:33Z,2017-04-11T06:10:22Z,2017-04-11T06:14:51Z,MERGED,True,1,0,1,https://github.com/nmred,add weiboad/aidp project links,1,[],https://github.com/edenhill/librdkafka/pull/1164,https://github.com/edenhill,2,https://github.com/edenhill/librdkafka/pull/1164#issuecomment-293160466,"We use librdkafka to develop an embedded Lua script to consume Kafka data process service
Now we have open source it, we hope you add our project
Our project : https://github.com/weiboad/aidp
Thanks",Great!,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,1164,2017-04-11T05:26:33Z,2017-04-11T06:10:22Z,2017-04-11T06:14:51Z,MERGED,True,1,0,1,https://github.com/nmred,add weiboad/aidp project links,1,[],https://github.com/edenhill/librdkafka/pull/1164,https://github.com/nmred,3,https://github.com/edenhill/librdkafka/pull/1164#issuecomment-293161220,"We use librdkafka to develop an embedded Lua script to consume Kafka data process service
Now we have open source it, we hope you add our project
Our project : https://github.com/weiboad/aidp
Thanks",Thanks,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,1166,2017-04-11T12:46:45Z,2017-05-10T11:31:57Z,2017-05-10T11:31:57Z,CLOSED,False,5867,727,27,https://github.com/edenhill,Add and build LZ4 in-tree to avoid external dependency,4,[],https://github.com/edenhill/librdkafka/pull/1166,https://github.com/edenhill,1,https://github.com/edenhill/librdkafka/pull/1166,,,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,1169,2017-04-12T08:28:43Z,2017-04-12T08:32:11Z,2017-04-12T08:32:16Z,MERGED,True,4,5,1,https://github.com/jhecking,Minor edits to introduction,1,[],https://github.com/edenhill/librdkafka/pull/1169,https://github.com/jhecking,1,https://github.com/edenhill/librdkafka/pull/1169,,,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,1169,2017-04-12T08:28:43Z,2017-04-12T08:32:11Z,2017-04-12T08:32:16Z,MERGED,True,4,5,1,https://github.com/jhecking,Minor edits to introduction,1,[],https://github.com/edenhill/librdkafka/pull/1169,https://github.com/edenhill,2,https://github.com/edenhill/librdkafka/pull/1169#issuecomment-293510246,,Thank you!,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,1171,2017-04-12T21:17:16Z,2017-05-11T08:35:54Z,2017-05-11T08:35:54Z,MERGED,True,163,1,7,https://github.com/hqin,added support for stats as events,2,[],https://github.com/edenhill/librdkafka/pull/1171,https://github.com/hqin,1,https://github.com/edenhill/librdkafka/pull/1171,,,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,1171,2017-04-12T21:17:16Z,2017-05-11T08:35:54Z,2017-05-11T08:35:54Z,MERGED,True,163,1,7,https://github.com/hqin,added support for stats as events,2,[],https://github.com/edenhill/librdkafka/pull/1171,https://github.com/edenhill,2,https://github.com/edenhill/librdkafka/pull/1171#issuecomment-299985323,,"ping @hqin , any updates on this?",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,1175,2017-04-14T12:54:15Z,2017-04-14T15:42:57Z,2017-04-14T15:43:00Z,MERGED,True,2,2,2,https://github.com/nyoung,Small grammar nitpick,2,[],https://github.com/edenhill/librdkafka/pull/1175,https://github.com/nyoung,1,https://github.com/edenhill/librdkafka/pull/1175,Just updated a couple of cases where the wrong word was used in the configuration description for topic.metadata.refresh.fast.interval.ms,Just updated a couple of cases where the wrong word was used in the configuration description for topic.metadata.refresh.fast.interval.ms,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,1175,2017-04-14T12:54:15Z,2017-04-14T15:42:57Z,2017-04-14T15:43:00Z,MERGED,True,2,2,2,https://github.com/nyoung,Small grammar nitpick,2,[],https://github.com/edenhill/librdkafka/pull/1175,https://github.com/edenhill,2,https://github.com/edenhill/librdkafka/pull/1175#issuecomment-294176825,Just updated a couple of cases where the wrong word was used in the configuration description for topic.metadata.refresh.fast.interval.ms,Thanks!,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,1178,2017-04-17T16:43:44Z,2017-04-18T16:01:01Z,2017-04-18T16:34:07Z,CLOSED,False,639,480,55,None,Portability toward xcl on z,7,[],https://github.com/edenhill/librdkafka/pull/1178,None,1,https://github.com/edenhill/librdkafka/pull/1178,mostly cosmetic and portability changes to help toward a support for xlc compiler on z/OS,mostly cosmetic and portability changes to help toward a support for xlc compiler on z/OS,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,1178,2017-04-17T16:43:44Z,2017-04-18T16:01:01Z,2017-04-18T16:34:07Z,CLOSED,False,639,480,55,None,Portability toward xcl on z,7,[],https://github.com/edenhill/librdkafka/pull/1178,https://github.com/edenhill,2,https://github.com/edenhill/librdkafka/pull/1178#issuecomment-294860829,mostly cosmetic and portability changes to help toward a support for xlc compiler on z/OS,"Did a quick scan through and it overall looked good.
Some early comments:

avoid white-space diffs (e.g., license cleanups, empty lines, tabs and spaces, etc). The PR should typically only cover one thing, and it will make it smaller and easier to handle. I will fix layout problems for the entire code base (the mix of spaces and tabs especially) in one go later when there is a style checker in place.
I believe ""<HEADER_NAME_H>"" is a more standard notation, e.g.: #ifdef _RDKAFKA_BROKER_H_, please also mark the trailing #endif with a #endif /* _RDKAFKA_BROKER_H_ */ comment.
Add comments for weird defines, such as 370, so that non-zos people knows what it means",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,1178,2017-04-17T16:43:44Z,2017-04-18T16:01:01Z,2017-04-18T16:34:07Z,CLOSED,False,639,480,55,None,Portability toward xcl on z,7,[],https://github.com/edenhill/librdkafka/pull/1178,None,3,https://github.com/edenhill/librdkafka/pull/1178#issuecomment-294887902,mostly cosmetic and portability changes to help toward a support for xlc compiler on z/OS,Note I dropped the white-space fixup patches from the series.. but it would be great if you did clean these up overall.. it is hard to fight finger memory on these. note: (setq-default show-trailing-whitespace t) will make them stick-out like sore thumb,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,1178,2017-04-17T16:43:44Z,2017-04-18T16:01:01Z,2017-04-18T16:34:07Z,CLOSED,False,639,480,55,None,Portability toward xcl on z,7,[],https://github.com/edenhill/librdkafka/pull/1178,None,4,https://github.com/edenhill/librdkafka/pull/1178#issuecomment-294891841,mostly cosmetic and portability changes to help toward a support for xlc compiler on z/OS,sorry I had to change the base branch (to drop whitepace patches for now)  and I do not know on github how to change the source branch of the pull (or even if it is possible) so I'll close this one and open a new one :-(,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,1178,2017-04-17T16:43:44Z,2017-04-18T16:01:01Z,2017-04-18T16:34:07Z,CLOSED,False,639,480,55,None,Portability toward xcl on z,7,[],https://github.com/edenhill/librdkafka/pull/1178,https://github.com/edenhill,5,https://github.com/edenhill/librdkafka/pull/1178#issuecomment-294892378,mostly cosmetic and portability changes to help toward a support for xlc compiler on z/OS,"(setq-default show-trailing-whitespace t) will make them stick-out like sore thumb

I do, along with the show-tabs-when-it-shouldvebeen-spaces-damnit, and I tell you that browsing the source hurts, but that's a good incentive to automate a style enforcer, that's my thinking :)",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,1178,2017-04-17T16:43:44Z,2017-04-18T16:01:01Z,2017-04-18T16:34:07Z,CLOSED,False,639,480,55,None,Portability toward xcl on z,7,[],https://github.com/edenhill/librdkafka/pull/1178,None,6,https://github.com/edenhill/librdkafka/pull/1178#issuecomment-294894343,mostly cosmetic and portability changes to help toward a support for xlc compiler on z/OS,"""show-tabs-when-it-shouldvebeen-spaces-damnit""
wait... I went out of my way to follow the 'linux' style here, which means using tab not space
what is your policy here:  I do not mind adjusting since it is foreign to me anyway, so foreign one way or the other : same difference to me :-)
PS: I normally use : no tabs and 4 indents)
PS2: when I started messign with the code.. I did not have yet an emacs config to adjust the style automatically for this project.. I do now, but all I do is set the c-style to ""linux""  is that correct ?",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,1178,2017-04-17T16:43:44Z,2017-04-18T16:01:01Z,2017-04-18T16:34:07Z,CLOSED,False,639,480,55,None,Portability toward xcl on z,7,[],https://github.com/edenhill/librdkafka/pull/1178,https://github.com/edenhill,7,https://github.com/edenhill/librdkafka/pull/1178#issuecomment-294900033,mostly cosmetic and portability changes to help toward a support for xlc compiler on z/OS,"There is a .dir-locals.el in the top directory which sets the style to linux, but changes tabs to spaces, at least it should.
Also see https://github.com/edenhill/librdkafka/blob/master/CONTRIBUTING.md",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,1178,2017-04-17T16:43:44Z,2017-04-18T16:01:01Z,2017-04-18T16:34:07Z,CLOSED,False,639,480,55,None,Portability toward xcl on z,7,[],https://github.com/edenhill/librdkafka/pull/1178,None,8,https://github.com/edenhill/librdkafka/pull/1178#issuecomment-294902763,mostly cosmetic and portability changes to help toward a support for xlc compiler on z/OS,"""There is a .dir-locals.el""  ahhh sorry too old emacs on my side.. I'll tweak my config to do the tabs to spaces",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,1179,2017-04-18T03:41:20Z,2017-04-18T06:23:49Z,2017-04-18T06:23:52Z,MERGED,True,3,3,1,https://github.com/bryant1410,Fix broken headings in Markdown files,1,[],https://github.com/edenhill/librdkafka/pull/1179,https://github.com/bryant1410,1,https://github.com/edenhill/librdkafka/pull/1179,"GitHub changed the way Markdown headings are parsed, so this change fixes it.
See bryant1410/readmesfix for more information.
Tackles bryant1410/readmesfix#1","GitHub changed the way Markdown headings are parsed, so this change fixes it.
See bryant1410/readmesfix for more information.
Tackles bryant1410/readmesfix#1",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,1179,2017-04-18T03:41:20Z,2017-04-18T06:23:49Z,2017-04-18T06:23:52Z,MERGED,True,3,3,1,https://github.com/bryant1410,Fix broken headings in Markdown files,1,[],https://github.com/edenhill/librdkafka/pull/1179,https://github.com/edenhill,2,https://github.com/edenhill/librdkafka/pull/1179#issuecomment-294699474,"GitHub changed the way Markdown headings are parsed, so this change fixes it.
See bryant1410/readmesfix for more information.
Tackles bryant1410/readmesfix#1",Thank you!,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,1181,2017-04-18T10:25:48Z,2017-04-18T11:11:37Z,2017-04-18T11:11:44Z,MERGED,True,1,0,1,https://github.com/AlexeyRaga,Added hw-kafka to Readme,1,[],https://github.com/edenhill/librdkafka/pull/1181,https://github.com/AlexeyRaga,1,https://github.com/edenhill/librdkafka/pull/1181,"Changes

Updated README to include Haskell ""Kafka ecosystem"" based on librdkafka","Changes

Updated README to include Haskell ""Kafka ecosystem"" based on librdkafka",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,1181,2017-04-18T10:25:48Z,2017-04-18T11:11:37Z,2017-04-18T11:11:44Z,MERGED,True,1,0,1,https://github.com/AlexeyRaga,Added hw-kafka to Readme,1,[],https://github.com/edenhill/librdkafka/pull/1181,https://github.com/edenhill,2,https://github.com/edenhill/librdkafka/pull/1181#issuecomment-294787946,"Changes

Updated README to include Haskell ""Kafka ecosystem"" based on librdkafka",,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,1183,2017-04-18T16:02:12Z,2019-07-12T12:48:05Z,2019-07-12T12:48:05Z,CLOSED,False,254,73,49,None,Compiler compat for z/OS xlc,5,[],https://github.com/edenhill/librdkafka/pull/1183,None,1,https://github.com/edenhill/librdkafka/pull/1183,,,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,1183,2017-04-18T16:02:12Z,2019-07-12T12:48:05Z,2019-07-12T12:48:05Z,CLOSED,False,254,73,49,None,Compiler compat for z/OS xlc,5,[],https://github.com/edenhill/librdkafka/pull/1183,None,2,https://github.com/edenhill/librdkafka/pull/1183#issuecomment-294892761,,"Note I have not addressed yet the 'explain 370' part... bear with me.
Due to many header-order issue I have been having I'd like to introduce a
rdkafka_compat.h header that will be included first in every c source and that
will deal with #define that need to be first and other platform specific define/system header quirk",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,1186,2017-04-21T00:35:45Z,2017-04-21T15:06:56Z,2017-04-21T15:51:14Z,MERGED,True,14,3,3,https://github.com/misterdjules,fix build on SmartOS,1,[],https://github.com/edenhill/librdkafka/pull/1186,https://github.com/misterdjules,1,https://github.com/edenhill/librdkafka/pull/1186,"SmartOS is a derivative of Illumos, itself a derivative of Solaris. Building librdkafka's master branch on a 15.4 image of SmartOS failed with the following error:
[root@e31d952d-043b-4188-e3fa-9ae120820740 /var/tmp/gh-repos/librdkafka]# make -j 6
make[1]: Entering directory '/var/tmp/gh-repos/librdkafka/src'
gcc -MD -MP -g -O2 -fPIC -Wall -Wsign-compare -Wfloat-equal -Wpointer-arith -Wcast-align -D_POSIX_PTHREAD_SEMANTICS -D_REENTRANT -D__EXTENSIONS__ -DLIBRDKAFKA_GIT_VERSION=""\""v0.9.4-132-g386516-dirty\""""  -c rdkafka.c -o rdkafka.o
gcc -MD -MP -g -O2 -fPIC -Wall -Wsign-compare -Wfloat-equal -Wpointer-arith -Wcast-align -D_POSIX_PTHREAD_SEMANTICS -D_REENTRANT -D__EXTENSIONS__ -DLIBRDKAFKA_GIT_VERSION=""\""v0.9.4-132-g386516-dirty\""""  -c rdkafka_broker.c -o rdkafka_broker.o
gcc -MD -MP -g -O2 -fPIC -Wall -Wsign-compare -Wfloat-equal -Wpointer-arith -Wcast-align -D_POSIX_PTHREAD_SEMANTICS -D_REENTRANT -D__EXTENSIONS__ -DLIBRDKAFKA_GIT_VERSION=""\""v0.9.4-132-g386516-dirty\""""  -c rdkafka_msg.c -o rdkafka_msg.o
gcc -MD -MP -g -O2 -fPIC -Wall -Wsign-compare -Wfloat-equal -Wpointer-arith -Wcast-align -D_POSIX_PTHREAD_SEMANTICS -D_REENTRANT -D__EXTENSIONS__ -DLIBRDKAFKA_GIT_VERSION=""\""v0.9.4-132-g386516-dirty\""""  -c rdkafka_topic.c -o rdkafka_topic.o
In file included from /usr/include/stdio.h:37:0,
                 from rd.h:43,
                 from rdkafka_msg.c:29:
/usr/include/sys/feature_tests.h:400:2: error: #error ""Compiler or options invalid; UNIX 03 and POSIX.1-2001 applications       require the use of c99""
 #error ""Compiler or options invalid; UNIX 03 and POSIX.1-2001 applications \
  ^
gcc -MD -MP -g -O2 -fPIC -Wall -Wsign-compare -Wfloat-equal -Wpointer-arith -Wcast-align -D_POSIX_PTHREAD_SEMANTICS -D_REENTRANT -D__EXTENSIONS__ -DLIBRDKAFKA_GIT_VERSION=""\""v0.9.4-132-g386516-dirty\""""  -c rdkafka_conf.c -o rdkafka_conf.o
In file included from /usr/include/stdio.h:37:0,
                 from rd.h:43,
                 from rdkafka_topic.c:29:
/usr/include/sys/feature_tests.h:400:2: error: #error ""Compiler or options invalid; UNIX 03 and POSIX.1-2001 applications       require the use of c99""
 #error ""Compiler or options invalid; UNIX 03 and POSIX.1-2001 applications \
  ^
gcc -MD -MP -g -O2 -fPIC -Wall -Wsign-compare -Wfloat-equal -Wpointer-arith -Wcast-align -D_POSIX_PTHREAD_SEMANTICS -D_REENTRANT -D__EXTENSIONS__ -DLIBRDKAFKA_GIT_VERSION=""\""v0.9.4-132-g386516-dirty\""""  -c rdkafka_timer.c -o rdkafka_timer.o
../mklove/Makefile.base:77: recipe for target 'rdkafka_msg.o' failed
make[1]: *** [rdkafka_msg.o] Error 1
make[1]: *** Waiting for unfinished jobs....
../mklove/Makefile.base:77: recipe for target 'rdkafka_topic.o' failed
make[1]: *** [rdkafka_topic.o] Error 1
make[1]: Leaving directory '/var/tmp/gh-repos/librdkafka/src'
Makefile:20: recipe for target 'libs' failed
make: *** [libs] Error 2
[root@e31d952d-043b-4188-e3fa-9ae120820740 /var/tmp/gh-repos/librdkafka]#

The changes in this PR allows users of SmartOS to build librdkafka without any error.","SmartOS is a derivative of Illumos, itself a derivative of Solaris. Building librdkafka's master branch on a 15.4 image of SmartOS failed with the following error:
[root@e31d952d-043b-4188-e3fa-9ae120820740 /var/tmp/gh-repos/librdkafka]# make -j 6
make[1]: Entering directory '/var/tmp/gh-repos/librdkafka/src'
gcc -MD -MP -g -O2 -fPIC -Wall -Wsign-compare -Wfloat-equal -Wpointer-arith -Wcast-align -D_POSIX_PTHREAD_SEMANTICS -D_REENTRANT -D__EXTENSIONS__ -DLIBRDKAFKA_GIT_VERSION=""\""v0.9.4-132-g386516-dirty\""""  -c rdkafka.c -o rdkafka.o
gcc -MD -MP -g -O2 -fPIC -Wall -Wsign-compare -Wfloat-equal -Wpointer-arith -Wcast-align -D_POSIX_PTHREAD_SEMANTICS -D_REENTRANT -D__EXTENSIONS__ -DLIBRDKAFKA_GIT_VERSION=""\""v0.9.4-132-g386516-dirty\""""  -c rdkafka_broker.c -o rdkafka_broker.o
gcc -MD -MP -g -O2 -fPIC -Wall -Wsign-compare -Wfloat-equal -Wpointer-arith -Wcast-align -D_POSIX_PTHREAD_SEMANTICS -D_REENTRANT -D__EXTENSIONS__ -DLIBRDKAFKA_GIT_VERSION=""\""v0.9.4-132-g386516-dirty\""""  -c rdkafka_msg.c -o rdkafka_msg.o
gcc -MD -MP -g -O2 -fPIC -Wall -Wsign-compare -Wfloat-equal -Wpointer-arith -Wcast-align -D_POSIX_PTHREAD_SEMANTICS -D_REENTRANT -D__EXTENSIONS__ -DLIBRDKAFKA_GIT_VERSION=""\""v0.9.4-132-g386516-dirty\""""  -c rdkafka_topic.c -o rdkafka_topic.o
In file included from /usr/include/stdio.h:37:0,
                 from rd.h:43,
                 from rdkafka_msg.c:29:
/usr/include/sys/feature_tests.h:400:2: error: #error ""Compiler or options invalid; UNIX 03 and POSIX.1-2001 applications       require the use of c99""
 #error ""Compiler or options invalid; UNIX 03 and POSIX.1-2001 applications \
  ^
gcc -MD -MP -g -O2 -fPIC -Wall -Wsign-compare -Wfloat-equal -Wpointer-arith -Wcast-align -D_POSIX_PTHREAD_SEMANTICS -D_REENTRANT -D__EXTENSIONS__ -DLIBRDKAFKA_GIT_VERSION=""\""v0.9.4-132-g386516-dirty\""""  -c rdkafka_conf.c -o rdkafka_conf.o
In file included from /usr/include/stdio.h:37:0,
                 from rd.h:43,
                 from rdkafka_topic.c:29:
/usr/include/sys/feature_tests.h:400:2: error: #error ""Compiler or options invalid; UNIX 03 and POSIX.1-2001 applications       require the use of c99""
 #error ""Compiler or options invalid; UNIX 03 and POSIX.1-2001 applications \
  ^
gcc -MD -MP -g -O2 -fPIC -Wall -Wsign-compare -Wfloat-equal -Wpointer-arith -Wcast-align -D_POSIX_PTHREAD_SEMANTICS -D_REENTRANT -D__EXTENSIONS__ -DLIBRDKAFKA_GIT_VERSION=""\""v0.9.4-132-g386516-dirty\""""  -c rdkafka_timer.c -o rdkafka_timer.o
../mklove/Makefile.base:77: recipe for target 'rdkafka_msg.o' failed
make[1]: *** [rdkafka_msg.o] Error 1
make[1]: *** Waiting for unfinished jobs....
../mklove/Makefile.base:77: recipe for target 'rdkafka_topic.o' failed
make[1]: *** [rdkafka_topic.o] Error 1
make[1]: Leaving directory '/var/tmp/gh-repos/librdkafka/src'
Makefile:20: recipe for target 'libs' failed
make: *** [libs] Error 2
[root@e31d952d-043b-4188-e3fa-9ae120820740 /var/tmp/gh-repos/librdkafka]#

The changes in this PR allows users of SmartOS to build librdkafka without any error.",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,1186,2017-04-21T00:35:45Z,2017-04-21T15:06:56Z,2017-04-21T15:51:14Z,MERGED,True,14,3,3,https://github.com/misterdjules,fix build on SmartOS,1,[],https://github.com/edenhill/librdkafka/pull/1186,https://github.com/edenhill,2,https://github.com/edenhill/librdkafka/pull/1186#issuecomment-296104071,"SmartOS is a derivative of Illumos, itself a derivative of Solaris. Building librdkafka's master branch on a 15.4 image of SmartOS failed with the following error:
[root@e31d952d-043b-4188-e3fa-9ae120820740 /var/tmp/gh-repos/librdkafka]# make -j 6
make[1]: Entering directory '/var/tmp/gh-repos/librdkafka/src'
gcc -MD -MP -g -O2 -fPIC -Wall -Wsign-compare -Wfloat-equal -Wpointer-arith -Wcast-align -D_POSIX_PTHREAD_SEMANTICS -D_REENTRANT -D__EXTENSIONS__ -DLIBRDKAFKA_GIT_VERSION=""\""v0.9.4-132-g386516-dirty\""""  -c rdkafka.c -o rdkafka.o
gcc -MD -MP -g -O2 -fPIC -Wall -Wsign-compare -Wfloat-equal -Wpointer-arith -Wcast-align -D_POSIX_PTHREAD_SEMANTICS -D_REENTRANT -D__EXTENSIONS__ -DLIBRDKAFKA_GIT_VERSION=""\""v0.9.4-132-g386516-dirty\""""  -c rdkafka_broker.c -o rdkafka_broker.o
gcc -MD -MP -g -O2 -fPIC -Wall -Wsign-compare -Wfloat-equal -Wpointer-arith -Wcast-align -D_POSIX_PTHREAD_SEMANTICS -D_REENTRANT -D__EXTENSIONS__ -DLIBRDKAFKA_GIT_VERSION=""\""v0.9.4-132-g386516-dirty\""""  -c rdkafka_msg.c -o rdkafka_msg.o
gcc -MD -MP -g -O2 -fPIC -Wall -Wsign-compare -Wfloat-equal -Wpointer-arith -Wcast-align -D_POSIX_PTHREAD_SEMANTICS -D_REENTRANT -D__EXTENSIONS__ -DLIBRDKAFKA_GIT_VERSION=""\""v0.9.4-132-g386516-dirty\""""  -c rdkafka_topic.c -o rdkafka_topic.o
In file included from /usr/include/stdio.h:37:0,
                 from rd.h:43,
                 from rdkafka_msg.c:29:
/usr/include/sys/feature_tests.h:400:2: error: #error ""Compiler or options invalid; UNIX 03 and POSIX.1-2001 applications       require the use of c99""
 #error ""Compiler or options invalid; UNIX 03 and POSIX.1-2001 applications \
  ^
gcc -MD -MP -g -O2 -fPIC -Wall -Wsign-compare -Wfloat-equal -Wpointer-arith -Wcast-align -D_POSIX_PTHREAD_SEMANTICS -D_REENTRANT -D__EXTENSIONS__ -DLIBRDKAFKA_GIT_VERSION=""\""v0.9.4-132-g386516-dirty\""""  -c rdkafka_conf.c -o rdkafka_conf.o
In file included from /usr/include/stdio.h:37:0,
                 from rd.h:43,
                 from rdkafka_topic.c:29:
/usr/include/sys/feature_tests.h:400:2: error: #error ""Compiler or options invalid; UNIX 03 and POSIX.1-2001 applications       require the use of c99""
 #error ""Compiler or options invalid; UNIX 03 and POSIX.1-2001 applications \
  ^
gcc -MD -MP -g -O2 -fPIC -Wall -Wsign-compare -Wfloat-equal -Wpointer-arith -Wcast-align -D_POSIX_PTHREAD_SEMANTICS -D_REENTRANT -D__EXTENSIONS__ -DLIBRDKAFKA_GIT_VERSION=""\""v0.9.4-132-g386516-dirty\""""  -c rdkafka_timer.c -o rdkafka_timer.o
../mklove/Makefile.base:77: recipe for target 'rdkafka_msg.o' failed
make[1]: *** [rdkafka_msg.o] Error 1
make[1]: *** Waiting for unfinished jobs....
../mklove/Makefile.base:77: recipe for target 'rdkafka_topic.o' failed
make[1]: *** [rdkafka_topic.o] Error 1
make[1]: Leaving directory '/var/tmp/gh-repos/librdkafka/src'
Makefile:20: recipe for target 'libs' failed
make: *** [libs] Error 2
[root@e31d952d-043b-4188-e3fa-9ae120820740 /var/tmp/gh-repos/librdkafka]#

The changes in this PR allows users of SmartOS to build librdkafka without any error.","Looks good to me.
@rthalley can you have a look as well?",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,1186,2017-04-21T00:35:45Z,2017-04-21T15:06:56Z,2017-04-21T15:51:14Z,MERGED,True,14,3,3,https://github.com/misterdjules,fix build on SmartOS,1,[],https://github.com/edenhill/librdkafka/pull/1186,https://github.com/rthalley,3,https://github.com/edenhill/librdkafka/pull/1186#issuecomment-296216575,"SmartOS is a derivative of Illumos, itself a derivative of Solaris. Building librdkafka's master branch on a 15.4 image of SmartOS failed with the following error:
[root@e31d952d-043b-4188-e3fa-9ae120820740 /var/tmp/gh-repos/librdkafka]# make -j 6
make[1]: Entering directory '/var/tmp/gh-repos/librdkafka/src'
gcc -MD -MP -g -O2 -fPIC -Wall -Wsign-compare -Wfloat-equal -Wpointer-arith -Wcast-align -D_POSIX_PTHREAD_SEMANTICS -D_REENTRANT -D__EXTENSIONS__ -DLIBRDKAFKA_GIT_VERSION=""\""v0.9.4-132-g386516-dirty\""""  -c rdkafka.c -o rdkafka.o
gcc -MD -MP -g -O2 -fPIC -Wall -Wsign-compare -Wfloat-equal -Wpointer-arith -Wcast-align -D_POSIX_PTHREAD_SEMANTICS -D_REENTRANT -D__EXTENSIONS__ -DLIBRDKAFKA_GIT_VERSION=""\""v0.9.4-132-g386516-dirty\""""  -c rdkafka_broker.c -o rdkafka_broker.o
gcc -MD -MP -g -O2 -fPIC -Wall -Wsign-compare -Wfloat-equal -Wpointer-arith -Wcast-align -D_POSIX_PTHREAD_SEMANTICS -D_REENTRANT -D__EXTENSIONS__ -DLIBRDKAFKA_GIT_VERSION=""\""v0.9.4-132-g386516-dirty\""""  -c rdkafka_msg.c -o rdkafka_msg.o
gcc -MD -MP -g -O2 -fPIC -Wall -Wsign-compare -Wfloat-equal -Wpointer-arith -Wcast-align -D_POSIX_PTHREAD_SEMANTICS -D_REENTRANT -D__EXTENSIONS__ -DLIBRDKAFKA_GIT_VERSION=""\""v0.9.4-132-g386516-dirty\""""  -c rdkafka_topic.c -o rdkafka_topic.o
In file included from /usr/include/stdio.h:37:0,
                 from rd.h:43,
                 from rdkafka_msg.c:29:
/usr/include/sys/feature_tests.h:400:2: error: #error ""Compiler or options invalid; UNIX 03 and POSIX.1-2001 applications       require the use of c99""
 #error ""Compiler or options invalid; UNIX 03 and POSIX.1-2001 applications \
  ^
gcc -MD -MP -g -O2 -fPIC -Wall -Wsign-compare -Wfloat-equal -Wpointer-arith -Wcast-align -D_POSIX_PTHREAD_SEMANTICS -D_REENTRANT -D__EXTENSIONS__ -DLIBRDKAFKA_GIT_VERSION=""\""v0.9.4-132-g386516-dirty\""""  -c rdkafka_conf.c -o rdkafka_conf.o
In file included from /usr/include/stdio.h:37:0,
                 from rd.h:43,
                 from rdkafka_topic.c:29:
/usr/include/sys/feature_tests.h:400:2: error: #error ""Compiler or options invalid; UNIX 03 and POSIX.1-2001 applications       require the use of c99""
 #error ""Compiler or options invalid; UNIX 03 and POSIX.1-2001 applications \
  ^
gcc -MD -MP -g -O2 -fPIC -Wall -Wsign-compare -Wfloat-equal -Wpointer-arith -Wcast-align -D_POSIX_PTHREAD_SEMANTICS -D_REENTRANT -D__EXTENSIONS__ -DLIBRDKAFKA_GIT_VERSION=""\""v0.9.4-132-g386516-dirty\""""  -c rdkafka_timer.c -o rdkafka_timer.o
../mklove/Makefile.base:77: recipe for target 'rdkafka_msg.o' failed
make[1]: *** [rdkafka_msg.o] Error 1
make[1]: *** Waiting for unfinished jobs....
../mklove/Makefile.base:77: recipe for target 'rdkafka_topic.o' failed
make[1]: *** [rdkafka_topic.o] Error 1
make[1]: Leaving directory '/var/tmp/gh-repos/librdkafka/src'
Makefile:20: recipe for target 'libs' failed
make: *** [libs] Error 2
[root@e31d952d-043b-4188-e3fa-9ae120820740 /var/tmp/gh-repos/librdkafka]#

The changes in this PR allows users of SmartOS to build librdkafka without any error.",I'm OK with those changes.  We always use a compiler in c99 mode and we also remove the define for _XOPEN_SOURCE,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,1186,2017-04-21T00:35:45Z,2017-04-21T15:06:56Z,2017-04-21T15:51:14Z,MERGED,True,14,3,3,https://github.com/misterdjules,fix build on SmartOS,1,[],https://github.com/edenhill/librdkafka/pull/1186,https://github.com/edenhill,4,https://github.com/edenhill/librdkafka/pull/1186#issuecomment-296216886,"SmartOS is a derivative of Illumos, itself a derivative of Solaris. Building librdkafka's master branch on a 15.4 image of SmartOS failed with the following error:
[root@e31d952d-043b-4188-e3fa-9ae120820740 /var/tmp/gh-repos/librdkafka]# make -j 6
make[1]: Entering directory '/var/tmp/gh-repos/librdkafka/src'
gcc -MD -MP -g -O2 -fPIC -Wall -Wsign-compare -Wfloat-equal -Wpointer-arith -Wcast-align -D_POSIX_PTHREAD_SEMANTICS -D_REENTRANT -D__EXTENSIONS__ -DLIBRDKAFKA_GIT_VERSION=""\""v0.9.4-132-g386516-dirty\""""  -c rdkafka.c -o rdkafka.o
gcc -MD -MP -g -O2 -fPIC -Wall -Wsign-compare -Wfloat-equal -Wpointer-arith -Wcast-align -D_POSIX_PTHREAD_SEMANTICS -D_REENTRANT -D__EXTENSIONS__ -DLIBRDKAFKA_GIT_VERSION=""\""v0.9.4-132-g386516-dirty\""""  -c rdkafka_broker.c -o rdkafka_broker.o
gcc -MD -MP -g -O2 -fPIC -Wall -Wsign-compare -Wfloat-equal -Wpointer-arith -Wcast-align -D_POSIX_PTHREAD_SEMANTICS -D_REENTRANT -D__EXTENSIONS__ -DLIBRDKAFKA_GIT_VERSION=""\""v0.9.4-132-g386516-dirty\""""  -c rdkafka_msg.c -o rdkafka_msg.o
gcc -MD -MP -g -O2 -fPIC -Wall -Wsign-compare -Wfloat-equal -Wpointer-arith -Wcast-align -D_POSIX_PTHREAD_SEMANTICS -D_REENTRANT -D__EXTENSIONS__ -DLIBRDKAFKA_GIT_VERSION=""\""v0.9.4-132-g386516-dirty\""""  -c rdkafka_topic.c -o rdkafka_topic.o
In file included from /usr/include/stdio.h:37:0,
                 from rd.h:43,
                 from rdkafka_msg.c:29:
/usr/include/sys/feature_tests.h:400:2: error: #error ""Compiler or options invalid; UNIX 03 and POSIX.1-2001 applications       require the use of c99""
 #error ""Compiler or options invalid; UNIX 03 and POSIX.1-2001 applications \
  ^
gcc -MD -MP -g -O2 -fPIC -Wall -Wsign-compare -Wfloat-equal -Wpointer-arith -Wcast-align -D_POSIX_PTHREAD_SEMANTICS -D_REENTRANT -D__EXTENSIONS__ -DLIBRDKAFKA_GIT_VERSION=""\""v0.9.4-132-g386516-dirty\""""  -c rdkafka_conf.c -o rdkafka_conf.o
In file included from /usr/include/stdio.h:37:0,
                 from rd.h:43,
                 from rdkafka_topic.c:29:
/usr/include/sys/feature_tests.h:400:2: error: #error ""Compiler or options invalid; UNIX 03 and POSIX.1-2001 applications       require the use of c99""
 #error ""Compiler or options invalid; UNIX 03 and POSIX.1-2001 applications \
  ^
gcc -MD -MP -g -O2 -fPIC -Wall -Wsign-compare -Wfloat-equal -Wpointer-arith -Wcast-align -D_POSIX_PTHREAD_SEMANTICS -D_REENTRANT -D__EXTENSIONS__ -DLIBRDKAFKA_GIT_VERSION=""\""v0.9.4-132-g386516-dirty\""""  -c rdkafka_timer.c -o rdkafka_timer.o
../mklove/Makefile.base:77: recipe for target 'rdkafka_msg.o' failed
make[1]: *** [rdkafka_msg.o] Error 1
make[1]: *** Waiting for unfinished jobs....
../mklove/Makefile.base:77: recipe for target 'rdkafka_topic.o' failed
make[1]: *** [rdkafka_topic.o] Error 1
make[1]: Leaving directory '/var/tmp/gh-repos/librdkafka/src'
Makefile:20: recipe for target 'libs' failed
make: *** [libs] Error 2
[root@e31d952d-043b-4188-e3fa-9ae120820740 /var/tmp/gh-repos/librdkafka]#

The changes in this PR allows users of SmartOS to build librdkafka without any error.",Thanks!,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,1186,2017-04-21T00:35:45Z,2017-04-21T15:06:56Z,2017-04-21T15:51:14Z,MERGED,True,14,3,3,https://github.com/misterdjules,fix build on SmartOS,1,[],https://github.com/edenhill/librdkafka/pull/1186,https://github.com/misterdjules,5,https://github.com/edenhill/librdkafka/pull/1186#issuecomment-296228882,"SmartOS is a derivative of Illumos, itself a derivative of Solaris. Building librdkafka's master branch on a 15.4 image of SmartOS failed with the following error:
[root@e31d952d-043b-4188-e3fa-9ae120820740 /var/tmp/gh-repos/librdkafka]# make -j 6
make[1]: Entering directory '/var/tmp/gh-repos/librdkafka/src'
gcc -MD -MP -g -O2 -fPIC -Wall -Wsign-compare -Wfloat-equal -Wpointer-arith -Wcast-align -D_POSIX_PTHREAD_SEMANTICS -D_REENTRANT -D__EXTENSIONS__ -DLIBRDKAFKA_GIT_VERSION=""\""v0.9.4-132-g386516-dirty\""""  -c rdkafka.c -o rdkafka.o
gcc -MD -MP -g -O2 -fPIC -Wall -Wsign-compare -Wfloat-equal -Wpointer-arith -Wcast-align -D_POSIX_PTHREAD_SEMANTICS -D_REENTRANT -D__EXTENSIONS__ -DLIBRDKAFKA_GIT_VERSION=""\""v0.9.4-132-g386516-dirty\""""  -c rdkafka_broker.c -o rdkafka_broker.o
gcc -MD -MP -g -O2 -fPIC -Wall -Wsign-compare -Wfloat-equal -Wpointer-arith -Wcast-align -D_POSIX_PTHREAD_SEMANTICS -D_REENTRANT -D__EXTENSIONS__ -DLIBRDKAFKA_GIT_VERSION=""\""v0.9.4-132-g386516-dirty\""""  -c rdkafka_msg.c -o rdkafka_msg.o
gcc -MD -MP -g -O2 -fPIC -Wall -Wsign-compare -Wfloat-equal -Wpointer-arith -Wcast-align -D_POSIX_PTHREAD_SEMANTICS -D_REENTRANT -D__EXTENSIONS__ -DLIBRDKAFKA_GIT_VERSION=""\""v0.9.4-132-g386516-dirty\""""  -c rdkafka_topic.c -o rdkafka_topic.o
In file included from /usr/include/stdio.h:37:0,
                 from rd.h:43,
                 from rdkafka_msg.c:29:
/usr/include/sys/feature_tests.h:400:2: error: #error ""Compiler or options invalid; UNIX 03 and POSIX.1-2001 applications       require the use of c99""
 #error ""Compiler or options invalid; UNIX 03 and POSIX.1-2001 applications \
  ^
gcc -MD -MP -g -O2 -fPIC -Wall -Wsign-compare -Wfloat-equal -Wpointer-arith -Wcast-align -D_POSIX_PTHREAD_SEMANTICS -D_REENTRANT -D__EXTENSIONS__ -DLIBRDKAFKA_GIT_VERSION=""\""v0.9.4-132-g386516-dirty\""""  -c rdkafka_conf.c -o rdkafka_conf.o
In file included from /usr/include/stdio.h:37:0,
                 from rd.h:43,
                 from rdkafka_topic.c:29:
/usr/include/sys/feature_tests.h:400:2: error: #error ""Compiler or options invalid; UNIX 03 and POSIX.1-2001 applications       require the use of c99""
 #error ""Compiler or options invalid; UNIX 03 and POSIX.1-2001 applications \
  ^
gcc -MD -MP -g -O2 -fPIC -Wall -Wsign-compare -Wfloat-equal -Wpointer-arith -Wcast-align -D_POSIX_PTHREAD_SEMANTICS -D_REENTRANT -D__EXTENSIONS__ -DLIBRDKAFKA_GIT_VERSION=""\""v0.9.4-132-g386516-dirty\""""  -c rdkafka_timer.c -o rdkafka_timer.o
../mklove/Makefile.base:77: recipe for target 'rdkafka_msg.o' failed
make[1]: *** [rdkafka_msg.o] Error 1
make[1]: *** Waiting for unfinished jobs....
../mklove/Makefile.base:77: recipe for target 'rdkafka_topic.o' failed
make[1]: *** [rdkafka_topic.o] Error 1
make[1]: Leaving directory '/var/tmp/gh-repos/librdkafka/src'
Makefile:20: recipe for target 'libs' failed
make: *** [libs] Error 2
[root@e31d952d-043b-4188-e3fa-9ae120820740 /var/tmp/gh-repos/librdkafka]#

The changes in this PR allows users of SmartOS to build librdkafka without any error.","@rthalley
Just to make sure that I understand correctly, when you mentioned:

We always use a compiler in c99 mode and we also remove the define for _XOPEN_SOURCE

Do you mean in librdkafka's source? If so, where is that done?
Thank you very much @edenhill and @rthalley for considering these changes and merging them!",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,1191,2017-04-25T16:15:53Z,2017-05-18T07:32:10Z,2017-06-28T22:21:40Z,MERGED,True,3466,328,53,https://github.com/edenhill,Plugin and interceptor support,22,[],https://github.com/edenhill/librdkafka/pull/1191,https://github.com/edenhill,1,https://github.com/edenhill/librdkafka/pull/1191,Ready for review,Ready for review,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,1191,2017-04-25T16:15:53Z,2017-05-18T07:32:10Z,2017-06-28T22:21:40Z,MERGED,True,3466,328,53,https://github.com/edenhill,Plugin and interceptor support,22,[],https://github.com/edenhill/librdkafka/pull/1191,https://github.com/edenhill,2,https://github.com/edenhill/librdkafka/pull/1191#issuecomment-301027520,Ready for review,@ewencp clusterid() and latency() are needed by some interceptor libraries.,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,1191,2017-04-25T16:15:53Z,2017-05-18T07:32:10Z,2017-06-28T22:21:40Z,MERGED,True,3466,328,53,https://github.com/edenhill,Plugin and interceptor support,22,[],https://github.com/edenhill/librdkafka/pull/1191,https://github.com/ewencp,3,https://github.com/edenhill/librdkafka/pull/1191#issuecomment-301594011,Ready for review,"@edenhill This LGTM aside from the merge conflicts, though I still need to review the code that uses this functionality.",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,1194,2017-04-26T13:55:44Z,2017-05-08T20:43:31Z,2017-05-08T20:43:31Z,CLOSED,False,1,1,1,https://github.com/mimaison,MINOR: Update the description of produce.offset.report,1,[],https://github.com/edenhill/librdkafka/pull/1194,https://github.com/mimaison,1,https://github.com/edenhill/librdkafka/pull/1194,"It works with both delivery callback settings: dr_cb and dr_msg_cb
Found while working with @edoardocomar","It works with both delivery callback settings: dr_cb and dr_msg_cb
Found while working with @edoardocomar",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,1194,2017-04-26T13:55:44Z,2017-05-08T20:43:31Z,2017-05-08T20:43:31Z,CLOSED,False,1,1,1,https://github.com/mimaison,MINOR: Update the description of produce.offset.report,1,[],https://github.com/edenhill/librdkafka/pull/1194,https://github.com/edenhill,2,https://github.com/edenhill/librdkafka/pull/1194#issuecomment-297434039,"It works with both delivery callback settings: dr_cb and dr_msg_cb
Found while working with @edoardocomar","Thanks for this.
CONFIGURATION.md is automatically generated from rdkafka_conf.c on build, so you need to update the original string in rdkafka_conf.c, rebuild, see that CONFIGURATION.md changed accordingly and then commit both files.",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,1199,2017-05-03T16:41:33Z,2017-05-03T17:27:03Z,2017-05-03T17:27:09Z,MERGED,True,2,0,1,https://github.com/matthew-d-jones,Partition count should be per topic in offset request buffer,1,[],https://github.com/edenhill/librdkafka/pull/1199,https://github.com/matthew-d-jones,1,https://github.com/edenhill/librdkafka/pull/1199,"Fixes #1190
offsetsForTimes() is failing when requesting offset for multiple partitions, which span multiple topics, on a single broker. This is because the number of partitions in the request was accumulated for all partitions, but should be per topic.
Note, there is no regression test in place yet, may want this before merging.","Fixes #1190
offsetsForTimes() is failing when requesting offset for multiple partitions, which span multiple topics, on a single broker. This is because the number of partitions in the request was accumulated for all partitions, but should be per topic.
Note, there is no regression test in place yet, may want this before merging.",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,1199,2017-05-03T16:41:33Z,2017-05-03T17:27:03Z,2017-05-03T17:27:09Z,MERGED,True,2,0,1,https://github.com/matthew-d-jones,Partition count should be per topic in offset request buffer,1,[],https://github.com/edenhill/librdkafka/pull/1199,https://github.com/edenhill,2,https://github.com/edenhill/librdkafka/pull/1199#issuecomment-298979117,"Fixes #1190
offsetsForTimes() is failing when requesting offset for multiple partitions, which span multiple topics, on a single broker. This is because the number of partitions in the request was accumulated for all partitions, but should be per topic.
Note, there is no regression test in place yet, may want this before merging.",LGTM. thanks!,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,1202,2017-05-06T01:44:39Z,2017-05-06T06:39:06Z,2017-05-06T06:39:10Z,MERGED,True,1,0,1,https://github.com/mapx,Add Gentoo package infomation,1,[],https://github.com/edenhill/librdkafka/pull/1202,https://github.com/mapx,1,https://github.com/edenhill/librdkafka/pull/1202,,,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,1202,2017-05-06T01:44:39Z,2017-05-06T06:39:06Z,2017-05-06T06:39:10Z,MERGED,True,1,0,1,https://github.com/mapx,Add Gentoo package infomation,1,[],https://github.com/edenhill/librdkafka/pull/1202,https://github.com/edenhill,2,https://github.com/edenhill/librdkafka/pull/1202#issuecomment-299619844,,Thanks!,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,1219,2017-05-18T13:26:32Z,2017-05-24T17:34:24Z,2017-05-24T23:49:37Z,MERGED,True,16,0,1,https://github.com/rthalley,Fix endianism issues that were causing snappy to compress incorrectly on SPARC,1,[],https://github.com/edenhill/librdkafka/pull/1219,https://github.com/rthalley,1,https://github.com/edenhill/librdkafka/pull/1219,"See [Issue #1156] for more info.
The two key things are:


getting __LITTLE_ENDIAN, __BIG_ENDIAN, and __BYTE_ORDER defined
properly


the (uint16_t) part of the
#define htole16(x) ((uint16_t)BSWAP_16(x))


line.
The cast is essential as otherwise in this part of a
macro in snappy_compat.h
typeof((v)) _v = (v); \
memcpy((x), &_v, sizeof(*(x))); })

the value of 'v' has typeof int, not uint16_t, and it takes the 32-bit
extended value and copies the first two bytes, which being big endian,
are zero and not the correct swapped value.","See [Issue #1156] for more info.
The two key things are:


getting __LITTLE_ENDIAN, __BIG_ENDIAN, and __BYTE_ORDER defined
properly


the (uint16_t) part of the
#define htole16(x) ((uint16_t)BSWAP_16(x))


line.
The cast is essential as otherwise in this part of a
macro in snappy_compat.h
typeof((v)) _v = (v); \
memcpy((x), &_v, sizeof(*(x))); })

the value of 'v' has typeof int, not uint16_t, and it takes the 32-bit
extended value and copies the first two bytes, which being big endian,
are zero and not the correct swapped value.",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,1219,2017-05-18T13:26:32Z,2017-05-24T17:34:24Z,2017-05-24T23:49:37Z,MERGED,True,16,0,1,https://github.com/rthalley,Fix endianism issues that were causing snappy to compress incorrectly on SPARC,1,[],https://github.com/edenhill/librdkafka/pull/1219,https://github.com/edenhill,2,https://github.com/edenhill/librdkafka/pull/1219#issuecomment-303796191,"See [Issue #1156] for more info.
The two key things are:


getting __LITTLE_ENDIAN, __BIG_ENDIAN, and __BYTE_ORDER defined
properly


the (uint16_t) part of the
#define htole16(x) ((uint16_t)BSWAP_16(x))


line.
The cast is essential as otherwise in this part of a
macro in snappy_compat.h
typeof((v)) _v = (v); \
memcpy((x), &_v, sizeof(*(x))); })

the value of 'v' has typeof int, not uint16_t, and it takes the 32-bit
extended value and copies the first two bytes, which being big endian,
are zero and not the correct swapped value.","LGTM.
Thank you! I'm trusting you to have verified this, because I can't :)",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,1219,2017-05-18T13:26:32Z,2017-05-24T17:34:24Z,2017-05-24T23:49:37Z,MERGED,True,16,0,1,https://github.com/rthalley,Fix endianism issues that were causing snappy to compress incorrectly on SPARC,1,[],https://github.com/edenhill/librdkafka/pull/1219,https://github.com/rthalley,3,https://github.com/edenhill/librdkafka/pull/1219#issuecomment-303884056,"See [Issue #1156] for more info.
The two key things are:


getting __LITTLE_ENDIAN, __BIG_ENDIAN, and __BYTE_ORDER defined
properly


the (uint16_t) part of the
#define htole16(x) ((uint16_t)BSWAP_16(x))


line.
The cast is essential as otherwise in this part of a
macro in snappy_compat.h
typeof((v)) _v = (v); \
memcpy((x), &_v, sizeof(*(x))); })

the value of 'v' has typeof int, not uint16_t, and it takes the 32-bit
extended value and copies the first two bytes, which being big endian,
are zero and not the correct swapped value.",We definitely verified the patch :),True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,1220,2017-05-18T16:32:09Z,2017-05-23T18:11:01Z,2017-05-23T18:11:22Z,MERGED,True,24,0,3,https://github.com/mhowlett,added serialization and deserialzation related error codes,4,[],https://github.com/edenhill/librdkafka/pull/1220,https://github.com/mhowlett,1,https://github.com/edenhill/librdkafka/pull/1220,"In confluent-kafka-dotnet we would like to expose serialization errors via the OnConsumeError event, which will require corresponding error codes. It's probably best if these are specified in librdkafka as other clients may also want to use them and if so, it would be best if they were consistent across clients.
Rather than put them in the block associated with local error codes, we may want to create a new block for error codes that are only produced by higher level clients (and prefix reasons with Client: not Local:). The argument against that though is it would cause problems if in the future any codes in that block became relevant to librdkafka.","In confluent-kafka-dotnet we would like to expose serialization errors via the OnConsumeError event, which will require corresponding error codes. It's probably best if these are specified in librdkafka as other clients may also want to use them and if so, it would be best if they were consistent across clients.
Rather than put them in the block associated with local error codes, we may want to create a new block for error codes that are only produced by higher level clients (and prefix reasons with Client: not Local:). The argument against that though is it would cause problems if in the future any codes in that block became relevant to librdkafka.",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,1220,2017-05-18T16:32:09Z,2017-05-23T18:11:01Z,2017-05-23T18:11:22Z,MERGED,True,24,0,3,https://github.com/mhowlett,added serialization and deserialzation related error codes,4,[],https://github.com/edenhill/librdkafka/pull/1220,https://github.com/edenhill,2,https://github.com/edenhill/librdkafka/pull/1220#issuecomment-303485814,"In confluent-kafka-dotnet we would like to expose serialization errors via the OnConsumeError event, which will require corresponding error codes. It's probably best if these are specified in librdkafka as other clients may also want to use them and if so, it would be best if they were consistent across clients.
Rather than put them in the block associated with local error codes, we may want to create a new block for error codes that are only produced by higher level clients (and prefix reasons with Client: not Local:). The argument against that though is it would cause problems if in the future any codes in that block became relevant to librdkafka.",Thanks and welcome back to C!,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,1221,2017-05-18T20:12:14Z,2017-05-23T14:27:07Z,2017-05-23T14:27:07Z,MERGED,True,3,0,1,https://github.com/asharma339,"Fix for issue#1211, random crash on aix",3,[],https://github.com/edenhill/librdkafka/pull/1221,https://github.com/asharma339,1,https://github.com/edenhill/librdkafka/pull/1221,"Issue was that we forgot to call rd_atomic32_init() for rd_kafka_op_cnt of rd_kafka_op.c. It mattered only for AIX as for Sun and Linux, rd_atomic32_add/sub operations do not go thru mtx_lock() and mtx_unlock().","Issue was that we forgot to call rd_atomic32_init() for rd_kafka_op_cnt of rd_kafka_op.c. It mattered only for AIX as for Sun and Linux, rd_atomic32_add/sub operations do not go thru mtx_lock() and mtx_unlock().",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,1221,2017-05-18T20:12:14Z,2017-05-23T14:27:07Z,2017-05-23T14:27:07Z,MERGED,True,3,0,1,https://github.com/asharma339,"Fix for issue#1211, random crash on aix",3,[],https://github.com/edenhill/librdkafka/pull/1221,https://github.com/edenhill,2,https://github.com/edenhill/librdkafka/pull/1221#issuecomment-303415442,"Issue was that we forgot to call rd_atomic32_init() for rd_kafka_op_cnt of rd_kafka_op.c. It mattered only for AIX as for Sun and Linux, rd_atomic32_add/sub operations do not go thru mtx_lock() and mtx_unlock().",Thanks for this!,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,1222,2017-05-19T13:37:28Z,2019-07-12T12:48:06Z,2019-07-12T12:48:06Z,CLOSED,False,121,0,7,https://github.com/trthulhu,ISSUE-926: Add SSL CTX cb,1,[],https://github.com/edenhill/librdkafka/pull/1222,https://github.com/trthulhu,1,https://github.com/edenhill/librdkafka/pull/1222,Add in callback configuration for SSL CTX configuration. C and C++ APIs,Add in callback configuration for SSL CTX configuration. C and C++ APIs,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,1222,2017-05-19T13:37:28Z,2019-07-12T12:48:06Z,2019-07-12T12:48:06Z,CLOSED,False,121,0,7,https://github.com/trthulhu,ISSUE-926: Add SSL CTX cb,1,[],https://github.com/edenhill/librdkafka/pull/1222,https://github.com/trthulhu,2,https://github.com/edenhill/librdkafka/pull/1222#issuecomment-303545211,Add in callback configuration for SSL CTX configuration. C and C++ APIs,Thanks for review - I'll hopefully get to it this week (super busy atm though),True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,1222,2017-05-19T13:37:28Z,2019-07-12T12:48:06Z,2019-07-12T12:48:06Z,CLOSED,False,121,0,7,https://github.com/trthulhu,ISSUE-926: Add SSL CTX cb,1,[],https://github.com/edenhill/librdkafka/pull/1222,https://github.com/trthulhu,3,https://github.com/edenhill/librdkafka/pull/1222#issuecomment-320046987,Add in callback configuration for SSL CTX configuration. C and C++ APIs,"Sorry it took so long to incorporate feedback, I've been entirely too busy and promptly forgot to update. I've made some changes now that I remembered: I hope this is alright!",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,1235,2017-05-26T18:04:07Z,2017-05-26T18:05:16Z,2017-05-26T18:05:16Z,CLOSED,False,0,0,0,https://github.com/uvfive,Merge pull request #1 from edenhill/master,1,[],https://github.com/edenhill/librdkafka/pull/1235,https://github.com/uvfive,1,https://github.com/edenhill/librdkafka/pull/1235,"merge on 2016-01-08, for topic group crash issues!","merge on 2016-01-08, for topic group crash issues!",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,1245,2017-05-31T17:51:05Z,2017-06-15T16:16:11Z,2017-06-28T22:21:42Z,MERGED,True,7635,2798,70,https://github.com/edenhill,Support for Message format v2,37,"['enhancement', 'conformance']",https://github.com/edenhill/librdkafka/pull/1245,https://github.com/edenhill,1,https://github.com/edenhill/librdkafka/pull/1245,"This PR contains a quite substantial refactoring of buffers, Produce & Fetch request and response handling, and addition of new MessageSet & Message reader and writer interfaces.
Review instructions
Since this PR is a bit all over the place I'm trying to ease the reviewing burden by organizing the PR into smaller and isolated (where possible) reviewable chunks.
There are some recommended reviewers below.
Pick off this list and update/comment on what parts you are reviewing/reviewed so we know what's left.
Small isolated changes (bite sized):

 minor modifications to how the CRC32C code is initialized and used: 900669d  (but no changes to the algo parts of course)
 Kafka protocol and error code enum update: d676455
 rd_kafka_op_new_fetch_msg() helper: https://github.com/edenhill/librdkafka/pull/1245/files#diff-ce1115b8fe0c6a6153c49786e23858db

Medium sized:

 Varint implementation: varint.c: https://github.com/edenhill/librdkafka/pull/1245/files#diff-fa9f3afadb2f717c80b7e4e8c1aebb04  varint.h: https://github.com/edenhill/librdkafka/pull/1245/files#diff-7e91a1dee72599e69085285502eb6081
 ProduceRequest/Response refactoring: 1cf8167 (@hachikuji)
 Fetch backoff improvements (per partition instead of only per broker): 8321e37 (@ewencp)
 Transport/socket layer changes to use new buffers: https://github.com/edenhill/librdkafka/pull/1245/files#diff-226483c4c83ab9400939e815bcb19564 (@ewencp)
 Changes to rdkafka_broker.c are mostly removed lines (due to refactoring to other files), but some changes were also made to use the new buffering API and the new MsgSet reader API. Ignore the chunks of removed code and focus on the actual changes and additions: https://github.com/edenhill/librdkafka/pull/1245/files#diff-214080cd229adac5b1596529b6eecfe4

Large:

 New buffer implementation, fairly isolated. Focus on rdbuf.c (https://github.com/edenhill/librdkafka/pull/1245/files#diff-db5d6dd5081347e4b6d74434e1ab1971) and rdbuf.h (https://github.com/edenhill/librdkafka/pull/1245/files#diff-f7c9b1e98cb07b002b0da760787eaf91), and to a lesser degree the changes in rdkafka_buf.h (and its rippling affects across the code base): a07001f @ewencp
 MessageSet reader interface (https://github.com/edenhill/librdkafka/pull/1245/files#diff-7e1c6d507dfcbd116ab122581cbe0c1c) @hachikuji
 MessageSet writer interface (https://github.com/edenhill/librdkafka/pull/1245/files#diff-458801fac9fb23317d7bc06b50d4c368) @hachikuji

What can be safely (..) ignored:

the plain CRC32C implementation import in commit c3ed355  (Mark Adler's implementation)
rdkafka_performance tweak: 15ec92c
FreeBSD porting fix: ba0ff8a
the unittest framework: 50c9a66
minor cleanups: 46df23d
a comment: 8c381f6
Makefile updates: de934df
Snappy/LZ4 refactoring (mostly moving code around): 1f788ac","This PR contains a quite substantial refactoring of buffers, Produce & Fetch request and response handling, and addition of new MessageSet & Message reader and writer interfaces.
Review instructions
Since this PR is a bit all over the place I'm trying to ease the reviewing burden by organizing the PR into smaller and isolated (where possible) reviewable chunks.
There are some recommended reviewers below.
Pick off this list and update/comment on what parts you are reviewing/reviewed so we know what's left.
Small isolated changes (bite sized):

 minor modifications to how the CRC32C code is initialized and used: 900669d  (but no changes to the algo parts of course)
 Kafka protocol and error code enum update: d676455
 rd_kafka_op_new_fetch_msg() helper: https://github.com/edenhill/librdkafka/pull/1245/files#diff-ce1115b8fe0c6a6153c49786e23858db

Medium sized:

 Varint implementation: varint.c: https://github.com/edenhill/librdkafka/pull/1245/files#diff-fa9f3afadb2f717c80b7e4e8c1aebb04  varint.h: https://github.com/edenhill/librdkafka/pull/1245/files#diff-7e91a1dee72599e69085285502eb6081
 ProduceRequest/Response refactoring: 1cf8167 (@hachikuji)
 Fetch backoff improvements (per partition instead of only per broker): 8321e37 (@ewencp)
 Transport/socket layer changes to use new buffers: https://github.com/edenhill/librdkafka/pull/1245/files#diff-226483c4c83ab9400939e815bcb19564 (@ewencp)
 Changes to rdkafka_broker.c are mostly removed lines (due to refactoring to other files), but some changes were also made to use the new buffering API and the new MsgSet reader API. Ignore the chunks of removed code and focus on the actual changes and additions: https://github.com/edenhill/librdkafka/pull/1245/files#diff-214080cd229adac5b1596529b6eecfe4

Large:

 New buffer implementation, fairly isolated. Focus on rdbuf.c (https://github.com/edenhill/librdkafka/pull/1245/files#diff-db5d6dd5081347e4b6d74434e1ab1971) and rdbuf.h (https://github.com/edenhill/librdkafka/pull/1245/files#diff-f7c9b1e98cb07b002b0da760787eaf91), and to a lesser degree the changes in rdkafka_buf.h (and its rippling affects across the code base): a07001f @ewencp
 MessageSet reader interface (https://github.com/edenhill/librdkafka/pull/1245/files#diff-7e1c6d507dfcbd116ab122581cbe0c1c) @hachikuji
 MessageSet writer interface (https://github.com/edenhill/librdkafka/pull/1245/files#diff-458801fac9fb23317d7bc06b50d4c368) @hachikuji

What can be safely (..) ignored:

the plain CRC32C implementation import in commit c3ed355  (Mark Adler's implementation)
rdkafka_performance tweak: 15ec92c
FreeBSD porting fix: ba0ff8a
the unittest framework: 50c9a66
minor cleanups: 46df23d
a comment: 8c381f6
Makefile updates: de934df
Snappy/LZ4 refactoring (mostly moving code around): 1f788ac",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,1245,2017-05-31T17:51:05Z,2017-06-15T16:16:11Z,2017-06-28T22:21:42Z,MERGED,True,7635,2798,70,https://github.com/edenhill,Support for Message format v2,37,"['enhancement', 'conformance']",https://github.com/edenhill/librdkafka/pull/1245,https://github.com/mhowlett,2,https://github.com/edenhill/librdkafka/pull/1245#issuecomment-305880651,"This PR contains a quite substantial refactoring of buffers, Produce & Fetch request and response handling, and addition of new MessageSet & Message reader and writer interfaces.
Review instructions
Since this PR is a bit all over the place I'm trying to ease the reviewing burden by organizing the PR into smaller and isolated (where possible) reviewable chunks.
There are some recommended reviewers below.
Pick off this list and update/comment on what parts you are reviewing/reviewed so we know what's left.
Small isolated changes (bite sized):

 minor modifications to how the CRC32C code is initialized and used: 900669d  (but no changes to the algo parts of course)
 Kafka protocol and error code enum update: d676455
 rd_kafka_op_new_fetch_msg() helper: https://github.com/edenhill/librdkafka/pull/1245/files#diff-ce1115b8fe0c6a6153c49786e23858db

Medium sized:

 Varint implementation: varint.c: https://github.com/edenhill/librdkafka/pull/1245/files#diff-fa9f3afadb2f717c80b7e4e8c1aebb04  varint.h: https://github.com/edenhill/librdkafka/pull/1245/files#diff-7e91a1dee72599e69085285502eb6081
 ProduceRequest/Response refactoring: 1cf8167 (@hachikuji)
 Fetch backoff improvements (per partition instead of only per broker): 8321e37 (@ewencp)
 Transport/socket layer changes to use new buffers: https://github.com/edenhill/librdkafka/pull/1245/files#diff-226483c4c83ab9400939e815bcb19564 (@ewencp)
 Changes to rdkafka_broker.c are mostly removed lines (due to refactoring to other files), but some changes were also made to use the new buffering API and the new MsgSet reader API. Ignore the chunks of removed code and focus on the actual changes and additions: https://github.com/edenhill/librdkafka/pull/1245/files#diff-214080cd229adac5b1596529b6eecfe4

Large:

 New buffer implementation, fairly isolated. Focus on rdbuf.c (https://github.com/edenhill/librdkafka/pull/1245/files#diff-db5d6dd5081347e4b6d74434e1ab1971) and rdbuf.h (https://github.com/edenhill/librdkafka/pull/1245/files#diff-f7c9b1e98cb07b002b0da760787eaf91), and to a lesser degree the changes in rdkafka_buf.h (and its rippling affects across the code base): a07001f @ewencp
 MessageSet reader interface (https://github.com/edenhill/librdkafka/pull/1245/files#diff-7e1c6d507dfcbd116ab122581cbe0c1c) @hachikuji
 MessageSet writer interface (https://github.com/edenhill/librdkafka/pull/1245/files#diff-458801fac9fb23317d7bc06b50d4c368) @hachikuji

What can be safely (..) ignored:

the plain CRC32C implementation import in commit c3ed355  (Mark Adler's implementation)
rdkafka_performance tweak: 15ec92c
FreeBSD porting fix: ba0ff8a
the unittest framework: 50c9a66
minor cleanups: 46df23d
a comment: 8c381f6
Makefile updates: de934df
Snappy/LZ4 refactoring (mostly moving code around): 1f788ac",'Added updated Kafka protocol and error enums' commit LGTM (checked details pretty carefully).,True,{'THUMBS_UP': ['https://github.com/edenhill']}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,1255,2017-06-06T03:46:40Z,2017-06-06T10:36:45Z,2017-06-06T10:36:48Z,MERGED,True,1,0,1,https://github.com/mfontanini,Add cppkafka to language bindings,1,[],https://github.com/edenhill/librdkafka/pull/1255,https://github.com/mfontanini,1,https://github.com/edenhill/librdkafka/pull/1255,"I finally released cppkafka, my C++11 rdkafka wrapper, so I think it's a good idea to add it here.","I finally released cppkafka, my C++11 rdkafka wrapper, so I think it's a good idea to add it here.",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,1255,2017-06-06T03:46:40Z,2017-06-06T10:36:45Z,2017-06-06T10:36:48Z,MERGED,True,1,0,1,https://github.com/mfontanini,Add cppkafka to language bindings,1,[],https://github.com/edenhill/librdkafka/pull/1255,https://github.com/edenhill,2,https://github.com/edenhill/librdkafka/pull/1255#issuecomment-306447629,"I finally released cppkafka, my C++11 rdkafka wrapper, so I think it's a good idea to add it here.",Thanks!,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,1264,2017-06-15T11:45:23Z,2017-06-15T19:18:53Z,2017-06-15T21:12:30Z,MERGED,True,1,0,1,https://github.com/theojepsen,Documented hard exit flag for rdkafka_performance,1,[],https://github.com/edenhill/librdkafka/pull/1264,https://github.com/theojepsen,1,https://github.com/edenhill/librdkafka/pull/1264,rdkafka_performance had the undocumented flag -x which causes the tool to hard exit after sending  messages. I documented this flag in the tool's help message.,rdkafka_performance had the undocumented flag -x which causes the tool to hard exit after sending  messages. I documented this flag in the tool's help message.,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,1264,2017-06-15T11:45:23Z,2017-06-15T19:18:53Z,2017-06-15T21:12:30Z,MERGED,True,1,0,1,https://github.com/theojepsen,Documented hard exit flag for rdkafka_performance,1,[],https://github.com/edenhill/librdkafka/pull/1264,https://github.com/edenhill,2,https://github.com/edenhill/librdkafka/pull/1264#issuecomment-308840731,rdkafka_performance had the undocumented flag -x which causes the tool to hard exit after sending  messages. I documented this flag in the tool's help message.,Thanks!,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,1275,2017-06-21T19:07:13Z,2019-07-12T12:48:06Z,2019-07-12T12:48:06Z,CLOSED,False,27,3,2,https://github.com/Quuxplusone,Implement `rd_kafka_topic_clone(rkt)` to increment the application refcnt,1,[],https://github.com/edenhill/librdkafka/pull/1275,https://github.com/Quuxplusone,1,https://github.com/edenhill/librdkafka/pull/1275,Fixes #1274.,Fixes #1274.,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,1275,2017-06-21T19:07:13Z,2019-07-12T12:48:06Z,2019-07-12T12:48:06Z,CLOSED,False,27,3,2,https://github.com/Quuxplusone,Implement `rd_kafka_topic_clone(rkt)` to increment the application refcnt,1,[],https://github.com/edenhill/librdkafka/pull/1275,https://github.com/edenhill,2,https://github.com/edenhill/librdkafka/pull/1275#issuecomment-311309099,Fixes #1274.,"Can you split this up into one PR to fix CMake and one for the clone stuff?
Thanks",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,1275,2017-06-21T19:07:13Z,2019-07-12T12:48:06Z,2019-07-12T12:48:06Z,CLOSED,False,27,3,2,https://github.com/Quuxplusone,Implement `rd_kafka_topic_clone(rkt)` to increment the application refcnt,1,[],https://github.com/edenhill/librdkafka/pull/1275,https://github.com/Quuxplusone,3,https://github.com/edenhill/librdkafka/pull/1275#issuecomment-311497287,Fixes #1274.,Filed the CMake PR as #1284.,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,1278,2017-06-22T14:27:31Z,2017-08-21T10:22:26Z,2017-08-21T10:22:31Z,MERGED,True,1,1,1,https://github.com/treziac,Typo in commentary,1,[],https://github.com/edenhill/librdkafka/pull/1278,https://github.com/treziac,1,https://github.com/edenhill/librdkafka/pull/1278,,,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,1278,2017-06-22T14:27:31Z,2017-08-21T10:22:26Z,2017-08-21T10:22:31Z,MERGED,True,1,1,1,https://github.com/treziac,Typo in commentary,1,[],https://github.com/edenhill/librdkafka/pull/1278,https://github.com/edenhill,2,https://github.com/edenhill/librdkafka/pull/1278#issuecomment-323706851,,Thanks buddy!,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,1284,2017-06-27T21:52:40Z,2017-06-28T07:10:26Z,2017-06-28T07:10:26Z,MERGED,True,40,10,4,https://github.com/Quuxplusone,Try to unbreak CMake buildbots.,3,[],https://github.com/edenhill/librdkafka/pull/1284,https://github.com/Quuxplusone,1,https://github.com/edenhill/librdkafka/pull/1284,,,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,1284,2017-06-27T21:52:40Z,2017-06-28T07:10:26Z,2017-06-28T07:10:26Z,MERGED,True,40,10,4,https://github.com/Quuxplusone,Try to unbreak CMake buildbots.,3,[],https://github.com/edenhill/librdkafka/pull/1284,https://github.com/edenhill,2,https://github.com/edenhill/librdkafka/pull/1284#issuecomment-311576229,,Thanks for this!,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,1289,2017-06-28T21:54:34Z,2017-06-28T22:02:44Z,2017-06-28T22:21:42Z,MERGED,True,10,20,3,https://github.com/edenhill,Call on_acknowledgement() interceptor from internal thread,1,[],https://github.com/edenhill/librdkafka/pull/1289,https://github.com/edenhill,1,https://github.com/edenhill/librdkafka/pull/1289,"To avoid the case where a buggy application does not call poll()
and thus not serving delivery reports, the on_ack..() interceptor
is now called immediately from the broker thread when the
final delivery result is known.","To avoid the case where a buggy application does not call poll()
and thus not serving delivery reports, the on_ack..() interceptor
is now called immediately from the broker thread when the
final delivery result is known.",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,1289,2017-06-28T21:54:34Z,2017-06-28T22:02:44Z,2017-06-28T22:21:42Z,MERGED,True,10,20,3,https://github.com/edenhill,Call on_acknowledgement() interceptor from internal thread,1,[],https://github.com/edenhill/librdkafka/pull/1289,https://github.com/mfontanini,2,https://github.com/edenhill/librdkafka/pull/1289#issuecomment-311806263,"To avoid the case where a buggy application does not call poll()
and thus not serving delivery reports, the on_ack..() interceptor
is now called immediately from the broker thread when the
final delivery result is known.",Does this mean that an application that previously assumed that the delivery deport callback would be executed in the context of the thread calling poll() would now have a potential race condition as this is called from another one?,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,1289,2017-06-28T21:54:34Z,2017-06-28T22:02:44Z,2017-06-28T22:21:42Z,MERGED,True,10,20,3,https://github.com/edenhill,Call on_acknowledgement() interceptor from internal thread,1,[],https://github.com/edenhill/librdkafka/pull/1289,https://github.com/edenhill,3,https://github.com/edenhill/librdkafka/pull/1289#issuecomment-311808579,"To avoid the case where a buggy application does not call poll()
and thus not serving delivery reports, the on_ack..() interceptor
is now called immediately from the broker thread when the
final delivery result is known.","@mfontanini The delivery report was not changed, it is still triggered from poll(), what we did was move the on_acknowledgement() interceptor call to the internal librdkafka broker threads to make sure it gets called regardless of what the application is doing (like, not calling poll() for some time, or at all).",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,1289,2017-06-28T21:54:34Z,2017-06-28T22:02:44Z,2017-06-28T22:21:42Z,MERGED,True,10,20,3,https://github.com/edenhill,Call on_acknowledgement() interceptor from internal thread,1,[],https://github.com/edenhill/librdkafka/pull/1289,https://github.com/mfontanini,4,https://github.com/edenhill/librdkafka/pull/1289#issuecomment-311808810,"To avoid the case where a buggy application does not call poll()
and thus not serving delivery reports, the on_ack..() interceptor
is now called immediately from the broker thread when the
final delivery result is known.","Ah okay, I was worried for a second. Never mind then!",True,{'THUMBS_UP': ['https://github.com/edenhill']}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,1315,2017-07-10T21:25:01Z,2017-07-12T17:18:19Z,2017-11-15T08:24:44Z,MERGED,True,791,195,25,https://github.com/edenhill,Plugin fixes,12,[],https://github.com/edenhill/librdkafka/pull/1315,https://github.com/edenhill,1,https://github.com/edenhill/librdkafka/pull/1315,"This PR addresses:

fixed how interceptors interact with configuration object copying.
the platform-dependent shared-library filename extension is now appended if the initial load fails (allowing the same config  to be used on multiple platforms).
minor small fixes","This PR addresses:

fixed how interceptors interact with configuration object copying.
the platform-dependent shared-library filename extension is now appended if the initial load fails (allowing the same config  to be used on multiple platforms).
minor small fixes",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,1337,2017-07-22T18:14:47Z,2017-07-23T17:10:54Z,2017-07-23T17:10:54Z,CLOSED,False,299,31,4,https://github.com/mikeydee36,In-memory SSL configuration,31,[],https://github.com/edenhill/librdkafka/pull/1337,https://github.com/mikeydee36,1,https://github.com/edenhill/librdkafka/pull/1337,"Allow SSL user-certificate, CA-certificate and private key to be passed in-memory into the library, and configured against the OpenSSL context.
The newly-supported configuration settings are (following the same naming convention as their file-based counterparts):

""ssl.key_inmemory"" - In-memory client private key (ASN.1-encoded DSA or RSA bytes: specify with ssl.key_inmemory_type), used for authentication.
""ssl.key_inmemory_type"" - In-memory client private key type: DSA or RSA.
""ssl.certificate.location_inmemory"" - In-memory X509 certificate bytes used for authentication.
""ssl.ca.location_inmemory"" - In-memory X509 CA certificate bytes for verifying the broker's key.","Allow SSL user-certificate, CA-certificate and private key to be passed in-memory into the library, and configured against the OpenSSL context.
The newly-supported configuration settings are (following the same naming convention as their file-based counterparts):

""ssl.key_inmemory"" - In-memory client private key (ASN.1-encoded DSA or RSA bytes: specify with ssl.key_inmemory_type), used for authentication.
""ssl.key_inmemory_type"" - In-memory client private key type: DSA or RSA.
""ssl.certificate.location_inmemory"" - In-memory X509 certificate bytes used for authentication.
""ssl.ca.location_inmemory"" - In-memory X509 CA certificate bytes for verifying the broker's key.",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,1337,2017-07-22T18:14:47Z,2017-07-23T17:10:54Z,2017-07-23T17:10:54Z,CLOSED,False,299,31,4,https://github.com/mikeydee36,In-memory SSL configuration,31,[],https://github.com/edenhill/librdkafka/pull/1337,https://github.com/mikeydee36,2,https://github.com/edenhill/librdkafka/pull/1337#issuecomment-317204465,"Allow SSL user-certificate, CA-certificate and private key to be passed in-memory into the library, and configured against the OpenSSL context.
The newly-supported configuration settings are (following the same naming convention as their file-based counterparts):

""ssl.key_inmemory"" - In-memory client private key (ASN.1-encoded DSA or RSA bytes: specify with ssl.key_inmemory_type), used for authentication.
""ssl.key_inmemory_type"" - In-memory client private key type: DSA or RSA.
""ssl.certificate.location_inmemory"" - In-memory X509 certificate bytes used for authentication.
""ssl.ca.location_inmemory"" - In-memory X509 CA certificate bytes for verifying the broker's key.","Hi, getting an error on the cmake check - looks like the source-file ""0067-empty_topic.cpp"" needs to be added to tests/CMakeLists.txt.
I'm happy to make any suggested changes :)",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,1338,2017-07-23T18:35:49Z,2019-05-07T07:32:48Z,2019-05-07T07:32:49Z,CLOSED,False,280,12,4,https://github.com/mikeydee36,Support for in-memory SSL configuration,1,[],https://github.com/edenhill/librdkafka/pull/1338,https://github.com/mikeydee36,1,https://github.com/edenhill/librdkafka/pull/1338,"Allow the SSL user-certificate, CA-certificate and private key to be passed in-memory into the library and configured against the OpenSSL context.
The newly-supported configuration settings are (following the same convention as their file-based counterparts):

""ssl.key_inmemory"" - In-memory client private key (ASN.1-encoded DSA or RSA bytes: specify with ssl.key_inmemory_type), used for authentication.
""ssl.key_inmemory_type"" - In-memory client private key type: DSA or RSA.
""ssl.certificate.location_inmemory"" - In-memory X509 certificate bytes used for authentication.
""ssl.ca.location_inmemory"" - In-memory X509 CA certificate bytes for verifying the broker's key.","Allow the SSL user-certificate, CA-certificate and private key to be passed in-memory into the library and configured against the OpenSSL context.
The newly-supported configuration settings are (following the same convention as their file-based counterparts):

""ssl.key_inmemory"" - In-memory client private key (ASN.1-encoded DSA or RSA bytes: specify with ssl.key_inmemory_type), used for authentication.
""ssl.key_inmemory_type"" - In-memory client private key type: DSA or RSA.
""ssl.certificate.location_inmemory"" - In-memory X509 certificate bytes used for authentication.
""ssl.ca.location_inmemory"" - In-memory X509 CA certificate bytes for verifying the broker's key.",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,1338,2017-07-23T18:35:49Z,2019-05-07T07:32:48Z,2019-05-07T07:32:49Z,CLOSED,False,280,12,4,https://github.com/mikeydee36,Support for in-memory SSL configuration,1,[],https://github.com/edenhill/librdkafka/pull/1338,https://github.com/mikeydee36,2,https://github.com/edenhill/librdkafka/pull/1338#issuecomment-317273254,"Allow the SSL user-certificate, CA-certificate and private key to be passed in-memory into the library and configured against the OpenSSL context.
The newly-supported configuration settings are (following the same convention as their file-based counterparts):

""ssl.key_inmemory"" - In-memory client private key (ASN.1-encoded DSA or RSA bytes: specify with ssl.key_inmemory_type), used for authentication.
""ssl.key_inmemory_type"" - In-memory client private key type: DSA or RSA.
""ssl.certificate.location_inmemory"" - In-memory X509 certificate bytes used for authentication.
""ssl.ca.location_inmemory"" - In-memory X509 CA certificate bytes for verifying the broker's key.","Hi, getting an error on the cmake check - looks like the source-file ""0067-empty_topic.cpp"" needs to be added to tests/CMakeLists.txt.
I'm happy to make any suggested changes :)",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,1338,2017-07-23T18:35:49Z,2019-05-07T07:32:48Z,2019-05-07T07:32:49Z,CLOSED,False,280,12,4,https://github.com/mikeydee36,Support for in-memory SSL configuration,1,[],https://github.com/edenhill/librdkafka/pull/1338,https://github.com/edenhill,3,https://github.com/edenhill/librdkafka/pull/1338#issuecomment-317668021,"Allow the SSL user-certificate, CA-certificate and private key to be passed in-memory into the library and configured against the OpenSSL context.
The newly-supported configuration settings are (following the same convention as their file-based counterparts):

""ssl.key_inmemory"" - In-memory client private key (ASN.1-encoded DSA or RSA bytes: specify with ssl.key_inmemory_type), used for authentication.
""ssl.key_inmemory_type"" - In-memory client private key type: DSA or RSA.
""ssl.certificate.location_inmemory"" - In-memory X509 certificate bytes used for authentication.
""ssl.ca.location_inmemory"" - In-memory X509 CA certificate bytes for verifying the broker's key.","Thank you for this, will review it soon!
The CMake build is maintained by the community and you don't need to bother getting it to pass.",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,1338,2017-07-23T18:35:49Z,2019-05-07T07:32:48Z,2019-05-07T07:32:49Z,CLOSED,False,280,12,4,https://github.com/mikeydee36,Support for in-memory SSL configuration,1,[],https://github.com/edenhill/librdkafka/pull/1338,https://github.com/edenhill,4,https://github.com/edenhill/librdkafka/pull/1338#issuecomment-489968174,"Allow the SSL user-certificate, CA-certificate and private key to be passed in-memory into the library and configured against the OpenSSL context.
The newly-supported configuration settings are (following the same convention as their file-based counterparts):

""ssl.key_inmemory"" - In-memory client private key (ASN.1-encoded DSA or RSA bytes: specify with ssl.key_inmemory_type), used for authentication.
""ssl.key_inmemory_type"" - In-memory client private key type: DSA or RSA.
""ssl.certificate.location_inmemory"" - In-memory X509 certificate bytes used for authentication.
""ssl.ca.location_inmemory"" - In-memory X509 CA certificate bytes for verifying the broker's key.",Being worked on in #2309,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,1369,2017-08-08T20:11:48Z,2017-08-08T21:04:48Z,2017-08-08T21:04:48Z,MERGED,True,4,1,1,https://github.com/gnpdt,sasl_cyrus: Fix dangling stack ptr to sasl_callback_t (#1329),1,[],https://github.com/edenhill/librdkafka/pull/1369,https://github.com/gnpdt,1,https://github.com/edenhill/librdkafka/pull/1369,"sasl_client_new does not copy its callback argument array, resulting
in a pointer to transient stack memory. This patch moves the callbacks
array into the rd_kafka_cyrus_state_t which has the same lifetime
as the connection and thus will remain valid.","sasl_client_new does not copy its callback argument array, resulting
in a pointer to transient stack memory. This patch moves the callbacks
array into the rd_kafka_cyrus_state_t which has the same lifetime
as the connection and thus will remain valid.",True,{'THUMBS_UP': ['https://github.com/hackerwin7']}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,1369,2017-08-08T20:11:48Z,2017-08-08T21:04:48Z,2017-08-08T21:04:48Z,MERGED,True,4,1,1,https://github.com/gnpdt,sasl_cyrus: Fix dangling stack ptr to sasl_callback_t (#1329),1,[],https://github.com/edenhill/librdkafka/pull/1369,https://github.com/gnpdt,2,https://github.com/edenhill/librdkafka/pull/1369#issuecomment-321068377,"sasl_client_new does not copy its callback argument array, resulting
in a pointer to transient stack memory. This patch moves the callbacks
array into the rd_kafka_cyrus_state_t which has the same lifetime
as the connection and thus will remain valid.","Valgrind picks this up, e.g. on RHEL7, v0.11.0:
==143978== Invalid read of size 8
==143978==    at 0x6B0FB5F: _sasl_getcallback (in /usr/lib64/libsasl2.so.3.0.0)
==143978==    by 0x6B1A428: _plug_get_simple (in /usr/lib64/libsasl2.so.3.0.0)
==143978==    by 0xB8D453B: ??? (in /usr/lib64/sasl2/libgssapiv2.so.3.0.0)
==143978==    by 0x6B0E7F4: sasl_client_step (in /usr/lib64/libsasl2.so.3.0.0)
==143978==    by 0x176A695: rd_kafka_sasl_cyrus_recv (rdkafka_sasl_cyrus.c:73)
==143978==    by 0x1757D6A: rd_kafka_sasl_io_event (rdkafka_sasl.c:137)
==143978==    by 0x1713ECC: rd_kafka_transport_io_event (rdkafka_transport.c:1247)
==143978==    by 0x17039E9: rd_kafka_broker_serve (rdkafka_broker.c:2208)
==143978==    by 0x1703E1B: rd_kafka_broker_ua_idle (rdkafka_broker.c:2270)
==143978==    by 0x170423C: rd_kafka_broker_thread_main (rdkafka_broker.c:3119)
==143978==    by 0x17509E6: _thrd_wrapper_function (tinycthread.c:624)
==143978==    by 0x769CDC4: start_thread (pthread_create.c:308)
==143978==  Address 0xd6e3c10 is on thread 5's stack
==143978==  1552 bytes below stack pointer",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,1375,2017-08-10T17:29:55Z,2017-08-10T17:33:20Z,2017-08-10T17:33:20Z,MERGED,True,22,11,3,https://github.com/lalinsky,Fix CMake build with SASL enabled,1,[],https://github.com/edenhill/librdkafka/pull/1375,https://github.com/lalinsky,1,https://github.com/edenhill/librdkafka/pull/1375,"When building librdkafka with SASL enabled (which happens by default if you have libsasl2 installed), the compilation fails because the macros
WITH_SASL_SCRAM and WITH_SASL_CYRUS are never defined and things in the code depend on them:
[ 34%] Building C object src/CMakeFiles/rdkafka.dir/rdkafka_sasl_cyrus.c.o
/home/lukas/work/librdkafka/src/rdkafka_sasl_cyrus.c: In function rd_kafka_sasl_cyrus_broker_term:
/home/lukas/work/librdkafka/src/rdkafka_sasl_cyrus.c:528:49: error: rd_kafka_broker_t {aka struct rd_kafka_broker_s} has no member named rkb_sasl_kinit_refresh_tmr
         rd_kafka_timer_stop(&rk->rk_timers, &rkb->rkb_sasl_kinit_refresh_tmr,1);
                                                 ^
/home/lukas/work/librdkafka/src/rdkafka_sasl_cyrus.c: In function rd_kafka_sasl_cyrus_broker_init:
/home/lukas/work/librdkafka/src/rdkafka_sasl_cyrus.c:543:50: error: rd_kafka_broker_t {aka struct rd_kafka_broker_s} has no member named rkb_sasl_kinit_refresh_tmr
         rd_kafka_timer_start(&rk->rk_timers, &rkb->rkb_sasl_kinit_refresh_tmr,
                                                  ^
src/CMakeFiles/rdkafka.dir/build.make:1214: recipe for target 'src/CMakeFiles/rdkafka.dir/rdkafka_sasl_cyrus.c.o' failed

After setting the macros in config.h from CMake, the build completes without problems.","When building librdkafka with SASL enabled (which happens by default if you have libsasl2 installed), the compilation fails because the macros
WITH_SASL_SCRAM and WITH_SASL_CYRUS are never defined and things in the code depend on them:
[ 34%] Building C object src/CMakeFiles/rdkafka.dir/rdkafka_sasl_cyrus.c.o
/home/lukas/work/librdkafka/src/rdkafka_sasl_cyrus.c: In function rd_kafka_sasl_cyrus_broker_term:
/home/lukas/work/librdkafka/src/rdkafka_sasl_cyrus.c:528:49: error: rd_kafka_broker_t {aka struct rd_kafka_broker_s} has no member named rkb_sasl_kinit_refresh_tmr
         rd_kafka_timer_stop(&rk->rk_timers, &rkb->rkb_sasl_kinit_refresh_tmr,1);
                                                 ^
/home/lukas/work/librdkafka/src/rdkafka_sasl_cyrus.c: In function rd_kafka_sasl_cyrus_broker_init:
/home/lukas/work/librdkafka/src/rdkafka_sasl_cyrus.c:543:50: error: rd_kafka_broker_t {aka struct rd_kafka_broker_s} has no member named rkb_sasl_kinit_refresh_tmr
         rd_kafka_timer_start(&rk->rk_timers, &rkb->rkb_sasl_kinit_refresh_tmr,
                                                  ^
src/CMakeFiles/rdkafka.dir/build.make:1214: recipe for target 'src/CMakeFiles/rdkafka.dir/rdkafka_sasl_cyrus.c.o' failed

After setting the macros in config.h from CMake, the build completes without problems.",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,1387,2017-08-21T18:46:48Z,2017-08-22T16:57:55Z,2017-11-15T08:24:41Z,MERGED,True,214,7,8,https://github.com/edenhill,"Fix 1386, messed up Null/Empty handling in the Consumer",3,[],https://github.com/edenhill/librdkafka/pull/1387,https://github.com/edenhill,1,https://github.com/edenhill/librdkafka/pull/1387,This also fixes an off-by-one error for the Producer's returned offsets when produce.offset.report=false (default),This also fixes an off-by-one error for the Producer's returned offsets when produce.offset.report=false (default),True,"{'THUMBS_UP': ['https://github.com/webmakersteve', 'https://github.com/edoardocomar']}"
edenhill/librdkafka,https://github.com/edenhill/librdkafka,1387,2017-08-21T18:46:48Z,2017-08-22T16:57:55Z,2017-11-15T08:24:41Z,MERGED,True,214,7,8,https://github.com/edenhill,"Fix 1386, messed up Null/Empty handling in the Consumer",3,[],https://github.com/edenhill/librdkafka/pull/1387,https://github.com/mhowlett,2,https://github.com/edenhill/librdkafka/pull/1387#issuecomment-323846776,This also fixes an off-by-one error for the Producer's returned offsets when produce.offset.report=false (default),I don't see any problems - LGTM conditional on you having verified the dotnet integration tests pass.,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,1387,2017-08-21T18:46:48Z,2017-08-22T16:57:55Z,2017-11-15T08:24:41Z,MERGED,True,214,7,8,https://github.com/edenhill,"Fix 1386, messed up Null/Empty handling in the Consumer",3,[],https://github.com/edenhill/librdkafka/pull/1387,https://github.com/webmakersteve,3,https://github.com/edenhill/librdkafka/pull/1387#issuecomment-324103186,This also fixes an off-by-one error for the Producer's returned offsets when produce.offset.report=false (default),Do you know when this will make it into a release?,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,1387,2017-08-21T18:46:48Z,2017-08-22T16:57:55Z,2017-11-15T08:24:41Z,MERGED,True,214,7,8,https://github.com/edenhill,"Fix 1386, messed up Null/Empty handling in the Consumer",3,[],https://github.com/edenhill/librdkafka/pull/1387,https://github.com/edenhill,4,https://github.com/edenhill/librdkafka/pull/1387#issuecomment-324138225,This also fixes an off-by-one error for the Producer's returned offsets when produce.offset.report=false (default),"@webmakersteve We don't have a set release date yet, but within the next 30 days.",True,"{'THUMBS_UP': ['https://github.com/webmakersteve', 'https://github.com/patrixgdd', 'https://github.com/ancashoria']}"
edenhill/librdkafka,https://github.com/edenhill/librdkafka,1387,2017-08-21T18:46:48Z,2017-08-22T16:57:55Z,2017-11-15T08:24:41Z,MERGED,True,214,7,8,https://github.com/edenhill,"Fix 1386, messed up Null/Empty handling in the Consumer",3,[],https://github.com/edenhill/librdkafka/pull/1387,https://github.com/patrixgdd,5,https://github.com/edenhill/librdkafka/pull/1387#issuecomment-325557626,This also fixes an off-by-one error for the Producer's returned offsets when produce.offset.report=false (default),"Hi, I have an issue on the consumer which is running scala kstream with 0.11.0.0 client, but the producer is node-rdkafka with 0.9.5.0 that the consumer has shutdown once it tried to connect the kafka 0.11 server! I should not show the code here because it is not related to the topic.
Looking forward to this 0.11.0 release to see if it is the nodejs issue.
Thanks a lot.",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,1394,2017-08-25T19:24:27Z,2017-09-06T14:17:27Z,2017-09-06T14:17:27Z,MERGED,True,2,2,1,https://github.com/kevinburke,Transport: fix typo in function name,1,[],https://github.com/edenhill/librdkafka/pull/1394,https://github.com/kevinburke,1,https://github.com/edenhill/librdkafka/pull/1394,,,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,1394,2017-08-25T19:24:27Z,2017-09-06T14:17:27Z,2017-09-06T14:17:27Z,MERGED,True,2,2,1,https://github.com/kevinburke,Transport: fix typo in function name,1,[],https://github.com/edenhill/librdkafka/pull/1394,https://github.com/edenhill,2,https://github.com/edenhill/librdkafka/pull/1394#issuecomment-327416687,,Please remove the whitespace diffs,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,1394,2017-08-25T19:24:27Z,2017-09-06T14:17:27Z,2017-09-06T14:17:27Z,MERGED,True,2,2,1,https://github.com/kevinburke,Transport: fix typo in function name,1,[],https://github.com/edenhill/librdkafka/pull/1394,https://github.com/kevinburke,3,https://github.com/edenhill/librdkafka/pull/1394#issuecomment-327495483,,Done,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,1394,2017-08-25T19:24:27Z,2017-09-06T14:17:27Z,2017-09-06T14:17:27Z,MERGED,True,2,2,1,https://github.com/kevinburke,Transport: fix typo in function name,1,[],https://github.com/edenhill/librdkafka/pull/1394,https://github.com/edenhill,4,https://github.com/edenhill/librdkafka/pull/1394#issuecomment-327497522,,Thank you!,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,1399,2017-08-29T09:58:20Z,2018-06-25T12:50:45Z,2020-05-08T08:57:42Z,CLOSED,False,412,4,17,https://github.com/edenhill,Delete topics,2,[],https://github.com/edenhill/librdkafka/pull/1399,https://github.com/edenhill,1,https://github.com/edenhill/librdkafka/pull/1399,"I needed this to speed up testing runs (using the kafka-topics.sh script to delete multiple topics is too slow).
@mhowlett While reviewing this, think of what it would look like in the .NET client.","I needed this to speed up testing runs (using the kafka-topics.sh script to delete multiple topics is too slow).
@mhowlett While reviewing this, think of what it would look like in the .NET client.",True,{'THUMBS_UP': ['https://github.com/TiMESPLiNTER']}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,1399,2017-08-29T09:58:20Z,2018-06-25T12:50:45Z,2020-05-08T08:57:42Z,CLOSED,False,412,4,17,https://github.com/edenhill,Delete topics,2,[],https://github.com/edenhill/librdkafka/pull/1399,https://github.com/mhowlett,2,https://github.com/edenhill/librdkafka/pull/1399#issuecomment-325719803,"I needed this to speed up testing runs (using the kafka-topics.sh script to delete multiple topics is too slow).
@mhowlett While reviewing this, think of what it would look like in the .NET client.","I like that there is no new handle type (i.e. RD_KAFKA_ADMIN).
In the bindings, I think we should have another class AdminClient that exposes this method. This could be constructed using a bootstrap server address, or a Handle exposed as a property on a Producer or Consumer (which is opaque from a users perspective). I also think we should move the generic methods like GetWatermarkOffsets currently on Producer / Consumer to this new AdminClient.",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,1402,2017-08-30T17:07:29Z,2017-09-07T18:43:07Z,2017-09-07T18:43:07Z,MERGED,True,8,3,4,https://github.com/vinipuh5,Issue #1277: Add parameter for setting apiVersion timeout request,5,[],https://github.com/edenhill/librdkafka/pull/1402,https://github.com/vinipuh5,1,https://github.com/edenhill/librdkafka/pull/1402,"Allow to config broker version request timeout by setting ""api.version.request.timeout.ms""","Allow to config broker version request timeout by setting ""api.version.request.timeout.ms""",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,1402,2017-08-30T17:07:29Z,2017-09-07T18:43:07Z,2017-09-07T18:43:07Z,MERGED,True,8,3,4,https://github.com/vinipuh5,Issue #1277: Add parameter for setting apiVersion timeout request,5,[],https://github.com/edenhill/librdkafka/pull/1402,https://github.com/vinipuh5,2,https://github.com/edenhill/librdkafka/pull/1402#issuecomment-326206393,"Allow to config broker version request timeout by setting ""api.version.request.timeout.ms""",@edenhill Done,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,1402,2017-08-30T17:07:29Z,2017-09-07T18:43:07Z,2017-09-07T18:43:07Z,MERGED,True,8,3,4,https://github.com/vinipuh5,Issue #1277: Add parameter for setting apiVersion timeout request,5,[],https://github.com/edenhill/librdkafka/pull/1402,https://github.com/vinipuh5,3,https://github.com/edenhill/librdkafka/pull/1402#issuecomment-326930410,"Allow to config broker version request timeout by setting ""api.version.request.timeout.ms""",@edenhill Is it ok that there's a test which failed in doozer/target/cmake-xenial-amd64? I noticed that this test fails in a lot of pull requests.,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,1402,2017-08-30T17:07:29Z,2017-09-07T18:43:07Z,2017-09-07T18:43:07Z,MERGED,True,8,3,4,https://github.com/vinipuh5,Issue #1277: Add parameter for setting apiVersion timeout request,5,[],https://github.com/edenhill/librdkafka/pull/1402,https://github.com/edenhill,4,https://github.com/edenhill/librdkafka/pull/1402#issuecomment-326930737,"Allow to config broker version request timeout by setting ""api.version.request.timeout.ms""","Yeah, I'm not maintaing the contributed cmake build system, I leave that to the community. So any cmake errors can be ignored for now.
Will look into this PR during this week.
Thanks",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,1406,2017-09-02T19:16:25Z,2017-09-03T06:31:37Z,2017-09-03T23:47:10Z,MERGED,True,3,4,1,https://github.com/Jose123456,Remove unused parameters,1,[],https://github.com/edenhill/librdkafka/pull/1406,https://github.com/Jose123456,1,https://github.com/edenhill/librdkafka/pull/1406,"There is no ""-s"" command line option. The opaque pointer passed to
msg_consume is not used by that function.","There is no ""-s"" command line option. The opaque pointer passed to
msg_consume is not used by that function.",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,1406,2017-09-02T19:16:25Z,2017-09-03T06:31:37Z,2017-09-03T23:47:10Z,MERGED,True,3,4,1,https://github.com/Jose123456,Remove unused parameters,1,[],https://github.com/edenhill/librdkafka/pull/1406,https://github.com/edenhill,2,https://github.com/edenhill/librdkafka/pull/1406#issuecomment-326787219,"There is no ""-s"" command line option. The opaque pointer passed to
msg_consume is not used by that function.",Thanks for this,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,1406,2017-09-02T19:16:25Z,2017-09-03T06:31:37Z,2017-09-03T23:47:10Z,MERGED,True,3,4,1,https://github.com/Jose123456,Remove unused parameters,1,[],https://github.com/edenhill/librdkafka/pull/1406,https://github.com/Jose123456,3,https://github.com/edenhill/librdkafka/pull/1406#issuecomment-326840004,"There is no ""-s"" command line option. The opaque pointer passed to
msg_consume is not used by that function.","You're welcome, and thank you for the speedy merge",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,1411,2017-09-05T08:44:13Z,2017-09-05T08:44:23Z,2017-11-15T08:24:39Z,MERGED,True,31,0,2,https://github.com/edenhill,Thread-safe rd_strerror() (#1410),1,[],https://github.com/edenhill/librdkafka/pull/1411,https://github.com/edenhill,1,https://github.com/edenhill/librdkafka/pull/1411,,,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,1417,2017-09-11T15:19:47Z,2017-09-19T05:52:47Z,2017-09-19T05:52:47Z,MERGED,True,19,14,2,https://github.com/mhowlett,property description clarifications,3,[],https://github.com/edenhill/librdkafka/pull/1417,https://github.com/mhowlett,1,https://github.com/edenhill/librdkafka/pull/1417,"@edenhill - some things in configuration.md I think would benefit from clarification (... and which I may or may not be interpreting correctly... please check carefully!).
Also: max.in.flight.requests.per.connection is a [*] property, but it's only producers that will ever have more than one request in-flight right? (because you want the last retrieved offset to construct the next response on the consumer). I think that would be worth noting if my understanding is correct here.","@edenhill - some things in configuration.md I think would benefit from clarification (... and which I may or may not be interpreting correctly... please check carefully!).
Also: max.in.flight.requests.per.connection is a [*] property, but it's only producers that will ever have more than one request in-flight right? (because you want the last retrieved offset to construct the next response on the consumer). I think that would be worth noting if my understanding is correct here.",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,1417,2017-09-11T15:19:47Z,2017-09-19T05:52:47Z,2017-09-19T05:52:47Z,MERGED,True,19,14,2,https://github.com/mhowlett,property description clarifications,3,[],https://github.com/edenhill/librdkafka/pull/1417,https://github.com/edenhill,2,https://github.com/edenhill/librdkafka/pull/1417#issuecomment-328609433,"@edenhill - some things in configuration.md I think would benefit from clarification (... and which I may or may not be interpreting correctly... please check carefully!).
Also: max.in.flight.requests.per.connection is a [*] property, but it's only producers that will ever have more than one request in-flight right? (because you want the last retrieved offset to construct the next response on the consumer). I think that would be worth noting if my understanding is correct here.","So max.in.flight.. is a generic property that is applied to all broker communication, but it doesn't do that much of a difference for anything but the Producer since other mechanisms (the fetcher) make sure there is only one outstanding Fetch request for the Consumer anyway.
It would affect things like asynchronous high-rate Offset commits, but it is highly questionable if there is any use to limit those by this property.",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,1417,2017-09-11T15:19:47Z,2017-09-19T05:52:47Z,2017-09-19T05:52:47Z,MERGED,True,19,14,2,https://github.com/mhowlett,property description clarifications,3,[],https://github.com/edenhill/librdkafka/pull/1417,https://github.com/mhowlett,3,https://github.com/edenhill/librdkafka/pull/1417#issuecomment-330247813,"@edenhill - some things in configuration.md I think would benefit from clarification (... and which I may or may not be interpreting correctly... please check carefully!).
Also: max.in.flight.requests.per.connection is a [*] property, but it's only producers that will ever have more than one request in-flight right? (because you want the last retrieved offset to construct the next response on the consumer). I think that would be worth noting if my understanding is correct here.","Yep, I noticed the bad indentation after pushing and did an additional commit to fix this up - did you miss that, or was this wrong as well? (it looks ok to me).",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,1426,2017-09-21T10:13:45Z,2017-10-04T18:30:50Z,2017-11-15T08:24:34Z,MERGED,True,1423,7,20,https://github.com/edenhill,Improved release automation and instructions,6,[],https://github.com/edenhill/librdkafka/pull/1426,https://github.com/edenhill,1,https://github.com/edenhill/librdkafka/pull/1426,Semi-automation for NuGet and Homebrew,Semi-automation for NuGet and Homebrew,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,1426,2017-09-21T10:13:45Z,2017-10-04T18:30:50Z,2017-11-15T08:24:34Z,MERGED,True,1423,7,20,https://github.com/edenhill,Improved release automation and instructions,6,[],https://github.com/edenhill/librdkafka/pull/1426,https://github.com/mageshn,2,https://github.com/edenhill/librdkafka/pull/1426#issuecomment-334050375,Semi-automation for NuGet and Homebrew,Don't find anything glary...LGTM,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,1431,2017-09-24T19:11:11Z,2017-09-25T08:13:12Z,2017-09-25T08:13:18Z,MERGED,True,1,0,1,https://github.com/silviucpp,Add erlang binding in readme,1,[],https://github.com/edenhill/librdkafka/pull/1431,https://github.com/silviucpp,1,https://github.com/edenhill/librdkafka/pull/1431,,,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,1431,2017-09-24T19:11:11Z,2017-09-25T08:13:12Z,2017-09-25T08:13:18Z,MERGED,True,1,0,1,https://github.com/silviucpp,Add erlang binding in readme,1,[],https://github.com/edenhill/librdkafka/pull/1431,https://github.com/edenhill,2,https://github.com/edenhill/librdkafka/pull/1431#issuecomment-331808502,,"Great stuff, thanks!",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,1446,2017-10-08T01:01:46Z,2017-10-11T11:00:14Z,2017-10-11T11:00:20Z,MERGED,True,16,9,1,https://github.com/tudor,Do not assume LZ4 worst (best?) case 255x compression,4,[],https://github.com/edenhill/librdkafka/pull/1446,https://github.com/tudor,1,https://github.com/edenhill/librdkafka/pull/1446,"Fix #1445
Self-explanatory, see the issue description for details.
cc: @igorcanadi","Fix #1445
Self-explanatory, see the issue description for details.
cc: @igorcanadi",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,1446,2017-10-08T01:01:46Z,2017-10-11T11:00:14Z,2017-10-11T11:00:20Z,MERGED,True,16,9,1,https://github.com/tudor,Do not assume LZ4 worst (best?) case 255x compression,4,[],https://github.com/edenhill/librdkafka/pull/1446,https://github.com/edenhill,2,https://github.com/edenhill/librdkafka/pull/1446#issuecomment-335773916,"Fix #1445
Self-explanatory, see the issue description for details.
cc: @igorcanadi",Thanks alot for this!,True,{'THUMBS_UP': ['https://github.com/tudor']}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,1450,2017-10-09T13:46:46Z,2017-10-09T15:27:10Z,2017-10-09T15:27:10Z,MERGED,True,8,2,1,https://github.com/tbsaunde,rdkafka-performance: busy wait to wait short periods of time,1,[],https://github.com/edenhill/librdkafka/pull/1450,https://github.com/tbsaunde,1,https://github.com/edenhill/librdkafka/pull/1450,"On linux once a process is done sleeping it takes about 5 microseconds
to start running again. By default linux allows 50 microseconds of
slack in the length of sleep() calls to optimize the number of wakeups.
The amount of timerslack can be reduced to 1 nanosecond, but the time
to get rescheduled and run does not appear to be easily modifiable.  As
a result it is not really possible to use a sleep() variant to wait for
less than 55 microseconds, and for lengths of time not much more than
that it will not be terribly accurate.  So if we need to produce
messages more frequently than every 100 microseconds we busy wait
instead of calling usleep().  100 is a somewhat arbitrary number, but
its close to twice the minimum length of time a process would sleep by
default.  This is also suboptimal for single core machines, but people
probably don't develope kafka related software on those, and if they do
producing messages at the kind of rate where this matters will be rather
difficult.","On linux once a process is done sleeping it takes about 5 microseconds
to start running again. By default linux allows 50 microseconds of
slack in the length of sleep() calls to optimize the number of wakeups.
The amount of timerslack can be reduced to 1 nanosecond, but the time
to get rescheduled and run does not appear to be easily modifiable.  As
a result it is not really possible to use a sleep() variant to wait for
less than 55 microseconds, and for lengths of time not much more than
that it will not be terribly accurate.  So if we need to produce
messages more frequently than every 100 microseconds we busy wait
instead of calling usleep().  100 is a somewhat arbitrary number, but
its close to twice the minimum length of time a process would sleep by
default.  This is also suboptimal for single core machines, but people
probably don't develope kafka related software on those, and if they do
producing messages at the kind of rate where this matters will be rather
difficult.",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,1451,2017-10-09T17:35:15Z,2018-01-09T22:14:04Z,2018-01-10T10:55:54Z,MERGED,True,51,10,5,https://github.com/tbsaunde,tell the os about the names of threads,1,[],https://github.com/edenhill/librdkafka/pull/1451,https://github.com/tbsaunde,1,https://github.com/edenhill/librdkafka/pull/1451,,,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,1451,2017-10-09T17:35:15Z,2018-01-09T22:14:04Z,2018-01-10T10:55:54Z,MERGED,True,51,10,5,https://github.com/tbsaunde,tell the os about the names of threads,1,[],https://github.com/edenhill/librdkafka/pull/1451,https://github.com/edenhill,2,https://github.com/edenhill/librdkafka/pull/1451#issuecomment-335769045,,"The thread name is also used in librdkafka logging and there is no length restriction there, and having unique thread names is critical from a logging/troubleshooting perspective (otherwise the thread name is irrelevant).
The only practical way to keep broker thread names unique and meaningful is to use the broker name and port (i.e., rkb_name), which will most of the time be longer than 15 characters.
I understand that it is nice to have somewhat meaningful thread names in ps/top output, are there any other reasons?
Maybe we should have two thread names per thread:

the existing rd_kafka_thread_name[] unchanged
a short form descriptive non-unique thread name for pthread: ""rdkafka:main"", ""rdkafka:broker""",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,1451,2017-10-09T17:35:15Z,2018-01-09T22:14:04Z,2018-01-10T10:55:54Z,MERGED,True,51,10,5,https://github.com/tbsaunde,tell the os about the names of threads,1,[],https://github.com/edenhill/librdkafka/pull/1451,https://github.com/edenhill,3,https://github.com/edenhill/librdkafka/pull/1451#issuecomment-356431640,,Thanks again!,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,1451,2017-10-09T17:35:15Z,2018-01-09T22:14:04Z,2018-01-10T10:55:54Z,MERGED,True,51,10,5,https://github.com/tbsaunde,tell the os about the names of threads,1,[],https://github.com/edenhill/librdkafka/pull/1451,https://github.com/edenhill,4,https://github.com/edenhill/librdkafka/pull/1451#issuecomment-356568344,,"Example output from top -H:
29341 user    20   0  646112 289348   5456 R 80,0  0,9   0:05.04 rdkafka_perform                           
29345 user    20   0  646112 289348   5456 R 13,3  0,9   0:01.04 rdk:broker3                               
29346 user    20   0  646112 289348   5456 R 13,3  0,9   0:00.63 rdk:broker4                               
29344 user    20   0  646112 289348   5456 R  6,7  0,9   0:00.85 rdk:broker2                               
29342 user    20   0  646112 289348   5456 S  0,0  0,9   0:00.00 rdk:main                                  
29343 user    20   0  646112 289348   5456 S  0,0  0,9   0:00.00 rdk:broker-1",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,1452,2017-10-10T20:52:41Z,2018-01-09T19:49:11Z,2018-01-09T19:49:52Z,MERGED,True,13,8,2,https://github.com/tbsaunde,avoid holding the toppar lock across a write syscall,2,[],https://github.com/edenhill/librdkafka/pull/1452,https://github.com/tbsaunde,1,https://github.com/edenhill/librdkafka/pull/1452,"Holding locks across syscalls is generally not a great idea since it
means spending longer with the lock and increases the chances the
process will spend time holding the lock but descheduled.  In this case
it is especially unfortunate because the write will make a broker thread
runnable if it is blocked on poll, but if the broker thread runs before
the thread calling write() it will immediately block again trying to
take the toppar lock.  This appears to reduce worst case latency by at
least 1ms, and quiet possibly much more, its hard to know exactly how
much should be attributed to this instead of other freek events hurting
worst case latency.  Its somewhat unfortunate we need to do a little
more work unconditionally, but the atomic accesses can be optimized
somewhat, and it may be a good idea to use the wakeup fd system more.","Holding locks across syscalls is generally not a great idea since it
means spending longer with the lock and increases the chances the
process will spend time holding the lock but descheduled.  In this case
it is especially unfortunate because the write will make a broker thread
runnable if it is blocked on poll, but if the broker thread runs before
the thread calling write() it will immediately block again trying to
take the toppar lock.  This appears to reduce worst case latency by at
least 1ms, and quiet possibly much more, its hard to know exactly how
much should be attributed to this instead of other freek events hurting
worst case latency.  Its somewhat unfortunate we need to do a little
more work unconditionally, but the atomic accesses can be optimized
somewhat, and it may be a good idea to use the wakeup fd system more.",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,1452,2017-10-10T20:52:41Z,2018-01-09T19:49:11Z,2018-01-09T19:49:52Z,MERGED,True,13,8,2,https://github.com/tbsaunde,avoid holding the toppar lock across a write syscall,2,[],https://github.com/edenhill/librdkafka/pull/1452,https://github.com/tbsaunde,2,https://github.com/edenhill/librdkafka/pull/1452#issuecomment-338685128,"Holding locks across syscalls is generally not a great idea since it
means spending longer with the lock and increases the chances the
process will spend time holding the lock but descheduled.  In this case
it is especially unfortunate because the write will make a broker thread
runnable if it is blocked on poll, but if the broker thread runs before
the thread calling write() it will immediately block again trying to
take the toppar lock.  This appears to reduce worst case latency by at
least 1ms, and quiet possibly much more, its hard to know exactly how
much should be attributed to this instead of other freek events hurting
worst case latency.  Its somewhat unfortunate we need to do a little
more work unconditionally, but the atomic accesses can be optimized
somewhat, and it may be a good idea to use the wakeup fd system more.","@edenhill, I believe I've addressed all your comments, can you please take another look?",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,1452,2017-10-10T20:52:41Z,2018-01-09T19:49:11Z,2018-01-09T19:49:52Z,MERGED,True,13,8,2,https://github.com/tbsaunde,avoid holding the toppar lock across a write syscall,2,[],https://github.com/edenhill/librdkafka/pull/1452,https://github.com/edenhill,3,https://github.com/edenhill/librdkafka/pull/1452#issuecomment-356394076,"Holding locks across syscalls is generally not a great idea since it
means spending longer with the lock and increases the chances the
process will spend time holding the lock but descheduled.  In this case
it is especially unfortunate because the write will make a broker thread
runnable if it is blocked on poll, but if the broker thread runs before
the thread calling write() it will immediately block again trying to
take the toppar lock.  This appears to reduce worst case latency by at
least 1ms, and quiet possibly much more, its hard to know exactly how
much should be attributed to this instead of other freek events hurting
worst case latency.  Its somewhat unfortunate we need to do a little
more work unconditionally, but the atomic accesses can be optimized
somewhat, and it may be a good idea to use the wakeup fd system more.",Thanks for this! Let's hope it works out as well as we think :),True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,1453,2017-10-11T18:01:11Z,2017-12-23T16:43:28Z,2018-01-05T13:33:24Z,MERGED,True,3,1,1,https://github.com/mfontanini,Don't store invalid offset as next one when pausing,1,[],https://github.com/edenhill/librdkafka/pull/1453,https://github.com/mfontanini,1,https://github.com/edenhill/librdkafka/pull/1453,"This fixes the issue where a consumer subscribes to a topic and then pause consumption where all consumer offsets are initially already EOF. Without this check, the offset that's used after resuming consumption is invalid, which ends up falling back to whatever auto.reset.policy is being used, which in turn makes the consumer ""forget"" about which offset was actually already committed.
This fixes #1307. I'm not sure if this has any undesired side effects but at least I'm sure I'm not hitting the issue anymore.","This fixes the issue where a consumer subscribes to a topic and then pause consumption where all consumer offsets are initially already EOF. Without this check, the offset that's used after resuming consumption is invalid, which ends up falling back to whatever auto.reset.policy is being used, which in turn makes the consumer ""forget"" about which offset was actually already committed.
This fixes #1307. I'm not sure if this has any undesired side effects but at least I'm sure I'm not hitting the issue anymore.",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,1453,2017-10-11T18:01:11Z,2017-12-23T16:43:28Z,2018-01-05T13:33:24Z,MERGED,True,3,1,1,https://github.com/mfontanini,Don't store invalid offset as next one when pausing,1,[],https://github.com/edenhill/librdkafka/pull/1453,https://github.com/mfontanini,2,https://github.com/edenhill/librdkafka/pull/1453#issuecomment-344348567,"This fixes the issue where a consumer subscribes to a topic and then pause consumption where all consumer offsets are initially already EOF. Without this check, the offset that's used after resuming consumption is invalid, which ends up falling back to whatever auto.reset.policy is being used, which in turn makes the consumer ""forget"" about which offset was actually already committed.
This fixes #1307. I'm not sure if this has any undesired side effects but at least I'm sure I'm not hitting the issue anymore.",Any updates on this one? I haven't hit the issue again after applying this fix.,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,1453,2017-10-11T18:01:11Z,2017-12-23T16:43:28Z,2018-01-05T13:33:24Z,MERGED,True,3,1,1,https://github.com/mfontanini,Don't store invalid offset as next one when pausing,1,[],https://github.com/edenhill/librdkafka/pull/1453,https://github.com/edenhill,3,https://github.com/edenhill/librdkafka/pull/1453#issuecomment-353736169,"This fixes the issue where a consumer subscribes to a topic and then pause consumption where all consumer offsets are initially already EOF. Without this check, the offset that's used after resuming consumption is invalid, which ends up falling back to whatever auto.reset.policy is being used, which in turn makes the consumer ""forget"" about which offset was actually already committed.
This fixes #1307. I'm not sure if this has any undesired side effects but at least I'm sure I'm not hitting the issue anymore.","Thank you for this clean fix!
I'm adding a test-case separately",True,{'THUMBS_UP': ['https://github.com/mfontanini']}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,1453,2017-10-11T18:01:11Z,2017-12-23T16:43:28Z,2018-01-05T13:33:24Z,MERGED,True,3,1,1,https://github.com/mfontanini,Don't store invalid offset as next one when pausing,1,[],https://github.com/edenhill/librdkafka/pull/1453,https://github.com/edenhill,4,https://github.com/edenhill/librdkafka/pull/1453#issuecomment-355555524,"This fixes the issue where a consumer subscribes to a topic and then pause consumption where all consumer offsets are initially already EOF. Without this check, the offset that's used after resuming consumption is invalid, which ends up falling back to whatever auto.reset.policy is being used, which in turn makes the consumer ""forget"" about which offset was actually already committed.
This fixes #1307. I'm not sure if this has any undesired side effects but at least I'm sure I'm not hitting the issue anymore.",Is this fixing #1435  as well you think?,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,1455,2017-10-12T18:52:39Z,2017-10-12T19:19:53Z,2017-10-12T19:19:53Z,CLOSED,False,831,52,11,https://github.com/barrotsteindev,Murmur2 partitioner,8,[],https://github.com/edenhill/librdkafka/pull/1455,https://github.com/barrotsteindev,1,https://github.com/edenhill/librdkafka/pull/1455,This branch includes murmur2 partitioner for rd_kafka producer,This branch includes murmur2 partitioner for rd_kafka producer,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,1460,2017-10-16T16:25:40Z,2017-10-19T16:50:03Z,2017-10-19T16:50:10Z,MERGED,True,1,1,1,https://github.com/proller,Raise cmake minimum version to 3.2,1,[],https://github.com/edenhill/librdkafka/pull/1460,https://github.com/proller,1,https://github.com/edenhill/librdkafka/pull/1460,3.2 is maximal available version from ppa for ubuntu trusty,3.2 is maximal available version from ppa for ubuntu trusty,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,1460,2017-10-16T16:25:40Z,2017-10-19T16:50:03Z,2017-10-19T16:50:10Z,MERGED,True,1,1,1,https://github.com/proller,Raise cmake minimum version to 3.2,1,[],https://github.com/edenhill/librdkafka/pull/1460,https://github.com/edenhill,2,https://github.com/edenhill/librdkafka/pull/1460#issuecomment-337969036,3.2 is maximal available version from ppa for ubuntu trusty,Thanks!,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,1465,2017-10-17T21:56:21Z,2017-10-18T06:42:12Z,2017-10-18T06:42:12Z,MERGED,True,6,1,1,https://github.com/mhowlett,allow nuget version to be specified independent of version tag,1,[],https://github.com/edenhill/librdkafka/pull/1465,https://github.com/mhowlett,1,https://github.com/edenhill/librdkafka/pull/1465,"@edenhill - this is much easier than unzipping, manually editing files and re-zipping. I needed to automate this so it might as well be here.","@edenhill - this is much easier than unzipping, manually editing files and re-zipping. I needed to automate this so it might as well be here.",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,1468,2017-10-18T19:59:25Z,2017-12-12T20:01:32Z,2017-12-12T20:01:41Z,MERGED,True,615,2,11,https://github.com/barrotsteindev,Murmur2 partitioner,33,[],https://github.com/edenhill/librdkafka/pull/1468,https://github.com/barrotsteindev,1,https://github.com/edenhill/librdkafka/pull/1468,"Add murmur2 partitioner, currently using the fastest algorithm for murmur2, which is not endian neutral. Also implements an endian neutral algorithm which can be used instead.
Fixes part of issue #1424","Add murmur2 partitioner, currently using the fastest algorithm for murmur2, which is not endian neutral. Also implements an endian neutral algorithm which can be used instead.
Fixes part of issue #1424",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,1468,2017-10-18T19:59:25Z,2017-12-12T20:01:32Z,2017-12-12T20:01:41Z,MERGED,True,615,2,11,https://github.com/barrotsteindev,Murmur2 partitioner,33,[],https://github.com/edenhill/librdkafka/pull/1468,https://github.com/edenhill,2,https://github.com/edenhill/librdkafka/pull/1468#issuecomment-337873707,"Add murmur2 partitioner, currently using the fastest algorithm for murmur2, which is not endian neutral. Also implements an endian neutral algorithm which can be used instead.
Fixes part of issue #1424","Thanks for this, much appreciated and anticipated!
I will review this more careful in a while, but from a quick glance there are some low hanging fruit that needs fixing:

remove all white space diffs
remove the CONFIGURATION.md changes (it is auto generated from your build options)
add a LICENSE.murmur2 file to the top level
is there any proof of correctness for the provided implementation?
I'd like to see some unit tests to verify it produces the same result as the Java counterpart (look at rd_varint.c and rd_unittest.c to see how unit tests are added)
I dont think the new rdkafka_simple_producer_murmur2 example is warranted, about 99% of it is duplicated from existing examples. It might be better to include it in one of the other examples as an option.
There are comments in the murmur code that it must be able to perform unaligned reads, this is not portable.
There are also endian comments in the murmur code. Does it work on big-endian?
Please follow the coding guideline: use 8 whitespaces for tab",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,1468,2017-10-18T19:59:25Z,2017-12-12T20:01:32Z,2017-12-12T20:01:41Z,MERGED,True,615,2,11,https://github.com/barrotsteindev,Murmur2 partitioner,33,[],https://github.com/edenhill/librdkafka/pull/1468,https://github.com/barrotsteindev,3,https://github.com/edenhill/librdkafka/pull/1468#issuecomment-338011145,"Add murmur2 partitioner, currently using the fastest algorithm for murmur2, which is not endian neutral. Also implements an endian neutral algorithm which can be used instead.
Fixes part of issue #1424","I will use the endian neutral hash.
The murmur2hash.c file is from a repo that has no direct mention of a license.
Will fix the tabs and configuration file.
Thanks for your patience I'm fairly new to this.",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,1468,2017-10-18T19:59:25Z,2017-12-12T20:01:32Z,2017-12-12T20:01:41Z,MERGED,True,615,2,11,https://github.com/barrotsteindev,Murmur2 partitioner,33,[],https://github.com/edenhill/librdkafka/pull/1468,https://github.com/barrotsteindev,4,https://github.com/edenhill/librdkafka/pull/1468#issuecomment-338115739,"Add murmur2 partitioner, currently using the fastest algorithm for murmur2, which is not endian neutral. Also implements an endian neutral algorithm which can be used instead.
Fixes part of issue #1424","this should probably be the hash function (MurmurHashNeutral2) which will be used by librdkafka,
since it seems to be endian neutral and can aligned readable
// MurmurHashNeutral2, by Austin Appleby
//
// Same as MurmurHash2, but endian- and alignment-neutral.
// Half the speed though, alas.
*/
would be lovely if another set of eyes would take a look at that implementation to see if there's anything suspicious. I will test it against the java implementation shortly",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,1468,2017-10-18T19:59:25Z,2017-12-12T20:01:32Z,2017-12-12T20:01:41Z,MERGED,True,615,2,11,https://github.com/barrotsteindev,Murmur2 partitioner,33,[],https://github.com/edenhill/librdkafka/pull/1468,https://github.com/barrotsteindev,5,https://github.com/edenhill/librdkafka/pull/1468#issuecomment-338489723,"Add murmur2 partitioner, currently using the fastest algorithm for murmur2, which is not endian neutral. Also implements an endian neutral algorithm which can be used instead.
Fixes part of issue #1424",I hope I addressed all of your requests with these commits,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,1468,2017-10-18T19:59:25Z,2017-12-12T20:01:32Z,2017-12-12T20:01:41Z,MERGED,True,615,2,11,https://github.com/barrotsteindev,Murmur2 partitioner,33,[],https://github.com/edenhill/librdkafka/pull/1468,https://github.com/barrotsteindev,6,https://github.com/edenhill/librdkafka/pull/1468#issuecomment-340188833,"Add murmur2 partitioner, currently using the fastest algorithm for murmur2, which is not endian neutral. Also implements an endian neutral algorithm which can be used instead.
Fixes part of issue #1424","Sure thing, I will change it.

On Oct 28, 2017 10:59 AM, ""Magnus Edenhill"" ***@***.***> wrote:
 ***@***.**** requested changes on this pull request.

 Can you also please rebase this on current master and fixup / squash the
 commits?
 ------------------------------

 In src/rdkafka_msg.c
 <#1468 (comment)>:

 > @@ -570,6 +541,35 @@ int32_t rd_kafka_msg_partitioner_consistent_random (const rd_kafka_topic_t *rkt,
                                                   msg_opaque);
  }

 +int32_t rd_kafka_msg_partitioner_murmur2_consistent (const rd_kafka_topic_t *rkt,
 + 					     const void *key, size_t keylen,
 +					     int32_t partition_cnt,
 +					     void *rkt_opaque,
 +					     void *msg_opaque) {
 +    return rd_murmur2(key, keylen) % partition_cnt;
 +}
 +
 +int32_t rd_kafka_msg_partitioner_murmur2_random (const rd_kafka_topic_t *rkt,
 +					 const void *key, size_t keylen,
 +					 int32_t partition_cnt,
 +					 void *rkt_opaque,
 +					 void *msg_opaque) {
 +	if (keylen == 0)

 Looking at the Java partitioner I believe this should probably be if
 (!key) since there is a difference between no key (null) and an empty key
 (which is just an arbitrary key).
 If so this is also incorrect on the existing partitioners, but we can't
 change their behaviour.

 
 You are receiving this because you authored the thread.
 Reply to this email directly, view it on GitHub
 <#1468 (review)>,
 or mute the thread
 <https://github.com/notifications/unsubscribe-auth/ACWANHTtwFhXS54IjnMNoW6ohCdWvBHIks5swt7lgaJpZM4P-Rip>
 .",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,1468,2017-10-18T19:59:25Z,2017-12-12T20:01:32Z,2017-12-12T20:01:41Z,MERGED,True,615,2,11,https://github.com/barrotsteindev,Murmur2 partitioner,33,[],https://github.com/edenhill/librdkafka/pull/1468,https://github.com/barrotsteindev,7,https://github.com/edenhill/librdkafka/pull/1468#issuecomment-341916997,"Add murmur2 partitioner, currently using the fastest algorithm for murmur2, which is not endian neutral. Also implements an endian neutral algorithm which can be used instead.
Fixes part of issue #1424",@edenhill I have implemented most fixes except for the word boundary. Could you give a hint because I did not quite understand the unit test you requested. I would really appreciate it.,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,1468,2017-10-18T19:59:25Z,2017-12-12T20:01:32Z,2017-12-12T20:01:41Z,MERGED,True,615,2,11,https://github.com/barrotsteindev,Murmur2 partitioner,33,[],https://github.com/edenhill/librdkafka/pull/1468,https://github.com/tmichaud314,8,https://github.com/edenhill/librdkafka/pull/1468#issuecomment-350271786,"Add murmur2 partitioner, currently using the fastest algorithm for murmur2, which is not endian neutral. Also implements an endian neutral algorithm which can be used instead.
Fixes part of issue #1424",@barrotsteindev I would love to see this PR happen and it looks close to completion. Do you foresee this getting merged? Thanks for your efforts here!,True,{'THUMBS_UP': ['https://github.com/barrotsteindev']}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,1468,2017-10-18T19:59:25Z,2017-12-12T20:01:32Z,2017-12-12T20:01:41Z,MERGED,True,615,2,11,https://github.com/barrotsteindev,Murmur2 partitioner,33,[],https://github.com/edenhill/librdkafka/pull/1468,https://github.com/barrotsteindev,9,https://github.com/edenhill/librdkafka/pull/1468#issuecomment-350573714,"Add murmur2 partitioner, currently using the fastest algorithm for murmur2, which is not endian neutral. Also implements an endian neutral algorithm which can be used instead.
Fixes part of issue #1424",i have rebased this pull request and responded to your requests @edenhill,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,1468,2017-10-18T19:59:25Z,2017-12-12T20:01:32Z,2017-12-12T20:01:41Z,MERGED,True,615,2,11,https://github.com/barrotsteindev,Murmur2 partitioner,33,[],https://github.com/edenhill/librdkafka/pull/1468,https://github.com/barrotsteindev,10,https://github.com/edenhill/librdkafka/pull/1468#issuecomment-350979651,"Add murmur2 partitioner, currently using the fastest algorithm for murmur2, which is not endian neutral. Also implements an endian neutral algorithm which can be used instead.
Fixes part of issue #1424","Hey @edenhill,
I addressed your new review, hope it's satisfactory.
Thanks in advance.",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,1468,2017-10-18T19:59:25Z,2017-12-12T20:01:32Z,2017-12-12T20:01:41Z,MERGED,True,615,2,11,https://github.com/barrotsteindev,Murmur2 partitioner,33,[],https://github.com/edenhill/librdkafka/pull/1468,https://github.com/edenhill,11,https://github.com/edenhill/librdkafka/pull/1468#issuecomment-350981189,"Add murmur2 partitioner, currently using the fastest algorithm for murmur2, which is not endian neutral. Also implements an endian neutral algorithm which can be used instead.
Fixes part of issue #1424","Looks good, thanks!
Just need to fix these OSX compile warnings and then we're good to merge:
https://travis-ci.org/edenhill/librdkafka/jobs/315112979#L229",True,{'THUMBS_UP': ['https://github.com/barrotsteindev']}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,1468,2017-10-18T19:59:25Z,2017-12-12T20:01:32Z,2017-12-12T20:01:41Z,MERGED,True,615,2,11,https://github.com/barrotsteindev,Murmur2 partitioner,33,[],https://github.com/edenhill/librdkafka/pull/1468,https://github.com/barrotsteindev,12,https://github.com/edenhill/librdkafka/pull/1468#issuecomment-351134348,"Add murmur2 partitioner, currently using the fastest algorithm for murmur2, which is not endian neutral. Also implements an endian neutral algorithm which can be used instead.
Fixes part of issue #1424","@edenhill , I have examined the warnings from the osx build,
and have noticed that they only come from the non endian neutral hash functions, and the aligned only hash function. These functions are not used.
We can either ignore those warnings, since we do not use that code, or I can remove it from the file.
Which action do you prefer?
thanks in advance.",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,1468,2017-10-18T19:59:25Z,2017-12-12T20:01:32Z,2017-12-12T20:01:41Z,MERGED,True,615,2,11,https://github.com/barrotsteindev,Murmur2 partitioner,33,[],https://github.com/edenhill/librdkafka/pull/1468,https://github.com/edenhill,13,https://github.com/edenhill/librdkafka/pull/1468#issuecomment-351176374,"Add murmur2 partitioner, currently using the fastest algorithm for murmur2, which is not endian neutral. Also implements an endian neutral algorithm which can be used instead.
Fixes part of issue #1424","We should probably use the optimized versions if the platform supports it, but we can fix that later.",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,1468,2017-10-18T19:59:25Z,2017-12-12T20:01:32Z,2017-12-12T20:01:41Z,MERGED,True,615,2,11,https://github.com/barrotsteindev,Murmur2 partitioner,33,[],https://github.com/edenhill/librdkafka/pull/1468,https://github.com/edenhill,14,https://github.com/edenhill/librdkafka/pull/1468#issuecomment-351176773,"Add murmur2 partitioner, currently using the fastest algorithm for murmur2, which is not endian neutral. Also implements an endian neutral algorithm which can be used instead.
Fixes part of issue #1424",Big thanks for all the effort on this!,True,"{'HOORAY': ['https://github.com/tmichaud314', 'https://github.com/barrotsteindev']}"
edenhill/librdkafka,https://github.com/edenhill/librdkafka,1480,2017-10-23T13:30:00Z,2018-01-04T08:03:53Z,2020-05-08T08:57:57Z,MERGED,True,1907,44,23,https://github.com/edenhill,Message headers support (C),3,[],https://github.com/edenhill/librdkafka/pull/1480,https://github.com/edenhill,1,https://github.com/edenhill/librdkafka/pull/1480,,,True,"{'THUMBS_UP': ['https://github.com/alexvaluyskiy', 'https://github.com/jeroenheijmans']}"
edenhill/librdkafka,https://github.com/edenhill/librdkafka,1480,2017-10-23T13:30:00Z,2018-01-04T08:03:53Z,2020-05-08T08:57:57Z,MERGED,True,1907,44,23,https://github.com/edenhill,Message headers support (C),3,[],https://github.com/edenhill/librdkafka/pull/1480,https://github.com/mhowlett,2,https://github.com/edenhill/librdkafka/pull/1480#issuecomment-341581063,,LGTM after you've addressed comments as you see fit.,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,1480,2017-10-23T13:30:00Z,2018-01-04T08:03:53Z,2020-05-08T08:57:57Z,MERGED,True,1907,44,23,https://github.com/edenhill,Message headers support (C),3,[],https://github.com/edenhill/librdkafka/pull/1480,https://github.com/almazik,3,https://github.com/edenhill/librdkafka/pull/1480#issuecomment-346383596,,Is there a progress on this merge request? This feature is important for us...,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,1480,2017-10-23T13:30:00Z,2018-01-04T08:03:53Z,2020-05-08T08:57:57Z,MERGED,True,1907,44,23,https://github.com/edenhill,Message headers support (C),3,[],https://github.com/edenhill/librdkafka/pull/1480,https://github.com/johnistan,4,https://github.com/edenhill/librdkafka/pull/1480#issuecomment-352515053,,"I have played around with python support for this feature. I haven't found a public API to find out how many headers an msg has. leading to hacks like:
        //HACK to figure out size of headers
        while (!rd_kafka_header_iter_all(hdrs, header_size++,
                                         &header_key, &header_value, &header_value_size)) {
        }

Can we add a rd_kafka_header_length to the public API? Or is there an existing approach I am not seeing.",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,1480,2017-10-23T13:30:00Z,2018-01-04T08:03:53Z,2020-05-08T08:57:57Z,MERGED,True,1907,44,23,https://github.com/edenhill,Message headers support (C),3,[],https://github.com/edenhill/librdkafka/pull/1480,https://github.com/edenhill,5,https://github.com/edenhill/librdkafka/pull/1480#issuecomment-352550928,,"@johnistan Good idea, will add.",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,1480,2017-10-23T13:30:00Z,2018-01-04T08:03:53Z,2020-05-08T08:57:57Z,MERGED,True,1907,44,23,https://github.com/edenhill,Message headers support (C),3,[],https://github.com/edenhill/librdkafka/pull/1480,https://github.com/edenhill,6,https://github.com/edenhill/librdkafka/pull/1480#issuecomment-354827613,,"@almazik This PR is pretty much ready to be merged, just waiting final review from @mhowlett  on iter API renaming.",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,1480,2017-10-23T13:30:00Z,2018-01-04T08:03:53Z,2020-05-08T08:57:57Z,MERGED,True,1907,44,23,https://github.com/edenhill,Message headers support (C),3,[],https://github.com/edenhill/librdkafka/pull/1480,https://github.com/mhowlett,7,https://github.com/edenhill/librdkafka/pull/1480#issuecomment-355192860,,@edenhill - go for it. lgtm.,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,1481,2017-10-23T14:16:48Z,2017-11-08T17:43:14Z,2017-11-08T17:43:14Z,MERGED,True,5,2,1,https://github.com/tbsaunde,stop calling cnd_timedwait() with a timeout of 0h,2,[],https://github.com/edenhill/librdkafka/pull/1481,https://github.com/tbsaunde,1,https://github.com/edenhill/librdkafka/pull/1481,"Waiting 0ms to be signaled the queue now contains elements doesn't
really mean anything.  However per the POSIX specification of
pthread_cond_timedwait() the lock must be released and reacquired.  As a
result if we skip calling cnd_timedwait() when the timeout is 0 we
improve average and best case latency in various conditions by between
0.1 and 0.2ms.","Waiting 0ms to be signaled the queue now contains elements doesn't
really mean anything.  However per the POSIX specification of
pthread_cond_timedwait() the lock must be released and reacquired.  As a
result if we skip calling cnd_timedwait() when the timeout is 0 we
improve average and best case latency in various conditions by between
0.1 and 0.2ms.",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,1481,2017-10-23T14:16:48Z,2017-11-08T17:43:14Z,2017-11-08T17:43:14Z,MERGED,True,5,2,1,https://github.com/tbsaunde,stop calling cnd_timedwait() with a timeout of 0h,2,[],https://github.com/edenhill/librdkafka/pull/1481,https://github.com/edenhill,2,https://github.com/edenhill/librdkafka/pull/1481#issuecomment-338699843,"Waiting 0ms to be signaled the queue now contains elements doesn't
really mean anything.  However per the POSIX specification of
pthread_cond_timedwait() the lock must be released and reacquired.  As a
result if we skip calling cnd_timedwait() when the timeout is 0 we
improve average and best case latency in various conditions by between
0.1 and 0.2ms.",Nice find :),True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,1482,2017-10-23T14:29:42Z,2017-10-23T17:59:51Z,2017-10-23T17:59:56Z,MERGED,True,3,2,1,https://github.com/tbsaunde,allow testing latency with different size messages,2,[],https://github.com/edenhill/librdkafka/pull/1482,https://github.com/tbsaunde,1,https://github.com/edenhill/librdkafka/pull/1482,,,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,1482,2017-10-23T14:29:42Z,2017-10-23T17:59:51Z,2017-10-23T17:59:56Z,MERGED,True,3,2,1,https://github.com/tbsaunde,allow testing latency with different size messages,2,[],https://github.com/edenhill/librdkafka/pull/1482,https://github.com/tbsaunde,2,https://github.com/edenhill/librdkafka/pull/1482#issuecomment-338719436,,"sorry about that, hopefully this looks better?",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,1482,2017-10-23T14:29:42Z,2017-10-23T17:59:51Z,2017-10-23T17:59:56Z,MERGED,True,3,2,1,https://github.com/tbsaunde,allow testing latency with different size messages,2,[],https://github.com/edenhill/librdkafka/pull/1482,https://github.com/edenhill,3,https://github.com/edenhill/librdkafka/pull/1482#issuecomment-338744753,,Thank you!,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,1483,2017-10-23T14:56:24Z,2017-10-24T08:40:26Z,2017-10-24T08:40:30Z,MERGED,True,23,0,3,https://github.com/akhi3030,Allow for calling rd_kafka_queue_io_event_enable() from the C++ world,1,[],https://github.com/edenhill/librdkafka/pull/1483,https://github.com/akhi3030,1,https://github.com/edenhill/librdkafka/pull/1483,"Hi,
As discussed in, #617, I've created a pull request.  Awaiting feedback.
Thanks,
Akhi","Hi,
As discussed in, #617, I've created a pull request.  Awaiting feedback.
Thanks,
Akhi",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,1483,2017-10-23T14:56:24Z,2017-10-24T08:40:26Z,2017-10-24T08:40:30Z,MERGED,True,23,0,3,https://github.com/akhi3030,Allow for calling rd_kafka_queue_io_event_enable() from the C++ world,1,[],https://github.com/edenhill/librdkafka/pull/1483,https://github.com/akhi3030,2,https://github.com/edenhill/librdkafka/pull/1483#issuecomment-338706896,"Hi,
As discussed in, #617, I've created a pull request.  Awaiting feedback.
Thanks,
Akhi",Thank you very much for the feedback.  This is my first PR so I hope I've done things correctly.,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,1483,2017-10-23T14:56:24Z,2017-10-24T08:40:26Z,2017-10-24T08:40:30Z,MERGED,True,23,0,3,https://github.com/akhi3030,Allow for calling rd_kafka_queue_io_event_enable() from the C++ world,1,[],https://github.com/edenhill/librdkafka/pull/1483,https://github.com/edenhill,3,https://github.com/edenhill/librdkafka/pull/1483#issuecomment-338917631,"Hi,
As discussed in, #617, I've created a pull request.  Awaiting feedback.
Thanks,
Akhi",Thanks for this!,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,1491,2017-10-27T22:09:14Z,2017-11-07T15:44:56Z,2017-11-07T18:59:37Z,MERGED,True,5,4,1,https://github.com/dacjames,Fix DNS cache logic and update default from 1s to 60s,4,[],https://github.com/edenhill/librdkafka/pull/1491,https://github.com/dacjames,1,https://github.com/edenhill/librdkafka/pull/1491,"To my eyes, the dns caching logic was not fully implemented.
This converts to using rd_clock() and sets the rkb_t_rsal_last timestamp after the value is loaded.
I also adjust the default dns cache time to 60s. 1s is very low in practice; when you have a large number of clients, a low DNS caching time leads to a flood of DNS requests when a broker is restarted. Could split out the config change if necessary","To my eyes, the dns caching logic was not fully implemented.
This converts to using rd_clock() and sets the rkb_t_rsal_last timestamp after the value is loaded.
I also adjust the default dns cache time to 60s. 1s is very low in practice; when you have a large number of clients, a low DNS caching time leads to a flood of DNS requests when a broker is restarted. Could split out the config change if necessary",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,1491,2017-10-27T22:09:14Z,2017-11-07T15:44:56Z,2017-11-07T18:59:37Z,MERGED,True,5,4,1,https://github.com/dacjames,Fix DNS cache logic and update default from 1s to 60s,4,[],https://github.com/edenhill/librdkafka/pull/1491,https://github.com/dacjames,2,https://github.com/edenhill/librdkafka/pull/1491#issuecomment-340291675,"To my eyes, the dns caching logic was not fully implemented.
This converts to using rd_clock() and sets the rkb_t_rsal_last timestamp after the value is loaded.
I also adjust the default dns cache time to 60s. 1s is very low in practice; when you have a large number of clients, a low DNS caching time leads to a flood of DNS requests when a broker is restarted. Could split out the config change if necessary","DNS caching is performed by the resolver (typically libc) based on the records TTL setting.

Thats not true. In general, libc does not perform DNS caching and major Linux distributions do not enable it by default.
Ill remove it if you want but the current default is bad and will overload DNS servers during broker maintenance once you have a few hundred clients.
A more robust form of backoff when restablishing the broker connection may be a better solution to the DNS flooding problem, though. Even with a longer DNS cache, the current retry loop makes decommissioning a broker from a large, live system impossible.",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,1491,2017-10-27T22:09:14Z,2017-11-07T15:44:56Z,2017-11-07T18:59:37Z,MERGED,True,5,4,1,https://github.com/dacjames,Fix DNS cache logic and update default from 1s to 60s,4,[],https://github.com/edenhill/librdkafka/pull/1491,https://github.com/dacjames,3,https://github.com/edenhill/librdkafka/pull/1491#issuecomment-340509099,"To my eyes, the dns caching logic was not fully implemented.
This converts to using rd_clock() and sets the rkb_t_rsal_last timestamp after the value is loaded.
I also adjust the default dns cache time to 60s. 1s is very low in practice; when you have a large number of clients, a low DNS caching time leads to a flood of DNS requests when a broker is restarted. Could split out the config change if necessary","@edenhill Applied changes per your review.
Tried to fix the whitespace but ending up confirming that it is already using tabs.
tabs",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,1491,2017-10-27T22:09:14Z,2017-11-07T15:44:56Z,2017-11-07T18:59:37Z,MERGED,True,5,4,1,https://github.com/dacjames,Fix DNS cache logic and update default from 1s to 60s,4,[],https://github.com/edenhill/librdkafka/pull/1491,https://github.com/dacjames,4,https://github.com/edenhill/librdkafka/pull/1491#issuecomment-342355512,"To my eyes, the dns caching logic was not fully implemented.
This converts to using rd_clock() and sets the rkb_t_rsal_last timestamp after the value is loaded.
I also adjust the default dns cache time to 60s. 1s is very low in practice; when you have a large number of clients, a low DNS caching time leads to a flood of DNS requests when a broker is restarted. Could split out the config change if necessary",@edenhill I think this resolves said technicalities.,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,1491,2017-10-27T22:09:14Z,2017-11-07T15:44:56Z,2017-11-07T18:59:37Z,MERGED,True,5,4,1,https://github.com/dacjames,Fix DNS cache logic and update default from 1s to 60s,4,[],https://github.com/edenhill/librdkafka/pull/1491,https://github.com/edenhill,5,https://github.com/edenhill/librdkafka/pull/1491#issuecomment-342524070,"To my eyes, the dns caching logic was not fully implemented.
This converts to using rd_clock() and sets the rkb_t_rsal_last timestamp after the value is loaded.
I also adjust the default dns cache time to 60s. 1s is very low in practice; when you have a large number of clients, a low DNS caching time leads to a flood of DNS requests when a broker is restarted. Could split out the config change if necessary",Big thanks for this!,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,1492,2017-10-28T20:47:27Z,2017-10-30T15:23:40Z,2020-05-11T21:46:58Z,CLOSED,False,2,2,2,https://github.com/AMHIT,0.11.0.x - Client Authentication using a PKCS#12 keystore,6,[],https://github.com/edenhill/librdkafka/pull/1492,https://github.com/AMHIT,1,https://github.com/edenhill/librdkafka/pull/1492,"I needed to connect to a Kafka broker using a client authentication.
Unluckily, the use of a key file not protected with a password wasn't allowed in my environment (it is a specific security policy).
So, I wrote an enhancement to allow the definition and use of a PKCS#12 keystore.
Even though the code is working fine, Id rather ask an assessment of my code to you.
Here are the file names I modified:
	rdkafka_conf.c
	rdkafka_conf.h
	rdkafka_transport.c
	rdkafka_transport_int.h
My changes are between the comment rows: /* Andrea Minuto (PKCS12) */.
Obviously, I integrated my client code with an encryption function to protect the keystore password.
Thank you for your great and useful library.
Regards,
Andrea Minuto
libdrkafka.zip","I needed to connect to a Kafka broker using a client authentication.
Unluckily, the use of a key file not protected with a password wasn't allowed in my environment (it is a specific security policy).
So, I wrote an enhancement to allow the definition and use of a PKCS#12 keystore.
Even though the code is working fine, Id rather ask an assessment of my code to you.
Here are the file names I modified:
	rdkafka_conf.c
	rdkafka_conf.h
	rdkafka_transport.c
	rdkafka_transport_int.h
My changes are between the comment rows: /* Andrea Minuto (PKCS12) */.
Obviously, I integrated my client code with an encryption function to protect the keystore password.
Thank you for your great and useful library.
Regards,
Andrea Minuto
libdrkafka.zip",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,1492,2017-10-28T20:47:27Z,2017-10-30T15:23:40Z,2020-05-11T21:46:58Z,CLOSED,False,2,2,2,https://github.com/AMHIT,0.11.0.x - Client Authentication using a PKCS#12 keystore,6,[],https://github.com/edenhill/librdkafka/pull/1492,https://github.com/edenhill,2,https://github.com/edenhill/librdkafka/pull/1492#issuecomment-340261172,"I needed to connect to a Kafka broker using a client authentication.
Unluckily, the use of a key file not protected with a password wasn't allowed in my environment (it is a specific security policy).
So, I wrote an enhancement to allow the definition and use of a PKCS#12 keystore.
Even though the code is working fine, Id rather ask an assessment of my code to you.
Here are the file names I modified:
	rdkafka_conf.c
	rdkafka_conf.h
	rdkafka_transport.c
	rdkafka_transport_int.h
My changes are between the comment rows: /* Andrea Minuto (PKCS12) */.
Obviously, I integrated my client code with an encryption function to protect the keystore password.
Thank you for your great and useful library.
Regards,
Andrea Minuto
libdrkafka.zip","This PR does not actually contain any of your changes, please look at these instructions for how to create a PR:
https://www.digitalocean.com/community/tutorials/how-to-create-a-pull-request-on-github",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,1492,2017-10-28T20:47:27Z,2017-10-30T15:23:40Z,2020-05-11T21:46:58Z,CLOSED,False,2,2,2,https://github.com/AMHIT,0.11.0.x - Client Authentication using a PKCS#12 keystore,6,[],https://github.com/edenhill/librdkafka/pull/1492,https://github.com/AMHIT,3,https://github.com/edenhill/librdkafka/pull/1492#issuecomment-340479016,"I needed to connect to a Kafka broker using a client authentication.
Unluckily, the use of a key file not protected with a password wasn't allowed in my environment (it is a specific security policy).
So, I wrote an enhancement to allow the definition and use of a PKCS#12 keystore.
Even though the code is working fine, Id rather ask an assessment of my code to you.
Here are the file names I modified:
	rdkafka_conf.c
	rdkafka_conf.h
	rdkafka_transport.c
	rdkafka_transport_int.h
My changes are between the comment rows: /* Andrea Minuto (PKCS12) */.
Obviously, I integrated my client code with an encryption function to protect the keystore password.
Thank you for your great and useful library.
Regards,
Andrea Minuto
libdrkafka.zip","It was wrongly opened.
I opened a new PR (#1494) using GitHub Desktop",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,1494,2017-10-30T07:51:04Z,2017-12-08T16:39:34Z,2018-01-21T09:55:43Z,MERGED,True,85,21,6,https://github.com/AMHIT,PKCS#12 Keystore Enhancement,4,[],https://github.com/edenhill/librdkafka/pull/1494,https://github.com/AMHIT,1,https://github.com/edenhill/librdkafka/pull/1494,Added the feature to use PKCS#12 keystores as the repository for a client authentication connection,Added the feature to use PKCS#12 keystores as the repository for a client authentication connection,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,1494,2017-10-30T07:51:04Z,2017-12-08T16:39:34Z,2018-01-21T09:55:43Z,MERGED,True,85,21,6,https://github.com/AMHIT,PKCS#12 Keystore Enhancement,4,[],https://github.com/edenhill/librdkafka/pull/1494,https://github.com/edenhill,2,https://github.com/edenhill/librdkafka/pull/1494#issuecomment-344045960,Added the feature to use PKCS#12 keystores as the repository for a client authentication connection,"Make sure to run the tests before pushing PR, this breaks the 0004 test suite (see CI links on PR page)",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,1494,2017-10-30T07:51:04Z,2017-12-08T16:39:34Z,2018-01-21T09:55:43Z,MERGED,True,85,21,6,https://github.com/AMHIT,PKCS#12 Keystore Enhancement,4,[],https://github.com/edenhill/librdkafka/pull/1494,https://github.com/AMHIT,3,https://github.com/edenhill/librdkafka/pull/1494#issuecomment-344379114,Added the feature to use PKCS#12 keystores as the repository for a client authentication connection,"I checked why the first test failed.
The only error was:
3: ### Test ""0066_plugins"" failed at /project/repo/checkout/tests/testcpp.h:67:test_FAIL(): ### 3: set(plugin.library.paths) failed: No such configuration property: ""plugin.library.paths"" 3: ### Test random seed was 91200605 ###
In my environment this test doesnt exist and Im sure that I never changed anything about the plugin.library.paths config property.
Could you help me to understand how to fix the error?
Thank you very much
Andrea",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,1494,2017-10-30T07:51:04Z,2017-12-08T16:39:34Z,2018-01-21T09:55:43Z,MERGED,True,85,21,6,https://github.com/AMHIT,PKCS#12 Keystore Enhancement,4,[],https://github.com/edenhill/librdkafka/pull/1494,https://github.com/edenhill,4,https://github.com/edenhill/librdkafka/pull/1494#issuecomment-345426609,Added the feature to use PKCS#12 keystores as the repository for a client authentication connection,"I think that test failure is unrelated to your change, try rebasing your commit on latest master and see if it is still occurring.
I will push off merging this (after all comments fixed) until after the upcoming maintenance release.",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,1494,2017-10-30T07:51:04Z,2017-12-08T16:39:34Z,2018-01-21T09:55:43Z,MERGED,True,85,21,6,https://github.com/AMHIT,PKCS#12 Keystore Enhancement,4,[],https://github.com/edenhill/librdkafka/pull/1494,https://github.com/edenhill,5,https://github.com/edenhill/librdkafka/pull/1494#issuecomment-350309701,Added the feature to use PKCS#12 keystores as the repository for a client authentication connection,Big thanks for this @AMHIT !,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,1496,2017-10-31T02:10:26Z,2017-10-31T14:22:10Z,2017-10-31T14:22:10Z,MERGED,True,1,1,1,https://github.com/yangyingchao,Make librdkafka compiles with ccache.,1,[],https://github.com/edenhill/librdkafka/pull/1496,https://github.com/yangyingchao,1,https://github.com/edenhill/librdkafka/pull/1496,"Ccache can be used to speed up compiling. When using ccache, we usually
set following environments variables:
export CC=""ccache gcc""
export CXX=""ccache g++""

In current version of configure.cc, we pass $CXX to function
mkl_mkvar_set without quoting marks:
mkl_mkvar_set ""CXX"" CXX $CXX

mkl_mkvar_set actually receives 4 variables: CXX, CXX, ccache and g++,
then mkl_mkvar_set will set CXX to ccache, but not ""ccache g++"", thus
causes errors when compiling files in folder src-cpp.","Ccache can be used to speed up compiling. When using ccache, we usually
set following environments variables:
export CC=""ccache gcc""
export CXX=""ccache g++""

In current version of configure.cc, we pass $CXX to function
mkl_mkvar_set without quoting marks:
mkl_mkvar_set ""CXX"" CXX $CXX

mkl_mkvar_set actually receives 4 variables: CXX, CXX, ccache and g++,
then mkl_mkvar_set will set CXX to ccache, but not ""ccache g++"", thus
causes errors when compiling files in folder src-cpp.",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,1496,2017-10-31T02:10:26Z,2017-10-31T14:22:10Z,2017-10-31T14:22:10Z,MERGED,True,1,1,1,https://github.com/yangyingchao,Make librdkafka compiles with ccache.,1,[],https://github.com/edenhill/librdkafka/pull/1496,https://github.com/edenhill,2,https://github.com/edenhill/librdkafka/pull/1496#issuecomment-340777333,"Ccache can be used to speed up compiling. When using ccache, we usually
set following environments variables:
export CC=""ccache gcc""
export CXX=""ccache g++""

In current version of configure.cc, we pass $CXX to function
mkl_mkvar_set without quoting marks:
mkl_mkvar_set ""CXX"" CXX $CXX

mkl_mkvar_set actually receives 4 variables: CXX, CXX, ccache and g++,
then mkl_mkvar_set will set CXX to ccache, but not ""ccache g++"", thus
causes errors when compiling files in folder src-cpp.","Good stuff, thanks!",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,1497,2017-10-31T22:13:29Z,2017-12-23T11:15:30Z,2017-12-23T11:15:30Z,CLOSED,False,19,14,1,https://github.com/dacjames,Feature/retry timeout,3,[],https://github.com/edenhill/librdkafka/pull/1497,https://github.com/dacjames,1,https://github.com/edenhill/librdkafka/pull/1497,"Requests that timeout appear not to be retried or rather they are retried and more or less instantly timeout again. It appears that this occurs because rkbuf_ts_timeout is not updated when a request is retried.
Before, we would see behavior like this.
%7|1508285460.943|RETRY|rdkafka#producer-1| [thrd:kafka-0.dev.idb.viasat.io:9092/bootstrap]: kafka-0.dev.idb.viasat.io:9092/0: Retrying MetadataRequest (v2, 91 bytes, retry 1/2)
%7|1508285460.943|RETRY|rdkafka#producer-1| [thrd:kafka-0.dev.idb.viasat.io:9092/bootstrap]: kafka-0.dev.idb.viasat.io:9092/0: Retrying MetadataRequest (v2, 91 bytes, retry 2/2)
%7|1508285460.943|REQTMOUT|rdkafka#producer-1| [thrd:kafka-0.dev.idb.viasat.io:9092/bootstrap]: kafka-0.dev.idb.viasat.io:9092/0: Timed out 1+1+0 requests
After this fix, the request waits a reasonable amount of time before retrying requests.
%7|1508427826.072|RETRY|rdkafka#producer-1| [thrd:kafka-8.dev.idb.viasat.io:9092/bootstrap]: kafka-8.dev.idb.viasat.io:9092/8: Retrying MetadataRequest (v2, 91 bytes, retry 1/2 timed out)
%7|1508427826.072|REQTMOUT|rdkafka#producer-1| [thrd:kafka-8.dev.idb.viasat.io:9092/bootstrap]: kafka-8.dev.idb.viasat.io:9092/8: Timed out 1+0+0 requests
%7|1508427827.113|SEND|rdkafka#producer-1| [thrd:kafka-8.dev.idb.viasat.io:9092/bootstrap]: kafka-8.dev.idb.viasat.io:9092/8: Sent MetadataRequest (v2, 91 bytes @ 0, CorrId 21)
%7|1508427887.204|RETRY|rdkafka#producer-1| [thrd:kafka-8.dev.idb.viasat.io:9092/bootstrap]: kafka-8.dev.idb.viasat.io:9092/8: Retrying MetadataRequest (v2, 91 bytes, retry 2/2 timed out)
%7|1508427887.204|REQTMOUT|rdkafka#producer-1| [thrd:kafka-8.dev.idb.viasat.io:9092/bootstrap]: kafka-8.dev.idb.viasat.io:9092/8: Timed out 1+0+0 requests
%7|1508427888.205|SEND|rdkafka#producer-1| [thrd:kafka-8.dev.idb.viasat.io:9092/bootstrap]: kafka-8.dev.idb.viasat.io:9092/8: Sent MetadataRequest (v2, 91 bytes @ 0, CorrId 22)
In this example, retry 2/2 also times out, which remains an issue we're trying to get to the bottom of, but in some cases the second retry does work.
Note that the logs above were pulled from slightly older code and this PR was updated for the latest changes.","Requests that timeout appear not to be retried or rather they are retried and more or less instantly timeout again. It appears that this occurs because rkbuf_ts_timeout is not updated when a request is retried.
Before, we would see behavior like this.
%7|1508285460.943|RETRY|rdkafka#producer-1| [thrd:kafka-0.dev.idb.viasat.io:9092/bootstrap]: kafka-0.dev.idb.viasat.io:9092/0: Retrying MetadataRequest (v2, 91 bytes, retry 1/2)
%7|1508285460.943|RETRY|rdkafka#producer-1| [thrd:kafka-0.dev.idb.viasat.io:9092/bootstrap]: kafka-0.dev.idb.viasat.io:9092/0: Retrying MetadataRequest (v2, 91 bytes, retry 2/2)
%7|1508285460.943|REQTMOUT|rdkafka#producer-1| [thrd:kafka-0.dev.idb.viasat.io:9092/bootstrap]: kafka-0.dev.idb.viasat.io:9092/0: Timed out 1+1+0 requests
After this fix, the request waits a reasonable amount of time before retrying requests.
%7|1508427826.072|RETRY|rdkafka#producer-1| [thrd:kafka-8.dev.idb.viasat.io:9092/bootstrap]: kafka-8.dev.idb.viasat.io:9092/8: Retrying MetadataRequest (v2, 91 bytes, retry 1/2 timed out)
%7|1508427826.072|REQTMOUT|rdkafka#producer-1| [thrd:kafka-8.dev.idb.viasat.io:9092/bootstrap]: kafka-8.dev.idb.viasat.io:9092/8: Timed out 1+0+0 requests
%7|1508427827.113|SEND|rdkafka#producer-1| [thrd:kafka-8.dev.idb.viasat.io:9092/bootstrap]: kafka-8.dev.idb.viasat.io:9092/8: Sent MetadataRequest (v2, 91 bytes @ 0, CorrId 21)
%7|1508427887.204|RETRY|rdkafka#producer-1| [thrd:kafka-8.dev.idb.viasat.io:9092/bootstrap]: kafka-8.dev.idb.viasat.io:9092/8: Retrying MetadataRequest (v2, 91 bytes, retry 2/2 timed out)
%7|1508427887.204|REQTMOUT|rdkafka#producer-1| [thrd:kafka-8.dev.idb.viasat.io:9092/bootstrap]: kafka-8.dev.idb.viasat.io:9092/8: Timed out 1+0+0 requests
%7|1508427888.205|SEND|rdkafka#producer-1| [thrd:kafka-8.dev.idb.viasat.io:9092/bootstrap]: kafka-8.dev.idb.viasat.io:9092/8: Sent MetadataRequest (v2, 91 bytes @ 0, CorrId 22)
In this example, retry 2/2 also times out, which remains an issue we're trying to get to the bottom of, but in some cases the second retry does work.
Note that the logs above were pulled from slightly older code and this PR was updated for the latest changes.",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,1497,2017-10-31T22:13:29Z,2017-12-23T11:15:30Z,2017-12-23T11:15:30Z,CLOSED,False,19,14,1,https://github.com/dacjames,Feature/retry timeout,3,[],https://github.com/edenhill/librdkafka/pull/1497,https://github.com/dacjames,2,https://github.com/edenhill/librdkafka/pull/1497#issuecomment-340923881,"Requests that timeout appear not to be retried or rather they are retried and more or less instantly timeout again. It appears that this occurs because rkbuf_ts_timeout is not updated when a request is retried.
Before, we would see behavior like this.
%7|1508285460.943|RETRY|rdkafka#producer-1| [thrd:kafka-0.dev.idb.viasat.io:9092/bootstrap]: kafka-0.dev.idb.viasat.io:9092/0: Retrying MetadataRequest (v2, 91 bytes, retry 1/2)
%7|1508285460.943|RETRY|rdkafka#producer-1| [thrd:kafka-0.dev.idb.viasat.io:9092/bootstrap]: kafka-0.dev.idb.viasat.io:9092/0: Retrying MetadataRequest (v2, 91 bytes, retry 2/2)
%7|1508285460.943|REQTMOUT|rdkafka#producer-1| [thrd:kafka-0.dev.idb.viasat.io:9092/bootstrap]: kafka-0.dev.idb.viasat.io:9092/0: Timed out 1+1+0 requests
After this fix, the request waits a reasonable amount of time before retrying requests.
%7|1508427826.072|RETRY|rdkafka#producer-1| [thrd:kafka-8.dev.idb.viasat.io:9092/bootstrap]: kafka-8.dev.idb.viasat.io:9092/8: Retrying MetadataRequest (v2, 91 bytes, retry 1/2 timed out)
%7|1508427826.072|REQTMOUT|rdkafka#producer-1| [thrd:kafka-8.dev.idb.viasat.io:9092/bootstrap]: kafka-8.dev.idb.viasat.io:9092/8: Timed out 1+0+0 requests
%7|1508427827.113|SEND|rdkafka#producer-1| [thrd:kafka-8.dev.idb.viasat.io:9092/bootstrap]: kafka-8.dev.idb.viasat.io:9092/8: Sent MetadataRequest (v2, 91 bytes @ 0, CorrId 21)
%7|1508427887.204|RETRY|rdkafka#producer-1| [thrd:kafka-8.dev.idb.viasat.io:9092/bootstrap]: kafka-8.dev.idb.viasat.io:9092/8: Retrying MetadataRequest (v2, 91 bytes, retry 2/2 timed out)
%7|1508427887.204|REQTMOUT|rdkafka#producer-1| [thrd:kafka-8.dev.idb.viasat.io:9092/bootstrap]: kafka-8.dev.idb.viasat.io:9092/8: Timed out 1+0+0 requests
%7|1508427888.205|SEND|rdkafka#producer-1| [thrd:kafka-8.dev.idb.viasat.io:9092/bootstrap]: kafka-8.dev.idb.viasat.io:9092/8: Sent MetadataRequest (v2, 91 bytes @ 0, CorrId 22)
In this example, retry 2/2 also times out, which remains an issue we're trying to get to the bottom of, but in some cases the second retry does work.
Note that the logs above were pulled from slightly older code and this PR was updated for the latest changes.",I separated the changes into separate commits for the actual fix and for additional logging that proved useful in identifying this problem.,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,1497,2017-10-31T22:13:29Z,2017-12-23T11:15:30Z,2017-12-23T11:15:30Z,CLOSED,False,19,14,1,https://github.com/dacjames,Feature/retry timeout,3,[],https://github.com/edenhill/librdkafka/pull/1497,https://github.com/dacjames,3,https://github.com/edenhill/librdkafka/pull/1497#issuecomment-340952045,"Requests that timeout appear not to be retried or rather they are retried and more or less instantly timeout again. It appears that this occurs because rkbuf_ts_timeout is not updated when a request is retried.
Before, we would see behavior like this.
%7|1508285460.943|RETRY|rdkafka#producer-1| [thrd:kafka-0.dev.idb.viasat.io:9092/bootstrap]: kafka-0.dev.idb.viasat.io:9092/0: Retrying MetadataRequest (v2, 91 bytes, retry 1/2)
%7|1508285460.943|RETRY|rdkafka#producer-1| [thrd:kafka-0.dev.idb.viasat.io:9092/bootstrap]: kafka-0.dev.idb.viasat.io:9092/0: Retrying MetadataRequest (v2, 91 bytes, retry 2/2)
%7|1508285460.943|REQTMOUT|rdkafka#producer-1| [thrd:kafka-0.dev.idb.viasat.io:9092/bootstrap]: kafka-0.dev.idb.viasat.io:9092/0: Timed out 1+1+0 requests
After this fix, the request waits a reasonable amount of time before retrying requests.
%7|1508427826.072|RETRY|rdkafka#producer-1| [thrd:kafka-8.dev.idb.viasat.io:9092/bootstrap]: kafka-8.dev.idb.viasat.io:9092/8: Retrying MetadataRequest (v2, 91 bytes, retry 1/2 timed out)
%7|1508427826.072|REQTMOUT|rdkafka#producer-1| [thrd:kafka-8.dev.idb.viasat.io:9092/bootstrap]: kafka-8.dev.idb.viasat.io:9092/8: Timed out 1+0+0 requests
%7|1508427827.113|SEND|rdkafka#producer-1| [thrd:kafka-8.dev.idb.viasat.io:9092/bootstrap]: kafka-8.dev.idb.viasat.io:9092/8: Sent MetadataRequest (v2, 91 bytes @ 0, CorrId 21)
%7|1508427887.204|RETRY|rdkafka#producer-1| [thrd:kafka-8.dev.idb.viasat.io:9092/bootstrap]: kafka-8.dev.idb.viasat.io:9092/8: Retrying MetadataRequest (v2, 91 bytes, retry 2/2 timed out)
%7|1508427887.204|REQTMOUT|rdkafka#producer-1| [thrd:kafka-8.dev.idb.viasat.io:9092/bootstrap]: kafka-8.dev.idb.viasat.io:9092/8: Timed out 1+0+0 requests
%7|1508427888.205|SEND|rdkafka#producer-1| [thrd:kafka-8.dev.idb.viasat.io:9092/bootstrap]: kafka-8.dev.idb.viasat.io:9092/8: Sent MetadataRequest (v2, 91 bytes @ 0, CorrId 22)
In this example, retry 2/2 also times out, which remains an issue we're trying to get to the bottom of, but in some cases the second retry does work.
Note that the logs above were pulled from slightly older code and this PR was updated for the latest changes.",Almost forgot. You have to update the retry logic to permit retries of timed out requests.,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,1497,2017-10-31T22:13:29Z,2017-12-23T11:15:30Z,2017-12-23T11:15:30Z,CLOSED,False,19,14,1,https://github.com/dacjames,Feature/retry timeout,3,[],https://github.com/edenhill/librdkafka/pull/1497,https://github.com/edenhill,4,https://github.com/edenhill/librdkafka/pull/1497#issuecomment-353720714,"Requests that timeout appear not to be retried or rather they are retried and more or less instantly timeout again. It appears that this occurs because rkbuf_ts_timeout is not updated when a request is retried.
Before, we would see behavior like this.
%7|1508285460.943|RETRY|rdkafka#producer-1| [thrd:kafka-0.dev.idb.viasat.io:9092/bootstrap]: kafka-0.dev.idb.viasat.io:9092/0: Retrying MetadataRequest (v2, 91 bytes, retry 1/2)
%7|1508285460.943|RETRY|rdkafka#producer-1| [thrd:kafka-0.dev.idb.viasat.io:9092/bootstrap]: kafka-0.dev.idb.viasat.io:9092/0: Retrying MetadataRequest (v2, 91 bytes, retry 2/2)
%7|1508285460.943|REQTMOUT|rdkafka#producer-1| [thrd:kafka-0.dev.idb.viasat.io:9092/bootstrap]: kafka-0.dev.idb.viasat.io:9092/0: Timed out 1+1+0 requests
After this fix, the request waits a reasonable amount of time before retrying requests.
%7|1508427826.072|RETRY|rdkafka#producer-1| [thrd:kafka-8.dev.idb.viasat.io:9092/bootstrap]: kafka-8.dev.idb.viasat.io:9092/8: Retrying MetadataRequest (v2, 91 bytes, retry 1/2 timed out)
%7|1508427826.072|REQTMOUT|rdkafka#producer-1| [thrd:kafka-8.dev.idb.viasat.io:9092/bootstrap]: kafka-8.dev.idb.viasat.io:9092/8: Timed out 1+0+0 requests
%7|1508427827.113|SEND|rdkafka#producer-1| [thrd:kafka-8.dev.idb.viasat.io:9092/bootstrap]: kafka-8.dev.idb.viasat.io:9092/8: Sent MetadataRequest (v2, 91 bytes @ 0, CorrId 21)
%7|1508427887.204|RETRY|rdkafka#producer-1| [thrd:kafka-8.dev.idb.viasat.io:9092/bootstrap]: kafka-8.dev.idb.viasat.io:9092/8: Retrying MetadataRequest (v2, 91 bytes, retry 2/2 timed out)
%7|1508427887.204|REQTMOUT|rdkafka#producer-1| [thrd:kafka-8.dev.idb.viasat.io:9092/bootstrap]: kafka-8.dev.idb.viasat.io:9092/8: Timed out 1+0+0 requests
%7|1508427888.205|SEND|rdkafka#producer-1| [thrd:kafka-8.dev.idb.viasat.io:9092/bootstrap]: kafka-8.dev.idb.viasat.io:9092/8: Sent MetadataRequest (v2, 91 bytes @ 0, CorrId 22)
In this example, retry 2/2 also times out, which remains an issue we're trying to get to the bottom of, but in some cases the second retry does work.
Note that the logs above were pulled from slightly older code and this PR was updated for the latest changes.","Closing in favour of #1571, these fixes are incorporated there.
Thanks!",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,1501,2017-11-01T19:55:04Z,2017-11-01T19:56:02Z,2017-11-01T19:56:02Z,CLOSED,False,710,40,13,https://github.com/barrotsteindev,Indents,33,[],https://github.com/edenhill/librdkafka/pull/1501,https://github.com/barrotsteindev,1,https://github.com/edenhill/librdkafka/pull/1501,,,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,1505,2017-11-06T20:21:59Z,2017-11-06T23:49:09Z,2017-11-15T08:24:32Z,MERGED,True,17,6,1,https://github.com/edenhill,Fix #1397,2,['bug'],https://github.com/edenhill/librdkafka/pull/1505,https://github.com/edenhill,1,https://github.com/edenhill/librdkafka/pull/1505,"This is two fixes for the same thing, but they are valuable each on their own.","This is two fixes for the same thing, but they are valuable each on their own.",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,1505,2017-11-06T20:21:59Z,2017-11-06T23:49:09Z,2017-11-15T08:24:32Z,MERGED,True,17,6,1,https://github.com/edenhill,Fix #1397,2,['bug'],https://github.com/edenhill/librdkafka/pull/1505,https://github.com/mhowlett,2,https://github.com/edenhill/librdkafka/pull/1505#issuecomment-342293048,"This is two fixes for the same thing, but they are valuable each on their own.",looks good to the extent of my knowledge. I don't completely understand the reasoning behind the change in rd_kafka_broker_ua_idle.,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,1507,2017-11-07T18:51:45Z,2017-11-08T17:42:27Z,2017-11-15T08:24:21Z,MERGED,True,10,8,3,https://github.com/edenhill,"win32: Use sasl.kerberos.service.name for broker principal, not sasl.",1,['bug'],https://github.com/edenhill/librdkafka/pull/1507,https://github.com/edenhill,1,https://github.com/edenhill/librdkafka/pull/1507,kerberos.principal (#1502),kerberos.principal (#1502),True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,1507,2017-11-07T18:51:45Z,2017-11-08T17:42:27Z,2017-11-15T08:24:21Z,MERGED,True,10,8,3,https://github.com/edenhill,"win32: Use sasl.kerberos.service.name for broker principal, not sasl.",1,['bug'],https://github.com/edenhill/librdkafka/pull/1507,https://github.com/mhowlett,2,https://github.com/edenhill/librdkafka/pull/1507#issuecomment-342896333,kerberos.principal (#1502),LGTM,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,1509,2017-11-08T00:28:52Z,2017-11-18T08:11:12Z,2017-11-18T08:11:21Z,CLOSED,False,16,0,2,https://github.com/dacjames,Possible race condition in rd_kafka_q_destroy,1,[],https://github.com/edenhill/librdkafka/pull/1509,https://github.com/dacjames,1,https://github.com/edenhill/librdkafka/pull/1509,"In rare cases, we've experienced a segfault with the following stack trace.
#0  0x00007ffff6a01bd0 in pthread_mutex_lock () from /usr/lib64/libpthread.so.0
#1  0x00007ffff73ef689 in mtx_lock () from /usr/lib64/libidb.so
#2  0x00007ffff7394903 in rd_kafka_q_enq () from /usr/lib64/libidb.so
#3  0x00007ffff739edf4 in rd_kafka_broker_update () from /usr/lib64/libidb.so
#4  0x00007ffff73f32a1 in rd_kafka_parse_Metadata () from /usr/lib64/libidb.so
#5  0x00007ffff73c3924 in rd_kafka_handle_Metadata () from /usr/lib64/libidb.so
#6  0x00007ffff73b8f5d in rd_kafka_buf_callback () from /usr/lib64/libidb.so
#7  0x00007ffff73c2dec in rd_kafka_op_handle_std () from /usr/lib64/libidb.so
#8  0x00007ffff73c2e1c in rd_kafka_op_handle () from /usr/lib64/libidb.so
#9  0x00007ffff73bd5d7 in rd_kafka_q_serve () from /usr/lib64/libidb.so
#10 0x00007ffff738af2c in rd_kafka_thread_main () from /usr/lib64/libidb.so
#11 0x00007ffff73ef607 in _thrd_wrapper_function () from /usr/lib64/libidb.so
#12 0x00007ffff69ffdc5 in start_thread () from /usr/lib64/libpthread.so.0
#13 0x00007ffff536976d in clone () from /usr/lib64/libc.so.6
It appears that rkq->rkq_lock is somehow null in rd_kafka_q_enq.
Upon inspection of the rd_kafka_q_destroy method, we identified a possible race condition where the rkq struct could be freed while another thread has a reference to it, which seems like it could cause the segfault above. Details of the race condition are added in comments and a straightforward check is added in rd_kafka_q_destroy_final to prevent freeing a queue with a reference count greater than 0.","In rare cases, we've experienced a segfault with the following stack trace.
#0  0x00007ffff6a01bd0 in pthread_mutex_lock () from /usr/lib64/libpthread.so.0
#1  0x00007ffff73ef689 in mtx_lock () from /usr/lib64/libidb.so
#2  0x00007ffff7394903 in rd_kafka_q_enq () from /usr/lib64/libidb.so
#3  0x00007ffff739edf4 in rd_kafka_broker_update () from /usr/lib64/libidb.so
#4  0x00007ffff73f32a1 in rd_kafka_parse_Metadata () from /usr/lib64/libidb.so
#5  0x00007ffff73c3924 in rd_kafka_handle_Metadata () from /usr/lib64/libidb.so
#6  0x00007ffff73b8f5d in rd_kafka_buf_callback () from /usr/lib64/libidb.so
#7  0x00007ffff73c2dec in rd_kafka_op_handle_std () from /usr/lib64/libidb.so
#8  0x00007ffff73c2e1c in rd_kafka_op_handle () from /usr/lib64/libidb.so
#9  0x00007ffff73bd5d7 in rd_kafka_q_serve () from /usr/lib64/libidb.so
#10 0x00007ffff738af2c in rd_kafka_thread_main () from /usr/lib64/libidb.so
#11 0x00007ffff73ef607 in _thrd_wrapper_function () from /usr/lib64/libidb.so
#12 0x00007ffff69ffdc5 in start_thread () from /usr/lib64/libpthread.so.0
#13 0x00007ffff536976d in clone () from /usr/lib64/libc.so.6
It appears that rkq->rkq_lock is somehow null in rd_kafka_q_enq.
Upon inspection of the rd_kafka_q_destroy method, we identified a possible race condition where the rkq struct could be freed while another thread has a reference to it, which seems like it could cause the segfault above. Details of the race condition are added in comments and a straightforward check is added in rd_kafka_q_destroy_final to prevent freeing a queue with a reference count greater than 0.",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,1509,2017-11-08T00:28:52Z,2017-11-18T08:11:12Z,2017-11-18T08:11:21Z,CLOSED,False,16,0,2,https://github.com/dacjames,Possible race condition in rd_kafka_q_destroy,1,[],https://github.com/edenhill/librdkafka/pull/1509,https://github.com/edenhill,2,https://github.com/edenhill/librdkafka/pull/1509#issuecomment-342670450,"In rare cases, we've experienced a segfault with the following stack trace.
#0  0x00007ffff6a01bd0 in pthread_mutex_lock () from /usr/lib64/libpthread.so.0
#1  0x00007ffff73ef689 in mtx_lock () from /usr/lib64/libidb.so
#2  0x00007ffff7394903 in rd_kafka_q_enq () from /usr/lib64/libidb.so
#3  0x00007ffff739edf4 in rd_kafka_broker_update () from /usr/lib64/libidb.so
#4  0x00007ffff73f32a1 in rd_kafka_parse_Metadata () from /usr/lib64/libidb.so
#5  0x00007ffff73c3924 in rd_kafka_handle_Metadata () from /usr/lib64/libidb.so
#6  0x00007ffff73b8f5d in rd_kafka_buf_callback () from /usr/lib64/libidb.so
#7  0x00007ffff73c2dec in rd_kafka_op_handle_std () from /usr/lib64/libidb.so
#8  0x00007ffff73c2e1c in rd_kafka_op_handle () from /usr/lib64/libidb.so
#9  0x00007ffff73bd5d7 in rd_kafka_q_serve () from /usr/lib64/libidb.so
#10 0x00007ffff738af2c in rd_kafka_thread_main () from /usr/lib64/libidb.so
#11 0x00007ffff73ef607 in _thrd_wrapper_function () from /usr/lib64/libidb.so
#12 0x00007ffff69ffdc5 in start_thread () from /usr/lib64/libpthread.so.0
#13 0x00007ffff536976d in clone () from /usr/lib64/libc.so.6
It appears that rkq->rkq_lock is somehow null in rd_kafka_q_enq.
Upon inspection of the rd_kafka_q_destroy method, we identified a possible race condition where the rkq struct could be freed while another thread has a reference to it, which seems like it could cause the segfault above. Details of the race condition are added in comments and a straightforward check is added in rd_kafka_q_destroy_final to prevent freeing a queue with a reference count greater than 0.","Huhm, destroy_final() should only be called if the refcount reaches 0, so I believe some part of the code is either calling destroy() too many times, or not holding a proper reference (keep()).",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,1509,2017-11-08T00:28:52Z,2017-11-18T08:11:12Z,2017-11-18T08:11:21Z,CLOSED,False,16,0,2,https://github.com/dacjames,Possible race condition in rd_kafka_q_destroy,1,[],https://github.com/edenhill/librdkafka/pull/1509,https://github.com/edenhill,3,https://github.com/edenhill/librdkafka/pull/1509#issuecomment-342670477,"In rare cases, we've experienced a segfault with the following stack trace.
#0  0x00007ffff6a01bd0 in pthread_mutex_lock () from /usr/lib64/libpthread.so.0
#1  0x00007ffff73ef689 in mtx_lock () from /usr/lib64/libidb.so
#2  0x00007ffff7394903 in rd_kafka_q_enq () from /usr/lib64/libidb.so
#3  0x00007ffff739edf4 in rd_kafka_broker_update () from /usr/lib64/libidb.so
#4  0x00007ffff73f32a1 in rd_kafka_parse_Metadata () from /usr/lib64/libidb.so
#5  0x00007ffff73c3924 in rd_kafka_handle_Metadata () from /usr/lib64/libidb.so
#6  0x00007ffff73b8f5d in rd_kafka_buf_callback () from /usr/lib64/libidb.so
#7  0x00007ffff73c2dec in rd_kafka_op_handle_std () from /usr/lib64/libidb.so
#8  0x00007ffff73c2e1c in rd_kafka_op_handle () from /usr/lib64/libidb.so
#9  0x00007ffff73bd5d7 in rd_kafka_q_serve () from /usr/lib64/libidb.so
#10 0x00007ffff738af2c in rd_kafka_thread_main () from /usr/lib64/libidb.so
#11 0x00007ffff73ef607 in _thrd_wrapper_function () from /usr/lib64/libidb.so
#12 0x00007ffff69ffdc5 in start_thread () from /usr/lib64/libpthread.so.0
#13 0x00007ffff536976d in clone () from /usr/lib64/libc.so.6
It appears that rkq->rkq_lock is somehow null in rd_kafka_q_enq.
Upon inspection of the rd_kafka_q_destroy method, we identified a possible race condition where the rkq struct could be freed while another thread has a reference to it, which seems like it could cause the segfault above. Details of the race condition are added in comments and a straightforward check is added in rd_kafka_q_destroy_final to prevent freeing a queue with a reference count greater than 0.",Do you have any way to reproduce this?,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,1509,2017-11-08T00:28:52Z,2017-11-18T08:11:12Z,2017-11-18T08:11:21Z,CLOSED,False,16,0,2,https://github.com/dacjames,Possible race condition in rd_kafka_q_destroy,1,[],https://github.com/edenhill/librdkafka/pull/1509,https://github.com/dacjames,4,https://github.com/edenhill/librdkafka/pull/1509#issuecomment-342671295,"In rare cases, we've experienced a segfault with the following stack trace.
#0  0x00007ffff6a01bd0 in pthread_mutex_lock () from /usr/lib64/libpthread.so.0
#1  0x00007ffff73ef689 in mtx_lock () from /usr/lib64/libidb.so
#2  0x00007ffff7394903 in rd_kafka_q_enq () from /usr/lib64/libidb.so
#3  0x00007ffff739edf4 in rd_kafka_broker_update () from /usr/lib64/libidb.so
#4  0x00007ffff73f32a1 in rd_kafka_parse_Metadata () from /usr/lib64/libidb.so
#5  0x00007ffff73c3924 in rd_kafka_handle_Metadata () from /usr/lib64/libidb.so
#6  0x00007ffff73b8f5d in rd_kafka_buf_callback () from /usr/lib64/libidb.so
#7  0x00007ffff73c2dec in rd_kafka_op_handle_std () from /usr/lib64/libidb.so
#8  0x00007ffff73c2e1c in rd_kafka_op_handle () from /usr/lib64/libidb.so
#9  0x00007ffff73bd5d7 in rd_kafka_q_serve () from /usr/lib64/libidb.so
#10 0x00007ffff738af2c in rd_kafka_thread_main () from /usr/lib64/libidb.so
#11 0x00007ffff73ef607 in _thrd_wrapper_function () from /usr/lib64/libidb.so
#12 0x00007ffff69ffdc5 in start_thread () from /usr/lib64/libpthread.so.0
#13 0x00007ffff536976d in clone () from /usr/lib64/libc.so.6
It appears that rkq->rkq_lock is somehow null in rd_kafka_q_enq.
Upon inspection of the rd_kafka_q_destroy method, we identified a possible race condition where the rkq struct could be freed while another thread has a reference to it, which seems like it could cause the segfault above. Details of the race condition are added in comments and a straightforward check is added in rd_kafka_q_destroy_final to prevent freeing a queue with a reference count greater than 0.","Not in isolation yet.
I think there's a race condition in destroy. You check that the ref count is zero, then release the lock before calling destroy_final. Another thread can come along between releasing the lock and calling destroy_final and increase the ref count.",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,1509,2017-11-08T00:28:52Z,2017-11-18T08:11:12Z,2017-11-18T08:11:21Z,CLOSED,False,16,0,2,https://github.com/dacjames,Possible race condition in rd_kafka_q_destroy,1,[],https://github.com/edenhill/librdkafka/pull/1509,https://github.com/dacjames,5,https://github.com/edenhill/librdkafka/pull/1509#issuecomment-342672989,"In rare cases, we've experienced a segfault with the following stack trace.
#0  0x00007ffff6a01bd0 in pthread_mutex_lock () from /usr/lib64/libpthread.so.0
#1  0x00007ffff73ef689 in mtx_lock () from /usr/lib64/libidb.so
#2  0x00007ffff7394903 in rd_kafka_q_enq () from /usr/lib64/libidb.so
#3  0x00007ffff739edf4 in rd_kafka_broker_update () from /usr/lib64/libidb.so
#4  0x00007ffff73f32a1 in rd_kafka_parse_Metadata () from /usr/lib64/libidb.so
#5  0x00007ffff73c3924 in rd_kafka_handle_Metadata () from /usr/lib64/libidb.so
#6  0x00007ffff73b8f5d in rd_kafka_buf_callback () from /usr/lib64/libidb.so
#7  0x00007ffff73c2dec in rd_kafka_op_handle_std () from /usr/lib64/libidb.so
#8  0x00007ffff73c2e1c in rd_kafka_op_handle () from /usr/lib64/libidb.so
#9  0x00007ffff73bd5d7 in rd_kafka_q_serve () from /usr/lib64/libidb.so
#10 0x00007ffff738af2c in rd_kafka_thread_main () from /usr/lib64/libidb.so
#11 0x00007ffff73ef607 in _thrd_wrapper_function () from /usr/lib64/libidb.so
#12 0x00007ffff69ffdc5 in start_thread () from /usr/lib64/libpthread.so.0
#13 0x00007ffff536976d in clone () from /usr/lib64/libc.so.6
It appears that rkq->rkq_lock is somehow null in rd_kafka_q_enq.
Upon inspection of the rd_kafka_q_destroy method, we identified a possible race condition where the rkq struct could be freed while another thread has a reference to it, which seems like it could cause the segfault above. Details of the race condition are added in comments and a straightforward check is added in rd_kafka_q_destroy_final to prevent freeing a queue with a reference count greater than 0.","Whether this race is the actual cause of the segfault I have no clue.
We're testing this fix as we speak so we'll know for sure in a couple days.",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,1509,2017-11-08T00:28:52Z,2017-11-18T08:11:12Z,2017-11-18T08:11:21Z,CLOSED,False,16,0,2,https://github.com/dacjames,Possible race condition in rd_kafka_q_destroy,1,[],https://github.com/edenhill/librdkafka/pull/1509,https://github.com/edenhill,6,https://github.com/edenhill/librdkafka/pull/1509#issuecomment-342677455,"In rare cases, we've experienced a segfault with the following stack trace.
#0  0x00007ffff6a01bd0 in pthread_mutex_lock () from /usr/lib64/libpthread.so.0
#1  0x00007ffff73ef689 in mtx_lock () from /usr/lib64/libidb.so
#2  0x00007ffff7394903 in rd_kafka_q_enq () from /usr/lib64/libidb.so
#3  0x00007ffff739edf4 in rd_kafka_broker_update () from /usr/lib64/libidb.so
#4  0x00007ffff73f32a1 in rd_kafka_parse_Metadata () from /usr/lib64/libidb.so
#5  0x00007ffff73c3924 in rd_kafka_handle_Metadata () from /usr/lib64/libidb.so
#6  0x00007ffff73b8f5d in rd_kafka_buf_callback () from /usr/lib64/libidb.so
#7  0x00007ffff73c2dec in rd_kafka_op_handle_std () from /usr/lib64/libidb.so
#8  0x00007ffff73c2e1c in rd_kafka_op_handle () from /usr/lib64/libidb.so
#9  0x00007ffff73bd5d7 in rd_kafka_q_serve () from /usr/lib64/libidb.so
#10 0x00007ffff738af2c in rd_kafka_thread_main () from /usr/lib64/libidb.so
#11 0x00007ffff73ef607 in _thrd_wrapper_function () from /usr/lib64/libidb.so
#12 0x00007ffff69ffdc5 in start_thread () from /usr/lib64/libpthread.so.0
#13 0x00007ffff536976d in clone () from /usr/lib64/libc.so.6
It appears that rkq->rkq_lock is somehow null in rd_kafka_q_enq.
Upon inspection of the rd_kafka_q_destroy method, we identified a possible race condition where the rkq struct could be freed while another thread has a reference to it, which seems like it could cause the segfault above. Details of the race condition are added in comments and a straightforward check is added in rd_kafka_q_destroy_final to prevent freeing a queue with a reference count greater than 0.","The thing is that it should not be possible to have a pointer to the queue and not have a refcount for it, if you have a pointer you need to maintain a reference.
So this indicates that there is a bug, somewhere, and refcount bugs are hard to track down :(",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,1509,2017-11-08T00:28:52Z,2017-11-18T08:11:12Z,2017-11-18T08:11:21Z,CLOSED,False,16,0,2,https://github.com/dacjames,Possible race condition in rd_kafka_q_destroy,1,[],https://github.com/edenhill/librdkafka/pull/1509,https://github.com/dacjames,7,https://github.com/edenhill/librdkafka/pull/1509#issuecomment-342680069,"In rare cases, we've experienced a segfault with the following stack trace.
#0  0x00007ffff6a01bd0 in pthread_mutex_lock () from /usr/lib64/libpthread.so.0
#1  0x00007ffff73ef689 in mtx_lock () from /usr/lib64/libidb.so
#2  0x00007ffff7394903 in rd_kafka_q_enq () from /usr/lib64/libidb.so
#3  0x00007ffff739edf4 in rd_kafka_broker_update () from /usr/lib64/libidb.so
#4  0x00007ffff73f32a1 in rd_kafka_parse_Metadata () from /usr/lib64/libidb.so
#5  0x00007ffff73c3924 in rd_kafka_handle_Metadata () from /usr/lib64/libidb.so
#6  0x00007ffff73b8f5d in rd_kafka_buf_callback () from /usr/lib64/libidb.so
#7  0x00007ffff73c2dec in rd_kafka_op_handle_std () from /usr/lib64/libidb.so
#8  0x00007ffff73c2e1c in rd_kafka_op_handle () from /usr/lib64/libidb.so
#9  0x00007ffff73bd5d7 in rd_kafka_q_serve () from /usr/lib64/libidb.so
#10 0x00007ffff738af2c in rd_kafka_thread_main () from /usr/lib64/libidb.so
#11 0x00007ffff73ef607 in _thrd_wrapper_function () from /usr/lib64/libidb.so
#12 0x00007ffff69ffdc5 in start_thread () from /usr/lib64/libpthread.so.0
#13 0x00007ffff536976d in clone () from /usr/lib64/libc.so.6
It appears that rkq->rkq_lock is somehow null in rd_kafka_q_enq.
Upon inspection of the rd_kafka_q_destroy method, we identified a possible race condition where the rkq struct could be freed while another thread has a reference to it, which seems like it could cause the segfault above. Details of the race condition are added in comments and a straightforward check is added in rd_kafka_q_destroy_final to prevent freeing a queue with a reference count greater than 0.","I don't follow how that avoids this race condition. Consider the following scenario:
Thread 1: calls rd_kafka_q_destroy
Thread 1: take rkq_lock
Thread 1: decrement rkq_refcnt to 0. Set do_delete = true
Thread 1: release rkq_lock
Thread 2: call rd_kafka_q_keep
Thread 2: take rkq_lock
Thread 2: increment rkq_refcnt to 1
Thread 2: release rkq_lock
Thread 1: calls rd_kafka_q_destroy_final
Thread 1: take rkq_lock
Thread 1: destroy rkq
Thread 2: Now holds an invalid reference to rkq
What am I missing here?",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,1509,2017-11-08T00:28:52Z,2017-11-18T08:11:12Z,2017-11-18T08:11:21Z,CLOSED,False,16,0,2,https://github.com/dacjames,Possible race condition in rd_kafka_q_destroy,1,[],https://github.com/edenhill/librdkafka/pull/1509,https://github.com/edenhill,8,https://github.com/edenhill/librdkafka/pull/1509#issuecomment-342683972,"In rare cases, we've experienced a segfault with the following stack trace.
#0  0x00007ffff6a01bd0 in pthread_mutex_lock () from /usr/lib64/libpthread.so.0
#1  0x00007ffff73ef689 in mtx_lock () from /usr/lib64/libidb.so
#2  0x00007ffff7394903 in rd_kafka_q_enq () from /usr/lib64/libidb.so
#3  0x00007ffff739edf4 in rd_kafka_broker_update () from /usr/lib64/libidb.so
#4  0x00007ffff73f32a1 in rd_kafka_parse_Metadata () from /usr/lib64/libidb.so
#5  0x00007ffff73c3924 in rd_kafka_handle_Metadata () from /usr/lib64/libidb.so
#6  0x00007ffff73b8f5d in rd_kafka_buf_callback () from /usr/lib64/libidb.so
#7  0x00007ffff73c2dec in rd_kafka_op_handle_std () from /usr/lib64/libidb.so
#8  0x00007ffff73c2e1c in rd_kafka_op_handle () from /usr/lib64/libidb.so
#9  0x00007ffff73bd5d7 in rd_kafka_q_serve () from /usr/lib64/libidb.so
#10 0x00007ffff738af2c in rd_kafka_thread_main () from /usr/lib64/libidb.so
#11 0x00007ffff73ef607 in _thrd_wrapper_function () from /usr/lib64/libidb.so
#12 0x00007ffff69ffdc5 in start_thread () from /usr/lib64/libpthread.so.0
#13 0x00007ffff536976d in clone () from /usr/lib64/libc.so.6
It appears that rkq->rkq_lock is somehow null in rd_kafka_q_enq.
Upon inspection of the rd_kafka_q_destroy method, we identified a possible race condition where the rkq struct could be freed while another thread has a reference to it, which seems like it could cause the segfault above. Details of the race condition are added in comments and a straightforward check is added in rd_kafka_q_destroy_final to prevent freeing a queue with a reference count greater than 0.","Thread 2: call rd_kafka_q_keep

how can thread 2 have a pointer to the queue without a reference? (which is the underlying bug)",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,1509,2017-11-08T00:28:52Z,2017-11-18T08:11:12Z,2017-11-18T08:11:21Z,CLOSED,False,16,0,2,https://github.com/dacjames,Possible race condition in rd_kafka_q_destroy,1,[],https://github.com/edenhill/librdkafka/pull/1509,https://github.com/dacjames,9,https://github.com/edenhill/librdkafka/pull/1509#issuecomment-342688749,"In rare cases, we've experienced a segfault with the following stack trace.
#0  0x00007ffff6a01bd0 in pthread_mutex_lock () from /usr/lib64/libpthread.so.0
#1  0x00007ffff73ef689 in mtx_lock () from /usr/lib64/libidb.so
#2  0x00007ffff7394903 in rd_kafka_q_enq () from /usr/lib64/libidb.so
#3  0x00007ffff739edf4 in rd_kafka_broker_update () from /usr/lib64/libidb.so
#4  0x00007ffff73f32a1 in rd_kafka_parse_Metadata () from /usr/lib64/libidb.so
#5  0x00007ffff73c3924 in rd_kafka_handle_Metadata () from /usr/lib64/libidb.so
#6  0x00007ffff73b8f5d in rd_kafka_buf_callback () from /usr/lib64/libidb.so
#7  0x00007ffff73c2dec in rd_kafka_op_handle_std () from /usr/lib64/libidb.so
#8  0x00007ffff73c2e1c in rd_kafka_op_handle () from /usr/lib64/libidb.so
#9  0x00007ffff73bd5d7 in rd_kafka_q_serve () from /usr/lib64/libidb.so
#10 0x00007ffff738af2c in rd_kafka_thread_main () from /usr/lib64/libidb.so
#11 0x00007ffff73ef607 in _thrd_wrapper_function () from /usr/lib64/libidb.so
#12 0x00007ffff69ffdc5 in start_thread () from /usr/lib64/libpthread.so.0
#13 0x00007ffff536976d in clone () from /usr/lib64/libc.so.6
It appears that rkq->rkq_lock is somehow null in rd_kafka_q_enq.
Upon inspection of the rd_kafka_q_destroy method, we identified a possible race condition where the rkq struct could be freed while another thread has a reference to it, which seems like it could cause the segfault above. Details of the race condition are added in comments and a straightforward check is added in rd_kafka_q_destroy_final to prevent freeing a queue with a reference count greater than 0.","It would have a pointer, obviously. There are half a dozen different places where rd_kafka_q_keep() is called, such as rd_kafka_replyq_copy, rd_kafka_q_fwd_get, and rd_kafka_set_replyq.
Are you saying there is no scenario in which one thread could be in the middle of taking a reference to a queue while another thread releases it? If that's the case, why take the lock when decrementing the reference count at all?",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,1509,2017-11-08T00:28:52Z,2017-11-18T08:11:12Z,2017-11-18T08:11:21Z,CLOSED,False,16,0,2,https://github.com/dacjames,Possible race condition in rd_kafka_q_destroy,1,[],https://github.com/edenhill/librdkafka/pull/1509,https://github.com/dacjames,10,https://github.com/edenhill/librdkafka/pull/1509#issuecomment-343617163,"In rare cases, we've experienced a segfault with the following stack trace.
#0  0x00007ffff6a01bd0 in pthread_mutex_lock () from /usr/lib64/libpthread.so.0
#1  0x00007ffff73ef689 in mtx_lock () from /usr/lib64/libidb.so
#2  0x00007ffff7394903 in rd_kafka_q_enq () from /usr/lib64/libidb.so
#3  0x00007ffff739edf4 in rd_kafka_broker_update () from /usr/lib64/libidb.so
#4  0x00007ffff73f32a1 in rd_kafka_parse_Metadata () from /usr/lib64/libidb.so
#5  0x00007ffff73c3924 in rd_kafka_handle_Metadata () from /usr/lib64/libidb.so
#6  0x00007ffff73b8f5d in rd_kafka_buf_callback () from /usr/lib64/libidb.so
#7  0x00007ffff73c2dec in rd_kafka_op_handle_std () from /usr/lib64/libidb.so
#8  0x00007ffff73c2e1c in rd_kafka_op_handle () from /usr/lib64/libidb.so
#9  0x00007ffff73bd5d7 in rd_kafka_q_serve () from /usr/lib64/libidb.so
#10 0x00007ffff738af2c in rd_kafka_thread_main () from /usr/lib64/libidb.so
#11 0x00007ffff73ef607 in _thrd_wrapper_function () from /usr/lib64/libidb.so
#12 0x00007ffff69ffdc5 in start_thread () from /usr/lib64/libpthread.so.0
#13 0x00007ffff536976d in clone () from /usr/lib64/libc.so.6
It appears that rkq->rkq_lock is somehow null in rd_kafka_q_enq.
Upon inspection of the rd_kafka_q_destroy method, we identified a possible race condition where the rkq struct could be freed while another thread has a reference to it, which seems like it could cause the segfault above. Details of the race condition are added in comments and a straightforward check is added in rd_kafka_q_destroy_final to prevent freeing a queue with a reference count greater than 0.","@edenhill Looks like you're right. Our segfault came back today despite this code being deployed.
#0  0x00007ffff6a01bd0 in pthread_mutex_lock () from /usr/lib64/libpthread.so.0
#1  0x00007ffff73ef6a9 in mtx_lock () from /usr/lib64/libidb.so
#2  0x00007ffff7394903 in rd_kafka_q_enq () from /usr/lib64/libidb.so
#3  0x00007ffff739edf4 in rd_kafka_broker_update () from /usr/lib64/libidb.so
#4  0x00007ffff73f32c1 in rd_kafka_parse_Metadata () from /usr/lib64/libidb.so
#5  0x00007ffff73c3944 in rd_kafka_handle_Metadata () from /usr/lib64/libidb.so
#6  0x00007ffff73b8f5d in rd_kafka_buf_callback () from /usr/lib64/libidb.so
#7  0x00007ffff73c2e0c in rd_kafka_op_handle_std () from /usr/lib64/libidb.so
#8  0x00007ffff73c2e3c in rd_kafka_op_handle () from /usr/lib64/libidb.so
#9  0x00007ffff73bd5f7 in rd_kafka_q_serve () from /usr/lib64/libidb.so
#10 0x00007ffff738af2c in rd_kafka_thread_main () from /usr/lib64/libidb.so
#11 0x00007ffff73ef627 in _thrd_wrapper_function () from /usr/lib64/libidb.so
#12 0x00007ffff69ffdc5 in start_thread () from /usr/lib64/libpthread.so.0
#13 0x00007ffff536976d in clone () from /usr/lib64/libc.so.6
Here are the logs from when the segfault occurred. Note that this process runs 4 separate producers but I'm only showing the one that crashed here for brevity. I replaced all the hostnames for security reasons.
%7|1510336699.987|METADATA|rdkafka#producer-2| [thrd:main]: kafka-26.some.tld.com:9092/26: 1/1 requested topic(s) seen in metadata
%7|1510336700.037|CONNECT|rdkafka#producer-2| [thrd:kafka-15.some.tld.com:9092/15]: kafka-15.some.tld.com:9092/15: broker in state DOWN connecting
%7|1510336700.070|CONNECT|rdkafka#producer-2| [thrd:kafka-15.some.tld.com:9092/15]: kafka-15.some.tld.com:9092/15: Connecting to ipv4#10.0.0.156:9092 (plaintext) with socket 165
%7|1510336700.070|STATE|rdkafka#producer-2| [thrd:kafka-15.some.tld.com:9092/15]: kafka-15.some.tld.com:9092/15: Broker changed state DOWN -> CONNECT
%7|1510336700.118|CONNECT|rdkafka#producer-2| [thrd:kafka-15.some.tld.com:9092/15]: kafka-15.some.tld.com:9092/15: Connected to ipv4#10.0.0.156:9092
%7|1510336700.118|CONNECTED|rdkafka#producer-2| [thrd:kafka-15.some.tld.com:9092/15]: kafka-15.some.tld.com:9092/15: Connected (#19)
%7|1510336700.118|STATE|rdkafka#producer-2| [thrd:kafka-15.some.tld.com:9092/15]: kafka-15.some.tld.com:9092/15: Broker changed state CONNECT -> APIVERSION_QUERY
%7|1510336700.166|STATE|rdkafka#producer-2| [thrd:kafka-15.some.tld.com:9092/15]: kafka-15.some.tld.com:9092/15: Broker changed state APIVERSION_QUERY -> UP
%7|1510336700.166|METADATA|rdkafka#producer-2| [thrd:kafka-15.some.tld.com:9092/15]: Skipping metadata refresh of 1 topic(s): connected: already being requested
%7|1510336818.864|METADATA|rdkafka#producer-2| [thrd:main]: Requesting metadata for 1/1 topics: periodic refresh
%7|1510336818.864|METADATA|rdkafka#producer-2| [thrd:main]: kafka-10.some.tld.com:9092/10: Request metadata for 1 topic(s): periodic refresh
%3|1510336841.398|FAIL|rdkafka#producer-2| [thrd:kafka-5.some.tld.com:9092/5]: kafka-5.some.tld.com:9092/5: 3 request(s) timed out: disconnect(3 >= 3)
%3|1510336841.398|ERROR|rdkafka#producer-2| [thrd:kafka-5.some.tld.com:9092/5]: kafka-5.some.tld.com:9092/5: 3 request(s) timed out: disconnect(3 >= 3)
%4|1510336841.398|METADATA|rdkafka#producer-2| [thrd:main]: kafka-5.some.tld.com:9092/5: Metadata request failed: Local: Timed out (58999ms)
%7|1510336878.864|REQTMOUT|rdkafka#producer-2| [thrd:kafka-10.some.tld.com:9092/10]: kafka-10.some.tld.com:9092/10: Timed out 1+0+0 requests
%7|1510336939.865|REQTMOUT|rdkafka#producer-2| [thrd:kafka-10.some.tld.com:9092/10]: kafka-10.some.tld.com:9092/10: Timed out 1+0+0 requests
%7|1510336998.866|REQTMOUT|rdkafka#producer-2| [thrd:kafka-10.some.tld.com:9092/10]: kafka-10.some.tld.com:9092/10: Timed out 1+0+0 requests
%7|1510336998.866|BROKERFAIL|rdkafka#producer-2| [thrd:kafka-10.some.tld.com:9092/10]: kafka-10.some.tld.com:9092/10: failed: err: Local: Message timed out: (errno: Connection timed out)
%3|1510336998.866|FAIL|rdkafka#producer-2| [thrd:kafka-10.some.tld.com:9092/10]: kafka-10.some.tld.com:9092/10: 3 request(s) timed out: disconnect(3 >= 3)
%3|1510336998.866|ERROR|rdkafka#producer-2| [thrd:kafka-10.some.tld.com:9092/10]: kafka-10.some.tld.com:9092/10: 3 request(s) timed out: disconnect(3 >= 3)
%7|1510336998.866|STATE|rdkafka#producer-2| [thrd:kafka-10.some.tld.com:9092/10]: kafka-10.some.tld.com:9092/10: Broker changed state UP -> DOWN
%7|1510336998.866|METADATA|rdkafka#producer-2| [thrd:kafka-10.some.tld.com:9092/10]: Requesting metadata for 1/1 topics: broker down
%7|1510336998.866|METADATA|rdkafka#producer-2| [thrd:kafka-10.some.tld.com:9092/10]: kafka-25.some.tld.com:9092/25: Request metadata for 1 topic(s): broker down
%4|1510336998.866|METADATA|rdkafka#producer-2| [thrd:main]: kafka-10.some.tld.com:9092/10: Metadata request failed: Local: Timed out (58000ms)
%7|1510336998.966|CONNECT|rdkafka#producer-2| [thrd:kafka-10.some.tld.com:9092/10]: kafka-10.some.tld.com:9092/10: broker in state DOWN connecting
%7|1510336999.001|CONNECT|rdkafka#producer-2| [thrd:kafka-10.some.tld.com:9092/10]: kafka-10.some.tld.com:9092/10: Connecting to ipv4#10.0.0.51:9092 (plaintext) with socket 217
%7|1510336999.001|STATE|rdkafka#producer-2| [thrd:kafka-10.some.tld.com:9092/10]: kafka-10.some.tld.com:9092/10: Broker changed state DOWN -> CONNECT
%7|1510336999.048|CONNECT|rdkafka#producer-2| [thrd:kafka-10.some.tld.com:9092/10]: kafka-10.some.tld.com:9092/10: Connected to ipv4#10.0.0.51:9092
%7|1510336999.048|CONNECTED|rdkafka#producer-2| [thrd:kafka-10.some.tld.com:9092/10]: kafka-10.some.tld.com:9092/10: Connected (#21)
%7|1510336999.048|STATE|rdkafka#producer-2| [thrd:kafka-10.some.tld.com:9092/10]: kafka-10.some.tld.com:9092/10: Broker changed state CONNECT -> APIVERSION_QUERY
%7|1510336999.095|STATE|rdkafka#producer-2| [thrd:kafka-10.some.tld.com:9092/10]: kafka-10.some.tld.com:9092/10: Broker changed state APIVERSION_QUERY -> UP
%7|1510336999.095|METADATA|rdkafka#producer-2| [thrd:kafka-10.some.tld.com:9092/10]: Skipping metadata refresh of 1 topic(s): connected: already being requested
%7|1510337059.867|REQTMOUT|rdkafka#producer-2| [thrd:kafka-25.some.tld.com:9092/25]: kafka-25.some.tld.com:9092/25: Timed out 1+0+0 requests
%7|1510337118.864|METADATA|rdkafka#producer-2| [thrd:main]: Requesting metadata for 1/1 topics: periodic refresh
%7|1510337118.864|METADATA|rdkafka#producer-2| [thrd:main]: kafka-23.some.tld.com:9092/23: Request metadata for 1 topic(s): periodic refresh
%7|1510337118.912|METADATA|rdkafka#producer-2| [thrd:main]: kafka-23.some.tld.com:9092/23: ===== Received metadata (for 1 requested topics): periodic refresh =====
%7|1510337118.912|METADATA|rdkafka#producer-2| [thrd:main]: kafka-23.some.tld.com:9092/23: ClusterId: ZNuEeKHnQTWLO0MA4-hZCg, ControllerId: 27
%7|1510337118.912|METADATA|rdkafka#producer-2| [thrd:main]: kafka-23.some.tld.com:9092/23: 29 brokers, 1 topics
%7|1510337118.912|METADATA|rdkafka#producer-2| [thrd:main]: kafka-23.some.tld.com:9092/23:   Broker #0/29: kafka-23.some.tld.com:9092 NodeId 23
%7|1510337118.912|METADATA|rdkafka#producer-2| [thrd:main]: kafka-23.some.tld.com:9092/23:   Broker #1/29: kafka-17.some.tld.com:9092 NodeId 17
%7|1510337118.912|METADATA|rdkafka#producer-2| [thrd:main]: kafka-23.some.tld.com:9092/23:   Broker #2/29: kafka-8.some.tld.com:9092 NodeId 8
%7|1510337118.912|METADATA|rdkafka#producer-2| [thrd:main]: kafka-23.some.tld.com:9092/23:   Broker #3/29: kafka-26.some.tld.com:9092 NodeId 26
%7|1510337118.912|METADATA|rdkafka#producer-2| [thrd:main]: kafka-23.some.tld.com:9092/23:   Broker #4/29: kafka-11.some.tld.com:9092 NodeId 11
%7|1510337118.912|METADATA|rdkafka#producer-2| [thrd:main]: kafka-23.some.tld.com:9092/23:   Broker #5/29: kafka-2.some.tld.com:9092 NodeId 2
%7|1510337118.912|METADATA|rdkafka#producer-2| [thrd:main]: kafka-23.some.tld.com:9092/23:   Broker #6/29: kafka-20.some.tld.com:9092 NodeId 20
%7|1510337118.912|METADATA|rdkafka#producer-2| [thrd:main]: kafka-23.some.tld.com:9092/23:   Broker #7/29: kafka-5.some.tld.com:9092 NodeId 5
%7|1510337118.912|METADATA|rdkafka#producer-2| [thrd:main]: kafka-23.some.tld.com:9092/23:   Broker #8/29: kafka-14.some.tld.com:9092 NodeId 14
%7|1510337118.912|METADATA|rdkafka#producer-2| [thrd:main]: kafka-23.some.tld.com:9092/23:   Broker #9/29: kafka-13.some.tld.com:9092 NodeId 13
%7|1510337118.912|METADATA|rdkafka#producer-2| [thrd:main]: kafka-23.some.tld.com:9092/23:   Broker #10/29: kafka-4.some.tld.com:9092 NodeId 4
%7|1510337118.912|METADATA|rdkafka#producer-2| [thrd:main]: kafka-23.some.tld.com:9092/23:   Broker #11/29: kafka-22.some.tld.com:9092 NodeId 22
%7|1510337118.912|METADATA|rdkafka#producer-2| [thrd:main]: kafka-23.some.tld.com:9092/23:   Broker #12/29: kafka-7.some.tld.com:9092 NodeId 7
%7|1510337118.912|METADATA|rdkafka#producer-2| [thrd:main]: kafka-23.some.tld.com:9092/23:   Broker #13/29: kafka-16.some.tld.com:9092 NodeId 16
%7|1510337118.912|METADATA|rdkafka#producer-2| [thrd:main]: kafka-23.some.tld.com:9092/23:   Broker #14/29: kafka-25.some.tld.com:9092 NodeId 25
%7|1510337118.912|METADATA|rdkafka#producer-2| [thrd:main]: kafka-23.some.tld.com:9092/23:   Broker #15/29: kafka-10.some.tld.com:9092 NodeId 10
%7|1510337118.912|METADATA|rdkafka#producer-2| [thrd:main]: kafka-23.some.tld.com:9092/23:   Broker #16/29: kafka-1.some.tld.com:9092 NodeId 1
%7|1510337118.912|METADATA|rdkafka#producer-2| [thrd:main]: kafka-23.some.tld.com:9092/23:   Broker #17/29: kafka-28.some.tld.com:9092 NodeId 28
%7|1510337118.912|METADATA|rdkafka#producer-2| [thrd:main]: kafka-23.some.tld.com:9092/23:   Broker #18/29: kafka-19.some.tld.com:9092 NodeId 19
%7|1510337118.912|METADATA|rdkafka#producer-2| [thrd:main]: kafka-23.some.tld.com:9092/23:   Broker #19/29: kafka-18.some.tld.com:9092 NodeId 18
%7|1510337118.912|METADATA|rdkafka#producer-2| [thrd:main]: kafka-23.some.tld.com:9092/23:   Broker #20/29: kafka-27.some.tld.com:9092 NodeId 27
%7|1510337118.912|METADATA|rdkafka#producer-2| [thrd:main]: kafka-23.some.tld.com:9092/23:   Broker #21/29: kafka-9.some.tld.com:9092 NodeId 9
%7|1510337118.912|METADATA|rdkafka#producer-2| [thrd:main]: kafka-23.some.tld.com:9092/23:   Broker #22/29: kafka-3.some.tld.com:9092 NodeId 3
%7|1510337118.912|METADATA|rdkafka#producer-2| [thrd:main]: kafka-23.some.tld.com:9092/23:   Broker #23/29: kafka-21.some.tld.com:9092 NodeId 21
%7|1510337118.912|METADATA|rdkafka#producer-2| [thrd:main]: kafka-23.some.tld.com:9092/23:   Broker #24/29: kafka-12.some.tld.com:9092 NodeId 12
%7|1510337118.912|METADATA|rdkafka#producer-2| [thrd:main]: kafka-23.some.tld.com:9092/23:   Broker #25/29: kafka-15.some.tld.com:9092 NodeId 15
%7|1510337118.912|METADATA|rdkafka#producer-2| [thrd:main]: kafka-23.some.tld.com:9092/23:   Broker #26/29: kafka-24.some.tld.com:9092 NodeId 24
%7|1510337118.912|METADATA|rdkafka#producer-2| [thrd:main]: kafka-23.some.tld.com:9092/23:   Broker #27/29: kafka-6.some.tld.com:9092 NodeId 6
%7|1510337118.912|METADATA|rdkafka#producer-2| [thrd:main]: kafka-23.some.tld.com:9092/23:   Broker #28/29: kafka-0.some.tld.com:9092 NodeId 0
Fri Nov 10 18:05:19 2017 QUA: PROC 2: SIGSEGV
Fri Nov 10 18:05:19 2017 QUA: PROC 3: SIGSEGV
Fri Nov 10 18:05:19 2017 QUA: PROC 4: SIGSEGV
Fri Nov 10 18:05:19 2017 QUA: PROC 5: SIGSEGV
Fri Nov 10 18:05:19 2017 QUA: PROC 6: SIGSEGV
Fri Nov 10 18:05:19 2017 QUA: PROC 7: SIGSEGV
Fri Nov 10 18:05:19 2017 QUA: PROC 8: SIGSEGV
Fri Nov 10 18:05:19 2017 QUA: PROC 9: SIGSEGV
Fri Nov 10 18:05:19 2017 QUA: PROC 10: SIGSEGV
Fri Nov 10 18:05:19 2017 QUA: PROC 11: SIGSEGV
Fri Nov 10 18:05:19 2017 QUA: PROC 12: SIGSEGV
Fri Nov 10 18:05:19 2017 QUA: PROC 13: SIGSEGV",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,1509,2017-11-08T00:28:52Z,2017-11-18T08:11:12Z,2017-11-18T08:11:21Z,CLOSED,False,16,0,2,https://github.com/dacjames,Possible race condition in rd_kafka_q_destroy,1,[],https://github.com/edenhill/librdkafka/pull/1509,https://github.com/edenhill,11,https://github.com/edenhill/librdkafka/pull/1509#issuecomment-344063891,"In rare cases, we've experienced a segfault with the following stack trace.
#0  0x00007ffff6a01bd0 in pthread_mutex_lock () from /usr/lib64/libpthread.so.0
#1  0x00007ffff73ef689 in mtx_lock () from /usr/lib64/libidb.so
#2  0x00007ffff7394903 in rd_kafka_q_enq () from /usr/lib64/libidb.so
#3  0x00007ffff739edf4 in rd_kafka_broker_update () from /usr/lib64/libidb.so
#4  0x00007ffff73f32a1 in rd_kafka_parse_Metadata () from /usr/lib64/libidb.so
#5  0x00007ffff73c3924 in rd_kafka_handle_Metadata () from /usr/lib64/libidb.so
#6  0x00007ffff73b8f5d in rd_kafka_buf_callback () from /usr/lib64/libidb.so
#7  0x00007ffff73c2dec in rd_kafka_op_handle_std () from /usr/lib64/libidb.so
#8  0x00007ffff73c2e1c in rd_kafka_op_handle () from /usr/lib64/libidb.so
#9  0x00007ffff73bd5d7 in rd_kafka_q_serve () from /usr/lib64/libidb.so
#10 0x00007ffff738af2c in rd_kafka_thread_main () from /usr/lib64/libidb.so
#11 0x00007ffff73ef607 in _thrd_wrapper_function () from /usr/lib64/libidb.so
#12 0x00007ffff69ffdc5 in start_thread () from /usr/lib64/libpthread.so.0
#13 0x00007ffff536976d in clone () from /usr/lib64/libc.so.6
It appears that rkq->rkq_lock is somehow null in rd_kafka_q_enq.
Upon inspection of the rd_kafka_q_destroy method, we identified a possible race condition where the rkq struct could be freed while another thread has a reference to it, which seems like it could cause the segfault above. Details of the race condition are added in comments and a straightforward check is added in rd_kafka_q_destroy_final to prevent freeing a queue with a reference count greater than 0.","Fri Nov 10 18:05:19 2017 QUA: PROC 2: SIGSEGV
Fri Nov 10 18:05:19 2017 QUA: PROC 3: SIGSEGV
Fri Nov 10 18:05:19 2017 QUA: PROC 4: SIGSEGV
Fri Nov 10 18:05:19 2017 QUA: PROC 5: SIGSEGV
Fri Nov 10 18:05:19 2017 QUA: PROC 6: SIGSEGV
Fri Nov 10 18:05:19 2017 QUA: PROC 7: SIGSEGV
Fri Nov 10 18:05:19 2017 QUA: PROC 8: SIGSEGV
Fri Nov 10 18:05:19 2017 QUA: PROC 9: SIGSEGV
Fri Nov 10 18:05:19 2017 QUA: PROC 10: SIGSEGV
Fri Nov 10 18:05:19 2017 QUA: PROC 11: SIGSEGV
Fri Nov 10 18:05:19 2017 QUA: PROC 12: SIGSEGV
Fri Nov 10 18:05:19 2017 QUA: PROC 13: SIGSEGV

Does this mean multiple instances crash at the same time, or just one?
Did you get a core file?
If so, can you print p *rkb` a couple of frames up from the crash?  (you can email me directly if you don't want to share on github).
Is this happening during normal operation or when terminating the client?",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,1509,2017-11-08T00:28:52Z,2017-11-18T08:11:12Z,2017-11-18T08:11:21Z,CLOSED,False,16,0,2,https://github.com/dacjames,Possible race condition in rd_kafka_q_destroy,1,[],https://github.com/edenhill/librdkafka/pull/1509,https://github.com/dacjames,12,https://github.com/edenhill/librdkafka/pull/1509#issuecomment-344422376,"In rare cases, we've experienced a segfault with the following stack trace.
#0  0x00007ffff6a01bd0 in pthread_mutex_lock () from /usr/lib64/libpthread.so.0
#1  0x00007ffff73ef689 in mtx_lock () from /usr/lib64/libidb.so
#2  0x00007ffff7394903 in rd_kafka_q_enq () from /usr/lib64/libidb.so
#3  0x00007ffff739edf4 in rd_kafka_broker_update () from /usr/lib64/libidb.so
#4  0x00007ffff73f32a1 in rd_kafka_parse_Metadata () from /usr/lib64/libidb.so
#5  0x00007ffff73c3924 in rd_kafka_handle_Metadata () from /usr/lib64/libidb.so
#6  0x00007ffff73b8f5d in rd_kafka_buf_callback () from /usr/lib64/libidb.so
#7  0x00007ffff73c2dec in rd_kafka_op_handle_std () from /usr/lib64/libidb.so
#8  0x00007ffff73c2e1c in rd_kafka_op_handle () from /usr/lib64/libidb.so
#9  0x00007ffff73bd5d7 in rd_kafka_q_serve () from /usr/lib64/libidb.so
#10 0x00007ffff738af2c in rd_kafka_thread_main () from /usr/lib64/libidb.so
#11 0x00007ffff73ef607 in _thrd_wrapper_function () from /usr/lib64/libidb.so
#12 0x00007ffff69ffdc5 in start_thread () from /usr/lib64/libpthread.so.0
#13 0x00007ffff536976d in clone () from /usr/lib64/libc.so.6
It appears that rkq->rkq_lock is somehow null in rd_kafka_q_enq.
Upon inspection of the rd_kafka_q_destroy method, we identified a possible race condition where the rkq struct could be freed while another thread has a reference to it, which seems like it could cause the segfault above. Details of the race condition are added in comments and a straightforward check is added in rd_kafka_q_destroy_final to prevent freeing a queue with a reference count greater than 0.","@edenhill The multiple instances crash is caused by an external watchdog and can be safely ignored here.
The strack trace is as follows:
#0  0x00007ffff69d1bd0 in pthread_mutex_lock () from /usr/lib64/libpthread.so.0
#1  0x00007ffff73bdc8f in mtx_lock (mtx=0x600) at tinycthread.c:136
#2  0x00007ffff73653b6 in rd_kafka_q_enq (rkq=0x600, rko=0x7fffec020d00) at rdkafka_queue.h:292
#3  0x00007ffff737923c in rd_kafka_broker_update (rk=0x7ffff2bccd00, proto=RD_KAFKA_PROTO_PLAINTEXT,
    mdb=0x7ffff31182f8) at rdkafka_broker.c:5376
#4  0x00007ffff73c793f in rd_kafka_parse_Metadata (rkb=0x7ffff3136b00, request=0x7fffe831a000, rkbuf=0x7fffe837ea00)
    at rdkafka_metadata.c:425
#5  0x00007ffff739c3eb in rd_kafka_handle_Metadata (rk=0x7ffff2bccd00, rkb=0x7ffff3136b00,
    err=RD_KAFKA_RESP_ERR_NO_ERROR, rkbuf=0x7fffe837ea00, request=0x7fffe831a000, opaque=0x0)
    at rdkafka_request.c:1297
#6  0x00007ffff738e05d in rd_kafka_buf_callback (rk=0x7ffff2bccd00, rkb=0x7ffff3136b00,
    err=RD_KAFKA_RESP_ERR_NO_ERROR, response=0x7fffe837ea00, request=0x7fffe831a000) at rdkafka_buf.c:599
#7  0x00007ffff738de47 in rd_kafka_buf_handle_op (rko=0x7fffeb928cf0, err=RD_KAFKA_RESP_ERR_NO_ERROR)
    at rdkafka_buf.c:540
#8  0x00007ffff7393550 in rd_kafka_op_handle_std (rk=0x7ffff2bccd00, rko=0x7fffeb928cf0, cb_type=1)
    at rdkafka_op.c:533
#9  0x00007ffff73935c3 in rd_kafka_op_handle (rk=0x7ffff2bccd00, rko=0x7fffeb928cf0, cb_type=1, opaque=0x0,
    callback=0x0) at rdkafka_op.c:558
#10 0x00007ffff73905a9 in rd_kafka_q_serve (rkq=0x7ffff259d4c0, timeout_ms=0, max_cnt=0, cb_type=1, callback=0x0,
    opaque=0x0) at rdkafka_queue.c:440
#11 0x00007ffff735b809 in rd_kafka_thread_main (arg=0x7ffff2bccd00) at rdkafka.c:1167
#12 0x00007ffff73bdff1 in _thrd_wrapper_function (aArg=0x7ffff262e610) at tinycthread.c:624
#13 0x00007ffff69cfdc5 in start_thread () from /usr/lib64/libpthread.so.0
#14 0x00007ffff533976d in __lseek_nocancel () from /usr/lib64/libc.so.6
#15 0x0000000000000000 in ?? ()
The output of *p *rkb is:
$1 = {rkb_link = {tqe_next = 0x37fcfc73000, tqe_prev = 0x47868c0000}, rkb_nodeid = 0, rkb_rsal = 0x100,
  rkb_t_rsal_last = 158976, rkb_addr_last = 0x900, rkb_transport = 0x0, rkb_corrid = 1024, rkb_connid = 0,
  rkb_ops = 0x600, rkb_lock = {__data = {__lock = 161280, __count = 0, __owner = 768, __nusers = 0, __kind = 0,
      __spins = 0, __elision = 0, __list = {__prev = 0x0, __next = 0x0}},
    __size = ""\000v\002\000\000\000\000\000\000\003"", '\000' <repeats 29 times>, __align = 161280},
  rkb_blocking_max_ms = 0, rkb_toppars = {tqh_first = 0x0, tqh_last = 0x1100}, rkb_toppar_cnt = 0,
  rkb_fetch_toppars = {cqh_first = 0x1abaa06800, cqh_last = 0x0}, rkb_fetch_toppar_cnt = -1133051904,
  rkb_fetch_toppar_next = 0x0, rkb_cgrp = 0x0, rkb_ts_fetch_backoff = 0, rkb_fetching = 0,
  rkb_state = RD_KAFKA_BROKER_STATE_INIT, rkb_ts_state = 0, rkb_timeout_scan_intvl = {ri_ts_last = 512,
    ri_fixed = 36028740056973312, ri_backoff = 921567232}, rkb_blocking_request_cnt = {val = 256000000},
  rkb_features = 0, rkb_ApiVersions = 0x0, rkb_ApiVersions_cnt = 36028741510754304, rkb_ApiVersion_fail_intvl = {
    ri_ts_last = 0, ri_fixed = 0, ri_backoff = 326371328}, rkb_source = RD_KAFKA_CONFIGURED, rkb_c = {tx_bytes = {
      val = 0}, tx = {val = 36028741510766592}, tx_err = {val = 0}, tx_retries = {val = 0}, req_timeouts = {
      val = 0}, rx_bytes = {val = 0}, rx = {val = 0}, rx_err = {val = 0}, rx_corrid_err = {val = 5551905211392},
    rx_partial = {val = 0}, zbuf_grow = {val = 0}, buf_grow = {val = 0}, wakeups = {val = 0}}, rkb_req_timeouts = 0,
  rkb_ts_metadata_poll = 0, rkb_metadata_fast_poll_cnt = 0, rkb_thread = 0, rkb_refcnt = {lock = {__data = {
        __lock = 0, __count = 0, __owner = 0, __nusers = 0, __kind = 0, __spins = 0, __elision = 0, __list = {
          __prev = 0x50ca7568500, __next = 0x0}},
      __size = '\000' <repeats 25 times>, ""\205V\247\f\005\000\000\000\000\000\000\000\000\000"", __align = 0},
    v = 1}, rkb_rk = 0x0, rkb_recv_buf = 0x0, rkb_max_inflight = 0, rkb_outbufs = {rkbq_bufs = {tqh_first = 0x0,
      tqh_last = 0x0}, rkbq_cnt = {val = 0}, rkbq_msg_cnt = {val = 0}}, rkb_waitresps = {rkbq_bufs = {
      tqh_first = 0x0, tqh_last = 0x0}, rkbq_cnt = {val = 0}, rkbq_msg_cnt = {val = 0}}, rkb_retrybufs = {
    rkbq_bufs = {tqh_first = 0x50ca7568500, tqh_last = 0x0}, rkbq_cnt = {val = 0}, rkbq_msg_cnt = {val = 0}},
  rkb_avg_int_latency = {ra_v = {maxv = 0, minv = 0, avg = 0, sum = 0, cnt = 1634890752,
      start = 7089907360485700722}, ra_lock = {__data = {__lock = 1634301486, __count = 779379059,
        __owner = 960130921, __nusers = 791820592, __kind = 48, __spins = 0, __elision = 0, __list = {__prev = 0x0,
          __next = 0x0}}, __size = "".viasat.io:9092/0"", '\000' <repeats 22 times>, __align = 3347407571226555950},
    ra_type = RD_AVG_GAUGE}, rkb_avg_rtt = {ra_v = {maxv = 0, minv = 0, avg = 0, sum = 0, cnt = 0, start = 0},
---Type <return> to continue, or q <return> to quit---
    ra_lock = {__data = {__lock = 0, __count = 0, __owner = 0, __nusers = 0, __kind = 1634890752, __spins = 29550,
        __elision = 28528, __list = {__prev = 0x6264692e302d7472, __next = 0x2e7461736169762e}},
      __size = '\000' <repeats 17 times>, ""transport-0.idb.viasat."", __align = 0},
    ra_type = (RD_AVG_COUNTER | unknown: 960130920)}, rkb_avg_throttle = {ra_v = {maxv = 0, minv = 0, avg = 0,
      sum = 0, cnt = 0, start = 0}, ra_lock = {__data = {__lock = 0, __count = 0, __owner = 0, __nusers = 0,
        __kind = 0, __spins = 0, __elision = 0, __list = {__prev = 0x0, __next = 0x0}},
      __size = '\000' <repeats 39 times>, __align = 0}, ra_type = RD_AVG_GAUGE},
  rkb_name = ""\000\204#\000\000\000\000\000\000\300g\363\362\377\177\000\000\020q\n\363\377\177"", '\000' <repeats 42 times>, ""\024\001\000\000\025\001\000\000\377\377\377\377\000\000\000\000\352\212\316\177\003"", '\000' <repeats 41 times>,
  rkb_nodename = '\000' <repeats 33 times>, ""transport-0.idb.viasat.io:9092/0: 3 request(s) timed out: disconnect(3 >= 3)"", '\000' <repeats 18 times>, rkb_port = 0, rkb_origname = 0x0, rkb_logname = 0x0, rkb_logname_lock = {__data = {
      __lock = 0, __count = 0, __owner = 0, __nusers = 0, __kind = 0, __spins = 0, __elision = 0, __list = {
        __prev = 0x0, __next = 0x0}}, __size = '\000' <repeats 39 times>, __align = 0}, rkb_wakeup_fd = {0, 0},
  rkb_toppar_wakeup_fd = 0, rkb_connect_intvl = {ri_ts_last = 0, ri_fixed = 0, ri_backoff = 0},
  rkb_proto = RD_KAFKA_PROTO_PLAINTEXT, rkb_down_reported = 0, rkb_sasl_kinit_refresh_tmr = {rtmr_link = {
      tqe_next = 0x0, tqe_prev = 0x0}, rtmr_next = 0, rtmr_interval = 0, rtmr_callback = 0x0, rtmr_arg = 0x0},
  rkb_err = {msg = '\000' <repeats 511 times>, err = 0}}
As you can see rkb->rkb_ops is (rd_kafka_q_t *) 0x600, which is, of course, an invalid address. In another crashes, we have seen this rkb->rkb_ops as 0x300.
I'll contact you over email about getting you the core itself to investigate.
In case it matters, all code is built and running on CentOS 7.3.1611.",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,1509,2017-11-08T00:28:52Z,2017-11-18T08:11:12Z,2017-11-18T08:11:21Z,CLOSED,False,16,0,2,https://github.com/dacjames,Possible race condition in rd_kafka_q_destroy,1,[],https://github.com/edenhill/librdkafka/pull/1509,https://github.com/dacjames,13,https://github.com/edenhill/librdkafka/pull/1509#issuecomment-344430734,"In rare cases, we've experienced a segfault with the following stack trace.
#0  0x00007ffff6a01bd0 in pthread_mutex_lock () from /usr/lib64/libpthread.so.0
#1  0x00007ffff73ef689 in mtx_lock () from /usr/lib64/libidb.so
#2  0x00007ffff7394903 in rd_kafka_q_enq () from /usr/lib64/libidb.so
#3  0x00007ffff739edf4 in rd_kafka_broker_update () from /usr/lib64/libidb.so
#4  0x00007ffff73f32a1 in rd_kafka_parse_Metadata () from /usr/lib64/libidb.so
#5  0x00007ffff73c3924 in rd_kafka_handle_Metadata () from /usr/lib64/libidb.so
#6  0x00007ffff73b8f5d in rd_kafka_buf_callback () from /usr/lib64/libidb.so
#7  0x00007ffff73c2dec in rd_kafka_op_handle_std () from /usr/lib64/libidb.so
#8  0x00007ffff73c2e1c in rd_kafka_op_handle () from /usr/lib64/libidb.so
#9  0x00007ffff73bd5d7 in rd_kafka_q_serve () from /usr/lib64/libidb.so
#10 0x00007ffff738af2c in rd_kafka_thread_main () from /usr/lib64/libidb.so
#11 0x00007ffff73ef607 in _thrd_wrapper_function () from /usr/lib64/libidb.so
#12 0x00007ffff69ffdc5 in start_thread () from /usr/lib64/libpthread.so.0
#13 0x00007ffff536976d in clone () from /usr/lib64/libc.so.6
It appears that rkq->rkq_lock is somehow null in rd_kafka_q_enq.
Upon inspection of the rd_kafka_q_destroy method, we identified a possible race condition where the rkq struct could be freed while another thread has a reference to it, which seems like it could cause the segfault above. Details of the race condition are added in comments and a straightforward check is added in rd_kafka_q_destroy_final to prevent freeing a queue with a reference count greater than 0.","Also, on further investigation, the struct for the broker appears to be intact within the rdkafa_brokers queue.
p *((rd_kafka_broker_t *) rk->rk_brokers.tqh_first->rkb_link.tqe_next->rkb_link.tqe_next->rkb_link.tqe_next->rkb_link.tqe_next->rkb_link.tqe_next->rkb_link.tqe_next->rkb_link.tqe_next->rkb_link.tqe_next->rkb_link.tqe_next->rkb_link.tqe_next->rkb_link.tqe_next->rkb_link.tqe_next->rkb_link.tqe_next->rkb_link.tqe_next->rkb_link.tqe_next->rkb_link.tqe_next->rkb_link.tqe_next->rkb_link.tqe_next->rkb_link.tqe_next->rkb_link.tqe_next->rkb_link.tqe_next->rkb_link.tqe_next->rkb_link.tqe_next->rkb_link.tqe_next->rkb_link.tqe_next->rkb_link.tqe_next->rkb_link.tqe_next->rkb_link.tqe_next)
$68 = {rkb_link = {tqe_next = 0x0, tqe_prev = 0x7ffff3136b00}, rkb_nodeid = 0, rkb_rsal = 0x7fffe82e0a70,
  rkb_t_rsal_last = 15029178170, rkb_addr_last = 0x7fffe82e0a78, rkb_transport = 0x7fffe82f0140, rkb_corrid = 9,
  rkb_connid = 3, rkb_ops = 0x7ffff30aad40, rkb_lock = {__data = {__lock = 0, __count = 0, __owner = 0,
      __nusers = 0, __kind = 0, __spins = 0, __elision = 0, __list = {__prev = 0x0, __next = 0x0}},
    __size = '\000' <repeats 39 times>, __align = 0}, rkb_blocking_max_ms = 1000, rkb_toppars = {tqh_first = 0x0,
    tqh_last = 0x7ffff3137278}, rkb_toppar_cnt = 0, rkb_fetch_toppars = {cqh_first = 0x7ffff3137290,
    cqh_last = 0x7ffff3137290}, rkb_fetch_toppar_cnt = 0, rkb_fetch_toppar_next = 0x0, rkb_cgrp = 0x0,
  rkb_ts_fetch_backoff = 0, rkb_fetching = 0, rkb_state = RD_KAFKA_BROKER_STATE_UP, rkb_ts_state = 15029272589,
  rkb_timeout_scan_intvl = {ri_ts_last = 21745303061, ri_fixed = 0, ri_backoff = 0}, rkb_blocking_request_cnt = {
    val = 0}, rkb_features = 511, rkb_ApiVersions = 0x7fffe8373c20, rkb_ApiVersions_cnt = 34,
  rkb_ApiVersion_fail_intvl = {ri_ts_last = 15029225264, ri_fixed = 1200000000, ri_backoff = 0},
  rkb_source = RD_KAFKA_LEARNED, rkb_c = {tx_bytes = {val = 621}, tx = {val = 9}, tx_err = {val = 0}, tx_retries = {
      val = 4}, req_timeouts = {val = 6}, rx_bytes = {val = 630}, rx = {val = 3}, rx_err = {val = 0},
    rx_corrid_err = {val = 0}, rx_partial = {val = 0}, zbuf_grow = {val = 0}, buf_grow = {val = 0}, wakeups = {
      val = 17}}, rkb_req_timeouts = 0, rkb_ts_metadata_poll = 448438376, rkb_metadata_fast_poll_cnt = 0,
  rkb_thread = 140731326691072, rkb_refcnt = {lock = {__data = {__lock = 0, __count = 0, __owner = 0, __nusers = 0,
        __kind = 0, __spins = 0, __elision = 0, __list = {__prev = 0x0, __next = 0x0}},
      __size = '\000' <repeats 39 times>, __align = 0}, v = 2}, rkb_rk = 0x7ffff2bccd00,
  rkb_recv_buf = 0x7fffe836ee00, rkb_max_inflight = 1000000, rkb_outbufs = {rkbq_bufs = {tqh_first = 0x0,
      tqh_last = 0x7ffff31373f0}, rkbq_cnt = {val = 0}, rkbq_msg_cnt = {val = 0}}, rkb_waitresps = {rkbq_bufs = {
      tqh_first = 0x0, tqh_last = 0x7ffff3137408}, rkbq_cnt = {val = 0}, rkbq_msg_cnt = {val = 0}}, rkb_retrybufs = {
    rkbq_bufs = {tqh_first = 0x0, tqh_last = 0x7ffff3137420}, rkbq_cnt = {val = 0}, rkbq_msg_cnt = {val = 0}},
  rkb_avg_int_latency = {ra_v = {maxv = 0, minv = 0, avg = 0, sum = 0, cnt = 0, start = 21687129732}, ra_lock = {
      __data = {__lock = 0, __count = 0, __owner = 0, __nusers = 0, __kind = 0, __spins = 0, __elision = 0,
        __list = {__prev = 0x0, __next = 0x0}}, __size = '\000' <repeats 39 times>, __align = 0},
    ra_type = RD_AVG_GAUGE}, rkb_avg_rtt = {ra_v = {maxv = 0, minv = 0, avg = 0, sum = 0, cnt = 0,
      start = 21687129733}, ra_lock = {__data = {__lock = 0, __count = 16777216, __owner = 0, __nusers = 0,
        __kind = 0, __spins = 0, __elision = 0, __list = {__prev = 0x0, __next = 0x0}},
---Type <return> to continue, or q <return> to quit---
      __size = ""\000\000\000\000\000\000\000\001"", '\000' <repeats 31 times>, __align = 72057594037927936},
    ra_type = RD_AVG_GAUGE}, rkb_avg_throttle = {ra_v = {maxv = 0, minv = 0, avg = 0, sum = 0, cnt = 0,
      start = 21687129733}, ra_lock = {__data = {__lock = 0, __count = 0, __owner = 0, __nusers = 0, __kind = 0,
        __spins = 0, __elision = 0, __list = {__prev = 0x0, __next = 0x0}}, __size = '\000' <repeats 39 times>,
      __align = 0}, ra_type = RD_AVG_GAUGE},
  rkb_name = ""transport-0.idb.viasat.io:9092/0"", '\000' <repeats 95 times>,
  rkb_nodename = ""transport-0.idb.viasat.io:9092"", '\000' <repeats 97 times>, rkb_port = 9092,
  rkb_origname = 0x7ffff2f367c0 ""transport-0.idb.viasat.io"",
  rkb_logname = 0x7ffff30a7110 ""transport-0.idb.viasat.io:9092/0"", rkb_logname_lock = {__data = {__lock = 0,
      __count = 0, __owner = 0, __nusers = 0, __kind = 0, __spins = 0, __elision = 0, __list = {__prev = 0x0,
        __next = 0x0}}, __size = '\000' <repeats 39 times>, __align = 0}, rkb_wakeup_fd = {276, 277},
  rkb_toppar_wakeup_fd = -1, rkb_connect_intvl = {ri_ts_last = 15029144298, ri_fixed = 0, ri_backoff = 0},
  rkb_proto = RD_KAFKA_PROTO_PLAINTEXT, rkb_down_reported = 0, rkb_sasl_kinit_refresh_tmr = {rtmr_link = {
      tqe_next = 0x0, tqe_prev = 0x0}, rtmr_next = 0, rtmr_interval = 0, rtmr_callback = 0x0, rtmr_arg = 0x0},
  rkb_err = {
    msg = ""transport-0.idb.viasat.io:9092/0: 3 request(s) timed out: disconnect(3 >= 3)"", '\000' <repeats 435 times>, err = 0}}",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,1509,2017-11-08T00:28:52Z,2017-11-18T08:11:12Z,2017-11-18T08:11:21Z,CLOSED,False,16,0,2,https://github.com/dacjames,Possible race condition in rd_kafka_q_destroy,1,[],https://github.com/edenhill/librdkafka/pull/1509,https://github.com/dacjames,14,https://github.com/edenhill/librdkafka/pull/1509#issuecomment-344461559,"In rare cases, we've experienced a segfault with the following stack trace.
#0  0x00007ffff6a01bd0 in pthread_mutex_lock () from /usr/lib64/libpthread.so.0
#1  0x00007ffff73ef689 in mtx_lock () from /usr/lib64/libidb.so
#2  0x00007ffff7394903 in rd_kafka_q_enq () from /usr/lib64/libidb.so
#3  0x00007ffff739edf4 in rd_kafka_broker_update () from /usr/lib64/libidb.so
#4  0x00007ffff73f32a1 in rd_kafka_parse_Metadata () from /usr/lib64/libidb.so
#5  0x00007ffff73c3924 in rd_kafka_handle_Metadata () from /usr/lib64/libidb.so
#6  0x00007ffff73b8f5d in rd_kafka_buf_callback () from /usr/lib64/libidb.so
#7  0x00007ffff73c2dec in rd_kafka_op_handle_std () from /usr/lib64/libidb.so
#8  0x00007ffff73c2e1c in rd_kafka_op_handle () from /usr/lib64/libidb.so
#9  0x00007ffff73bd5d7 in rd_kafka_q_serve () from /usr/lib64/libidb.so
#10 0x00007ffff738af2c in rd_kafka_thread_main () from /usr/lib64/libidb.so
#11 0x00007ffff73ef607 in _thrd_wrapper_function () from /usr/lib64/libidb.so
#12 0x00007ffff69ffdc5 in start_thread () from /usr/lib64/libpthread.so.0
#13 0x00007ffff536976d in clone () from /usr/lib64/libc.so.6
It appears that rkq->rkq_lock is somehow null in rd_kafka_q_enq.
Upon inspection of the rd_kafka_q_destroy method, we identified a possible race condition where the rkq struct could be freed while another thread has a reference to it, which seems like it could cause the segfault above. Details of the race condition are added in comments and a straightforward check is added in rd_kafka_q_destroy_final to prevent freeing a queue with a reference count greater than 0.","The first element of the rk_broker_by_id list is wrong, somehow.
p ((rd_kafka_broker_t *) rk->rk_brokers.tqh_first->rkb_link.tqe_next->rkb_link.tqe_next->rkb_link.tqe_next->rkb_link.tqe_next->rkb_link.tqe_next->rkb_link.tqe_next->rkb_link.tqe_next->rkb_link.tqe_next->rkb_link.tqe_next->rkb_link.tqe_next->rkb_link.tqe_next->rkb_link.tqe_next->rkb_link.tqe_next->rkb_link.tqe_next->rkb_link.tqe_next->rkb_link.tqe_next->rkb_link.tqe_next->rkb_link.tqe_next->rkb_link.tqe_next->rkb_link.tqe_next->rkb_link.tqe_next->rkb_link.tqe_next->rkb_link.tqe_next->rkb_link.tqe_next->rkb_link.tqe_next->rkb_link.tqe_next->rkb_link.tqe_next->rkb_link.tqe_next)
(gdb) $119 = (rd_kafka_broker_t *) 0x7ffff3137200
Versus
(gdb) p ((rd_kafka_broker_t *) rk->rk_broker_by_id->rl_elems[0])
$169 = (rd_kafka_broker_t *) 0x7ffff31372ff
Very weird pattern. I've confirmed in three separate crashes that the invalid pointer is off by 255: the correct pointer is always 0x123456789a00, while the invalid one is 0x123456789aff.
We have no idea how this could happen, but did notice that in rd_kafka_broker_find_by_nodeid0, rd_kafka_broker_unlock is called before rd_kafka_broker_keep whereas the order is reversed in rd_kafka_broker_find.",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,1509,2017-11-08T00:28:52Z,2017-11-18T08:11:12Z,2017-11-18T08:11:21Z,CLOSED,False,16,0,2,https://github.com/dacjames,Possible race condition in rd_kafka_q_destroy,1,[],https://github.com/edenhill/librdkafka/pull/1509,https://github.com/dacjames,15,https://github.com/edenhill/librdkafka/pull/1509#issuecomment-344772634,"In rare cases, we've experienced a segfault with the following stack trace.
#0  0x00007ffff6a01bd0 in pthread_mutex_lock () from /usr/lib64/libpthread.so.0
#1  0x00007ffff73ef689 in mtx_lock () from /usr/lib64/libidb.so
#2  0x00007ffff7394903 in rd_kafka_q_enq () from /usr/lib64/libidb.so
#3  0x00007ffff739edf4 in rd_kafka_broker_update () from /usr/lib64/libidb.so
#4  0x00007ffff73f32a1 in rd_kafka_parse_Metadata () from /usr/lib64/libidb.so
#5  0x00007ffff73c3924 in rd_kafka_handle_Metadata () from /usr/lib64/libidb.so
#6  0x00007ffff73b8f5d in rd_kafka_buf_callback () from /usr/lib64/libidb.so
#7  0x00007ffff73c2dec in rd_kafka_op_handle_std () from /usr/lib64/libidb.so
#8  0x00007ffff73c2e1c in rd_kafka_op_handle () from /usr/lib64/libidb.so
#9  0x00007ffff73bd5d7 in rd_kafka_q_serve () from /usr/lib64/libidb.so
#10 0x00007ffff738af2c in rd_kafka_thread_main () from /usr/lib64/libidb.so
#11 0x00007ffff73ef607 in _thrd_wrapper_function () from /usr/lib64/libidb.so
#12 0x00007ffff69ffdc5 in start_thread () from /usr/lib64/libpthread.so.0
#13 0x00007ffff536976d in clone () from /usr/lib64/libc.so.6
It appears that rkq->rkq_lock is somehow null in rd_kafka_q_enq.
Upon inspection of the rd_kafka_q_destroy method, we identified a possible race condition where the rkq struct could be freed while another thread has a reference to it, which seems like it could cause the segfault above. Details of the race condition are added in comments and a straightforward check is added in rd_kafka_q_destroy_final to prevent freeing a queue with a reference count greater than 0.","@edenhill This issue has been tracked to a memory corruption bug outside of librdkafka. Apologies if this wasted any of your time.
I still think you should consider this PR. There's still a race condition here even if it's not triggered.",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,1509,2017-11-08T00:28:52Z,2017-11-18T08:11:12Z,2017-11-18T08:11:21Z,CLOSED,False,16,0,2,https://github.com/dacjames,Possible race condition in rd_kafka_q_destroy,1,[],https://github.com/edenhill/librdkafka/pull/1509,https://github.com/edenhill,16,https://github.com/edenhill/librdkafka/pull/1509#issuecomment-345426543,"In rare cases, we've experienced a segfault with the following stack trace.
#0  0x00007ffff6a01bd0 in pthread_mutex_lock () from /usr/lib64/libpthread.so.0
#1  0x00007ffff73ef689 in mtx_lock () from /usr/lib64/libidb.so
#2  0x00007ffff7394903 in rd_kafka_q_enq () from /usr/lib64/libidb.so
#3  0x00007ffff739edf4 in rd_kafka_broker_update () from /usr/lib64/libidb.so
#4  0x00007ffff73f32a1 in rd_kafka_parse_Metadata () from /usr/lib64/libidb.so
#5  0x00007ffff73c3924 in rd_kafka_handle_Metadata () from /usr/lib64/libidb.so
#6  0x00007ffff73b8f5d in rd_kafka_buf_callback () from /usr/lib64/libidb.so
#7  0x00007ffff73c2dec in rd_kafka_op_handle_std () from /usr/lib64/libidb.so
#8  0x00007ffff73c2e1c in rd_kafka_op_handle () from /usr/lib64/libidb.so
#9  0x00007ffff73bd5d7 in rd_kafka_q_serve () from /usr/lib64/libidb.so
#10 0x00007ffff738af2c in rd_kafka_thread_main () from /usr/lib64/libidb.so
#11 0x00007ffff73ef607 in _thrd_wrapper_function () from /usr/lib64/libidb.so
#12 0x00007ffff69ffdc5 in start_thread () from /usr/lib64/libpthread.so.0
#13 0x00007ffff536976d in clone () from /usr/lib64/libc.so.6
It appears that rkq->rkq_lock is somehow null in rd_kafka_q_enq.
Upon inspection of the rd_kafka_q_destroy method, we identified a possible race condition where the rkq struct could be freed while another thread has a reference to it, which seems like it could cause the segfault above. Details of the race condition are added in comments and a straightforward check is added in rd_kafka_q_destroy_final to prevent freeing a queue with a reference count greater than 0.","This refcount approach is used for all shared objects in librdkafka, and I think it is water tight as long as relevant code obtains a reference from someone else holding a reference to an object - that's the contract.
Failure to follow this pattern will result in weird crashes, but that is a bug in the referencing code rather than the queue refcount code, and should be fixed accordingly.
Adding a second layer of checking in destroy_final() will not solve the actual bug in the referencing code (if there is one), only randomly prolong the surfacing of that bug.
What could be useful, if this problem reoccurs even after your memory corruption fix, is to add an assert to destroy_final() that verifies that the refcount is infact still at 0, this would catch poorly referenced bugs sooner.
 on digging and getting your hands dirty on this, much appreciated!",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,1513,2017-11-09T18:37:58Z,2017-11-17T17:33:27Z,2017-11-17T17:33:27Z,MERGED,True,136,324,3,https://github.com/mhowlett,new nuget package layout + debian9 librdkafka build + simplifying,3,[],https://github.com/edenhill/librdkafka/pull/1513,https://github.com/mhowlett,1,https://github.com/edenhill/librdkafka/pull/1513,"@edenhill - the nuget package generated from this should be fine, but I should finish testing all usage scenarios before merging.","@edenhill - the nuget package generated from this should be fine, but I should finish testing all usage scenarios before merging.",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,1516,2017-11-12T14:03:46Z,2017-11-12T15:59:15Z,2017-11-12T16:17:06Z,CLOSED,False,277,707,1,https://github.com/l-rossetti,first clean up [to be tested],2,[],https://github.com/edenhill/librdkafka/pull/1516,https://github.com/l-rossetti,1,https://github.com/edenhill/librdkafka/pull/1516,trying to remove code not need for the Producer tests,trying to remove code not need for the Producer tests,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,1518,2017-11-12T19:07:00Z,2017-11-12T20:12:48Z,2017-11-12T20:12:48Z,MERGED,True,35,3,3,https://github.com/andoma,Add MIPS build and fix CRC32 to work on big endian CPUs,4,[],https://github.com/edenhill/librdkafka/pull/1518,https://github.com/andoma,1,https://github.com/edenhill/librdkafka/pull/1518,,,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,1519,2017-11-12T21:27:37Z,2017-11-13T19:01:05Z,2017-11-15T08:24:19Z,MERGED,True,134,24,12,https://github.com/edenhill,Fix cmake builds,9,[],https://github.com/edenhill/librdkafka/pull/1519,https://github.com/edenhill,1,https://github.com/edenhill/librdkafka/pull/1519,,,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,1523,2017-11-14T16:06:07Z,2017-11-15T08:24:00Z,2017-11-15T08:24:15Z,MERGED,True,17,5,4,https://github.com/edenhill,Offsets store err fix,2,[],https://github.com/edenhill/librdkafka/pull/1523,https://github.com/edenhill,1,https://github.com/edenhill/librdkafka/pull/1523,For confluentinc/confluent-kafka-dotnet#339,For confluentinc/confluent-kafka-dotnet#339,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,1523,2017-11-14T16:06:07Z,2017-11-15T08:24:00Z,2017-11-15T08:24:15Z,MERGED,True,17,5,4,https://github.com/edenhill,Offsets store err fix,2,[],https://github.com/edenhill/librdkafka/pull/1523,https://github.com/mhowlett,2,https://github.com/edenhill/librdkafka/pull/1523#issuecomment-344370219,For confluentinc/confluent-kafka-dotnet#339,"asside from spelling errors, LGTM.",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,1525,2017-11-14T17:53:49Z,2017-11-17T17:34:02Z,2017-11-17T17:34:02Z,CLOSED,False,17,1,2,https://github.com/johnistan,Handle infinite timeout (-1) on offsets for time,4,[],https://github.com/edenhill/librdkafka/pull/1525,https://github.com/johnistan,1,https://github.com/edenhill/librdkafka/pull/1525,Example of failing test for issue raised in confluentinc/confluent-kafka-python#268,Example of failing test for issue raised in confluentinc/confluent-kafka-python#268,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,1525,2017-11-14T17:53:49Z,2017-11-17T17:34:02Z,2017-11-17T17:34:02Z,CLOSED,False,17,1,2,https://github.com/johnistan,Handle infinite timeout (-1) on offsets for time,4,[],https://github.com/edenhill/librdkafka/pull/1525,https://github.com/johnistan,2,https://github.com/edenhill/librdkafka/pull/1525#issuecomment-344357279,Example of failing test for issue raised in confluentinc/confluent-kafka-python#268,Responded to test changes. I have added a small fix to handle -1. Might not be the cleanest approach. Happy to take more comments,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,1525,2017-11-14T17:53:49Z,2017-11-17T17:34:02Z,2017-11-17T17:34:02Z,CLOSED,False,17,1,2,https://github.com/johnistan,Handle infinite timeout (-1) on offsets for time,4,[],https://github.com/edenhill/librdkafka/pull/1525,https://github.com/edenhill,3,https://github.com/edenhill/librdkafka/pull/1525#issuecomment-344738559,Example of failing test for issue raised in confluentinc/confluent-kafka-python#268,"This looks good, I'm making some updates to your branch and will push it tomorrow.
Thanks!",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,1525,2017-11-14T17:53:49Z,2017-11-17T17:34:02Z,2017-11-17T17:34:02Z,CLOSED,False,17,1,2,https://github.com/johnistan,Handle infinite timeout (-1) on offsets for time,4,[],https://github.com/edenhill/librdkafka/pull/1525,https://github.com/edenhill,4,https://github.com/edenhill/librdkafka/pull/1525#issuecomment-345018624,Example of failing test for issue raised in confluentinc/confluent-kafka-python#268,"The fixes grew a bit..
Can you give librdkafka infin_tmout_fixes branch a shot?
https://github.com/edenhill/librdkafka/tree/infin_tmout_fixes",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,1525,2017-11-14T17:53:49Z,2017-11-17T17:34:02Z,2017-11-17T17:34:02Z,CLOSED,False,17,1,2,https://github.com/johnistan,Handle infinite timeout (-1) on offsets for time,4,[],https://github.com/edenhill/librdkafka/pull/1525,https://github.com/edenhill,5,https://github.com/edenhill/librdkafka/pull/1525#issuecomment-345310678,Example of failing test for issue raised in confluentinc/confluent-kafka-python#268,"Closing this in favour of #1530 (which includes this commit)
Thank you!",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,1527,2017-11-15T16:16:48Z,2017-11-20T18:56:12Z,2017-11-20T18:56:17Z,MERGED,True,9,8,3,https://github.com/edenhill,Change/fix defaults for ..kbytes configs,2,[],https://github.com/edenhill/librdkafka/pull/1527,https://github.com/edenhill,1,https://github.com/edenhill/librdkafka/pull/1527,Re #1304,Re #1304,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,1527,2017-11-15T16:16:48Z,2017-11-20T18:56:12Z,2017-11-20T18:56:17Z,MERGED,True,9,8,3,https://github.com/edenhill,Change/fix defaults for ..kbytes configs,2,[],https://github.com/edenhill/librdkafka/pull/1527,https://github.com/mhowlett,2,https://github.com/edenhill/librdkafka/pull/1527#issuecomment-345790980,Re #1304,LGTM,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,1530,2017-11-16T18:10:25Z,2017-11-24T07:41:35Z,2020-05-08T08:57:53Z,MERGED,True,252,94,21,https://github.com/edenhill,"Timeout fixes, and more.",13,[],https://github.com/edenhill/librdkafka/pull/1530,https://github.com/edenhill,1,https://github.com/edenhill/librdkafka/pull/1530,One thing lead to another... and we now have a bunch of core-infrastructure bugs fixed in librdkafka.,One thing lead to another... and we now have a bunch of core-infrastructure bugs fixed in librdkafka.,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,1530,2017-11-16T18:10:25Z,2017-11-24T07:41:35Z,2020-05-08T08:57:53Z,MERGED,True,252,94,21,https://github.com/edenhill,"Timeout fixes, and more.",13,[],https://github.com/edenhill/librdkafka/pull/1530,https://github.com/johnistan,2,https://github.com/edenhill/librdkafka/pull/1530#issuecomment-345325815,One thing lead to another... and we now have a bunch of core-infrastructure bugs fixed in librdkafka.,"Building confluentinc/confluent-kafka-python#268 against this branch works locally for me.
Thanks",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,1530,2017-11-16T18:10:25Z,2017-11-24T07:41:35Z,2020-05-08T08:57:53Z,MERGED,True,252,94,21,https://github.com/edenhill,"Timeout fixes, and more.",13,[],https://github.com/edenhill/librdkafka/pull/1530,https://github.com/edenhill,3,https://github.com/edenhill/librdkafka/pull/1530#issuecomment-345328189,One thing lead to another... and we now have a bunch of core-infrastructure bugs fixed in librdkafka.,@johnistan Awsome! thanks!,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,1530,2017-11-16T18:10:25Z,2017-11-24T07:41:35Z,2020-05-08T08:57:53Z,MERGED,True,252,94,21,https://github.com/edenhill,"Timeout fixes, and more.",13,[],https://github.com/edenhill/librdkafka/pull/1530,https://github.com/mhowlett,4,https://github.com/edenhill/librdkafka/pull/1530#issuecomment-346705777,One thing lead to another... and we now have a bunch of core-infrastructure bugs fixed in librdkafka.,"ASRTM
(all seems reasonable to me)",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,1535,2017-11-20T14:51:29Z,2017-11-20T14:53:29Z,2017-11-20T14:55:25Z,MERGED,True,1,0,1,https://github.com/jurre,Mention rdkafka-ruby in README,1,[],https://github.com/edenhill/librdkafka/pull/1535,https://github.com/jurre,1,https://github.com/edenhill/librdkafka/pull/1535,"I heard a colleague mention this lib but hard a hard time finding it, figured it would be nice to mention in the README.","I heard a colleague mention this lib but hard a hard time finding it, figured it would be nice to mention in the README.",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,1535,2017-11-20T14:51:29Z,2017-11-20T14:53:29Z,2017-11-20T14:55:25Z,MERGED,True,1,0,1,https://github.com/jurre,Mention rdkafka-ruby in README,1,[],https://github.com/edenhill/librdkafka/pull/1535,https://github.com/edenhill,2,https://github.com/edenhill/librdkafka/pull/1535#issuecomment-345718887,"I heard a colleague mention this lib but hard a hard time finding it, figured it would be nice to mention in the README.",Thank you!,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,1551,2017-11-23T13:03:00Z,2017-11-23T13:11:44Z,2017-11-23T13:11:44Z,MERGED,True,1,1,1,https://github.com/thijsc,Update url of rdkafka-ruby,1,[],https://github.com/edenhill/librdkafka/pull/1551,https://github.com/thijsc,1,https://github.com/edenhill/librdkafka/pull/1551,,,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,1571,2017-11-30T11:51:17Z,2018-01-15T09:58:46Z,2020-05-08T08:58:01Z,MERGED,True,3134,473,49,https://github.com/edenhill,Request retry fixes,51,[],https://github.com/edenhill/librdkafka/pull/1571,https://github.com/edenhill,1,https://github.com/edenhill/librdkafka/pull/1571,And fixes for other issues that surfaced while testing retries..,And fixes for other issues that surfaced while testing retries..,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,1580,2017-12-05T17:56:02Z,2017-12-05T18:13:01Z,2017-12-05T18:13:01Z,MERGED,True,1,1,1,https://github.com/skarlsson,Fixes crash on alpine,1,[],https://github.com/edenhill/librdkafka/pull/1580,https://github.com/skarlsson,1,https://github.com/edenhill/librdkafka/pull/1580,,,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,1587,2017-12-07T23:14:55Z,2017-12-12T19:54:18Z,2017-12-12T19:54:18Z,MERGED,True,4,4,1,https://github.com/mhowlett,fixed .lib paths in nuget packaging,2,[],https://github.com/edenhill/librdkafka/pull/1587,https://github.com/mhowlett,1,https://github.com/edenhill/librdkafka/pull/1587,@edenhill - fixes #1583,@edenhill - fixes #1583,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,1587,2017-12-07T23:14:55Z,2017-12-12T19:54:18Z,2017-12-12T19:54:18Z,MERGED,True,4,4,1,https://github.com/mhowlett,fixed .lib paths in nuget packaging,2,[],https://github.com/edenhill/librdkafka/pull/1587,https://github.com/mhowlett,2,https://github.com/edenhill/librdkafka/pull/1587#issuecomment-350124253,@edenhill - fixes #1583,tested by building an x64 c++ project that references this.,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,1587,2017-12-07T23:14:55Z,2017-12-12T19:54:18Z,2017-12-12T19:54:18Z,MERGED,True,4,4,1,https://github.com/mhowlett,fixed .lib paths in nuget packaging,2,[],https://github.com/edenhill/librdkafka/pull/1587,https://github.com/edenhill,3,https://github.com/edenhill/librdkafka/pull/1587#issuecomment-350207038,@edenhill - fixes #1583,"tested by building an x64 c++ project that references this.

that was referencing the C library though, right?",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,1587,2017-12-07T23:14:55Z,2017-12-12T19:54:18Z,2017-12-12T19:54:18Z,MERGED,True,4,4,1,https://github.com/mhowlett,fixed .lib paths in nuget packaging,2,[],https://github.com/edenhill/librdkafka/pull/1587,https://github.com/mhowlett,4,https://github.com/edenhill/librdkafka/pull/1587#issuecomment-350377941,@edenhill - fixes #1583,"re-checked build of an empty C++ project that references the new nuget package after path updates suggested in review. the build completes. In 0.11.3, the build fails with a linker error (cannot find file).",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,1592,2017-12-12T20:59:02Z,2017-12-17T08:37:35Z,2017-12-17T08:37:39Z,MERGED,True,12,12,1,https://github.com/vavrusa,use #if instead of #ifdef / defined() for atomics,1,[],https://github.com/edenhill/librdkafka/pull/1592,https://github.com/vavrusa,1,https://github.com/edenhill/librdkafka/pull/1592,"The #cmakedefine01 always defines the names e.g. HAVE_ATOMICS_32 with
either 0 or 1, but the code in https://github.com/edenhill/librdkafka/blob/master/src/rdatomic.h makes checks such as:
#ifndef HAVE_ATOMICS_32
	mtx_t lock;
#endif
This will always compile support for atomics regardless of whether
it's supported or not.","The #cmakedefine01 always defines the names e.g. HAVE_ATOMICS_32 with
either 0 or 1, but the code in https://github.com/edenhill/librdkafka/blob/master/src/rdatomic.h makes checks such as:
#ifndef HAVE_ATOMICS_32
	mtx_t lock;
#endif
This will always compile support for atomics regardless of whether
it's supported or not.",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,1592,2017-12-12T20:59:02Z,2017-12-17T08:37:35Z,2017-12-17T08:37:39Z,MERGED,True,12,12,1,https://github.com/vavrusa,use #if instead of #ifdef / defined() for atomics,1,[],https://github.com/edenhill/librdkafka/pull/1592,https://github.com/edenhill,2,https://github.com/edenhill/librdkafka/pull/1592#issuecomment-351193772,"The #cmakedefine01 always defines the names e.g. HAVE_ATOMICS_32 with
either 0 or 1, but the code in https://github.com/edenhill/librdkafka/blob/master/src/rdatomic.h makes checks such as:
#ifndef HAVE_ATOMICS_32
	mtx_t lock;
#endif
This will always compile support for atomics regardless of whether
it's supported or not.","Good catch!
I think it might be better to change the #ifdefs to #if though, it is only the ATOMIC checks that do ifdef/defined():
$ git grep '#if.*HAVE'
packaging/cmake/config.h.in:#if (HAVE_ATOMICS_32)
packaging/cmake/config.h.in:#if (HAVE_ATOMICS_64)
src/rd.h:#if HAVE_STRNDUP
src/rdatomic.h:#ifndef HAVE_ATOMICS_32
src/rdatomic.h:#ifndef HAVE_ATOMICS_64
src/rdatomic.h:#if !defined(_MSC_VER) && !defined(HAVE_ATOMICS_32)
src/rdatomic.h:#if !defined(_MSC_VER) && !defined(HAVE_ATOMICS_64)
src/rdposix.h:#if HAVE_STRERROR_R
src/rdregex.c:#if HAVE_REGEX
src/rdregex.c:#if HAVE_REGEX
src/rdregex.c:#if HAVE_REGEX
src/rdregex.c:#if HAVE_REGEX
src/rdregex.c:#if HAVE_REGEX  /* use libc regex */
win32/wingetopt.h:#ifndef HAVE_DECL_GETOPT",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,1592,2017-12-12T20:59:02Z,2017-12-17T08:37:35Z,2017-12-17T08:37:39Z,MERGED,True,12,12,1,https://github.com/vavrusa,use #if instead of #ifdef / defined() for atomics,1,[],https://github.com/edenhill/librdkafka/pull/1592,https://github.com/vavrusa,3,https://github.com/edenhill/librdkafka/pull/1592#issuecomment-351196090,"The #cmakedefine01 always defines the names e.g. HAVE_ATOMICS_32 with
either 0 or 1, but the code in https://github.com/edenhill/librdkafka/blob/master/src/rdatomic.h makes checks such as:
#ifndef HAVE_ATOMICS_32
	mtx_t lock;
#endif
This will always compile support for atomics regardless of whether
it's supported or not.","Makes sense, amended the PR.",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,1592,2017-12-12T20:59:02Z,2017-12-17T08:37:35Z,2017-12-17T08:37:39Z,MERGED,True,12,12,1,https://github.com/vavrusa,use #if instead of #ifdef / defined() for atomics,1,[],https://github.com/edenhill/librdkafka/pull/1592,https://github.com/vavrusa,4,https://github.com/edenhill/librdkafka/pull/1592#issuecomment-352069720,"The #cmakedefine01 always defines the names e.g. HAVE_ATOMICS_32 with
either 0 or 1, but the code in https://github.com/edenhill/librdkafka/blob/master/src/rdatomic.h makes checks such as:
#ifndef HAVE_ATOMICS_32
	mtx_t lock;
#endif
This will always compile support for atomics regardless of whether
it's supported or not.",Do you want me to make more  changes or does it look okay? The CI build seems to have failed for unrelated reasons and I can't rerun it.,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,1592,2017-12-12T20:59:02Z,2017-12-17T08:37:35Z,2017-12-17T08:37:39Z,MERGED,True,12,12,1,https://github.com/vavrusa,use #if instead of #ifdef / defined() for atomics,1,[],https://github.com/edenhill/librdkafka/pull/1592,https://github.com/edenhill,5,https://github.com/edenhill/librdkafka/pull/1592#issuecomment-352240357,"The #cmakedefine01 always defines the names e.g. HAVE_ATOMICS_32 with
either 0 or 1, but the code in https://github.com/edenhill/librdkafka/blob/master/src/rdatomic.h makes checks such as:
#ifndef HAVE_ATOMICS_32
	mtx_t lock;
#endif
This will always compile support for atomics regardless of whether
it's supported or not.",Thank you for this!,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,1602,2017-12-21T12:43:47Z,2018-01-02T14:55:53Z,2018-01-02T17:19:21Z,MERGED,True,20,1,2,https://github.com/proller,Allow build static library,9,[],https://github.com/edenhill/librdkafka/pull/1602,https://github.com/proller,1,https://github.com/edenhill/librdkafka/pull/1602,,,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,1602,2017-12-21T12:43:47Z,2018-01-02T14:55:53Z,2018-01-02T17:19:21Z,MERGED,True,20,1,2,https://github.com/proller,Allow build static library,9,[],https://github.com/edenhill/librdkafka/pull/1602,https://github.com/edenhill,2,https://github.com/edenhill/librdkafka/pull/1602#issuecomment-354820133,,Thanks for this!,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,1610,2017-12-28T17:39:56Z,2018-01-02T18:06:15Z,2018-01-02T18:06:15Z,CLOSED,False,1911,44,23,https://github.com/johnistan,Adds `rd_kafka_header_length` to public api,3,[],https://github.com/edenhill/librdkafka/pull/1610,https://github.com/johnistan,1,https://github.com/edenhill/librdkafka/pull/1610,,,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,1610,2017-12-28T17:39:56Z,2018-01-02T18:06:15Z,2018-01-02T18:06:15Z,CLOSED,False,1911,44,23,https://github.com/johnistan,Adds `rd_kafka_header_length` to public api,3,[],https://github.com/edenhill/librdkafka/pull/1610,https://github.com/johnistan,2,https://github.com/edenhill/librdkafka/pull/1610#issuecomment-354834045,,closed in favor of #1614,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,1612,2018-01-02T07:41:21Z,2018-01-03T07:35:56Z,2020-05-08T08:57:58Z,MERGED,True,482,518,22,https://github.com/edenhill,"Add ""partitioner"" topic config property",3,"['enhancement', 'producer']",https://github.com/edenhill/librdkafka/pull/1612,https://github.com/edenhill,1,https://github.com/edenhill/librdkafka/pull/1612,"This makes the builtin partitioners (including the new Java-compatible murmur2 partitioners) available to any application or language binding that exposes the string-based key=value configuration property interface.
E.g.:
p = confluent_kafka.Producer({.., 'default.topic.config': {'partitioner':'murmur2_random'}})","This makes the builtin partitioners (including the new Java-compatible murmur2 partitioners) available to any application or language binding that exposes the string-based key=value configuration property interface.
E.g.:
p = confluent_kafka.Producer({.., 'default.topic.config': {'partitioner':'murmur2_random'}})",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,1612,2018-01-02T07:41:21Z,2018-01-03T07:35:56Z,2020-05-08T08:57:58Z,MERGED,True,482,518,22,https://github.com/edenhill,"Add ""partitioner"" topic config property",3,"['enhancement', 'producer']",https://github.com/edenhill/librdkafka/pull/1612,https://github.com/mhowlett,2,https://github.com/edenhill/librdkafka/pull/1612#issuecomment-354900864,"This makes the builtin partitioners (including the new Java-compatible murmur2 partitioners) available to any application or language binding that exposes the string-based key=value configuration property interface.
E.g.:
p = confluent_kafka.Producer({.., 'default.topic.config': {'partitioner':'murmur2_random'}})","LGTM, only minor comments.",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,1614,2018-01-02T18:03:17Z,2018-01-02T18:55:10Z,2018-01-02T18:55:16Z,MERGED,True,46,1,3,https://github.com/johnistan,add headers count function to public api,1,[],https://github.com/edenhill/librdkafka/pull/1614,https://github.com/johnistan,1,https://github.com/edenhill/librdkafka/pull/1614,,,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,1614,2018-01-02T18:03:17Z,2018-01-02T18:55:10Z,2018-01-02T18:55:16Z,MERGED,True,46,1,3,https://github.com/johnistan,add headers count function to public api,1,[],https://github.com/edenhill/librdkafka/pull/1614,https://github.com/johnistan,2,https://github.com/edenhill/librdkafka/pull/1614#issuecomment-354833866,,Pulling your upstream changes caused endless merge conflicts. I made a new branch with changes,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,1614,2018-01-02T18:03:17Z,2018-01-02T18:55:10Z,2018-01-02T18:55:16Z,MERGED,True,46,1,3,https://github.com/johnistan,add headers count function to public api,1,[],https://github.com/edenhill/librdkafka/pull/1614,https://github.com/edenhill,3,https://github.com/edenhill/librdkafka/pull/1614#issuecomment-354846526,,"Awesome, thanks for this!",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,1617,2018-01-03T12:32:58Z,2018-01-03T14:42:11Z,2018-01-03T14:42:14Z,MERGED,True,17,8,2,https://github.com/edenhill,rdmurmur: fix for big-endian aligned,1,[],https://github.com/edenhill/librdkafka/pull/1617,https://github.com/edenhill,1,https://github.com/edenhill/librdkafka/pull/1617,,,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,1621,2018-01-04T03:02:33Z,2018-02-02T09:13:04Z,2018-02-02T09:13:10Z,MERGED,True,9,0,1,https://github.com/ChenyuanHu,"Fix When rebalancing and there is uncommited msg, the consumer will s",1,[],https://github.com/edenhill/librdkafka/pull/1621,https://github.com/ChenyuanHu,1,https://github.com/edenhill/librdkafka/pull/1621,"top consume, Close #1605","top consume, Close #1605",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,1621,2018-01-04T03:02:33Z,2018-02-02T09:13:04Z,2018-02-02T09:13:10Z,MERGED,True,9,0,1,https://github.com/ChenyuanHu,"Fix When rebalancing and there is uncommited msg, the consumer will s",1,[],https://github.com/edenhill/librdkafka/pull/1621,https://github.com/ChenyuanHu,2,https://github.com/edenhill/librdkafka/pull/1621#issuecomment-355187898,"top consume, Close #1605","@edenhill Please review.
#1605",True,{'THUMBS_UP': ['https://github.com/andreycha']}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,1621,2018-01-04T03:02:33Z,2018-02-02T09:13:04Z,2018-02-02T09:13:10Z,MERGED,True,9,0,1,https://github.com/ChenyuanHu,"Fix When rebalancing and there is uncommited msg, the consumer will s",1,[],https://github.com/edenhill/librdkafka/pull/1621,https://github.com/andreycha,3,https://github.com/edenhill/librdkafka/pull/1621#issuecomment-359401571,"top consume, Close #1605","Hi @edenhill, any chances this request is going to be checked and reviewed any time soon?",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,1621,2018-01-04T03:02:33Z,2018-02-02T09:13:04Z,2018-02-02T09:13:10Z,MERGED,True,9,0,1,https://github.com/ChenyuanHu,"Fix When rebalancing and there is uncommited msg, the consumer will s",1,[],https://github.com/edenhill/librdkafka/pull/1621,https://github.com/edenhill,4,https://github.com/edenhill/librdkafka/pull/1621#issuecomment-359929735,"top consume, Close #1605","Yes, thank you for this, we'll tend to it shortly!",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,1621,2018-01-04T03:02:33Z,2018-02-02T09:13:04Z,2018-02-02T09:13:10Z,MERGED,True,9,0,1,https://github.com/ChenyuanHu,"Fix When rebalancing and there is uncommited msg, the consumer will s",1,[],https://github.com/edenhill/librdkafka/pull/1621,https://github.com/edenhill,5,https://github.com/edenhill/librdkafka/pull/1621#issuecomment-362529451,"top consume, Close #1605",Thank you for this!,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,1626,2018-01-06T12:44:29Z,2018-01-08T18:55:40Z,2018-01-08T18:55:40Z,CLOSED,False,2208,384,37,https://github.com/barrotsteindev,Produce batch partition per message flag,30,[],https://github.com/edenhill/librdkafka/pull/1626,https://github.com/barrotsteindev,1,https://github.com/edenhill/librdkafka/pull/1626,for #1604,for #1604,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,1626,2018-01-06T12:44:29Z,2018-01-08T18:55:40Z,2018-01-08T18:55:40Z,CLOSED,False,2208,384,37,https://github.com/barrotsteindev,Produce batch partition per message flag,30,[],https://github.com/edenhill/librdkafka/pull/1626,https://github.com/barrotsteindev,2,https://github.com/edenhill/librdkafka/pull/1626#issuecomment-355771899,for #1604,"@edenhill ,
I am having trouble with the.merge conflicts as I am not familiar with the retry_fixes changes.
It would be a great help if you could help me with that.
Thanks in Advance",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,1626,2018-01-06T12:44:29Z,2018-01-08T18:55:40Z,2018-01-08T18:55:40Z,CLOSED,False,2208,384,37,https://github.com/barrotsteindev,Produce batch partition per message flag,30,[],https://github.com/edenhill/librdkafka/pull/1626,https://github.com/edenhill,3,https://github.com/edenhill/librdkafka/pull/1626#issuecomment-355964560,for #1604,You should base your branch on retry_fixes and when creating the PR you set the base branch to retry_fixes.,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,1626,2018-01-06T12:44:29Z,2018-01-08T18:55:40Z,2018-01-08T18:55:40Z,CLOSED,False,2208,384,37,https://github.com/barrotsteindev,Produce batch partition per message flag,30,[],https://github.com/edenhill/librdkafka/pull/1626,https://github.com/barrotsteindev,4,https://github.com/edenhill/librdkafka/pull/1626#issuecomment-356026344,for #1604,"Sorry guess I didn't get it the first time.
Will fix asap.

On Jan 8, 2018 3:21 PM, ""Magnus Edenhill"" ***@***.***> wrote:
 You should base your branch on retry_fixes and when creating the PR you
 set the base branch to retry_fixes.

 
 You are receiving this because you authored the thread.
 Reply to this email directly, view it on GitHub
 <#1626 (comment)>,
 or mute the thread
 <https://github.com/notifications/unsubscribe-auth/ACWANFuRIxR0i2-mOx2WmCBzkaADSlklks5tIhZxgaJpZM4RVTLY>
 .",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,1626,2018-01-06T12:44:29Z,2018-01-08T18:55:40Z,2018-01-08T18:55:40Z,CLOSED,False,2208,384,37,https://github.com/barrotsteindev,Produce batch partition per message flag,30,[],https://github.com/edenhill/librdkafka/pull/1626,https://github.com/barrotsteindev,5,https://github.com/edenhill/librdkafka/pull/1626#issuecomment-356059640,for #1604,#1628 opened new pull request to different branch,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,1628,2018-01-08T18:54:54Z,2018-01-21T23:54:28Z,2018-01-21T23:54:36Z,MERGED,True,286,14,3,https://github.com/barrotsteindev,Produce batch flag per-message partition flag,41,[],https://github.com/edenhill/librdkafka/pull/1628,https://github.com/barrotsteindev,1,https://github.com/edenhill/librdkafka/pull/1628,"for #1604,
based on #1626","for #1604,
based on #1626",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,1628,2018-01-08T18:54:54Z,2018-01-21T23:54:28Z,2018-01-21T23:54:36Z,MERGED,True,286,14,3,https://github.com/barrotsteindev,Produce batch flag per-message partition flag,41,[],https://github.com/edenhill/librdkafka/pull/1628,https://github.com/edenhill,2,https://github.com/edenhill/librdkafka/pull/1628#issuecomment-356623008,"for #1604,
based on #1626","I force-pushed to retry_fixes early today which most likely messed up your stuff, do a pull --force, fix the conflicts, and push --force again to get up to speed.",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,1628,2018-01-08T18:54:54Z,2018-01-21T23:54:28Z,2018-01-21T23:54:36Z,MERGED,True,286,14,3,https://github.com/barrotsteindev,Produce batch flag per-message partition flag,41,[],https://github.com/edenhill/librdkafka/pull/1628,https://github.com/barrotsteindev,3,https://github.com/edenhill/librdkafka/pull/1628#issuecomment-357998562,"for #1604,
based on #1626","@edenhill,
do I now rebase this against the latest commit on master,
since the retry_fixes branch was merged?",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,1628,2018-01-08T18:54:54Z,2018-01-21T23:54:28Z,2018-01-21T23:54:36Z,MERGED,True,286,14,3,https://github.com/barrotsteindev,Produce batch flag per-message partition flag,41,[],https://github.com/edenhill/librdkafka/pull/1628,https://github.com/edenhill,4,https://github.com/edenhill/librdkafka/pull/1628#issuecomment-358024646,"for #1604,
based on #1626",Yes sir!,True,{'THUMBS_UP': ['https://github.com/barrotsteindev']}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,1628,2018-01-08T18:54:54Z,2018-01-21T23:54:28Z,2018-01-21T23:54:36Z,MERGED,True,286,14,3,https://github.com/barrotsteindev,Produce batch flag per-message partition flag,41,[],https://github.com/edenhill/librdkafka/pull/1628,https://github.com/barrotsteindev,5,https://github.com/edenhill/librdkafka/pull/1628#issuecomment-358100618,"for #1604,
based on #1626","@edenhill
would love to hear your opinion",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,1628,2018-01-08T18:54:54Z,2018-01-21T23:54:28Z,2018-01-21T23:54:36Z,MERGED,True,286,14,3,https://github.com/barrotsteindev,Produce batch flag per-message partition flag,41,[],https://github.com/edenhill/librdkafka/pull/1628,https://github.com/barrotsteindev,6,https://github.com/edenhill/librdkafka/pull/1628#issuecomment-358206235,"for #1604,
based on #1626","@edenhill ,
Tavis ci has been having trouble with their builds infrastructure.
It would be lovely if you could restart the build.",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,1628,2018-01-08T18:54:54Z,2018-01-21T23:54:28Z,2018-01-21T23:54:36Z,MERGED,True,286,14,3,https://github.com/barrotsteindev,Produce batch flag per-message partition flag,41,[],https://github.com/edenhill/librdkafka/pull/1628,https://github.com/barrotsteindev,7,https://github.com/edenhill/librdkafka/pull/1628#issuecomment-359271423,"for #1604,
based on #1626","@edenhill,
Is there anything else that has to be done?",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,1628,2018-01-08T18:54:54Z,2018-01-21T23:54:28Z,2018-01-21T23:54:36Z,MERGED,True,286,14,3,https://github.com/barrotsteindev,Produce batch flag per-message partition flag,41,[],https://github.com/edenhill/librdkafka/pull/1628,https://github.com/edenhill,8,https://github.com/edenhill/librdkafka/pull/1628#issuecomment-359293402,"for #1604,
based on #1626",Thanks alot for this!,True,{'HOORAY': ['https://github.com/barrotsteindev']}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,1629,2018-01-09T11:20:50Z,2018-01-09T15:51:52Z,2018-01-09T15:51:55Z,MERGED,True,86,28,5,https://github.com/edenhill,Fix #1623,2,[],https://github.com/edenhill/librdkafka/pull/1629,https://github.com/edenhill,1,https://github.com/edenhill/librdkafka/pull/1629,,,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,1632,2018-01-12T08:43:24Z,2018-08-20T15:52:06Z,2018-08-20T15:52:06Z,CLOSED,False,3,1,1,https://github.com/qdore,Fix bug: that broker stucked and recovered will cause producer stuck,1,[],https://github.com/edenhill/librdkafka/pull/1632,https://github.com/qdore,1,https://github.com/edenhill/librdkafka/pull/1632,"We found this problem with a network error. The producer lost connect with the broker, and after a few minutes, the network recovered but the client stuck forever. (The broker's status in client is CONNECT)
How to Reproduce: client connect a kafka broker, send SIGSTOP to kafka broker, wait 10~15 minutes, then send SIGCONT to kafka broker, the producer will stuck with a big probability.","We found this problem with a network error. The producer lost connect with the broker, and after a few minutes, the network recovered but the client stuck forever. (The broker's status in client is CONNECT)
How to Reproduce: client connect a kafka broker, send SIGSTOP to kafka broker, wait 10~15 minutes, then send SIGCONT to kafka broker, the producer will stuck with a big probability.",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,1632,2018-01-12T08:43:24Z,2018-08-20T15:52:06Z,2018-08-20T15:52:06Z,CLOSED,False,3,1,1,https://github.com/qdore,Fix bug: that broker stucked and recovered will cause producer stuck,1,[],https://github.com/edenhill/librdkafka/pull/1632,https://github.com/edenhill,2,https://github.com/edenhill/librdkafka/pull/1632#issuecomment-371490272,"We found this problem with a network error. The producer lost connect with the broker, and after a few minutes, the network recovered but the client stuck forever. (The broker's status in client is CONNECT)
How to Reproduce: client connect a kafka broker, send SIGSTOP to kafka broker, wait 10~15 minutes, then send SIGCONT to kafka broker, the producer will stuck with a big probability.",Did you set socket.blocking.max.ms explicitly to a non-default value?,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,1637,2018-01-16T13:29:07Z,2018-01-16T13:38:18Z,2018-01-16T13:38:18Z,MERGED,True,3,1,1,https://github.com/raulbocanegra,Fix build error on Windows,1,[],https://github.com/edenhill/librdkafka/pull/1637,https://github.com/raulbocanegra,1,https://github.com/edenhill/librdkafka/pull/1637,Fix C2491: definition of dllimport static data member not allowed,Fix C2491: definition of dllimport static data member not allowed,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,1637,2018-01-16T13:29:07Z,2018-01-16T13:38:18Z,2018-01-16T13:38:18Z,MERGED,True,3,1,1,https://github.com/raulbocanegra,Fix build error on Windows,1,[],https://github.com/edenhill/librdkafka/pull/1637,https://github.com/edenhill,2,https://github.com/edenhill/librdkafka/pull/1637#issuecomment-357962436,Fix C2491: definition of dllimport static data member not allowed,Thanks for this!,True,{'THUMBS_UP': ['https://github.com/raulbocanegra']}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,1638,2018-01-16T13:43:38Z,2018-01-22T21:44:09Z,2018-01-22T21:44:09Z,MERGED,True,74,20,7,https://github.com/raulbocanegra,cmake build improve on windows,13,[],https://github.com/edenhill/librdkafka/pull/1638,https://github.com/raulbocanegra,1,https://github.com/edenhill/librdkafka/pull/1638,"In Windows, the option WITH_PLUGINS is set to OFF, but in src/win32_config.h you can find #define WITH_PLUGINS 1.
This cause link errors when you link against librdkafka. So you have to manually set #define WITH_PLUGINS 0 for the cmake build to work properly.","In Windows, the option WITH_PLUGINS is set to OFF, but in src/win32_config.h you can find #define WITH_PLUGINS 1.
This cause link errors when you link against librdkafka. So you have to manually set #define WITH_PLUGINS 0 for the cmake build to work properly.",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,1638,2018-01-16T13:43:38Z,2018-01-22T21:44:09Z,2018-01-22T21:44:09Z,MERGED,True,74,20,7,https://github.com/raulbocanegra,cmake build improve on windows,13,[],https://github.com/edenhill/librdkafka/pull/1638,https://github.com/raulbocanegra,2,https://github.com/edenhill/librdkafka/pull/1638#issuecomment-357965538,"In Windows, the option WITH_PLUGINS is set to OFF, but in src/win32_config.h you can find #define WITH_PLUGINS 1.
This cause link errors when you link against librdkafka. So you have to manually set #define WITH_PLUGINS 0 for the cmake build to work properly.","I'm not sure how this can be fixed to make cmake builds and regular Windows builds both to work fine.
Probably we could make a configured cmake header, but not sure how to do this without breaking Windows build without cmake",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,1638,2018-01-16T13:43:38Z,2018-01-22T21:44:09Z,2018-01-22T21:44:09Z,MERGED,True,74,20,7,https://github.com/raulbocanegra,cmake build improve on windows,13,[],https://github.com/edenhill/librdkafka/pull/1638,https://github.com/edenhill,3,https://github.com/edenhill/librdkafka/pull/1638#issuecomment-357986812,"In Windows, the option WITH_PLUGINS is set to OFF, but in src/win32_config.h you can find #define WITH_PLUGINS 1.
This cause link errors when you link against librdkafka. So you have to manually set #define WITH_PLUGINS 0 for the cmake build to work properly.","WITH_PLUGINS should be enabled on Windows, it is using LoadLibrary which is available in the standard library.",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,1638,2018-01-16T13:43:38Z,2018-01-22T21:44:09Z,2018-01-22T21:44:09Z,MERGED,True,74,20,7,https://github.com/raulbocanegra,cmake build improve on windows,13,[],https://github.com/edenhill/librdkafka/pull/1638,https://github.com/raulbocanegra,4,https://github.com/edenhill/librdkafka/pull/1638#issuecomment-358004226,"In Windows, the option WITH_PLUGINS is set to OFF, but in src/win32_config.h you can find #define WITH_PLUGINS 1.
This cause link errors when you link against librdkafka. So you have to manually set #define WITH_PLUGINS 0 for the cmake build to work properly.",And should WITH_PLUGINS also be enabled when building static version ol librdkafka? I don't know what plugins are used for.,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,1638,2018-01-16T13:43:38Z,2018-01-22T21:44:09Z,2018-01-22T21:44:09Z,MERGED,True,74,20,7,https://github.com/raulbocanegra,cmake build improve on windows,13,[],https://github.com/edenhill/librdkafka/pull/1638,https://github.com/edenhill,5,https://github.com/edenhill/librdkafka/pull/1638#issuecomment-358011211,"In Windows, the option WITH_PLUGINS is set to OFF, but in src/win32_config.h you can find #define WITH_PLUGINS 1.
This cause link errors when you link against librdkafka. So you have to manually set #define WITH_PLUGINS 0 for the cmake build to work properly.",https://github.com/edenhill/librdkafka/blob/master/src/rdkafka.h#L3650,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,1638,2018-01-16T13:43:38Z,2018-01-22T21:44:09Z,2018-01-22T21:44:09Z,MERGED,True,74,20,7,https://github.com/raulbocanegra,cmake build improve on windows,13,[],https://github.com/edenhill/librdkafka/pull/1638,https://github.com/raulbocanegra,6,https://github.com/edenhill/librdkafka/pull/1638#issuecomment-358016879,"In Windows, the option WITH_PLUGINS is set to OFF, but in src/win32_config.h you can find #define WITH_PLUGINS 1.
This cause link errors when you link against librdkafka. So you have to manually set #define WITH_PLUGINS 0 for the cmake build to work properly.","Thanks!
So, if I'm not going to use any plugin I can safely turn this off, right?",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,1638,2018-01-16T13:43:38Z,2018-01-22T21:44:09Z,2018-01-22T21:44:09Z,MERGED,True,74,20,7,https://github.com/raulbocanegra,cmake build improve on windows,13,[],https://github.com/edenhill/librdkafka/pull/1638,https://github.com/edenhill,7,https://github.com/edenhill/librdkafka/pull/1638#issuecomment-358017256,"In Windows, the option WITH_PLUGINS is set to OFF, but in src/win32_config.h you can find #define WITH_PLUGINS 1.
This cause link errors when you link against librdkafka. So you have to manually set #define WITH_PLUGINS 0 for the cmake build to work properly.",Yep,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,1638,2018-01-16T13:43:38Z,2018-01-22T21:44:09Z,2018-01-22T21:44:09Z,MERGED,True,74,20,7,https://github.com/raulbocanegra,cmake build improve on windows,13,[],https://github.com/edenhill/librdkafka/pull/1638,https://github.com/edenhill,8,https://github.com/edenhill/librdkafka/pull/1638#issuecomment-358261602,"In Windows, the option WITH_PLUGINS is set to OFF, but in src/win32_config.h you can find #define WITH_PLUGINS 1.
This cause link errors when you link against librdkafka. So you have to manually set #define WITH_PLUGINS 0 for the cmake build to work properly.","Could you have CMake emit a command line define for ""WITH_PLUGINS=0"" when building static library and then add an #ifndef WITH_PLUGINS guard to the define in win32_config.h?",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,1638,2018-01-16T13:43:38Z,2018-01-22T21:44:09Z,2018-01-22T21:44:09Z,MERGED,True,74,20,7,https://github.com/raulbocanegra,cmake build improve on windows,13,[],https://github.com/edenhill/librdkafka/pull/1638,https://github.com/rbocanegra,9,https://github.com/edenhill/librdkafka/pull/1638#issuecomment-358268912,"In Windows, the option WITH_PLUGINS is set to OFF, but in src/win32_config.h you can find #define WITH_PLUGINS 1.
This cause link errors when you link against librdkafka. So you have to manually set #define WITH_PLUGINS 0 for the cmake build to work properly.","Yep I will try to do it during the day.
I have been trying to compile with cmake with -DWITH_PLUGINS=ON on Windows and it has som link errors.
error LNK2001: unresolved external symbol rd_dl_open
error LNK2001: unresolved external symbol rd_dl_close
error LNK2001: unresolved external symbol rd_dl_sym
I will try your last suggestion to see if that works",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,1638,2018-01-16T13:43:38Z,2018-01-22T21:44:09Z,2018-01-22T21:44:09Z,MERGED,True,74,20,7,https://github.com/raulbocanegra,cmake build improve on windows,13,[],https://github.com/edenhill/librdkafka/pull/1638,https://github.com/raulbocanegra,10,https://github.com/edenhill/librdkafka/pull/1638#issuecomment-359222384,"In Windows, the option WITH_PLUGINS is set to OFF, but in src/win32_config.h you can find #define WITH_PLUGINS 1.
This cause link errors when you link against librdkafka. So you have to manually set #define WITH_PLUGINS 0 for the cmake build to work properly.","I think it is done.
The link errors I reported in my previous message:
error LNK2001: unresolved external symbol rd_dl_open
error LNK2001: unresolved external symbol rd_dl_close
error LNK2001: unresolved external symbol rd_dl_sym
were due to the fact that rddl.c was not added to cmake project on Windows.
I have also make some other changes to compile shared and statically on Windows.
Feel free to ask if you need any clarification.",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,1638,2018-01-16T13:43:38Z,2018-01-22T21:44:09Z,2018-01-22T21:44:09Z,MERGED,True,74,20,7,https://github.com/raulbocanegra,cmake build improve on windows,13,[],https://github.com/edenhill/librdkafka/pull/1638,https://github.com/edenhill,11,https://github.com/edenhill/librdkafka/pull/1638#issuecomment-359293023,"In Windows, the option WITH_PLUGINS is set to OFF, but in src/win32_config.h you can find #define WITH_PLUGINS 1.
This cause link errors when you link against librdkafka. So you have to manually set #define WITH_PLUGINS 0 for the cmake build to work properly.","Looks good!
Is this ready to be merged?",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,1638,2018-01-16T13:43:38Z,2018-01-22T21:44:09Z,2018-01-22T21:44:09Z,MERGED,True,74,20,7,https://github.com/raulbocanegra,cmake build improve on windows,13,[],https://github.com/edenhill/librdkafka/pull/1638,https://github.com/raulbocanegra,12,https://github.com/edenhill/librdkafka/pull/1638#issuecomment-359356608,"In Windows, the option WITH_PLUGINS is set to OFF, but in src/win32_config.h you can find #define WITH_PLUGINS 1.
This cause link errors when you link against librdkafka. So you have to manually set #define WITH_PLUGINS 0 for the cmake build to work properly.",yep,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,1644,2018-01-19T15:00:15Z,2018-02-02T11:08:20Z,2018-02-02T11:08:26Z,CLOSED,False,3,11,2,https://github.com/Soundman32,Fixed unaligned pointer warning when building on arm (BeagleBone/Debi,1,[],https://github.com/edenhill/librdkafka/pull/1644,https://github.com/Soundman32,1,https://github.com/edenhill/librdkafka/pull/1644,an).,an).,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,1644,2018-01-19T15:00:15Z,2018-02-02T11:08:20Z,2018-02-02T11:08:26Z,CLOSED,False,3,11,2,https://github.com/Soundman32,Fixed unaligned pointer warning when building on arm (BeagleBone/Debi,1,[],https://github.com/edenhill/librdkafka/pull/1644,https://github.com/edenhill,2,https://github.com/edenhill/librdkafka/pull/1644#issuecomment-362557840,an).,"Thanks for this.
Since the PR can't be merged due to CONFIGURATION.md changes I fixed this out of bound but referenced you.",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,1645,2018-01-22T04:48:05Z,2018-03-08T13:40:15Z,2018-03-08T13:40:15Z,CLOSED,False,44,2,7,https://github.com/4alexey,Create a new rd_kafka_conf_set_free_payload_cb API to set the callbac,1,[],https://github.com/edenhill/librdkafka/pull/1645,https://github.com/4alexey,1,https://github.com/edenhill/librdkafka/pull/1645,"Create a new rd_kafka_conf_set_free_payload_cb API to set the callback to free memory pointer passed to rd_kafka_produce API when RD_KAFKA_MSG_F_FREE flag is set. This is a common scenario when a custom application memory management is used. For performance-critical applications passing RD_KAFKA_MSG_F_FREE will eliminate extra malloc and memcpy calls to allocate and copy message payload memory. Librdkafka currently calls free(payload) to free memory in rd_kafka_msg_destroy when RD_KAFKA_MSG_F_FREE is set. This will result in undefined behavior when a custom memory allocation is used in the application and the payload pointer memory is not allocated withcalloc,malloc, orrealloc. To enable this scenario following code changes are proposed:

Create a new  rd_kafka_conf_set_free_payload_cb to set a free payload callback intended to call  custom memory deallocation application function.
Create a new RD_KAFKA_OP_PAYLOAD_FREE operation.
In rd_kafka_msg_destroy enqueue  RD_KAFKA_OP_PAYLOAD_FREE operation to the tail of the 'rkq' queue with RD_KAFKA_PRIO_MEDIUM priority. This will result in d_kafka_poll_cb call.
In rd_kafka_poll_cb free the payload by calling the application callback ( rkrk_conf.free_payload_cb).

Only C code is currently implemented. C++ changes are proposed.

rdkafka_op.h
rdkafka_msg.c
rdkafka_event.c
rdkafka_conf.h
rdkafka_conf.c
rdkafka.h
rdkafka.c","Create a new rd_kafka_conf_set_free_payload_cb API to set the callback to free memory pointer passed to rd_kafka_produce API when RD_KAFKA_MSG_F_FREE flag is set. This is a common scenario when a custom application memory management is used. For performance-critical applications passing RD_KAFKA_MSG_F_FREE will eliminate extra malloc and memcpy calls to allocate and copy message payload memory. Librdkafka currently calls free(payload) to free memory in rd_kafka_msg_destroy when RD_KAFKA_MSG_F_FREE is set. This will result in undefined behavior when a custom memory allocation is used in the application and the payload pointer memory is not allocated withcalloc,malloc, orrealloc. To enable this scenario following code changes are proposed:

Create a new  rd_kafka_conf_set_free_payload_cb to set a free payload callback intended to call  custom memory deallocation application function.
Create a new RD_KAFKA_OP_PAYLOAD_FREE operation.
In rd_kafka_msg_destroy enqueue  RD_KAFKA_OP_PAYLOAD_FREE operation to the tail of the 'rkq' queue with RD_KAFKA_PRIO_MEDIUM priority. This will result in d_kafka_poll_cb call.
In rd_kafka_poll_cb free the payload by calling the application callback ( rkrk_conf.free_payload_cb).

Only C code is currently implemented. C++ changes are proposed.

rdkafka_op.h
rdkafka_msg.c
rdkafka_event.c
rdkafka_conf.h
rdkafka_conf.c
rdkafka.h
rdkafka.c",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,1645,2018-01-22T04:48:05Z,2018-03-08T13:40:15Z,2018-03-08T13:40:15Z,CLOSED,False,44,2,7,https://github.com/4alexey,Create a new rd_kafka_conf_set_free_payload_cb API to set the callbac,1,[],https://github.com/edenhill/librdkafka/pull/1645,https://github.com/edenhill,2,https://github.com/edenhill/librdkafka/pull/1645#issuecomment-359576563,"Create a new rd_kafka_conf_set_free_payload_cb API to set the callback to free memory pointer passed to rd_kafka_produce API when RD_KAFKA_MSG_F_FREE flag is set. This is a common scenario when a custom application memory management is used. For performance-critical applications passing RD_KAFKA_MSG_F_FREE will eliminate extra malloc and memcpy calls to allocate and copy message payload memory. Librdkafka currently calls free(payload) to free memory in rd_kafka_msg_destroy when RD_KAFKA_MSG_F_FREE is set. This will result in undefined behavior when a custom memory allocation is used in the application and the payload pointer memory is not allocated withcalloc,malloc, orrealloc. To enable this scenario following code changes are proposed:

Create a new  rd_kafka_conf_set_free_payload_cb to set a free payload callback intended to call  custom memory deallocation application function.
Create a new RD_KAFKA_OP_PAYLOAD_FREE operation.
In rd_kafka_msg_destroy enqueue  RD_KAFKA_OP_PAYLOAD_FREE operation to the tail of the 'rkq' queue with RD_KAFKA_PRIO_MEDIUM priority. This will result in d_kafka_poll_cb call.
In rd_kafka_poll_cb free the payload by calling the application callback ( rkrk_conf.free_payload_cb).

Only C code is currently implemented. C++ changes are proposed.

rdkafka_op.h
rdkafka_msg.c
rdkafka_event.c
rdkafka_conf.h
rdkafka_conf.c
rdkafka.h
rdkafka.c","Thanks for your contirbution.
This API makes sense but I believe it is already covered by the functionality of the delivery report callback which is not only meant for delivery signaling but also to allow the application to free its own payload.",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,1645,2018-01-22T04:48:05Z,2018-03-08T13:40:15Z,2018-03-08T13:40:15Z,CLOSED,False,44,2,7,https://github.com/4alexey,Create a new rd_kafka_conf_set_free_payload_cb API to set the callbac,1,[],https://github.com/edenhill/librdkafka/pull/1645,https://github.com/4alexey,3,https://github.com/edenhill/librdkafka/pull/1645#issuecomment-359615679,"Create a new rd_kafka_conf_set_free_payload_cb API to set the callback to free memory pointer passed to rd_kafka_produce API when RD_KAFKA_MSG_F_FREE flag is set. This is a common scenario when a custom application memory management is used. For performance-critical applications passing RD_KAFKA_MSG_F_FREE will eliminate extra malloc and memcpy calls to allocate and copy message payload memory. Librdkafka currently calls free(payload) to free memory in rd_kafka_msg_destroy when RD_KAFKA_MSG_F_FREE is set. This will result in undefined behavior when a custom memory allocation is used in the application and the payload pointer memory is not allocated withcalloc,malloc, orrealloc. To enable this scenario following code changes are proposed:

Create a new  rd_kafka_conf_set_free_payload_cb to set a free payload callback intended to call  custom memory deallocation application function.
Create a new RD_KAFKA_OP_PAYLOAD_FREE operation.
In rd_kafka_msg_destroy enqueue  RD_KAFKA_OP_PAYLOAD_FREE operation to the tail of the 'rkq' queue with RD_KAFKA_PRIO_MEDIUM priority. This will result in d_kafka_poll_cb call.
In rd_kafka_poll_cb free the payload by calling the application callback ( rkrk_conf.free_payload_cb).

Only C code is currently implemented. C++ changes are proposed.

rdkafka_op.h
rdkafka_msg.c
rdkafka_event.c
rdkafka_conf.h
rdkafka_conf.c
rdkafka.h
rdkafka.c","Hi,
The delivery report callback could have been used to call the custom memory management deallocator.  The problem is with rd_kafka_msg_destroy function though, when RD_KAFKA_MSG_F_FREE flag is set, as it calls rd_free(rkm->rkm_payload). This results in a crush when the rkm_payload pointer is not allocated with calloc, malloc, or realloc, but points to the address pre-allocated with the custom memory allocator.
The goal of using RD_KAFKA_MSG_F_FREE flag is to avoid extra malloc and memcpy calls for the payload, as it's already stored on the custom memory management pool.
Another way to solve this is by calling rd_free(rkm->rkm_payload) conditionally, but other changes in the rdkafka_conf.h are need then to signal the custom memory allocator is used. Since callbacks are the common way in the library design, I implemented it using the callback.
The issue has already been discussed in #1310 before.
void rd_kafka_msg_destroy (rd_kafka_t *rk, rd_kafka_msg_t *rkm) {
...
if (rkm->rkm_flags & RD_KAFKA_MSG_F_FREE && rkm->rkm_payload)
rd_free(rkm->rkm_payload);
...
}",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,1645,2018-01-22T04:48:05Z,2018-03-08T13:40:15Z,2018-03-08T13:40:15Z,CLOSED,False,44,2,7,https://github.com/4alexey,Create a new rd_kafka_conf_set_free_payload_cb API to set the callbac,1,[],https://github.com/edenhill/librdkafka/pull/1645,https://github.com/edenhill,4,https://github.com/edenhill/librdkafka/pull/1645#issuecomment-359637191,"Create a new rd_kafka_conf_set_free_payload_cb API to set the callback to free memory pointer passed to rd_kafka_produce API when RD_KAFKA_MSG_F_FREE flag is set. This is a common scenario when a custom application memory management is used. For performance-critical applications passing RD_KAFKA_MSG_F_FREE will eliminate extra malloc and memcpy calls to allocate and copy message payload memory. Librdkafka currently calls free(payload) to free memory in rd_kafka_msg_destroy when RD_KAFKA_MSG_F_FREE is set. This will result in undefined behavior when a custom memory allocation is used in the application and the payload pointer memory is not allocated withcalloc,malloc, orrealloc. To enable this scenario following code changes are proposed:

Create a new  rd_kafka_conf_set_free_payload_cb to set a free payload callback intended to call  custom memory deallocation application function.
Create a new RD_KAFKA_OP_PAYLOAD_FREE operation.
In rd_kafka_msg_destroy enqueue  RD_KAFKA_OP_PAYLOAD_FREE operation to the tail of the 'rkq' queue with RD_KAFKA_PRIO_MEDIUM priority. This will result in d_kafka_poll_cb call.
In rd_kafka_poll_cb free the payload by calling the application callback ( rkrk_conf.free_payload_cb).

Only C code is currently implemented. C++ changes are proposed.

rdkafka_op.h
rdkafka_msg.c
rdkafka_event.c
rdkafka_conf.h
rdkafka_conf.c
rdkafka.h
rdkafka.c","So what you do is not to pass FREE or COPY to produce(), that way it will not copy your payload and it relies on you freeing it in your delivery report callback.",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,1645,2018-01-22T04:48:05Z,2018-03-08T13:40:15Z,2018-03-08T13:40:15Z,CLOSED,False,44,2,7,https://github.com/4alexey,Create a new rd_kafka_conf_set_free_payload_cb API to set the callbac,1,[],https://github.com/edenhill/librdkafka/pull/1645,https://github.com/4alexey,5,https://github.com/edenhill/librdkafka/pull/1645#issuecomment-359685735,"Create a new rd_kafka_conf_set_free_payload_cb API to set the callback to free memory pointer passed to rd_kafka_produce API when RD_KAFKA_MSG_F_FREE flag is set. This is a common scenario when a custom application memory management is used. For performance-critical applications passing RD_KAFKA_MSG_F_FREE will eliminate extra malloc and memcpy calls to allocate and copy message payload memory. Librdkafka currently calls free(payload) to free memory in rd_kafka_msg_destroy when RD_KAFKA_MSG_F_FREE is set. This will result in undefined behavior when a custom memory allocation is used in the application and the payload pointer memory is not allocated withcalloc,malloc, orrealloc. To enable this scenario following code changes are proposed:

Create a new  rd_kafka_conf_set_free_payload_cb to set a free payload callback intended to call  custom memory deallocation application function.
Create a new RD_KAFKA_OP_PAYLOAD_FREE operation.
In rd_kafka_msg_destroy enqueue  RD_KAFKA_OP_PAYLOAD_FREE operation to the tail of the 'rkq' queue with RD_KAFKA_PRIO_MEDIUM priority. This will result in d_kafka_poll_cb call.
In rd_kafka_poll_cb free the payload by calling the application callback ( rkrk_conf.free_payload_cb).

Only C code is currently implemented. C++ changes are proposed.

rdkafka_op.h
rdkafka_msg.c
rdkafka_event.c
rdkafka_conf.h
rdkafka_conf.c
rdkafka.h
rdkafka.c","Hi, thanks for the tip. When neither FREE nor COPY is passed the payload memory is not allocated nor freed indeed, and it is possible to free the payload in the delivery report callback. When compression is used, rd_kafka_msgset_writer_compress_snappy allocates memory in the code below. Is original payload memory used to copy over compressed message over, or it can be freed after message is compressed? In that case there is no need to hold on the original payload, as it may take a while to deliver the message.
    /* Calculate maximum compressed size and
     * allocate an output buffer accordingly. */
    ciov->iov_len = rd_kafka_snappy_max_compressed_length(len);
    ciov->iov_base = rd_malloc(ciov->iov_len);",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,1645,2018-01-22T04:48:05Z,2018-03-08T13:40:15Z,2018-03-08T13:40:15Z,CLOSED,False,44,2,7,https://github.com/4alexey,Create a new rd_kafka_conf_set_free_payload_cb API to set the callbac,1,[],https://github.com/edenhill/librdkafka/pull/1645,https://github.com/edenhill,6,https://github.com/edenhill/librdkafka/pull/1645#issuecomment-359922942,"Create a new rd_kafka_conf_set_free_payload_cb API to set the callback to free memory pointer passed to rd_kafka_produce API when RD_KAFKA_MSG_F_FREE flag is set. This is a common scenario when a custom application memory management is used. For performance-critical applications passing RD_KAFKA_MSG_F_FREE will eliminate extra malloc and memcpy calls to allocate and copy message payload memory. Librdkafka currently calls free(payload) to free memory in rd_kafka_msg_destroy when RD_KAFKA_MSG_F_FREE is set. This will result in undefined behavior when a custom memory allocation is used in the application and the payload pointer memory is not allocated withcalloc,malloc, orrealloc. To enable this scenario following code changes are proposed:

Create a new  rd_kafka_conf_set_free_payload_cb to set a free payload callback intended to call  custom memory deallocation application function.
Create a new RD_KAFKA_OP_PAYLOAD_FREE operation.
In rd_kafka_msg_destroy enqueue  RD_KAFKA_OP_PAYLOAD_FREE operation to the tail of the 'rkq' queue with RD_KAFKA_PRIO_MEDIUM priority. This will result in d_kafka_poll_cb call.
In rd_kafka_poll_cb free the payload by calling the application callback ( rkrk_conf.free_payload_cb).

Only C code is currently implemented. C++ changes are proposed.

rdkafka_op.h
rdkafka_msg.c
rdkafka_event.c
rdkafka_conf.h
rdkafka_conf.c
rdkafka.h
rdkafka.c","The payload memory is never modified, so when compression is enabled a new internal buffer is allocated.",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,1645,2018-01-22T04:48:05Z,2018-03-08T13:40:15Z,2018-03-08T13:40:15Z,CLOSED,False,44,2,7,https://github.com/4alexey,Create a new rd_kafka_conf_set_free_payload_cb API to set the callbac,1,[],https://github.com/edenhill/librdkafka/pull/1645,https://github.com/4alexey,7,https://github.com/edenhill/librdkafka/pull/1645#issuecomment-359961451,"Create a new rd_kafka_conf_set_free_payload_cb API to set the callback to free memory pointer passed to rd_kafka_produce API when RD_KAFKA_MSG_F_FREE flag is set. This is a common scenario when a custom application memory management is used. For performance-critical applications passing RD_KAFKA_MSG_F_FREE will eliminate extra malloc and memcpy calls to allocate and copy message payload memory. Librdkafka currently calls free(payload) to free memory in rd_kafka_msg_destroy when RD_KAFKA_MSG_F_FREE is set. This will result in undefined behavior when a custom memory allocation is used in the application and the payload pointer memory is not allocated withcalloc,malloc, orrealloc. To enable this scenario following code changes are proposed:

Create a new  rd_kafka_conf_set_free_payload_cb to set a free payload callback intended to call  custom memory deallocation application function.
Create a new RD_KAFKA_OP_PAYLOAD_FREE operation.
In rd_kafka_msg_destroy enqueue  RD_KAFKA_OP_PAYLOAD_FREE operation to the tail of the 'rkq' queue with RD_KAFKA_PRIO_MEDIUM priority. This will result in d_kafka_poll_cb call.
In rd_kafka_poll_cb free the payload by calling the application callback ( rkrk_conf.free_payload_cb).

Only C code is currently implemented. C++ changes are proposed.

rdkafka_op.h
rdkafka_msg.c
rdkafka_event.c
rdkafka_conf.h
rdkafka_conf.c
rdkafka.h
rdkafka.c","In that case the free_payload_cb is useful, as it makes sense to free payload memory as soon as possible. I'll look at how to implement this as we need to improve the performance and memory utilization for my current project work.
I'd propose one more feature: a callback to allocate memory using a custom memory allocator. I'd use it in the above rd_malloc call when memory is allocated for compressed payload buffer.",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,1645,2018-01-22T04:48:05Z,2018-03-08T13:40:15Z,2018-03-08T13:40:15Z,CLOSED,False,44,2,7,https://github.com/4alexey,Create a new rd_kafka_conf_set_free_payload_cb API to set the callbac,1,[],https://github.com/edenhill/librdkafka/pull/1645,https://github.com/edenhill,8,https://github.com/edenhill/librdkafka/pull/1645#issuecomment-359964161,"Create a new rd_kafka_conf_set_free_payload_cb API to set the callback to free memory pointer passed to rd_kafka_produce API when RD_KAFKA_MSG_F_FREE flag is set. This is a common scenario when a custom application memory management is used. For performance-critical applications passing RD_KAFKA_MSG_F_FREE will eliminate extra malloc and memcpy calls to allocate and copy message payload memory. Librdkafka currently calls free(payload) to free memory in rd_kafka_msg_destroy when RD_KAFKA_MSG_F_FREE is set. This will result in undefined behavior when a custom memory allocation is used in the application and the payload pointer memory is not allocated withcalloc,malloc, orrealloc. To enable this scenario following code changes are proposed:

Create a new  rd_kafka_conf_set_free_payload_cb to set a free payload callback intended to call  custom memory deallocation application function.
Create a new RD_KAFKA_OP_PAYLOAD_FREE operation.
In rd_kafka_msg_destroy enqueue  RD_KAFKA_OP_PAYLOAD_FREE operation to the tail of the 'rkq' queue with RD_KAFKA_PRIO_MEDIUM priority. This will result in d_kafka_poll_cb call.
In rd_kafka_poll_cb free the payload by calling the application callback ( rkrk_conf.free_payload_cb).

Only C code is currently implemented. C++ changes are proposed.

rdkafka_op.h
rdkafka_msg.c
rdkafka_event.c
rdkafka_conf.h
rdkafka_conf.c
rdkafka.h
rdkafka.c","We do appreciate your effort but I don't think a free_payload_cb is really warranted since in the typical case it will differ in allocation lifetime by approximately the round-trip-time of the broker (e.g., a couple of milliseconds).
As for custom allocator, there is a previous issue #1310 to track that feature request.",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,1645,2018-01-22T04:48:05Z,2018-03-08T13:40:15Z,2018-03-08T13:40:15Z,CLOSED,False,44,2,7,https://github.com/4alexey,Create a new rd_kafka_conf_set_free_payload_cb API to set the callbac,1,[],https://github.com/edenhill/librdkafka/pull/1645,https://github.com/4alexey,9,https://github.com/edenhill/librdkafka/pull/1645#issuecomment-360028098,"Create a new rd_kafka_conf_set_free_payload_cb API to set the callback to free memory pointer passed to rd_kafka_produce API when RD_KAFKA_MSG_F_FREE flag is set. This is a common scenario when a custom application memory management is used. For performance-critical applications passing RD_KAFKA_MSG_F_FREE will eliminate extra malloc and memcpy calls to allocate and copy message payload memory. Librdkafka currently calls free(payload) to free memory in rd_kafka_msg_destroy when RD_KAFKA_MSG_F_FREE is set. This will result in undefined behavior when a custom memory allocation is used in the application and the payload pointer memory is not allocated withcalloc,malloc, orrealloc. To enable this scenario following code changes are proposed:

Create a new  rd_kafka_conf_set_free_payload_cb to set a free payload callback intended to call  custom memory deallocation application function.
Create a new RD_KAFKA_OP_PAYLOAD_FREE operation.
In rd_kafka_msg_destroy enqueue  RD_KAFKA_OP_PAYLOAD_FREE operation to the tail of the 'rkq' queue with RD_KAFKA_PRIO_MEDIUM priority. This will result in d_kafka_poll_cb call.
In rd_kafka_poll_cb free the payload by calling the application callback ( rkrk_conf.free_payload_cb).

Only C code is currently implemented. C++ changes are proposed.

rdkafka_op.h
rdkafka_msg.c
rdkafka_event.c
rdkafka_conf.h
rdkafka_conf.c
rdkafka.h
rdkafka.c","I appreciate your response. I'll benchmark the performance of our application using different implementations: (a) by freeing memory in the delivery report callback, (b) freeing memory  in the free_payload_callback after compression happens, and (c) by overriding rd_malloc(ciov->iov_len) for the compressed memory with our custom memory allocator. Possibly the perf gain, if any, doesn't worth the effort.",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,1645,2018-01-22T04:48:05Z,2018-03-08T13:40:15Z,2018-03-08T13:40:15Z,CLOSED,False,44,2,7,https://github.com/4alexey,Create a new rd_kafka_conf_set_free_payload_cb API to set the callbac,1,[],https://github.com/edenhill/librdkafka/pull/1645,https://github.com/4alexey,10,https://github.com/edenhill/librdkafka/pull/1645#issuecomment-362119647,"Create a new rd_kafka_conf_set_free_payload_cb API to set the callback to free memory pointer passed to rd_kafka_produce API when RD_KAFKA_MSG_F_FREE flag is set. This is a common scenario when a custom application memory management is used. For performance-critical applications passing RD_KAFKA_MSG_F_FREE will eliminate extra malloc and memcpy calls to allocate and copy message payload memory. Librdkafka currently calls free(payload) to free memory in rd_kafka_msg_destroy when RD_KAFKA_MSG_F_FREE is set. This will result in undefined behavior when a custom memory allocation is used in the application and the payload pointer memory is not allocated withcalloc,malloc, orrealloc. To enable this scenario following code changes are proposed:

Create a new  rd_kafka_conf_set_free_payload_cb to set a free payload callback intended to call  custom memory deallocation application function.
Create a new RD_KAFKA_OP_PAYLOAD_FREE operation.
In rd_kafka_msg_destroy enqueue  RD_KAFKA_OP_PAYLOAD_FREE operation to the tail of the 'rkq' queue with RD_KAFKA_PRIO_MEDIUM priority. This will result in d_kafka_poll_cb call.
In rd_kafka_poll_cb free the payload by calling the application callback ( rkrk_conf.free_payload_cb).

Only C code is currently implemented. C++ changes are proposed.

rdkafka_op.h
rdkafka_msg.c
rdkafka_event.c
rdkafka_conf.h
rdkafka_conf.c
rdkafka.h
rdkafka.c","When passing 0 flag value to rd_kafka_produce and freeing the memory in the delivery report callback, we occasionally see the library crashing. Attaching the core dump stack trace. Any idea why this is happening?
(gdb) where
#0  rd_crc32_update (rkbuf=0x7f2694804650, buf=,
len=, allow_crc_calc=1, free_cb=)
at rdcrc32.h:108
#1  rd_kafka_buf_push0 (rkbuf=0x7f2694804650, buf=,
len=, allow_crc_calc=1, free_cb=)
at rdkafka_buf.c:87
#2  0x00000000005ba3fe in rd_kafka_msgset_writer_write_msg_payload (
msetw=0x7f2675ff32a0, rkm=0x7f2688000be0, Offset=,
MsgAttributes=0 '\000', free_cb=0) at rdkafka_msgset_writer.c:440
#3  rd_kafka_msgset_writer_write_msg_v0_1 (msetw=0x7f2675ff32a0,
rkm=0x7f2688000be0, Offset=, MsgAttributes=0 '\000',
free_cb=0) at rdkafka_msgset_writer.c:502
#4  0x00000000005b9dae in rd_kafka_msgset_writer_write_msg (
msetw=0x7f2675ff32a0, rkm=0x7f2688000be0, Offset=,
MsgAttributes=, free_cb=)
at rdkafka_msgset_writer.c:635
#5  0x00000000005bad51 in rd_kafka_msgset_writer_write_msgq (
rkb=, rktp=0x7f26a0001a90,
MessageSetSizep=0x7f2675ff3398) at rdkafka_msgset_writer.c:706
#6  rd_kafka_msgset_create_ProduceRequest (rkb=,
rktp=0x7f26a0001a90, MessageSetSizep=0x7f2675ff3398)
at rdkafka_msgset_writer.c:1158
#7  0x000000000059391a in rd_kafka_ProduceRequest (rkb=0x7f2688002b70,",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,1645,2018-01-22T04:48:05Z,2018-03-08T13:40:15Z,2018-03-08T13:40:15Z,CLOSED,False,44,2,7,https://github.com/4alexey,Create a new rd_kafka_conf_set_free_payload_cb API to set the callbac,1,[],https://github.com/edenhill/librdkafka/pull/1645,https://github.com/edenhill,11,https://github.com/edenhill/librdkafka/pull/1645#issuecomment-362174588,"Create a new rd_kafka_conf_set_free_payload_cb API to set the callback to free memory pointer passed to rd_kafka_produce API when RD_KAFKA_MSG_F_FREE flag is set. This is a common scenario when a custom application memory management is used. For performance-critical applications passing RD_KAFKA_MSG_F_FREE will eliminate extra malloc and memcpy calls to allocate and copy message payload memory. Librdkafka currently calls free(payload) to free memory in rd_kafka_msg_destroy when RD_KAFKA_MSG_F_FREE is set. This will result in undefined behavior when a custom memory allocation is used in the application and the payload pointer memory is not allocated withcalloc,malloc, orrealloc. To enable this scenario following code changes are proposed:

Create a new  rd_kafka_conf_set_free_payload_cb to set a free payload callback intended to call  custom memory deallocation application function.
Create a new RD_KAFKA_OP_PAYLOAD_FREE operation.
In rd_kafka_msg_destroy enqueue  RD_KAFKA_OP_PAYLOAD_FREE operation to the tail of the 'rkq' queue with RD_KAFKA_PRIO_MEDIUM priority. This will result in d_kafka_poll_cb call.
In rd_kafka_poll_cb free the payload by calling the application callback ( rkrk_conf.free_payload_cb).

Only C code is currently implemented. C++ changes are proposed.

rdkafka_op.h
rdkafka_msg.c
rdkafka_event.c
rdkafka_conf.h
rdkafka_conf.c
rdkafka.h
rdkafka.c","Try to reproduce with valgrind or asan, it will tell you what the problem is",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,1645,2018-01-22T04:48:05Z,2018-03-08T13:40:15Z,2018-03-08T13:40:15Z,CLOSED,False,44,2,7,https://github.com/4alexey,Create a new rd_kafka_conf_set_free_payload_cb API to set the callbac,1,[],https://github.com/edenhill/librdkafka/pull/1645,https://github.com/4alexey,12,https://github.com/edenhill/librdkafka/pull/1645#issuecomment-363585896,"Create a new rd_kafka_conf_set_free_payload_cb API to set the callback to free memory pointer passed to rd_kafka_produce API when RD_KAFKA_MSG_F_FREE flag is set. This is a common scenario when a custom application memory management is used. For performance-critical applications passing RD_KAFKA_MSG_F_FREE will eliminate extra malloc and memcpy calls to allocate and copy message payload memory. Librdkafka currently calls free(payload) to free memory in rd_kafka_msg_destroy when RD_KAFKA_MSG_F_FREE is set. This will result in undefined behavior when a custom memory allocation is used in the application and the payload pointer memory is not allocated withcalloc,malloc, orrealloc. To enable this scenario following code changes are proposed:

Create a new  rd_kafka_conf_set_free_payload_cb to set a free payload callback intended to call  custom memory deallocation application function.
Create a new RD_KAFKA_OP_PAYLOAD_FREE operation.
In rd_kafka_msg_destroy enqueue  RD_KAFKA_OP_PAYLOAD_FREE operation to the tail of the 'rkq' queue with RD_KAFKA_PRIO_MEDIUM priority. This will result in d_kafka_poll_cb call.
In rd_kafka_poll_cb free the payload by calling the application callback ( rkrk_conf.free_payload_cb).

Only C code is currently implemented. C++ changes are proposed.

rdkafka_op.h
rdkafka_msg.c
rdkafka_event.c
rdkafka_conf.h
rdkafka_conf.c
rdkafka.h
rdkafka.c","I could not reproduce the exact problem in the valgrind. However, by our application design, the threads where kafka handle and topic were created, get killed occasionally, and the new thread is created, instantiating another kafka handle and topic. When the thread is killed, the kafka handle/topic do was not cleaned up with rd_kafka_topic_destroy and rd_kafka_destroy. Could this cause the crash represented above i.e. not cleared instance of kafka hande/topic cause a crash in the newly created instances.",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,1645,2018-01-22T04:48:05Z,2018-03-08T13:40:15Z,2018-03-08T13:40:15Z,CLOSED,False,44,2,7,https://github.com/4alexey,Create a new rd_kafka_conf_set_free_payload_cb API to set the callbac,1,[],https://github.com/edenhill/librdkafka/pull/1645,https://github.com/edenhill,13,https://github.com/edenhill/librdkafka/pull/1645#issuecomment-363694493,"Create a new rd_kafka_conf_set_free_payload_cb API to set the callback to free memory pointer passed to rd_kafka_produce API when RD_KAFKA_MSG_F_FREE flag is set. This is a common scenario when a custom application memory management is used. For performance-critical applications passing RD_KAFKA_MSG_F_FREE will eliminate extra malloc and memcpy calls to allocate and copy message payload memory. Librdkafka currently calls free(payload) to free memory in rd_kafka_msg_destroy when RD_KAFKA_MSG_F_FREE is set. This will result in undefined behavior when a custom memory allocation is used in the application and the payload pointer memory is not allocated withcalloc,malloc, orrealloc. To enable this scenario following code changes are proposed:

Create a new  rd_kafka_conf_set_free_payload_cb to set a free payload callback intended to call  custom memory deallocation application function.
Create a new RD_KAFKA_OP_PAYLOAD_FREE operation.
In rd_kafka_msg_destroy enqueue  RD_KAFKA_OP_PAYLOAD_FREE operation to the tail of the 'rkq' queue with RD_KAFKA_PRIO_MEDIUM priority. This will result in d_kafka_poll_cb call.
In rd_kafka_poll_cb free the payload by calling the application callback ( rkrk_conf.free_payload_cb).

Only C code is currently implemented. C++ changes are proposed.

rdkafka_op.h
rdkafka_msg.c
rdkafka_event.c
rdkafka_conf.h
rdkafka_conf.c
rdkafka.h
rdkafka.c","Killing threads, leaking resources and leaving kafka client instances running in the background, is obviously neither recommended or supported.",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,1646,2018-01-23T08:16:43Z,2018-03-08T13:42:39Z,2018-03-08T13:42:39Z,CLOSED,False,1,1,1,https://github.com/solsson,Fixes doc for the acks property to refer to the correct broker property,1,[],https://github.com/edenhill/librdkafka/pull/1646,https://github.com/solsson,1,https://github.com/edenhill/librdkafka/pull/1646,https://kafka.apache.org/documentation/#brokerconfigs,https://kafka.apache.org/documentation/#brokerconfigs,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,1646,2018-01-23T08:16:43Z,2018-03-08T13:42:39Z,2018-03-08T13:42:39Z,CLOSED,False,1,1,1,https://github.com/solsson,Fixes doc for the acks property to refer to the correct broker property,1,[],https://github.com/edenhill/librdkafka/pull/1646,https://github.com/edenhill,2,https://github.com/edenhill/librdkafka/pull/1646#issuecomment-362518994,https://kafka.apache.org/documentation/#brokerconfigs,"CONFIGURATION.md is automatically generated based on rdkafka_conf.c, that's where you need to change it, rebuild (to regen CONFIGURATION.md), and file a PR containing changes to both those files.",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,1646,2018-01-23T08:16:43Z,2018-03-08T13:42:39Z,2018-03-08T13:42:39Z,CLOSED,False,1,1,1,https://github.com/solsson,Fixes doc for the acks property to refer to the correct broker property,1,[],https://github.com/edenhill/librdkafka/pull/1646,https://github.com/edenhill,3,https://github.com/edenhill/librdkafka/pull/1646#issuecomment-371489733,https://kafka.apache.org/documentation/#brokerconfigs,fixing this in separate commit. Thanks!,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,1654,2018-01-29T08:41:56Z,2018-02-02T10:59:37Z,2020-05-08T08:58:09Z,MERGED,True,759,440,28,https://github.com/edenhill,Nonatomic msgq,17,[],https://github.com/edenhill/librdkafka/pull/1654,https://github.com/edenhill,1,https://github.com/edenhill/librdkafka/pull/1654,,,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,1659,2018-01-30T13:02:43Z,2019-05-07T07:32:14Z,2019-06-12T18:33:09Z,CLOSED,False,185,57,10,https://github.com/gduranceau,Connection on demand to brokers,4,[],https://github.com/edenhill/librdkafka/pull/1659,https://github.com/gduranceau,1,https://github.com/edenhill/librdkafka/pull/1659,"Define a new global property connect.on.demand. When set, connections to
brokers are not established immediately, but delayed until they are
about to be used. Another property inactivity.timeout.ms controls the
closure of connections to brokers, after they have been inactive for a
while.
This partly addresses:
#825","Define a new global property connect.on.demand. When set, connections to
brokers are not established immediately, but delayed until they are
about to be used. Another property inactivity.timeout.ms controls the
closure of connections to brokers, after they have been inactive for a
while.
This partly addresses:
#825",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,1659,2018-01-30T13:02:43Z,2019-05-07T07:32:14Z,2019-06-12T18:33:09Z,CLOSED,False,185,57,10,https://github.com/gduranceau,Connection on demand to brokers,4,[],https://github.com/edenhill/librdkafka/pull/1659,https://github.com/gduranceau,2,https://github.com/edenhill/librdkafka/pull/1659#issuecomment-361587989,"Define a new global property connect.on.demand. When set, connections to
brokers are not established immediately, but delayed until they are
about to be used. Another property inactivity.timeout.ms controls the
closure of connections to brokers, after they have been inactive for a
while.
This partly addresses:
#825","Hi @edenhill ,
I implemented connection on demand scheme in librdkafka, following the discussion on #825
The patch adds 2 new configuration settings to librdkafka:

connect.on.demand (can be 0 or 1) to activate connection on demand (default=0)
inactivity.timeout.ms (default = 1 min), after which inactive sockets to broker are closed.

When using connection on demand, a single connection to one of the bootstrap nodes is established initially when a metadata request has to be sent. This connection is closed after the inactivity timeout if this broker isnt used anymore afterwards. New connections to targeted leader brokers are opened when required (and closed in case of inactivity).
I didn't change the threading model. This patch only limits the number of connections opened to Kafka brokers.
Implementation details:

When connection on demand is set, brokers are initially put in STATE_UP state (rather than STATE_INIT), so as to be selected by calls to rd_kafka_broker_any (state=STATE_UP). Elements can immediately be queued for any broker.
The status for the connection (up/being-established or down) is identified by the rkb_transport attribute of the broker.
If rd_kafka_broker_serve identifies that the connection is down, but elements are queued, it triggers connection establishment (by switching to STATE_DOWN).
When any usable broker is needed (for metadata request for example), rd_kafka_broker_any_usable (and rd_kafka_cgrp_coord_query) gives priority to connected brokers. If there isn't any, it triggers connection for one of them, which should eventually become available. If this connection fails, another broker will eventually be selected.
Activity on connection is monitored in rd_kafka_transport_io_serve, when polling events are retrieved. Inactivity is checked in the broker producer_serve and consumer_server loops.

I validated the patch in a number of producer/consumer scenarios (mostly with kafkacat), but of course, it's possible that I missed things. Any comment is welcome.
Cheers  -  Guillaume",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,1659,2018-01-30T13:02:43Z,2019-05-07T07:32:14Z,2019-06-12T18:33:09Z,CLOSED,False,185,57,10,https://github.com/gduranceau,Connection on demand to brokers,4,[],https://github.com/edenhill/librdkafka/pull/1659,https://github.com/edenhill,3,https://github.com/edenhill/librdkafka/pull/1659#issuecomment-361702134,"Define a new global property connect.on.demand. When set, connections to
brokers are not established immediately, but delayed until they are
about to be used. Another property inactivity.timeout.ms controls the
closure of connections to brokers, after they have been inactive for a
while.
This partly addresses:
#825","This is great, will review in a couple of days!
The long term plan is to make thread creation on-demand, and maybe worker-thread-pooled, but this is a great medium term solution for what is essentially the real problems - the number of connections. The number of threads is in most cases an aesthetic.
Do you think we could rely on the broker's idle connection reaper instead of our own? This would make it this even simpler:

connect to a broker when we need to
don't automatically reconnect unless we need to",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,1659,2018-01-30T13:02:43Z,2019-05-07T07:32:14Z,2019-06-12T18:33:09Z,CLOSED,False,185,57,10,https://github.com/gduranceau,Connection on demand to brokers,4,[],https://github.com/edenhill/librdkafka/pull/1659,https://github.com/gduranceau,4,https://github.com/edenhill/librdkafka/pull/1659#issuecomment-361999545,"Define a new global property connect.on.demand. When set, connections to
brokers are not established immediately, but delayed until they are
about to be used. Another property inactivity.timeout.ms controls the
closure of connections to brokers, after they have been inactive for a
while.
This partly addresses:
#825","Hi Magnus,

The long term plan is to make thread creation on-demand, and maybe worker-thread-pooled, but this is a great medium term solution for what is essentially the real problems - the number of connections. The number of threads is in most cases an aesthetic.

I fully agree. I initially thought about implementing thread creation on demand when I started this, but I quickly gave up as it looked like impact was much bigger.

Do you think we could rely on the broker's idle connection reaper instead of our own? This would make it this even simpler

I didn't think at all about relying on the broker closing idle connections. This is a good point.
I removed today the inactivity.timeout.ms property that I added, and also don't reconnect anymore to the broker in rd_kafka_broker_fail in case connection on demand is activated (and previous state was UP).
I'm still not entirely satisfied with this whole change though. It works well for producers, but for some reason, consumers still open at least 2 bootstrap connections (which are closed after a while if idle). I'll try to understand this tomorrow.
There's  one other annoying thing when consuming in a consumer group. Leader brokers from revoked partitions never get disconnected. It looks like the broker never consider those connections as idle, although librdkafka logs don't seem to show any sort of activity on those connections. This is something that I will look up as well.
Cheers  -  Guillaume",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,1659,2018-01-30T13:02:43Z,2019-05-07T07:32:14Z,2019-06-12T18:33:09Z,CLOSED,False,185,57,10,https://github.com/gduranceau,Connection on demand to brokers,4,[],https://github.com/edenhill/librdkafka/pull/1659,https://github.com/gduranceau,5,https://github.com/edenhill/librdkafka/pull/1659#issuecomment-362320892,"Define a new global property connect.on.demand. When set, connections to
brokers are not established immediately, but delayed until they are
about to be used. Another property inactivity.timeout.ms controls the
closure of connections to brokers, after they have been inactive for a
while.
This partly addresses:
#825","Current state on this pull request after the last commit today:
Connection on demand is working pretty well, but connection handling is still not optimum when dealing with consumer groups.",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,1659,2018-01-30T13:02:43Z,2019-05-07T07:32:14Z,2019-06-12T18:33:09Z,CLOSED,False,185,57,10,https://github.com/gduranceau,Connection on demand to brokers,4,[],https://github.com/edenhill/librdkafka/pull/1659,https://github.com/gduranceau,6,https://github.com/edenhill/librdkafka/pull/1659#issuecomment-363125166,"Define a new global property connect.on.demand. When set, connections to
brokers are not established immediately, but delayed until they are
about to be used. Another property inactivity.timeout.ms controls the
closure of connections to brokers, after they have been inactive for a
while.
This partly addresses:
#825","I fixed connection on demand when used with consumers in a group. With the latest change, the connection to a random broker to query for the coordinator is sent immediately (no more idle time of 1 second), and the connection to the coordinator once known is also established immediately (in both cases, by making use of rd_kafka_broker_wakeup).
Note that I squashed all the commits in the pull request.
This is now in a state where I have no pending change. Feel free to comment and ask for questions or clarifications.
Cheers  -  Guillaume",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,1659,2018-01-30T13:02:43Z,2019-05-07T07:32:14Z,2019-06-12T18:33:09Z,CLOSED,False,185,57,10,https://github.com/gduranceau,Connection on demand to brokers,4,[],https://github.com/edenhill/librdkafka/pull/1659,https://github.com/edenhill,7,https://github.com/edenhill/librdkafka/pull/1659#issuecomment-363181312,"Define a new global property connect.on.demand. When set, connections to
brokers are not established immediately, but delayed until they are
about to be used. Another property inactivity.timeout.ms controls the
closure of connections to brokers, after they have been inactive for a
while.
This partly addresses:
#825","Great work, Guillaume!
I'll review this shortly.
Unfortunately this will not make the 0.11.4 release (end of february), since it is a drastic change that will require a lot of test time, but we'll schedule it for the release after thta.",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,1659,2018-01-30T13:02:43Z,2019-05-07T07:32:14Z,2019-06-12T18:33:09Z,CLOSED,False,185,57,10,https://github.com/gduranceau,Connection on demand to brokers,4,[],https://github.com/edenhill/librdkafka/pull/1659,https://github.com/edenhill,8,https://github.com/edenhill/librdkafka/pull/1659#issuecomment-377171568,"Define a new global property connect.on.demand. When set, connections to
brokers are not established immediately, but delayed until they are
about to be used. Another property inactivity.timeout.ms controls the
closure of connections to brokers, after they have been inactive for a
while.
This partly addresses:
#825","Sorry for the delay, been busy driving the v0.11.4 client releases but that is almost out of the way now.
After the current work on the Topic Admin API (KIP-4) is done I'll start focusing on Idempotent Producer, followed by EoS.
EoS is a big task that will require a lot of testing, so I'm thinking it will make sense to incorporate this PR in that development branch to make sure this PR gets thorough testing.
I'm suspecting there are some corner cases with regards to the current assumption of brokers always being available that needs to be vetted out, and the additional testing load of EoS will give this enough runtime to find any regressions from the connection changes.",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,1659,2018-01-30T13:02:43Z,2019-05-07T07:32:14Z,2019-06-12T18:33:09Z,CLOSED,False,185,57,10,https://github.com/gduranceau,Connection on demand to brokers,4,[],https://github.com/edenhill/librdkafka/pull/1659,https://github.com/gduranceau,9,https://github.com/edenhill/librdkafka/pull/1659#issuecomment-389168525,"Define a new global property connect.on.demand. When set, connections to
brokers are not established immediately, but delayed until they are
about to be used. Another property inactivity.timeout.ms controls the
closure of connections to brokers, after they have been inactive for a
while.
This partly addresses:
#825","For information. there is a problem / limitation that I identified with this implementation of connection on demand when producing data, which can lead to delivery failure for few messages when a broker comes down.
Connection to a leader broker is established only when the producer queues messages in the broker queue. Messages queued just before a leader broker shuts down and which had no time to be delivered trigger regular re-connection attempts (until delivery timeout), which all fail. Even though a new leader is elected in the meantime, those queued messages are not re-assigned to the new leader queue. As a consequence, they are reported to the client as undelivered after the delivery timeout.
It would be much better to be able to re-push those messages to the new elected leader queue, but I have not found out how to do that in a simple way yet.",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,1659,2018-01-30T13:02:43Z,2019-05-07T07:32:14Z,2019-06-12T18:33:09Z,CLOSED,False,185,57,10,https://github.com/gduranceau,Connection on demand to brokers,4,[],https://github.com/edenhill/librdkafka/pull/1659,https://github.com/billygout,10,https://github.com/edenhill/librdkafka/pull/1659#issuecomment-389210590,"Define a new global property connect.on.demand. When set, connections to
brokers are not established immediately, but delayed until they are
about to be used. Another property inactivity.timeout.ms controls the
closure of connections to brokers, after they have been inactive for a
while.
This partly addresses:
#825","@gduranceau

It would be much better to be able to re-push those messages to the new elected leader queue, but I have not found out how to do that in a simple way yet.

I wonder whether that is a result of the on-demand connections, or the same would have happened without on-demand connections? In other words, is this really a regression your patch is introducing?",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,1659,2018-01-30T13:02:43Z,2019-05-07T07:32:14Z,2019-06-12T18:33:09Z,CLOSED,False,185,57,10,https://github.com/gduranceau,Connection on demand to brokers,4,[],https://github.com/edenhill/librdkafka/pull/1659,https://github.com/edenhill,11,https://github.com/edenhill/librdkafka/pull/1659#issuecomment-389213480,"Define a new global property connect.on.demand. When set, connections to
brokers are not established immediately, but delayed until they are
about to be used. Another property inactivity.timeout.ms controls the
closure of connections to brokers, after they have been inactive for a
while.
This partly addresses:
#825","Metadata is being regularily fetched to find the new leader for partitions which lack a valid leader. When a new leader is found the partitions are moved from the previous broker to the new one using the RD_KAFKA_OP_PARTITION_LEAVE op (of the previous broker) and .._PARTITION_JOIN (for the new broker).
The LEAVE causes any messages queued on the broker's xmitq to be put back on the partition queue:
https://github.com/edenhill/librdkafka/blob/master/src/rdkafka_broker.c#L2159
So in theory this should work even with your modifications, given that the cluster elects a new leader in a timely manner.",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,1659,2018-01-30T13:02:43Z,2019-05-07T07:32:14Z,2019-06-12T18:33:09Z,CLOSED,False,185,57,10,https://github.com/gduranceau,Connection on demand to brokers,4,[],https://github.com/edenhill/librdkafka/pull/1659,https://github.com/gduranceau,12,https://github.com/edenhill/librdkafka/pull/1659#issuecomment-389225554,"Define a new global property connect.on.demand. When set, connections to
brokers are not established immediately, but delayed until they are
about to be used. Another property inactivity.timeout.ms controls the
closure of connections to brokers, after they have been inactive for a
while.
This partly addresses:
#825","Maybe the problem is possible without my changes, but connection on demand makes it worse in any case.
When the connection is broken, without connection on demand, this immediately sets the broker state to DOWN, preventing any message to be queued to this broker. With connection on demand, the state is actually reset to UP, precisely so that messages can still be queued to this broker, and allow a re-connection when required (note that a connection can be reset even if the remote broker is up, when it has been idle for a while. In this case, it's important that the state is set to UP indeed).
When a broker fails with connection on demand, all messages queued while the new leader isn't elected (and this new leader is unknown to the client) are queued to the failed broker, and not delivered.

So in theory this should work even with your modifications, given that the cluster elects a new leader in a timely manner.

I have to look this up again (I've run those tests a couple of weeks ago), but with a steady flow of messages produced, some are systematically not delivered as they remain in the failed broker queue. When the delivery timeout finally expires, the re-connection attempts are stopped (this was the point of commit d098e94).",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,1659,2018-01-30T13:02:43Z,2019-05-07T07:32:14Z,2019-06-12T18:33:09Z,CLOSED,False,185,57,10,https://github.com/gduranceau,Connection on demand to brokers,4,[],https://github.com/edenhill/librdkafka/pull/1659,https://github.com/gduranceau,13,https://github.com/edenhill/librdkafka/pull/1659#issuecomment-390234820,"Define a new global property connect.on.demand. When set, connections to
brokers are not established immediately, but delayed until they are
about to be used. Another property inactivity.timeout.ms controls the
closure of connections to brokers, after they have been inactive for a
while.
This partly addresses:
#825","The LEAVE causes any messages queued on the broker's xmitq to be put back on the partition queue:
https://github.com/edenhill/librdkafka/blob/master/src/rdkafka_broker.c#L2159

Yes, this is ok for messages in the xmitq queue. But here, I end up with a MessageSet already generated and queued in the rkb_outbufs queue of the failed broker (this is actually the condition which allows to trigger a new connection). This one isn't moved on partition LEAVE. Ideally, it should be pushed in the rkb_outbufs queue (or maybe rkb_retrybufs) of the new leader (on partition JOIN).",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,1659,2018-01-30T13:02:43Z,2019-05-07T07:32:14Z,2019-06-12T18:33:09Z,CLOSED,False,185,57,10,https://github.com/gduranceau,Connection on demand to brokers,4,[],https://github.com/edenhill/librdkafka/pull/1659,https://github.com/edenhill,14,https://github.com/edenhill/librdkafka/pull/1659#issuecomment-390681464,"Define a new global property connect.on.demand. When set, connections to
brokers are not established immediately, but delayed until they are
about to be used. Another property inactivity.timeout.ms controls the
closure of connections to brokers, after they have been inactive for a
while.
This partly addresses:
#825","@gduranceau Could you try adding RD_KAFKAP_Produce to this switch?
https://github.com/edenhill/librdkafka/blob/master/src/rdkafka_buf.c#L266
I'm not sure what error code will be the most appropriate, but probably ERR__TRANSPORT.",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,1659,2018-01-30T13:02:43Z,2019-05-07T07:32:14Z,2019-06-12T18:33:09Z,CLOSED,False,185,57,10,https://github.com/gduranceau,Connection on demand to brokers,4,[],https://github.com/edenhill/librdkafka/pull/1659,https://github.com/gduranceau,15,https://github.com/edenhill/librdkafka/pull/1659#issuecomment-391244589,"Define a new global property connect.on.demand. When set, connections to
brokers are not established immediately, but delayed until they are
about to be used. Another property inactivity.timeout.ms controls the
closure of connections to brokers, after they have been inactive for a
while.
This partly addresses:
#825","@edenhill It looks like it's working with your suggestion. I've just ran a few quick tests. I'll do a proper validation soon, and update the pull request.",True,"{'THUMBS_UP': ['https://github.com/edenhill', 'https://github.com/joelpfaff']}"
edenhill/librdkafka,https://github.com/edenhill/librdkafka,1659,2018-01-30T13:02:43Z,2019-05-07T07:32:14Z,2019-06-12T18:33:09Z,CLOSED,False,185,57,10,https://github.com/gduranceau,Connection on demand to brokers,4,[],https://github.com/edenhill/librdkafka/pull/1659,https://github.com/billygout,16,https://github.com/edenhill/librdkafka/pull/1659#issuecomment-399171795,"Define a new global property connect.on.demand. When set, connections to
brokers are not established immediately, but delayed until they are
about to be used. Another property inactivity.timeout.ms controls the
closure of connections to brokers, after they have been inactive for a
while.
This partly addresses:
#825","Hi @gduranceau and @edenhill : I am still very interested in this feature, and would like to inquire what remains to be done to merge this PR? If no more changes are expected and this is being held up for lack of testing, I can offer to test this on a large-scale kafka deployment. Thanks!",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,1659,2018-01-30T13:02:43Z,2019-05-07T07:32:14Z,2019-06-12T18:33:09Z,CLOSED,False,185,57,10,https://github.com/gduranceau,Connection on demand to brokers,4,[],https://github.com/edenhill/librdkafka/pull/1659,https://github.com/gduranceau,17,https://github.com/edenhill/librdkafka/pull/1659#issuecomment-399339413,"Define a new global property connect.on.demand. When set, connections to
brokers are not established immediately, but delayed until they are
about to be used. Another property inactivity.timeout.ms controls the
closure of connections to brokers, after they have been inactive for a
while.
This partly addresses:
#825","Hi @billygout ,
Magnus and I had a discussion a few weeks ago about this. To make sure this is properly tested, this will be integrated in the branch to be created soon for Exactly-once Semantic developments. Integration will come when this branch will be merged into master (likely not before end of Q3 2018).
In the meantime, this patch will make it way to production applications in my company in the coming weeks/months (several dozen of thousands of processes, mostly producer oriented). I'll report any issue (or lack of issue) here.",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,1659,2018-01-30T13:02:43Z,2019-05-07T07:32:14Z,2019-06-12T18:33:09Z,CLOSED,False,185,57,10,https://github.com/gduranceau,Connection on demand to brokers,4,[],https://github.com/edenhill/librdkafka/pull/1659,https://github.com/billygout,18,https://github.com/edenhill/librdkafka/pull/1659#issuecomment-399347175,"Define a new global property connect.on.demand. When set, connections to
brokers are not established immediately, but delayed until they are
about to be used. Another property inactivity.timeout.ms controls the
closure of connections to brokers, after they have been inactive for a
while.
This partly addresses:
#825","@gduranceau Thank you for the detailed information! Q3 of 2018 is a bit later than I'd like to have this, but beggars can't be choosers :) I see there are merge conflicts, but do we have any reasonably up-to-date branch that this has been applied to that I can try this out on sooner, even if I'm willing to void my warranty? thanks!",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,1659,2018-01-30T13:02:43Z,2019-05-07T07:32:14Z,2019-06-12T18:33:09Z,CLOSED,False,185,57,10,https://github.com/gduranceau,Connection on demand to brokers,4,[],https://github.com/edenhill/librdkafka/pull/1659,https://github.com/gduranceau,19,https://github.com/edenhill/librdkafka/pull/1659#issuecomment-399368533,"Define a new global property connect.on.demand. When set, connections to
brokers are not established immediately, but delayed until they are
about to be used. Another property inactivity.timeout.ms controls the
closure of connections to brokers, after they have been inactive for a
while.
This partly addresses:
#825","@billygout
I rebased the patch on master.",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,1659,2018-01-30T13:02:43Z,2019-05-07T07:32:14Z,2019-06-12T18:33:09Z,CLOSED,False,185,57,10,https://github.com/gduranceau,Connection on demand to brokers,4,[],https://github.com/edenhill/librdkafka/pull/1659,https://github.com/billygout,20,https://github.com/edenhill/librdkafka/pull/1659#issuecomment-399460107,"Define a new global property connect.on.demand. When set, connections to
brokers are not established immediately, but delayed until they are
about to be used. Another property inactivity.timeout.ms controls the
closure of connections to brokers, after they have been inactive for a
while.
This partly addresses:
#825",Thanks so much @gduranceau !,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,1659,2018-01-30T13:02:43Z,2019-05-07T07:32:14Z,2019-06-12T18:33:09Z,CLOSED,False,185,57,10,https://github.com/gduranceau,Connection on demand to brokers,4,[],https://github.com/edenhill/librdkafka/pull/1659,https://github.com/billygout,21,https://github.com/edenhill/librdkafka/pull/1659#issuecomment-405666734,"Define a new global property connect.on.demand. When set, connections to
brokers are not established immediately, but delayed until they are
about to be used. Another property inactivity.timeout.ms controls the
closure of connections to brokers, after they have been inactive for a
while.
This partly addresses:
#825","@gduranceau @edenhill  We've been running on thousands of producers using the previously rebased branch you had, at commit ed23da6
So far, we've experienced no new problems that we've seen. Slight memory usage drop. No cpu usage changes. No reliability changes. Just major drop in connections. We also haven't noticed any regression in terms of latency which you might expect due to the need to connect before sending to a broker, but we are looking at latency on the order of seconds, so we probably wouldn't have noticed this unless it became eggregious.",True,"{'THUMBS_UP': ['https://github.com/rnpridgeon', 'https://github.com/gduranceau']}"
edenhill/librdkafka,https://github.com/edenhill/librdkafka,1659,2018-01-30T13:02:43Z,2019-05-07T07:32:14Z,2019-06-12T18:33:09Z,CLOSED,False,185,57,10,https://github.com/gduranceau,Connection on demand to brokers,4,[],https://github.com/edenhill/librdkafka/pull/1659,https://github.com/gduranceau,22,https://github.com/edenhill/librdkafka/pull/1659#issuecomment-405832870,"Define a new global property connect.on.demand. When set, connections to
brokers are not established immediately, but delayed until they are
about to be used. Another property inactivity.timeout.ms controls the
closure of connections to brokers, after they have been inactive for a
while.
This partly addresses:
#825","@billygout Thanks for sharing your (positive) experience with this patch. On my side, it has also been integrated lately in our producers (actually, a backport for branch 0.9), but it's not yet sufficiently widely deployed to really comment on it.
I'm trying to rebase the patch regularly on the master branch. When 0.11.5 will be out, I'll provide a tag in my repo for 0.11.5+this patch.",True,{'HEART': ['https://github.com/billygout']}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,1659,2018-01-30T13:02:43Z,2019-05-07T07:32:14Z,2019-06-12T18:33:09Z,CLOSED,False,185,57,10,https://github.com/gduranceau,Connection on demand to brokers,4,[],https://github.com/edenhill/librdkafka/pull/1659,https://github.com/billygout,23,https://github.com/edenhill/librdkafka/pull/1659#issuecomment-411616649,"Define a new global property connect.on.demand. When set, connections to
brokers are not established immediately, but delayed until they are
about to be used. Another property inactivity.timeout.ms controls the
closure of connections to brokers, after they have been inactive for a
while.
This partly addresses:
#825","@gduranceau can you please provide an update on the progress of your testing? I'm enjoying your code in my own fork for now, but would love to get back to the mainline :)",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,1659,2018-01-30T13:02:43Z,2019-05-07T07:32:14Z,2019-06-12T18:33:09Z,CLOSED,False,185,57,10,https://github.com/gduranceau,Connection on demand to brokers,4,[],https://github.com/edenhill/librdkafka/pull/1659,https://github.com/gduranceau,24,https://github.com/edenhill/librdkafka/pull/1659#issuecomment-415711261,"Define a new global property connect.on.demand. When set, connections to
brokers are not established immediately, but delayed until they are
about to be used. Another property inactivity.timeout.ms controls the
closure of connections to brokers, after they have been inactive for a
while.
This partly addresses:
#825","@billygout So far, everything is good. No problem reported (producer clients only).
For info, I created the branch v0.11.5_connect_on_demand in my repo with just this patch on top of v0.11.5 tag.",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,1659,2018-01-30T13:02:43Z,2019-05-07T07:32:14Z,2019-06-12T18:33:09Z,CLOSED,False,185,57,10,https://github.com/gduranceau,Connection on demand to brokers,4,[],https://github.com/edenhill/librdkafka/pull/1659,https://github.com/jonefeewang,25,https://github.com/edenhill/librdkafka/pull/1659#issuecomment-501380755,"Define a new global property connect.on.demand. When set, connections to
brokers are not established immediately, but delayed until they are
about to be used. Another property inactivity.timeout.ms controls the
closure of connections to brokers, after they have been inactive for a
while.
This partly addresses:
#825","hi guys, this feature is really nice. We have a large amount of clients, probably 10k-80k , but not all of them produce message all the time. On demand connection will really decrease many ""idle"" connection to kafka broker. Is this feature released ?
just noticed it is merged in v1.0.0.  forget this post.",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,1659,2018-01-30T13:02:43Z,2019-05-07T07:32:14Z,2019-06-12T18:33:09Z,CLOSED,False,185,57,10,https://github.com/gduranceau,Connection on demand to brokers,4,[],https://github.com/edenhill/librdkafka/pull/1659,https://github.com/billygout,26,https://github.com/edenhill/librdkafka/pull/1659#issuecomment-501400863,"Define a new global property connect.on.demand. When set, connections to
brokers are not established immediately, but delayed until they are
about to be used. Another property inactivity.timeout.ms controls the
closure of connections to brokers, after they have been inactive for a
while.
This partly addresses:
#825","Yes, it's on by default as of librdkafka 1.0.0

On Wed, Jun 12, 2019, 10:40 AM jonefee wang ***@***.***> wrote:
 hi guys, this feature is really nice. We have a large amount of clients,
 probably 10k-80k , but not all of them produce message all the time. On
 demand connection will really decrease many ""idle"" connection to kafka
 broker. Is this feature released ?

 
 You are receiving this because you were mentioned.
 Reply to this email directly, view it on GitHub
 <#1659?email_source=notifications&email_token=AANACEV2KDZRPFOIJJIRXETP2EYIXA5CNFSM4EOJFPTKYY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGODXRHNEY#issuecomment-501380755>,
 or mute the thread
 <https://github.com/notifications/unsubscribe-auth/AANACEXVAHUM35EUZCIOTYDP2EYIXANCNFSM4EOJFPTA>
 .",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,1667,2018-02-02T09:33:40Z,2018-02-02T09:42:39Z,2018-02-05T13:07:23Z,MERGED,True,4,2,2,https://github.com/gduranceau,Fix broker wake up,1,[],https://github.com/edenhill/librdkafka/pull/1667,https://github.com/gduranceau,1,https://github.com/edenhill/librdkafka/pull/1667,"A WAKEUP operation in a broker queue didn't trigger immediate wakeup (rd_kafka_q_pop didn't return on WAKEUP). This fix makes sure that the broker immediately stops waiting on its operation queue in case of wake-up, instead of waiting the timeout in rd_kafka_broker_ops_serve.
I noticed this problem while working on #1659.","A WAKEUP operation in a broker queue didn't trigger immediate wakeup (rd_kafka_q_pop didn't return on WAKEUP). This fix makes sure that the broker immediately stops waiting on its operation queue in case of wake-up, instead of waiting the timeout in rd_kafka_broker_ops_serve.
I noticed this problem while working on #1659.",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,1667,2018-02-02T09:33:40Z,2018-02-02T09:42:39Z,2018-02-05T13:07:23Z,MERGED,True,4,2,2,https://github.com/gduranceau,Fix broker wake up,1,[],https://github.com/edenhill/librdkafka/pull/1667,https://github.com/edenhill,2,https://github.com/edenhill/librdkafka/pull/1667#issuecomment-362534996,"A WAKEUP operation in a broker queue didn't trigger immediate wakeup (rd_kafka_q_pop didn't return on WAKEUP). This fix makes sure that the broker immediately stops waiting on its operation queue in case of wake-up, instead of waiting the timeout in rd_kafka_broker_ops_serve.
I noticed this problem while working on #1659.",Good stuff!,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,1667,2018-02-02T09:33:40Z,2018-02-02T09:42:39Z,2018-02-05T13:07:23Z,MERGED,True,4,2,2,https://github.com/gduranceau,Fix broker wake up,1,[],https://github.com/edenhill/librdkafka/pull/1667,https://github.com/edenhill,3,https://github.com/edenhill/librdkafka/pull/1667#issuecomment-362536624,"A WAKEUP operation in a broker queue didn't trigger immediate wakeup (rd_kafka_q_pop didn't return on WAKEUP). This fix makes sure that the broker immediately stops waiting on its operation queue in case of wake-up, instead of waiting the timeout in rd_kafka_broker_ops_serve.
I noticed this problem while working on #1659.",Thank you for this!,True,{'THUMBS_UP': ['https://github.com/gduranceau']}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,1675,2018-02-07T21:21:40Z,2018-02-07T21:45:46Z,2018-02-07T21:45:46Z,CLOSED,False,8,3,4,https://github.com/spasam,metadata: Expose broker rack information from metadata request.,1,[],https://github.com/edenhill/librdkafka/pull/1675,https://github.com/spasam,1,https://github.com/edenhill/librdkafka/pull/1675,"Currently rack information is read (if available) and discarded.
This change preserves that and makes it available to the caller.","Currently rack information is read (if available) and discarded.
This change preserves that and makes it available to the caller.",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,1688,2018-02-14T20:56:06Z,2018-03-05T19:34:51Z,2018-03-05T19:34:51Z,MERGED,True,208,98,58,https://github.com/tbsaunde,replace use of #pragma once with header guards,2,[],https://github.com/edenhill/librdkafka/pull/1688,https://github.com/tbsaunde,1,https://github.com/edenhill/librdkafka/pull/1688,"Being a pragma #pragma once is non standard, and in practice the Sun
Studio compiler does not support it.  We can fix that by just using
standard #ifndef / #define / #endif header guards.  Since the
rdkafka_subscription.h header is empty other than the license text it is
simply removed.","Being a pragma #pragma once is non standard, and in practice the Sun
Studio compiler does not support it.  We can fix that by just using
standard #ifndef / #define / #endif header guards.  Since the
rdkafka_subscription.h header is empty other than the license text it is
simply removed.",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,1688,2018-02-14T20:56:06Z,2018-03-05T19:34:51Z,2018-03-05T19:34:51Z,MERGED,True,208,98,58,https://github.com/tbsaunde,replace use of #pragma once with header guards,2,[],https://github.com/edenhill/librdkafka/pull/1688,https://github.com/edenhill,2,https://github.com/edenhill/librdkafka/pull/1688#issuecomment-370344923,"Being a pragma #pragma once is non standard, and in practice the Sun
Studio compiler does not support it.  We can fix that by just using
standard #ifndef / #define / #endif header guards.  Since the
rdkafka_subscription.h header is empty other than the license text it is
simply removed.","LGTM.
Can you fix the merge conflicts and then I can merge it",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,1692,2018-02-15T14:25:14Z,2018-02-18T08:24:29Z,2020-05-08T08:58:12Z,MERGED,True,14,14,2,https://github.com/edenhill,Allow arbitrary lengthed (>255) SASL PLAIN user/pass (#1691),1,['bug'],https://github.com/edenhill/librdkafka/pull/1692,https://github.com/edenhill,1,https://github.com/edenhill/librdkafka/pull/1692,for #1691,for #1691,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,1692,2018-02-15T14:25:14Z,2018-02-18T08:24:29Z,2020-05-08T08:58:12Z,MERGED,True,14,14,2,https://github.com/edenhill,Allow arbitrary lengthed (>255) SASL PLAIN user/pass (#1691),1,['bug'],https://github.com/edenhill/librdkafka/pull/1692,https://github.com/mhowlett,2,https://github.com/edenhill/librdkafka/pull/1692#issuecomment-366352638,for #1691,LGTM.,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,1708,2018-02-27T14:27:28Z,2018-03-04T19:23:35Z,2018-03-04T19:23:35Z,MERGED,True,2,2,1,https://github.com/tbsaunde,stop using typeof in rdkafka_transport.c,1,['portability'],https://github.com/edenhill/librdkafka/pull/1708,https://github.com/tbsaunde,1,https://github.com/edenhill/librdkafka/pull/1708,"The typeof operator is non standard, and at least XLC doesn't support
it.  While the type of msghdr::msg_iovlen is a bit of a mess the largest
value we will ever want to store in it is IOV_MAX. IOV_MAX should be
much smaller than INT_MAX, and in both cases the array whose length we
are providing is on the stack, so it should easily fit in an int.  We
know that the length should be non negative, so it should be safe to
cast to int, and then allow C promotion rules to widen to the actual
type of msg_iovlen if larger.","The typeof operator is non standard, and at least XLC doesn't support
it.  While the type of msghdr::msg_iovlen is a bit of a mess the largest
value we will ever want to store in it is IOV_MAX. IOV_MAX should be
much smaller than INT_MAX, and in both cases the array whose length we
are providing is on the stack, so it should easily fit in an int.  We
know that the length should be non negative, so it should be safe to
cast to int, and then allow C promotion rules to widen to the actual
type of msg_iovlen if larger.",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,1708,2018-02-27T14:27:28Z,2018-03-04T19:23:35Z,2018-03-04T19:23:35Z,MERGED,True,2,2,1,https://github.com/tbsaunde,stop using typeof in rdkafka_transport.c,1,['portability'],https://github.com/edenhill/librdkafka/pull/1708,https://github.com/edenhill,2,https://github.com/edenhill/librdkafka/pull/1708#issuecomment-370255957,"The typeof operator is non standard, and at least XLC doesn't support
it.  While the type of msghdr::msg_iovlen is a bit of a mess the largest
value we will ever want to store in it is IOV_MAX. IOV_MAX should be
much smaller than INT_MAX, and in both cases the array whose length we
are providing is on the stack, so it should easily fit in an int.  We
know that the length should be non negative, so it should be safe to
cast to int, and then allow C promotion rules to widen to the actual
type of msg_iovlen if larger.","Looks good, thanks!",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,1711,2018-03-01T14:09:13Z,2018-03-01T14:10:09Z,2018-03-04T19:45:04Z,CLOSED,False,57,8,6,https://github.com/mattclarke,Win32 fix,3,[],https://github.com/edenhill/librdkafka/pull/1711,https://github.com/mattclarke,1,https://github.com/edenhill/librdkafka/pull/1711,,,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,1715,2018-03-07T13:37:14Z,2018-03-08T15:57:56Z,2019-02-18T07:51:29Z,MERGED,True,3,2,1,https://github.com/mattclarke,Small change to proprogate the WITHOUT_WIN32_CONFIG flag to the build,1,[],https://github.com/edenhill/librdkafka/pull/1715,https://github.com/mattclarke,1,https://github.com/edenhill/librdkafka/pull/1715,"I think this small change is needed to enable overriding of the settings in src/win32_config.h.
Perhaps @raulbocanegra could check this?","I think this small change is needed to enable overriding of the settings in src/win32_config.h.
Perhaps @raulbocanegra could check this?",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,1715,2018-03-07T13:37:14Z,2018-03-08T15:57:56Z,2019-02-18T07:51:29Z,MERGED,True,3,2,1,https://github.com/mattclarke,Small change to proprogate the WITHOUT_WIN32_CONFIG flag to the build,1,[],https://github.com/edenhill/librdkafka/pull/1715,https://github.com/raulbocanegra,2,https://github.com/edenhill/librdkafka/pull/1715#issuecomment-371228596,"I think this small change is needed to enable overriding of the settings in src/win32_config.h.
Perhaps @raulbocanegra could check this?",You are right. I should have added this on my last PR. Thanks! :-),True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,1715,2018-03-07T13:37:14Z,2018-03-08T15:57:56Z,2019-02-18T07:51:29Z,MERGED,True,3,2,1,https://github.com/mattclarke,Small change to proprogate the WITHOUT_WIN32_CONFIG flag to the build,1,[],https://github.com/edenhill/librdkafka/pull/1715,https://github.com/edenhill,3,https://github.com/edenhill/librdkafka/pull/1715#issuecomment-371488957,"I think this small change is needed to enable overriding of the settings in src/win32_config.h.
Perhaps @raulbocanegra could check this?",Are you guys happy with this?,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,1715,2018-03-07T13:37:14Z,2018-03-08T15:57:56Z,2019-02-18T07:51:29Z,MERGED,True,3,2,1,https://github.com/mattclarke,Small change to proprogate the WITHOUT_WIN32_CONFIG flag to the build,1,[],https://github.com/edenhill/librdkafka/pull/1715,https://github.com/raulbocanegra,4,https://github.com/edenhill/librdkafka/pull/1715#issuecomment-371507714,"I think this small change is needed to enable overriding of the settings in src/win32_config.h.
Perhaps @raulbocanegra could check this?",Yep,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,1715,2018-03-07T13:37:14Z,2018-03-08T15:57:56Z,2019-02-18T07:51:29Z,MERGED,True,3,2,1,https://github.com/mattclarke,Small change to proprogate the WITHOUT_WIN32_CONFIG flag to the build,1,[],https://github.com/edenhill/librdkafka/pull/1715,https://github.com/Ultraman95,5,https://github.com/edenhill/librdkafka/pull/1715#issuecomment-464593235,"I think this small change is needed to enable overriding of the settings in src/win32_config.h.
Perhaps @raulbocanegra could check this?",What does WITHOUT_WIN32_CONFIG do?Is WITHOUT_WIN32_CONFIG necessary?,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,1715,2018-03-07T13:37:14Z,2018-03-08T15:57:56Z,2019-02-18T07:51:29Z,MERGED,True,3,2,1,https://github.com/mattclarke,Small change to proprogate the WITHOUT_WIN32_CONFIG flag to the build,1,[],https://github.com/edenhill/librdkafka/pull/1715,https://github.com/mattclarke,6,https://github.com/edenhill/librdkafka/pull/1715#issuecomment-464622672,"I think this small change is needed to enable overriding of the settings in src/win32_config.h.
Perhaps @raulbocanegra could check this?","It allows one to override/replace the settings in win32_config.h without editing that file directly.
If you are happy with the defaults in win32_config.h then it can be ignored.",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,1718,2018-03-09T20:25:11Z,2018-06-25T13:43:10Z,2018-06-25T13:43:17Z,MERGED,True,52,42,1,https://github.com/agl,Avoid allocating BIOs and copying for base64 processing.,1,[],https://github.com/edenhill/librdkafka/pull/1718,https://github.com/agl,1,https://github.com/edenhill/librdkafka/pull/1718,"OpenSSL offers a more direct API for base64 encoding and decoding.
Previously the code created a couple of BIOs and used the streaming
interface, this results in unneccessary allocations.
Also, we've received a request to integrate this library with TensorFlow
and the base64 BIO interface is one of the things we're trying to trim
from our version of OpenSSL.
Note: if there's an easy alternative to using OpenSSL for base64
processing then I would recommand that. OpenSSL has some odd behaviours
around ignoring whitespace etc. While they don't immediately appear to
matter here, something simplier would be preferable.","OpenSSL offers a more direct API for base64 encoding and decoding.
Previously the code created a couple of BIOs and used the streaming
interface, this results in unneccessary allocations.
Also, we've received a request to integrate this library with TensorFlow
and the base64 BIO interface is one of the things we're trying to trim
from our version of OpenSSL.
Note: if there's an easy alternative to using OpenSSL for base64
processing then I would recommand that. OpenSSL has some odd behaviours
around ignoring whitespace etc. While they don't immediately appear to
matter here, something simplier would be preferable.",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,1718,2018-03-09T20:25:11Z,2018-06-25T13:43:10Z,2018-06-25T13:43:17Z,MERGED,True,52,42,1,https://github.com/agl,Avoid allocating BIOs and copying for base64 processing.,1,[],https://github.com/edenhill/librdkafka/pull/1718,https://github.com/agl,2,https://github.com/edenhill/librdkafka/pull/1718#issuecomment-371935746,"OpenSSL offers a more direct API for base64 encoding and decoding.
Previously the code created a couple of BIOs and used the streaming
interface, this results in unneccessary allocations.
Also, we've received a request to integrate this library with TensorFlow
and the base64 BIO interface is one of the things we're trying to trim
from our version of OpenSSL.
Note: if there's an easy alternative to using OpenSSL for base64
processing then I would recommand that. OpenSSL has some odd behaviours
around ignoring whitespace etc. While they don't immediately appear to
matter here, something simplier would be preferable.","Please note: I was unable to get the SASL tests to run (before or after this change):
#### Version 0.10.2.0, suite SASL PLAIN: STARTING
### /agl/trivup/build/lib.linux-x86_64-2.7/trivup/apps/KafkaBrokerApp/deploy.sh: Downloading 0.10.2.0 from http://apache.mirrors.spacedump.net/kafka/0.10.2.0/kafka_2.11-0.10.2.0.tgz

gzip: stdin: not in gzip format
tar: Child returned status 1
tar: Error is not recoverable: exiting now",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,1718,2018-03-09T20:25:11Z,2018-06-25T13:43:10Z,2018-06-25T13:43:17Z,MERGED,True,52,42,1,https://github.com/agl,Avoid allocating BIOs and copying for base64 processing.,1,[],https://github.com/edenhill/librdkafka/pull/1718,https://github.com/edenhill,3,https://github.com/edenhill/librdkafka/pull/1718#issuecomment-372935370,"OpenSSL offers a more direct API for base64 encoding and decoding.
Previously the code created a couple of BIOs and used the streaming
interface, this results in unneccessary allocations.
Also, we've received a request to integrate this library with TensorFlow
and the base64 BIO interface is one of the things we're trying to trim
from our version of OpenSSL.
Note: if there's an easy alternative to using OpenSSL for base64
processing then I would recommand that. OpenSSL has some odd behaviours
around ignoring whitespace etc. While they don't immediately appear to
matter here, something simplier would be preferable.","Thanks for this!
I'm definitely open to use something leaner than OpenSSL for base64 and I'll look into viable alternatives (I think I did when I implemented SCRAM and didn't really find something I liked, but I'll have another look).
As for tests, it seems that mirror has gone away, you can override the download URL by setting env KAFKA_URL=https://archive.apache.org/dist/kafka/0.10.2.0/kafka_2.12-0.10.2.0.tgz",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,1718,2018-03-09T20:25:11Z,2018-06-25T13:43:10Z,2018-06-25T13:43:17Z,MERGED,True,52,42,1,https://github.com/agl,Avoid allocating BIOs and copying for base64 processing.,1,[],https://github.com/edenhill/librdkafka/pull/1718,https://github.com/edenhill,4,https://github.com/edenhill/librdkafka/pull/1718#issuecomment-372936266,"OpenSSL offers a more direct API for base64 encoding and decoding.
Previously the code created a couple of BIOs and used the streaming
interface, this results in unneccessary allocations.
Also, we've received a request to integrate this library with TensorFlow
and the base64 BIO interface is one of the things we're trying to trim
from our version of OpenSSL.
Note: if there's an easy alternative to using OpenSSL for base64
processing then I would recommand that. OpenSSL has some odd behaviours
around ignoring whitespace etc. While they don't immediately appear to
matter here, something simplier would be preferable.","On a tangent:
depending on OpenSSL has proven problematic: the version on OSX is outdated and lacks devel files, the Windows solution with a 3rd party build is far from optimal, and relying on external dependencies is problematic for packaging the higher-level clients (confluent-kafka-{python,go,dotnet}).
What we would like to do, as an opt-in, is use an alternate TLS library that is portable and is easy to link statically - there are a number of OpenSSL forks and alternatives, but it is hard to tell which one to use.",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,1718,2018-03-09T20:25:11Z,2018-06-25T13:43:10Z,2018-06-25T13:43:17Z,MERGED,True,52,42,1,https://github.com/agl,Avoid allocating BIOs and copying for base64 processing.,1,[],https://github.com/edenhill/librdkafka/pull/1718,https://github.com/agl,5,https://github.com/edenhill/librdkafka/pull/1718#issuecomment-373549682,"OpenSSL offers a more direct API for base64 encoding and decoding.
Previously the code created a couple of BIOs and used the streaming
interface, this results in unneccessary allocations.
Also, we've received a request to integrate this library with TensorFlow
and the base64 BIO interface is one of the things we're trying to trim
from our version of OpenSSL.
Note: if there's an easy alternative to using OpenSSL for base64
processing then I would recommand that. OpenSSL has some odd behaviours
around ignoring whitespace etc. While they don't immediately appear to
matter here, something simplier would be preferable.","What we would like to do, as an opt-in, is use an alternate TLS library that is portable and is easy to link statically - there are a number of OpenSSL forks and alternatives, but it is hard to tell which one to use.

Could you just use OpenSSL? It can be built statically. The only issue arises with possible symbol collisions if something else tries to link OpenSSL too. Generally this is resolved by making your library a shared-object and using symbol visibility.
(In TensorFlow we'll use BoringSSL, but we generally only recommend BoringSSL to Google-related projects because we don't attempt to be generally compatible like OpenSSL does.)

As for tests, it seems that mirror has gone away, you can override the download URL by setting env KAFKA_URL=https://archive.apache.org/dist/kafka/0.10.2.0/kafka_2.12-0.10.2.0.tgz

Thank you for that. The tests appeared to run with that, although quite a number failed. However, the same tests failed both before and after the change. (Thus if you can run the tests successfully then I'd strongly recommend doing so before committing this.)",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,1718,2018-03-09T20:25:11Z,2018-06-25T13:43:10Z,2018-06-25T13:43:17Z,MERGED,True,52,42,1,https://github.com/agl,Avoid allocating BIOs and copying for base64 processing.,1,[],https://github.com/edenhill/librdkafka/pull/1718,https://github.com/edenhill,6,https://github.com/edenhill/librdkafka/pull/1718#issuecomment-399956273,"OpenSSL offers a more direct API for base64 encoding and decoding.
Previously the code created a couple of BIOs and used the streaming
interface, this results in unneccessary allocations.
Also, we've received a request to integrate this library with TensorFlow
and the base64 BIO interface is one of the things we're trying to trim
from our version of OpenSSL.
Note: if there's an easy alternative to using OpenSSL for base64
processing then I would recommand that. OpenSSL has some odd behaviours
around ignoring whitespace etc. While they don't immediately appear to
matter here, something simplier would be preferable.",Thank you for this!,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,1731,2018-03-20T10:28:25Z,2018-03-21T06:13:50Z,2018-03-21T06:13:54Z,MERGED,True,171,13,10,https://github.com/edenhill,"Add and use `fetch.max.bytes` to limit total Fetch response size (KIP-74, #1616)",1,['bug'],https://github.com/edenhill/librdkafka/pull/1731,https://github.com/edenhill,1,https://github.com/edenhill/librdkafka/pull/1731,"receive.message.max.bytes was used in FetchRequest.max_bytes to tell
the broker to limit the response size, but since the broker may overshoot
this value up to one message.max.bytes it resulted in an endless config
chase where receive.message.max.bytes was increased and the response
size would follow + a couple of extra bytes.
Using the new fetch.max.bytes property and making sure it is lower than
receive.message.max.bytes fixes the issue.
This also adds verification for these configuration values to
the rd_kafka_new() call.","receive.message.max.bytes was used in FetchRequest.max_bytes to tell
the broker to limit the response size, but since the broker may overshoot
this value up to one message.max.bytes it resulted in an endless config
chase where receive.message.max.bytes was increased and the response
size would follow + a couple of extra bytes.
Using the new fetch.max.bytes property and making sure it is lower than
receive.message.max.bytes fixes the issue.
This also adds verification for these configuration values to
the rd_kafka_new() call.",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,1731,2018-03-20T10:28:25Z,2018-03-21T06:13:50Z,2018-03-21T06:13:54Z,MERGED,True,171,13,10,https://github.com/edenhill,"Add and use `fetch.max.bytes` to limit total Fetch response size (KIP-74, #1616)",1,['bug'],https://github.com/edenhill/librdkafka/pull/1731,https://github.com/ijuma,2,https://github.com/edenhill/librdkafka/pull/1731#issuecomment-374615032,"receive.message.max.bytes was used in FetchRequest.max_bytes to tell
the broker to limit the response size, but since the broker may overshoot
this value up to one message.max.bytes it resulted in an endless config
chase where receive.message.max.bytes was increased and the response
size would follow + a couple of extra bytes.
Using the new fetch.max.bytes property and making sure it is lower than
receive.message.max.bytes fixes the issue.
This also adds verification for these configuration values to
the rd_kafka_new() call.","To be clear, the broker will only exceed max bytes if a single batch is larger than the limit (i.e. it would never return two batches where the second would cause it to go over). Given that, it seems like a config bug is needed to trigger this issue. I don't quite understand how this PR fixes that. It would be good to talk about a specific example of how it would break previously and now would work.",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,1731,2018-03-20T10:28:25Z,2018-03-21T06:13:50Z,2018-03-21T06:13:54Z,MERGED,True,171,13,10,https://github.com/edenhill,"Add and use `fetch.max.bytes` to limit total Fetch response size (KIP-74, #1616)",1,['bug'],https://github.com/edenhill/librdkafka/pull/1731,https://github.com/edenhill,3,https://github.com/edenhill/librdkafka/pull/1731#issuecomment-374653083,"receive.message.max.bytes was used in FetchRequest.max_bytes to tell
the broker to limit the response size, but since the broker may overshoot
this value up to one message.max.bytes it resulted in an endless config
chase where receive.message.max.bytes was increased and the response
size would follow + a couple of extra bytes.
Using the new fetch.max.bytes property and making sure it is lower than
receive.message.max.bytes fixes the issue.
This also adds verification for these configuration values to
the rd_kafka_new() call.","This is a typical error message from this case:
Receive failed: Invalid response size 1000000040 (0..1000000000): increase receive.message.max.bytes
receive.message.max.bytes is set to 1000000000, and reused as FetchRequest.max_bytes, the response has the overhead of FetchResponse framing and thus exceeds receive.message.max.bytes.
The problem typically surfaces on high-volume topics when fetching multiple partitions and there is no easy workaround.",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,1731,2018-03-20T10:28:25Z,2018-03-21T06:13:50Z,2018-03-21T06:13:54Z,MERGED,True,171,13,10,https://github.com/edenhill,"Add and use `fetch.max.bytes` to limit total Fetch response size (KIP-74, #1616)",1,['bug'],https://github.com/edenhill/librdkafka/pull/1731,https://github.com/mhowlett,4,https://github.com/edenhill/librdkafka/pull/1731#issuecomment-374697562,"receive.message.max.bytes was used in FetchRequest.max_bytes to tell
the broker to limit the response size, but since the broker may overshoot
this value up to one message.max.bytes it resulted in an endless config
chase where receive.message.max.bytes was increased and the response
size would follow + a couple of extra bytes.
Using the new fetch.max.bytes property and making sure it is lower than
receive.message.max.bytes fixes the issue.
This also adds verification for these configuration values to
the rd_kafka_new() call.","Wondering if there enough value in the extra configuration property to justify adding it? (making the library overly tunable acts to make it harder to use). Can you just use a const internal to librdkafka corresponding to the protocol overhead instead?
If you definitely believe the extra config property is justified, I think the doc string could do with a bit of re-wording to really highlight the difference with receive.message.max.bytes (and implicitly justify it's existence).",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,1731,2018-03-20T10:28:25Z,2018-03-21T06:13:50Z,2018-03-21T06:13:54Z,MERGED,True,171,13,10,https://github.com/edenhill,"Add and use `fetch.max.bytes` to limit total Fetch response size (KIP-74, #1616)",1,['bug'],https://github.com/edenhill/librdkafka/pull/1731,https://github.com/ijuma,5,https://github.com/edenhill/librdkafka/pull/1731#issuecomment-374715613,"receive.message.max.bytes was used in FetchRequest.max_bytes to tell
the broker to limit the response size, but since the broker may overshoot
this value up to one message.max.bytes it resulted in an endless config
chase where receive.message.max.bytes was increased and the response
size would follow + a couple of extra bytes.
Using the new fetch.max.bytes property and making sure it is lower than
receive.message.max.bytes fixes the issue.
This also adds verification for these configuration values to
the rd_kafka_new() call.","Yeah, if it's just the framing issue, we should consider whether we can make it work without an additional config.",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,1731,2018-03-20T10:28:25Z,2018-03-21T06:13:50Z,2018-03-21T06:13:54Z,MERGED,True,171,13,10,https://github.com/edenhill,"Add and use `fetch.max.bytes` to limit total Fetch response size (KIP-74, #1616)",1,['bug'],https://github.com/edenhill/librdkafka/pull/1731,https://github.com/edenhill,6,https://github.com/edenhill/librdkafka/pull/1731#issuecomment-374728478,"receive.message.max.bytes was used in FetchRequest.max_bytes to tell
the broker to limit the response size, but since the broker may overshoot
this value up to one message.max.bytes it resulted in an endless config
chase where receive.message.max.bytes was increased and the response
size would follow + a couple of extra bytes.
Using the new fetch.max.bytes property and making sure it is lower than
receive.message.max.bytes fixes the issue.
This also adds verification for these configuration values to
the rd_kafka_new() call.","Note that fetch.max.bytes is the new config property, receive.message.max.bytes on the other is an existing property that we can't remove - in hindsight there might be little need for it.
fetch.max.bytes does have some use for people with certain performance requirements or unusually large messages.",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,1731,2018-03-20T10:28:25Z,2018-03-21T06:13:50Z,2018-03-21T06:13:54Z,MERGED,True,171,13,10,https://github.com/edenhill,"Add and use `fetch.max.bytes` to limit total Fetch response size (KIP-74, #1616)",1,['bug'],https://github.com/edenhill/librdkafka/pull/1731,https://github.com/edenhill,7,https://github.com/edenhill/librdkafka/pull/1731#issuecomment-374728725,"receive.message.max.bytes was used in FetchRequest.max_bytes to tell
the broker to limit the response size, but since the broker may overshoot
this value up to one message.max.bytes it resulted in an endless config
chase where receive.message.max.bytes was increased and the response
size would follow + a couple of extra bytes.
Using the new fetch.max.bytes property and making sure it is lower than
receive.message.max.bytes fixes the issue.
This also adds verification for these configuration values to
the rd_kafka_new() call.","Will reconstruct PR as to external discussion:

automatically adjust receive.message.max.bytes to be at least fetch.max.bytes + framing
remove reference to receive.message.max.bytes in fetch.max.bytes config doc to avoid confusion.",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,1731,2018-03-20T10:28:25Z,2018-03-21T06:13:50Z,2018-03-21T06:13:54Z,MERGED,True,171,13,10,https://github.com/edenhill,"Add and use `fetch.max.bytes` to limit total Fetch response size (KIP-74, #1616)",1,['bug'],https://github.com/edenhill/librdkafka/pull/1731,https://github.com/mhowlett,8,https://github.com/edenhill/librdkafka/pull/1731#issuecomment-374759922,"receive.message.max.bytes was used in FetchRequest.max_bytes to tell
the broker to limit the response size, but since the broker may overshoot
this value up to one message.max.bytes it resulted in an endless config
chase where receive.message.max.bytes was increased and the response
size would follow + a couple of extra bytes.
Using the new fetch.max.bytes property and making sure it is lower than
receive.message.max.bytes fixes the issue.
This also adds verification for these configuration values to
the rd_kafka_new() call.",LGTM,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,1733,2018-03-21T15:40:08Z,2018-07-02T13:51:22Z,2018-07-02T13:58:42Z,MERGED,True,27,2,2,https://github.com/ankon,Log the value that couldn't be found for s2f configuration options,1,[],https://github.com/edenhill/librdkafka/pull/1733,https://github.com/ankon,1,https://github.com/edenhill/librdkafka/pull/1733,"This helped me to find an issue in our configuration: node-rdkafka is lagging behind master (obviously), and the consumer value for the debug configuration option wasn't supported in that version yet.","This helped me to find an issue in our configuration: node-rdkafka is lagging behind master (obviously), and the consumer value for the debug configuration option wasn't supported in that version yet.",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,1733,2018-03-21T15:40:08Z,2018-07-02T13:51:22Z,2018-07-02T13:58:42Z,MERGED,True,27,2,2,https://github.com/ankon,Log the value that couldn't be found for s2f configuration options,1,[],https://github.com/edenhill/librdkafka/pull/1733,https://github.com/ankon,2,https://github.com/edenhill/librdkafka/pull/1733#issuecomment-376573647,"This helped me to find an issue in our configuration: node-rdkafka is lagging behind master (obviously), and the consumer value for the debug configuration option wasn't supported in that version yet.","Hmm. AppVeyor's failure doesn't look like it is related to this PR:
Using credentials from config. UserName: nuget
Please provide credentials for: https://ci.appveyor.com/nuget/edenhill
UserName:",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,1733,2018-03-21T15:40:08Z,2018-07-02T13:51:22Z,2018-07-02T13:58:42Z,MERGED,True,27,2,2,https://github.com/ankon,Log the value that couldn't be found for s2f configuration options,1,[],https://github.com/edenhill/librdkafka/pull/1733,https://github.com/ankon,3,https://github.com/edenhill/librdkafka/pull/1733#issuecomment-376583851,"This helped me to find an issue in our configuration: node-rdkafka is lagging behind master (obviously), and the consumer value for the debug configuration option wasn't supported in that version yet.",Updated with the review comments & rebased onto current master.,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,1733,2018-03-21T15:40:08Z,2018-07-02T13:51:22Z,2018-07-02T13:58:42Z,MERGED,True,27,2,2,https://github.com/ankon,Log the value that couldn't be found for s2f configuration options,1,[],https://github.com/edenhill/librdkafka/pull/1733,https://github.com/ankon,4,https://github.com/edenhill/librdkafka/pull/1733#issuecomment-376601320,"This helped me to find an issue in our configuration: node-rdkafka is lagging behind master (obviously), and the consumer value for the debug configuration option wasn't supported in that version yet.","Done. I only modified the tabs->spaces for the touched lines in rdkafka_conf.c though, so now things are a bit more wonky around that area than before.",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,1733,2018-03-21T15:40:08Z,2018-07-02T13:51:22Z,2018-07-02T13:58:42Z,MERGED,True,27,2,2,https://github.com/ankon,Log the value that couldn't be found for s2f configuration options,1,[],https://github.com/edenhill/librdkafka/pull/1733,https://github.com/edenhill,5,https://github.com/edenhill/librdkafka/pull/1733#issuecomment-376607485,"This helped me to find an issue in our configuration: node-rdkafka is lagging behind master (obviously), and the consumer value for the debug configuration option wasn't supported in that version yet.",The wonkiness is okay at this point :),True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,1733,2018-03-21T15:40:08Z,2018-07-02T13:51:22Z,2018-07-02T13:58:42Z,MERGED,True,27,2,2,https://github.com/ankon,Log the value that couldn't be found for s2f configuration options,1,[],https://github.com/edenhill/librdkafka/pull/1733,https://github.com/edenhill,6,https://github.com/edenhill/librdkafka/pull/1733#issuecomment-376607637,"This helped me to find an issue in our configuration: node-rdkafka is lagging behind master (obviously), and the consumer value for the debug configuration option wasn't supported in that version yet.",This looks good now! Will wait with merging this until after the upcoming release.,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,1733,2018-03-21T15:40:08Z,2018-07-02T13:51:22Z,2018-07-02T13:58:42Z,MERGED,True,27,2,2,https://github.com/ankon,Log the value that couldn't be found for s2f configuration options,1,[],https://github.com/edenhill/librdkafka/pull/1733,https://github.com/edenhill,7,https://github.com/edenhill/librdkafka/pull/1733#issuecomment-401811287,"This helped me to find an issue in our configuration: node-rdkafka is lagging behind master (obviously), and the consumer value for the debug configuration option wasn't supported in that version yet.",Thank you!,True,{'HOORAY': ['https://github.com/ankon']}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,1734,2018-03-21T16:01:06Z,2018-03-22T08:26:52Z,2018-03-22T08:26:52Z,CLOSED,False,3,3,1,https://github.com/adyach,updated upload version,1,[],https://github.com/edenhill/librdkafka/pull/1734,https://github.com/adyach,1,https://github.com/edenhill/librdkafka/pull/1734,updated upload version of librdkafka to be consistent with what is in the homebrew,updated upload version of librdkafka to be consistent with what is in the homebrew,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,1740,2018-03-27T12:03:41Z,2019-07-12T12:48:06Z,2019-07-12T12:48:06Z,CLOSED,False,36,12,3,https://github.com/tbsaunde,more compatibility for AIX and Solaris,2,[],https://github.com/edenhill/librdkafka/pull/1740,https://github.com/tbsaunde,1,https://github.com/edenhill/librdkafka/pull/1740,,,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,1747,2018-03-29T16:39:05Z,2019-07-12T12:48:06Z,2019-07-12T12:48:06Z,CLOSED,False,243,218,1,https://github.com/tbsaunde,dynamically initialize rd_kafka_err_descs on solaris,1,[],https://github.com/edenhill/librdkafka/pull/1747,https://github.com/tbsaunde,1,https://github.com/edenhill/librdkafka/pull/1747,"There is a bug in the way Sun Studio generates relocations for pointers
into the middle of strings.  That means the name field of all the
rd_kafka_err_desc objects ends up being null instead of the correct
name.  Basically you can't have a compile time initialized pointer to
any part of a string literal other than the beginning.  Since the layout
of rd_kafka_err_desc is part of the ABI and we can't compile time
initialize it on Solaris we need to initialize it as part of the global
library initialization.","There is a bug in the way Sun Studio generates relocations for pointers
into the middle of strings.  That means the name field of all the
rd_kafka_err_desc objects ends up being null instead of the correct
name.  Basically you can't have a compile time initialized pointer to
any part of a string literal other than the beginning.  Since the layout
of rd_kafka_err_desc is part of the ABI and we can't compile time
initialize it on Solaris we need to initialize it as part of the global
library initialization.",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,1749,2018-04-02T19:23:32Z,2018-07-02T13:49:21Z,2018-07-02T13:49:34Z,MERGED,True,1,1,1,https://github.com/blackeyepanda,Fix a segment fault error in rdkafka_buf with zero-length string.,1,[],https://github.com/edenhill/librdkafka/pull/1749,https://github.com/blackeyepanda,1,https://github.com/edenhill/librdkafka/pull/1749,"(RD_KAFKAP_STR_LEN0(_klen) == 0) will treat zero-length as true as well,
and then assign NULL to kstr->str to cause segment fault in future access.
Fixed it by change it to (RD_KAFKAP_STR_IS_NULL(kstr))","(RD_KAFKAP_STR_LEN0(_klen) == 0) will treat zero-length as true as well,
and then assign NULL to kstr->str to cause segment fault in future access.
Fixed it by change it to (RD_KAFKAP_STR_IS_NULL(kstr))",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,1749,2018-04-02T19:23:32Z,2018-07-02T13:49:21Z,2018-07-02T13:49:34Z,MERGED,True,1,1,1,https://github.com/blackeyepanda,Fix a segment fault error in rdkafka_buf with zero-length string.,1,[],https://github.com/edenhill/librdkafka/pull/1749,https://github.com/edenhill,2,https://github.com/edenhill/librdkafka/pull/1749#issuecomment-395367814,"(RD_KAFKAP_STR_LEN0(_klen) == 0) will treat zero-length as true as well,
and then assign NULL to kstr->str to cause segment fault in future access.
Fixed it by change it to (RD_KAFKAP_STR_IS_NULL(kstr))",Where was the segmentation fault seen?,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,1749,2018-04-02T19:23:32Z,2018-07-02T13:49:21Z,2018-07-02T13:49:34Z,MERGED,True,1,1,1,https://github.com/blackeyepanda,Fix a segment fault error in rdkafka_buf with zero-length string.,1,[],https://github.com/edenhill/librdkafka/pull/1749,https://github.com/blackeyepanda,3,https://github.com/edenhill/librdkafka/pull/1749#issuecomment-395465815,"(RD_KAFKAP_STR_LEN0(_klen) == 0) will treat zero-length as true as well,
and then assign NULL to kstr->str to cause segment fault in future access.
Fixed it by change it to (RD_KAFKAP_STR_IS_NULL(kstr))","We have a coverage test to test edge cases by passing zero-length/empty string as host string or topic strings, etc. This is passed in previous versions like 0.9.2 and failed when we upgrade to 0.11.3. This simple change would preserve the old behavior.
Definitely user can check the value before they pass them into librdkafka, but empty string instead of null should be a valid value and should only case a warning but not segfault/crash the process.
Thanks",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,1749,2018-04-02T19:23:32Z,2018-07-02T13:49:21Z,2018-07-02T13:49:34Z,MERGED,True,1,1,1,https://github.com/blackeyepanda,Fix a segment fault error in rdkafka_buf with zero-length string.,1,[],https://github.com/edenhill/librdkafka/pull/1749,https://github.com/edenhill,4,https://github.com/edenhill/librdkafka/pull/1749#issuecomment-401810566,"(RD_KAFKAP_STR_LEN0(_klen) == 0) will treat zero-length as true as well,
and then assign NULL to kstr->str to cause segment fault in future access.
Fixed it by change it to (RD_KAFKAP_STR_IS_NULL(kstr))",Thank you for this!,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,1750,2018-04-02T21:09:02Z,2021-03-25T08:29:16Z,2021-03-25T08:29:17Z,CLOSED,False,91,93,8,https://github.com/accelerated,Added generic pollable interface,3,[],https://github.com/edenhill/librdkafka/pull/1750,https://github.com/accelerated,1,https://github.com/edenhill/librdkafka/pull/1750,"Description:

Refactored poll/yield into base class for generic programming
Removed unwanted virtual derivation in some classes as it makes static up-casting impossible (and has heavy runtime performance cost)
Removed superfluous destructors (much better to leave compiler defaulted ones in place until specific logic needs to be inserted in them)
Queue class now also has a yield() method. This is actually desirable as Queue::poll() calls rd_kafka_q_serve() which does check for the yield flag to be set.
Fixed a linker dependency on librt (CMake would not compile on Redhat 7)

Example use of Pollable interface:
using poll_vector = std::vector<std::shared_ptr<RdKafka::Pollable>>;
poll_vector polls;

for (size_t i = 0; i < 10; ++i)
{
    polls.emplace_back(Producer::Create(...));
    polls.emplace_back(Consumer::Create(...));
}

//generic poll loop
void pollLoop(const poll_vector& polls)
{
    while (1)
    {
        for (auto&& pollable : polls) pollable->poll();
        std::this_thread::sleep_for(std::chrono::milliseconds(100));
    }
}

std::thread t(pollLoop, polls); //run loop","Description:

Refactored poll/yield into base class for generic programming
Removed unwanted virtual derivation in some classes as it makes static up-casting impossible (and has heavy runtime performance cost)
Removed superfluous destructors (much better to leave compiler defaulted ones in place until specific logic needs to be inserted in them)
Queue class now also has a yield() method. This is actually desirable as Queue::poll() calls rd_kafka_q_serve() which does check for the yield flag to be set.
Fixed a linker dependency on librt (CMake would not compile on Redhat 7)

Example use of Pollable interface:
using poll_vector = std::vector<std::shared_ptr<RdKafka::Pollable>>;
poll_vector polls;

for (size_t i = 0; i < 10; ++i)
{
    polls.emplace_back(Producer::Create(...));
    polls.emplace_back(Consumer::Create(...));
}

//generic poll loop
void pollLoop(const poll_vector& polls)
{
    while (1)
    {
        for (auto&& pollable : polls) pollable->poll();
        std::this_thread::sleep_for(std::chrono::milliseconds(100));
    }
}

std::thread t(pollLoop, polls); //run loop",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,1750,2018-04-02T21:09:02Z,2021-03-25T08:29:16Z,2021-03-25T08:29:17Z,CLOSED,False,91,93,8,https://github.com/accelerated,Added generic pollable interface,3,[],https://github.com/edenhill/librdkafka/pull/1750,https://github.com/edenhill,2,https://github.com/edenhill/librdkafka/pull/1750#issuecomment-378352676,"Description:

Refactored poll/yield into base class for generic programming
Removed unwanted virtual derivation in some classes as it makes static up-casting impossible (and has heavy runtime performance cost)
Removed superfluous destructors (much better to leave compiler defaulted ones in place until specific logic needs to be inserted in them)
Queue class now also has a yield() method. This is actually desirable as Queue::poll() calls rd_kafka_q_serve() which does check for the yield flag to be set.
Fixed a linker dependency on librt (CMake would not compile on Redhat 7)

Example use of Pollable interface:
using poll_vector = std::vector<std::shared_ptr<RdKafka::Pollable>>;
poll_vector polls;

for (size_t i = 0; i < 10; ++i)
{
    polls.emplace_back(Producer::Create(...));
    polls.emplace_back(Consumer::Create(...));
}

//generic poll loop
void pollLoop(const poll_vector& polls)
{
    while (1)
    {
        for (auto&& pollable : polls) pollable->poll();
        std::this_thread::sleep_for(std::chrono::milliseconds(100));
    }
}

std::thread t(pollLoop, polls); //run loop","MSVC doesn't like this:
https://ci.appveyor.com/project/edenhill/librdkafka/build/0.11.4-R-pre451-sxbiosha/job/o6drorbape99s021#L263",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,1750,2018-04-02T21:09:02Z,2021-03-25T08:29:16Z,2021-03-25T08:29:17Z,CLOSED,False,91,93,8,https://github.com/accelerated,Added generic pollable interface,3,[],https://github.com/edenhill/librdkafka/pull/1750,https://github.com/accelerated,3,https://github.com/edenhill/librdkafka/pull/1750#issuecomment-378406951,"Description:

Refactored poll/yield into base class for generic programming
Removed unwanted virtual derivation in some classes as it makes static up-casting impossible (and has heavy runtime performance cost)
Removed superfluous destructors (much better to leave compiler defaulted ones in place until specific logic needs to be inserted in them)
Queue class now also has a yield() method. This is actually desirable as Queue::poll() calls rd_kafka_q_serve() which does check for the yield flag to be set.
Fixed a linker dependency on librt (CMake would not compile on Redhat 7)

Example use of Pollable interface:
using poll_vector = std::vector<std::shared_ptr<RdKafka::Pollable>>;
poll_vector polls;

for (size_t i = 0; i < 10; ++i)
{
    polls.emplace_back(Producer::Create(...));
    polls.emplace_back(Consumer::Create(...));
}

//generic poll loop
void pollLoop(const poll_vector& polls)
{
    while (1)
    {
        for (auto&& pollable : polls) pollable->poll();
        std::this_thread::sleep_for(std::chrono::milliseconds(100));
    }
}

std::thread t(pollLoop, polls); //run loop",Good reference topic on casting with diamond structure here,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,1750,2018-04-02T21:09:02Z,2021-03-25T08:29:16Z,2021-03-25T08:29:17Z,CLOSED,False,91,93,8,https://github.com/accelerated,Added generic pollable interface,3,[],https://github.com/edenhill/librdkafka/pull/1750,https://github.com/edenhill,4,https://github.com/edenhill/librdkafka/pull/1750#issuecomment-806462754,"Description:

Refactored poll/yield into base class for generic programming
Removed unwanted virtual derivation in some classes as it makes static up-casting impossible (and has heavy runtime performance cost)
Removed superfluous destructors (much better to leave compiler defaulted ones in place until specific logic needs to be inserted in them)
Queue class now also has a yield() method. This is actually desirable as Queue::poll() calls rd_kafka_q_serve() which does check for the yield flag to be set.
Fixed a linker dependency on librt (CMake would not compile on Redhat 7)

Example use of Pollable interface:
using poll_vector = std::vector<std::shared_ptr<RdKafka::Pollable>>;
poll_vector polls;

for (size_t i = 0; i < 10; ++i)
{
    polls.emplace_back(Producer::Create(...));
    polls.emplace_back(Consumer::Create(...));
}

//generic poll loop
void pollLoop(const poll_vector& polls)
{
    while (1)
    {
        for (auto&& pollable : polls) pollable->poll();
        std::this_thread::sleep_for(std::chrono::milliseconds(100));
    }
}

std::thread t(pollLoop, polls); //run loop","Outdated. Not sure if this is still of interest, if so, reopen it.",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,1756,2018-04-05T23:30:18Z,2018-04-09T21:14:02Z,2018-04-09T21:14:02Z,CLOSED,False,2003,1783,3,https://github.com/accelerated,Runtime instrospection of config options,3,[],https://github.com/edenhill/librdkafka/pull/1756,https://github.com/accelerated,1,https://github.com/edenhill/librdkafka/pull/1756,Description: Added two methods for obtaining runtime metadata info about configuration options. This will enable clients such as CppKafka to perform validation of user inputs (type and scope). No change in logic was made with the exception of exposing two private enums to the public header.,Description: Added two methods for obtaining runtime metadata info about configuration options. This will enable clients such as CppKafka to perform validation of user inputs (type and scope). No change in logic was made with the exception of exposing two private enums to the public header.,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,1756,2018-04-05T23:30:18Z,2018-04-09T21:14:02Z,2018-04-09T21:14:02Z,CLOSED,False,2003,1783,3,https://github.com/accelerated,Runtime instrospection of config options,3,[],https://github.com/edenhill/librdkafka/pull/1756,https://github.com/edenhill,2,https://github.com/edenhill/librdkafka/pull/1756#issuecomment-379269120,Description: Added two methods for obtaining runtime metadata info about configuration options. This will enable clients such as CppKafka to perform validation of user inputs (type and scope). No change in logic was made with the exception of exposing two private enums to the public header.,"This will enable clients such as CppKafka to perform validation of user inputs (type and scope)

Why is this validation needed in cppkafka, it is already performed by librdkafka?",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,1756,2018-04-05T23:30:18Z,2018-04-09T21:14:02Z,2018-04-09T21:14:02Z,CLOSED,False,2003,1783,3,https://github.com/accelerated,Runtime instrospection of config options,3,[],https://github.com/edenhill/librdkafka/pull/1756,https://github.com/accelerated,3,https://github.com/edenhill/librdkafka/pull/1756#issuecomment-379287944,Description: Added two methods for obtaining runtime metadata info about configuration options. This will enable clients such as CppKafka to perform validation of user inputs (type and scope). No change in logic was made with the exception of exposing two private enums to the public header.,"I am still working on a PR for CppKafka. Librdkafka only validates the type and the existence of the option, there is no validation to enforce setting a producer option on a producer, etc...Anyone can set just about any option on any type of object and there's no error as long as the config name is valid. Librdkafka only checks scope with RD_GLOBAL everywhere or RD_TOPIC for topic based options. But today I can set a consumer topic option on a producer with no errors.
Note that as an alternative you would have to expose overloaded APIs which will take the scope as a parameter and then you would perform some tighter control on the setters. However this would have required more changes to your codebase and I felt that getting config metadata at runtime can give more flexibility to users in the long run. Also it's nice to be able to introspect which options are currently valid.",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,1756,2018-04-05T23:30:18Z,2018-04-09T21:14:02Z,2018-04-09T21:14:02Z,CLOSED,False,2003,1783,3,https://github.com/accelerated,Runtime instrospection of config options,3,[],https://github.com/edenhill/librdkafka/pull/1756,https://github.com/accelerated,4,https://github.com/edenhill/librdkafka/pull/1756#issuecomment-379292144,Description: Added two methods for obtaining runtime metadata info about configuration options. This will enable clients such as CppKafka to perform validation of user inputs (type and scope). No change in logic was made with the exception of exposing two private enums to the public header.,"I have discussed this with @mfontanini here and he seemed to agree that it would be nice to allow better config option control for users.
In particular see excerpt:

my comment:
As for checking the Configuration::set(), one way of always being in sync is to validate against all settings at runtime and check for the global/consumer/producer flags. Right now it's not exported but this can be easily fixed with an extern declaration in the header file.
mfontanini's reply:
That is nice but if it's not currently being exported then we can't really use it. I do think this should be exported as it's useful in cases like this one.",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,1756,2018-04-05T23:30:18Z,2018-04-09T21:14:02Z,2018-04-09T21:14:02Z,CLOSED,False,2003,1783,3,https://github.com/accelerated,Runtime instrospection of config options,3,[],https://github.com/edenhill/librdkafka/pull/1756,https://github.com/mfontanini,5,https://github.com/edenhill/librdkafka/pull/1756#issuecomment-379301063,Description: Added two methods for obtaining runtime metadata info about configuration options. This will enable clients such as CppKafka to perform validation of user inputs (type and scope). No change in logic was made with the exception of exposing two private enums to the public header.,"For the record, I'm okay with how things are now. This would only be useful for cppkafka if Configuration was changed to use a type specific to Consumer and Producer (e.g. ConsumerConfiguration) but this is not great because:

It involves a breaking change unless you keep both the old and new classes but that's just confusing.
There are still cases where you want a generic config to be used as the base for both the consumer and producer. e.g. in my code I always have a base config:

Configuration base_config = {
    { ""metadata.broker.list"", brokers },
    { ""log.connection.close"", false },
    { ""client.id"", getHostname() },
    { ""api.version.request"", true }
};
That I use to bootstrap the actual consumer and producer configs I'll use in the application. With a consumer/producer specific config, this wouldn't be possible unless you introduced yet another ""common configuration"" type but I'm not sure about this.
Anyway I don't want to divert the discussion towards cppkafka. I'm not 100% sure of how useful this would be but I do agree that this should be exported by rdkafka, mostly because I don't think there's a reason not to export it.",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,1756,2018-04-05T23:30:18Z,2018-04-09T21:14:02Z,2018-04-09T21:14:02Z,CLOSED,False,2003,1783,3,https://github.com/accelerated,Runtime instrospection of config options,3,[],https://github.com/edenhill/librdkafka/pull/1756,https://github.com/accelerated,6,https://github.com/edenhill/librdkafka/pull/1756#issuecomment-379321229,Description: Added two methods for obtaining runtime metadata info about configuration options. This will enable clients such as CppKafka to perform validation of user inputs (type and scope). No change in logic was made with the exception of exposing two private enums to the public header.,@mfontanini code will be 100% backwards compatible both compile and runtime. I am adding some traits to make things easier to use.,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,1756,2018-04-05T23:30:18Z,2018-04-09T21:14:02Z,2018-04-09T21:14:02Z,CLOSED,False,2003,1783,3,https://github.com/accelerated,Runtime instrospection of config options,3,[],https://github.com/edenhill/librdkafka/pull/1756,https://github.com/accelerated,7,https://github.com/edenhill/librdkafka/pull/1756#issuecomment-379502259,Description: Added two methods for obtaining runtime metadata info about configuration options. This will enable clients such as CppKafka to perform validation of user inputs (type and scope). No change in logic was made with the exception of exposing two private enums to the public header.,Please see cppkafka PR here,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,1756,2018-04-05T23:30:18Z,2018-04-09T21:14:02Z,2018-04-09T21:14:02Z,CLOSED,False,2003,1783,3,https://github.com/accelerated,Runtime instrospection of config options,3,[],https://github.com/edenhill/librdkafka/pull/1756,https://github.com/edenhill,8,https://github.com/edenhill/librdkafka/pull/1756#issuecomment-379747035,Description: Added two methods for obtaining runtime metadata info about configuration options. This will enable clients such as CppKafka to perform validation of user inputs (type and scope). No change in logic was made with the exception of exposing two private enums to the public header.,"This is a well crafted PR .
However, I think it is leaking too much of the internal librdkafka implementation into the public API, something that we try hard not to do since it makes maintenance harder.
If I understand the use-case correctly you basically want to error out if consumer config properties are used on the producer and vice versa.
I suggest a simpler approach:

add rd_kafka_resp_err_t rd_kafka_conf_set_type (rd_kafka_conf_t *conf, rd_kafka_type_t type); which sets the instance type of the configuration. Have it default to the current behavioiur (untyped) and only allow it to be changed once, otherwise return ERR__CONFLICT.
change the internal conf setters and getters, e.g., anyconf_set.., to check that scope matches the type, otherwise return RD_KAFKA_CONF_UNKNOWN (or perhaps INVALID).
errors will now propagate automatically for each .._conf_set() call where the type does not match the property's scope.
Existing applications and bindings only need to add a single line of code to enable this type checking.",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,1756,2018-04-05T23:30:18Z,2018-04-09T21:14:02Z,2018-04-09T21:14:02Z,CLOSED,False,2003,1783,3,https://github.com/accelerated,Runtime instrospection of config options,3,[],https://github.com/edenhill/librdkafka/pull/1756,https://github.com/accelerated,9,https://github.com/edenhill/librdkafka/pull/1756#issuecomment-379830432,Description: Added two methods for obtaining runtime metadata info about configuration options. This will enable clients such as CppKafka to perform validation of user inputs (type and scope). No change in logic was made with the exception of exposing two private enums to the public header.,"@edenhill Thanks for looking into this. I have made a different set of changes to validate scope internally and which will be invisible to the user (see here). If the changes are OK with you, I can remove the first commit which exposes the underlying configuration lists. Please check and let me know.
Instead of adding the rd_kafka_resp_err_t rd_kafka_conf_set_type (rd_kafka_conf_t *conf, rd_kafka_type_t type), I keep track of which configuration options have been set. If the user at some point in time adds an option which is a mismatch, then we just return a MISMATCH error.
PS: I had made these changes before seeing your comment, which is why I didn't take your suggestions into account. However we seem to have almost reached a similar solution.",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,1756,2018-04-05T23:30:18Z,2018-04-09T21:14:02Z,2018-04-09T21:14:02Z,CLOSED,False,2003,1783,3,https://github.com/accelerated,Runtime instrospection of config options,3,[],https://github.com/edenhill/librdkafka/pull/1756,https://github.com/edenhill,10,https://github.com/edenhill/librdkafka/pull/1756#issuecomment-379832181,Description: Added two methods for obtaining runtime metadata info about configuration options. This will enable clients such as CppKafka to perform validation of user inputs (type and scope). No change in logic was made with the exception of exposing two private enums to the public header.,"@accelerated Great! Remove the previous commits (to clean up the history) and make sure to read the coding guidelines with regards to style:
https://github.com/edenhill/librdkafka/blob/master/CONTRIBUTING.md",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,1756,2018-04-05T23:30:18Z,2018-04-09T21:14:02Z,2018-04-09T21:14:02Z,CLOSED,False,2003,1783,3,https://github.com/accelerated,Runtime instrospection of config options,3,[],https://github.com/edenhill/librdkafka/pull/1756,https://github.com/accelerated,11,https://github.com/edenhill/librdkafka/pull/1756#issuecomment-379833886,Description: Added two methods for obtaining runtime metadata info about configuration options. This will enable clients such as CppKafka to perform validation of user inputs (type and scope). No change in logic was made with the exception of exposing two private enums to the public header.,"Yeah, I am using spaces instead of tabs, but for some reason new line alignments are inserted with tabs. It's a bit annoying to say the least. BTW i noticed files use tabs instead of spaces in lots of places. I think this is not desirable long term for formatting reasons. The style guide should mandate always using spaces IMHO. Please let me know.",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,1758,2018-04-09T12:30:56Z,2018-06-18T08:07:16Z,2018-06-18T10:07:35Z,MERGED,True,62,6,7,https://github.com/erkoln,added `compresion.level' configuration option for fine-tuning gzip and LZ4,2,[],https://github.com/edenhill/librdkafka/pull/1758,https://github.com/erkoln,1,https://github.com/edenhill/librdkafka/pull/1758,"Hello,
librdkafka supports message sets compressed with either gzip, LZ4 or Snappy but currently provides no control over compression level used by the underlying algorithm when producing messages, meaning the default compression level is always used.
Default zlib level provides a good compression ratio, but does so at the cost of high CPU usage. When producing messages to a remote cluster, CPU can easily become a bottleneck for per-partition throughput.
On the other hand, Snappy and LZ4 use very little CPU but the compression ratio is not so good when compared to gzip. In this case, network bandwidth quickly becomes the bottleneck.
This patch allows compression level to be fine-tuned for LZ4 and gzip (this seemingly can't be controlled in the Snappy implementation used by librdkafka), thus making it possible to trade between CPU and bandwidth usage.
In my use-case, given the particular CPU and network constraints I have to do with, level-1 gzip typically provide the best overall message throughput, while providing about the same compression ratio as default-level gzip.","Hello,
librdkafka supports message sets compressed with either gzip, LZ4 or Snappy but currently provides no control over compression level used by the underlying algorithm when producing messages, meaning the default compression level is always used.
Default zlib level provides a good compression ratio, but does so at the cost of high CPU usage. When producing messages to a remote cluster, CPU can easily become a bottleneck for per-partition throughput.
On the other hand, Snappy and LZ4 use very little CPU but the compression ratio is not so good when compared to gzip. In this case, network bandwidth quickly becomes the bottleneck.
This patch allows compression level to be fine-tuned for LZ4 and gzip (this seemingly can't be controlled in the Snappy implementation used by librdkafka), thus making it possible to trade between CPU and bandwidth usage.
In my use-case, given the particular CPU and network constraints I have to do with, level-1 gzip typically provide the best overall message throughput, while providing about the same compression ratio as default-level gzip.",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,1758,2018-04-09T12:30:56Z,2018-06-18T08:07:16Z,2018-06-18T10:07:35Z,MERGED,True,62,6,7,https://github.com/erkoln,added `compresion.level' configuration option for fine-tuning gzip and LZ4,2,[],https://github.com/edenhill/librdkafka/pull/1758,https://github.com/erkoln,2,https://github.com/edenhill/librdkafka/pull/1758#issuecomment-396896232,"Hello,
librdkafka supports message sets compressed with either gzip, LZ4 or Snappy but currently provides no control over compression level used by the underlying algorithm when producing messages, meaning the default compression level is always used.
Default zlib level provides a good compression ratio, but does so at the cost of high CPU usage. When producing messages to a remote cluster, CPU can easily become a bottleneck for per-partition throughput.
On the other hand, Snappy and LZ4 use very little CPU but the compression ratio is not so good when compared to gzip. In this case, network bandwidth quickly becomes the bottleneck.
This patch allows compression level to be fine-tuned for LZ4 and gzip (this seemingly can't be controlled in the Snappy implementation used by librdkafka), thus making it possible to trade between CPU and bandwidth usage.
In my use-case, given the particular CPU and network constraints I have to do with, level-1 gzip typically provide the best overall message throughput, while providing about the same compression ratio as default-level gzip.","Hello,
I rebased my fork on current master to avoid polluting the commit log in case of merge.
This seems to have broken change requests tracking but I tried to reply with relevant commit hashes.
Sorry for that",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,1758,2018-04-09T12:30:56Z,2018-06-18T08:07:16Z,2018-06-18T10:07:35Z,MERGED,True,62,6,7,https://github.com/erkoln,added `compresion.level' configuration option for fine-tuning gzip and LZ4,2,[],https://github.com/edenhill/librdkafka/pull/1758,https://github.com/erkoln,3,https://github.com/edenhill/librdkafka/pull/1758#issuecomment-397537322,"Hello,
librdkafka supports message sets compressed with either gzip, LZ4 or Snappy but currently provides no control over compression level used by the underlying algorithm when producing messages, meaning the default compression level is always used.
Default zlib level provides a good compression ratio, but does so at the cost of high CPU usage. When producing messages to a remote cluster, CPU can easily become a bottleneck for per-partition throughput.
On the other hand, Snappy and LZ4 use very little CPU but the compression ratio is not so good when compared to gzip. In this case, network bandwidth quickly becomes the bottleneck.
This patch allows compression level to be fine-tuned for LZ4 and gzip (this seemingly can't be controlled in the Snappy implementation used by librdkafka), thus making it possible to trade between CPU and bandwidth usage.
In my use-case, given the particular CPU and network constraints I have to do with, level-1 gzip typically provide the best overall message throughput, while providing about the same compression ratio as default-level gzip.",Any reason why the stretch-mips build disappeared from doozer? I think the build should have passed.,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,1758,2018-04-09T12:30:56Z,2018-06-18T08:07:16Z,2018-06-18T10:07:35Z,MERGED,True,62,6,7,https://github.com/erkoln,added `compresion.level' configuration option for fine-tuning gzip and LZ4,2,[],https://github.com/edenhill/librdkafka/pull/1758,https://github.com/erkoln,4,https://github.com/edenhill/librdkafka/pull/1758#issuecomment-397613017,"Hello,
librdkafka supports message sets compressed with either gzip, LZ4 or Snappy but currently provides no control over compression level used by the underlying algorithm when producing messages, meaning the default compression level is always used.
Default zlib level provides a good compression ratio, but does so at the cost of high CPU usage. When producing messages to a remote cluster, CPU can easily become a bottleneck for per-partition throughput.
On the other hand, Snappy and LZ4 use very little CPU but the compression ratio is not so good when compared to gzip. In this case, network bandwidth quickly becomes the bottleneck.
This patch allows compression level to be fine-tuned for LZ4 and gzip (this seemingly can't be controlled in the Snappy implementation used by librdkafka), thus making it possible to trade between CPU and bandwidth usage.
In my use-case, given the particular CPU and network constraints I have to do with, level-1 gzip typically provide the best overall message throughput, while providing about the same compression ratio as default-level gzip.","Squashed history, which will hopefully also fix PR status.",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,1758,2018-04-09T12:30:56Z,2018-06-18T08:07:16Z,2018-06-18T10:07:35Z,MERGED,True,62,6,7,https://github.com/erkoln,added `compresion.level' configuration option for fine-tuning gzip and LZ4,2,[],https://github.com/edenhill/librdkafka/pull/1758,https://github.com/edenhill,5,https://github.com/edenhill/librdkafka/pull/1758#issuecomment-397684158,"Hello,
librdkafka supports message sets compressed with either gzip, LZ4 or Snappy but currently provides no control over compression level used by the underlying algorithm when producing messages, meaning the default compression level is always used.
Default zlib level provides a good compression ratio, but does so at the cost of high CPU usage. When producing messages to a remote cluster, CPU can easily become a bottleneck for per-partition throughput.
On the other hand, Snappy and LZ4 use very little CPU but the compression ratio is not so good when compared to gzip. In this case, network bandwidth quickly becomes the bottleneck.
This patch allows compression level to be fine-tuned for LZ4 and gzip (this seemingly can't be controlled in the Snappy implementation used by librdkafka), thus making it possible to trade between CPU and bandwidth usage.
In my use-case, given the particular CPU and network constraints I have to do with, level-1 gzip typically provide the best overall message throughput, while providing about the same compression ratio as default-level gzip.",@andoma What's up with the mips builder?,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,1758,2018-04-09T12:30:56Z,2018-06-18T08:07:16Z,2018-06-18T10:07:35Z,MERGED,True,62,6,7,https://github.com/erkoln,added `compresion.level' configuration option for fine-tuning gzip and LZ4,2,[],https://github.com/edenhill/librdkafka/pull/1758,https://github.com/edenhill,6,https://github.com/edenhill/librdkafka/pull/1758#issuecomment-397974223,"Hello,
librdkafka supports message sets compressed with either gzip, LZ4 or Snappy but currently provides no control over compression level used by the underlying algorithm when producing messages, meaning the default compression level is always used.
Default zlib level provides a good compression ratio, but does so at the cost of high CPU usage. When producing messages to a remote cluster, CPU can easily become a bottleneck for per-partition throughput.
On the other hand, Snappy and LZ4 use very little CPU but the compression ratio is not so good when compared to gzip. In this case, network bandwidth quickly becomes the bottleneck.
This patch allows compression level to be fine-tuned for LZ4 and gzip (this seemingly can't be controlled in the Snappy implementation used by librdkafka), thus making it possible to trade between CPU and bandwidth usage.
In my use-case, given the particular CPU and network constraints I have to do with, level-1 gzip typically provide the best overall message throughput, while providing about the same compression ratio as default-level gzip.",Thank you for this PR!,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,1758,2018-04-09T12:30:56Z,2018-06-18T08:07:16Z,2018-06-18T10:07:35Z,MERGED,True,62,6,7,https://github.com/erkoln,added `compresion.level' configuration option for fine-tuning gzip and LZ4,2,[],https://github.com/edenhill/librdkafka/pull/1758,https://github.com/erkoln,7,https://github.com/edenhill/librdkafka/pull/1758#issuecomment-398005987,"Hello,
librdkafka supports message sets compressed with either gzip, LZ4 or Snappy but currently provides no control over compression level used by the underlying algorithm when producing messages, meaning the default compression level is always used.
Default zlib level provides a good compression ratio, but does so at the cost of high CPU usage. When producing messages to a remote cluster, CPU can easily become a bottleneck for per-partition throughput.
On the other hand, Snappy and LZ4 use very little CPU but the compression ratio is not so good when compared to gzip. In this case, network bandwidth quickly becomes the bottleneck.
This patch allows compression level to be fine-tuned for LZ4 and gzip (this seemingly can't be controlled in the Snappy implementation used by librdkafka), thus making it possible to trade between CPU and bandwidth usage.
In my use-case, given the particular CPU and network constraints I have to do with, level-1 gzip typically provide the best overall message throughput, while providing about the same compression ratio as default-level gzip.","You're welcome. :-)
Thank you for the great piece of software that librdkafka is!",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,1760,2018-04-09T21:12:15Z,2018-04-10T15:32:39Z,2018-04-10T15:38:13Z,CLOSED,False,403,196,5,https://github.com/accelerated,provide scope validation for config options,1,[],https://github.com/edenhill/librdkafka/pull/1760,https://github.com/accelerated,1,https://github.com/edenhill/librdkafka/pull/1760,"Description

Provided scope validation for handle and topic configuration settings.
Provided scope validation for callbacks.
State is stored in the scope member of the configuration handles. As options are added, the configuration scope is also updated to reflect the scope of the option. If at some point an option is added which does not match the current scope state, RD_KAFKA_CONF_SCOPE_MISMATCH is returned.
Default topic configurations are made to reflect the scope of the handle configuration and vice-versa.
Duplication of a configuration will preserve it's current scope value.
By default the scope starts as RD_GLOBAL for handle configurations and RD_TOPIC for topic configurations.
RD_CGRP will set the configuration scope to RD_CONSUMER.
The feature can be enabled/disabled via void rd_kafka_conf_enable_scope_validation(int enable). By default it is disabled.","Description

Provided scope validation for handle and topic configuration settings.
Provided scope validation for callbacks.
State is stored in the scope member of the configuration handles. As options are added, the configuration scope is also updated to reflect the scope of the option. If at some point an option is added which does not match the current scope state, RD_KAFKA_CONF_SCOPE_MISMATCH is returned.
Default topic configurations are made to reflect the scope of the handle configuration and vice-versa.
Duplication of a configuration will preserve it's current scope value.
By default the scope starts as RD_GLOBAL for handle configurations and RD_TOPIC for topic configurations.
RD_CGRP will set the configuration scope to RD_CONSUMER.
The feature can be enabled/disabled via void rd_kafka_conf_enable_scope_validation(int enable). By default it is disabled.",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,1760,2018-04-09T21:12:15Z,2018-04-10T15:32:39Z,2018-04-10T15:38:13Z,CLOSED,False,403,196,5,https://github.com/accelerated,provide scope validation for config options,1,[],https://github.com/edenhill/librdkafka/pull/1760,https://github.com/accelerated,2,https://github.com/edenhill/librdkafka/pull/1760#issuecomment-379928377,"Description

Provided scope validation for handle and topic configuration settings.
Provided scope validation for callbacks.
State is stored in the scope member of the configuration handles. As options are added, the configuration scope is also updated to reflect the scope of the option. If at some point an option is added which does not match the current scope state, RD_KAFKA_CONF_SCOPE_MISMATCH is returned.
Default topic configurations are made to reflect the scope of the handle configuration and vice-versa.
Duplication of a configuration will preserve it's current scope value.
By default the scope starts as RD_GLOBAL for handle configurations and RD_TOPIC for topic configurations.
RD_CGRP will set the configuration scope to RD_CONSUMER.
The feature can be enabled/disabled via void rd_kafka_conf_enable_scope_validation(int enable). By default it is disabled.","@mfontanini The logic for the default topic is essentially the same. If you cannot intermix default topic configuration for both consumers and producers. Global configs are ok however. Since the default topic is attached to a handle config and thus can be used to set topic configs in other configurations, it will throw an error if applied to a config of the wrong scope.",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,1760,2018-04-09T21:12:15Z,2018-04-10T15:32:39Z,2018-04-10T15:38:13Z,CLOSED,False,403,196,5,https://github.com/accelerated,provide scope validation for config options,1,[],https://github.com/edenhill/librdkafka/pull/1760,https://github.com/mfontanini,3,https://github.com/edenhill/librdkafka/pull/1760#issuecomment-379930212,"Description

Provided scope validation for handle and topic configuration settings.
Provided scope validation for callbacks.
State is stored in the scope member of the configuration handles. As options are added, the configuration scope is also updated to reflect the scope of the option. If at some point an option is added which does not match the current scope state, RD_KAFKA_CONF_SCOPE_MISMATCH is returned.
Default topic configurations are made to reflect the scope of the handle configuration and vice-versa.
Duplication of a configuration will preserve it's current scope value.
By default the scope starts as RD_GLOBAL for handle configurations and RD_TOPIC for topic configurations.
RD_CGRP will set the configuration scope to RD_CONSUMER.
The feature can be enabled/disabled via void rd_kafka_conf_enable_scope_validation(int enable). By default it is disabled.","Does this mean that this (using cppkafka syntax and using it as it is today) doesn't work anymore?
Configuration config = {
    // this is a topic configuration attribute and will fall through
    // to the default topic configuration
    { ""auto.offset.reset"", ""latest"" }
};",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,1760,2018-04-09T21:12:15Z,2018-04-10T15:32:39Z,2018-04-10T15:38:13Z,CLOSED,False,403,196,5,https://github.com/accelerated,provide scope validation for config options,1,[],https://github.com/edenhill/librdkafka/pull/1760,https://github.com/accelerated,4,https://github.com/edenhill/librdkafka/pull/1760#issuecomment-379932275,"Description

Provided scope validation for handle and topic configuration settings.
Provided scope validation for callbacks.
State is stored in the scope member of the configuration handles. As options are added, the configuration scope is also updated to reflect the scope of the option. If at some point an option is added which does not match the current scope state, RD_KAFKA_CONF_SCOPE_MISMATCH is returned.
Default topic configurations are made to reflect the scope of the handle configuration and vice-versa.
Duplication of a configuration will preserve it's current scope value.
By default the scope starts as RD_GLOBAL for handle configurations and RD_TOPIC for topic configurations.
RD_CGRP will set the configuration scope to RD_CONSUMER.
The feature can be enabled/disabled via void rd_kafka_conf_enable_scope_validation(int enable). By default it is disabled.",The fall through should work as before. There are test cases which pass in librdkafka.,True,{'THUMBS_UP': ['https://github.com/mfontanini']}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,1761,2018-04-10T03:06:44Z,2018-07-02T13:41:58Z,2018-07-02T13:41:58Z,CLOSED,False,3,1,1,https://github.com/TopHK,revise src/rdkafka_sasl_cyrus.c,1,[],https://github.com/edenhill/librdkafka/pull/1761,https://github.com/TopHK,1,https://github.com/edenhill/librdkafka/pull/1761,"When I use kerberos, I found a bug in src/rdkafka_sasl_cyrus.c.
When the keytab file is a wrong one, the data can still be transfered to kafka brokers.
After I review the source of rdkafka for several times, I find that, the function rd_kafka_sasl_cyrus_client_new invokes the function rd_kafka_sasl_cyrus_kinit_refresh to kinit a keytab file. However, when rd_kafka_sasl_cyrus_kinit_refresh returns -1, it means kinit operation comes to a failure. But rd_kafka_sasl_cyrus_client_new can still execute the following code lines after invoking the function rd_kafka_sasl_cyrus_kinit_refresh.","When I use kerberos, I found a bug in src/rdkafka_sasl_cyrus.c.
When the keytab file is a wrong one, the data can still be transfered to kafka brokers.
After I review the source of rdkafka for several times, I find that, the function rd_kafka_sasl_cyrus_client_new invokes the function rd_kafka_sasl_cyrus_kinit_refresh to kinit a keytab file. However, when rd_kafka_sasl_cyrus_kinit_refresh returns -1, it means kinit operation comes to a failure. But rd_kafka_sasl_cyrus_client_new can still execute the following code lines after invoking the function rd_kafka_sasl_cyrus_kinit_refresh.",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,1761,2018-04-10T03:06:44Z,2018-07-02T13:41:58Z,2018-07-02T13:41:58Z,CLOSED,False,3,1,1,https://github.com/TopHK,revise src/rdkafka_sasl_cyrus.c,1,[],https://github.com/edenhill/librdkafka/pull/1761,https://github.com/edenhill,2,https://github.com/edenhill/librdkafka/pull/1761#issuecomment-395365329,"When I use kerberos, I found a bug in src/rdkafka_sasl_cyrus.c.
When the keytab file is a wrong one, the data can still be transfered to kafka brokers.
After I review the source of rdkafka for several times, I find that, the function rd_kafka_sasl_cyrus_client_new invokes the function rd_kafka_sasl_cyrus_kinit_refresh to kinit a keytab file. However, when rd_kafka_sasl_cyrus_kinit_refresh returns -1, it means kinit operation comes to a failure. But rd_kafka_sasl_cyrus_client_new can still execute the following code lines after invoking the function rd_kafka_sasl_cyrus_kinit_refresh.","The intention here was to be a bit lax with kinit, it is possible that we already have a ticket and kinit failing, in which case authentication work anyway.
And if kinit fails to refresh our ticket the broker-side authentication will fail anyway, so there is no need to fail early here, which your example also shows since you are able to authenticate even when kinit fails, probably due to cached credentials.",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,1764,2018-04-10T15:32:21Z,2021-03-25T08:29:56Z,2021-03-25T08:29:56Z,CLOSED,False,669,195,14,https://github.com/accelerated,Added scope validation functionality for producers and consumers,1,[],https://github.com/edenhill/librdkafka/pull/1764,https://github.com/accelerated,1,https://github.com/edenhill/librdkafka/pull/1764,"Description
This is an important feature which allows proper scope validation across all configuration options. It prevents a user from mistakenly setting the wrong configuration which is silently absorbed and never takes effect. Two new APIs have been added which allow a user to set strict scope guards on the all configuration options.

Provided scope validation for handle and topic configuration settings by calling rd_kafka_conf_set_type() and rd_kafka_topic_conf_set_type(). By default this is disabled i.e. scope is unset. This scope setting is on a per-handle basis.
Provided scope validation for callbacks.
State is stored in the scope member of the configuration handles. As options are added, the configuration scope is also updated to reflect the scope of the option. If at some point an option is added which does not match the current scope state, RD_KAFKA_CONF_UNKNOWN is returned and the internal errno is set.
Default topic configurations are made to reflect the scope of the handle configuration and vice-versa.
Duplication of a configuration will preserve it's current scope value.
RD_CGRP will set the configuration scope to RD_CONSUMER.","Description
This is an important feature which allows proper scope validation across all configuration options. It prevents a user from mistakenly setting the wrong configuration which is silently absorbed and never takes effect. Two new APIs have been added which allow a user to set strict scope guards on the all configuration options.

Provided scope validation for handle and topic configuration settings by calling rd_kafka_conf_set_type() and rd_kafka_topic_conf_set_type(). By default this is disabled i.e. scope is unset. This scope setting is on a per-handle basis.
Provided scope validation for callbacks.
State is stored in the scope member of the configuration handles. As options are added, the configuration scope is also updated to reflect the scope of the option. If at some point an option is added which does not match the current scope state, RD_KAFKA_CONF_UNKNOWN is returned and the internal errno is set.
Default topic configurations are made to reflect the scope of the handle configuration and vice-versa.
Duplication of a configuration will preserve it's current scope value.
RD_CGRP will set the configuration scope to RD_CONSUMER.",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,1764,2018-04-10T15:32:21Z,2021-03-25T08:29:56Z,2021-03-25T08:29:56Z,CLOSED,False,669,195,14,https://github.com/accelerated,Added scope validation functionality for producers and consumers,1,[],https://github.com/edenhill/librdkafka/pull/1764,https://github.com/accelerated,2,https://github.com/edenhill/librdkafka/pull/1764#issuecomment-380609097,"Description
This is an important feature which allows proper scope validation across all configuration options. It prevents a user from mistakenly setting the wrong configuration which is silently absorbed and never takes effect. Two new APIs have been added which allow a user to set strict scope guards on the all configuration options.

Provided scope validation for handle and topic configuration settings by calling rd_kafka_conf_set_type() and rd_kafka_topic_conf_set_type(). By default this is disabled i.e. scope is unset. This scope setting is on a per-handle basis.
Provided scope validation for callbacks.
State is stored in the scope member of the configuration handles. As options are added, the configuration scope is also updated to reflect the scope of the option. If at some point an option is added which does not match the current scope state, RD_KAFKA_CONF_UNKNOWN is returned and the internal errno is set.
Default topic configurations are made to reflect the scope of the handle configuration and vice-versa.
Duplication of a configuration will preserve it's current scope value.
RD_CGRP will set the configuration scope to RD_CONSUMER.","@edenhill Also for all the config settings which returned void (you specifically rejected any ABI change), I am setting errno and also I've added a compile time setting for returning a proper config error type. By default, in all builds and when you release the code these will always be turned off so no ABI impact. However if anyone builds their own version, they can enable this flag and check proper return value instead of errno. Makes error checking much nicer IMHO. Please let me know what you think of this?
I have also updated the C++ version of the library to have the scope checks.",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,1764,2018-04-10T15:32:21Z,2021-03-25T08:29:56Z,2021-03-25T08:29:56Z,CLOSED,False,669,195,14,https://github.com/accelerated,Added scope validation functionality for producers and consumers,1,[],https://github.com/edenhill/librdkafka/pull/1764,https://github.com/accelerated,3,https://github.com/edenhill/librdkafka/pull/1764#issuecomment-383588481,"Description
This is an important feature which allows proper scope validation across all configuration options. It prevents a user from mistakenly setting the wrong configuration which is silently absorbed and never takes effect. Two new APIs have been added which allow a user to set strict scope guards on the all configuration options.

Provided scope validation for handle and topic configuration settings by calling rd_kafka_conf_set_type() and rd_kafka_topic_conf_set_type(). By default this is disabled i.e. scope is unset. This scope setting is on a per-handle basis.
Provided scope validation for callbacks.
State is stored in the scope member of the configuration handles. As options are added, the configuration scope is also updated to reflect the scope of the option. If at some point an option is added which does not match the current scope state, RD_KAFKA_CONF_UNKNOWN is returned and the internal errno is set.
Default topic configurations are made to reflect the scope of the handle configuration and vice-versa.
Duplication of a configuration will preserve it's current scope value.
RD_CGRP will set the configuration scope to RD_CONSUMER.","@edenhill Hi, any chance you could review this (pls see my last comments as well)? I will incorporate whichever changes you want. I would also like to roll this into cppkafka as soon as this is approved. Much appreciated, thanks!",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,1764,2018-04-10T15:32:21Z,2021-03-25T08:29:56Z,2021-03-25T08:29:56Z,CLOSED,False,669,195,14,https://github.com/accelerated,Added scope validation functionality for producers and consumers,1,[],https://github.com/edenhill/librdkafka/pull/1764,https://github.com/edenhill,4,https://github.com/edenhill/librdkafka/pull/1764#issuecomment-806463140,"Description
This is an important feature which allows proper scope validation across all configuration options. It prevents a user from mistakenly setting the wrong configuration which is silently absorbed and never takes effect. Two new APIs have been added which allow a user to set strict scope guards on the all configuration options.

Provided scope validation for handle and topic configuration settings by calling rd_kafka_conf_set_type() and rd_kafka_topic_conf_set_type(). By default this is disabled i.e. scope is unset. This scope setting is on a per-handle basis.
Provided scope validation for callbacks.
State is stored in the scope member of the configuration handles. As options are added, the configuration scope is also updated to reflect the scope of the option. If at some point an option is added which does not match the current scope state, RD_KAFKA_CONF_UNKNOWN is returned and the internal errno is set.
Default topic configurations are made to reflect the scope of the handle configuration and vice-versa.
Duplication of a configuration will preserve it's current scope value.
RD_CGRP will set the configuration scope to RD_CONSUMER.","Outdated.
We now warn when producer properties are set on consumers, and vice versa.",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,1766,2018-04-11T13:08:50Z,2018-06-05T09:33:15Z,2018-06-05T09:33:15Z,MERGED,True,9923,214,69,https://github.com/edenhill,Topic Admin API,73,['enhancement'],https://github.com/edenhill/librdkafka/pull/1766,https://github.com/edenhill,1,https://github.com/edenhill/librdkafka/pull/1766,,,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,1766,2018-04-11T13:08:50Z,2018-06-05T09:33:15Z,2018-06-05T09:33:15Z,MERGED,True,9923,214,69,https://github.com/edenhill,Topic Admin API,73,['enhancement'],https://github.com/edenhill/librdkafka/pull/1766,https://github.com/edenhill,2,https://github.com/edenhill/librdkafka/pull/1766#issuecomment-380446328,,"After the .NET SR and Avro PR reviews I'd like to return the favour.
I hereby present you with a 8.8 KLOC PR, congratulations!
I would focus on:

src/rdkafka_admin.c - the actual implementation of the admin request workers
src/rdkafka.h - the great (in size) public API
src/rdkafka_request.c - protocol request construction",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,1766,2018-04-11T13:08:50Z,2018-06-05T09:33:15Z,2018-06-05T09:33:15Z,MERGED,True,9923,214,69,https://github.com/edenhill,Topic Admin API,73,['enhancement'],https://github.com/edenhill/librdkafka/pull/1766,https://github.com/mhowlett,3,https://github.com/edenhill/librdkafka/pull/1766#issuecomment-380590700,,"first comment: is it easy to make a librdkafka.redist nuget package from this PR? if so, that would probably help with the review of the API as i'd start implementing the .NET binding simultaneously and surely notice if i bumped into anything that seemed off.",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,1766,2018-04-11T13:08:50Z,2018-06-05T09:33:15Z,2018-06-05T09:33:15Z,MERGED,True,9923,214,69,https://github.com/edenhill,Topic Admin API,73,['enhancement'],https://github.com/edenhill/librdkafka/pull/1766,https://github.com/edenhill,4,https://github.com/edenhill/librdkafka/pull/1766#issuecomment-380592126,,"Yeah, should be sufficient to create a tag on the adminapi branch, which will trigger the CI to upload artifacts to S3, which you then package with packaging/nuget/release.py and then upload to NuGet.
I'll push a tag and see what happens",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,1766,2018-04-11T13:08:50Z,2018-06-05T09:33:15Z,2018-06-05T09:33:15Z,MERGED,True,9923,214,69,https://github.com/edenhill,Topic Admin API,73,['enhancement'],https://github.com/edenhill/librdkafka/pull/1766,https://github.com/jeffwidman,5,https://github.com/edenhill/librdkafka/pull/1766#issuecomment-391276400,,"Can you cleanup the git history on this a little? I was curious to review it commit-by-commit, but looks like they aren't really separated into logical commits, for example 51fc40f and c7d7f0e could be a single commit (at least based on a quick skim).",True,{'THUMBS_UP': ['https://github.com/edenhill']}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,1766,2018-04-11T13:08:50Z,2018-06-05T09:33:15Z,2018-06-05T09:33:15Z,MERGED,True,9923,214,69,https://github.com/edenhill,Topic Admin API,73,['enhancement'],https://github.com/edenhill/librdkafka/pull/1766,https://github.com/mhowlett,6,https://github.com/edenhill/librdkafka/pull/1766#issuecomment-392987737,,"a few outstanding comments, otherwise i'm happy.
I thought about the public API in a fair bit of detail, which i think is the most critical thing to get right. In the interests of time i only looked very briefly at the implementation (but will give it plenty of test time as i make the .NET binding).",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,1774,2018-04-17T19:07:23Z,2018-04-18T08:59:31Z,2018-04-18T08:59:34Z,MERGED,True,19,6,6,https://github.com/edenhill, Improve latency by using high-precision QPC clock on Windows ,2,[],https://github.com/edenhill/librdkafka/pull/1774,https://github.com/edenhill,1,https://github.com/edenhill/librdkafka/pull/1774,.. and a test fix.,.. and a test fix.,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,1774,2018-04-17T19:07:23Z,2018-04-18T08:59:31Z,2018-04-18T08:59:34Z,MERGED,True,19,6,6,https://github.com/edenhill, Improve latency by using high-precision QPC clock on Windows ,2,[],https://github.com/edenhill/librdkafka/pull/1774,https://github.com/mhowlett,2,https://github.com/edenhill/librdkafka/pull/1774#issuecomment-382203368,.. and a test fix.,lgtm,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,1793,2018-05-04T05:24:24Z,2018-05-04T07:35:48Z,2018-05-04T07:35:48Z,MERGED,True,16,16,1,https://github.com/stanislavkozlovski,[Issues/1791] Add relative hyperlinks to table of contents in INTRODUCTION.md,2,[],https://github.com/edenhill/librdkafka/pull/1793,https://github.com/stanislavkozlovski,1,https://github.com/edenhill/librdkafka/pull/1793,"Fixes #1791
Also fixed a typo: ""Test Detailts"" => ""Test Details""
It makes the document a tad bit uglier in plain-text mode but I believe the trade-off is okay, seeing as most people will be introduced through the GitHub interface for the first time.","Fixes #1791
Also fixed a typo: ""Test Detailts"" => ""Test Details""
It makes the document a tad bit uglier in plain-text mode but I believe the trade-off is okay, seeing as most people will be introduced through the GitHub interface for the first time.",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,1793,2018-05-04T05:24:24Z,2018-05-04T07:35:48Z,2018-05-04T07:35:48Z,MERGED,True,16,16,1,https://github.com/stanislavkozlovski,[Issues/1791] Add relative hyperlinks to table of contents in INTRODUCTION.md,2,[],https://github.com/edenhill/librdkafka/pull/1793,https://github.com/edenhill,2,https://github.com/edenhill/librdkafka/pull/1793#issuecomment-386510280,"Fixes #1791
Also fixed a typo: ""Test Detailts"" => ""Test Details""
It makes the document a tad bit uglier in plain-text mode but I believe the trade-off is okay, seeing as most people will be introduced through the GitHub interface for the first time.",Thanks for this! Is it ready to merge?,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,1793,2018-05-04T05:24:24Z,2018-05-04T07:35:48Z,2018-05-04T07:35:48Z,MERGED,True,16,16,1,https://github.com/stanislavkozlovski,[Issues/1791] Add relative hyperlinks to table of contents in INTRODUCTION.md,2,[],https://github.com/edenhill/librdkafka/pull/1793,https://github.com/stanislavkozlovski,3,https://github.com/edenhill/librdkafka/pull/1793#issuecomment-386524544,"Fixes #1791
Also fixed a typo: ""Test Detailts"" => ""Test Details""
It makes the document a tad bit uglier in plain-text mode but I believe the trade-off is okay, seeing as most people will be introduced through the GitHub interface for the first time.",@edenhill yes,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,1797,2018-05-06T08:35:51Z,2018-05-15T09:12:02Z,2018-05-15T09:12:09Z,MERGED,True,2,2,1,https://github.com/xbolshe,[FIX] pkg-config detection of libraries does not work,3,[],https://github.com/edenhill/librdkafka/pull/1797,https://github.com/xbolshe,1,https://github.com/edenhill/librdkafka/pull/1797,"This PR closes Issue #1796 
A log below shows that zlib, libcrypto, libssl are detected by pkg-config. A compilation was performed with a parameter --pkg-config-path=$PKG_CONFIG_PATH.
Removing config.log
rm -f config.log config.log.old
patching file mklove/modules/configure.cc
Hunk #1 succeeded at 169 (offset 2 lines).
patching file mklove/modules/configure.cc
checking for OS or distribution... ok (Ubuntu)
checking for C compiler from CC env... ok
checking for C++ compiler from CXX env... failed
checking for C++ compiler (g++)... ok
checking executable ld... ok
checking executable nm... ok
checking executable objdump... ok
checking executable strip... ok
checking for pkgconfig (by command)... ok
checking for install (by command)... ok
checking for PIC (by compile)... ok
checking for GNU-compatible linker options... ok
checking for GNU linker-script ld flag... ok
checking for __atomic_32 (by compile)... ok
checking for __atomic_64 (by compile)... ok
checking for socket (by compile)... ok
parsing version '0x000b04ff'... ok (0.11.4)
checking for librt (by pkg-config)... failed
checking for librt (by compile)... ok
checking for libpthread (by pkg-config)... failed
checking for libpthread (by compile)... ok
checking for libdl (by pkg-config)... failed
checking for libdl (by compile)... ok
checking for zlib (by pkg-config)... ok
checking for zlib (by compile)... ok (cached)
checking for libcrypto (by pkg-config)... ok
checking for libcrypto (by compile)... ok (cached)
checking for liblz4 (by pkg-config)... failed
checking for liblz4 (by compile)... failed (disable)
checking for libssl (by pkg-config)... ok
checking for libssl (by compile)... ok (cached)
checking for libsasl2 (by pkg-config)... failed
checking for libsasl2 (by compile)... failed (disable)
checking for libsasl (by pkg-config)... failed
checking for libsasl (by compile)... failed (disable)
checking for crc32chw (by compile)... failed (disable)
checking for regex (by compile)... ok
checking for strndup (by compile)... ok
checking for strerror_r (by compile)... ok
checking for pthread_setname_gnu (by compile)... ok
checking for nm (by env NM)... ok (cached)
checking for python (by command)... ok
Generated Makefile.config
Generated config.h","This PR closes Issue #1796 
A log below shows that zlib, libcrypto, libssl are detected by pkg-config. A compilation was performed with a parameter --pkg-config-path=$PKG_CONFIG_PATH.
Removing config.log
rm -f config.log config.log.old
patching file mklove/modules/configure.cc
Hunk #1 succeeded at 169 (offset 2 lines).
patching file mklove/modules/configure.cc
checking for OS or distribution... ok (Ubuntu)
checking for C compiler from CC env... ok
checking for C++ compiler from CXX env... failed
checking for C++ compiler (g++)... ok
checking executable ld... ok
checking executable nm... ok
checking executable objdump... ok
checking executable strip... ok
checking for pkgconfig (by command)... ok
checking for install (by command)... ok
checking for PIC (by compile)... ok
checking for GNU-compatible linker options... ok
checking for GNU linker-script ld flag... ok
checking for __atomic_32 (by compile)... ok
checking for __atomic_64 (by compile)... ok
checking for socket (by compile)... ok
parsing version '0x000b04ff'... ok (0.11.4)
checking for librt (by pkg-config)... failed
checking for librt (by compile)... ok
checking for libpthread (by pkg-config)... failed
checking for libpthread (by compile)... ok
checking for libdl (by pkg-config)... failed
checking for libdl (by compile)... ok
checking for zlib (by pkg-config)... ok
checking for zlib (by compile)... ok (cached)
checking for libcrypto (by pkg-config)... ok
checking for libcrypto (by compile)... ok (cached)
checking for liblz4 (by pkg-config)... failed
checking for liblz4 (by compile)... failed (disable)
checking for libssl (by pkg-config)... ok
checking for libssl (by compile)... ok (cached)
checking for libsasl2 (by pkg-config)... failed
checking for libsasl2 (by compile)... failed (disable)
checking for libsasl (by pkg-config)... failed
checking for libsasl (by compile)... failed (disable)
checking for crc32chw (by compile)... failed (disable)
checking for regex (by compile)... ok
checking for strndup (by compile)... ok
checking for strerror_r (by compile)... ok
checking for pthread_setname_gnu (by compile)... ok
checking for nm (by env NM)... ok (cached)
checking for python (by command)... ok
Generated Makefile.config
Generated config.h",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,1797,2018-05-06T08:35:51Z,2018-05-15T09:12:02Z,2018-05-15T09:12:09Z,MERGED,True,2,2,1,https://github.com/xbolshe,[FIX] pkg-config detection of libraries does not work,3,[],https://github.com/edenhill/librdkafka/pull/1797,https://github.com/edenhill,2,https://github.com/edenhill/librdkafka/pull/1797#issuecomment-389098956,"This PR closes Issue #1796 
A log below shows that zlib, libcrypto, libssl are detected by pkg-config. A compilation was performed with a parameter --pkg-config-path=$PKG_CONFIG_PATH.
Removing config.log
rm -f config.log config.log.old
patching file mklove/modules/configure.cc
Hunk #1 succeeded at 169 (offset 2 lines).
patching file mklove/modules/configure.cc
checking for OS or distribution... ok (Ubuntu)
checking for C compiler from CC env... ok
checking for C++ compiler from CXX env... failed
checking for C++ compiler (g++)... ok
checking executable ld... ok
checking executable nm... ok
checking executable objdump... ok
checking executable strip... ok
checking for pkgconfig (by command)... ok
checking for install (by command)... ok
checking for PIC (by compile)... ok
checking for GNU-compatible linker options... ok
checking for GNU linker-script ld flag... ok
checking for __atomic_32 (by compile)... ok
checking for __atomic_64 (by compile)... ok
checking for socket (by compile)... ok
parsing version '0x000b04ff'... ok (0.11.4)
checking for librt (by pkg-config)... failed
checking for librt (by compile)... ok
checking for libpthread (by pkg-config)... failed
checking for libpthread (by compile)... ok
checking for libdl (by pkg-config)... failed
checking for libdl (by compile)... ok
checking for zlib (by pkg-config)... ok
checking for zlib (by compile)... ok (cached)
checking for libcrypto (by pkg-config)... ok
checking for libcrypto (by compile)... ok (cached)
checking for liblz4 (by pkg-config)... failed
checking for liblz4 (by compile)... failed (disable)
checking for libssl (by pkg-config)... ok
checking for libssl (by compile)... ok (cached)
checking for libsasl2 (by pkg-config)... failed
checking for libsasl2 (by compile)... failed (disable)
checking for libsasl (by pkg-config)... failed
checking for libsasl (by compile)... failed (disable)
checking for crc32chw (by compile)... failed (disable)
checking for regex (by compile)... ok
checking for strndup (by compile)... ok
checking for strerror_r (by compile)... ok
checking for pthread_setname_gnu (by compile)... ok
checking for nm (by env NM)... ok (cached)
checking for python (by command)... ok
Generated Makefile.config
Generated config.h",Thanks for this!,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,1799,2018-05-09T12:24:32Z,2018-06-07T09:59:24Z,2018-06-07T09:59:29Z,MERGED,True,56,6,4,https://github.com/briot,Minor documentation updates from replies to #1794,6,[],https://github.com/edenhill/librdkafka/pull/1799,https://github.com/briot,1,https://github.com/edenhill/librdkafka/pull/1799,"From the very useful feedback you gave in #1794.
I am not sure of the best place for this kind of documentation, so I have duplicated part of it in the CONFIGURATION.md (which is a nice summary of all settings) and in rdkafka.h (which I think is a nice place where to describe the interaction between all the settings).","From the very useful feedback you gave in #1794.
I am not sure of the best place for this kind of documentation, so I have duplicated part of it in the CONFIGURATION.md (which is a nice summary of all settings) and in rdkafka.h (which I think is a nice place where to describe the interaction between all the settings).",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,1799,2018-05-09T12:24:32Z,2018-06-07T09:59:24Z,2018-06-07T09:59:29Z,MERGED,True,56,6,4,https://github.com/briot,Minor documentation updates from replies to #1794,6,[],https://github.com/edenhill/librdkafka/pull/1799,https://github.com/briot,2,https://github.com/edenhill/librdkafka/pull/1799#issuecomment-388063568,"From the very useful feedback you gave in #1794.
I am not sure of the best place for this kind of documentation, so I have duplicated part of it in the CONFIGURATION.md (which is a nice summary of all settings) and in rdkafka.h (which I think is a nice place where to describe the interaction between all the settings).","Updated as per your comments. Thanks for taking the time to explain (I'll have further changes for the doc based on other email threads, so this was ""testing the water""). The whitespace changes came from my default .vimrc that automatically remove end-of-line spaces. I am curious to know why you chose to preserve them, they are hard to see in diffs.",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,1799,2018-05-09T12:24:32Z,2018-06-07T09:59:24Z,2018-06-07T09:59:29Z,MERGED,True,56,6,4,https://github.com/briot,Minor documentation updates from replies to #1794,6,[],https://github.com/edenhill/librdkafka/pull/1799,https://github.com/edenhill,3,https://github.com/edenhill/librdkafka/pull/1799#issuecomment-388065105,"From the very useful feedback you gave in #1794.
I am not sure of the best place for this kind of documentation, so I have duplicated part of it in the CONFIGURATION.md (which is a nice summary of all settings) and in rdkafka.h (which I think is a nice place where to describe the interaction between all the settings).","librdkafka sources has a mix of tab and whitespace indention and some trailing whitespacing, this all needs to be cleaned up, but instead of doing it incrementally the plan is to put a code style checker in place (astyle) and fix this across the board in one single dedicated commit.
So until that happens I ask people not to make whitespace diffs :)
Looking forward to more documentation updates, they're very valuable!
Meatier docs should go into INTRODUCTION.md (which should probably be renamed MANUAL.md at some point).",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,1799,2018-05-09T12:24:32Z,2018-06-07T09:59:24Z,2018-06-07T09:59:29Z,MERGED,True,56,6,4,https://github.com/briot,Minor documentation updates from replies to #1794,6,[],https://github.com/edenhill/librdkafka/pull/1799,https://github.com/edenhill,4,https://github.com/edenhill/librdkafka/pull/1799#issuecomment-395364555,"From the very useful feedback you gave in #1794.
I am not sure of the best place for this kind of documentation, so I have duplicated part of it in the CONFIGURATION.md (which is a nice summary of all settings) and in rdkafka.h (which I think is a nice place where to describe the interaction between all the settings).","Great stuff, thank you!",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,1800,2018-05-09T12:43:23Z,2018-06-07T09:54:06Z,2018-06-07T09:54:13Z,MERGED,True,9,7,3,https://github.com/raulbocanegra,Update interface compile definitions for Windows using CMake,26,[],https://github.com/edenhill/librdkafka/pull/1800,https://github.com/raulbocanegra,1,https://github.com/edenhill/librdkafka/pull/1800,This fixes #1777,This fixes #1777,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,1800,2018-05-09T12:43:23Z,2018-06-07T09:54:06Z,2018-06-07T09:54:13Z,MERGED,True,9,7,3,https://github.com/raulbocanegra,Update interface compile definitions for Windows using CMake,26,[],https://github.com/edenhill/librdkafka/pull/1800,https://github.com/raulbocanegra,2,https://github.com/edenhill/librdkafka/pull/1800#issuecomment-392334947,This fixes #1777,Hi @edenhill I am not sure why is failing on Travis. It seems to compile and past the tests.,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,1800,2018-05-09T12:43:23Z,2018-06-07T09:54:06Z,2018-06-07T09:54:13Z,MERGED,True,9,7,3,https://github.com/raulbocanegra,Update interface compile definitions for Windows using CMake,26,[],https://github.com/edenhill/librdkafka/pull/1800,https://github.com/edenhill,3,https://github.com/edenhill/librdkafka/pull/1800#issuecomment-393076517,This fixes #1777,"Can you please rebase this branch on master, it has conflicts. Thanks",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,1800,2018-05-09T12:43:23Z,2018-06-07T09:54:06Z,2018-06-07T09:54:13Z,MERGED,True,9,7,3,https://github.com/raulbocanegra,Update interface compile definitions for Windows using CMake,26,[],https://github.com/edenhill/librdkafka/pull/1800,https://github.com/raulbocanegra,4,https://github.com/edenhill/librdkafka/pull/1800#issuecomment-393331050,This fixes #1777,"Still the same Error on Travis:
ERROR: [Errno 2] No such file or directory: '/var/lib/mock/el7-x86_64/root/etc/mtab'
The same is happening on #1799",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,1800,2018-05-09T12:43:23Z,2018-06-07T09:54:06Z,2018-06-07T09:54:13Z,MERGED,True,9,7,3,https://github.com/raulbocanegra,Update interface compile definitions for Windows using CMake,26,[],https://github.com/edenhill/librdkafka/pull/1800,https://github.com/edenhill,5,https://github.com/edenhill/librdkafka/pull/1800#issuecomment-393429992,This fixes #1777,"The travis failure is unrelated (rpm mock is acting up for some reason), you can ignore it.
This branch still seems to have conflicts",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,1800,2018-05-09T12:43:23Z,2018-06-07T09:54:06Z,2018-06-07T09:54:13Z,MERGED,True,9,7,3,https://github.com/raulbocanegra,Update interface compile definitions for Windows using CMake,26,[],https://github.com/edenhill/librdkafka/pull/1800,https://github.com/edenhill,6,https://github.com/edenhill/librdkafka/pull/1800#issuecomment-393430618,This fixes #1777,"I think you want to do this:
$ git fetch  origin  # or the remote that is edenhill/librdkafka.git
$ git rebase origin/master   # or the remote that is edenhill..",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,1800,2018-05-09T12:43:23Z,2018-06-07T09:54:06Z,2018-06-07T09:54:13Z,MERGED,True,9,7,3,https://github.com/raulbocanegra,Update interface compile definitions for Windows using CMake,26,[],https://github.com/edenhill/librdkafka/pull/1800,https://github.com/edenhill,7,https://github.com/edenhill/librdkafka/pull/1800#issuecomment-393789104,This fixes #1777,Still conflicts,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,1800,2018-05-09T12:43:23Z,2018-06-07T09:54:06Z,2018-06-07T09:54:13Z,MERGED,True,9,7,3,https://github.com/raulbocanegra,Update interface compile definitions for Windows using CMake,26,[],https://github.com/edenhill/librdkafka/pull/1800,https://github.com/raulbocanegra,8,https://github.com/edenhill/librdkafka/pull/1800#issuecomment-393990630,This fixes #1777,"I did the following:
$ git fetch  upstream # upstream = https://github.com/edenhill/librdkafka.git
$ git rebase upstream/master

Then resolved all the conflicts. After that, my branch origin/master and the current diverged so I did a git merge with origin (git@github.com:raulbocanegra/librdkafka.git) that showed conflicts which I resolved again and pushed. Hope this works.
Sorry for all the ping pong but I didn't notice the conflict until the rebase, GitHub says there is no conflicts.",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,1800,2018-05-09T12:43:23Z,2018-06-07T09:54:06Z,2018-06-07T09:54:13Z,MERGED,True,9,7,3,https://github.com/raulbocanegra,Update interface compile definitions for Windows using CMake,26,[],https://github.com/edenhill/librdkafka/pull/1800,https://github.com/edenhill,9,https://github.com/edenhill/librdkafka/pull/1800#issuecomment-394266909,This fixes #1777,"Sorry, but github still reports conflicts.
It might be easier to checkout a new branch based on updated origin/master and then cherrypick your changes from your existing branch, then force-pushing the new branch to your PR branch, or opening a new PR.",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,1800,2018-05-09T12:43:23Z,2018-06-07T09:54:06Z,2018-06-07T09:54:13Z,MERGED,True,9,7,3,https://github.com/raulbocanegra,Update interface compile definitions for Windows using CMake,26,[],https://github.com/edenhill/librdkafka/pull/1800,https://github.com/rbocanegra,10,https://github.com/edenhill/librdkafka/pull/1800#issuecomment-394343261,This fixes #1777,I'll try that,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,1800,2018-05-09T12:43:23Z,2018-06-07T09:54:06Z,2018-06-07T09:54:13Z,MERGED,True,9,7,3,https://github.com/raulbocanegra,Update interface compile definitions for Windows using CMake,26,[],https://github.com/edenhill/librdkafka/pull/1800,https://github.com/edenhill,11,https://github.com/edenhill/librdkafka/pull/1800#issuecomment-395363033,This fixes #1777,Thank you for this!,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,1805,2018-05-15T09:45:13Z,2018-06-04T10:59:44Z,2018-06-04T10:59:44Z,MERGED,True,2954,193,38,https://github.com/edenhill,"More stats, histograms (for #1798)",16,['enhancement'],https://github.com/edenhill/librdkafka/pull/1805,https://github.com/edenhill,1,https://github.com/edenhill/librdkafka/pull/1805,,,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,1805,2018-05-15T09:45:13Z,2018-06-04T10:59:44Z,2018-06-04T10:59:44Z,MERGED,True,2954,193,38,https://github.com/edenhill,"More stats, histograms (for #1798)",16,['enhancement'],https://github.com/edenhill/librdkafka/pull/1805,https://github.com/mhowlett,2,https://github.com/edenhill/librdkafka/pull/1805#issuecomment-389682476,,"LGTM apart from some minor comments + complaining about docs.
I do note a lack of testing, but that's fine with me if:

you've done some manual testing and checked the numbers are what you'd expect.
valgrind is happy.

Structurally, i'm definitely happy. I could have easily missed issues in the detail.",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,1805,2018-05-15T09:45:13Z,2018-06-04T10:59:44Z,2018-06-04T10:59:44Z,MERGED,True,2954,193,38,https://github.com/edenhill,"More stats, histograms (for #1798)",16,['enhancement'],https://github.com/edenhill/librdkafka/pull/1805,https://github.com/edenhill,3,https://github.com/edenhill/librdkafka/pull/1805#issuecomment-389982924,,"Really great review, @mhowlett !",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,1805,2018-05-15T09:45:13Z,2018-06-04T10:59:44Z,2018-06-04T10:59:44Z,MERGED,True,2954,193,38,https://github.com/edenhill,"More stats, histograms (for #1798)",16,['enhancement'],https://github.com/edenhill/librdkafka/pull/1805,https://github.com/mhowlett,4,https://github.com/edenhill/librdkafka/pull/1805#issuecomment-391158418,,"this is LGTM (again!) with a side note that I'll be using it tomorrow (i.e. doing some intensive testing), so maybe wait a day before merging in case anything else comes up. but i'm done otherwise.",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,1807,2018-05-16T03:51:24Z,2018-05-16T06:22:23Z,2018-05-16T06:22:34Z,MERGED,True,1,0,1,https://github.com/sidhpurwala-huzaifa,Missing return on error causes use-after-free in SASL code,1,[],https://github.com/edenhill/librdkafka/pull/1807,https://github.com/sidhpurwala-huzaifa,1,https://github.com/edenhill/librdkafka/pull/1807,"rd_kafka_sasl_scram_handle_server_first_message() in rdkafka_sasl_scram.c has a missing return in the case of ""if (rd_base64_decode(&salt_b64, &salt) == -1)""
This results in salt_b64.ptr being free'ed twice and causes a use-after-free. Does not seem like a security issue to me, since it is not controlled by the attacker, but still worth fixing imo,","rd_kafka_sasl_scram_handle_server_first_message() in rdkafka_sasl_scram.c has a missing return in the case of ""if (rd_base64_decode(&salt_b64, &salt) == -1)""
This results in salt_b64.ptr being free'ed twice and causes a use-after-free. Does not seem like a security issue to me, since it is not controlled by the attacker, but still worth fixing imo,",True,{'THUMBS_UP': ['https://github.com/attritionorg']}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,1807,2018-05-16T03:51:24Z,2018-05-16T06:22:23Z,2018-05-16T06:22:34Z,MERGED,True,1,0,1,https://github.com/sidhpurwala-huzaifa,Missing return on error causes use-after-free in SASL code,1,[],https://github.com/edenhill/librdkafka/pull/1807,https://github.com/edenhill,2,https://github.com/edenhill/librdkafka/pull/1807#issuecomment-389408120,"rd_kafka_sasl_scram_handle_server_first_message() in rdkafka_sasl_scram.c has a missing return in the case of ""if (rd_base64_decode(&salt_b64, &salt) == -1)""
This results in salt_b64.ptr being free'ed twice and causes a use-after-free. Does not seem like a security issue to me, since it is not controlled by the attacker, but still worth fixing imo,",Thank you!,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,1809,2018-05-17T09:16:43Z,2018-05-28T09:13:42Z,2018-06-14T13:01:08Z,MERGED,True,49,0,4,https://github.com/jvgutierrez,Implement ssl.curves.list and ssl.sigalgs.list configuration settings,1,[],https://github.com/edenhill/librdkafka/pull/1809,https://github.com/jvgutierrez,1,https://github.com/edenhill/librdkafka/pull/1809,"At WMF we are conducting a TLS security review of our kafka stack. As part of this review we identified that we need to:

Restrict the Eliptic curves (ssl.curves.list) being used.
Restrict the certificate signature algorithms (ssl.sigalgs.list) being used.

Both settings can be controlled using the following OpenSSL API functions introduced in OpenSSL 1.0.2:

SSL_CTX_set1_curves_list(3)
SSL_CTX_set1_sigalgs_list(3)

I've successfully tested the submitted code using the provided rdkafka_example with the following parameters:
examples/rdkafka_example -P -b localhost:4433 -t test -d SECURITY -X security.protocol=SSL -X ssl.cipher.suites=ECDHE-ECDSA-AES256-GCM-SHA384 -X ssl.ca.location=ca/ca.pem -X ssl.certificate.location=ca/prototype-client.pem -X ssl.key.location=ca/prototype-client-key.pem -X ssl.curves.list=P-256 -X ssl.sigalgs.list=ECDSA+SHA256

On the server side I've used openssl s_server. Here is the output from the server without and with the new settings:
without ssl.curves.list and without ssl.sigalgs.list:
Signature Algorithms: RSA+SHA512:DSA+SHA512:ECDSA+SHA512:RSA+SHA384:DSA+SHA384:ECDSA+SHA384:RSA+SHA256:DSA+SHA256:ECDSA+SHA256:RSA+SHA224:DSA+SHA224:ECDSA+SHA224:RSA+SHA1:DSA+SHA1:ECDSA+SHA1
Supported Elliptic Curves: X25519:P-256:P-521:P-384

with -X ssl.curves.list=P-256 -X ssl.sigalgs.list=ECDSA+SHA256:
Shared Signature Algorithms: ECDSA+SHA256
Supported Elliptic Curves: P-256","At WMF we are conducting a TLS security review of our kafka stack. As part of this review we identified that we need to:

Restrict the Eliptic curves (ssl.curves.list) being used.
Restrict the certificate signature algorithms (ssl.sigalgs.list) being used.

Both settings can be controlled using the following OpenSSL API functions introduced in OpenSSL 1.0.2:

SSL_CTX_set1_curves_list(3)
SSL_CTX_set1_sigalgs_list(3)

I've successfully tested the submitted code using the provided rdkafka_example with the following parameters:
examples/rdkafka_example -P -b localhost:4433 -t test -d SECURITY -X security.protocol=SSL -X ssl.cipher.suites=ECDHE-ECDSA-AES256-GCM-SHA384 -X ssl.ca.location=ca/ca.pem -X ssl.certificate.location=ca/prototype-client.pem -X ssl.key.location=ca/prototype-client-key.pem -X ssl.curves.list=P-256 -X ssl.sigalgs.list=ECDSA+SHA256

On the server side I've used openssl s_server. Here is the output from the server without and with the new settings:
without ssl.curves.list and without ssl.sigalgs.list:
Signature Algorithms: RSA+SHA512:DSA+SHA512:ECDSA+SHA512:RSA+SHA384:DSA+SHA384:ECDSA+SHA384:RSA+SHA256:DSA+SHA256:ECDSA+SHA256:RSA+SHA224:DSA+SHA224:ECDSA+SHA224:RSA+SHA1:DSA+SHA1:ECDSA+SHA1
Supported Elliptic Curves: X25519:P-256:P-521:P-384

with -X ssl.curves.list=P-256 -X ssl.sigalgs.list=ECDSA+SHA256:
Shared Signature Algorithms: ECDSA+SHA256
Supported Elliptic Curves: P-256",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,1809,2018-05-17T09:16:43Z,2018-05-28T09:13:42Z,2018-06-14T13:01:08Z,MERGED,True,49,0,4,https://github.com/jvgutierrez,Implement ssl.curves.list and ssl.sigalgs.list configuration settings,1,[],https://github.com/edenhill/librdkafka/pull/1809,https://github.com/jvgutierrez,2,https://github.com/edenhill/librdkafka/pull/1809#issuecomment-389860621,"At WMF we are conducting a TLS security review of our kafka stack. As part of this review we identified that we need to:

Restrict the Eliptic curves (ssl.curves.list) being used.
Restrict the certificate signature algorithms (ssl.sigalgs.list) being used.

Both settings can be controlled using the following OpenSSL API functions introduced in OpenSSL 1.0.2:

SSL_CTX_set1_curves_list(3)
SSL_CTX_set1_sigalgs_list(3)

I've successfully tested the submitted code using the provided rdkafka_example with the following parameters:
examples/rdkafka_example -P -b localhost:4433 -t test -d SECURITY -X security.protocol=SSL -X ssl.cipher.suites=ECDHE-ECDSA-AES256-GCM-SHA384 -X ssl.ca.location=ca/ca.pem -X ssl.certificate.location=ca/prototype-client.pem -X ssl.key.location=ca/prototype-client-key.pem -X ssl.curves.list=P-256 -X ssl.sigalgs.list=ECDSA+SHA256

On the server side I've used openssl s_server. Here is the output from the server without and with the new settings:
without ssl.curves.list and without ssl.sigalgs.list:
Signature Algorithms: RSA+SHA512:DSA+SHA512:ECDSA+SHA512:RSA+SHA384:DSA+SHA384:ECDSA+SHA384:RSA+SHA256:DSA+SHA256:ECDSA+SHA256:RSA+SHA224:DSA+SHA224:ECDSA+SHA224:RSA+SHA1:DSA+SHA1:ECDSA+SHA1
Supported Elliptic Curves: X25519:P-256:P-521:P-384

with -X ssl.curves.list=P-256 -X ssl.sigalgs.list=ECDSA+SHA256:
Shared Signature Algorithms: ECDSA+SHA256
Supported Elliptic Curves: P-256","Oops! sorry about that, hopefully I got it right this time :)",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,1809,2018-05-17T09:16:43Z,2018-05-28T09:13:42Z,2018-06-14T13:01:08Z,MERGED,True,49,0,4,https://github.com/jvgutierrez,Implement ssl.curves.list and ssl.sigalgs.list configuration settings,1,[],https://github.com/edenhill/librdkafka/pull/1809,https://github.com/jvgutierrez,3,https://github.com/edenhill/librdkafka/pull/1809#issuecomment-391254252,"At WMF we are conducting a TLS security review of our kafka stack. As part of this review we identified that we need to:

Restrict the Eliptic curves (ssl.curves.list) being used.
Restrict the certificate signature algorithms (ssl.sigalgs.list) being used.

Both settings can be controlled using the following OpenSSL API functions introduced in OpenSSL 1.0.2:

SSL_CTX_set1_curves_list(3)
SSL_CTX_set1_sigalgs_list(3)

I've successfully tested the submitted code using the provided rdkafka_example with the following parameters:
examples/rdkafka_example -P -b localhost:4433 -t test -d SECURITY -X security.protocol=SSL -X ssl.cipher.suites=ECDHE-ECDSA-AES256-GCM-SHA384 -X ssl.ca.location=ca/ca.pem -X ssl.certificate.location=ca/prototype-client.pem -X ssl.key.location=ca/prototype-client-key.pem -X ssl.curves.list=P-256 -X ssl.sigalgs.list=ECDSA+SHA256

On the server side I've used openssl s_server. Here is the output from the server without and with the new settings:
without ssl.curves.list and without ssl.sigalgs.list:
Signature Algorithms: RSA+SHA512:DSA+SHA512:ECDSA+SHA512:RSA+SHA384:DSA+SHA384:ECDSA+SHA384:RSA+SHA256:DSA+SHA256:ECDSA+SHA256:RSA+SHA224:DSA+SHA224:ECDSA+SHA224:RSA+SHA1:DSA+SHA1:ECDSA+SHA1
Supported Elliptic Curves: X25519:P-256:P-521:P-384

with -X ssl.curves.list=P-256 -X ssl.sigalgs.list=ECDSA+SHA256:
Shared Signature Algorithms: ECDSA+SHA256
Supported Elliptic Curves: P-256",@edenhill anything else to move this PR forward?,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,1809,2018-05-17T09:16:43Z,2018-05-28T09:13:42Z,2018-06-14T13:01:08Z,MERGED,True,49,0,4,https://github.com/jvgutierrez,Implement ssl.curves.list and ssl.sigalgs.list configuration settings,1,[],https://github.com/edenhill/librdkafka/pull/1809,https://github.com/edenhill,4,https://github.com/edenhill/librdkafka/pull/1809#issuecomment-391317538,"At WMF we are conducting a TLS security review of our kafka stack. As part of this review we identified that we need to:

Restrict the Eliptic curves (ssl.curves.list) being used.
Restrict the certificate signature algorithms (ssl.sigalgs.list) being used.

Both settings can be controlled using the following OpenSSL API functions introduced in OpenSSL 1.0.2:

SSL_CTX_set1_curves_list(3)
SSL_CTX_set1_sigalgs_list(3)

I've successfully tested the submitted code using the provided rdkafka_example with the following parameters:
examples/rdkafka_example -P -b localhost:4433 -t test -d SECURITY -X security.protocol=SSL -X ssl.cipher.suites=ECDHE-ECDSA-AES256-GCM-SHA384 -X ssl.ca.location=ca/ca.pem -X ssl.certificate.location=ca/prototype-client.pem -X ssl.key.location=ca/prototype-client-key.pem -X ssl.curves.list=P-256 -X ssl.sigalgs.list=ECDSA+SHA256

On the server side I've used openssl s_server. Here is the output from the server without and with the new settings:
without ssl.curves.list and without ssl.sigalgs.list:
Signature Algorithms: RSA+SHA512:DSA+SHA512:ECDSA+SHA512:RSA+SHA384:DSA+SHA384:ECDSA+SHA384:RSA+SHA256:DSA+SHA256:ECDSA+SHA256:RSA+SHA224:DSA+SHA224:ECDSA+SHA224:RSA+SHA1:DSA+SHA1:ECDSA+SHA1
Supported Elliptic Curves: X25519:P-256:P-521:P-384

with -X ssl.curves.list=P-256 -X ssl.sigalgs.list=ECDSA+SHA256:
Shared Signature Algorithms: ECDSA+SHA256
Supported Elliptic Curves: P-256","I'm happy with this but there's been some discussion whether changes like this should go through a KIP, or at least kafka-dev mailing list discussion, to make sure the client configuration properties are synced between librdkafka and Java.",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,1809,2018-05-17T09:16:43Z,2018-05-28T09:13:42Z,2018-06-14T13:01:08Z,MERGED,True,49,0,4,https://github.com/jvgutierrez,Implement ssl.curves.list and ssl.sigalgs.list configuration settings,1,[],https://github.com/edenhill/librdkafka/pull/1809,https://github.com/jvgutierrez,5,https://github.com/edenhill/librdkafka/pull/1809#issuecomment-392078568,"At WMF we are conducting a TLS security review of our kafka stack. As part of this review we identified that we need to:

Restrict the Eliptic curves (ssl.curves.list) being used.
Restrict the certificate signature algorithms (ssl.sigalgs.list) being used.

Both settings can be controlled using the following OpenSSL API functions introduced in OpenSSL 1.0.2:

SSL_CTX_set1_curves_list(3)
SSL_CTX_set1_sigalgs_list(3)

I've successfully tested the submitted code using the provided rdkafka_example with the following parameters:
examples/rdkafka_example -P -b localhost:4433 -t test -d SECURITY -X security.protocol=SSL -X ssl.cipher.suites=ECDHE-ECDSA-AES256-GCM-SHA384 -X ssl.ca.location=ca/ca.pem -X ssl.certificate.location=ca/prototype-client.pem -X ssl.key.location=ca/prototype-client-key.pem -X ssl.curves.list=P-256 -X ssl.sigalgs.list=ECDSA+SHA256

On the server side I've used openssl s_server. Here is the output from the server without and with the new settings:
without ssl.curves.list and without ssl.sigalgs.list:
Signature Algorithms: RSA+SHA512:DSA+SHA512:ECDSA+SHA512:RSA+SHA384:DSA+SHA384:ECDSA+SHA384:RSA+SHA256:DSA+SHA256:ECDSA+SHA256:RSA+SHA224:DSA+SHA224:ECDSA+SHA224:RSA+SHA1:DSA+SHA1:ECDSA+SHA1
Supported Elliptic Curves: X25519:P-256:P-521:P-384

with -X ssl.curves.list=P-256 -X ssl.sigalgs.list=ECDSA+SHA256:
Shared Signature Algorithms: ECDSA+SHA256
Supported Elliptic Curves: P-256",Awesome! nice to see this PR moving forward :),True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,1809,2018-05-17T09:16:43Z,2018-05-28T09:13:42Z,2018-06-14T13:01:08Z,MERGED,True,49,0,4,https://github.com/jvgutierrez,Implement ssl.curves.list and ssl.sigalgs.list configuration settings,1,[],https://github.com/edenhill/librdkafka/pull/1809,https://github.com/edenhill,6,https://github.com/edenhill/librdkafka/pull/1809#issuecomment-392469698,"At WMF we are conducting a TLS security review of our kafka stack. As part of this review we identified that we need to:

Restrict the Eliptic curves (ssl.curves.list) being used.
Restrict the certificate signature algorithms (ssl.sigalgs.list) being used.

Both settings can be controlled using the following OpenSSL API functions introduced in OpenSSL 1.0.2:

SSL_CTX_set1_curves_list(3)
SSL_CTX_set1_sigalgs_list(3)

I've successfully tested the submitted code using the provided rdkafka_example with the following parameters:
examples/rdkafka_example -P -b localhost:4433 -t test -d SECURITY -X security.protocol=SSL -X ssl.cipher.suites=ECDHE-ECDSA-AES256-GCM-SHA384 -X ssl.ca.location=ca/ca.pem -X ssl.certificate.location=ca/prototype-client.pem -X ssl.key.location=ca/prototype-client-key.pem -X ssl.curves.list=P-256 -X ssl.sigalgs.list=ECDSA+SHA256

On the server side I've used openssl s_server. Here is the output from the server without and with the new settings:
without ssl.curves.list and without ssl.sigalgs.list:
Signature Algorithms: RSA+SHA512:DSA+SHA512:ECDSA+SHA512:RSA+SHA384:DSA+SHA384:ECDSA+SHA384:RSA+SHA256:DSA+SHA256:ECDSA+SHA256:RSA+SHA224:DSA+SHA224:ECDSA+SHA224:RSA+SHA1:DSA+SHA1:ECDSA+SHA1
Supported Elliptic Curves: X25519:P-256:P-521:P-384

with -X ssl.curves.list=P-256 -X ssl.sigalgs.list=ECDSA+SHA256:
Shared Signature Algorithms: ECDSA+SHA256
Supported Elliptic Curves: P-256",Thanks for this!,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,1816,2018-05-23T04:11:32Z,2018-05-23T06:58:28Z,2018-05-23T11:38:50Z,MERGED,True,2,2,1,https://github.com/lins05,Fixed murmur2 partitioner to make it compatible with java version,1,[],https://github.com/edenhill/librdkafka/pull/1816,https://github.com/lins05,1,https://github.com/edenhill/librdkafka/pull/1816,"In kafka java producer, the partition is abs(hash(key)) % num_partition
while in librdkafka it'shash(key) % num_partition.

This incompatibility would cause trouble when the python producer and java producer are used to write to the same topic.","In kafka java producer, the partition is abs(hash(key)) % num_partition
while in librdkafka it'shash(key) % num_partition.

This incompatibility would cause trouble when the python producer and java producer are used to write to the same topic.",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,1816,2018-05-23T04:11:32Z,2018-05-23T06:58:28Z,2018-05-23T11:38:50Z,MERGED,True,2,2,1,https://github.com/lins05,Fixed murmur2 partitioner to make it compatible with java version,1,[],https://github.com/edenhill/librdkafka/pull/1816,https://github.com/lins05,2,https://github.com/edenhill/librdkafka/pull/1816#issuecomment-391215755,"In kafka java producer, the partition is abs(hash(key)) % num_partition
while in librdkafka it'shash(key) % num_partition.

This incompatibility would cause trouble when the python producer and java producer are used to write to the same topic.",The travis-ci build failed for some reason other than the change.,True,{'THUMBS_UP': ['https://github.com/edenhill']}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,1816,2018-05-23T04:11:32Z,2018-05-23T06:58:28Z,2018-05-23T11:38:50Z,MERGED,True,2,2,1,https://github.com/lins05,Fixed murmur2 partitioner to make it compatible with java version,1,[],https://github.com/edenhill/librdkafka/pull/1816,https://github.com/edenhill,3,https://github.com/edenhill/librdkafka/pull/1816#issuecomment-391241215,"In kafka java producer, the partition is abs(hash(key)) % num_partition
while in librdkafka it'shash(key) % num_partition.

This incompatibility would cause trouble when the python producer and java producer are used to write to the same topic.","This is great, thank you!",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,1823,2018-05-25T08:45:58Z,2018-05-28T06:28:05Z,2018-05-28T06:28:12Z,MERGED,True,4,4,1,https://github.com/jcaesar,Some small enhancements to gen-ssl-certs.sh,1,[],https://github.com/edenhill/librdkafka/pull/1823,https://github.com/jcaesar,1,https://github.com/edenhill/librdkafka/pull/1823,"I promised a pull request in #1765 and forgot about it. Not necessarily bad, because I found a few more things. In detail, this changes:

specify the keyalg to be RSA as it may default to DSA, which Java/Kafka(?) does not cope well with
pass the validity period when signing a non-java client key
change the key length to the current ""recommended"" minimum

(The last one might not be the best idea, if you are also using this script in some kind of CI setting. Please tell me what you think.)","I promised a pull request in #1765 and forgot about it. Not necessarily bad, because I found a few more things. In detail, this changes:

specify the keyalg to be RSA as it may default to DSA, which Java/Kafka(?) does not cope well with
pass the validity period when signing a non-java client key
change the key length to the current ""recommended"" minimum

(The last one might not be the best idea, if you are also using this script in some kind of CI setting. Please tell me what you think.)",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,1823,2018-05-25T08:45:58Z,2018-05-28T06:28:05Z,2018-05-28T06:28:12Z,MERGED,True,4,4,1,https://github.com/jcaesar,Some small enhancements to gen-ssl-certs.sh,1,[],https://github.com/edenhill/librdkafka/pull/1823,https://github.com/edenhill,2,https://github.com/edenhill/librdkafka/pull/1823#issuecomment-391986584,"I promised a pull request in #1765 and forgot about it. Not necessarily bad, because I found a few more things. In detail, this changes:

specify the keyalg to be RSA as it may default to DSA, which Java/Kafka(?) does not cope well with
pass the validity period when signing a non-java client key
change the key length to the current ""recommended"" minimum

(The last one might not be the best idea, if you are also using this script in some kind of CI setting. Please tell me what you think.)","Great stuff, thanks!
Is this ready to be merged?",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,1823,2018-05-25T08:45:58Z,2018-05-28T06:28:05Z,2018-05-28T06:28:12Z,MERGED,True,4,4,1,https://github.com/jcaesar,Some small enhancements to gen-ssl-certs.sh,1,[],https://github.com/edenhill/librdkafka/pull/1823,https://github.com/jcaesar,3,https://github.com/edenhill/librdkafka/pull/1823#issuecomment-392414613,"I promised a pull request in #1765 and forgot about it. Not necessarily bad, because I found a few more things. In detail, this changes:

specify the keyalg to be RSA as it may default to DSA, which Java/Kafka(?) does not cope well with
pass the validity period when signing a non-java client key
change the key length to the current ""recommended"" minimum

(The last one might not be the best idea, if you are also using this script in some kind of CI setting. Please tell me what you think.)","Checked it again, should be ok.",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,1823,2018-05-25T08:45:58Z,2018-05-28T06:28:05Z,2018-05-28T06:28:12Z,MERGED,True,4,4,1,https://github.com/jcaesar,Some small enhancements to gen-ssl-certs.sh,1,[],https://github.com/edenhill/librdkafka/pull/1823,https://github.com/edenhill,4,https://github.com/edenhill/librdkafka/pull/1823#issuecomment-392431640,"I promised a pull request in #1765 and forgot about it. Not necessarily bad, because I found a few more things. In detail, this changes:

specify the keyalg to be RSA as it may default to DSA, which Java/Kafka(?) does not cope well with
pass the validity period when signing a non-java client key
change the key length to the current ""recommended"" minimum

(The last one might not be the best idea, if you are also using this script in some kind of CI setting. Please tell me what you think.)",Thanks for this!,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,1826,2018-05-28T17:23:09Z,2018-05-29T17:02:51Z,2018-05-29T17:02:55Z,MERGED,True,67,21,4,https://github.com/edenhill,Proper log message on SSL connection close,1,['bug'],https://github.com/edenhill/librdkafka/pull/1826,https://github.com/edenhill,1,https://github.com/edenhill/librdkafka/pull/1826,Includes honouring log.connection.close=true for SSL disconnects.,Includes honouring log.connection.close=true for SSL disconnects.,True,{'THUMBS_UP': ['https://github.com/rnpridgeon']}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,1826,2018-05-28T17:23:09Z,2018-05-29T17:02:51Z,2018-05-29T17:02:55Z,MERGED,True,67,21,4,https://github.com/edenhill,Proper log message on SSL connection close,1,['bug'],https://github.com/edenhill/librdkafka/pull/1826,https://github.com/mhowlett,2,https://github.com/edenhill/librdkafka/pull/1826#issuecomment-392853758,Includes honouring log.connection.close=true for SSL disconnects.,LGTM aside from two minor comments.,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,1838,2018-06-07T16:21:54Z,2018-06-07T16:53:22Z,2018-06-07T16:53:26Z,MERGED,True,28,23,3,https://github.com/edenhill,socket.nagle.disable=true/TCP_NODELAY was never applied,1,['bug'],https://github.com/edenhill/librdkafka/pull/1838,https://github.com/edenhill,1,https://github.com/edenhill/librdkafka/pull/1838,"..due to lacking include files.
Both socket.nagle.disable and socket.keepalive.enable are
now properly ifdef-guarded in the config definition.","..due to lacking include files.
Both socket.nagle.disable and socket.keepalive.enable are
now properly ifdef-guarded in the config definition.",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,1838,2018-06-07T16:21:54Z,2018-06-07T16:53:22Z,2018-06-07T16:53:26Z,MERGED,True,28,23,3,https://github.com/edenhill,socket.nagle.disable=true/TCP_NODELAY was never applied,1,['bug'],https://github.com/edenhill/librdkafka/pull/1838,https://github.com/mhowlett,2,https://github.com/edenhill/librdkafka/pull/1838#issuecomment-395484357,"..due to lacking include files.
Both socket.nagle.disable and socket.keepalive.enable are
now properly ifdef-guarded in the config definition.",LGTM,True,{'HOORAY': ['https://github.com/rnpridgeon']}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,1844,2018-06-13T07:26:58Z,2018-07-02T13:41:09Z,2018-07-02T13:41:09Z,CLOSED,False,6,3,3,https://github.com/theidexisted,"win32: Make win32_config macro switch:WITH_SSL,WITH_SASL_SCRAM work",1,[],https://github.com/edenhill/librdkafka/pull/1844,https://github.com/theidexisted,1,https://github.com/edenhill/librdkafka/pull/1844,,,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,1844,2018-06-13T07:26:58Z,2018-07-02T13:41:09Z,2018-07-02T13:41:09Z,CLOSED,False,6,3,3,https://github.com/theidexisted,"win32: Make win32_config macro switch:WITH_SSL,WITH_SASL_SCRAM work",1,[],https://github.com/edenhill/librdkafka/pull/1844,https://github.com/theidexisted,2,https://github.com/edenhill/librdkafka/pull/1844#issuecomment-397187504,,doozer/target/stretch-mips,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,1844,2018-06-13T07:26:58Z,2018-07-02T13:41:09Z,2018-07-02T13:41:09Z,CLOSED,False,6,3,3,https://github.com/theidexisted,"win32: Make win32_config macro switch:WITH_SSL,WITH_SASL_SCRAM work",1,[],https://github.com/edenhill/librdkafka/pull/1844,https://github.com/theidexisted,3,https://github.com/edenhill/librdkafka/pull/1844#issuecomment-399815405,,Fixed.,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,1844,2018-06-13T07:26:58Z,2018-07-02T13:41:09Z,2018-07-02T13:41:09Z,CLOSED,False,6,3,3,https://github.com/theidexisted,"win32: Make win32_config macro switch:WITH_SSL,WITH_SASL_SCRAM work",1,[],https://github.com/edenhill/librdkafka/pull/1844,https://github.com/theidexisted,4,https://github.com/edenhill/librdkafka/pull/1844#issuecomment-401274834,,doozer/target/stretch-mips,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,1844,2018-06-13T07:26:58Z,2018-07-02T13:41:09Z,2018-07-02T13:41:09Z,CLOSED,False,6,3,3,https://github.com/theidexisted,"win32: Make win32_config macro switch:WITH_SSL,WITH_SASL_SCRAM work",1,[],https://github.com/edenhill/librdkafka/pull/1844,https://github.com/edenhill,5,https://github.com/edenhill/librdkafka/pull/1844#issuecomment-401807784,,Thank you for your effort but we'll need to decline this submission. see previous comment,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,1845,2018-06-13T18:37:12Z,2018-06-15T23:48:04Z,2018-06-15T23:48:33Z,MERGED,True,90,15,4,https://github.com/edenhill,Don't log connection close for idle connections,1,[],https://github.com/edenhill/librdkafka/pull/1845,https://github.com/edenhill,1,https://github.com/edenhill/librdkafka/pull/1845,,,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,1845,2018-06-13T18:37:12Z,2018-06-15T23:48:04Z,2018-06-15T23:48:33Z,MERGED,True,90,15,4,https://github.com/edenhill,Don't log connection close for idle connections,1,[],https://github.com/edenhill/librdkafka/pull/1845,https://github.com/mhowlett,2,https://github.com/edenhill/librdkafka/pull/1845#issuecomment-397368374,,LGTM,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,1848,2018-06-15T16:29:07Z,2018-06-15T19:31:53Z,2018-06-15T19:31:57Z,MERGED,True,11,6,3,https://github.com/mhowlett,changed queue.buffering.backpressure.threshold default,2,[],https://github.com/edenhill/librdkafka/pull/1848,https://github.com/mhowlett,1,https://github.com/edenhill/librdkafka/pull/1848,,,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,1848,2018-06-15T16:29:07Z,2018-06-15T19:31:53Z,2018-06-15T19:31:57Z,MERGED,True,11,6,3,https://github.com/mhowlett,changed queue.buffering.backpressure.threshold default,2,[],https://github.com/edenhill/librdkafka/pull/1848,https://github.com/edenhill,2,https://github.com/edenhill/librdkafka/pull/1848#issuecomment-397721125,,Thankyou!,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,1850,2018-06-15T23:47:00Z,2018-06-25T14:57:05Z,2018-06-25T14:57:07Z,MERGED,True,72,8,6,https://github.com/edenhill,Add support for set_events(EVENT_ERROR),1,['enhancement'],https://github.com/edenhill/librdkafka/pull/1850,https://github.com/edenhill,1,https://github.com/edenhill/librdkafka/pull/1850,".. which allows errors to be emitted as events.
Needed by Golang client issue 200.",".. which allows errors to be emitted as events.
Needed by Golang client issue 200.",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,1850,2018-06-15T23:47:00Z,2018-06-25T14:57:05Z,2018-06-25T14:57:07Z,MERGED,True,72,8,6,https://github.com/edenhill,Add support for set_events(EVENT_ERROR),1,['enhancement'],https://github.com/edenhill/librdkafka/pull/1850,https://github.com/rnpridgeon,2,https://github.com/edenhill/librdkafka/pull/1850#issuecomment-399968734,".. which allows errors to be emitted as events.
Needed by Golang client issue 200.",LGTM,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,1860,2018-06-24T17:59:38Z,2018-07-02T12:13:06Z,2018-07-02T12:13:09Z,MERGED,True,287,1,7,https://github.com/fede1024,Add callback-based IO event notifications,1,[],https://github.com/edenhill/librdkafka/pull/1860,https://github.com/fede1024,1,https://github.com/edenhill/librdkafka/pull/1860,"See issue #1859.
I've added a test based on 0040. I'm not super happy with the sleep-based waiting loop, but even with a better approach I don't think we'd be able to reduce the execution time of the test by more than 2 seconds. Let me know if you have any suggestion.","See issue #1859.
I've added a test based on 0040. I'm not super happy with the sleep-based waiting loop, but even with a better approach I don't think we'd be able to reduce the execution time of the test by more than 2 seconds. Let me know if you have any suggestion.",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,1860,2018-06-24T17:59:38Z,2018-07-02T12:13:06Z,2018-07-02T12:13:09Z,MERGED,True,287,1,7,https://github.com/fede1024,Add callback-based IO event notifications,1,[],https://github.com/edenhill/librdkafka/pull/1860,https://github.com/edenhill,2,https://github.com/edenhill/librdkafka/pull/1860#issuecomment-400243216,"See issue #1859.
I've added a test based on 0040. I'm not super happy with the sleep-based waiting loop, but even with a better approach I don't think we'd be able to reduce the execution time of the test by more than 2 seconds. Let me know if you have any suggestion.","The branch is conflicting, rebase on latest master.",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,1860,2018-06-24T17:59:38Z,2018-07-02T12:13:06Z,2018-07-02T12:13:09Z,MERGED,True,287,1,7,https://github.com/fede1024,Add callback-based IO event notifications,1,[],https://github.com/edenhill/librdkafka/pull/1860,https://github.com/fede1024,3,https://github.com/edenhill/librdkafka/pull/1860#issuecomment-400662976,"See issue #1859.
I've added a test based on 0040. I'm not super happy with the sleep-based waiting loop, but even with a better approach I don't think we'd be able to reduce the execution time of the test by more than 2 seconds. Let me know if you have any suggestion.","I've rebased, squashed and pushed again. Let me know if you have any additional comment.
Is there any chance this could make it into 0.11.5 or is it too close to release?",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,1860,2018-06-24T17:59:38Z,2018-07-02T12:13:06Z,2018-07-02T12:13:09Z,MERGED,True,287,1,7,https://github.com/fede1024,Add callback-based IO event notifications,1,[],https://github.com/edenhill/librdkafka/pull/1860,https://github.com/edenhill,4,https://github.com/edenhill/librdkafka/pull/1860#issuecomment-401784247,"See issue #1859.
I've added a test based on 0040. I'm not super happy with the sleep-based waiting loop, but even with a better approach I don't think we'd be able to reduce the execution time of the test by more than 2 seconds. Let me know if you have any suggestion.",Thank you!,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,1870,2018-07-02T12:27:02Z,,2019-10-17T07:27:51Z,OPEN,False,66,22,1,https://github.com/edenhill,Block broker thread on event queue instead of IO if IO is irrelevant in current state,1,['bug'],https://github.com/edenhill/librdkafka/pull/1870,https://github.com/edenhill,1,https://github.com/edenhill/librdkafka/pull/1870,"This fixes the case where idle (typically bootstrap) broker threads
are consuming CPU on Windows (with a low socket.blocking.max.ms) due
to aggressively short timeouts on IO.
The drawback is that it might take up to 1000ms to detect disconnects
on idle connections, which should not be a problem in practice.
Related issue:

#1858
confluentinc/confluent-kafka-dotnet#468","This fixes the case where idle (typically bootstrap) broker threads
are consuming CPU on Windows (with a low socket.blocking.max.ms) due
to aggressively short timeouts on IO.
The drawback is that it might take up to 1000ms to detect disconnects
on idle connections, which should not be a problem in practice.
Related issue:

#1858
confluentinc/confluent-kafka-dotnet#468",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,1870,2018-07-02T12:27:02Z,,2019-10-17T07:27:51Z,OPEN,False,66,22,1,https://github.com/edenhill,Block broker thread on event queue instead of IO if IO is irrelevant in current state,1,['bug'],https://github.com/edenhill/librdkafka/pull/1870,https://github.com/wuqingjun,2,https://github.com/edenhill/librdkafka/pull/1870#issuecomment-404636497,"This fixes the case where idle (typically bootstrap) broker threads
are consuming CPU on Windows (with a low socket.blocking.max.ms) due
to aggressively short timeouts on IO.
The drawback is that it might take up to 1000ms to detect disconnects
on idle connections, which should not be a problem in practice.
Related issue:

#1858
confluentinc/confluent-kafka-dotnet#468",Is this PR under review?,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,1870,2018-07-02T12:27:02Z,,2019-10-17T07:27:51Z,OPEN,False,66,22,1,https://github.com/edenhill,Block broker thread on event queue instead of IO if IO is irrelevant in current state,1,['bug'],https://github.com/edenhill/librdkafka/pull/1870,https://github.com/edenhill,3,https://github.com/edenhill/librdkafka/pull/1870#issuecomment-413123351,"This fixes the case where idle (typically bootstrap) broker threads
are consuming CPU on Windows (with a low socket.blocking.max.ms) due
to aggressively short timeouts on IO.
The drawback is that it might take up to 1000ms to detect disconnects
on idle connections, which should not be a problem in practice.
Related issue:

#1858
confluentinc/confluent-kafka-dotnet#468","I'm not sure this is the best approach.
We should considering using forwardable condvars, rather than the entire queue, but that's a bigger change.
PR #1930 enables low-latency wakeups on Windows by using TCP sockets, so that might a better way forward now.",True,{'THUMBS_UP': ['https://github.com/rnpridgeon']}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,1870,2018-07-02T12:27:02Z,,2019-10-17T07:27:51Z,OPEN,False,66,22,1,https://github.com/edenhill,Block broker thread on event queue instead of IO if IO is irrelevant in current state,1,['bug'],https://github.com/edenhill/librdkafka/pull/1870,https://github.com/rnpridgeon,4,https://github.com/edenhill/librdkafka/pull/1870#issuecomment-542738238,"This fixes the case where idle (typically bootstrap) broker threads
are consuming CPU on Windows (with a low socket.blocking.max.ms) due
to aggressively short timeouts on IO.
The drawback is that it might take up to 1000ms to detect disconnects
on idle connections, which should not be a problem in practice.
Related issue:

#1858
confluentinc/confluent-kafka-dotnet#468",Is this still needed since the merger of #1930? If not should we go ahead and close this one out,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,1870,2018-07-02T12:27:02Z,,2019-10-17T07:27:51Z,OPEN,False,66,22,1,https://github.com/edenhill,Block broker thread on event queue instead of IO if IO is irrelevant in current state,1,['bug'],https://github.com/edenhill/librdkafka/pull/1870,https://github.com/edenhill,5,https://github.com/edenhill/librdkafka/pull/1870#issuecomment-543044655,"This fixes the case where idle (typically bootstrap) broker threads
are consuming CPU on Windows (with a low socket.blocking.max.ms) due
to aggressively short timeouts on IO.
The drawback is that it might take up to 1000ms to detect disconnects
on idle connections, which should not be a problem in practice.
Related issue:

#1858
confluentinc/confluent-kafka-dotnet#468",I'm keeping this open as a reminder to check if it still applies,True,{'THUMBS_UP': ['https://github.com/rnpridgeon']}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,1881,2018-07-14T00:03:09Z,2018-07-14T00:30:31Z,2018-07-14T00:30:31Z,CLOSED,False,521,17,9,https://github.com/noahdav,Provide SSL callbacks to handle retrieving and processing certificates,27,[],https://github.com/edenhill/librdkafka/pull/1881,https://github.com/noahdav,1,https://github.com/edenhill/librdkafka/pull/1881,"We have a custom security provider.  In our case we need to be able to allow this security provider to auth the certificate from the broker as well as retrieve the client certificate.  Since this is custom logic providing a callback to the client seems to handle this scenario,","We have a custom security provider.  In our case we need to be able to allow this security provider to auth the certificate from the broker as well as retrieve the client certificate.  Since this is custom logic providing a callback to the client seems to handle this scenario,",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,1882,2018-07-14T00:32:38Z,2019-07-12T12:51:07Z,2019-07-12T12:51:07Z,CLOSED,False,787,5,9,https://github.com/noahdav,Provide SSL callbacks to handle retrieving and processing certificates,61,['enhancement'],https://github.com/edenhill/librdkafka/pull/1882,https://github.com/noahdav,1,https://github.com/edenhill/librdkafka/pull/1882,We have a custom security provider. In our case we need to be able to allow this security provider to auth the certificate from the broker as well as retrieve the client certificate. Since this is custom logic providing a callback to the client seems to handle this scenario.,We have a custom security provider. In our case we need to be able to allow this security provider to auth the certificate from the broker as well as retrieve the client certificate. Since this is custom logic providing a callback to the client seems to handle this scenario.,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,1882,2018-07-14T00:32:38Z,2019-07-12T12:51:07Z,2019-07-12T12:51:07Z,CLOSED,False,787,5,9,https://github.com/noahdav,Provide SSL callbacks to handle retrieving and processing certificates,61,['enhancement'],https://github.com/edenhill/librdkafka/pull/1882,https://github.com/noahdav,2,https://github.com/edenhill/librdkafka/pull/1882#issuecomment-405344055,We have a custom security provider. In our case we need to be able to allow this security provider to auth the certificate from the broker as well as retrieve the client certificate. Since this is custom logic providing a callback to the client seems to handle this scenario.,@edenhill I am seeing this failure in doozer/target/stretch-mips.  I am confused on why this is failing to compile since it is working on other arch.  I do not see any arch specific code here.  I do not have access to this arch to test this on so I am having troubles to fix this break.  Is there any way to test this offline?,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,1882,2018-07-14T00:32:38Z,2019-07-12T12:51:07Z,2019-07-12T12:51:07Z,CLOSED,False,787,5,9,https://github.com/noahdav,Provide SSL callbacks to handle retrieving and processing certificates,61,['enhancement'],https://github.com/edenhill/librdkafka/pull/1882,https://github.com/edenhill,3,https://github.com/edenhill/librdkafka/pull/1882#issuecomment-411022424,We have a custom security provider. In our case we need to be able to allow this security provider to auth the certificate from the broker as well as retrieve the client certificate. Since this is custom logic providing a callback to the client seems to handle this scenario.,"The MIPS build is now fixed on master, so try rebasing your branch.",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,1882,2018-07-14T00:32:38Z,2019-07-12T12:51:07Z,2019-07-12T12:51:07Z,CLOSED,False,787,5,9,https://github.com/noahdav,Provide SSL callbacks to handle retrieving and processing certificates,61,['enhancement'],https://github.com/edenhill/librdkafka/pull/1882,https://github.com/noahdav,4,https://github.com/edenhill/librdkafka/pull/1882#issuecomment-411550026,We have a custom security provider. In our case we need to be able to allow this security provider to auth the certificate from the broker as well as retrieve the client certificate. Since this is custom logic providing a callback to the client seems to handle this scenario.,"@edenhill rebasing did not seem to help.  I tried to asking on OpenSSL but the only help that I received is that there are some issues with the headers for MIPS in 1.0.2.  At this point I am not sure the best way to proceed.  Do you have any suggestions?  If not how do you feel about adding #ifdef to exclude MIPS since this does not seem to work for MIPS?
Since in MIPS apparently the X509_STORE_CTX does not have the cert field we cannot get the peer cert to validate.  X509_store_ctx_get_current_cert will only return the error and in the normal case when no errors occur it returns NULL.",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,1882,2018-07-14T00:32:38Z,2019-07-12T12:51:07Z,2019-07-12T12:51:07Z,CLOSED,False,787,5,9,https://github.com/noahdav,Provide SSL callbacks to handle retrieving and processing certificates,61,['enhancement'],https://github.com/edenhill/librdkafka/pull/1882,https://github.com/edenhill,5,https://github.com/edenhill/librdkafka/pull/1882#issuecomment-411689334,We have a custom security provider. In our case we need to be able to allow this security provider to auth the certificate from the broker as well as retrieve the client certificate. Since this is custom logic providing a callback to the client seems to handle this scenario.,"Okay, let's do this:

Add a configure check to configure.librdkafka that checks for the ctx field
#ifdef out the relevant (smallest) section of rdkafka_transport.c using the define set by the configure check
Add an error return to rd_kafka_conf_set_ssl_verify_cb() that checks the ifdef too, return ERR__NOT_IMPLEMENTED if not set.
Document the @return .. for  ..set_ssl..() and mention it is not supported on MIPS platforms.",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,1882,2018-07-14T00:32:38Z,2019-07-12T12:51:07Z,2019-07-12T12:51:07Z,CLOSED,False,787,5,9,https://github.com/noahdav,Provide SSL callbacks to handle retrieving and processing certificates,61,['enhancement'],https://github.com/edenhill/librdkafka/pull/1882,https://github.com/noahdav,6,https://github.com/edenhill/librdkafka/pull/1882#issuecomment-411922875,We have a custom security provider. In our case we need to be able to allow this security provider to auth the certificate from the broker as well as retrieve the client certificate. Since this is custom logic providing a callback to the client seems to handle this scenario.,"@edenhill After digging into this further it looks like struct x509_store_ctx_st is not defined.
rdkafka_transport.c:682:23: error: dereferencing pointer to incomplete type 'X509_STORE_CTX {aka struct x509_store_ctx_st}'
cert = ctx->cert;
^~
In openssl/ssl.h:
#ifndef OPENSSL_NO_DEPRECATED
#ifndef OPENSSL_NO_X509
#include <openssl/x509.h>
#endif
However I do not see anywhere that we are defining these which would cause x509.h to not be included and therefore x509_vfy.h to not get included either.  I went ahead and #ifdef form MIPS since I do not know what else I can check for at this point.  If you have any ideas why this struct is not being included for MIPS or a better check I am all for improving this.",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,1882,2018-07-14T00:32:38Z,2019-07-12T12:51:07Z,2019-07-12T12:51:07Z,CLOSED,False,787,5,9,https://github.com/noahdav,Provide SSL callbacks to handle retrieving and processing certificates,61,['enhancement'],https://github.com/edenhill/librdkafka/pull/1882,https://github.com/noahdav,7,https://github.com/edenhill/librdkafka/pull/1882#issuecomment-416399126,We have a custom security provider. In our case we need to be able to allow this security provider to auth the certificate from the broker as well as retrieve the client certificate. Since this is custom logic providing a callback to the client seems to handle this scenario.,@edenhill What do we need to do in order to get this merged into master?,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,1882,2018-07-14T00:32:38Z,2019-07-12T12:51:07Z,2019-07-12T12:51:07Z,CLOSED,False,787,5,9,https://github.com/noahdav,Provide SSL callbacks to handle retrieving and processing certificates,61,['enhancement'],https://github.com/edenhill/librdkafka/pull/1882,https://github.com/noahdav,8,https://github.com/edenhill/librdkafka/pull/1882#issuecomment-416793834,We have a custom security provider. In our case we need to be able to allow this security provider to auth the certificate from the broker as well as retrieve the client certificate. Since this is custom logic providing a callback to the client seems to handle this scenario.,@edenhill I believe that I addressed all of your comments.  Can you please take another look and let me know if there are still any outstanding issues.  Thanks:),True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,1882,2018-07-14T00:32:38Z,2019-07-12T12:51:07Z,2019-07-12T12:51:07Z,CLOSED,False,787,5,9,https://github.com/noahdav,Provide SSL callbacks to handle retrieving and processing certificates,61,['enhancement'],https://github.com/edenhill/librdkafka/pull/1882,https://github.com/edenhill,9,https://github.com/edenhill/librdkafka/pull/1882#issuecomment-416846172,We have a custom security provider. In our case we need to be able to allow this security provider to auth the certificate from the broker as well as retrieve the client certificate. Since this is custom logic providing a callback to the client seems to handle this scenario.,Can you make a test-case that uses this functionality on Windows?,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,1882,2018-07-14T00:32:38Z,2019-07-12T12:51:07Z,2019-07-12T12:51:07Z,CLOSED,False,787,5,9,https://github.com/noahdav,Provide SSL callbacks to handle retrieving and processing certificates,61,['enhancement'],https://github.com/edenhill/librdkafka/pull/1882,https://github.com/noahdav,10,https://github.com/edenhill/librdkafka/pull/1882#issuecomment-417460301,We have a custom security provider. In our case we need to be able to allow this security provider to auth the certificate from the broker as well as retrieve the client certificate. Since this is custom logic providing a callback to the client seems to handle this scenario.,@edenhill Thank you for your time reviewing this. I went through to again to fix coding styles.  Please respond to my comments as I believe that the others are addressed. Sorry if I missed some of your comments in last iteration. I may of lost a few with how many that there were.  I believe that they should now be addressed.,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,1882,2018-07-14T00:32:38Z,2019-07-12T12:51:07Z,2019-07-12T12:51:07Z,CLOSED,False,787,5,9,https://github.com/noahdav,Provide SSL callbacks to handle retrieving and processing certificates,61,['enhancement'],https://github.com/edenhill/librdkafka/pull/1882,https://github.com/noahdav,11,https://github.com/edenhill/librdkafka/pull/1882#issuecomment-424072722,We have a custom security provider. In our case we need to be able to allow this security provider to auth the certificate from the broker as well as retrieve the client certificate. Since this is custom logic providing a callback to the client seems to handle this scenario.,"@edenhill
I am finally getting back to this.  For the test I am looking at how to do this but I am not seeing any tests which configure kafka.  In order to do this I will need to set up kafka with correct SSL certs.  I assume we will need to generate self signed ones for both client and broker.  Is there a place which does something like this?  If not I can create the tests assuming that the certs exist.",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,1882,2018-07-14T00:32:38Z,2019-07-12T12:51:07Z,2019-07-12T12:51:07Z,CLOSED,False,787,5,9,https://github.com/noahdav,Provide SSL callbacks to handle retrieving and processing certificates,61,['enhancement'],https://github.com/edenhill/librdkafka/pull/1882,https://github.com/edenhill,12,https://github.com/edenhill/librdkafka/pull/1882#issuecomment-427305393,We have a custom security provider. In our case we need to be able to allow this security provider to auth the certificate from the broker as well as retrieve the client certificate. Since this is custom logic providing a callback to the client seems to handle this scenario.,Could you provide an example of how to use this with the Windows key store?,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,1882,2018-07-14T00:32:38Z,2019-07-12T12:51:07Z,2019-07-12T12:51:07Z,CLOSED,False,787,5,9,https://github.com/noahdav,Provide SSL callbacks to handle retrieving and processing certificates,61,['enhancement'],https://github.com/edenhill/librdkafka/pull/1882,https://github.com/edenhill,13,https://github.com/edenhill/librdkafka/pull/1882#issuecomment-427308305,We have a custom security provider. In our case we need to be able to allow this security provider to auth the certificate from the broker as well as retrieve the client certificate. Since this is custom logic providing a callback to the client seems to handle this scenario.,Let's schedule this feature for the 1.0.0 release.,True,{'THUMBS_UP': ['https://github.com/helgeklein']}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,1882,2018-07-14T00:32:38Z,2019-07-12T12:51:07Z,2019-07-12T12:51:07Z,CLOSED,False,787,5,9,https://github.com/noahdav,Provide SSL callbacks to handle retrieving and processing certificates,61,['enhancement'],https://github.com/edenhill/librdkafka/pull/1882,https://github.com/noahdav,14,https://github.com/edenhill/librdkafka/pull/1882#issuecomment-427461747,We have a custom security provider. In our case we need to be able to allow this security provider to auth the certificate from the broker as well as retrieve the client certificate. Since this is custom logic providing a callback to the client seems to handle this scenario.,"Yes I will provide an example code to use this with the windows store.  We have been using it for a few months now internally.  I will write up a sample.
@edenhill When is the 1.0.0 release scheduled?",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,1882,2018-07-14T00:32:38Z,2019-07-12T12:51:07Z,2019-07-12T12:51:07Z,CLOSED,False,787,5,9,https://github.com/noahdav,Provide SSL callbacks to handle retrieving and processing certificates,61,['enhancement'],https://github.com/edenhill/librdkafka/pull/1882,https://github.com/edenhill,15,https://github.com/edenhill/librdkafka/pull/1882#issuecomment-427555097,We have a custom security provider. In our case we need to be able to allow this security provider to auth the certificate from the broker as well as retrieve the client certificate. Since this is custom logic providing a callback to the client seems to handle this scenario.,"Perfect!
1.0.0 is scheduled for November",True,{'THUMBS_UP': ['https://github.com/helgeklein']}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,1882,2018-07-14T00:32:38Z,2019-07-12T12:51:07Z,2019-07-12T12:51:07Z,CLOSED,False,787,5,9,https://github.com/noahdav,Provide SSL callbacks to handle retrieving and processing certificates,61,['enhancement'],https://github.com/edenhill/librdkafka/pull/1882,https://github.com/noahdav,16,https://github.com/edenhill/librdkafka/pull/1882#issuecomment-429166339,We have a custom security provider. In our case we need to be able to allow this security provider to auth the certificate from the broker as well as retrieve the client certificate. Since this is custom logic providing a callback to the client seems to handle this scenario.,@edenhill I updated examples/rdkafka_example.cpp with example of using these callbacks as we discussed on windows.  Please take a look and let me know what else needs to be addressed before this feature can be checked in.,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,1882,2018-07-14T00:32:38Z,2019-07-12T12:51:07Z,2019-07-12T12:51:07Z,CLOSED,False,787,5,9,https://github.com/noahdav,Provide SSL callbacks to handle retrieving and processing certificates,61,['enhancement'],https://github.com/edenhill/librdkafka/pull/1882,https://github.com/noahdav,17,https://github.com/edenhill/librdkafka/pull/1882#issuecomment-431453448,We have a custom security provider. In our case we need to be able to allow this security provider to auth the certificate from the broker as well as retrieve the client certificate. Since this is custom logic providing a callback to the client seems to handle this scenario.,"@edenhill as we discusses to use a certificate in the windows I generated the cert using OpenSSL then converted it to a pfx file which can easily be imported to the Windows cert store
openssl.exe pkcs12 -export -out cert.pfx -inkey client.key -in client.pem",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,1882,2018-07-14T00:32:38Z,2019-07-12T12:51:07Z,2019-07-12T12:51:07Z,CLOSED,False,787,5,9,https://github.com/noahdav,Provide SSL callbacks to handle retrieving and processing certificates,61,['enhancement'],https://github.com/edenhill/librdkafka/pull/1882,https://github.com/noahdav,18,https://github.com/edenhill/librdkafka/pull/1882#issuecomment-439130529,We have a custom security provider. In our case we need to be able to allow this security provider to auth the certificate from the broker as well as retrieve the client certificate. Since this is custom logic providing a callback to the client seems to handle this scenario.,@edenhill any update when we can merge this change?  Or if there are still any outstanding issues please let me know as I think I have addressed the previous comments.,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,1882,2018-07-14T00:32:38Z,2019-07-12T12:51:07Z,2019-07-12T12:51:07Z,CLOSED,False,787,5,9,https://github.com/noahdav,Provide SSL callbacks to handle retrieving and processing certificates,61,['enhancement'],https://github.com/edenhill/librdkafka/pull/1882,https://github.com/LighthouseJ,19,https://github.com/edenhill/librdkafka/pull/1882#issuecomment-440372981,We have a custom security provider. In our case we need to be able to allow this security provider to auth the certificate from the broker as well as retrieve the client certificate. Since this is custom logic providing a callback to the client seems to handle this scenario.,"Hello.  I've been using this branch (noahdav/librdkafka:ssl) for about a week or so but I've had a small issue.  When I define the retrieve_cb in the config, after I create the high-level consumer, the receive_cb function pointer is never copied, so left as a NULL pointer and thus is never executed.
I had to modify a few lines to not skip copying the callback pointer at a particular point.
Attached here is my change.
0001-Not-skipping-copy-of-the-retrieve_cb.patch.txt
I don't believe that this is the best fix for this issue, but it ensured that the receive_cb is still available when it is used at runtime.
The verify_cb may similarly be affected.
Thoughts or any feedback in case I did something wrong?",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,1882,2018-07-14T00:32:38Z,2019-07-12T12:51:07Z,2019-07-12T12:51:07Z,CLOSED,False,787,5,9,https://github.com/noahdav,Provide SSL callbacks to handle retrieving and processing certificates,61,['enhancement'],https://github.com/edenhill/librdkafka/pull/1882,https://github.com/noahdav,20,https://github.com/edenhill/librdkafka/pull/1882#issuecomment-440382924,We have a custom security provider. In our case we need to be able to allow this security provider to auth the certificate from the broker as well as retrieve the client certificate. Since this is custom logic providing a callback to the client seems to handle this scenario.,"@LighthouseJ
Thanks for the feedback.  I am not sure how this callback is being handled any differently than any of the other callbacks.  It should be copied in a similar way.  Can you share your sample code.  Did you look at any other callback?",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,1882,2018-07-14T00:32:38Z,2019-07-12T12:51:07Z,2019-07-12T12:51:07Z,CLOSED,False,787,5,9,https://github.com/noahdav,Provide SSL callbacks to handle retrieving and processing certificates,61,['enhancement'],https://github.com/edenhill/librdkafka/pull/1882,https://github.com/LighthouseJ,21,https://github.com/edenhill/librdkafka/pull/1882#issuecomment-440390343,We have a custom security provider. In our case we need to be able to allow this security provider to auth the certificate from the broker as well as retrieve the client certificate. Since this is custom logic providing a callback to the client seems to handle this scenario.,"@noahdav I'm not sure either.  I hoped @edenhill would comment if my patch is appropriate or not.  I've been using the SSL soft certs in the high-level consumer without issue so I was under the (possibly false) impression that when the high-level consumer is created and the provided config is duplicated, that these two new callback function pointers are not copied since the dupe code (what calls that function) does not correct discern between fields that should be copied and rdkafka system fields that shouldn't.
I will try and reproduce on the branch and see if I can demonstrate it.",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,1882,2018-07-14T00:32:38Z,2019-07-12T12:51:07Z,2019-07-12T12:51:07Z,CLOSED,False,787,5,9,https://github.com/noahdav,Provide SSL callbacks to handle retrieving and processing certificates,61,['enhancement'],https://github.com/edenhill/librdkafka/pull/1882,https://github.com/edenhill,22,https://github.com/edenhill/librdkafka/pull/1882#issuecomment-440574076,We have a custom security provider. In our case we need to be able to allow this security provider to auth the certificate from the broker as well as retrieve the client certificate. Since this is custom logic providing a callback to the client seems to handle this scenario.,"I'm rebasing and making some final adjustments to this PR, I'll have a look at the callback issue when that's done.",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,1882,2018-07-14T00:32:38Z,2019-07-12T12:51:07Z,2019-07-12T12:51:07Z,CLOSED,False,787,5,9,https://github.com/noahdav,Provide SSL callbacks to handle retrieving and processing certificates,61,['enhancement'],https://github.com/edenhill/librdkafka/pull/1882,https://github.com/noahdav,23,https://github.com/edenhill/librdkafka/pull/1882#issuecomment-447223239,We have a custom security provider. In our case we need to be able to allow this security provider to auth the certificate from the broker as well as retrieve the client certificate. Since this is custom logic providing a callback to the client seems to handle this scenario.,@edenhill is there any update to when this will merge?  Which release are we targeting?,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,1882,2018-07-14T00:32:38Z,2019-07-12T12:51:07Z,2019-07-12T12:51:07Z,CLOSED,False,787,5,9,https://github.com/noahdav,Provide SSL callbacks to handle retrieving and processing certificates,61,['enhancement'],https://github.com/edenhill/librdkafka/pull/1882,https://github.com/edenhill,24,https://github.com/edenhill/librdkafka/pull/1882#issuecomment-447239649,We have a custom security provider. In our case we need to be able to allow this security provider to auth the certificate from the broker as well as retrieve the client certificate. Since this is custom logic providing a callback to the client seems to handle this scenario.,"@noahdav Sorry for the delay. I'm currently busy with v1.0.0 release tasks, this PR will not make the v1.0.0 release but will be merged soon after for v1.0.1",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,1882,2018-07-14T00:32:38Z,2019-07-12T12:51:07Z,2019-07-12T12:51:07Z,CLOSED,False,787,5,9,https://github.com/noahdav,Provide SSL callbacks to handle retrieving and processing certificates,61,['enhancement'],https://github.com/edenhill/librdkafka/pull/1882,https://github.com/noahdav,25,https://github.com/edenhill/librdkafka/pull/1882#issuecomment-447244707,We have a custom security provider. In our case we need to be able to allow this security provider to auth the certificate from the broker as well as retrieve the client certificate. Since this is custom logic providing a callback to the client seems to handle this scenario.,@edenhill Sounds great.  Thanks for the update.  This will be great for at least us to get in the official branch.,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,1882,2018-07-14T00:32:38Z,2019-07-12T12:51:07Z,2019-07-12T12:51:07Z,CLOSED,False,787,5,9,https://github.com/noahdav,Provide SSL callbacks to handle retrieving and processing certificates,61,['enhancement'],https://github.com/edenhill/librdkafka/pull/1882,https://github.com/edenhill,26,https://github.com/edenhill/librdkafka/pull/1882#issuecomment-488752111,We have a custom security provider. In our case we need to be able to allow this security provider to auth the certificate from the broker as well as retrieve the client certificate. Since this is custom logic providing a callback to the client seems to handle this scenario.,"A summary of changes in the upcoming force-push:

The retrieve_cb() has been removed in favour of using standard configuration properties. The reasoning behind this s simplification, reduced code size, cross-language maintenance (some high-level languages, such as Go, can't use callbacks in a good way), and memory ownership issues (having the callback return cert memory to librdkafka in an effective and allocator-agnostic way is tricky).
Refactored the code and made it more in line with the librdkafka style guide.
Values of security-sensitive configuration properties are now cleared from memory when the value is no longer used, or on rd_kafka_t destruction. This is used for ssl.certificate.pem, ssl.key.pem and ssl.key.password.
The ssl_cert_verify_cb was changed to provide more information to the callback, including the full X509_STORE_CTX.
Add integration tests.

Future commits:

Rework the Windows SSL cert store example to its own file.",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,1882,2018-07-14T00:32:38Z,2019-07-12T12:51:07Z,2019-07-12T12:51:07Z,CLOSED,False,787,5,9,https://github.com/noahdav,Provide SSL callbacks to handle retrieving and processing certificates,61,['enhancement'],https://github.com/edenhill/librdkafka/pull/1882,https://github.com/edenhill,27,https://github.com/edenhill/librdkafka/pull/1882#issuecomment-488816057,We have a custom security provider. In our case we need to be able to allow this security provider to auth the certificate from the broker as well as retrieve the client certificate. Since this is custom logic providing a callback to the client seems to handle this scenario.,"@noahdav Can you please have a look at PR #2309, which is based on your PR with the above changes?",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,1887,2018-07-16T13:52:34Z,2018-07-17T09:09:54Z,2018-07-17T09:09:58Z,MERGED,True,21,10,4,https://github.com/edenhill,"Calculate consumer_lag using hi_offset=MAX(app_offset,committed_offset #1878",1,['bug'],https://github.com/edenhill/librdkafka/pull/1887,https://github.com/edenhill,1,https://github.com/edenhill/librdkafka/pull/1887,And reset app_offset to INVALID when the fetcher is stopped.,And reset app_offset to INVALID when the fetcher is stopped.,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,1899,2018-07-26T08:36:43Z,2018-08-15T07:57:20Z,2018-08-15T13:32:03Z,MERGED,True,4,1,1,https://github.com/ankon,Update the message size when a pattern is provided via -m,1,[],https://github.com/edenhill/librdkafka/pull/1899,https://github.com/ankon,1,https://github.com/edenhill/librdkafka/pull/1899,"I wanted to use the performance tool to stress our application, which requires messages with specific content (JSON, authorization parts, types, ...). Using -m to specify the JSON was easy, but I found it very counter-intuitive that it would repeat the pattern/cut it off unless I explicitly also specified the length.
This PR changes this behavior so that the length gets adjusted by default when -m is used. A user that wants cut-off/repeat can specify the length as before by setting it after the -m switch.
For my purposes this is enough, but it would break uses where the caller runs rdkafka_performance -s SIZE -m PATTERN. One approach for avoiding this breakage would be track whether the user set the size explicitly, and only update in -m when that is not the case. Thoughts?","I wanted to use the performance tool to stress our application, which requires messages with specific content (JSON, authorization parts, types, ...). Using -m to specify the JSON was easy, but I found it very counter-intuitive that it would repeat the pattern/cut it off unless I explicitly also specified the length.
This PR changes this behavior so that the length gets adjusted by default when -m is used. A user that wants cut-off/repeat can specify the length as before by setting it after the -m switch.
For my purposes this is enough, but it would break uses where the caller runs rdkafka_performance -s SIZE -m PATTERN. One approach for avoiding this breakage would be track whether the user set the size explicitly, and only update in -m when that is not the case. Thoughts?",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,1899,2018-07-26T08:36:43Z,2018-08-15T07:57:20Z,2018-08-15T13:32:03Z,MERGED,True,4,1,1,https://github.com/ankon,Update the message size when a pattern is provided via -m,1,[],https://github.com/edenhill/librdkafka/pull/1899,https://github.com/mhowlett,2,https://github.com/edenhill/librdkafka/pull/1899#issuecomment-408969724,"I wanted to use the performance tool to stress our application, which requires messages with specific content (JSON, authorization parts, types, ...). Using -m to specify the JSON was easy, but I found it very counter-intuitive that it would repeat the pattern/cut it off unless I explicitly also specified the length.
This PR changes this behavior so that the length gets adjusted by default when -m is used. A user that wants cut-off/repeat can specify the length as before by setting it after the -m switch.
For my purposes this is enough, but it would break uses where the caller runs rdkafka_performance -s SIZE -m PATTERN. One approach for avoiding this breakage would be track whether the user set the size explicitly, and only update in -m when that is not the case. Thoughts?","thanks @ankon - i do like the idea of size automatically being set to the message pattern length in the case it's not specified (implementation as you suggest). i don't think it's a good idea for behavior to depend on order in which flags are specified though - it's arguably manageable for just one option, but allowing this sort of thing would quickly get out of hand.",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,1899,2018-07-26T08:36:43Z,2018-08-15T07:57:20Z,2018-08-15T13:32:03Z,MERGED,True,4,1,1,https://github.com/ankon,Update the message size when a pattern is provided via -m,1,[],https://github.com/edenhill/librdkafka/pull/1899,https://github.com/ankon,3,https://github.com/edenhill/librdkafka/pull/1899#issuecomment-409012839,"I wanted to use the performance tool to stress our application, which requires messages with specific content (JSON, authorization parts, types, ...). Using -m to specify the JSON was easy, but I found it very counter-intuitive that it would repeat the pattern/cut it off unless I explicitly also specified the length.
This PR changes this behavior so that the length gets adjusted by default when -m is used. A user that wants cut-off/repeat can specify the length as before by setting it after the -m switch.
For my purposes this is enough, but it would break uses where the caller runs rdkafka_performance -s SIZE -m PATTERN. One approach for avoiding this breakage would be track whether the user set the size explicitly, and only update in -m when that is not the case. Thoughts?","One idea could be to have a different option for ""size-of-message-with-repeating"". The other idea would be exactly the opposite: Keep -m to mean pattern (and therefore -s to mean repeated-size), and add -M or such to mean ""complete message"". The more I think about it, the more I like the latter -- it wouldn't break any existing code, and just make the life easier for new users.",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,1899,2018-07-26T08:36:43Z,2018-08-15T07:57:20Z,2018-08-15T13:32:03Z,MERGED,True,4,1,1,https://github.com/ankon,Update the message size when a pattern is provided via -m,1,[],https://github.com/edenhill/librdkafka/pull/1899,https://github.com/mhowlett,4,https://github.com/edenhill/librdkafka/pull/1899#issuecomment-409037294,"I wanted to use the performance tool to stress our application, which requires messages with specific content (JSON, authorization parts, types, ...). Using -m to specify the JSON was easy, but I found it very counter-intuitive that it would repeat the pattern/cut it off unless I explicitly also specified the length.
This PR changes this behavior so that the length gets adjusted by default when -m is used. A user that wants cut-off/repeat can specify the length as before by setting it after the -m switch.
For my purposes this is enough, but it would break uses where the caller runs rdkafka_performance -s SIZE -m PATTERN. One approach for avoiding this breakage would be track whether the user set the size explicitly, and only update in -m when that is not the case. Thoughts?","I think I agree with you. I think the interface is better with just two arguments, but your 3 argument suggestion seems very reasonable and as you point out avoids breaking anything (which would seem an important consideration here). Would suggest revising the PR (and @edenhill will review when he's available).",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,1899,2018-07-26T08:36:43Z,2018-08-15T07:57:20Z,2018-08-15T13:32:03Z,MERGED,True,4,1,1,https://github.com/ankon,Update the message size when a pattern is provided via -m,1,[],https://github.com/edenhill/librdkafka/pull/1899,https://github.com/ankon,5,https://github.com/edenhill/librdkafka/pull/1899#issuecomment-409172787,"I wanted to use the performance tool to stress our application, which requires messages with specific content (JSON, authorization parts, types, ...). Using -m to specify the JSON was easy, but I found it very counter-intuitive that it would repeat the pattern/cut it off unless I explicitly also specified the length.
This PR changes this behavior so that the length gets adjusted by default when -m is used. A user that wants cut-off/repeat can specify the length as before by setting it after the -m switch.
For my purposes this is enough, but it would break uses where the caller runs rdkafka_performance -s SIZE -m PATTERN. One approach for avoiding this breakage would be track whether the user set the size explicitly, and only update in -m when that is not the case. Thoughts?","The tool is seriously running out of option characters ... :)
I randomly picked -e now, as that was the first free one, but cannot yet come up with a good name for that.
I'm also looking a bit into the defaulting logic: Ideally the existing uses all stay what they mean, so the logic would be: ""if -e is given we set the message pattern to the value, and adjust the size.""
What is a bit interesting is conflict handling: ""If a -s or -m is given before that -e, the -e wins. If -s or -m is given after -e, then these win."" My argument here would be that someone providing both -e and -s/-m should know what they are doing, since that -e is a new option.
An alternative approach could be to abort with an error if -s/-m is given together with -e.",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,1899,2018-07-26T08:36:43Z,2018-08-15T07:57:20Z,2018-08-15T13:32:03Z,MERGED,True,4,1,1,https://github.com/ankon,Update the message size when a pattern is provided via -m,1,[],https://github.com/edenhill/librdkafka/pull/1899,https://github.com/edenhill,6,https://github.com/edenhill/librdkafka/pull/1899#issuecomment-410630682,"I wanted to use the performance tool to stress our application, which requires messages with specific content (JSON, authorization parts, types, ...). Using -m to specify the JSON was easy, but I found it very counter-intuitive that it would repeat the pattern/cut it off unless I explicitly also specified the length.
This PR changes this behavior so that the length gets adjusted by default when -m is used. A user that wants cut-off/repeat can specify the length as before by setting it after the -m switch.
For my purposes this is enough, but it would break uses where the caller runs rdkafka_performance -s SIZE -m PATTERN. One approach for avoiding this breakage would be track whether the user set the size explicitly, and only update in -m when that is not the case. Thoughts?","I think the most intuitive approach is to let the message size be strlen(-m), unless it was explicitly set with -s. And yes, it should be possible to set -s before -m.
When -s is > -m, repeat -m pattern.
When -s is < -m, truncate -m.",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,1899,2018-07-26T08:36:43Z,2018-08-15T07:57:20Z,2018-08-15T13:32:03Z,MERGED,True,4,1,1,https://github.com/ankon,Update the message size when a pattern is provided via -m,1,[],https://github.com/edenhill/librdkafka/pull/1899,https://github.com/ankon,7,https://github.com/edenhill/librdkafka/pull/1899#issuecomment-411720419,"I wanted to use the performance tool to stress our application, which requires messages with specific content (JSON, authorization parts, types, ...). Using -m to specify the JSON was easy, but I found it very counter-intuitive that it would repeat the pattern/cut it off unless I explicitly also specified the length.
This PR changes this behavior so that the length gets adjusted by default when -m is used. A user that wants cut-off/repeat can specify the length as before by setting it after the -m switch.
For my purposes this is enough, but it would break uses where the caller runs rdkafka_performance -s SIZE -m PATTERN. One approach for avoiding this breakage would be track whether the user set the size explicitly, and only update in -m when that is not the case. Thoughts?","I think the most intuitive approach is to let the message size be strlen(-m), unless it was explicitly set with -s. And yes, it should be possible to set -s before -m.

Works for me! :)",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,1899,2018-07-26T08:36:43Z,2018-08-15T07:57:20Z,2018-08-15T13:32:03Z,MERGED,True,4,1,1,https://github.com/ankon,Update the message size when a pattern is provided via -m,1,[],https://github.com/edenhill/librdkafka/pull/1899,https://github.com/edenhill,8,https://github.com/edenhill/librdkafka/pull/1899#issuecomment-413122948,"I wanted to use the performance tool to stress our application, which requires messages with specific content (JSON, authorization parts, types, ...). Using -m to specify the JSON was easy, but I found it very counter-intuitive that it would repeat the pattern/cut it off unless I explicitly also specified the length.
This PR changes this behavior so that the length gets adjusted by default when -m is used. A user that wants cut-off/repeat can specify the length as before by setting it after the -m switch.
For my purposes this is enough, but it would break uses where the caller runs rdkafka_performance -s SIZE -m PATTERN. One approach for avoiding this breakage would be track whether the user set the size explicitly, and only update in -m when that is not the case. Thoughts?",Great stuff!,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,1901,2018-07-26T19:08:29Z,2018-08-17T07:51:05Z,2018-08-20T13:57:08Z,MERGED,True,3,3,3,https://github.com/secretmike,Use `configure` to test for SSL capabilities to support LibreSSL,1,[],https://github.com/edenhill/librdkafka/pull/1901,https://github.com/secretmike,1,https://github.com/edenhill/librdkafka/pull/1901,"The commit adding support for the ssl.sigalgs.list config option (3cc0ab6) checks the version of openssl (#if OPENSSL_VERSION_NUMBER >= 0x1000200fL) to determine if the SSL_CTX_set1_sigalgs_list() function is available.
Libressl (for some reason) defines the OPENSSL_VERSION_NUMBER as 0x20000000L, but it doesn't (yet) support these newer APIs.
Augment version checks with && !defined(LIBRESSL_VERSION_NUMBER) to compile with LibreSSL.
Closes #1896","The commit adding support for the ssl.sigalgs.list config option (3cc0ab6) checks the version of openssl (#if OPENSSL_VERSION_NUMBER >= 0x1000200fL) to determine if the SSL_CTX_set1_sigalgs_list() function is available.
Libressl (for some reason) defines the OPENSSL_VERSION_NUMBER as 0x20000000L, but it doesn't (yet) support these newer APIs.
Augment version checks with && !defined(LIBRESSL_VERSION_NUMBER) to compile with LibreSSL.
Closes #1896",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,1901,2018-07-26T19:08:29Z,2018-08-17T07:51:05Z,2018-08-20T13:57:08Z,MERGED,True,3,3,3,https://github.com/secretmike,Use `configure` to test for SSL capabilities to support LibreSSL,1,[],https://github.com/edenhill/librdkafka/pull/1901,https://github.com/edenhill,2,https://github.com/edenhill/librdkafka/pull/1901#issuecomment-413787216,"The commit adding support for the ssl.sigalgs.list config option (3cc0ab6) checks the version of openssl (#if OPENSSL_VERSION_NUMBER >= 0x1000200fL) to determine if the SSL_CTX_set1_sigalgs_list() function is available.
Libressl (for some reason) defines the OPENSSL_VERSION_NUMBER as 0x20000000L, but it doesn't (yet) support these newer APIs.
Augment version checks with && !defined(LIBRESSL_VERSION_NUMBER) to compile with LibreSSL.
Closes #1896",Thank you!,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,1906,2018-07-30T23:20:15Z,2018-08-27T20:20:06Z,2018-08-27T20:20:10Z,MERGED,True,2,32,1,https://github.com/chaodhib,Remove callback in KafkaConsumer's example since it is actually not implemeted,1,[],https://github.com/edenhill/librdkafka/pull/1906,https://github.com/chaodhib,1,https://github.com/edenhill/librdkafka/pull/1906,"remove the callback from the high level consumer example since it is not implemented
The callback is not implemented. Yet, it is featured in the example. This is highly confusing as can be shown by these tickets: HERE, HERE  and HERE.","remove the callback from the high level consumer example since it is not implemented
The callback is not implemented. Yet, it is featured in the example. This is highly confusing as can be shown by these tickets: HERE, HERE  and HERE.",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,1906,2018-07-30T23:20:15Z,2018-08-27T20:20:06Z,2018-08-27T20:20:10Z,MERGED,True,2,32,1,https://github.com/chaodhib,Remove callback in KafkaConsumer's example since it is actually not implemeted,1,[],https://github.com/edenhill/librdkafka/pull/1906,https://github.com/chaodhib,2,https://github.com/edenhill/librdkafka/pull/1906#issuecomment-409393218,"remove the callback from the high level consumer example since it is not implemented
The callback is not implemented. Yet, it is featured in the example. This is highly confusing as can be shown by these tickets: HERE, HERE  and HERE.",Maybe the entire commit c09efc1 should be rolled back?,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,1906,2018-07-30T23:20:15Z,2018-08-27T20:20:06Z,2018-08-27T20:20:10Z,MERGED,True,2,32,1,https://github.com/chaodhib,Remove callback in KafkaConsumer's example since it is actually not implemeted,1,[],https://github.com/edenhill/librdkafka/pull/1906,https://github.com/edenhill,3,https://github.com/edenhill/librdkafka/pull/1906#issuecomment-414367216,"remove the callback from the high level consumer example since it is not implemented
The callback is not implemented. Yet, it is featured in the example. This is highly confusing as can be shown by these tickets: HERE, HERE  and HERE.",ping,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,1906,2018-07-30T23:20:15Z,2018-08-27T20:20:06Z,2018-08-27T20:20:10Z,MERGED,True,2,32,1,https://github.com/chaodhib,Remove callback in KafkaConsumer's example since it is actually not implemeted,1,[],https://github.com/edenhill/librdkafka/pull/1906,https://github.com/chaodhib,4,https://github.com/edenhill/librdkafka/pull/1906#issuecomment-414443511,"remove the callback from the high level consumer example since it is not implemented
The callback is not implemented. Yet, it is featured in the example. This is highly confusing as can be shown by these tickets: HERE, HERE  and HERE.",Done. What about the rest of the changes in c09efc1? Should it be kept?,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,1906,2018-07-30T23:20:15Z,2018-08-27T20:20:06Z,2018-08-27T20:20:10Z,MERGED,True,2,32,1,https://github.com/chaodhib,Remove callback in KafkaConsumer's example since it is actually not implemeted,1,[],https://github.com/edenhill/librdkafka/pull/1906,https://github.com/edenhill,5,https://github.com/edenhill/librdkafka/pull/1906#issuecomment-416355121,"remove the callback from the high level consumer example since it is not implemented
The callback is not implemented. Yet, it is featured in the example. This is highly confusing as can be shown by these tickets: HERE, HERE  and HERE.",Thank you!,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,1916,2018-08-03T07:49:10Z,2018-08-05T19:45:02Z,2018-08-05T19:45:06Z,MERGED,True,1,1,1,https://github.com/mattclarke,Missing word in documentation,1,[],https://github.com/edenhill/librdkafka/pull/1916,https://github.com/mattclarke,1,https://github.com/edenhill/librdkafka/pull/1916,Just add a missing word in the documentation for a function.,Just add a missing word in the documentation for a function.,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,1916,2018-08-03T07:49:10Z,2018-08-05T19:45:02Z,2018-08-05T19:45:06Z,MERGED,True,1,1,1,https://github.com/mattclarke,Missing word in documentation,1,[],https://github.com/edenhill/librdkafka/pull/1916,https://github.com/edenhill,2,https://github.com/edenhill/librdkafka/pull/1916#issuecomment-410543152,Just add a missing word in the documentation for a function.,Thank you!,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,1918,2018-08-03T09:01:55Z,2018-08-09T11:28:54Z,2018-08-09T11:37:48Z,MERGED,True,10,4,1,https://github.com/ankon,rdkafka_performance: Don't sleep while waiting for delivery reports,1,[],https://github.com/edenhill/librdkafka/pull/1918,https://github.com/ankon,1,https://github.com/edenhill/librdkafka/pull/1918,"Sleeping here means we won't be looking for delivery reports, which in latency mode
is problematic: The latency is calculated in the delivery report callback based on the time
when that is called, which would be offset by the slept time.
In my case I'm using the tool with low message rates (1-10 messages/s), and the latencies reported
by the producer ended up between 1s (1 message/s) and 100ms (10 messages/s). Looking into this
showed that the sleeping for the rate_sleep means that the tool doesn't process delivery
reports until it has slept -- but the latency is based on the actual time of running the delivery report callback,
not the time the response was accepted by rdkafka.
Fix this problem by replacing the sleep with a busy loop that polls (with a hand-wavy timeout of at most 100ms).
I opted for not adding a latency_mode branch, but just do the polling in either case when the rate limiter is enabled.","Sleeping here means we won't be looking for delivery reports, which in latency mode
is problematic: The latency is calculated in the delivery report callback based on the time
when that is called, which would be offset by the slept time.
In my case I'm using the tool with low message rates (1-10 messages/s), and the latencies reported
by the producer ended up between 1s (1 message/s) and 100ms (10 messages/s). Looking into this
showed that the sleeping for the rate_sleep means that the tool doesn't process delivery
reports until it has slept -- but the latency is based on the actual time of running the delivery report callback,
not the time the response was accepted by rdkafka.
Fix this problem by replacing the sleep with a busy loop that polls (with a hand-wavy timeout of at most 100ms).
I opted for not adding a latency_mode branch, but just do the polling in either case when the rate limiter is enabled.",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,1918,2018-08-03T09:01:55Z,2018-08-09T11:28:54Z,2018-08-09T11:37:48Z,MERGED,True,10,4,1,https://github.com/ankon,rdkafka_performance: Don't sleep while waiting for delivery reports,1,[],https://github.com/edenhill/librdkafka/pull/1918,https://github.com/edenhill,2,https://github.com/edenhill/librdkafka/pull/1918#issuecomment-411726287,"Sleeping here means we won't be looking for delivery reports, which in latency mode
is problematic: The latency is calculated in the delivery report callback based on the time
when that is called, which would be offset by the slept time.
In my case I'm using the tool with low message rates (1-10 messages/s), and the latencies reported
by the producer ended up between 1s (1 message/s) and 100ms (10 messages/s). Looking into this
showed that the sleeping for the rate_sleep means that the tool doesn't process delivery
reports until it has slept -- but the latency is based on the actual time of running the delivery report callback,
not the time the response was accepted by rdkafka.
Fix this problem by replacing the sleep with a busy loop that polls (with a hand-wavy timeout of at most 100ms).
I opted for not adding a latency_mode branch, but just do the polling in either case when the rate limiter is enabled.",Thank you for this!,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,1921,2018-08-06T00:26:37Z,2018-08-06T06:26:26Z,2018-08-06T06:26:39Z,MERGED,True,1,1,1,https://github.com/andoma,rdhdrhistogram: Fix incorrect float -> int cast causing havoc on MIPS.,1,[],https://github.com/edenhill/librdkafka/pull/1921,https://github.com/andoma,1,https://github.com/edenhill/librdkafka/pull/1921,"In rd_hdr_histogram_new() when minValue is 0, log2() will return -inf.
Casting -inf to int is undefined, and on MIPS this results in 2^31
but on other architectures it becomes zero instead (which is desired
in this function). Fix this by changing so the RD_MAX() compare
is done on floating point (0 is > -inf) and thus unitMagnitude
becomes 0 (as expected).
This bug caused the while() loop in said function to never terminate
which in turn caused the unit tests to hang.","In rd_hdr_histogram_new() when minValue is 0, log2() will return -inf.
Casting -inf to int is undefined, and on MIPS this results in 2^31
but on other architectures it becomes zero instead (which is desired
in this function). Fix this by changing so the RD_MAX() compare
is done on floating point (0 is > -inf) and thus unitMagnitude
becomes 0 (as expected).
This bug caused the while() loop in said function to never terminate
which in turn caused the unit tests to hang.",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,1921,2018-08-06T00:26:37Z,2018-08-06T06:26:26Z,2018-08-06T06:26:39Z,MERGED,True,1,1,1,https://github.com/andoma,rdhdrhistogram: Fix incorrect float -> int cast causing havoc on MIPS.,1,[],https://github.com/edenhill/librdkafka/pull/1921,https://github.com/edenhill,2,https://github.com/edenhill/librdkafka/pull/1921#issuecomment-410601898,"In rd_hdr_histogram_new() when minValue is 0, log2() will return -inf.
Casting -inf to int is undefined, and on MIPS this results in 2^31
but on other architectures it becomes zero instead (which is desired
in this function). Fix this by changing so the RD_MAX() compare
is done on floating point (0 is > -inf) and thus unitMagnitude
becomes 0 (as expected).
This bug caused the while() loop in said function to never terminate
which in turn caused the unit tests to hang.","Good find, thank you mr Smas! ",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,1923,2018-08-06T16:00:16Z,2018-08-07T08:27:21Z,2018-08-07T08:27:23Z,MERGED,True,22,7,5,https://github.com/edenhill,Purge and retry buffers in outbuf queue on connection fail (#1913),1,"['bug', 'producer']",https://github.com/edenhill/librdkafka/pull/1923,https://github.com/edenhill,1,https://github.com/edenhill/librdkafka/pull/1923,"There are multiple parts to this fix:

A request/response handler can now check if a failing request
was sent out on the wire (or did not make it past the output
queue).
The retry code now only increments the retry count for actually
sent requests.
When the broker connection goes down, requests in the output queue
are now purged to have their handler callbacks called which in turn
will trigger a (now free) retry.
ProduceRequests are not retried, but their messages are put back
on the partition queue (existing behaviour).","There are multiple parts to this fix:

A request/response handler can now check if a failing request
was sent out on the wire (or did not make it past the output
queue).
The retry code now only increments the retry count for actually
sent requests.
When the broker connection goes down, requests in the output queue
are now purged to have their handler callbacks called which in turn
will trigger a (now free) retry.
ProduceRequests are not retried, but their messages are put back
on the partition queue (existing behaviour).",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,1923,2018-08-06T16:00:16Z,2018-08-07T08:27:21Z,2018-08-07T08:27:23Z,MERGED,True,22,7,5,https://github.com/edenhill,Purge and retry buffers in outbuf queue on connection fail (#1913),1,"['bug', 'producer']",https://github.com/edenhill/librdkafka/pull/1923,https://github.com/chronidev,2,https://github.com/edenhill/librdkafka/pull/1923#issuecomment-410798214,"There are multiple parts to this fix:

A request/response handler can now check if a failing request
was sent out on the wire (or did not make it past the output
queue).
The retry code now only increments the retry count for actually
sent requests.
When the broker connection goes down, requests in the output queue
are now purged to have their handler callbacks called which in turn
will trigger a (now free) retry.
ProduceRequests are not retried, but their messages are put back
on the partition queue (existing behaviour).","I was able to test the code, it seems to correct the issue.",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,1924,2018-08-06T19:14:32Z,2018-08-07T07:42:31Z,2018-08-07T07:42:41Z,MERGED,True,5,3,1,https://github.com/JonECG,consumer: use RD_KAFKA_NO_REPLYQ when calling rd_kafka_seek async,1,['bug'],https://github.com/edenhill/librdkafka/pull/1924,https://github.com/JonECG,1,https://github.com/edenhill/librdkafka/pull/1924,"Fixes an error where a null mtx would attempted to be accessed within
RD_KAFKA_REPLYQ as tmpq would not be created if no timeout is provided.","Fixes an error where a null mtx would attempted to be accessed within
RD_KAFKA_REPLYQ as tmpq would not be created if no timeout is provided.",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,1924,2018-08-06T19:14:32Z,2018-08-07T07:42:31Z,2018-08-07T07:42:41Z,MERGED,True,5,3,1,https://github.com/JonECG,consumer: use RD_KAFKA_NO_REPLYQ when calling rd_kafka_seek async,1,['bug'],https://github.com/edenhill/librdkafka/pull/1924,https://github.com/edenhill,2,https://github.com/edenhill/librdkafka/pull/1924#issuecomment-410965078,"Fixes an error where a null mtx would attempted to be accessed within
RD_KAFKA_REPLYQ as tmpq would not be created if no timeout is provided.",Awesome! Thank you!,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,1929,2018-08-08T21:50:06Z,2018-08-08T22:28:42Z,2018-08-08T22:28:42Z,CLOSED,False,122,35,4,https://github.com/LavaSpider,Enable low latency mode on win32.,1,[],https://github.com/edenhill/librdkafka/pull/1929,https://github.com/LavaSpider,1,https://github.com/edenhill/librdkafka/pull/1929,"Enable low latency mode on windows by using a TCP connection in place of a named pipe.
This allows WSAPoll to wait on both the broker socket and wakeup events.","Enable low latency mode on windows by using a TCP connection in place of a named pipe.
This allows WSAPoll to wait on both the broker socket and wakeup events.",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,1930,2018-08-09T00:06:37Z,2018-08-20T16:21:18Z,2018-12-17T18:39:56Z,MERGED,True,68,38,4,https://github.com/LavaSpider,Enable low latency mode on win32.,6,[],https://github.com/edenhill/librdkafka/pull/1930,https://github.com/LavaSpider,1,https://github.com/edenhill/librdkafka/pull/1930,Enable low latency mode on windows by using a TCP connection in place of a named pipe. This allows WSAPoll to wait on both the broker socket and wakeup events.,Enable low latency mode on windows by using a TCP connection in place of a named pipe. This allows WSAPoll to wait on both the broker socket and wakeup events.,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,1930,2018-08-09T00:06:37Z,2018-08-20T16:21:18Z,2018-12-17T18:39:56Z,MERGED,True,68,38,4,https://github.com/LavaSpider,Enable low latency mode on win32.,6,[],https://github.com/edenhill/librdkafka/pull/1930,https://github.com/edenhill,2,https://github.com/edenhill/librdkafka/pull/1930#issuecomment-412606104,Enable low latency mode on windows by using a TCP connection in place of a named pipe. This allows WSAPoll to wait on both the broker socket and wakeup events.,"Side question:
how does opening a local listener interact with the Windows firewall or other types of firewall software?
There's this ""This application is trying to open a port.."" dialogue popping up every now and then on Windows",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,1930,2018-08-09T00:06:37Z,2018-08-20T16:21:18Z,2018-12-17T18:39:56Z,MERGED,True,68,38,4,https://github.com/LavaSpider,Enable low latency mode on win32.,6,[],https://github.com/edenhill/librdkafka/pull/1930,https://github.com/LavaSpider,3,https://github.com/edenhill/librdkafka/pull/1930#issuecomment-412957580,Enable low latency mode on windows by using a TCP connection in place of a named pipe. This allows WSAPoll to wait on both the broker socket and wakeup events.,"I'm not sure. I'm on a computer where it looks like running a program with my account will auto-create windows firewall rules. If the socket is blocked, then the connect call would time out and it would fall back to not using low latency mode.
It may be better to use an event based approach to this issue rather than doing this TCP socket thing. I liked this since the change was fairly non-intrusive.",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,1930,2018-08-09T00:06:37Z,2018-08-20T16:21:18Z,2018-12-17T18:39:56Z,MERGED,True,68,38,4,https://github.com/LavaSpider,Enable low latency mode on win32.,6,[],https://github.com/edenhill/librdkafka/pull/1930,https://github.com/edenhill,4,https://github.com/edenhill/librdkafka/pull/1930#issuecomment-413124906,Enable low latency mode on windows by using a TCP connection in place of a named pipe. This allows WSAPoll to wait on both the broker socket and wakeup events.,"In your testing of this feature, what latency improvements did you see?",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,1930,2018-08-09T00:06:37Z,2018-08-20T16:21:18Z,2018-12-17T18:39:56Z,MERGED,True,68,38,4,https://github.com/LavaSpider,Enable low latency mode on win32.,6,[],https://github.com/edenhill/librdkafka/pull/1930,https://github.com/edenhill,5,https://github.com/edenhill/librdkafka/pull/1930#issuecomment-413125164,Enable low latency mode on windows by using a TCP connection in place of a named pipe. This allows WSAPoll to wait on both the broker socket and wakeup events.,"There's a bunch of warnings on appveyor x64:
c:\projects\librdkafka\src\rdwin32.h(277): warning C4244: 'function' : conversion from 'SOCKET' to 'int', possible loss of data [C:\projects\librdkafka\win32\librdkafka.vcxproj]
2>c:\projects\librdkafka\src\rdwin32.h(280): warning C4244: 'function' : conversion from 'SOCKET' to 'int', possible loss of data [C:\projects\librdkafka\win32\librdkafka.vcxproj]
https://ci.appveyor.com/project/edenhill/librdkafka/build/0.11.4-R-pre594-fhbmtsrb/job/it1y8nosgcr027pw",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,1930,2018-08-09T00:06:37Z,2018-08-20T16:21:18Z,2018-12-17T18:39:56Z,MERGED,True,68,38,4,https://github.com/LavaSpider,Enable low latency mode on win32.,6,[],https://github.com/edenhill/librdkafka/pull/1930,https://github.com/LavaSpider,6,https://github.com/edenhill/librdkafka/pull/1930#issuecomment-413743027,Enable low latency mode on windows by using a TCP connection in place of a named pipe. This allows WSAPoll to wait on both the broker socket and wakeup events.,"Here is some of the output from my testing, the change significantly reduces latency on windows.

tests are run using default config options.
all tests are on the same machine, run one after the other.
the broker is a three node cluster.
each line represents one second of data collection, the lines were taken in the middle of the test.
the message payload is just a timestamp.
all messages are sent to a topic with one partition and a replication factor of 1.

values are in the format of avg(sum).
timing periods represent latency between the message being enqueued in rd_kafka_msg_partitioner and the reflected position:

msgset_ms - message set creation
send_start_ms - collected right above rd_kafka_broker_send
send_complete_ms - collected below rd_kafka_broker_send when no more slices remain
receipt_latency - receipt received in rd_kafka_handle_Produce
dr_latency - delivery report consumed in rd_kafka_poll_cb

low throughput test: attempt to produce 10 messages per second.
high throughput test: attempt to produce 400000 messages per second.
low latency mode enabled
low throughput
msgset_ms 0.00(0.00), send_start_ms 0.00(0.00), send_complete_ms 0.00(0.00), receipt_latency 7.90(16.00), dr_latency 11.10(16.00), messages_in_set 1.00, message_sets 10, messages 10
msgset_ms 0.00(0.00), send_start_ms 0.00(0.00), send_complete_ms 0.00(0.00), receipt_latency 4.80(16.00), dr_latency 15.90(16.00), messages_in_set 1.00, message_sets 10, messages 10
msgset_ms 0.00(0.00), send_start_ms 0.00(0.00), send_complete_ms 0.00(0.00), receipt_latency 4.80(16.00), dr_latency 8.00(16.00), messages_in_set 1.00, message_sets 10, messages 10
msgset_ms 0.00(0.00), send_start_ms 0.00(0.00), send_complete_ms 0.00(0.00), receipt_latency 8.00(16.00), dr_latency 14.30(16.00), messages_in_set 1.00, message_sets 10, messages 10
msgset_ms 0.00(0.00), send_start_ms 0.00(0.00), send_complete_ms 0.00(0.00), receipt_latency 0.00(0.00), dr_latency 12.60(16.00), messages_in_set 1.00, message_sets 10, messages 10
high throughput
msgset_ms 15.75(47.00), send_start_ms 33.27(94.00), send_complete_ms 44.86(125.00), receipt_latency 80.60(172.00), dr_latency 86.24(172.00), messages_in_set 44.81, message_sets 6265, messages 280732
msgset_ms 26.83(93.00), send_start_ms 52.58(140.00), send_complete_ms 70.16(156.00), receipt_latency 103.70(234.00), dr_latency 116.02(250.00), messages_in_set 45.31, message_sets 5834, messages 264326
msgset_ms 29.26(78.00), send_start_ms 66.06(157.00), send_complete_ms 85.67(187.00), receipt_latency 121.87(234.00), dr_latency 132.65(235.00), messages_in_set 43.51, message_sets 5292, messages 230256
msgset_ms 20.63(94.00), send_start_ms 39.44(110.00), send_complete_ms 55.28(125.00), receipt_latency 87.03(171.00), dr_latency 96.45(172.00), messages_in_set 73.01, message_sets 4078, messages 297745
msgset_ms 20.19(79.00), send_start_ms 38.71(110.00), send_complete_ms 52.47(141.00), receipt_latency 87.63(188.00), dr_latency 97.54(203.00), messages_in_set 42.37, message_sets 5923, messages 250976
low latency mode disabled
low throughput
msgset_ms 906.00(906.00), send_start_ms 906.00(906.00), send_complete_ms 906.00(906.00), receipt_latency 906.00(906.00), dr_latency 906.00(906.00), messages_in_set 10.00, message_sets 1, messages 10
msgset_ms 906.00(906.00), send_start_ms 906.00(906.00), send_complete_ms 906.00(906.00), receipt_latency 922.00(922.00), dr_latency 922.00(922.00), messages_in_set 10.00, message_sets 1, messages 10
msgset_ms 938.00(938.00), send_start_ms 938.00(938.00), send_complete_ms 938.00(938.00), receipt_latency 938.00(938.00), dr_latency 938.00(938.00), messages_in_set 10.00, message_sets 1, messages 10
msgset_ms 922.00(922.00), send_start_ms 922.00(922.00), send_complete_ms 922.00(922.00), receipt_latency 937.00(937.00), dr_latency 937.00(937.00), messages_in_set 10.00, message_sets 1, messages 10
msgset_ms 937.00(937.00), send_start_ms 937.00(937.00), send_complete_ms 937.00(937.00), receipt_latency 937.00(937.00), dr_latency 953.00(953.00), messages_in_set 10.00, message_sets 1, messages 10
high throughput
msgset_ms 2.02(16.00), send_start_ms 2.54(32.00), send_complete_ms 3.52(32.00), receipt_latency 20.88(47.00), dr_latency 27.88(63.00), messages_in_set 66.56, message_sets 2090, messages 139118
msgset_ms 583.05(1000.00), send_start_ms 642.54(1015.00), send_complete_ms 652.97(1015.00), receipt_latency 667.28(1031.00), dr_latency 673.99(1047.00), messages_in_set 2156.44, message_sets 68, messages 146638
msgset_ms 517.44(1000.00), send_start_ms 553.74(1000.00), send_complete_ms 560.87(1000.00), receipt_latency 573.79(1031.00), dr_latency 585.88(1047.00), messages_in_set 181.36, message_sets 911, messages 165222
msgset_ms 4.47(16.00), send_start_ms 5.79(16.00), send_complete_ms 6.19(16.00), receipt_latency 21.58(47.00), dr_latency 26.36(47.00), messages_in_set 122.67, message_sets 365, messages 44774
msgset_ms 275.66(1000.00), send_start_ms 325.06(1015.00), send_complete_ms 332.73(1015.00), receipt_latency 353.20(1031.00), dr_latency 360.99(1031.00), messages_in_set 120.56, message_sets 2591, messages 312380",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,1930,2018-08-09T00:06:37Z,2018-08-20T16:21:18Z,2018-12-17T18:39:56Z,MERGED,True,68,38,4,https://github.com/LavaSpider,Enable low latency mode on win32.,6,[],https://github.com/edenhill/librdkafka/pull/1930,https://github.com/edenhill,7,https://github.com/edenhill/librdkafka/pull/1930#issuecomment-414369880,Enable low latency mode on windows by using a TCP connection in place of a named pipe. This allows WSAPoll to wait on both the broker socket and wakeup events.,"Those are great numbers, good work!",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,1930,2018-08-09T00:06:37Z,2018-08-20T16:21:18Z,2018-12-17T18:39:56Z,MERGED,True,68,38,4,https://github.com/LavaSpider,Enable low latency mode on win32.,6,[],https://github.com/edenhill/librdkafka/pull/1930,https://github.com/edenhill,8,https://github.com/edenhill/librdkafka/pull/1930#issuecomment-414376620,Enable low latency mode on windows by using a TCP connection in place of a named pipe. This allows WSAPoll to wait on both the broker socket and wakeup events.,"Big thanks for this PR, great stuff!",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,1930,2018-08-09T00:06:37Z,2018-08-20T16:21:18Z,2018-12-17T18:39:56Z,MERGED,True,68,38,4,https://github.com/LavaSpider,Enable low latency mode on win32.,6,[],https://github.com/edenhill/librdkafka/pull/1930,https://github.com/fredespo,9,https://github.com/edenhill/librdkafka/pull/1930#issuecomment-447027940,Enable low latency mode on windows by using a TCP connection in place of a named pipe. This allows WSAPoll to wait on both the broker socket and wakeup events.,What is low latency mode in this context? Is this something that needs to be explicitly enabled if I want to use it or is it enabled by default?,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,1930,2018-08-09T00:06:37Z,2018-08-20T16:21:18Z,2018-12-17T18:39:56Z,MERGED,True,68,38,4,https://github.com/LavaSpider,Enable low latency mode on win32.,6,[],https://github.com/edenhill/librdkafka/pull/1930,https://github.com/edenhill,10,https://github.com/edenhill/librdkafka/pull/1930#issuecomment-447105842,Enable low latency mode on windows by using a TCP connection in place of a named pipe. This allows WSAPoll to wait on both the broker socket and wakeup events.,"@fredespo
Current versions of librdkafka requires some tweaking to reduce internal latency for the producer on Windows, namely decreasing socket.blocking.max.ms, which unfortunately comes at more frequent CPU wakeups and thus higher CPU usage.
With the upcoming v1.0.0 release of librdkafka this has been properly fixed and low latency is always provided. The socket.blocking.max.ms will no longer be used.",True,{'THUMBS_UP': ['https://github.com/fredespo']}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,1930,2018-08-09T00:06:37Z,2018-08-20T16:21:18Z,2018-12-17T18:39:56Z,MERGED,True,68,38,4,https://github.com/LavaSpider,Enable low latency mode on win32.,6,[],https://github.com/edenhill/librdkafka/pull/1930,https://github.com/fredespo,11,https://github.com/edenhill/librdkafka/pull/1930#issuecomment-447343524,Enable low latency mode on windows by using a TCP connection in place of a named pipe. This allows WSAPoll to wait on both the broker socket and wakeup events.,"@edenhill
Cool, thanks for the info! Are you sure that feature didn't make it into v0.11.6 though? I see the following release note for that version:
Enable low latency mode on Windows by using TCP ""pipe"". Users no longer need to set socket.blocking.max.ms to improve latency. (#1930, @LavaSpider)",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,1930,2018-08-09T00:06:37Z,2018-08-20T16:21:18Z,2018-12-17T18:39:56Z,MERGED,True,68,38,4,https://github.com/LavaSpider,Enable low latency mode on win32.,6,[],https://github.com/edenhill/librdkafka/pull/1930,https://github.com/edenhill,12,https://github.com/edenhill/librdkafka/pull/1930#issuecomment-447952382,Enable low latency mode on windows by using a TCP connection in place of a named pipe. This allows WSAPoll to wait on both the broker socket and wakeup events.,"Aye, you're right, low latency on Windows was fixed in v0.11.6.",True,{'THUMBS_UP': ['https://github.com/fredespo']}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,1932,2018-08-09T10:06:11Z,2018-08-13T07:25:05Z,2018-08-13T07:25:09Z,MERGED,True,450,103,13,https://github.com/edenhill,Add rd_kafka_destroy_flags(),2,['enhancement'],https://github.com/edenhill/librdkafka/pull/1932,https://github.com/edenhill,1,https://github.com/edenhill/librdkafka/pull/1932,"From Dispose() you would call rd_kafka_destroy_flags(RD_KAFKA_DESTROY_F_NO_CONSUMER_CLOSE).
This should probably be changed in the Python destructor as well.","From Dispose() you would call rd_kafka_destroy_flags(RD_KAFKA_DESTROY_F_NO_CONSUMER_CLOSE).
This should probably be changed in the Python destructor as well.",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,1932,2018-08-09T10:06:11Z,2018-08-13T07:25:05Z,2018-08-13T07:25:09Z,MERGED,True,450,103,13,https://github.com/edenhill,Add rd_kafka_destroy_flags(),2,['enhancement'],https://github.com/edenhill/librdkafka/pull/1932,https://github.com/mhowlett,2,https://github.com/edenhill/librdkafka/pull/1932#issuecomment-411927683,"From Dispose() you would call rd_kafka_destroy_flags(RD_KAFKA_DESTROY_F_NO_CONSUMER_CLOSE).
This should probably be changed in the Python destructor as well.",lgtm with the disclaimer that the logic in this PR is difficult to review properly given limited familiarity with the code.,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,1935,2018-08-10T08:59:48Z,2018-11-15T08:52:21Z,2018-11-15T10:56:08Z,MERGED,True,38,2,2,https://github.com/ankon,Add an option to dump the configuration,1,[],https://github.com/edenhill/librdkafka/pull/1935,https://github.com/ankon,1,https://github.com/edenhill/librdkafka/pull/1935,"This is modelled to match the code in rdkafka_example.c: -X dump will dump
the configuration, and then exit the program successfully.
Fixes #1900","This is modelled to match the code in rdkafka_example.c: -X dump will dump
the configuration, and then exit the program successfully.
Fixes #1900",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,1935,2018-08-10T08:59:48Z,2018-11-15T08:52:21Z,2018-11-15T10:56:08Z,MERGED,True,38,2,2,https://github.com/ankon,Add an option to dump the configuration,1,[],https://github.com/edenhill/librdkafka/pull/1935,https://github.com/edenhill,2,https://github.com/edenhill/librdkafka/pull/1935#issuecomment-438964459,"This is modelled to match the code in rdkafka_example.c: -X dump will dump
the configuration, and then exit the program successfully.
Fixes #1900",Thank you for your contribution!,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,1936,2018-08-10T15:11:07Z,2018-08-12T07:33:16Z,2018-08-13T07:55:41Z,MERGED,True,1,2,1,https://github.com/ankon,Update STATISTICS.md to match code,1,[],https://github.com/edenhill/librdkafka/pull/1936,https://github.com/ankon,1,https://github.com/edenhill/librdkafka/pull/1936,"Apply the changes from d8b3004:

""histoor"" renamed to ""outofrange""
""mean"" removed
reordered","Apply the changes from d8b3004:

""histoor"" renamed to ""outofrange""
""mean"" removed
reordered",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,1936,2018-08-10T15:11:07Z,2018-08-12T07:33:16Z,2018-08-13T07:55:41Z,MERGED,True,1,2,1,https://github.com/ankon,Update STATISTICS.md to match code,1,[],https://github.com/edenhill/librdkafka/pull/1936,https://github.com/edenhill,2,https://github.com/edenhill/librdkafka/pull/1936#issuecomment-412324580,"Apply the changes from d8b3004:

""histoor"" renamed to ""outofrange""
""mean"" removed
reordered",Thank you!,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,1941,2018-08-13T10:04:20Z,2018-08-14T13:25:46Z,2018-08-14T13:25:49Z,MERGED,True,79,37,6,https://github.com/edenhill,Fix timeout reuse in queue calls (#1863),3,['bug'],https://github.com/edenhill/librdkafka/pull/1941,https://github.com/edenhill,1,https://github.com/edenhill/librdkafka/pull/1941,"The timeout_ms parameter was reused as-is in certain loops, causing the timeout to never actually decrease.
This PR fixes this, and optimizes a few other places, to use an absolute end time instead.
Also chucked in an unrelated atomic fix for good measures.
Testing revealed that rd_kafka_flush() was unnecessarily slow, so fixed that as well.","The timeout_ms parameter was reused as-is in certain loops, causing the timeout to never actually decrease.
This PR fixes this, and optimizes a few other places, to use an absolute end time instead.
Also chucked in an unrelated atomic fix for good measures.
Testing revealed that rd_kafka_flush() was unnecessarily slow, so fixed that as well.",True,{'HOORAY': ['https://github.com/manugarri']}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,1944,2018-08-13T13:12:57Z,2018-08-13T15:32:50Z,2018-08-13T15:32:53Z,MERGED,True,39,39,4,https://github.com/edenhill,NuGet: change runtime from win7-.. to more generic win-.. (CLIENTS-1188),1,[],https://github.com/edenhill/librdkafka/pull/1944,https://github.com/edenhill,1,https://github.com/edenhill/librdkafka/pull/1944,confluentinc/confluent-kafka-dotnet#513,confluentinc/confluent-kafka-dotnet#513,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,1945,2018-08-13T15:30:33Z,2018-08-20T17:03:29Z,2018-08-20T17:03:43Z,CLOSED,False,218,7,6,https://github.com/davidtrihy,Early feedback on headers support for librdkafka,2,[],https://github.com/edenhill/librdkafka/pull/1945,https://github.com/davidtrihy,1,https://github.com/edenhill/librdkafka/pull/1945,"Looking for feedback on some changes I've made to support Headers in line with this issue here #1861
As of now there are no tests, documentation isn't up to date and the coding standards are not conforming but this builds fine.
I'm mainly looking for early feedback on the implementation and interface.","Looking for feedback on some changes I've made to support Headers in line with this issue here #1861
As of now there are no tests, documentation isn't up to date and the coding standards are not conforming but this builds fine.
I'm mainly looking for early feedback on the implementation and interface.",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,1945,2018-08-13T15:30:33Z,2018-08-20T17:03:29Z,2018-08-20T17:03:43Z,CLOSED,False,218,7,6,https://github.com/davidtrihy,Early feedback on headers support for librdkafka,2,[],https://github.com/edenhill/librdkafka/pull/1945,https://github.com/davidtrihy,2,https://github.com/edenhill/librdkafka/pull/1945#issuecomment-414390108,"Looking for feedback on some changes I've made to support Headers in line with this issue here #1861
As of now there are no tests, documentation isn't up to date and the coding standards are not conforming but this builds fine.
I'm mainly looking for early feedback on the implementation and interface.",Closing as I'm opening up a new one with a lot more changes,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,1949,2018-08-14T12:16:43Z,2022-01-20T15:35:35Z,2022-01-20T15:35:35Z,CLOSED,False,239,7,8,https://github.com/edenhill,Use astyle as a C style checker,1,[],https://github.com/edenhill/librdkafka/pull/1949,https://github.com/edenhill,1,https://github.com/edenhill/librdkafka/pull/1949,"...using a modified version of astyle that allows
space pad after the function name in the function definition.
This currently only checks the C code since the --style=google
check in astyle is not fully compatible with Google C++ style guide.
Use make checkstyle to report all C style violations,
and make fixstyle to automatically fix them.","...using a modified version of astyle that allows
space pad after the function name in the function definition.
This currently only checks the C code since the --style=google
check in astyle is not fully compatible with Google C++ style guide.
Use make checkstyle to report all C style violations,
and make fixstyle to automatically fix them.",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,1949,2018-08-14T12:16:43Z,2022-01-20T15:35:35Z,2022-01-20T15:35:35Z,CLOSED,False,239,7,8,https://github.com/edenhill,Use astyle as a C style checker,1,[],https://github.com/edenhill/librdkafka/pull/1949,https://github.com/phrocker,2,https://github.com/edenhill/librdkafka/pull/1949#issuecomment-506733568,"...using a modified version of astyle that allows
space pad after the function name in the function definition.
This currently only checks the C code since the --style=google
check in astyle is not fully compatible with Google C++ style guide.
Use make checkstyle to report all C style violations,
and make fixstyle to automatically fix them.",I'm curious why this never moved forward? Seems like a great approach,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,1949,2018-08-14T12:16:43Z,2022-01-20T15:35:35Z,2022-01-20T15:35:35Z,CLOSED,False,239,7,8,https://github.com/edenhill,Use astyle as a C style checker,1,[],https://github.com/edenhill/librdkafka/pull/1949,https://github.com/edenhill,3,https://github.com/edenhill/librdkafka/pull/1949#issuecomment-506843680,"...using a modified version of astyle that allows
space pad after the function name in the function definition.
This currently only checks the C code since the --style=google
check in astyle is not fully compatible with Google C++ style guide.
Use make checkstyle to report all C style violations,
and make fixstyle to automatically fix them.",@phrocker Need to find a good time (no big or critical code changes) for when to fix all the formatting/style issues in the current-tree.,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,1949,2018-08-14T12:16:43Z,2022-01-20T15:35:35Z,2022-01-20T15:35:35Z,CLOSED,False,239,7,8,https://github.com/edenhill,Use astyle as a C style checker,1,[],https://github.com/edenhill/librdkafka/pull/1949,https://github.com/phrocker,4,https://github.com/edenhill/librdkafka/pull/1949#issuecomment-506846816,"...using a modified version of astyle that allows
space pad after the function name in the function definition.
This currently only checks the C code since the --style=google
check in astyle is not fully compatible with Google C++ style guide.
Use make checkstyle to report all C style violations,
and make fixstyle to automatically fix them.",@edenhill I'll try to carve some time to help out if that's cool with you,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,1951,2018-08-14T15:58:54Z,2018-08-14T16:10:17Z,2018-08-14T16:10:21Z,MERGED,True,9,7,2,https://github.com/mhowlett,highlighting legacy properties,1,[],https://github.com/edenhill/librdkafka/pull/1951,https://github.com/mhowlett,1,https://github.com/edenhill/librdkafka/pull/1951,"When reading the config property docs quickly, it's easy to miss that these are legacy properties. I did, as did the reporter of confluentinc/confluent-kafka-dotnet#362","When reading the config property docs quickly, it's easy to miss that these are legacy properties. I did, as did the reporter of confluentinc/confluent-kafka-dotnet#362",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,1951,2018-08-14T15:58:54Z,2018-08-14T16:10:17Z,2018-08-14T16:10:21Z,MERGED,True,9,7,2,https://github.com/mhowlett,highlighting legacy properties,1,[],https://github.com/edenhill/librdkafka/pull/1951,https://github.com/edenhill,2,https://github.com/edenhill/librdkafka/pull/1951#issuecomment-412927281,"When reading the config property docs quickly, it's easy to miss that these are legacy properties. I did, as did the reporter of confluentinc/confluent-kafka-dotnet#362",good stuff!,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,1952,2018-08-15T05:04:33Z,2018-08-15T06:55:04Z,2018-08-15T06:55:04Z,MERGED,True,13,5,1,https://github.com/mhowlett,test unsubscribe doesn't cause destroy_flags to hang,4,[],https://github.com/edenhill/librdkafka/pull/1952,https://github.com/mhowlett,1,https://github.com/edenhill/librdkafka/pull/1952,the test hangs ...,the test hangs ...,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,1953,2018-08-15T08:27:47Z,2018-08-15T13:49:50Z,2018-08-15T13:49:53Z,MERGED,True,21,10,4,https://github.com/edenhill,Fix hang on destroy_flags(NO_CONSUMER_CLOSE) when unsubscribe has been called,2,['bug'],https://github.com/edenhill/librdkafka/pull/1953,https://github.com/edenhill,1,https://github.com/edenhill/librdkafka/pull/1953,,,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,1954,2018-08-15T14:03:56Z,2018-08-15T16:55:57Z,2018-08-15T16:56:00Z,MERGED,True,72,50,9,https://github.com/edenhill,Improved handling of removed partitions,6,['bug'],https://github.com/edenhill/librdkafka/pull/1954,https://github.com/edenhill,1,https://github.com/edenhill/librdkafka/pull/1954,..and some minor other fixes,..and some minor other fixes,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,1954,2018-08-15T14:03:56Z,2018-08-15T16:55:57Z,2018-08-15T16:56:00Z,MERGED,True,72,50,9,https://github.com/edenhill,Improved handling of removed partitions,6,['bug'],https://github.com/edenhill/librdkafka/pull/1954,https://github.com/edenhill,2,https://github.com/edenhill/librdkafka/pull/1954#issuecomment-413208040,..and some minor other fixes,It is probably easiest to review this per commit.,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,1959,2018-08-20T17:03:46Z,2018-11-15T12:59:22Z,2018-11-15T12:59:22Z,CLOSED,False,909,16,12,https://github.com/davidtrihy,Header support for C++ API,5,[],https://github.com/edenhill/librdkafka/pull/1959,https://github.com/davidtrihy,1,https://github.com/edenhill/librdkafka/pull/1959,"Producing and consuming header support in the C++ API.
Two test cases added in the e2e tests, was trying to add test cases for header support for empty/null messages but it appears that is not supported and the headers do not get attached. Is this correct behaviour?
Reference issue #1861","Producing and consuming header support in the C++ API.
Two test cases added in the e2e tests, was trying to add test cases for header support for empty/null messages but it appears that is not supported and the headers do not get attached. Is this correct behaviour?
Reference issue #1861",True,{'THUMBS_UP': ['https://github.com/offlinehacker']}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,1959,2018-08-20T17:03:46Z,2018-11-15T12:59:22Z,2018-11-15T12:59:22Z,CLOSED,False,909,16,12,https://github.com/davidtrihy,Header support for C++ API,5,[],https://github.com/edenhill/librdkafka/pull/1959,https://github.com/alexander-alvarez,2,https://github.com/edenhill/librdkafka/pull/1959#issuecomment-421454377,"Producing and consuming header support in the C++ API.
Two test cases added in the e2e tests, was trying to add test cases for header support for empty/null messages but it appears that is not supported and the headers do not get attached. Is this correct behaviour?
Reference issue #1861",@davidtrihy  thanks so much for pushing this forward! I know you're also looking to get this upstream in node-rdkafka will you have time to address the feedback and conflicts?,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,1959,2018-08-20T17:03:46Z,2018-11-15T12:59:22Z,2018-11-15T12:59:22Z,CLOSED,False,909,16,12,https://github.com/davidtrihy,Header support for C++ API,5,[],https://github.com/edenhill/librdkafka/pull/1959,https://github.com/alexander-alvarez,3,https://github.com/edenhill/librdkafka/pull/1959#issuecomment-423606937,"Producing and consuming header support in the C++ API.
Two test cases added in the e2e tests, was trying to add test cases for header support for empty/null messages but it appears that is not supported and the headers do not get attached. Is this correct behaviour?
Reference issue #1861",bump @davidtrihy,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,1959,2018-08-20T17:03:46Z,2018-11-15T12:59:22Z,2018-11-15T12:59:22Z,CLOSED,False,909,16,12,https://github.com/davidtrihy,Header support for C++ API,5,[],https://github.com/edenhill/librdkafka/pull/1959,https://github.com/davidtrihy,4,https://github.com/edenhill/librdkafka/pull/1959#issuecomment-425033870,"Producing and consuming header support in the C++ API.
Two test cases added in the e2e tests, was trying to add test cases for header support for empty/null messages but it appears that is not supported and the headers do not get attached. Is this correct behaviour?
Reference issue #1861","@alexander-alvarez sorry I've been busy with my day job around this and we're working with the forked version at the moment, I nearly have all the changes requested done but I'm just trying to get the tests working properly again. I'll try and get it done in the next week when I hopefully have some spare time.",True,{'THUMBS_UP': ['https://github.com/Fr8Traindb']}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,1959,2018-08-20T17:03:46Z,2018-11-15T12:59:22Z,2018-11-15T12:59:22Z,CLOSED,False,909,16,12,https://github.com/davidtrihy,Header support for C++ API,5,[],https://github.com/edenhill/librdkafka/pull/1959,https://github.com/alexander-alvarez,5,https://github.com/edenhill/librdkafka/pull/1959#issuecomment-428284113,"Producing and consuming header support in the C++ API.
Two test cases added in the e2e tests, was trying to add test cases for header support for empty/null messages but it appears that is not supported and the headers do not get attached. Is this correct behaviour?
Reference issue #1861","@davidtrihy friendly ping. I'd help out myself, but my C++ is only enough to read it and sort of understand what's going on.",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,1959,2018-08-20T17:03:46Z,2018-11-15T12:59:22Z,2018-11-15T12:59:22Z,CLOSED,False,909,16,12,https://github.com/davidtrihy,Header support for C++ API,5,[],https://github.com/edenhill/librdkafka/pull/1959,https://github.com/davidtrihy,6,https://github.com/edenhill/librdkafka/pull/1959#issuecomment-428293249,"Producing and consuming header support in the C++ API.
Two test cases added in the e2e tests, was trying to add test cases for header support for empty/null messages but it appears that is not supported and the headers do not get attached. Is this correct behaviour?
Reference issue #1861","@alexander-alvarez Hey sorry been very busy but I'll spend some time at it tonight and tomorrow to address the comments, the node-rdkafka maintainer is happy with the changes on that side but I'll address the rest of the issues on this soon!",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,1959,2018-08-20T17:03:46Z,2018-11-15T12:59:22Z,2018-11-15T12:59:22Z,CLOSED,False,909,16,12,https://github.com/davidtrihy,Header support for C++ API,5,[],https://github.com/edenhill/librdkafka/pull/1959,https://github.com/alexander-alvarez,7,https://github.com/edenhill/librdkafka/pull/1959#issuecomment-430026635,"Producing and consuming header support in the C++ API.
Two test cases added in the e2e tests, was trying to add test cases for header support for empty/null messages but it appears that is not supported and the headers do not get attached. Is this correct behaviour?
Reference issue #1861",@davidtrihy Friendly reminder from your neighborly open source C++ freeloader.,True,{'LAUGH': ['https://github.com/davidtrihy']}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,1959,2018-08-20T17:03:46Z,2018-11-15T12:59:22Z,2018-11-15T12:59:22Z,CLOSED,False,909,16,12,https://github.com/davidtrihy,Header support for C++ API,5,[],https://github.com/edenhill/librdkafka/pull/1959,https://github.com/davidtrihy,8,https://github.com/edenhill/librdkafka/pull/1959#issuecomment-430195404,"Producing and consuming header support in the C++ API.
Two test cases added in the e2e tests, was trying to add test cases for header support for empty/null messages but it appears that is not supported and the headers do not get attached. Is this correct behaviour?
Reference issue #1861","@edenhill apologies about the lateness in getting the changes requested for this one, been very busy with work and I needed to find some time! I hope this suffices and if you have any further feedback let me know and I'll try and address ASAP so we can get this over the line.",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,1959,2018-08-20T17:03:46Z,2018-11-15T12:59:22Z,2018-11-15T12:59:22Z,CLOSED,False,909,16,12,https://github.com/davidtrihy,Header support for C++ API,5,[],https://github.com/edenhill/librdkafka/pull/1959,https://github.com/davidtrihy,9,https://github.com/edenhill/librdkafka/pull/1959#issuecomment-430195560,"Producing and consuming header support in the C++ API.
Two test cases added in the e2e tests, was trying to add test cases for header support for empty/null messages but it appears that is not supported and the headers do not get attached. Is this correct behaviour?
Reference issue #1861","@davidtrihy Friendly reminder from your neighborly open source C++ freeloader.

If there's any further feedback I'll address it ASAP so I can get this over the line",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,1959,2018-08-20T17:03:46Z,2018-11-15T12:59:22Z,2018-11-15T12:59:22Z,CLOSED,False,909,16,12,https://github.com/davidtrihy,Header support for C++ API,5,[],https://github.com/edenhill/librdkafka/pull/1959,https://github.com/alexander-alvarez,10,https://github.com/edenhill/librdkafka/pull/1959#issuecomment-430287710,"Producing and consuming header support in the C++ API.
Two test cases added in the e2e tests, was trying to add test cases for header support for empty/null messages but it appears that is not supported and the headers do not get attached. Is this correct behaviour?
Reference issue #1861",Thanks @davidtrihy I owe you a  or  (or whatever you want lol) if you stop by NYC,True,{'THUMBS_UP': ['https://github.com/davidtrihy']}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,1959,2018-08-20T17:03:46Z,2018-11-15T12:59:22Z,2018-11-15T12:59:22Z,CLOSED,False,909,16,12,https://github.com/davidtrihy,Header support for C++ API,5,[],https://github.com/edenhill/librdkafka/pull/1959,https://github.com/alexander-alvarez,11,https://github.com/edenhill/librdkafka/pull/1959#issuecomment-432020150,"Producing and consuming header support in the C++ API.
Two test cases added in the e2e tests, was trying to add test cases for header support for empty/null messages but it appears that is not supported and the headers do not get attached. Is this correct behaviour?
Reference issue #1861","looks like there's more conflicts :(
@edenhill  can we get a look at this once they're fixed up?",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,1959,2018-08-20T17:03:46Z,2018-11-15T12:59:22Z,2018-11-15T12:59:22Z,CLOSED,False,909,16,12,https://github.com/davidtrihy,Header support for C++ API,5,[],https://github.com/edenhill/librdkafka/pull/1959,https://github.com/alexander-alvarez,12,https://github.com/edenhill/librdkafka/pull/1959#issuecomment-435411125,"Producing and consuming header support in the C++ API.
Two test cases added in the e2e tests, was trying to add test cases for header support for empty/null messages but it appears that is not supported and the headers do not get attached. Is this correct behaviour?
Reference issue #1861",@davidtrihy It's close to the finish line,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,1959,2018-08-20T17:03:46Z,2018-11-15T12:59:22Z,2018-11-15T12:59:22Z,CLOSED,False,909,16,12,https://github.com/davidtrihy,Header support for C++ API,5,[],https://github.com/edenhill/librdkafka/pull/1959,https://github.com/alexander-alvarez,13,https://github.com/edenhill/librdkafka/pull/1959#issuecomment-436238685,"Producing and consuming header support in the C++ API.
Two test cases added in the e2e tests, was trying to add test cases for header support for empty/null messages but it appears that is not supported and the headers do not get attached. Is this correct behaviour?
Reference issue #1861",Bump @davidtrihy,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,1959,2018-08-20T17:03:46Z,2018-11-15T12:59:22Z,2018-11-15T12:59:22Z,CLOSED,False,909,16,12,https://github.com/davidtrihy,Header support for C++ API,5,[],https://github.com/edenhill/librdkafka/pull/1959,https://github.com/Fr8Traindb,14,https://github.com/edenhill/librdkafka/pull/1959#issuecomment-438827476,"Producing and consuming header support in the C++ API.
Two test cases added in the e2e tests, was trying to add test cases for header support for empty/null messages but it appears that is not supported and the headers do not get attached. Is this correct behaviour?
Reference issue #1861",I am patiently waiting for this as well.  Any idea on when this will be completed?,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,1959,2018-08-20T17:03:46Z,2018-11-15T12:59:22Z,2018-11-15T12:59:22Z,CLOSED,False,909,16,12,https://github.com/davidtrihy,Header support for C++ API,5,[],https://github.com/edenhill/librdkafka/pull/1959,https://github.com/edenhill,15,https://github.com/edenhill/librdkafka/pull/1959#issuecomment-438988864,"Producing and consuming header support in the C++ API.
Two test cases added in the e2e tests, was trying to add test cases for header support for empty/null messages but it appears that is not supported and the headers do not get attached. Is this correct behaviour?
Reference issue #1861",I'm picking this up to do the finishing touches,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,1959,2018-08-20T17:03:46Z,2018-11-15T12:59:22Z,2018-11-15T12:59:22Z,CLOSED,False,909,16,12,https://github.com/davidtrihy,Header support for C++ API,5,[],https://github.com/edenhill/librdkafka/pull/1959,https://github.com/edenhill,16,https://github.com/edenhill/librdkafka/pull/1959#issuecomment-439031805,"Producing and consuming header support in the C++ API.
Two test cases added in the e2e tests, was trying to add test cases for header support for empty/null messages but it appears that is not supported and the headers do not get attached. Is this correct behaviour?
Reference issue #1861","Made the final changes and created a new PR here: #2109
Please review and test out.",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,1960,2018-08-21T08:42:40Z,2018-08-21T17:59:12Z,2018-08-21T17:59:12Z,MERGED,True,36,28,3,https://github.com/edenhill,Disconnect regardless of socket.max.fails when partial request times out (#1955),1,"['bug', 'CRITICAL']",https://github.com/edenhill/librdkafka/pull/1960,https://github.com/edenhill,1,https://github.com/edenhill/librdkafka/pull/1960,"We can't restore the connection to a clean state when a partial
request times out, instead fail the connection and reconnect,
regardless of the socket.max.fails setting.","We can't restore the connection to a clean state when a partial
request times out, instead fail the connection and reconnect,
regardless of the socket.max.fails setting.",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,1966,2018-08-23T16:11:52Z,2018-08-23T16:49:33Z,2018-08-23T16:49:37Z,MERGED,True,296,5,4,https://github.com/edenhill,Fix crash on partition leader change,4,"['bug', 'producer']",https://github.com/edenhill/librdkafka/pull/1966,https://github.com/edenhill,1,https://github.com/edenhill/librdkafka/pull/1966,,,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,1967,2018-08-23T19:49:18Z,2018-08-25T05:14:15Z,2018-08-25T05:14:18Z,MERGED,True,3,2,2,https://github.com/edenhill,Fix consumer_lag to -1 when neither app_offset or commmitted_offset i,1,"['bug', 'consumer']",https://github.com/edenhill/librdkafka/pull/1967,https://github.com/edenhill,1,https://github.com/edenhill/librdkafka/pull/1967,s available (#1911),s available (#1911),True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,1968,2018-08-24T07:17:44Z,2018-08-25T05:14:37Z,2018-08-25T05:14:39Z,MERGED,True,10,3,3,https://github.com/edenhill,Message err was not set for on_ack interceptors on broker reply (#1892),1,['bug'],https://github.com/edenhill/librdkafka/pull/1968,https://github.com/edenhill,1,https://github.com/edenhill/librdkafka/pull/1968,,,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,1970,2018-08-25T02:56:50Z,2018-08-25T05:07:24Z,2018-08-26T21:13:08Z,MERGED,True,1,1,1,https://github.com/pmbuko,Update batch.num.messages description,1,[],https://github.com/edenhill/librdkafka/pull/1970,https://github.com/pmbuko,1,https://github.com/edenhill/librdkafka/pull/1970,Makes the batch.num.messages description under the Performance header of INTRODUCTION.md agree with the description in CONFIGURATION.md,Makes the batch.num.messages description under the Performance header of INTRODUCTION.md agree with the description in CONFIGURATION.md,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,1970,2018-08-25T02:56:50Z,2018-08-25T05:07:24Z,2018-08-26T21:13:08Z,MERGED,True,1,1,1,https://github.com/pmbuko,Update batch.num.messages description,1,[],https://github.com/edenhill/librdkafka/pull/1970,https://github.com/edenhill,2,https://github.com/edenhill/librdkafka/pull/1970#issuecomment-415937708,Makes the batch.num.messages description under the Performance header of INTRODUCTION.md agree with the description in CONFIGURATION.md,Thank you!,True,{'THUMBS_UP': ['https://github.com/pmbuko']}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,1971,2018-08-27T07:19:54Z,2018-08-27T19:49:52Z,2018-08-27T19:49:56Z,MERGED,True,52,32,10,https://github.com/edenhill,Improve disconnect detection on Windows #1937,5,['bug'],https://github.com/edenhill/librdkafka/pull/1971,https://github.com/edenhill,1,https://github.com/edenhill/librdkafka/pull/1971,,,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,1972,2018-08-27T08:51:34Z,2018-08-27T16:05:48Z,2018-08-27T16:05:50Z,MERGED,True,2,2,1,https://github.com/edenhill,Use atomics for refcounts on all platforms with atomics support (#1873),1,[],https://github.com/edenhill/librdkafka/pull/1972,https://github.com/edenhill,1,https://github.com/edenhill/librdkafka/pull/1972,,,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,1978,2018-08-30T20:40:54Z,2021-03-25T08:30:20Z,2021-03-25T08:30:20Z,CLOSED,False,15,13,2,https://github.com/mutability,Fix warnings on FreeBSD; it is very picky about _POSIX_C_SOURCE et al,1,[],https://github.com/edenhill/librdkafka/pull/1978,https://github.com/mutability,1,https://github.com/edenhill/librdkafka/pull/1978,"This fixes the warnings in #1962 on FreeBSD
This turned out to be a bit of a rabbit hole: FreeBSD disables all the usual default visibility if you set _POSIX_C_SOURCE at all, so the warnings were coming from the implicit _XOPEN_SOURCE=700 being disabled.
Also, there doesn't seem to be any way to get the standard headers to set __BSD_VISIBLE (needed for alloca) when the default visibility is disabled, so we have to do that directly, which is ugly.","This fixes the warnings in #1962 on FreeBSD
This turned out to be a bit of a rabbit hole: FreeBSD disables all the usual default visibility if you set _POSIX_C_SOURCE at all, so the warnings were coming from the implicit _XOPEN_SOURCE=700 being disabled.
Also, there doesn't seem to be any way to get the standard headers to set __BSD_VISIBLE (needed for alloca) when the default visibility is disabled, so we have to do that directly, which is ugly.",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,1978,2018-08-30T20:40:54Z,2021-03-25T08:30:20Z,2021-03-25T08:30:20Z,CLOSED,False,15,13,2,https://github.com/mutability,Fix warnings on FreeBSD; it is very picky about _POSIX_C_SOURCE et al,1,[],https://github.com/edenhill/librdkafka/pull/1978,https://github.com/edenhill,2,https://github.com/edenhill/librdkafka/pull/1978#issuecomment-438973655,"This fixes the warnings in #1962 on FreeBSD
This turned out to be a bit of a rabbit hole: FreeBSD disables all the usual default visibility if you set _POSIX_C_SOURCE at all, so the warnings were coming from the implicit _XOPEN_SOURCE=700 being disabled.
Also, there doesn't seem to be any way to get the standard headers to set __BSD_VISIBLE (needed for alloca) when the default visibility is disabled, so we have to do that directly, which is ugly.",Please address the CI build failures,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,1979,2018-09-03T13:17:09Z,2018-09-03T19:44:56Z,2018-09-03T19:45:00Z,MERGED,True,7,7,5,https://github.com/ctrochalakis,Typo fixes,1,[],https://github.com/edenhill/librdkafka/pull/1979,https://github.com/ctrochalakis,1,https://github.com/edenhill/librdkafka/pull/1979,"Fixing some typos as found by Debian's linter.
Note that I left commited_offset stats key intact :) (see #80)","Fixing some typos as found by Debian's linter.
Note that I left commited_offset stats key intact :) (see #80)",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,1979,2018-09-03T13:17:09Z,2018-09-03T19:44:56Z,2018-09-03T19:45:00Z,MERGED,True,7,7,5,https://github.com/ctrochalakis,Typo fixes,1,[],https://github.com/edenhill/librdkafka/pull/1979,https://github.com/edenhill,2,https://github.com/edenhill/librdkafka/pull/1979#issuecomment-418183567,"Fixing some typos as found by Debian's linter.
Note that I left commited_offset stats key intact :) (see #80)",Thank you!,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,1981,2018-09-03T19:20:19Z,2018-09-07T15:39:22Z,2018-09-07T15:39:25Z,MERGED,True,92,5,2,https://github.com/edenhill,Win32: fix rd_clock() calculation overflow (#1980),2,['bug'],https://github.com/edenhill/librdkafka/pull/1981,https://github.com/edenhill,1,https://github.com/edenhill/librdkafka/pull/1981,,,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,1981,2018-09-03T19:20:19Z,2018-09-07T15:39:22Z,2018-09-07T15:39:25Z,MERGED,True,92,5,2,https://github.com/edenhill,Win32: fix rd_clock() calculation overflow (#1980),2,['bug'],https://github.com/edenhill/librdkafka/pull/1981,https://github.com/rnpridgeon,2,https://github.com/edenhill/librdkafka/pull/1981#issuecomment-418325105,,,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,1981,2018-09-03T19:20:19Z,2018-09-07T15:39:22Z,2018-09-07T15:39:25Z,MERGED,True,92,5,2,https://github.com/edenhill,Win32: fix rd_clock() calculation overflow (#1980),2,['bug'],https://github.com/edenhill/librdkafka/pull/1981,https://github.com/mhowlett,3,https://github.com/edenhill/librdkafka/pull/1981#issuecomment-419473221,,unit test lgtm,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,1984,2018-09-04T20:52:02Z,2018-10-23T16:00:20Z,2018-10-23T16:00:20Z,MERGED,True,24,18,1,https://github.com/ameihm0912,"locking callbacks as required, dont call CRYPTO_cleanup_all_ex_data",1,[],https://github.com/edenhill/librdkafka/pull/1984,https://github.com/ameihm0912,1,https://github.com/edenhill/librdkafka/pull/1984,"Updates global SSL initialization to only configure callbacks if they
have not already been set. This makes the library behave better in
processes where OpenSSL is being used by other shared libraries, and
will not arbitrarily clobber callback functions set by the main
application using rdkafka.
Also remove call to CRYPTO_cleanup_all_ex_data, which makes an
assumption nothing will utilize OpenSSL in the process anymore and can
cause problems in the scenarios described above.","Updates global SSL initialization to only configure callbacks if they
have not already been set. This makes the library behave better in
processes where OpenSSL is being used by other shared libraries, and
will not arbitrarily clobber callback functions set by the main
application using rdkafka.
Also remove call to CRYPTO_cleanup_all_ex_data, which makes an
assumption nothing will utilize OpenSSL in the process anymore and can
cause problems in the scenarios described above.",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,1984,2018-09-04T20:52:02Z,2018-10-23T16:00:20Z,2018-10-23T16:00:20Z,MERGED,True,24,18,1,https://github.com/ameihm0912,"locking callbacks as required, dont call CRYPTO_cleanup_all_ex_data",1,[],https://github.com/edenhill/librdkafka/pull/1984,https://github.com/edenhill,2,https://github.com/edenhill/librdkafka/pull/1984#issuecomment-431187847,"Updates global SSL initialization to only configure callbacks if they
have not already been set. This makes the library behave better in
processes where OpenSSL is being used by other shared libraries, and
will not arbitrarily clobber callback functions set by the main
application using rdkafka.
Also remove call to CRYPTO_cleanup_all_ex_data, which makes an
assumption nothing will utilize OpenSSL in the process anymore and can
cause problems in the scenarios described above.","Can you rebase this on latest master?
Also pay attention that OpenSSL v1.1.0 support was added",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,1984,2018-09-04T20:52:02Z,2018-10-23T16:00:20Z,2018-10-23T16:00:20Z,MERGED,True,24,18,1,https://github.com/ameihm0912,"locking callbacks as required, dont call CRYPTO_cleanup_all_ex_data",1,[],https://github.com/edenhill/librdkafka/pull/1984,https://github.com/ameihm0912,3,https://github.com/edenhill/librdkafka/pull/1984#issuecomment-431492908,"Updates global SSL initialization to only configure callbacks if they
have not already been set. This makes the library behave better in
processes where OpenSSL is being used by other shared libraries, and
will not arbitrarily clobber callback functions set by the main
application using rdkafka.
Also remove call to CRYPTO_cleanup_all_ex_data, which makes an
assumption nothing will utilize OpenSSL in the process anymore and can
cause problems in the scenarios described above.",@edenhill this has been rebased against master and adjusted for the OpenSSL version ifdef.,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,1988,2018-09-07T11:29:22Z,2018-09-07T16:18:21Z,2018-09-07T16:18:24Z,MERGED,True,181,3,5,https://github.com/edenhill,Producer: Serve UA queue when transitioning topic from UNKNOWN (#1985),1,"['bug', 'producer']",https://github.com/edenhill/librdkafka/pull/1988,https://github.com/edenhill,1,https://github.com/edenhill/librdkafka/pull/1988,,,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,1993,2018-09-10T21:18:36Z,2018-09-11T06:46:17Z,2018-09-11T06:46:17Z,MERGED,True,2,2,2,https://github.com/mhowlett,changed '<' to 'before',1,[],https://github.com/edenhill/librdkafka/pull/1993,https://github.com/mhowlett,1,https://github.com/edenhill/librdkafka/pull/1993,"the < character doesn't play nicely with XML comments, which are now being derived from here by the dotnet client.","the < character doesn't play nicely with XML comments, which are now being derived from here by the dotnet client.",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,1995,2018-09-11T17:58:48Z,2018-09-11T19:56:53Z,2018-09-14T12:39:31Z,CLOSED,False,6,1,1,https://github.com/mike-coolfront,Librdkafka.so ref issue,2,[],https://github.com/edenhill/librdkafka/pull/1995,https://github.com/mike-coolfront,1,https://github.com/edenhill/librdkafka/pull/1995,"This pull request attempts to solves a library not found issue by updating librdkafka++ library flags to include librdkafka++ 's current directory at runtime as a search path when looking for dependencies.  I've intentionally left TODO comments in with some of my concerns/questions with the understanding I'll need to refactor.
Why/what happened:
In working with a node.js module ""node-rdkafka"" version 2.4.1 which uses librdkafka I encountered a file not found issue when librdkafka++.so was attempting to load shared library librdkafak.so. In summary it appears that even though node-rdkafka is compiled with librdkafka++ library's location as a search path, since librdkafka++ owns the librdkafka.so dependency there is no effect on where librdkafka++.so searches. In order to effect it I needed to update librdkafka++ to include a search path in its ELF Header.
see attached log (lines 1, 3-35, 54) showing failure to find library.
librdkafka++.so.ldd.log
additional log showing node-rdkafka ldd output (lines 30-64, 79) with failure to find library.
node-rdkafka.ldd.log
After adding librdkafka++'s runtime path to the search I have the following results:
see attached log (lines 1-10, 55): showing librdkafka.so found using librdkafka++'s runtime path.
librdkafka++.so.updated.ldd.log
additionally here is node-rdkafka's ldd output after updating librdkafka++'s Makefile (lines 30-32, 65): showing librdkafka.so found using librdkafka++ 's runtime path.
node-rdkafka.ldd.updated.log","This pull request attempts to solves a library not found issue by updating librdkafka++ library flags to include librdkafka++ 's current directory at runtime as a search path when looking for dependencies.  I've intentionally left TODO comments in with some of my concerns/questions with the understanding I'll need to refactor.
Why/what happened:
In working with a node.js module ""node-rdkafka"" version 2.4.1 which uses librdkafka I encountered a file not found issue when librdkafka++.so was attempting to load shared library librdkafak.so. In summary it appears that even though node-rdkafka is compiled with librdkafka++ library's location as a search path, since librdkafka++ owns the librdkafka.so dependency there is no effect on where librdkafka++.so searches. In order to effect it I needed to update librdkafka++ to include a search path in its ELF Header.
see attached log (lines 1, 3-35, 54) showing failure to find library.
librdkafka++.so.ldd.log
additional log showing node-rdkafka ldd output (lines 30-64, 79) with failure to find library.
node-rdkafka.ldd.log
After adding librdkafka++'s runtime path to the search I have the following results:
see attached log (lines 1-10, 55): showing librdkafka.so found using librdkafka++'s runtime path.
librdkafka++.so.updated.ldd.log
additionally here is node-rdkafka's ldd output after updating librdkafka++'s Makefile (lines 30-32, 65): showing librdkafka.so found using librdkafka++ 's runtime path.
node-rdkafka.ldd.updated.log",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,1995,2018-09-11T17:58:48Z,2018-09-11T19:56:53Z,2018-09-14T12:39:31Z,CLOSED,False,6,1,1,https://github.com/mike-coolfront,Librdkafka.so ref issue,2,[],https://github.com/edenhill/librdkafka/pull/1995,https://github.com/edenhill,2,https://github.com/edenhill/librdkafka/pull/1995#issuecomment-420367471,"This pull request attempts to solves a library not found issue by updating librdkafka++ library flags to include librdkafka++ 's current directory at runtime as a search path when looking for dependencies.  I've intentionally left TODO comments in with some of my concerns/questions with the understanding I'll need to refactor.
Why/what happened:
In working with a node.js module ""node-rdkafka"" version 2.4.1 which uses librdkafka I encountered a file not found issue when librdkafka++.so was attempting to load shared library librdkafak.so. In summary it appears that even though node-rdkafka is compiled with librdkafka++ library's location as a search path, since librdkafka++ owns the librdkafka.so dependency there is no effect on where librdkafka++.so searches. In order to effect it I needed to update librdkafka++ to include a search path in its ELF Header.
see attached log (lines 1, 3-35, 54) showing failure to find library.
librdkafka++.so.ldd.log
additional log showing node-rdkafka ldd output (lines 30-64, 79) with failure to find library.
node-rdkafka.ldd.log
After adding librdkafka++'s runtime path to the search I have the following results:
see attached log (lines 1-10, 55): showing librdkafka.so found using librdkafka++'s runtime path.
librdkafka++.so.updated.ldd.log
additionally here is node-rdkafka's ldd output after updating librdkafka++'s Makefile (lines 30-32, 65): showing librdkafka.so found using librdkafka++ 's runtime path.
node-rdkafka.ldd.updated.log","Did you try passing these to configure, as so: LDFLAGS='-Wl,-rpath,\$$ORIGIN'  ./configure ?",True,"{'THUMBS_UP': ['https://github.com/mike-coolfront', 'https://github.com/hufangtao']}"
edenhill/librdkafka,https://github.com/edenhill/librdkafka,1995,2018-09-11T17:58:48Z,2018-09-11T19:56:53Z,2018-09-14T12:39:31Z,CLOSED,False,6,1,1,https://github.com/mike-coolfront,Librdkafka.so ref issue,2,[],https://github.com/edenhill/librdkafka/pull/1995,https://github.com/mike-coolfront,3,https://github.com/edenhill/librdkafka/pull/1995#issuecomment-420403673,"This pull request attempts to solves a library not found issue by updating librdkafka++ library flags to include librdkafka++ 's current directory at runtime as a search path when looking for dependencies.  I've intentionally left TODO comments in with some of my concerns/questions with the understanding I'll need to refactor.
Why/what happened:
In working with a node.js module ""node-rdkafka"" version 2.4.1 which uses librdkafka I encountered a file not found issue when librdkafka++.so was attempting to load shared library librdkafak.so. In summary it appears that even though node-rdkafka is compiled with librdkafka++ library's location as a search path, since librdkafka++ owns the librdkafka.so dependency there is no effect on where librdkafka++.so searches. In order to effect it I needed to update librdkafka++ to include a search path in its ELF Header.
see attached log (lines 1, 3-35, 54) showing failure to find library.
librdkafka++.so.ldd.log
additional log showing node-rdkafka ldd output (lines 30-64, 79) with failure to find library.
node-rdkafka.ldd.log
After adding librdkafka++'s runtime path to the search I have the following results:
see attached log (lines 1-10, 55): showing librdkafka.so found using librdkafka++'s runtime path.
librdkafka++.so.updated.ldd.log
additionally here is node-rdkafka's ldd output after updating librdkafka++'s Makefile (lines 30-32, 65): showing librdkafka.so found using librdkafka++ 's runtime path.
node-rdkafka.ldd.updated.log","Did you try passing these to configure, as so: LDFLAGS='-Wl,-rpath,\$$ORIGIN' ./configure ?

Hey edenhill,
I wasn't aware. I tried that out and I do get the same effect as the code change I made.
Thank you for your insights.  I'm going to see if I can figure out how to get what you suggested incorporated into a PR for the node-rdkafka project.
Thanks again,
-Mike",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,1997,2018-09-12T12:20:31Z,2018-10-18T22:38:39Z,2018-10-26T14:31:19Z,MERGED,True,22,4,3,https://github.com/Oxymoron79,CMake: Build shared C++ library,3,['portability'],https://github.com/edenhill/librdkafka/pull/1997,https://github.com/Oxymoron79,1,https://github.com/edenhill/librdkafka/pull/1997,"Allow the C++ library to be built as a shared library.
Also, the shared libraries are versioned by setting the SOVERSION property, similar to the mklove build.","Allow the C++ library to be built as a shared library.
Also, the shared libraries are versioned by setting the SOVERSION property, similar to the mklove build.",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,1997,2018-09-12T12:20:31Z,2018-10-18T22:38:39Z,2018-10-26T14:31:19Z,MERGED,True,22,4,3,https://github.com/Oxymoron79,CMake: Build shared C++ library,3,['portability'],https://github.com/edenhill/librdkafka/pull/1997,https://github.com/edenhill,2,https://github.com/edenhill/librdkafka/pull/1997#issuecomment-422297845,"Allow the C++ library to be built as a shared library.
Also, the shared libraries are versioned by setting the SOVERSION property, similar to the mklove build.",Will merge after the v0.11.6 maintenance release,True,{'THUMBS_UP': ['https://github.com/Oxymoron79']}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,1999,2018-09-13T08:19:09Z,2018-09-13T08:26:10Z,2018-09-13T08:26:10Z,CLOSED,False,12,2,1,https://github.com/nouzun,OpenSSL 1.1.0 compatibility,1,[],https://github.com/edenhill/librdkafka/pull/1999,https://github.com/nouzun,1,https://github.com/edenhill/librdkafka/pull/1999,Changes to be able to build librdkafka with OpenSSL 1.1.0,Changes to be able to build librdkafka with OpenSSL 1.1.0,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2000,2018-09-13T08:34:24Z,2018-10-18T22:31:30Z,2018-10-18T22:31:30Z,MERGED,True,20,2,1,https://github.com/nouzun,OpenSSL 1.1.0 compatibility,1,[],https://github.com/edenhill/librdkafka/pull/2000,https://github.com/nouzun,1,https://github.com/edenhill/librdkafka/pull/2000,Changes to be able to build librdkafka with OpenSSL 1.1.0,Changes to be able to build librdkafka with OpenSSL 1.1.0,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2001,2018-09-13T14:01:43Z,2018-09-22T11:25:53Z,2018-09-22T11:25:53Z,CLOSED,False,4753,519,68,https://github.com/edenhill,Idempotent Producer support,16,['enhancement'],https://github.com/edenhill/librdkafka/pull/2001,https://github.com/edenhill,1,https://github.com/edenhill/librdkafka/pull/2001,"And some additional goodies:

stricter config checking (for 1.0)
Added purge() API
fatal errors
Added message_status() API

Review guide:

start with reading the Idempotent Producer section in INTRODUCTION.md
Produce error handling in rdkafka_request.c is the most critical piece of code
utilizes the existing test suite for testing (is tested with and without idempotent producer enabled), but a formal idemp-specific test will need to be added too unless AK system tests can cover that ground (it hard to test all error cases without a mock broker).
think of what the two new APIs will look like in the high-level clients.","And some additional goodies:

stricter config checking (for 1.0)
Added purge() API
fatal errors
Added message_status() API

Review guide:

start with reading the Idempotent Producer section in INTRODUCTION.md
Produce error handling in rdkafka_request.c is the most critical piece of code
utilizes the existing test suite for testing (is tested with and without idempotent producer enabled), but a formal idemp-specific test will need to be added too unless AK system tests can cover that ground (it hard to test all error cases without a mock broker).
think of what the two new APIs will look like in the high-level clients.",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2001,2018-09-13T14:01:43Z,2018-09-22T11:25:53Z,2018-09-22T11:25:53Z,CLOSED,False,4753,519,68,https://github.com/edenhill,Idempotent Producer support,16,['enhancement'],https://github.com/edenhill/librdkafka/pull/2001,https://github.com/edenhill,2,https://github.com/edenhill/librdkafka/pull/2001#issuecomment-421257091,"And some additional goodies:

stricter config checking (for 1.0)
Added purge() API
fatal errors
Added message_status() API

Review guide:

start with reading the Idempotent Producer section in INTRODUCTION.md
Produce error handling in rdkafka_request.c is the most critical piece of code
utilizes the existing test suite for testing (is tested with and without idempotent producer enabled), but a formal idemp-specific test will need to be added too unless AK system tests can cover that ground (it hard to test all error cases without a mock broker).
think of what the two new APIs will look like in the high-level clients.","Naming-wise I'm not fully convinced that ""POSSIBLY_PERSISTED"" and ""gapless"" are 100%, comments, suggestions?",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2001,2018-09-13T14:01:43Z,2018-09-22T11:25:53Z,2018-09-22T11:25:53Z,CLOSED,False,4753,519,68,https://github.com/edenhill,Idempotent Producer support,16,['enhancement'],https://github.com/edenhill/librdkafka/pull/2001,https://github.com/edenhill,3,https://github.com/edenhill/librdkafka/pull/2001#issuecomment-421368874,"And some additional goodies:

stricter config checking (for 1.0)
Added purge() API
fatal errors
Added message_status() API

Review guide:

start with reading the Idempotent Producer section in INTRODUCTION.md
Produce error handling in rdkafka_request.c is the most critical piece of code
utilizes the existing test suite for testing (is tested with and without idempotent producer enabled), but a formal idemp-specific test will need to be added too unless AK system tests can cover that ground (it hard to test all error cases without a mock broker).
think of what the two new APIs will look like in the high-level clients.",WIP Python branch https://github.com/confluentinc/confluent-kafka-python/tree/idempotence which adds support for fatal errors.,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2001,2018-09-13T14:01:43Z,2018-09-22T11:25:53Z,2018-09-22T11:25:53Z,CLOSED,False,4753,519,68,https://github.com/edenhill,Idempotent Producer support,16,['enhancement'],https://github.com/edenhill/librdkafka/pull/2001,https://github.com/mhowlett,4,https://github.com/edenhill/librdkafka/pull/2001#issuecomment-422589937,"And some additional goodies:

stricter config checking (for 1.0)
Added purge() API
fatal errors
Added message_status() API

Review guide:

start with reading the Idempotent Producer section in INTRODUCTION.md
Produce error handling in rdkafka_request.c is the most critical piece of code
utilizes the existing test suite for testing (is tested with and without idempotent producer enabled), but a formal idemp-specific test will need to be added too unless AK system tests can cover that ground (it hard to test all error cases without a mock broker).
think of what the two new APIs will look like in the high-level clients.",This is LGTM except I'm a bit un-easy about the level of test coverage.,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2007,2018-09-17T10:34:05Z,2018-09-17T13:41:43Z,2018-09-17T13:41:43Z,MERGED,True,365,181,16,https://github.com/edenhill,Use C11 threads when available,2,"['bug', 'portability']",https://github.com/edenhill/librdkafka/pull/2007,https://github.com/edenhill,1,https://github.com/edenhill/librdkafka/pull/2007,"When libc provides C11 threads, which is the case with musl on alpine, the internal librdkafka c11 thread implementation (tinycthread), interferes with the musl symbols, leading to weird errors and crashes.
There are no code changes, only refactoring.
This PR makes librdkafka use the system C11 threads if available.
Reported in #1998","When libc provides C11 threads, which is the case with musl on alpine, the internal librdkafka c11 thread implementation (tinycthread), interferes with the musl symbols, leading to weird errors and crashes.
There are no code changes, only refactoring.
This PR makes librdkafka use the system C11 threads if available.
Reported in #1998",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2009,2018-09-18T09:59:23Z,2020-10-21T11:02:28Z,2020-10-21T11:02:32Z,CLOSED,False,348,1,17,https://github.com/edenhill,Added copyright check (#1806),1,[],https://github.com/edenhill/librdkafka/pull/2009,https://github.com/edenhill,1,https://github.com/edenhill/librdkafka/pull/2009,"This is the right thing to do and helps packagers (e.g. debian) to avoid lintian warnings.
No code changes.","This is the right thing to do and helps packagers (e.g. debian) to avoid lintian warnings.
No code changes.",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2009,2018-09-18T09:59:23Z,2020-10-21T11:02:28Z,2020-10-21T11:02:32Z,CLOSED,False,348,1,17,https://github.com/edenhill,Added copyright check (#1806),1,[],https://github.com/edenhill/librdkafka/pull/2009,https://github.com/rnpridgeon,2,https://github.com/edenhill/librdkafka/pull/2009#issuecomment-580065744,"This is the right thing to do and helps packagers (e.g. debian) to avoid lintian warnings.
No code changes.","Please elaborate on why this ""is the right thing to do"" to the uninitiated",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2009,2018-09-18T09:59:23Z,2020-10-21T11:02:28Z,2020-10-21T11:02:32Z,CLOSED,False,348,1,17,https://github.com/edenhill,Added copyright check (#1806),1,[],https://github.com/edenhill/librdkafka/pull/2009,https://github.com/edenhill,3,https://github.com/edenhill/librdkafka/pull/2009#issuecomment-580450407,"This is the right thing to do and helps packagers (e.g. debian) to avoid lintian warnings.
No code changes.","When possible all files should have a license header, and I think this surfaced for the debian package maintainers as some of the packaged files did not.",True,{'THUMBS_UP': ['https://github.com/rnpridgeon']}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2017,2018-09-21T12:56:30Z,2018-10-18T21:37:57Z,2018-10-18T21:37:57Z,MERGED,True,9,1,1,https://github.com/ThePrez,Fix 64-bit IBM i build error,3,[],https://github.com/edenhill/librdkafka/pull/2017,https://github.com/ThePrez,1,https://github.com/edenhill/librdkafka/pull/2017,"64-bit builds on IBM i fail, as follows:
ld: 0711-317 ERROR: Undefined symbol: .le64toh
ld: 0711-345 Use the -bloadmap or -bnoquiet option to obtain more information.

(IBM i is using parts of the AIX runtime, and this fix is also applicable to AIX, hence why it is included in the AIX path)","64-bit builds on IBM i fail, as follows:
ld: 0711-317 ERROR: Undefined symbol: .le64toh
ld: 0711-345 Use the -bloadmap or -bnoquiet option to obtain more information.

(IBM i is using parts of the AIX runtime, and this fix is also applicable to AIX, hence why it is included in the AIX path)",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2017,2018-09-21T12:56:30Z,2018-10-18T21:37:57Z,2018-10-18T21:37:57Z,MERGED,True,9,1,1,https://github.com/ThePrez,Fix 64-bit IBM i build error,3,[],https://github.com/edenhill/librdkafka/pull/2017,https://github.com/GoogTech,2,https://github.com/edenhill/librdkafka/pull/2017#issuecomment-423735793,"64-bit builds on IBM i fail, as follows:
ld: 0711-317 ERROR: Undefined symbol: .le64toh
ld: 0711-345 Use the -bloadmap or -bnoquiet option to obtain more information.

(IBM i is using parts of the AIX runtime, and this fix is also applicable to AIX, hence why it is included in the AIX path)",HI,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2019,2018-09-22T16:35:54Z,2018-10-02T20:16:24Z,2018-10-02T20:16:24Z,MERGED,True,1315,377,40,https://github.com/edenhill,Sparse/on-demand connections,20,['enhancement'],https://github.com/edenhill/librdkafka/pull/2019,https://github.com/edenhill,1,https://github.com/edenhill/librdkafka/pull/2019,"Sparse/On-demand broker connections (enable with enable.sparse.connections=true - currently disabled by default)
Formalizes, cleanups up, and optimizes broker handler thread Ops/IO waiting.
Removes all internal uses of socket.blocking.max.ms, this property is now a no-op
Bug fix: Producer did not round-robin partitions.

Big thanks to @gduranceau for his work on the Connection on demand to brokers PR.
TODO: The test suite passes with sparse connections enabled, but a sparse.conn specific test needs to be added.","Sparse/On-demand broker connections (enable with enable.sparse.connections=true - currently disabled by default)
Formalizes, cleanups up, and optimizes broker handler thread Ops/IO waiting.
Removes all internal uses of socket.blocking.max.ms, this property is now a no-op
Bug fix: Producer did not round-robin partitions.

Big thanks to @gduranceau for his work on the Connection on demand to brokers PR.
TODO: The test suite passes with sparse connections enabled, but a sparse.conn specific test needs to be added.",True,"{'THUMBS_UP': ['https://github.com/gduranceau', 'https://github.com/billygout', 'https://github.com/jcarey03']}"
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2019,2018-09-22T16:35:54Z,2018-10-02T20:16:24Z,2018-10-02T20:16:24Z,MERGED,True,1315,377,40,https://github.com/edenhill,Sparse/on-demand connections,20,['enhancement'],https://github.com/edenhill/librdkafka/pull/2019,https://github.com/edenhill,2,https://github.com/edenhill/librdkafka/pull/2019#issuecomment-423941999,"Sparse/On-demand broker connections (enable with enable.sparse.connections=true - currently disabled by default)
Formalizes, cleanups up, and optimizes broker handler thread Ops/IO waiting.
Removes all internal uses of socket.blocking.max.ms, this property is now a no-op
Bug fix: Producer did not round-robin partitions.

Big thanks to @gduranceau for his work on the Connection on demand to brokers PR.
TODO: The test suite passes with sparse connections enabled, but a sparse.conn specific test needs to be added.","I think we'll want to make sparse connections the default right away, to get as much pre-release coverage as possible, and leave the disable-mode as a workaround, to eventually remove it alltogether.
I don't see a case where the disable-mode is actually needed.",True,"{'THUMBS_UP': ['https://github.com/rnpridgeon', 'https://github.com/billygout', 'https://github.com/ijuma']}"
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2019,2018-09-22T16:35:54Z,2018-10-02T20:16:24Z,2018-10-02T20:16:24Z,MERGED,True,1315,377,40,https://github.com/edenhill,Sparse/on-demand connections,20,['enhancement'],https://github.com/edenhill/librdkafka/pull/2019,https://github.com/edenhill,3,https://github.com/edenhill/librdkafka/pull/2019#issuecomment-425829110,"Sparse/On-demand broker connections (enable with enable.sparse.connections=true - currently disabled by default)
Formalizes, cleanups up, and optimizes broker handler thread Ops/IO waiting.
Removes all internal uses of socket.blocking.max.ms, this property is now a no-op
Bug fix: Producer did not round-robin partitions.

Big thanks to @gduranceau for his work on the Connection on demand to brokers PR.
TODO: The test suite passes with sparse connections enabled, but a sparse.conn specific test needs to be added.",@mhowlett @rnpridgeon (and whoever else wants to help out!) Please review the exponential backoff commit.,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2027,2018-09-27T16:39:43Z,2018-10-11T07:26:22Z,2018-10-11T13:38:44Z,CLOSED,False,4,1,1,https://github.com/pacovn,Makefile: fix install rule,1,[],https://github.com/edenhill/librdkafka/pull/2027,https://github.com/pacovn,1,https://github.com/edenhill/librdkafka/pull/2027,"Added the 'all' dependency to the 'install' rule so it builds and installs when calling ""make install"" after the ""./configure""","Added the 'all' dependency to the 'install' rule so it builds and installs when calling ""make install"" after the ""./configure""",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2027,2018-09-27T16:39:43Z,2018-10-11T07:26:22Z,2018-10-11T13:38:44Z,CLOSED,False,4,1,1,https://github.com/pacovn,Makefile: fix install rule,1,[],https://github.com/edenhill/librdkafka/pull/2027,https://github.com/edenhill,2,https://github.com/edenhill/librdkafka/pull/2027#issuecomment-428849687,"Added the 'all' dependency to the 'install' rule so it builds and installs when calling ""make install"" after the ""./configure""",We actually don't want install to make any changes to the build tree since it is typically run with sudo make install that would mess up permissions.,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2027,2018-09-27T16:39:43Z,2018-10-11T07:26:22Z,2018-10-11T13:38:44Z,CLOSED,False,4,1,1,https://github.com/pacovn,Makefile: fix install rule,1,[],https://github.com/edenhill/librdkafka/pull/2027,https://github.com/pacovn,3,https://github.com/edenhill/librdkafka/pull/2027#issuecomment-428956975,"Added the 'all' dependency to the 'install' rule so it builds and installs when calling ""make install"" after the ""./configure""","@edenhill The actual issue is that ""make install"" returns exit code 0 when failing because no previous ""make"" was done.",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2027,2018-09-27T16:39:43Z,2018-10-11T07:26:22Z,2018-10-11T13:38:44Z,CLOSED,False,4,1,1,https://github.com/pacovn,Makefile: fix install rule,1,[],https://github.com/edenhill/librdkafka/pull/2027,https://github.com/pacovn,4,https://github.com/edenhill/librdkafka/pull/2027#issuecomment-428958708,"Added the 'all' dependency to the 'install' rule so it builds and installs when calling ""make install"" after the ""./configure""",I've created issue #2048 for this.,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2028,2018-09-27T17:47:12Z,2018-10-18T21:33:35Z,2018-10-18T21:33:36Z,MERGED,True,0,3,1,https://github.com/gnanasekarl,Fixing Counting error in rdkafka_performance #1542,3,['bug'],https://github.com/edenhill/librdkafka/pull/2028,https://github.com/gnanasekarl,1,https://github.com/edenhill/librdkafka/pull/2028,"Removing the deduction of outq len from the total message produced count, which creates unnecessary mismatch between delivered and produced counters if in case there is some messages in outq.
Reference: issue#1542","Removing the deduction of outq len from the total message produced count, which creates unnecessary mismatch between delivered and produced counters if in case there is some messages in outq.
Reference: issue#1542",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2028,2018-09-27T17:47:12Z,2018-10-18T21:33:35Z,2018-10-18T21:33:36Z,MERGED,True,0,3,1,https://github.com/gnanasekarl,Fixing Counting error in rdkafka_performance #1542,3,['bug'],https://github.com/edenhill/librdkafka/pull/2028,https://github.com/gnanasekarl,2,https://github.com/edenhill/librdkafka/pull/2028#issuecomment-425408317,"Removing the deduction of outq len from the total message produced count, which creates unnecessary mismatch between delivered and produced counters if in case there is some messages in outq.
Reference: issue#1542","Hi,
AppVeyor build failed due to the below reason
_""
Install CoApp module
#Install-Module CoApp -Force
Installing OpenSSL v1.0 32-bit ...
Downloading...
Exception calling ""DownloadFile"" with ""2"" argument(s): ""The request was aborted: Could not create SSL/TLS secure channel.""
At line:10 char:1

(New-Object Net.WebClient).DownloadFile('https://slproweb.com/downloa ...

  + CategoryInfo          : NotSpecified: (:) [], MethodInvocationException
  + FullyQualifiedErrorId : WebException




Installing...
""_
What is this?",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2028,2018-09-27T17:47:12Z,2018-10-18T21:33:35Z,2018-10-18T21:33:36Z,MERGED,True,0,3,1,https://github.com/gnanasekarl,Fixing Counting error in rdkafka_performance #1542,3,['bug'],https://github.com/edenhill/librdkafka/pull/2028,https://github.com/edenhill,3,https://github.com/edenhill/librdkafka/pull/2028#issuecomment-427302799,"Removing the deduction of outq len from the total message produced count, which creates unnecessary mismatch between delivered and produced counters if in case there is some messages in outq.
Reference: issue#1542","You can ignore that error, CI environment related",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2028,2018-09-27T17:47:12Z,2018-10-18T21:33:35Z,2018-10-18T21:33:36Z,MERGED,True,0,3,1,https://github.com/gnanasekarl,Fixing Counting error in rdkafka_performance #1542,3,['bug'],https://github.com/edenhill/librdkafka/pull/2028,https://github.com/gnanasekarl,4,https://github.com/edenhill/librdkafka/pull/2028#issuecomment-427713332,"Removing the deduction of outq len from the total message produced count, which creates unnecessary mismatch between delivered and produced counters if in case there is some messages in outq.
Reference: issue#1542","Thanks Edenhill, Removed the whitespaces",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2032,2018-10-01T15:32:46Z,2018-10-03T07:33:12Z,2018-10-03T07:37:54Z,MERGED,True,10,0,1,https://github.com/stefanseufert,Fix high cpu sasl loop after disconnect,1,[],https://github.com/edenhill/librdkafka/pull/2032,https://github.com/stefanseufert,1,https://github.com/edenhill/librdkafka/pull/2032,Should fix #1937,Should fix #1937,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2032,2018-10-01T15:32:46Z,2018-10-03T07:33:12Z,2018-10-03T07:37:54Z,MERGED,True,10,0,1,https://github.com/stefanseufert,Fix high cpu sasl loop after disconnect,1,[],https://github.com/edenhill/librdkafka/pull/2032,https://github.com/stefanseufert,2,https://github.com/edenhill/librdkafka/pull/2032#issuecomment-426057308,Should fix #1937,Rebased and squashed fixes into original commit as per contributing guidelines,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2032,2018-10-01T15:32:46Z,2018-10-03T07:33:12Z,2018-10-03T07:37:54Z,MERGED,True,10,0,1,https://github.com/stefanseufert,Fix high cpu sasl loop after disconnect,1,[],https://github.com/edenhill/librdkafka/pull/2032,https://github.com/stefanseufert,3,https://github.com/edenhill/librdkafka/pull/2032#issuecomment-426382647,Should fix #1937,"@edenhill anything else you want me to do here? According to your comments in various threads, 11.6 seems to be close and   I really hope to see this included.",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2032,2018-10-01T15:32:46Z,2018-10-03T07:33:12Z,2018-10-03T07:37:54Z,MERGED,True,10,0,1,https://github.com/stefanseufert,Fix high cpu sasl loop after disconnect,1,[],https://github.com/edenhill/librdkafka/pull/2032,https://github.com/edenhill,4,https://github.com/edenhill/librdkafka/pull/2032#issuecomment-426383434,Should fix #1937,"0.11.6 would be a stretch, it is way past its deadline and needs to get out the door.
But we're doing 1.0.0 release candidates very soon, which would be a better fit for this.",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2032,2018-10-01T15:32:46Z,2018-10-03T07:33:12Z,2018-10-03T07:37:54Z,MERGED,True,10,0,1,https://github.com/stefanseufert,Fix high cpu sasl loop after disconnect,1,[],https://github.com/edenhill/librdkafka/pull/2032,https://github.com/stefanseufert,5,https://github.com/edenhill/librdkafka/pull/2032#issuecomment-426392887,Should fix #1937,"This is a really nasty bug for everyone using SASL auth. A restart of any broker node will immediately make the clients go 100% CPU per thread. I've seen boxes logging CPU loads in excess of 350 due to this issue, making them completely inaccessible. Since this also affects producers it will result in message loss (on the consumer side it ""only"" prevents the affected threads from receiving messages). There are no known workarounds and there is no way to detect the problem in the calling code. As far as I can see all platforms are affected. Not sure how much worse a bug could be?
The only reason, why this hasn't caught much more attention seems to be the fact that auth isn't used that commonly.
The fix on the other hand seems trivial ans will only affect those people using auth.
... but it certainly is your call.",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2032,2018-10-01T15:32:46Z,2018-10-03T07:33:12Z,2018-10-03T07:37:54Z,MERGED,True,10,0,1,https://github.com/stefanseufert,Fix high cpu sasl loop after disconnect,1,[],https://github.com/edenhill/librdkafka/pull/2032,https://github.com/edenhill,6,https://github.com/edenhill/librdkafka/pull/2032#issuecomment-426416871,Should fix #1937,"@stefanseufert Valid points, I'll get back to you on this tomorrow.",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2032,2018-10-01T15:32:46Z,2018-10-03T07:33:12Z,2018-10-03T07:37:54Z,MERGED,True,10,0,1,https://github.com/stefanseufert,Fix high cpu sasl loop after disconnect,1,[],https://github.com/edenhill/librdkafka/pull/2032,https://github.com/edenhill,7,https://github.com/edenhill/librdkafka/pull/2032#issuecomment-426539469,Should fix #1937,"Thanks alot for this!
(yes, it will go into 0.11.6 ;) )",True,"{'LAUGH': ['https://github.com/stefanseufert', 'https://github.com/OMSKremer'], 'HEART': ['https://github.com/stefanseufert', 'https://github.com/OMSKremer']}"
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2037,2018-10-03T13:15:41Z,2019-07-12T12:51:07Z,2020-05-08T08:58:21Z,CLOSED,False,70,5,6,https://github.com/edenhill,include monitoring-interceptors in NuGet package,2,[],https://github.com/edenhill/librdkafka/pull/2037,https://github.com/edenhill,1,https://github.com/edenhill/librdkafka/pull/2037,,,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2038,2018-10-03T16:14:54Z,2018-10-03T17:12:21Z,2018-10-03T17:23:04Z,MERGED,True,1,1,1,https://github.com/Romain-Geissler-1A,Fix LTO warning with gcc 8.,1,['portability'],https://github.com/edenhill/librdkafka/pull/2038,https://github.com/Romain-Geissler-1A,1,https://github.com/edenhill/librdkafka/pull/2038,"Hi,
We are having some compilation warnings (error in our case since we use -Werror) when linking a program against librdkafka built using LTO and enabling LTO.
We have:
In function rd_addrinfo_prepare,
    inlined from rd_getaddrinfo.constprop at rdaddr.c:160:17,
    inlined from rd_kafka_broker_resolve at rdkafka_broker.c:634:19,
    inlined from rd_kafka_broker_connect at rdkafka_broker.c:1287:6,
    inlined from rd_kafka_broker_thread_main at rdkafka_broker.c:3328:8:
rdaddr.c:134:3: error: strncpy specified bound depends on the length of the source argument [-Werror=stringop-overflow=]
rdkafka_broker.c: In function rd_kafka_broker_thread_main:
rdaddr.c:129:13: note: length computed here
lto1: all warnings being treated as errors

While here gcc is clearly being over-strict (gcc uses LTO to infer from cross-compilation unit that actually we used strlen() for the length of strncpy, but other cases from other call sites might still be different), I actually don't think you really need strncpy here, given that you explicitely add yourself a '\0' just after the strncpy, just using memcpy should be fine.
Note: a similar ""fix"" for not a real problem was recently merged in curl (we hit that issue too):
curl/curl@357161a
Cheers,
Romain","Hi,
We are having some compilation warnings (error in our case since we use -Werror) when linking a program against librdkafka built using LTO and enabling LTO.
We have:
In function rd_addrinfo_prepare,
    inlined from rd_getaddrinfo.constprop at rdaddr.c:160:17,
    inlined from rd_kafka_broker_resolve at rdkafka_broker.c:634:19,
    inlined from rd_kafka_broker_connect at rdkafka_broker.c:1287:6,
    inlined from rd_kafka_broker_thread_main at rdkafka_broker.c:3328:8:
rdaddr.c:134:3: error: strncpy specified bound depends on the length of the source argument [-Werror=stringop-overflow=]
rdkafka_broker.c: In function rd_kafka_broker_thread_main:
rdaddr.c:129:13: note: length computed here
lto1: all warnings being treated as errors

While here gcc is clearly being over-strict (gcc uses LTO to infer from cross-compilation unit that actually we used strlen() for the length of strncpy, but other cases from other call sites might still be different), I actually don't think you really need strncpy here, given that you explicitely add yourself a '\0' just after the strncpy, just using memcpy should be fine.
Note: a similar ""fix"" for not a real problem was recently merged in curl (we hit that issue too):
curl/curl@357161a
Cheers,
Romain",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2038,2018-10-03T16:14:54Z,2018-10-03T17:12:21Z,2018-10-03T17:23:04Z,MERGED,True,1,1,1,https://github.com/Romain-Geissler-1A,Fix LTO warning with gcc 8.,1,['portability'],https://github.com/edenhill/librdkafka/pull/2038,https://github.com/edenhill,2,https://github.com/edenhill/librdkafka/pull/2038#issuecomment-426719686,"Hi,
We are having some compilation warnings (error in our case since we use -Werror) when linking a program against librdkafka built using LTO and enabling LTO.
We have:
In function rd_addrinfo_prepare,
    inlined from rd_getaddrinfo.constprop at rdaddr.c:160:17,
    inlined from rd_kafka_broker_resolve at rdkafka_broker.c:634:19,
    inlined from rd_kafka_broker_connect at rdkafka_broker.c:1287:6,
    inlined from rd_kafka_broker_thread_main at rdkafka_broker.c:3328:8:
rdaddr.c:134:3: error: strncpy specified bound depends on the length of the source argument [-Werror=stringop-overflow=]
rdkafka_broker.c: In function rd_kafka_broker_thread_main:
rdaddr.c:129:13: note: length computed here
lto1: all warnings being treated as errors

While here gcc is clearly being over-strict (gcc uses LTO to infer from cross-compilation unit that actually we used strlen() for the length of strncpy, but other cases from other call sites might still be different), I actually don't think you really need strncpy here, given that you explicitely add yourself a '\0' just after the strncpy, just using memcpy should be fine.
Note: a similar ""fix"" for not a real problem was recently merged in curl (we hit that issue too):
curl/curl@357161a
Cheers,
Romain",Thank you for this!,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2038,2018-10-03T16:14:54Z,2018-10-03T17:12:21Z,2018-10-03T17:23:04Z,MERGED,True,1,1,1,https://github.com/Romain-Geissler-1A,Fix LTO warning with gcc 8.,1,['portability'],https://github.com/edenhill/librdkafka/pull/2038,https://github.com/Romain-Geissler-1A,3,https://github.com/edenhill/librdkafka/pull/2038#issuecomment-426722680,"Hi,
We are having some compilation warnings (error in our case since we use -Werror) when linking a program against librdkafka built using LTO and enabling LTO.
We have:
In function rd_addrinfo_prepare,
    inlined from rd_getaddrinfo.constprop at rdaddr.c:160:17,
    inlined from rd_kafka_broker_resolve at rdkafka_broker.c:634:19,
    inlined from rd_kafka_broker_connect at rdkafka_broker.c:1287:6,
    inlined from rd_kafka_broker_thread_main at rdkafka_broker.c:3328:8:
rdaddr.c:134:3: error: strncpy specified bound depends on the length of the source argument [-Werror=stringop-overflow=]
rdkafka_broker.c: In function rd_kafka_broker_thread_main:
rdaddr.c:129:13: note: length computed here
lto1: all warnings being treated as errors

While here gcc is clearly being over-strict (gcc uses LTO to infer from cross-compilation unit that actually we used strlen() for the length of strncpy, but other cases from other call sites might still be different), I actually don't think you really need strncpy here, given that you explicitely add yourself a '\0' just after the strncpy, just using memcpy should be fine.
Note: a similar ""fix"" for not a real problem was recently merged in curl (we hit that issue too):
curl/curl@357161a
Cheers,
Romain","I see you are currently working on some 0.11.6 release candidates + experimental 1.0.0 releases. Do you think this small fix can make it into the actual release 0.11.6 ?
Thanks",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2038,2018-10-03T16:14:54Z,2018-10-03T17:12:21Z,2018-10-03T17:23:04Z,MERGED,True,1,1,1,https://github.com/Romain-Geissler-1A,Fix LTO warning with gcc 8.,1,['portability'],https://github.com/edenhill/librdkafka/pull/2038,https://github.com/edenhill,4,https://github.com/edenhill/librdkafka/pull/2038#issuecomment-426723114,"Hi,
We are having some compilation warnings (error in our case since we use -Werror) when linking a program against librdkafka built using LTO and enabling LTO.
We have:
In function rd_addrinfo_prepare,
    inlined from rd_getaddrinfo.constprop at rdaddr.c:160:17,
    inlined from rd_kafka_broker_resolve at rdkafka_broker.c:634:19,
    inlined from rd_kafka_broker_connect at rdkafka_broker.c:1287:6,
    inlined from rd_kafka_broker_thread_main at rdkafka_broker.c:3328:8:
rdaddr.c:134:3: error: strncpy specified bound depends on the length of the source argument [-Werror=stringop-overflow=]
rdkafka_broker.c: In function rd_kafka_broker_thread_main:
rdaddr.c:129:13: note: length computed here
lto1: all warnings being treated as errors

While here gcc is clearly being over-strict (gcc uses LTO to infer from cross-compilation unit that actually we used strlen() for the length of strncpy, but other cases from other call sites might still be different), I actually don't think you really need strncpy here, given that you explicitely add yourself a '\0' just after the strncpy, just using memcpy should be fine.
Note: a similar ""fix"" for not a real problem was recently merged in curl (we hit that issue too):
curl/curl@357161a
Cheers,
Romain","Yep, given it is minimal size it is scheduled for 0.11.6",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2038,2018-10-03T16:14:54Z,2018-10-03T17:12:21Z,2018-10-03T17:23:04Z,MERGED,True,1,1,1,https://github.com/Romain-Geissler-1A,Fix LTO warning with gcc 8.,1,['portability'],https://github.com/edenhill/librdkafka/pull/2038,https://github.com/Romain-Geissler-1A,5,https://github.com/edenhill/librdkafka/pull/2038#issuecomment-426723410,"Hi,
We are having some compilation warnings (error in our case since we use -Werror) when linking a program against librdkafka built using LTO and enabling LTO.
We have:
In function rd_addrinfo_prepare,
    inlined from rd_getaddrinfo.constprop at rdaddr.c:160:17,
    inlined from rd_kafka_broker_resolve at rdkafka_broker.c:634:19,
    inlined from rd_kafka_broker_connect at rdkafka_broker.c:1287:6,
    inlined from rd_kafka_broker_thread_main at rdkafka_broker.c:3328:8:
rdaddr.c:134:3: error: strncpy specified bound depends on the length of the source argument [-Werror=stringop-overflow=]
rdkafka_broker.c: In function rd_kafka_broker_thread_main:
rdaddr.c:129:13: note: length computed here
lto1: all warnings being treated as errors

While here gcc is clearly being over-strict (gcc uses LTO to infer from cross-compilation unit that actually we used strlen() for the length of strncpy, but other cases from other call sites might still be different), I actually don't think you really need strncpy here, given that you explicitely add yourself a '\0' just after the strncpy, just using memcpy should be fine.
Note: a similar ""fix"" for not a real problem was recently merged in curl (we hit that issue too):
curl/curl@357161a
Cheers,
Romain","Perfect, thank you !",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2049,2018-10-11T15:41:09Z,2018-10-18T21:20:12Z,2018-10-18T21:20:12Z,MERGED,True,9,9,1,https://github.com/pacovn,Makefile: fix install rule,1,[],https://github.com/edenhill/librdkafka/pull/2049,https://github.com/pacovn,1,https://github.com/edenhill/librdkafka/pull/2049,Fix following your suggestion on issue #2048,Fix following your suggestion on issue #2048,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2050,2018-10-12T09:25:51Z,2018-10-18T21:21:10Z,2018-10-19T12:00:29Z,MERGED,True,2,2,2,https://github.com/KseniyaYakil,Prevent int overflow while computing abs_timeout for producer request,2,[],https://github.com/edenhill/librdkafka/pull/2050,https://github.com/KseniyaYakil,1,https://github.com/edenhill/librdkafka/pull/2050,tried to prevent int overflow and avoid incorrect message timeout (#2015),tried to prevent int overflow and avoid incorrect message timeout (#2015),True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2050,2018-10-12T09:25:51Z,2018-10-18T21:21:10Z,2018-10-19T12:00:29Z,MERGED,True,2,2,2,https://github.com/KseniyaYakil,Prevent int overflow while computing abs_timeout for producer request,2,[],https://github.com/edenhill/librdkafka/pull/2050,https://github.com/KseniyaYakil,2,https://github.com/edenhill/librdkafka/pull/2050#issuecomment-429752159,tried to prevent int overflow and avoid incorrect message timeout (#2015),"Hi, @edenhill !
Is code ok now?)
could you tell when this fix will be available in release/master branch?",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2050,2018-10-12T09:25:51Z,2018-10-18T21:21:10Z,2018-10-19T12:00:29Z,MERGED,True,2,2,2,https://github.com/KseniyaYakil,Prevent int overflow while computing abs_timeout for producer request,2,[],https://github.com/edenhill/librdkafka/pull/2050,https://github.com/edenhill,3,https://github.com/edenhill/librdkafka/pull/2050#issuecomment-429986086,tried to prevent int overflow and avoid incorrect message timeout (#2015),"Looks good!
This will be released in v1.0.0 (november)",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2050,2018-10-12T09:25:51Z,2018-10-18T21:21:10Z,2018-10-19T12:00:29Z,MERGED,True,2,2,2,https://github.com/KseniyaYakil,Prevent int overflow while computing abs_timeout for producer request,2,[],https://github.com/edenhill/librdkafka/pull/2050,https://github.com/KseniyaYakil,4,https://github.com/edenhill/librdkafka/pull/2050#issuecomment-430152294,tried to prevent int overflow and avoid incorrect message timeout (#2015),Could you tell me what the release flow is? I've expected you will merge this change to master so we could use master branch in production)) Or do you merge just before release?,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2050,2018-10-12T09:25:51Z,2018-10-18T21:21:10Z,2018-10-19T12:00:29Z,MERGED,True,2,2,2,https://github.com/KseniyaYakil,Prevent int overflow while computing abs_timeout for producer request,2,[],https://github.com/edenhill/librdkafka/pull/2050,https://github.com/edenhill,5,https://github.com/edenhill/librdkafka/pull/2050#issuecomment-431105149,tried to prevent int overflow and avoid incorrect message timeout (#2015),"We typically create new releases from master, and when getting closer to release date we will tag release candidates (RC) and only merge Pull requests that are critical to the release.
As soon as the final release is out we'll merge any PRs that have been approved but were not critical for the release.
v0.11.6 will be released this week, after that we'll merge this PR.",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2050,2018-10-12T09:25:51Z,2018-10-18T21:21:10Z,2018-10-19T12:00:29Z,MERGED,True,2,2,2,https://github.com/KseniyaYakil,Prevent int overflow while computing abs_timeout for producer request,2,[],https://github.com/edenhill/librdkafka/pull/2050,https://github.com/KseniyaYakil,6,https://github.com/edenhill/librdkafka/pull/2050#issuecomment-431339662,tried to prevent int overflow and avoid incorrect message timeout (#2015),Thanks a lot)),True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2053,2018-10-12T23:31:44Z,2018-10-22T11:22:13Z,2019-01-04T17:14:24Z,MERGED,True,417,11,21,https://github.com/vavrusa,Add support for KIP-110 ZSTD compression,2,['enhancement'],https://github.com/edenhill/librdkafka/pull/2053,https://github.com/vavrusa,1,https://github.com/edenhill/librdkafka/pull/2053,"This was introduced recently in https://cwiki.apache.org/confluence/display/KAFKA/KIP-110%3A+Add+Codec+for+ZStandard+Compression
And merged in apache/kafka#2267
I'm going to need some help figuring out how to test this. I think I added the prerequisites right - Fetch APIv10 and Produce APIv7.","This was introduced recently in https://cwiki.apache.org/confluence/display/KAFKA/KIP-110%3A+Add+Codec+for+ZStandard+Compression
And merged in apache/kafka#2267
I'm going to need some help figuring out how to test this. I think I added the prerequisites right - Fetch APIv10 and Produce APIv7.",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2053,2018-10-12T23:31:44Z,2018-10-22T11:22:13Z,2019-01-04T17:14:24Z,MERGED,True,417,11,21,https://github.com/vavrusa,Add support for KIP-110 ZSTD compression,2,['enhancement'],https://github.com/edenhill/librdkafka/pull/2053,https://github.com/edenhill,2,https://github.com/edenhill/librdkafka/pull/2053#issuecomment-429566173,"This was introduced recently in https://cwiki.apache.org/confluence/display/KAFKA/KIP-110%3A+Add+Codec+for+ZStandard+Compression
And merged in apache/kafka#2267
I'm going to need some help figuring out how to test this. I think I added the prerequisites right - Fetch APIv10 and Produce APIv7.","This will go in v1.0.0, which is currently the idempotence branch.
It would be great if you could retarget and rebase this on idempotence.",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2053,2018-10-12T23:31:44Z,2018-10-22T11:22:13Z,2019-01-04T17:14:24Z,MERGED,True,417,11,21,https://github.com/vavrusa,Add support for KIP-110 ZSTD compression,2,['enhancement'],https://github.com/edenhill/librdkafka/pull/2053,https://github.com/vavrusa,3,https://github.com/edenhill/librdkafka/pull/2053#issuecomment-429581904,"This was introduced recently in https://cwiki.apache.org/confluence/display/KAFKA/KIP-110%3A+Add+Codec+for+ZStandard+Compression
And merged in apache/kafka#2267
I'm going to need some help figuring out how to test this. I think I added the prerequisites right - Fetch APIv10 and Produce APIv7.","Yeah, we've been using zstd in Kafka for a while at Cloudflare, but it only currently ""works"" with Sarama, so I can't move without it.",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2053,2018-10-12T23:31:44Z,2018-10-22T11:22:13Z,2019-01-04T17:14:24Z,MERGED,True,417,11,21,https://github.com/vavrusa,Add support for KIP-110 ZSTD compression,2,['enhancement'],https://github.com/edenhill/librdkafka/pull/2053,https://github.com/edenhill,4,https://github.com/edenhill/librdkafka/pull/2053#issuecomment-429582029,"This was introduced recently in https://cwiki.apache.org/confluence/display/KAFKA/KIP-110%3A+Add+Codec+for+ZStandard+Compression
And merged in apache/kafka#2267
I'm going to need some help figuring out how to test this. I think I added the prerequisites right - Fetch APIv10 and Produce APIv7.",".. so I can't move without it.

Move to what? :)
v1.0.0 is scheduled for release within a month.",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2053,2018-10-12T23:31:44Z,2018-10-22T11:22:13Z,2019-01-04T17:14:24Z,MERGED,True,417,11,21,https://github.com/vavrusa,Add support for KIP-110 ZSTD compression,2,['enhancement'],https://github.com/edenhill/librdkafka/pull/2053,https://github.com/vavrusa,5,https://github.com/edenhill/librdkafka/pull/2053#issuecomment-429582325,"This was introduced recently in https://cwiki.apache.org/confluence/display/KAFKA/KIP-110%3A+Add+Codec+for+ZStandard+Compression
And merged in apache/kafka#2267
I'm going to need some help figuring out how to test this. I think I added the prerequisites right - Fetch APIv10 and Produce APIv7.","Move to what? :)

I wrote the Kafka table engine for ClickHouse using librdkafka, but can't use it with most topics without zstd. The librdkafka is embedded as a submodule there, so I don't have to wait for the release tag before testing it.",True,"{'THUMBS_UP': ['https://github.com/ijuma', 'https://github.com/billygout']}"
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2053,2018-10-12T23:31:44Z,2018-10-22T11:22:13Z,2019-01-04T17:14:24Z,MERGED,True,417,11,21,https://github.com/vavrusa,Add support for KIP-110 ZSTD compression,2,['enhancement'],https://github.com/edenhill/librdkafka/pull/2053,https://github.com/vavrusa,6,https://github.com/edenhill/librdkafka/pull/2053#issuecomment-429582341,"This was introduced recently in https://cwiki.apache.org/confluence/display/KAFKA/KIP-110%3A+Add+Codec+for+ZStandard+Compression
And merged in apache/kafka#2267
I'm going to need some help figuring out how to test this. I think I added the prerequisites right - Fetch APIv10 and Produce APIv7.",Rebased to idempotence branch.,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2053,2018-10-12T23:31:44Z,2018-10-22T11:22:13Z,2019-01-04T17:14:24Z,MERGED,True,417,11,21,https://github.com/vavrusa,Add support for KIP-110 ZSTD compression,2,['enhancement'],https://github.com/edenhill/librdkafka/pull/2053,https://github.com/vavrusa,7,https://github.com/edenhill/librdkafka/pull/2053#issuecomment-429582373,"This was introduced recently in https://cwiki.apache.org/confluence/display/KAFKA/KIP-110%3A+Add+Codec+for+ZStandard+Compression
And merged in apache/kafka#2267
I'm going to need some help figuring out how to test this. I think I added the prerequisites right - Fetch APIv10 and Produce APIv7.",Thanks for reviewing! ,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2053,2018-10-12T23:31:44Z,2018-10-22T11:22:13Z,2019-01-04T17:14:24Z,MERGED,True,417,11,21,https://github.com/vavrusa,Add support for KIP-110 ZSTD compression,2,['enhancement'],https://github.com/edenhill/librdkafka/pull/2053,https://github.com/Donis-,8,https://github.com/edenhill/librdkafka/pull/2053#issuecomment-451015660,"This was introduced recently in https://cwiki.apache.org/confluence/display/KAFKA/KIP-110%3A+Add+Codec+for+ZStandard+Compression
And merged in apache/kafka#2267
I'm going to need some help figuring out how to test this. I think I added the prerequisites right - Fetch APIv10 and Produce APIv7.","@vavrusa did you get it to work in your case?
We upgraded to confluent kafka 5.1.0 / 2.1.0 and set the broker default compression to ZSTD.
Java clients seem to be working fine, but i can't get kafkacat with librdkafka to work:
Consumption fails. Based on logs, zstd was enabled via compilation and is supported by broker:
%7|1546470506.414|APIVERSION|rdkafka#consumer-1| [thrd:sasl_plaintext://xxx/bootstrap]: sasl_plaintext://xxx/bootstrap:  Feature ZSTD: Produce (7..7) supported by broker
%7|1546470506.415|APIVERSION|rdkafka#consumer-1| [thrd:sasl_plaintext://xxx/bootstrap]: sasl_plaintext://xxx/bootstrap:  Feature ZSTD: Fetch (10..10) supported by broker
%7|1546470506.417|APIVERSION|rdkafka#consumer-1| [thrd:sasl_plaintext://xxx/bootstrap]: sasl_plaintext://xxx/bootstrap: Enabling feature ZSTD
%7|1546470506.417|FEATURE|rdkafka#consumer-1| [thrd:sasl_plaintext://xxx/bootstrap]: sasl_plaintext://xxx/bootstrap: Updated enabled protocol features to MsgVer1,ApiVersion,BrokerBalancedConsumer,ThrottleTime,Sasl,SaslHandshake,BrokerGroupCoordinator,LZ4,OffsetTime,MsgVer2,IdempotentProducer,ZSTD
...
%7|1546470508.106|FETCH|rdkafka#consumer-1| [thrd:sasl_plaintext://xxx/105]: sasl_plaintext://xxx/105: Fetch topic XXX  [55] at offset 0 (v2)
%7|1546470508.108|FETCH|rdkafka#consumer-1| [thrd:sasl_plaintext://xxx/105]: sasl_plaintext://xxx/105: Fetch 1/1/16 toppar(s)
%7|1546470508.108|SEND|rdkafka#consumer-1| [thrd:sasl_plaintext://xxx/105]: sasl_plaintext://xxx/105: Sent FetchRequest (v4, 112 bytes @ 0, CorrId 3)
%7|1546470508.155|RECV|rdkafka#consumer-1| [thrd:sasl_plaintext://xxx/105]: sasl_plaintext://xxx/105: Received FetchResponse (v4, 92 bytes, CorrId 3, rtt 46.73ms)
%7|1546470508.156|FETCH|rdkafka#consumer-1| [thrd:sasl_plaintext://xxx/105]: sasl_plaintext://xxx/105: Topic XXX [55] MessageSet size 0, error ""Err-76?"", MaxOffset -1, Ver 2/2
76 is the error introduced with KIP-110: UNSUPPORTED_COMPRESSION_TYPE.
https://cwiki.apache.org/confluence/display/KAFKA/KIP-110%3A+Add+Codec+for+ZStandard+Compression
Maybe we need to bump the v4 fetch version somehow as per ""Zstd will only be allowed for the bumped fetch API "".? or am i missing something?
version:
tag 1.0.0-RC5 with dynamic zstd linking, win64 build.",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2053,2018-10-12T23:31:44Z,2018-10-22T11:22:13Z,2019-01-04T17:14:24Z,MERGED,True,417,11,21,https://github.com/vavrusa,Add support for KIP-110 ZSTD compression,2,['enhancement'],https://github.com/edenhill/librdkafka/pull/2053,https://github.com/vavrusa,9,https://github.com/edenhill/librdkafka/pull/2053#issuecomment-451020169,"This was introduced recently in https://cwiki.apache.org/confluence/display/KAFKA/KIP-110%3A+Add+Codec+for+ZStandard+Compression
And merged in apache/kafka#2267
I'm going to need some help figuring out how to test this. I think I added the prerequisites right - Fetch APIv10 and Produce APIv7.","Yes, it's been working for some time. Maybe the negotiation isn't working properly for your case? I don't know that much about Kafka itself to help you, sorry!",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2053,2018-10-12T23:31:44Z,2018-10-22T11:22:13Z,2019-01-04T17:14:24Z,MERGED,True,417,11,21,https://github.com/vavrusa,Add support for KIP-110 ZSTD compression,2,['enhancement'],https://github.com/edenhill/librdkafka/pull/2053,https://github.com/Donis-,10,https://github.com/edenhill/librdkafka/pull/2053#issuecomment-451073273,"This was introduced recently in https://cwiki.apache.org/confluence/display/KAFKA/KIP-110%3A+Add+Codec+for+ZStandard+Compression
And merged in apache/kafka#2267
I'm going to need some help figuring out how to test this. I think I added the prerequisites right - Fetch APIv10 and Produce APIv7.","thanks for confirming!
just to double check, you're both producing and consuming via librdkafka?
@edenhill maybe you have an idea on what could be going wrong in my case?",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2053,2018-10-12T23:31:44Z,2018-10-22T11:22:13Z,2019-01-04T17:14:24Z,MERGED,True,417,11,21,https://github.com/vavrusa,Add support for KIP-110 ZSTD compression,2,['enhancement'],https://github.com/edenhill/librdkafka/pull/2053,https://github.com/Donis-,11,https://github.com/edenhill/librdkafka/pull/2053#issuecomment-451507124,"This was introduced recently in https://cwiki.apache.org/confluence/display/KAFKA/KIP-110%3A+Add+Codec+for+ZStandard+Compression
And merged in apache/kafka#2267
I'm going to need some help figuring out how to test this. I think I added the prerequisites right - Fetch APIv10 and Produce APIv7.","seems fetch api v10 support was missing, by adding the the fetch protocol support for v10 i got it to work. i can clean up and submit a PR.",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2054,2018-10-14T05:43:55Z,2018-10-22T11:23:07Z,2018-10-22T11:23:12Z,MERGED,True,440,63,16,https://github.com/edenhill,KIP-62: add support for max processing time (max.poll.interval.ms),1,['enhancement'],https://github.com/edenhill/librdkafka/pull/2054,https://github.com/edenhill,1,https://github.com/edenhill/librdkafka/pull/2054,"Changed defaults:

session.timeout.ms = 10000","Changed defaults:

session.timeout.ms = 10000",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2054,2018-10-14T05:43:55Z,2018-10-22T11:23:07Z,2018-10-22T11:23:12Z,MERGED,True,440,63,16,https://github.com/edenhill,KIP-62: add support for max processing time (max.poll.interval.ms),1,['enhancement'],https://github.com/edenhill/librdkafka/pull/2054,https://github.com/ijuma,2,https://github.com/edenhill/librdkafka/pull/2054#issuecomment-429854744,"Changed defaults:

session.timeout.ms = 10000",Nice! Do we track these config default changes somewhere so that we can tell users in upgrade notes and such?,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2054,2018-10-14T05:43:55Z,2018-10-22T11:23:07Z,2018-10-22T11:23:12Z,MERGED,True,440,63,16,https://github.com/edenhill,KIP-62: add support for max processing time (max.poll.interval.ms),1,['enhancement'],https://github.com/edenhill/librdkafka/pull/2054,https://github.com/edenhill,3,https://github.com/edenhill/librdkafka/pull/2054#issuecomment-429911528,"Changed defaults:

session.timeout.ms = 10000","@ijuma Yep, something like ""Default changes"" here: https://github.com/edenhill/librdkafka/releases/tag/v0.11.4",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2054,2018-10-14T05:43:55Z,2018-10-22T11:23:07Z,2018-10-22T11:23:12Z,MERGED,True,440,63,16,https://github.com/edenhill,KIP-62: add support for max processing time (max.poll.interval.ms),1,['enhancement'],https://github.com/edenhill/librdkafka/pull/2054,https://github.com/ijuma,4,https://github.com/edenhill/librdkafka/pull/2054#issuecomment-429968475,"Changed defaults:

session.timeout.ms = 10000",Sounds good.,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2054,2018-10-14T05:43:55Z,2018-10-22T11:23:07Z,2018-10-22T11:23:12Z,MERGED,True,440,63,16,https://github.com/edenhill,KIP-62: add support for max processing time (max.poll.interval.ms),1,['enhancement'],https://github.com/edenhill/librdkafka/pull/2054,https://github.com/edenhill,5,https://github.com/edenhill/librdkafka/pull/2054#issuecomment-431768847,"Changed defaults:

session.timeout.ms = 10000","Added rebalance_reason.
A max_poll_interval metric would definitely be useful, but requires a lock rather than an atomic, so will leave this until it is really needed.",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2054,2018-10-14T05:43:55Z,2018-10-22T11:23:07Z,2018-10-22T11:23:12Z,MERGED,True,440,63,16,https://github.com/edenhill,KIP-62: add support for max processing time (max.poll.interval.ms),1,['enhancement'],https://github.com/edenhill/librdkafka/pull/2054,https://github.com/edenhill,6,https://github.com/edenhill/librdkafka/pull/2054#issuecomment-431768903,"Changed defaults:

session.timeout.ms = 10000","Added rebalance_reason.
A max_poll_interval metric would definitely be useful, but requires a lock rather than an atomic, so will leave this until it is really needed.",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2054,2018-10-14T05:43:55Z,2018-10-22T11:23:07Z,2018-10-22T11:23:12Z,MERGED,True,440,63,16,https://github.com/edenhill,KIP-62: add support for max processing time (max.poll.interval.ms),1,['enhancement'],https://github.com/edenhill/librdkafka/pull/2054,https://github.com/edenhill,7,https://github.com/edenhill/librdkafka/pull/2054#issuecomment-431768956,"Changed defaults:

session.timeout.ms = 10000","Added rebalance_reason.
A max_poll_interval metric would definitely be useful, but requires a lock rather than an atomic, so will leave this until it is really needed.",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2061,2018-10-17T09:06:09Z,2018-10-18T21:20:33Z,2018-10-19T13:33:44Z,MERGED,True,1,1,1,https://github.com/ankon,Remove an excess word in a @brief,1,[],https://github.com/edenhill/librdkafka/pull/2061,https://github.com/ankon,1,https://github.com/edenhill/librdkafka/pull/2061,,,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2065,2018-10-20T14:17:20Z,2020-12-16T16:25:00Z,2020-12-16T16:25:00Z,CLOSED,False,651,2,9,https://github.com/mlongob,KIP-107: DeleteRecords API,3,[],https://github.com/edenhill/librdkafka/pull/2065,https://github.com/mlongob,1,https://github.com/edenhill/librdkafka/pull/2065,"Fixes #2056
@edenhill This is still a work in progress. I'm sending this early so that the public interface can be reviewed. I'm still working on the tests.
TODOs:

 Add unit tests to 0080
 Add integration tests to 0081","Fixes #2056
@edenhill This is still a work in progress. I'm sending this early so that the public interface can be reviewed. I'm still working on the tests.
TODOs:

 Add unit tests to 0080
 Add integration tests to 0081",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2065,2018-10-20T14:17:20Z,2020-12-16T16:25:00Z,2020-12-16T16:25:00Z,CLOSED,False,651,2,9,https://github.com/mlongob,KIP-107: DeleteRecords API,3,[],https://github.com/edenhill/librdkafka/pull/2065,https://github.com/mlongob,2,https://github.com/edenhill/librdkafka/pull/2065#issuecomment-431724189,"Fixes #2056
@edenhill This is still a work in progress. I'm sending this early so that the public interface can be reviewed. I'm still working on the tests.
TODOs:

 Add unit tests to 0080
 Add integration tests to 0081","@edenhill
It looks like I missed the part in the specs where it says that the DeleteRecords Request needs to be sent to the broker which is leader for the partition. I need to revise the implementation to:

Issue a metadata request to identify partition leaders
Split the topic_partition_list into N lists where N is the number of brokers being leaders for the partitions in the request
Send N requests
Wait for N events
Combine results

This will probably affect the public interface",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2065,2018-10-20T14:17:20Z,2020-12-16T16:25:00Z,2020-12-16T16:25:00Z,CLOSED,False,651,2,9,https://github.com/mlongob,KIP-107: DeleteRecords API,3,[],https://github.com/edenhill/librdkafka/pull/2065,https://github.com/mlongob,3,https://github.com/edenhill/librdkafka/pull/2065#issuecomment-746577362,"Fixes #2056
@edenhill This is still a work in progress. I'm sending this early so that the public interface can be reviewed. I'm still working on the tests.
TODOs:

 Add unit tests to 0080
 Add integration tests to 0081",Thanks @gridaphobe and @edenhill for taking this to the finish line in #2701!,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2068,2018-10-24T05:49:22Z,2018-10-24T06:46:28Z,2018-10-24T06:46:28Z,MERGED,True,3,3,2,https://github.com/mhowlett,make enable.idempotence doc XML friendly,2,[],https://github.com/edenhill/librdkafka/pull/2068,https://github.com/mhowlett,1,https://github.com/edenhill/librdkafka/pull/2068,,,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2071,2018-10-24T21:22:57Z,2018-10-25T07:15:05Z,2018-10-25T14:23:00Z,MERGED,True,0,3,1,https://github.com/motorspin,Remove Conflict Markers in rdlist.h,1,[],https://github.com/edenhill/librdkafka/pull/2071,https://github.com/motorspin,1,https://github.com/edenhill/librdkafka/pull/2071,,,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2071,2018-10-24T21:22:57Z,2018-10-25T07:15:05Z,2018-10-25T14:23:00Z,MERGED,True,0,3,1,https://github.com/motorspin,Remove Conflict Markers in rdlist.h,1,[],https://github.com/edenhill/librdkafka/pull/2071,https://github.com/motorspin,2,https://github.com/edenhill/librdkafka/pull/2071#issuecomment-432851254,,Let me know if you would like me to add anything to the commit as a description. This seemed like a small oversight so wasn't sure if I should put anything!,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2071,2018-10-24T21:22:57Z,2018-10-25T07:15:05Z,2018-10-25T14:23:00Z,MERGED,True,0,3,1,https://github.com/motorspin,Remove Conflict Markers in rdlist.h,1,[],https://github.com/edenhill/librdkafka/pull/2071,https://github.com/edenhill,3,https://github.com/edenhill/librdkafka/pull/2071#issuecomment-432940571,,Good find! Thank you!,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2072,2018-10-25T15:37:24Z,2018-10-31T16:20:48Z,2018-10-31T16:20:51Z,MERGED,True,248,99,17,https://github.com/edenhill,ZStd fixes,4,[],https://github.com/edenhill/librdkafka/pull/2072,https://github.com/edenhill,1,https://github.com/edenhill/librdkafka/pull/2072,,,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2072,2018-10-25T15:37:24Z,2018-10-31T16:20:48Z,2018-10-31T16:20:51Z,MERGED,True,248,99,17,https://github.com/edenhill,ZStd fixes,4,[],https://github.com/edenhill/librdkafka/pull/2072,https://github.com/rnpridgeon,2,https://github.com/edenhill/librdkafka/pull/2072#issuecomment-434680215,,I left some comments which are mostly just questions. Other than that LGTM,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2075,2018-10-26T15:12:00Z,2018-10-30T12:50:50Z,2018-10-30T13:05:18Z,MERGED,True,193,1,5,https://github.com/Oxymoron79,CMake: Generate pk-gconfig files,5,[],https://github.com/edenhill/librdkafka/pull/2075,https://github.com/Oxymoron79,1,https://github.com/edenhill/librdkafka/pull/2075,"Generate the pkg-config files with CMake.
It uses the configure_file function from CMake to generate the pkg-config files for the C and C++ libraries (shared and optionally static variant) from the same template file.
Since the pkg-config files contain the version, the CMake project version is set.","Generate the pkg-config files with CMake.
It uses the configure_file function from CMake to generate the pkg-config files for the C and C++ libraries (shared and optionally static variant) from the same template file.
Since the pkg-config files contain the version, the CMake project version is set.",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2075,2018-10-26T15:12:00Z,2018-10-30T12:50:50Z,2018-10-30T13:05:18Z,MERGED,True,193,1,5,https://github.com/Oxymoron79,CMake: Generate pk-gconfig files,5,[],https://github.com/edenhill/librdkafka/pull/2075,https://github.com/edenhill,2,https://github.com/edenhill/librdkafka/pull/2075#issuecomment-434288273,"Generate the pkg-config files with CMake.
It uses the configure_file function from CMake to generate the pkg-config files for the C and C++ libraries (shared and optionally static variant) from the same template file.
Since the pkg-config files contain the version, the CMake project version is set.","This is great, thanks!",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2075,2018-10-26T15:12:00Z,2018-10-30T12:50:50Z,2018-10-30T13:05:18Z,MERGED,True,193,1,5,https://github.com/Oxymoron79,CMake: Generate pk-gconfig files,5,[],https://github.com/edenhill/librdkafka/pull/2075,https://github.com/Oxymoron79,3,https://github.com/edenhill/librdkafka/pull/2075#issuecomment-434289723,"Generate the pkg-config files with CMake.
It uses the configure_file function from CMake to generate the pkg-config files for the C and C++ libraries (shared and optionally static variant) from the same template file.
Since the pkg-config files contain the version, the CMake project version is set.","You're welcome.
Thanks for merging!",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2081,2018-10-31T16:25:20Z,2018-11-21T11:27:41Z,2020-05-08T08:58:26Z,MERGED,True,1979,740,38,https://github.com/edenhill,Support implicit acks in idempotent producer,23,[],https://github.com/edenhill/librdkafka/pull/2081,https://github.com/edenhill,1,https://github.com/edenhill/librdkafka/pull/2081,"This is a critical bug fix (implicit acks) to idempo producer, an added idempo test, and some minor enhancements and fixes","This is a critical bug fix (implicit acks) to idempo producer, an added idempo test, and some minor enhancements and fixes",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2081,2018-10-31T16:25:20Z,2018-11-21T11:27:41Z,2020-05-08T08:58:26Z,MERGED,True,1979,740,38,https://github.com/edenhill,Support implicit acks in idempotent producer,23,[],https://github.com/edenhill/librdkafka/pull/2081,https://github.com/edenhill,2,https://github.com/edenhill/librdkafka/pull/2081#issuecomment-438261188,"This is a critical bug fix (implicit acks) to idempo producer, an added idempo test, and some minor enhancements and fixes",@rnpridgeon This grew substantially and will need another round of reviews.,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2095,2018-11-08T09:21:22Z,2018-11-08T09:29:19Z,2018-11-08T09:29:19Z,CLOSED,False,0,3,1,https://github.com/souradeep100,bug #1550,3,[],https://github.com/edenhill/librdkafka/pull/2095,https://github.com/souradeep100,1,https://github.com/edenhill/librdkafka/pull/2095,#1550,#1550,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2101,2018-11-12T08:02:01Z,2018-11-12T10:00:52Z,2018-11-12T10:00:56Z,MERGED,True,3,3,2,https://github.com/ctrochalakis,Fix build failure on hurd-i386,1,[],https://github.com/edenhill/librdkafka/pull/2101,https://github.com/ctrochalakis,1,https://github.com/edenhill/librdkafka/pull/2101,"Hello Magnus, this addresses an issue on hurd-i386 build failure we had
in Debian.
See https://bugs.debian.org/900716
Build logs https://buildd.debian.org/status/fetch.php?pkg=librdkafka&arch=hurd-i386&ver=0.11.6-1&stamp=1540378777&raw=0
Patch by Samuel Thibault sthibault@debian.org","Hello Magnus, this addresses an issue on hurd-i386 build failure we had
in Debian.
See https://bugs.debian.org/900716
Build logs https://buildd.debian.org/status/fetch.php?pkg=librdkafka&arch=hurd-i386&ver=0.11.6-1&stamp=1540378777&raw=0
Patch by Samuel Thibault sthibault@debian.org",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2101,2018-11-12T08:02:01Z,2018-11-12T10:00:52Z,2018-11-12T10:00:56Z,MERGED,True,3,3,2,https://github.com/ctrochalakis,Fix build failure on hurd-i386,1,[],https://github.com/edenhill/librdkafka/pull/2101,https://github.com/edenhill,2,https://github.com/edenhill/librdkafka/pull/2101#issuecomment-437822344,"Hello Magnus, this addresses an issue on hurd-i386 build failure we had
in Debian.
See https://bugs.debian.org/900716
Build logs https://buildd.debian.org/status/fetch.php?pkg=librdkafka&arch=hurd-i386&ver=0.11.6-1&stamp=1540378777&raw=0
Patch by Samuel Thibault sthibault@debian.org",Thank you!,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2102,2018-11-12T10:59:22Z,2019-05-07T09:11:49Z,2019-05-07T09:11:49Z,CLOSED,False,8,2,1,https://github.com/souradeep100,"Fix for #1550 Misleading ""All messages delivered!"" message ",8,[],https://github.com/edenhill/librdkafka/pull/2102,https://github.com/souradeep100,1,https://github.com/edenhill/librdkafka/pull/2102,"#1550 this is the fix , to change the message in msg_delivered. Earlier irrespective of delivery failures
it was showing ""All messages delivered"". Now post this change it will give the delivery failure percentage.
The Unit test result:
souracha@souracha-VirtualBox:~/librdkafka/librdkafka/examples$ ./rdkafka_performance -P -t test -b localhost:9092 -s 1000 -c 450000 -v
% Using random seed 1542016931, verbosity level 2
% Sending 450000 messages of size 1000 bytes
% All messages produced, now waiting for 398102 deliveries
% 450000 messages produced (450000000 bytes), 232741 delivered (offset 0, 0 failed) in 1001ms: 232438 msgs/s and 232.44 MB/s, 0 produce failures, 217259 in queue, no compression
Messages delivered with failure percentage of 0.000000!
% 0 messages in outq
% 450000 messages produced (450000000 bytes), 450000 delivered (offset 0, 0 failed) in 1687ms: 266638 msgs/s and 266.64 MB/s, 0 produce failures, 0 in queue, no compression","#1550 this is the fix , to change the message in msg_delivered. Earlier irrespective of delivery failures
it was showing ""All messages delivered"". Now post this change it will give the delivery failure percentage.
The Unit test result:
souracha@souracha-VirtualBox:~/librdkafka/librdkafka/examples$ ./rdkafka_performance -P -t test -b localhost:9092 -s 1000 -c 450000 -v
% Using random seed 1542016931, verbosity level 2
% Sending 450000 messages of size 1000 bytes
% All messages produced, now waiting for 398102 deliveries
% 450000 messages produced (450000000 bytes), 232741 delivered (offset 0, 0 failed) in 1001ms: 232438 msgs/s and 232.44 MB/s, 0 produce failures, 217259 in queue, no compression
Messages delivered with failure percentage of 0.000000!
% 0 messages in outq
% 450000 messages produced (450000000 bytes), 450000 delivered (offset 0, 0 failed) in 1687ms: 266638 msgs/s and 266.64 MB/s, 0 produce failures, 0 in queue, no compression",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2102,2018-11-12T10:59:22Z,2019-05-07T09:11:49Z,2019-05-07T09:11:49Z,CLOSED,False,8,2,1,https://github.com/souradeep100,"Fix for #1550 Misleading ""All messages delivered!"" message ",8,[],https://github.com/edenhill/librdkafka/pull/2102,https://github.com/souradeep100,2,https://github.com/edenhill/librdkafka/pull/2102#issuecomment-439807216,"#1550 this is the fix , to change the message in msg_delivered. Earlier irrespective of delivery failures
it was showing ""All messages delivered"". Now post this change it will give the delivery failure percentage.
The Unit test result:
souracha@souracha-VirtualBox:~/librdkafka/librdkafka/examples$ ./rdkafka_performance -P -t test -b localhost:9092 -s 1000 -c 450000 -v
% Using random seed 1542016931, verbosity level 2
% Sending 450000 messages of size 1000 bytes
% All messages produced, now waiting for 398102 deliveries
% 450000 messages produced (450000000 bytes), 232741 delivered (offset 0, 0 failed) in 1001ms: 232438 msgs/s and 232.44 MB/s, 0 produce failures, 217259 in queue, no compression
Messages delivered with failure percentage of 0.000000!
% 0 messages in outq
% 450000 messages produced (450000000 bytes), 450000 delivered (offset 0, 0 failed) in 1687ms: 266638 msgs/s and 266.64 MB/s, 0 produce failures, 0 in queue, no compression","Hi Edenhill, I can see some checks have not completed yet here. Can you please let me know what can be the cause ?",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2102,2018-11-12T10:59:22Z,2019-05-07T09:11:49Z,2019-05-07T09:11:49Z,CLOSED,False,8,2,1,https://github.com/souradeep100,"Fix for #1550 Misleading ""All messages delivered!"" message ",8,[],https://github.com/edenhill/librdkafka/pull/2102,https://github.com/souradeep100,3,https://github.com/edenhill/librdkafka/pull/2102#issuecomment-444791892,"#1550 this is the fix , to change the message in msg_delivered. Earlier irrespective of delivery failures
it was showing ""All messages delivered"". Now post this change it will give the delivery failure percentage.
The Unit test result:
souracha@souracha-VirtualBox:~/librdkafka/librdkafka/examples$ ./rdkafka_performance -P -t test -b localhost:9092 -s 1000 -c 450000 -v
% Using random seed 1542016931, verbosity level 2
% Sending 450000 messages of size 1000 bytes
% All messages produced, now waiting for 398102 deliveries
% 450000 messages produced (450000000 bytes), 232741 delivered (offset 0, 0 failed) in 1001ms: 232438 msgs/s and 232.44 MB/s, 0 produce failures, 217259 in queue, no compression
Messages delivered with failure percentage of 0.000000!
% 0 messages in outq
% 450000 messages produced (450000000 bytes), 450000 delivered (offset 0, 0 failed) in 1687ms: 266638 msgs/s and 266.64 MB/s, 0 produce failures, 0 in queue, no compression","Hi, please let me know if there is any comments on this change",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2102,2018-11-12T10:59:22Z,2019-05-07T09:11:49Z,2019-05-07T09:11:49Z,CLOSED,False,8,2,1,https://github.com/souradeep100,"Fix for #1550 Misleading ""All messages delivered!"" message ",8,[],https://github.com/edenhill/librdkafka/pull/2102,https://github.com/souradeep100,4,https://github.com/edenhill/librdkafka/pull/2102#issuecomment-445127555,"#1550 this is the fix , to change the message in msg_delivered. Earlier irrespective of delivery failures
it was showing ""All messages delivered"". Now post this change it will give the delivery failure percentage.
The Unit test result:
souracha@souracha-VirtualBox:~/librdkafka/librdkafka/examples$ ./rdkafka_performance -P -t test -b localhost:9092 -s 1000 -c 450000 -v
% Using random seed 1542016931, verbosity level 2
% Sending 450000 messages of size 1000 bytes
% All messages produced, now waiting for 398102 deliveries
% 450000 messages produced (450000000 bytes), 232741 delivered (offset 0, 0 failed) in 1001ms: 232438 msgs/s and 232.44 MB/s, 0 produce failures, 217259 in queue, no compression
Messages delivered with failure percentage of 0.000000!
% 0 messages in outq
% 450000 messages produced (450000000 bytes), 450000 delivered (offset 0, 0 failed) in 1687ms: 266638 msgs/s and 266.64 MB/s, 0 produce failures, 0 in queue, no compression","I have fixed the review comments , please let me know if anything else needs to be changed for this.",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2102,2018-11-12T10:59:22Z,2019-05-07T09:11:49Z,2019-05-07T09:11:49Z,CLOSED,False,8,2,1,https://github.com/souradeep100,"Fix for #1550 Misleading ""All messages delivered!"" message ",8,[],https://github.com/edenhill/librdkafka/pull/2102,https://github.com/souradeep100,5,https://github.com/edenhill/librdkafka/pull/2102#issuecomment-445528694,"#1550 this is the fix , to change the message in msg_delivered. Earlier irrespective of delivery failures
it was showing ""All messages delivered"". Now post this change it will give the delivery failure percentage.
The Unit test result:
souracha@souracha-VirtualBox:~/librdkafka/librdkafka/examples$ ./rdkafka_performance -P -t test -b localhost:9092 -s 1000 -c 450000 -v
% Using random seed 1542016931, verbosity level 2
% Sending 450000 messages of size 1000 bytes
% All messages produced, now waiting for 398102 deliveries
% 450000 messages produced (450000000 bytes), 232741 delivered (offset 0, 0 failed) in 1001ms: 232438 msgs/s and 232.44 MB/s, 0 produce failures, 217259 in queue, no compression
Messages delivered with failure percentage of 0.000000!
% 0 messages in outq
% 450000 messages produced (450000000 bytes), 450000 delivered (offset 0, 0 failed) in 1687ms: 266638 msgs/s and 266.64 MB/s, 0 produce failures, 0 in queue, no compression",please let me know if any other changes are required for this bug.,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2102,2018-11-12T10:59:22Z,2019-05-07T09:11:49Z,2019-05-07T09:11:49Z,CLOSED,False,8,2,1,https://github.com/souradeep100,"Fix for #1550 Misleading ""All messages delivered!"" message ",8,[],https://github.com/edenhill/librdkafka/pull/2102,https://github.com/edenhill,6,https://github.com/edenhill/librdkafka/pull/2102#issuecomment-456069841,"#1550 this is the fix , to change the message in msg_delivered. Earlier irrespective of delivery failures
it was showing ""All messages delivered"". Now post this change it will give the delivery failure percentage.
The Unit test result:
souracha@souracha-VirtualBox:~/librdkafka/librdkafka/examples$ ./rdkafka_performance -P -t test -b localhost:9092 -s 1000 -c 450000 -v
% Using random seed 1542016931, verbosity level 2
% Sending 450000 messages of size 1000 bytes
% All messages produced, now waiting for 398102 deliveries
% 450000 messages produced (450000000 bytes), 232741 delivered (offset 0, 0 failed) in 1001ms: 232438 msgs/s and 232.44 MB/s, 0 produce failures, 217259 in queue, no compression
Messages delivered with failure percentage of 0.000000!
% 0 messages in outq
% 450000 messages produced (450000000 bytes), 450000 delivered (offset 0, 0 failed) in 1687ms: 266638 msgs/s and 266.64 MB/s, 0 produce failures, 0 in queue, no compression","Thank you for your patience, will tend to this PR when v1.0.0 is released.",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2102,2018-11-12T10:59:22Z,2019-05-07T09:11:49Z,2019-05-07T09:11:49Z,CLOSED,False,8,2,1,https://github.com/souradeep100,"Fix for #1550 Misleading ""All messages delivered!"" message ",8,[],https://github.com/edenhill/librdkafka/pull/2102,https://github.com/souradeep100,7,https://github.com/edenhill/librdkafka/pull/2102#issuecomment-456315552,"#1550 this is the fix , to change the message in msg_delivered. Earlier irrespective of delivery failures
it was showing ""All messages delivered"". Now post this change it will give the delivery failure percentage.
The Unit test result:
souracha@souracha-VirtualBox:~/librdkafka/librdkafka/examples$ ./rdkafka_performance -P -t test -b localhost:9092 -s 1000 -c 450000 -v
% Using random seed 1542016931, verbosity level 2
% Sending 450000 messages of size 1000 bytes
% All messages produced, now waiting for 398102 deliveries
% 450000 messages produced (450000000 bytes), 232741 delivered (offset 0, 0 failed) in 1001ms: 232438 msgs/s and 232.44 MB/s, 0 produce failures, 217259 in queue, no compression
Messages delivered with failure percentage of 0.000000!
% 0 messages in outq
% 450000 messages produced (450000000 bytes), 450000 delivered (offset 0, 0 failed) in 1687ms: 266638 msgs/s and 266.64 MB/s, 0 produce failures, 0 in queue, no compression","thank you .

On Mon, Jan 21, 2019 at 6:44 PM Magnus Edenhill ***@***.***> wrote:
 Thank you for your patience, will tend to this PR when v1.0.0 is released.

 
 You are receiving this because you authored the thread.
 Reply to this email directly, view it on GitHub
 <#2102 (comment)>,
 or mute the thread
 <https://github.com/notifications/unsubscribe-auth/AK3XRdmEBamKIcwFSHs212WF3xS91D0Pks5vFb0agaJpZM4YZahU>
 .


-- 
Thanks & Regards,
Souradeep
Mob: 09663082628
Bangalore, India",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2102,2018-11-12T10:59:22Z,2019-05-07T09:11:49Z,2019-05-07T09:11:49Z,CLOSED,False,8,2,1,https://github.com/souradeep100,"Fix for #1550 Misleading ""All messages delivered!"" message ",8,[],https://github.com/edenhill/librdkafka/pull/2102,https://github.com/souradeep100,8,https://github.com/edenhill/librdkafka/pull/2102#issuecomment-480160244,"#1550 this is the fix , to change the message in msg_delivered. Earlier irrespective of delivery failures
it was showing ""All messages delivered"". Now post this change it will give the delivery failure percentage.
The Unit test result:
souracha@souracha-VirtualBox:~/librdkafka/librdkafka/examples$ ./rdkafka_performance -P -t test -b localhost:9092 -s 1000 -c 450000 -v
% Using random seed 1542016931, verbosity level 2
% Sending 450000 messages of size 1000 bytes
% All messages produced, now waiting for 398102 deliveries
% 450000 messages produced (450000000 bytes), 232741 delivered (offset 0, 0 failed) in 1001ms: 232438 msgs/s and 232.44 MB/s, 0 produce failures, 217259 in queue, no compression
Messages delivered with failure percentage of 0.000000!
% 0 messages in outq
% 450000 messages produced (450000000 bytes), 450000 delivered (offset 0, 0 failed) in 1687ms: 266638 msgs/s and 266.64 MB/s, 0 produce failures, 0 in queue, no compression",When it will be merged ?,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2102,2018-11-12T10:59:22Z,2019-05-07T09:11:49Z,2019-05-07T09:11:49Z,CLOSED,False,8,2,1,https://github.com/souradeep100,"Fix for #1550 Misleading ""All messages delivered!"" message ",8,[],https://github.com/edenhill/librdkafka/pull/2102,https://github.com/edenhill,9,https://github.com/edenhill/librdkafka/pull/2102#issuecomment-480773347,"#1550 this is the fix , to change the message in msg_delivered. Earlier irrespective of delivery failures
it was showing ""All messages delivered"". Now post this change it will give the delivery failure percentage.
The Unit test result:
souracha@souracha-VirtualBox:~/librdkafka/librdkafka/examples$ ./rdkafka_performance -P -t test -b localhost:9092 -s 1000 -c 450000 -v
% Using random seed 1542016931, verbosity level 2
% Sending 450000 messages of size 1000 bytes
% All messages produced, now waiting for 398102 deliveries
% 450000 messages produced (450000000 bytes), 232741 delivered (offset 0, 0 failed) in 1001ms: 232438 msgs/s and 232.44 MB/s, 0 produce failures, 217259 in queue, no compression
Messages delivered with failure percentage of 0.000000!
% 0 messages in outq
% 450000 messages produced (450000000 bytes), 450000 delivered (offset 0, 0 failed) in 1687ms: 266638 msgs/s and 266.64 MB/s, 0 produce failures, 0 in queue, no compression",This PR needs to be rebased on latest master to fix the conflict.,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2104,2018-11-14T02:35:10Z,2018-11-14T09:40:18Z,2018-11-14T18:00:32Z,MERGED,True,9,2,1,https://github.com/wiml,OpenSSL initialization compatibility for v1.1.0 and later,1,[],https://github.com/edenhill/librdkafka/pull/2104,https://github.com/wiml,1,https://github.com/edenhill/librdkafka/pull/2104,"OpenSSL v1.1.0 made all of the init stuff optional and deprecated the old functions in favor of OPENSSL_init_ssl and OPENSSL_init_crypto. There are compatibility macros in place, but only if you compile with openssl's api compatibility switch set.
This change lets librdkafka compile against OpenSSL with the API compatibility level set as late as 1.1.0.
I've only done moderate testing (""it works for me"").","OpenSSL v1.1.0 made all of the init stuff optional and deprecated the old functions in favor of OPENSSL_init_ssl and OPENSSL_init_crypto. There are compatibility macros in place, but only if you compile with openssl's api compatibility switch set.
This change lets librdkafka compile against OpenSSL with the API compatibility level set as late as 1.1.0.
I've only done moderate testing (""it works for me"").",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2104,2018-11-14T02:35:10Z,2018-11-14T09:40:18Z,2018-11-14T18:00:32Z,MERGED,True,9,2,1,https://github.com/wiml,OpenSSL initialization compatibility for v1.1.0 and later,1,[],https://github.com/edenhill/librdkafka/pull/2104,https://github.com/edenhill,2,https://github.com/edenhill/librdkafka/pull/2104#issuecomment-438598610,"OpenSSL v1.1.0 made all of the init stuff optional and deprecated the old functions in favor of OPENSSL_init_ssl and OPENSSL_init_crypto. There are compatibility macros in place, but only if you compile with openssl's api compatibility switch set.
This change lets librdkafka compile against OpenSSL with the API compatibility level set as late as 1.1.0.
I've only done moderate testing (""it works for me"").",Verified with openssl v1.1.0f and ASAN on osx. ,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2104,2018-11-14T02:35:10Z,2018-11-14T09:40:18Z,2018-11-14T18:00:32Z,MERGED,True,9,2,1,https://github.com/wiml,OpenSSL initialization compatibility for v1.1.0 and later,1,[],https://github.com/edenhill/librdkafka/pull/2104,https://github.com/edenhill,3,https://github.com/edenhill/librdkafka/pull/2104#issuecomment-438599369,"OpenSSL v1.1.0 made all of the init stuff optional and deprecated the old functions in favor of OPENSSL_init_ssl and OPENSSL_init_crypto. There are compatibility macros in place, but only if you compile with openssl's api compatibility switch set.
This change lets librdkafka compile against OpenSSL with the API compatibility level set as late as 1.1.0.
I've only done moderate testing (""it works for me"").",Thank you for this!,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2106,2018-11-14T13:05:24Z,2018-11-15T07:51:05Z,2018-11-15T07:51:08Z,MERGED,True,96,0,3,https://github.com/edenhill, Remember assign()/seek():ed offset when pause()ing (#2105),2,"['bug', 'consumer']",https://github.com/edenhill/librdkafka/pull/2106,https://github.com/edenhill,1,https://github.com/edenhill/librdkafka/pull/2106,,,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2109,2018-11-15T12:58:58Z,2018-11-26T15:18:09Z,2018-11-26T15:18:11Z,MERGED,True,965,44,12,https://github.com/edenhill,Header support for C++ API by @davidtrihy (Superceeds #1959),4,[],https://github.com/edenhill/librdkafka/pull/2109,https://github.com/edenhill,1,https://github.com/edenhill/librdkafka/pull/2109,,,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2109,2018-11-15T12:58:58Z,2018-11-26T15:18:09Z,2018-11-26T15:18:11Z,MERGED,True,965,44,12,https://github.com/edenhill,Header support for C++ API by @davidtrihy (Superceeds #1959),4,[],https://github.com/edenhill/librdkafka/pull/2109,https://github.com/davidtrihy,2,https://github.com/edenhill/librdkafka/pull/2109#issuecomment-439051515,,"Thanks for taking this, apologies for the delay in applying the review comments as I've been busy at work",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2109,2018-11-15T12:58:58Z,2018-11-26T15:18:09Z,2018-11-26T15:18:11Z,MERGED,True,965,44,12,https://github.com/edenhill,Header support for C++ API by @davidtrihy (Superceeds #1959),4,[],https://github.com/edenhill/librdkafka/pull/2109,https://github.com/rnpridgeon,3,https://github.com/edenhill/librdkafka/pull/2109#issuecomment-441661599,,LGTM,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2116,2018-11-22T13:16:13Z,2018-12-04T07:30:49Z,2018-12-04T07:33:53Z,MERGED,True,5,3,2,https://github.com/briot,Minor doc update for queue.buffering.max.*,1,[],https://github.com/edenhill/librdkafka/pull/2116,https://github.com/briot,1,https://github.com/edenhill/librdkafka/pull/2116,"The producer queue is shared for all topics and partitions (we do
not have one queue per toppar).","The producer queue is shared for all topics and partitions (we do
not have one queue per toppar).",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2116,2018-11-22T13:16:13Z,2018-12-04T07:30:49Z,2018-12-04T07:33:53Z,MERGED,True,5,3,2,https://github.com/briot,Minor doc update for queue.buffering.max.*,1,[],https://github.com/edenhill/librdkafka/pull/2116,https://github.com/briot,2,https://github.com/edenhill/librdkafka/pull/2116#issuecomment-441030977,"The producer queue is shared for all topics and partitions (we do
not have one queue per toppar).","Having second thoughts on this patch in fact. I had read the code to check whether there is a single queue, but I don't see how that would work, since we might be sending messages to different brokers depending on which one handles a specific topic).
So in fact quite possibly the size of the queue is per toppar ?",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2116,2018-11-22T13:16:13Z,2018-12-04T07:30:49Z,2018-12-04T07:33:53Z,MERGED,True,5,3,2,https://github.com/briot,Minor doc update for queue.buffering.max.*,1,[],https://github.com/edenhill/librdkafka/pull/2116,https://github.com/edenhill,3,https://github.com/edenhill/librdkafka/pull/2116#issuecomment-442871763,"The producer queue is shared for all topics and partitions (we do
not have one queue per toppar).","The count and size are per producer, not per partition:
https://github.com/edenhill/librdkafka/blob/master/src/rdkafka_int.h#L296
So your doc change is fine, but it needs to go in rdkafka_conf.c, then rebuild and commit both rdkafka_conf.c and CONFIGURATION.md, the latter being auto generated from the former.
Make sure to only commit lines to CONFIGURATION.md that you actually intended since its output depends on build options.",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2116,2018-11-22T13:16:13Z,2018-12-04T07:30:49Z,2018-12-04T07:33:53Z,MERGED,True,5,3,2,https://github.com/briot,Minor doc update for queue.buffering.max.*,1,[],https://github.com/edenhill/librdkafka/pull/2116,https://github.com/briot,4,https://github.com/edenhill/librdkafka/pull/2116#issuecomment-442884211,"The producer queue is shared for all topics and partitions (we do
not have one queue per toppar).","I had forgotten this was auto-generated, will amend the commit thanks.
After some more thoughts on the topic, it had seemed like the producer queue limits (count and size) might be per broker. If a producer sends to two brokers (because messages are sent to different topics/partition), are we still sharing the same queue ?
On the one hand, it makes it easier to limit memory usage by the producer, but it also makes it somewhat harder to control the batches in particular to get better compression",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2116,2018-11-22T13:16:13Z,2018-12-04T07:30:49Z,2018-12-04T07:33:53Z,MERGED,True,5,3,2,https://github.com/briot,Minor doc update for queue.buffering.max.*,1,[],https://github.com/edenhill/librdkafka/pull/2116,https://github.com/edenhill,5,https://github.com/edenhill/librdkafka/pull/2116#issuecomment-442885984,"The producer queue is shared for all topics and partitions (we do
not have one queue per toppar).","These limits are global per rd_kafka_t instance, not per partition nor broker.",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2116,2018-11-22T13:16:13Z,2018-12-04T07:30:49Z,2018-12-04T07:33:53Z,MERGED,True,5,3,2,https://github.com/briot,Minor doc update for queue.buffering.max.*,1,[],https://github.com/edenhill/librdkafka/pull/2116,https://github.com/edenhill,6,https://github.com/edenhill/librdkafka/pull/2116#issuecomment-444000119,"The producer queue is shared for all topics and partitions (we do
not have one queue per toppar).",Thank you!,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2120,2018-11-27T12:41:18Z,2018-11-27T12:42:45Z,2018-11-27T12:42:49Z,MERGED,True,2,2,1,https://github.com/mattclarke,Fixed some small typos in the docs,1,[],https://github.com/edenhill/librdkafka/pull/2120,https://github.com/mattclarke,1,https://github.com/edenhill/librdkafka/pull/2120,Just two minor typos,Just two minor typos,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2120,2018-11-27T12:41:18Z,2018-11-27T12:42:45Z,2018-11-27T12:42:49Z,MERGED,True,2,2,1,https://github.com/mattclarke,Fixed some small typos in the docs,1,[],https://github.com/edenhill/librdkafka/pull/2120,https://github.com/edenhill,2,https://github.com/edenhill/librdkafka/pull/2120#issuecomment-442044896,Just two minor typos,Thank you!,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2124,2018-11-28T10:48:04Z,2018-11-29T15:11:31Z,2018-11-29T15:11:34Z,MERGED,True,330,40,8,https://github.com/edenhill,Fix #2090 (Multiple MsgVers in same Fetch) and #2118 (POLLOUT loop when connecting),2,[],https://github.com/edenhill/librdkafka/pull/2124,https://github.com/edenhill,1,https://github.com/edenhill/librdkafka/pull/2124,,,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2128,2018-12-01T19:16:13Z,2018-12-01T19:21:27Z,2020-05-08T08:58:29Z,CLOSED,False,9,1,2,https://github.com/edenhill,Fixes build on OSX Mojave,2,[],https://github.com/edenhill/librdkafka/pull/2128,https://github.com/edenhill,1,https://github.com/edenhill/librdkafka/pull/2128,For #2089,For #2089,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2129,2018-12-01T19:36:04Z,2018-12-04T06:53:44Z,2018-12-04T06:54:00Z,MERGED,True,96,47,21,https://github.com/edenhill,v1.0.0 deprecations,4,[],https://github.com/edenhill/librdkafka/pull/2129,https://github.com/edenhill,1,https://github.com/edenhill/librdkafka/pull/2129,See #2020,See #2020,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2132,2018-12-05T16:26:33Z,2018-12-10T12:12:32Z,2018-12-10T20:45:35Z,MERGED,True,21,19,2,https://github.com/edenhill,Change producer default to acks=all,2,[],https://github.com/edenhill/librdkafka/pull/2132,https://github.com/edenhill,1,https://github.com/edenhill/librdkafka/pull/2132,,,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2139,2018-12-10T20:07:35Z,2018-12-10T21:50:20Z,2018-12-10T22:12:20Z,MERGED,True,14,2,2,https://github.com/edenhill,"Change connection close log levels, depending on in-flight requests",1,[],https://github.com/edenhill/librdkafka/pull/2139,https://github.com/edenhill,1,https://github.com/edenhill/librdkafka/pull/2139,,,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2139,2018-12-10T20:07:35Z,2018-12-10T21:50:20Z,2018-12-10T22:12:20Z,MERGED,True,14,2,2,https://github.com/edenhill,"Change connection close log levels, depending on in-flight requests",1,[],https://github.com/edenhill/librdkafka/pull/2139,https://github.com/edenhill,2,https://github.com/edenhill/librdkafka/pull/2139#issuecomment-445957306,,Ping @jcarey03,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2139,2018-12-10T20:07:35Z,2018-12-10T21:50:20Z,2018-12-10T22:12:20Z,MERGED,True,14,2,2,https://github.com/edenhill,"Change connection close log levels, depending on in-flight requests",1,[],https://github.com/edenhill/librdkafka/pull/2139,https://github.com/mhowlett,3,https://github.com/edenhill/librdkafka/pull/2139#issuecomment-445966371,,"LGTM. I don't have context for this change, but in case it's relevant, want to note that if the word 'error' appears in a log message, people will assume it's an error even if the level is info or debug.",True,"{'THUMBS_UP': ['https://github.com/edenhill', 'https://github.com/rnpridgeon', 'https://github.com/jcarey03']}"
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2141,2018-12-11T10:10:30Z,2021-03-25T08:31:18Z,2021-03-25T08:31:25Z,MERGED,True,2,2,1,https://github.com/pacovn,Build: avoid interactive 'mv' prompt,1,[],https://github.com/edenhill/librdkafka/pull/2141,https://github.com/pacovn,1,https://github.com/edenhill/librdkafka/pull/2141,Signed-off-by: F. Aragon paco@voltanet.io,Signed-off-by: F. Aragon paco@voltanet.io,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2141,2018-12-11T10:10:30Z,2021-03-25T08:31:18Z,2021-03-25T08:31:25Z,MERGED,True,2,2,1,https://github.com/pacovn,Build: avoid interactive 'mv' prompt,1,[],https://github.com/edenhill/librdkafka/pull/2141,https://github.com/edenhill,2,https://github.com/edenhill/librdkafka/pull/2141#issuecomment-446310099,Signed-off-by: F. Aragon paco@voltanet.io,"Thank you for this, will merge after the next release.",True,{'THUMBS_UP': ['https://github.com/pacovn']}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2141,2018-12-11T10:10:30Z,2021-03-25T08:31:18Z,2021-03-25T08:31:25Z,MERGED,True,2,2,1,https://github.com/pacovn,Build: avoid interactive 'mv' prompt,1,[],https://github.com/edenhill/librdkafka/pull/2141,https://github.com/edenhill,3,https://github.com/edenhill/librdkafka/pull/2141#issuecomment-806464042,Signed-off-by: F. Aragon paco@voltanet.io,"It took some time, but thank you!",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2142,2018-12-11T12:27:43Z,2018-12-11T20:53:41Z,2018-12-11T20:53:45Z,MERGED,True,4,0,1,https://github.com/edenhill,Consumer: don't send Leave when terminating with NO_CONSUMER_CLOSE (#2138),1,[],https://github.com/edenhill/librdkafka/pull/2142,https://github.com/edenhill,1,https://github.com/edenhill/librdkafka/pull/2142,@mhowlett Please verify this fix with the .NET client,@mhowlett Please verify this fix with the .NET client,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2142,2018-12-11T12:27:43Z,2018-12-11T20:53:41Z,2018-12-11T20:53:45Z,MERGED,True,4,0,1,https://github.com/edenhill,Consumer: don't send Leave when terminating with NO_CONSUMER_CLOSE (#2138),1,[],https://github.com/edenhill/librdkafka/pull/2142,https://github.com/mhowlett,2,https://github.com/edenhill/librdkafka/pull/2142#issuecomment-446318241,@mhowlett Please verify this fix with the .NET client,"LGTM & verified that with this branch of librdkafka, the .NET Consumer_Exiting integration test no longer hangs here: https://github.com/confluentinc/confluent-kafka-dotnet/blob/master/test/Confluent.Kafka.IntegrationTests/Tests/Consumer_Exiting.cs#L76",True,{'THUMBS_UP': ['https://github.com/edenhill']}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2146,2018-12-12T05:47:34Z,2021-03-25T08:31:50Z,2021-03-25T08:31:50Z,CLOSED,False,32,0,3,https://github.com/noahdav,Add dup config to C++ interface,4,[],https://github.com/edenhill/librdkafka/pull/2146,https://github.com/noahdav,1,https://github.com/edenhill/librdkafka/pull/2146,Added the ability to duplicate a configuration in the C++ interface.  This is exposed in the C interface but we need this in C++ interface too.,Added the ability to duplicate a configuration in the C++ interface.  This is exposed in the C interface but we need this in C++ interface too.,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2146,2018-12-12T05:47:34Z,2021-03-25T08:31:50Z,2021-03-25T08:31:50Z,CLOSED,False,32,0,3,https://github.com/noahdav,Add dup config to C++ interface,4,[],https://github.com/edenhill/librdkafka/pull/2146,https://github.com/edenhill,2,https://github.com/edenhill/librdkafka/pull/2146#issuecomment-806464257,Added the ability to duplicate a configuration in the C++ interface.  This is exposed in the C interface but we need this in C++ interface too.,"Outdated. Please update and reopen if still relevant.
Thanks",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2149,2018-12-12T16:27:34Z,2018-12-12T16:41:49Z,2018-12-12T16:41:49Z,CLOSED,False,1,1,1,https://github.com/edoardocomar,fixed typo,1,[],https://github.com/edenhill/librdkafka/pull/2149,https://github.com/edoardocomar,1,https://github.com/edenhill/librdkafka/pull/2149,,,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2149,2018-12-12T16:27:34Z,2018-12-12T16:41:49Z,2018-12-12T16:41:49Z,CLOSED,False,1,1,1,https://github.com/edoardocomar,fixed typo,1,[],https://github.com/edenhill/librdkafka/pull/2149,https://github.com/edoardocomar,2,https://github.com/edenhill/librdkafka/pull/2149#issuecomment-446655752,,"closing, opened #2150",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2150,2018-12-12T16:41:26Z,2019-01-15T07:31:38Z,2019-01-15T07:31:44Z,MERGED,True,3,3,3,https://github.com/edoardocomar,fixed typo,1,[],https://github.com/edenhill/librdkafka/pull/2150,https://github.com/edoardocomar,1,https://github.com/edenhill/librdkafka/pull/2150,,,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2150,2018-12-12T16:41:26Z,2019-01-15T07:31:38Z,2019-01-15T07:31:44Z,MERGED,True,3,3,3,https://github.com/edoardocomar,fixed typo,1,[],https://github.com/edenhill/librdkafka/pull/2150,https://github.com/dnwe,2,https://github.com/edenhill/librdkafka/pull/2150#issuecomment-453560779,,Fixes #2148,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2150,2018-12-12T16:41:26Z,2019-01-15T07:31:38Z,2019-01-15T07:31:44Z,MERGED,True,3,3,3,https://github.com/edoardocomar,fixed typo,1,[],https://github.com/edenhill/librdkafka/pull/2150,https://github.com/edenhill,3,https://github.com/edenhill/librdkafka/pull/2150#issuecomment-454293275,,Thank you!,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2152,2018-12-12T22:18:38Z,2018-12-17T18:34:10Z,2018-12-17T18:34:13Z,MERGED,True,697,266,16,https://github.com/edenhill,Use separate connection to the group coordinator,4,[],https://github.com/edenhill/librdkafka/pull/2152,https://github.com/edenhill,1,https://github.com/edenhill/librdkafka/pull/2152,"Create separate connection to consumer Group Coordinator

Since JoinGroupRequests may block for up to max.poll.interval.ms,
which may be set very high (hours..), any sub-sequent requests
on the same connection, such as Metadata refreshes, would time out
and tear down the connection, triggering another rebalance.

This commit adds support for logical brokers, in this case
a GroupCoordinator broker, which initially doesn't have an address
but is updated by the cgrp to connect to the group coordinator.
This connection is then used for all group-related protocol requests,
Join, Sync, Leave, OffsetCommit, leaving the standard broker
connections for other protocol use.

As a side-effect this commit also deletes legacy cgrp code from when
it used to run on the broker thread.

Also improves HOLB detection and handling.","Create separate connection to consumer Group Coordinator

Since JoinGroupRequests may block for up to max.poll.interval.ms,
which may be set very high (hours..), any sub-sequent requests
on the same connection, such as Metadata refreshes, would time out
and tear down the connection, triggering another rebalance.

This commit adds support for logical brokers, in this case
a GroupCoordinator broker, which initially doesn't have an address
but is updated by the cgrp to connect to the group coordinator.
This connection is then used for all group-related protocol requests,
Join, Sync, Leave, OffsetCommit, leaving the standard broker
connections for other protocol use.

As a side-effect this commit also deletes legacy cgrp code from when
it used to run on the broker thread.

Also improves HOLB detection and handling.",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2152,2018-12-12T22:18:38Z,2018-12-17T18:34:10Z,2018-12-17T18:34:13Z,MERGED,True,697,266,16,https://github.com/edenhill,Use separate connection to the group coordinator,4,[],https://github.com/edenhill/librdkafka/pull/2152,https://github.com/mhowlett,2,https://github.com/edenhill/librdkafka/pull/2152#issuecomment-446784371,"Create separate connection to consumer Group Coordinator

Since JoinGroupRequests may block for up to max.poll.interval.ms,
which may be set very high (hours..), any sub-sequent requests
on the same connection, such as Metadata refreshes, would time out
and tear down the connection, triggering another rebalance.

This commit adds support for logical brokers, in this case
a GroupCoordinator broker, which initially doesn't have an address
but is updated by the cgrp to connect to the group coordinator.
This connection is then used for all group-related protocol requests,
Join, Sync, Leave, OffsetCommit, leaving the standard broker
connections for other protocol use.

As a side-effect this commit also deletes legacy cgrp code from when
it used to run on the broker thread.

Also improves HOLB detection and handling.","LGTM, probably want a test or two?",True,{'THUMBS_UP': ['https://github.com/edenhill']}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2155,2018-12-13T17:15:21Z,2019-01-10T10:40:17Z,2019-01-10T10:42:07Z,MERGED,True,1,4,1,https://github.com/mimaison,Include error message in CreatePartitions result,2,[],https://github.com/edenhill/librdkafka/pull/2155,https://github.com/mimaison,1,https://github.com/edenhill/librdkafka/pull/2155,"fixes #2154
Co-authored-by: Mickael Maison mickael.maison@gmail.com
Co-authored-by: Edoardo Comar ecomar@uk.ibm.com","fixes #2154
Co-authored-by: Mickael Maison mickael.maison@gmail.com
Co-authored-by: Edoardo Comar ecomar@uk.ibm.com",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2155,2018-12-13T17:15:21Z,2019-01-10T10:40:17Z,2019-01-10T10:42:07Z,MERGED,True,1,4,1,https://github.com/mimaison,Include error message in CreatePartitions result,2,[],https://github.com/edenhill/librdkafka/pull/2155,https://github.com/edenhill,2,https://github.com/edenhill/librdkafka/pull/2155#issuecomment-453050419,"fixes #2154
Co-authored-by: Mickael Maison mickael.maison@gmail.com
Co-authored-by: Edoardo Comar ecomar@uk.ibm.com",Thank you for this!,True,{'THUMBS_UP': ['https://github.com/mimaison']}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2157,2018-12-17T07:01:08Z,2019-01-24T10:43:27Z,2019-01-24T10:43:27Z,CLOSED,False,1,1,1,https://github.com/yedong113,Update librdkafka.vcxproj,1,[],https://github.com/edenhill/librdkafka/pull/2157,https://github.com/yedong113,1,https://github.com/edenhill/librdkafka/pull/2157,,,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2157,2018-12-17T07:01:08Z,2019-01-24T10:43:27Z,2019-01-24T10:43:27Z,CLOSED,False,1,1,1,https://github.com/yedong113,Update librdkafka.vcxproj,1,[],https://github.com/edenhill/librdkafka/pull/2157,https://github.com/edenhill,2,https://github.com/edenhill/librdkafka/pull/2157#issuecomment-447763531,,What's the reason for this change?,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2160,2018-12-18T22:55:23Z,,2021-03-04T07:36:37Z,OPEN,False,160,3,3,https://github.com/JonECG,[WIP] Add rd_kafka_query_watermark_offsets_list api (#1946),1,[],https://github.com/edenhill/librdkafka/pull/2160,https://github.com/JonECG,1,https://github.com/edenhill/librdkafka/pull/2160,"Allows multiple watermarks in a single request
#1946","Allows multiple watermarks in a single request
#1946",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2160,2018-12-18T22:55:23Z,,2021-03-04T07:36:37Z,OPEN,False,160,3,3,https://github.com/JonECG,[WIP] Add rd_kafka_query_watermark_offsets_list api (#1946),1,[],https://github.com/edenhill/librdkafka/pull/2160,https://github.com/QuentinPerez,2,https://github.com/edenhill/librdkafka/pull/2160#issuecomment-679933793,"Allows multiple watermarks in a single request
#1946","hi @edenhill, any plan to merge it ?
I need to fetch the high watermark for many partitions and with the current API that can take a lot of time to fetch them. (don't know if there is another way to do it)
Let me know if I can help :)",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2160,2018-12-18T22:55:23Z,,2021-03-04T07:36:37Z,OPEN,False,160,3,3,https://github.com/JonECG,[WIP] Add rd_kafka_query_watermark_offsets_list api (#1946),1,[],https://github.com/edenhill/librdkafka/pull/2160,https://github.com/mxk1235,3,https://github.com/edenhill/librdkafka/pull/2160#issuecomment-785672750,"Allows multiple watermarks in a single request
#1946","@JonECG @QuentinPerez this is cool, we use something similar, although we grab both earliest and latest offset in one function call.
my fork isn't quite ready for a PR, but i would welcome any input mxk1235@4e08ca4
fwiw, I am trying to track down an UNKNOWN_PARTITION error that pops up relatively often, yet appears transient.",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2160,2018-12-18T22:55:23Z,,2021-03-04T07:36:37Z,OPEN,False,160,3,3,https://github.com/JonECG,[WIP] Add rd_kafka_query_watermark_offsets_list api (#1946),1,[],https://github.com/edenhill/librdkafka/pull/2160,https://github.com/mxk1235,4,https://github.com/edenhill/librdkafka/pull/2160#issuecomment-788644723,"Allows multiple watermarks in a single request
#1946","just wanted to add, UKNOWN_PARTITION errors appear to be retry-able on the consumer side.  it's hard to catch in progress, possibly due to OffsetRequest responses being in-flight.   consumer->poll(100) makes sure all responses are processed, and then rd_kafka_query_watermark_offsets_list() can be retried.",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2161,2018-12-19T12:19:52Z,2018-12-20T08:40:51Z,2018-12-20T08:40:54Z,MERGED,True,1,1,1,https://github.com/ankon,Fix trivial typo in comment,1,[],https://github.com/edenhill/librdkafka/pull/2161,https://github.com/ankon,1,https://github.com/edenhill/librdkafka/pull/2161,,,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2161,2018-12-19T12:19:52Z,2018-12-20T08:40:51Z,2018-12-20T08:40:54Z,MERGED,True,1,1,1,https://github.com/ankon,Fix trivial typo in comment,1,[],https://github.com/edenhill/librdkafka/pull/2161,https://github.com/edenhill,2,https://github.com/edenhill/librdkafka/pull/2161#issuecomment-448917783,,Gracias!,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2162,2018-12-19T12:23:58Z,2018-12-20T08:40:33Z,2018-12-20T08:40:37Z,MERGED,True,1,1,1,https://github.com/ankon,Fix trivial typo in documentation,1,[],https://github.com/edenhill/librdkafka/pull/2162,https://github.com/ankon,1,https://github.com/edenhill/librdkafka/pull/2162,,,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2162,2018-12-19T12:23:58Z,2018-12-20T08:40:33Z,2018-12-20T08:40:37Z,MERGED,True,1,1,1,https://github.com/ankon,Fix trivial typo in documentation,1,[],https://github.com/edenhill/librdkafka/pull/2162,https://github.com/edenhill,2,https://github.com/edenhill/librdkafka/pull/2162#issuecomment-448917715,,Thank you!,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2177,2019-01-08T00:20:28Z,,2020-04-23T07:49:36Z,OPEN,False,34,2,2,https://github.com/LavaSpider,"If the fetch list has changed, start fetching from a random position.",1,[],https://github.com/edenhill/librdkafka/pull/2177,https://github.com/LavaSpider,1,https://github.com/edenhill/librdkafka/pull/2177,"This is to address the case where a program is too busy, so it has paused all consuming, then resumes when it is no longer busy. On resume, the fetch order will always be in the order of the topic/partition assignment. This means that if the busy state happens frequently, partitions earlier in the assignment will be preferred for consuming messages.","This is to address the case where a program is too busy, so it has paused all consuming, then resumes when it is no longer busy. On resume, the fetch order will always be in the order of the topic/partition assignment. This means that if the busy state happens frequently, partitions earlier in the assignment will be preferred for consuming messages.",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2177,2019-01-08T00:20:28Z,,2020-04-23T07:49:36Z,OPEN,False,34,2,2,https://github.com/LavaSpider,"If the fetch list has changed, start fetching from a random position.",1,[],https://github.com/edenhill/librdkafka/pull/2177,https://github.com/edenhill,2,https://github.com/edenhill/librdkafka/pull/2177#issuecomment-452210413,"This is to address the case where a program is too busy, so it has paused all consuming, then resumes when it is no longer busy. On resume, the fetch order will always be in the order of the topic/partition assignment. This means that if the busy state happens frequently, partitions earlier in the assignment will be preferred for consuming messages.",Thank you for this. This will not make the v1.0.0 release so will let it sit until the release is out.,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2177,2019-01-08T00:20:28Z,,2020-04-23T07:49:36Z,OPEN,False,34,2,2,https://github.com/LavaSpider,"If the fetch list has changed, start fetching from a random position.",1,[],https://github.com/edenhill/librdkafka/pull/2177,https://github.com/LavaSpider,3,https://github.com/edenhill/librdkafka/pull/2177#issuecomment-452397931,"This is to address the case where a program is too busy, so it has paused all consuming, then resumes when it is no longer busy. On resume, the fetch order will always be in the order of the topic/partition assignment. This means that if the busy state happens frequently, partitions earlier in the assignment will be preferred for consuming messages.","Sounds good, glad I could help",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2178,2019-01-08T06:40:31Z,2019-01-08T08:01:21Z,2019-01-08T08:01:26Z,MERGED,True,1,1,1,https://github.com/Vijendra07Kulhade,Error: Local: Host resolution failure #2171,1,[],https://github.com/edenhill/librdkafka/pull/2178,https://github.com/Vijendra07Kulhade,1,https://github.com/edenhill/librdkafka/pull/2178,"@edenhill Please review

Updated the  NODE Name size to 256","@edenhill Please review

Updated the  NODE Name size to 256",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2178,2019-01-08T06:40:31Z,2019-01-08T08:01:21Z,2019-01-08T08:01:26Z,MERGED,True,1,1,1,https://github.com/Vijendra07Kulhade,Error: Local: Host resolution failure #2171,1,[],https://github.com/edenhill/librdkafka/pull/2178,https://github.com/edenhill,2,https://github.com/edenhill/librdkafka/pull/2178#issuecomment-452207602,"@edenhill Please review

Updated the  NODE Name size to 256",Thank you!,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2181,2019-01-09T08:36:21Z,2019-01-09T15:49:19Z,2019-01-09T15:49:23Z,MERGED,True,34,19,2,https://github.com/edenhill,"CONFIGURATION.md: provide full range,default,description,.. for aliases",1,[],https://github.com/edenhill/librdkafka/pull/2181,https://github.com/edenhill,1,https://github.com/edenhill/librdkafka/pull/2181,,,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2183,2019-01-10T23:02:46Z,2020-02-12T16:08:18Z,2020-02-12T16:08:18Z,CLOSED,False,177,33,3,https://github.com/Donis-,Added minimal v10 fetch protocol support for ZSTD,1,[],https://github.com/edenhill/librdkafka/pull/2183,https://github.com/Donis-,1,https://github.com/edenhill/librdkafka/pull/2183,"As per KIP-110, consumers must support v10 fetch to consume records
compressed with zstd.
This commit fixes Err-76 when fetching ZSTD enabled records from broker version >= 2.1.
Changes that have impact encapsulated under WITH_ZSTD define check","As per KIP-110, consumers must support v10 fetch to consume records
compressed with zstd.
This commit fixes Err-76 when fetching ZSTD enabled records from broker version >= 2.1.
Changes that have impact encapsulated under WITH_ZSTD define check",True,"{'HOORAY': ['https://github.com/rgomezcasas'], 'THUMBS_UP': ['https://github.com/brunosaboia', 'https://github.com/ochedru']}"
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2183,2019-01-10T23:02:46Z,2020-02-12T16:08:18Z,2020-02-12T16:08:18Z,CLOSED,False,177,33,3,https://github.com/Donis-,Added minimal v10 fetch protocol support for ZSTD,1,[],https://github.com/edenhill/librdkafka/pull/2183,https://github.com/Donis-,2,https://github.com/edenhill/librdkafka/pull/2183#issuecomment-453509716,"As per KIP-110, consumers must support v10 fetch to consume records
compressed with zstd.
This commit fixes Err-76 when fetching ZSTD enabled records from broker version >= 2.1.
Changes that have impact encapsulated under WITH_ZSTD define check","Local tests had one failure (likely unrelated?):
TEST 20190111131440 (bare) SUMMARY
#==================================================================#
|                                    |     PASSED |  45.753s |
| 0000_unittests                           |     PASSED |   0.108s |
| 0004_conf                                |     PASSED |   0.827s |
| 0006_symbols                             |     PASSED |   0.000s |
| 0025_timers                              |     PASSED |   6.131s |
| 0033_regex_subscribe_local               |     PASSED |   0.050s |
| 0037_destroy_hang_local                  |     PASSED |   1.220s |
| 0039_event                               |     PASSED |   0.182s |
| 0043_no_connection                       |     PASSED |   1.269s |
| 0046_rkt_cache                           |     PASSED |   0.077s |
| 0053_stats_timing                        |     PASSED |   1.234s |
| 0058_log                                 |     PASSED |   8.112s |
| 0062_stats_event                         |     FAILED |  44.016s | main_0062_stats_event():109: Stats duration 44.339ms is >= 50% outside statistics.interval.ms 100
| 0072_headers_ut                          |     PASSED |   1.040s |
| 0074_producev                            |     PASSED |   0.016s |
| 0078_c_from_cpp                          |     PASSED |   0.039s |
| 0080_admin_ut                            |     PASSED |  42.617s |
| 0084_destroy_flags_local                 |     PASSED |   4.808s |
| 0086_purge_local                         |     PASSED |   0.510s |
#==================================================================#
I've only run manual test for fetch with kafkacat, it works on a 2.1 broker with zstd set as default (the reason for this PR).
At the moment i don't have the environment for other integration tests, would need to set it up. Maybe it's automated somewhere? or we need to do it locally?",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2183,2019-01-10T23:02:46Z,2020-02-12T16:08:18Z,2020-02-12T16:08:18Z,CLOSED,False,177,33,3,https://github.com/Donis-,Added minimal v10 fetch protocol support for ZSTD,1,[],https://github.com/edenhill/librdkafka/pull/2183,https://github.com/edenhill,3,https://github.com/edenhill/librdkafka/pull/2183#issuecomment-453544727,"As per KIP-110, consumers must support v10 fetch to consume records
compressed with zstd.
This commit fixes Err-76 when fetching ZSTD enabled records from broker version >= 2.1.
Changes that have impact encapsulated under WITH_ZSTD define check","Those are local tests not using a broker.
You should run the full test suite, which requires a cluster with topic-auto-creation enabled.
If you have an existing cluster you can add bootstrap.servers=yourbroker in tests/test.conf and run make in tests/.
If you want a cluster set up for you, you can do:
$ virtualenv venv
$ source venv/bin/activate
$ pip install trivup
$ cd tests
$ ./interactive_broker_version.py 2.0.0  # this executes a sub-shell
trivup-something-prompt$ make   # to run tests
# repeat as necessary (make or 'TESTS=0017 make' to run a single test)
$ exit   # to kill cluster",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2183,2019-01-10T23:02:46Z,2020-02-12T16:08:18Z,2020-02-12T16:08:18Z,CLOSED,False,177,33,3,https://github.com/Donis-,Added minimal v10 fetch protocol support for ZSTD,1,[],https://github.com/edenhill/librdkafka/pull/2183,https://github.com/Donis-,4,https://github.com/edenhill/librdkafka/pull/2183#issuecomment-453769178,"As per KIP-110, consumers must support v10 fetch to consume records
compressed with zstd.
This commit fixes Err-76 when fetching ZSTD enabled records from broker version >= 2.1.
Changes that have impact encapsulated under WITH_ZSTD define check","i got the environment up and running.
Following regression went fine:
broker
broker_idempotent
But i'm getting failures with:
non_sparse_connections
rdkafka error: Local: All broker connections are down: 1/1 brokers are down
Same issue without my changes, directly on 8c5c46e@master
E.g. test case 0019:
PASS enable.sparse.connections=true
pass.log
FAIL enable.sparse.connections=false
fail.log
Already increased ulimit -n to 999999,
Something still wrong with my environment?",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2183,2019-01-10T23:02:46Z,2020-02-12T16:08:18Z,2020-02-12T16:08:18Z,CLOSED,False,177,33,3,https://github.com/Donis-,Added minimal v10 fetch protocol support for ZSTD,1,[],https://github.com/edenhill/librdkafka/pull/2183,https://github.com/Donis-,5,https://github.com/edenhill/librdkafka/pull/2183#issuecomment-460062596,"As per KIP-110, consumers must support v10 fetch to consume records
compressed with zstd.
This commit fixes Err-76 when fetching ZSTD enabled records from broker version >= 2.1.
Changes that have impact encapsulated under WITH_ZSTD define check","baseline ""enable.sparse.connections=false"" failures with:
rdkafka error: Local: All broker connections are down: 1/1 brokers are down
Seem to be caused by rd_kafka_op_err with RD_KAFKA_RESP_ERR__RESOLVE / ""Logical broker has no address yet"".
PASS.log
FAIL.log
rd_kafka_op_err_callstack.txt
Cherry-pick of 953814d fixes the issue.",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2183,2019-01-10T23:02:46Z,2020-02-12T16:08:18Z,2020-02-12T16:08:18Z,CLOSED,False,177,33,3,https://github.com/Donis-,Added minimal v10 fetch protocol support for ZSTD,1,[],https://github.com/edenhill/librdkafka/pull/2183,https://github.com/Donis-,6,https://github.com/edenhill/librdkafka/pull/2183#issuecomment-460086912,"As per KIP-110, consumers must support v10 fetch to consume records
compressed with zstd.
This commit fixes Err-76 when fetching ZSTD enabled records from broker version >= 2.1.
Changes that have impact encapsulated under WITH_ZSTD define check","With 953814d on top of v10_fetch_for_zstd make non_sparse_connections is also a PASS:
./broker_version_tests.py --brokers 5 \
        --conf '{""parallel"":1, ""args"": ""-L"", ""sparse_connections"": ""false""}' 2.0.0
#### Version 2.0.0, suite standard: STARTING
# librdkafka regression tests started, logs in /home/donis/kafka/librdkafka/tests/tmp/LibrdkafkaTestCluster/49211205/LibrdkafkaTestApp/c70a36e0
#### Version 2.0.0, suite standard: PASSED: All 62/62 tests passed as expected
#### Test output: /home/donis/kafka/librdkafka/tests/tmp/LibrdkafkaTestCluster/49211205/LibrdkafkaTestApp/c70a36e0/stderr.log
#### Full test suite report (1 suite(s))
PASSED  standard @ 2.0.0                                  : All 62/62 tests passed as expected
#### 1 suites PASSED, 0 suites FAILED
#### Full test suites report in: /home/donis/kafka/librdkafka/tests/test_suite_H5Mpcx.json

./broker_version_tests.py --conf '{""parallel"":1}' 2.0.0
#### Version 2.0.0, suite standard: STARTING
# librdkafka regression tests started, logs in /home/donis/kafka/librdkafka/tests/tmp/LibrdkafkaTestCluster/49208233/LibrdkafkaTestApp/75ef4fe1
#### Version 2.0.0, suite standard: PASSED: All 81/81 tests passed as expected
#### Test output: /home/donis/kafka/librdkafka/tests/tmp/LibrdkafkaTestCluster/49208233/LibrdkafkaTestApp/75ef4fe1/stderr.log
#### Full test suite report (1 suite(s))
PASSED  standard @ 2.0.0                                  : All 81/81 tests passed as expected
#### 1 suites PASSED, 0 suites FAILED
#### Full test suites report in: /home/donis/kafka/librdkafka/tests/test_suite_oFUF9L.json

./broker_version_tests.py --conf '{""parallel"":1, ""args"":""-P -L""}' 2.0.0
#### Version 2.0.0, suite standard: STARTING
# librdkafka regression tests started, logs in /home/donis/kafka/librdkafka/tests/tmp/LibrdkafkaTestCluster/49209199/LibrdkafkaTestApp/beacc4bd
#### Version 2.0.0, suite standard: PASSED: All 61/61 tests passed as expected
#### Test output: /home/donis/kafka/librdkafka/tests/tmp/LibrdkafkaTestCluster/49209199/LibrdkafkaTestApp/beacc4bd/stderr.log
#### Full test suite report (1 suite(s))
PASSED  standard @ 2.0.0                                  : All 61/61 tests passed as expected
#### 1 suites PASSED, 0 suites FAILED
#### Full test suites report in: /home/donis/kafka/librdkafka/tests/test_suite_WCygaY.json",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2183,2019-01-10T23:02:46Z,2020-02-12T16:08:18Z,2020-02-12T16:08:18Z,CLOSED,False,177,33,3,https://github.com/Donis-,Added minimal v10 fetch protocol support for ZSTD,1,[],https://github.com/edenhill/librdkafka/pull/2183,https://github.com/Donis-,7,https://github.com/edenhill/librdkafka/pull/2183#issuecomment-460087348,"As per KIP-110, consumers must support v10 fetch to consume records
compressed with zstd.
This commit fixes Err-76 when fetching ZSTD enabled records from broker version >= 2.1.
Changes that have impact encapsulated under WITH_ZSTD define check","test_suite_H5Mpcx.json.txt
test_suite_oFUF9L.json.txt
test_suite_WCygaY.json.txt",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2183,2019-01-10T23:02:46Z,2020-02-12T16:08:18Z,2020-02-12T16:08:18Z,CLOSED,False,177,33,3,https://github.com/Donis-,Added minimal v10 fetch protocol support for ZSTD,1,[],https://github.com/edenhill/librdkafka/pull/2183,https://github.com/edenhill,8,https://github.com/edenhill/librdkafka/pull/2183#issuecomment-462244045,"As per KIP-110, consumers must support v10 fetch to consume records
compressed with zstd.
This commit fixes Err-76 when fetching ZSTD enabled records from broker version >= 2.1.
Changes that have impact encapsulated under WITH_ZSTD define check","This only affects topics configured with compression.type=zstd, for ..=producer (default) the existing Fetch v4 is sufficient.",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2183,2019-01-10T23:02:46Z,2020-02-12T16:08:18Z,2020-02-12T16:08:18Z,CLOSED,False,177,33,3,https://github.com/Donis-,Added minimal v10 fetch protocol support for ZSTD,1,[],https://github.com/edenhill/librdkafka/pull/2183,https://github.com/edenhill,9,https://github.com/edenhill/librdkafka/pull/2183#issuecomment-462244155,"As per KIP-110, consumers must support v10 fetch to consume records
compressed with zstd.
This commit fixes Err-76 when fetching ZSTD enabled records from broker version >= 2.1.
Changes that have impact encapsulated under WITH_ZSTD define check","You can ignore the non_sparse_connections tests (they should be removed).
Thank you for running the full test suite ",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2183,2019-01-10T23:02:46Z,2020-02-12T16:08:18Z,2020-02-12T16:08:18Z,CLOSED,False,177,33,3,https://github.com/Donis-,Added minimal v10 fetch protocol support for ZSTD,1,[],https://github.com/edenhill/librdkafka/pull/2183,https://github.com/Donis-,10,https://github.com/edenhill/librdkafka/pull/2183#issuecomment-465812754,"As per KIP-110, consumers must support v10 fetch to consume records
compressed with zstd.
This commit fixes Err-76 when fetching ZSTD enabled records from broker version >= 2.1.
Changes that have impact encapsulated under WITH_ZSTD define check","This only affects topics configured with compression.type=zstd, for ..=producer (default) the existing Fetch v4 is sufficient.

In order to detect compression.type=zstd in librdkafka we would need to initiate rd_kafka_DescribeConfigs via the admin interface, right? (e.g. from in kafka_topics.c)
Can it be too risky to introduce for a workaround that would become obsolete once v10 fetch is fully supported by librdkafka?
Not sure if we can somehow further reduce the risk of:
`
ApiVersion = rd_kafka_broker_ApiVersion_supported(
rkb, RD_KAFKAP_Fetch, 0, 10, NULL);
    /* if v10 fetch is supported, set it to fetch from topics with
     * compression.type = zstd, otherwise default to use max v4 */
    if (ApiVersion == 10)
            rd_kafka_buf_ApiVersion_set(rkbuf, ApiVersion, 0);
    else if (rkb->rkb_features & RD_KAFKA_FEATURE_MSGVER2)

`
?
Test results:
/broker_version_tests.py --conf '{""parallel"":1}' 2.0.0
#### Version 2.0.0, suite standard: STARTING
### /home/donis/.local/lib/python2.7/site-packages/trivup/apps/KafkaBrokerApp/deploy.sh: Downloading 2.0.0 from http://apache.mirrors.spacedump.net/kafka/2.0.0/kafka_2.11-2.0.0.tgz
### /home/donis/.local/lib/python2.7/site-packages/trivup/apps/KafkaBrokerApp/deploy.sh: Successfully installed 2.0.0 to /home/donis/kafka/librdkafka/tests/tmp/LibrdkafkaTestCluster/KafkaBrokerApp/kafka/2.0.0
# librdkafka regression tests started, logs in /home/donis/kafka/librdkafka/tests/tmp/LibrdkafkaTestCluster/50706926/LibrdkafkaTestApp/afcc8e74
#### Version 2.0.0, suite standard: PASSED: All 80/80 tests passed as expected
#### Test output: /home/donis/kafka/librdkafka/tests/tmp/LibrdkafkaTestCluster/50706926/LibrdkafkaTestApp/afcc8e74/stderr.log




#### Full test suite report (1 suite(s))
PASSED  standard @ 2.0.0                                  : All 80/80 tests passed as expected
#### 1 suites PASSED, 0 suites FAILED
#### Full test suites report in: /home/donis/kafka/librdkafka/tests/test_suite_XKl30i.json
./broker_version_tests.py --conf '{""parallel"":1, ""args"":""-P -L""}' 2.0.0
#### Version 2.0.0, suite standard: STARTING
# librdkafka regression tests started, logs in /home/donis/kafka/librdkafka/tests/tmp/LibrdkafkaTestCluster/50707927/LibrdkafkaTestApp/d4e30219
#### Version 2.0.0, suite standard: PASSED: All 60/60 tests passed as expected
#### Test output: /home/donis/kafka/librdkafka/tests/tmp/LibrdkafkaTestCluster/50707927/LibrdkafkaTestApp/d4e30219/stderr.log




#### Full test suite report (1 suite(s))
PASSED  standard @ 2.0.0                                  : All 60/60 tests passed as expected
#### 1 suites PASSED, 0 suites FAILED
#### Full test suites report in: /home/donis/kafka/librdkafka/tests/test_suite_kKAdIk.json",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2183,2019-01-10T23:02:46Z,2020-02-12T16:08:18Z,2020-02-12T16:08:18Z,CLOSED,False,177,33,3,https://github.com/Donis-,Added minimal v10 fetch protocol support for ZSTD,1,[],https://github.com/edenhill/librdkafka/pull/2183,https://github.com/jeffwidman,11,https://github.com/edenhill/librdkafka/pull/2183#issuecomment-485545672,"As per KIP-110, consumers must support v10 fetch to consume records
compressed with zstd.
This commit fixes Err-76 when fetching ZSTD enabled records from broker version >= 2.1.
Changes that have impact encapsulated under WITH_ZSTD define check","Nudge @edenhill / @Donis-, not sure which one of you this is waiting on, but it'd be nice to see it land... ",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2183,2019-01-10T23:02:46Z,2020-02-12T16:08:18Z,2020-02-12T16:08:18Z,CLOSED,False,177,33,3,https://github.com/Donis-,Added minimal v10 fetch protocol support for ZSTD,1,[],https://github.com/edenhill/librdkafka/pull/2183,https://github.com/Donis-,12,https://github.com/edenhill/librdkafka/pull/2183#issuecomment-487421205,"As per KIP-110, consumers must support v10 fetch to consume records
compressed with zstd.
This commit fixes Err-76 when fetching ZSTD enabled records from broker version >= 2.1.
Changes that have impact encapsulated under WITH_ZSTD define check","@edenhill, in the lastest push added a test that failed before this PR and is passing with the changes. The check is still in rd_kafka_broker_fetch_toppars without checking of ""compression.type"" topic config.
Do we need a check against ""compression.type"" via rd_kafka_DescribeConfigs ?",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2183,2019-01-10T23:02:46Z,2020-02-12T16:08:18Z,2020-02-12T16:08:18Z,CLOSED,False,177,33,3,https://github.com/Donis-,Added minimal v10 fetch protocol support for ZSTD,1,[],https://github.com/edenhill/librdkafka/pull/2183,https://github.com/inv2004,13,https://github.com/edenhill/librdkafka/pull/2183#issuecomment-502767982,"As per KIP-110, consumers must support v10 fetch to consume records
compressed with zstd.
This commit fixes Err-76 when fetching ZSTD enabled records from broker version >= 2.1.
Changes that have impact encapsulated under WITH_ZSTD define check","Hello.
Any news on it?",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2183,2019-01-10T23:02:46Z,2020-02-12T16:08:18Z,2020-02-12T16:08:18Z,CLOSED,False,177,33,3,https://github.com/Donis-,Added minimal v10 fetch protocol support for ZSTD,1,[],https://github.com/edenhill/librdkafka/pull/2183,https://github.com/edenhill,14,https://github.com/edenhill/librdkafka/pull/2183#issuecomment-502778124,"As per KIP-110, consumers must support v10 fetch to consume records
compressed with zstd.
This commit fixes Err-76 when fetching ZSTD enabled records from broker version >= 2.1.
Changes that have impact encapsulated under WITH_ZSTD define check","Sorry for the delay, this will go in the v1.2.0 (after the summer).",True,{'THUMBS_UP': ['https://github.com/inv2004']}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2183,2019-01-10T23:02:46Z,2020-02-12T16:08:18Z,2020-02-12T16:08:18Z,CLOSED,False,177,33,3,https://github.com/Donis-,Added minimal v10 fetch protocol support for ZSTD,1,[],https://github.com/edenhill/librdkafka/pull/2183,https://github.com/edenhill,15,https://github.com/edenhill/librdkafka/pull/2183#issuecomment-521581124,"As per KIP-110, consumers must support v10 fetch to consume records
compressed with zstd.
This commit fixes Err-76 when fetching ZSTD enabled records from broker version >= 2.1.
Changes that have impact encapsulated under WITH_ZSTD define check",Need to punt this one one more release since it needs to be synched with @mhowlett's consumer work.,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2183,2019-01-10T23:02:46Z,2020-02-12T16:08:18Z,2020-02-12T16:08:18Z,CLOSED,False,177,33,3,https://github.com/Donis-,Added minimal v10 fetch protocol support for ZSTD,1,[],https://github.com/edenhill/librdkafka/pull/2183,https://github.com/edenhill,16,https://github.com/edenhill/librdkafka/pull/2183#issuecomment-582333561,"As per KIP-110, consumers must support v10 fetch to consume records
compressed with zstd.
This commit fixes Err-76 when fetching ZSTD enabled records from broker version >= 2.1.
Changes that have impact encapsulated under WITH_ZSTD define check","librdkafka now supports Fetch version 11, so I think this PR is no longer needed.",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2185,2019-01-11T17:40:52Z,2019-01-15T07:39:55Z,2019-01-15T07:39:55Z,MERGED,True,8,6,1,https://github.com/mhowlett,less aggressive lz4 estimated uncompressed size,2,[],https://github.com/edenhill/librdkafka/pull/2185,https://github.com/mhowlett,1,https://github.com/edenhill/librdkafka/pull/2185,"I believe 4x is a good constant here. Based on measurements included in the cited reference, it will cover the majority of cases. On the flip side of this, any adjustment up results in meaningfully more memory usage in the scenario i've been testing.
including contentSize in the lz4 frame is marked as a TODO in the java code. we should encourage them to add this, as it would be very beneficial for librdkafka.","I believe 4x is a good constant here. Based on measurements included in the cited reference, it will cover the majority of cases. On the flip side of this, any adjustment up results in meaningfully more memory usage in the scenario i've been testing.
including contentSize in the lz4 frame is marked as a TODO in the java code. we should encourage them to add this, as it would be very beneficial for librdkafka.",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2188,2019-01-16T15:05:54Z,2019-02-05T13:53:02Z,2019-02-05T13:53:06Z,MERGED,True,1890,675,50,https://github.com/edenhill,Proper handling of message timeouts with the idempotent producer,19,[],https://github.com/edenhill/librdkafka/pull/2188,https://github.com/edenhill,1,https://github.com/edenhill/librdkafka/pull/2188,"Issue #2163
.. and a recent regression fix for consumer hanging when group coordinator broker goes down and comes up, thank you soak tests.","Issue #2163
.. and a recent regression fix for consumer hanging when group coordinator broker goes down and comes up, thank you soak tests.",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2188,2019-01-16T15:05:54Z,2019-02-05T13:53:02Z,2019-02-05T13:53:06Z,MERGED,True,1890,675,50,https://github.com/edenhill,Proper handling of message timeouts with the idempotent producer,19,[],https://github.com/edenhill/librdkafka/pull/2188,https://github.com/edenhill,2,https://github.com/edenhill/librdkafka/pull/2188#issuecomment-455556186,"Issue #2163
.. and a recent regression fix for consumer hanging when group coordinator broker goes down and comes up, thank you soak tests.","Changes since last round of reviews:

Disabled gapless guarantee by default, and mark the property as experimental.
Fix idempo state hang when draining partitions and there is nothing to drain.
Fix race condition in produce_batch() which could cause messages to get stuck in the unassigned partition queue.
Log warning when experimental properties are used
Fix some minor issues (stats, output, tests, etc).",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2189,2019-01-18T04:09:25Z,2019-04-11T12:24:42Z,2019-05-20T15:33:37Z,MERGED,True,2259,14,30,https://github.com/rondagostino,Add KIP-255 SASL/OAUTHBEARER support,105,[],https://github.com/edenhill/librdkafka/pull/2189,https://github.com/rondagostino,1,https://github.com/edenhill/librdkafka/pull/2189,"This is an initial commit for feedback.
There are no tests as of yet.
No C++, hard-coded unsecured token regardless of config, no SASL extensions.
Note that kafkacat only works if we add the statement rd_kafka_poll(conf.rk, 0); at lines 309, 584, 667, and 860 in kafkacat.c.  See the comment associated with rd_kafka_conf_set_oauthbearer_token_refresh_cb() for details (https://github.com/rondagostino/librdkafka/blob/rtd_kip-255-oauthbearer/src/rdkafka.h#L1748).
Signed-off-by: Ron Dagostino rndgstn@gmail.com","This is an initial commit for feedback.
There are no tests as of yet.
No C++, hard-coded unsecured token regardless of config, no SASL extensions.
Note that kafkacat only works if we add the statement rd_kafka_poll(conf.rk, 0); at lines 309, 584, 667, and 860 in kafkacat.c.  See the comment associated with rd_kafka_conf_set_oauthbearer_token_refresh_cb() for details (https://github.com/rondagostino/librdkafka/blob/rtd_kip-255-oauthbearer/src/rdkafka.h#L1748).
Signed-off-by: Ron Dagostino rndgstn@gmail.com",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2189,2019-01-18T04:09:25Z,2019-04-11T12:24:42Z,2019-05-20T15:33:37Z,MERGED,True,2259,14,30,https://github.com/rondagostino,Add KIP-255 SASL/OAUTHBEARER support,105,[],https://github.com/edenhill/librdkafka/pull/2189,https://github.com/rondagostino,2,https://github.com/edenhill/librdkafka/pull/2189#issuecomment-463723608,"This is an initial commit for feedback.
There are no tests as of yet.
No C++, hard-coded unsecured token regardless of config, no SASL extensions.
Note that kafkacat only works if we add the statement rd_kafka_poll(conf.rk, 0); at lines 309, 584, 667, and 860 in kafkacat.c.  See the comment associated with rd_kafka_conf_set_oauthbearer_token_refresh_cb() for details (https://github.com/rondagostino/librdkafka/blob/rtd_kip-255-oauthbearer/src/rdkafka.h#L1748).
Signed-off-by: Ron Dagostino rndgstn@gmail.com","@edenhill All comments are addressed (except for the unit/integration tests, which I am working on now), and this is now rebased onto latest master.",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2189,2019-01-18T04:09:25Z,2019-04-11T12:24:42Z,2019-05-20T15:33:37Z,MERGED,True,2259,14,30,https://github.com/rondagostino,Add KIP-255 SASL/OAUTHBEARER support,105,[],https://github.com/edenhill/librdkafka/pull/2189,https://github.com/rondagostino,3,https://github.com/edenhill/librdkafka/pull/2189#issuecomment-466524842,"This is an initial commit for feedback.
There are no tests as of yet.
No C++, hard-coded unsecured token regardless of config, no SASL extensions.
Note that kafkacat only works if we add the statement rd_kafka_poll(conf.rk, 0); at lines 309, 584, 667, and 860 in kafkacat.c.  See the comment associated with rd_kafka_conf_set_oauthbearer_token_refresh_cb() for details (https://github.com/rondagostino/librdkafka/blob/rtd_kip-255-oauthbearer/src/rdkafka.h#L1748).
Signed-off-by: Ron Dagostino rndgstn@gmail.com",@edenhill I added event support in C++ for token refresh.  It isn't clear to me what else needs to be wrapped in C++ vs. simply being left exposed for direct usage: rd_kafka_oauthbearer_set_token()?  rd_kafka_oauthbearer_set_token_failure()?  Also not clear on what -- if any -- additional unit or integration tests need to be written for the C++ portion.,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2189,2019-01-18T04:09:25Z,2019-04-11T12:24:42Z,2019-05-20T15:33:37Z,MERGED,True,2259,14,30,https://github.com/rondagostino,Add KIP-255 SASL/OAUTHBEARER support,105,[],https://github.com/edenhill/librdkafka/pull/2189,https://github.com/edenhill,4,https://github.com/edenhill/librdkafka/pull/2189#issuecomment-466526093,"This is an initial commit for feedback.
There are no tests as of yet.
No C++, hard-coded unsecured token regardless of config, no SASL extensions.
Note that kafkacat only works if we add the statement rd_kafka_poll(conf.rk, 0); at lines 309, 584, 667, and 860 in kafkacat.c.  See the comment associated with rd_kafka_conf_set_oauthbearer_token_refresh_cb() for details (https://github.com/rondagostino/librdkafka/blob/rtd_kip-255-oauthbearer/src/rdkafka.h#L1748).
Signed-off-by: Ron Dagostino rndgstn@gmail.com","I added event support in C++ for token refresh.

Awesome!

... needs to be wrapped in C++ vs. ..

The C++ API must not expose any of the C API, so we would need set and fail token on the Handle base, etc.",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2189,2019-01-18T04:09:25Z,2019-04-11T12:24:42Z,2019-05-20T15:33:37Z,MERGED,True,2259,14,30,https://github.com/rondagostino,Add KIP-255 SASL/OAUTHBEARER support,105,[],https://github.com/edenhill/librdkafka/pull/2189,https://github.com/edenhill,5,https://github.com/edenhill/librdkafka/pull/2189#issuecomment-482092704,"This is an initial commit for feedback.
There are no tests as of yet.
No C++, hard-coded unsecured token regardless of config, no SASL extensions.
Note that kafkacat only works if we add the statement rd_kafka_poll(conf.rk, 0); at lines 309, 584, 667, and 860 in kafkacat.c.  See the comment associated with rd_kafka_conf_set_oauthbearer_token_refresh_cb() for details (https://github.com/rondagostino/librdkafka/blob/rtd_kip-255-oauthbearer/src/rdkafka.h#L1748).
Signed-off-by: Ron Dagostino rndgstn@gmail.com","Great work on this, Ron! Thank you!",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2189,2019-01-18T04:09:25Z,2019-04-11T12:24:42Z,2019-05-20T15:33:37Z,MERGED,True,2259,14,30,https://github.com/rondagostino,Add KIP-255 SASL/OAUTHBEARER support,105,[],https://github.com/edenhill/librdkafka/pull/2189,https://github.com/rondagostino,6,https://github.com/edenhill/librdkafka/pull/2189#issuecomment-482179572,"This is an initial commit for feedback.
There are no tests as of yet.
No C++, hard-coded unsecured token regardless of config, no SASL extensions.
Note that kafkacat only works if we add the statement rd_kafka_poll(conf.rk, 0); at lines 309, 584, 667, and 860 in kafkacat.c.  See the comment associated with rd_kafka_conf_set_oauthbearer_token_refresh_cb() for details (https://github.com/rondagostino/librdkafka/blob/rtd_kip-255-oauthbearer/src/rdkafka.h#L1748).
Signed-off-by: Ron Dagostino rndgstn@gmail.com",Likewise -- thanks for the reviews.,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2189,2019-01-18T04:09:25Z,2019-04-11T12:24:42Z,2019-05-20T15:33:37Z,MERGED,True,2259,14,30,https://github.com/rondagostino,Add KIP-255 SASL/OAUTHBEARER support,105,[],https://github.com/edenhill/librdkafka/pull/2189,https://github.com/noahdav,7,https://github.com/edenhill/librdkafka/pull/2189#issuecomment-493622031,"This is an initial commit for feedback.
There are no tests as of yet.
No C++, hard-coded unsecured token regardless of config, no SASL extensions.
Note that kafkacat only works if we add the statement rd_kafka_poll(conf.rk, 0); at lines 309, 584, 667, and 860 in kafkacat.c.  See the comment associated with rd_kafka_conf_set_oauthbearer_token_refresh_cb() for details (https://github.com/rondagostino/librdkafka/blob/rtd_kip-255-oauthbearer/src/rdkafka.h#L1748).
Signed-off-by: Ron Dagostino rndgstn@gmail.com","Are there any examples which show how to use this?  I was looking through the code and it seems that I should be able to set the following settings.
sasl.mechanisms=OAUTHBEARER
sasl.oauthbearer.config= ...
oauthbearer_token_refresh_cb=<refresh_cb>
However I was unable to get callback to get invoked after calling Poll().  So when I looked into this the rk_oauthbearer is still NULL which causes this to not get invoked.  The only tests I see were in rdkafka_sasl_oauthbearer.c which did not do this the same way since it is not a client.  Do you have some documentation or examples of what is required to use this feature.",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2189,2019-01-18T04:09:25Z,2019-04-11T12:24:42Z,2019-05-20T15:33:37Z,MERGED,True,2259,14,30,https://github.com/rondagostino,Add KIP-255 SASL/OAUTHBEARER support,105,[],https://github.com/edenhill/librdkafka/pull/2189,https://github.com/edenhill,8,https://github.com/edenhill/librdkafka/pull/2189#issuecomment-494038359,"This is an initial commit for feedback.
There are no tests as of yet.
No C++, hard-coded unsecured token regardless of config, no SASL extensions.
Note that kafkacat only works if we add the statement rd_kafka_poll(conf.rk, 0); at lines 309, 584, 667, and 860 in kafkacat.c.  See the comment associated with rd_kafka_conf_set_oauthbearer_token_refresh_cb() for details (https://github.com/rondagostino/librdkafka/blob/rtd_kip-255-oauthbearer/src/rdkafka.h#L1748).
Signed-off-by: Ron Dagostino rndgstn@gmail.com","@noahdav There are some changes to oauthbearer on the singlekinit branch (I know.. don't ask), it is a better point to try out, or wait for it to be merged to master (hopefully this week).
You will also need to set security.protocol=SASL_.. and sasl.mechanism=OAUTHBEARER",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2195,2019-01-22T06:23:53Z,2019-01-23T16:49:15Z,2019-03-12T12:14:41Z,MERGED,True,1,1,1,https://github.com/masahiro-kondo-quick,Fix issue #1704,1,[],https://github.com/edenhill/librdkafka/pull/2195,https://github.com/masahiro-kondo-quick,1,https://github.com/edenhill/librdkafka/pull/2195,"I challenged a small fix. Please confirm it.
Fix issue #1704 (RdKafka::Conf::get(propName, propVal) returns an empty value for topic configuration properties)","I challenged a small fix. Please confirm it.
Fix issue #1704 (RdKafka::Conf::get(propName, propVal) returns an empty value for topic configuration properties)",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2195,2019-01-22T06:23:53Z,2019-01-23T16:49:15Z,2019-03-12T12:14:41Z,MERGED,True,1,1,1,https://github.com/masahiro-kondo-quick,Fix issue #1704,1,[],https://github.com/edenhill/librdkafka/pull/2195,https://github.com/edenhill,2,https://github.com/edenhill/librdkafka/pull/2195#issuecomment-456877654,"I challenged a small fix. Please confirm it.
Fix issue #1704 (RdKafka::Conf::get(propName, propVal) returns an empty value for topic configuration properties)",Thank you!,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2197,2019-01-23T11:03:24Z,2019-01-23T12:01:08Z,2019-01-23T12:24:16Z,MERGED,True,28,3,1,https://github.com/daenney,packaging: Fix the Arch Linux PKGBUILD,1,[],https://github.com/edenhill/librdkafka/pull/2197,https://github.com/daenney,1,https://github.com/edenhill/librdkafka/pull/2197,"Build based on latest tag, as recommended by the packaging guidelines
pkgver() will automatically override the pkgver value (but a static
value must be defined either way)
Do a complete build enabling all supported compression algorithms
SSL/TLS for encryption and SASL for authentication. The dependencies
are small enough that there's not much of a point in having them as
optional dependencies, they're likely already installed on the target
system and it avoids a bunch of guessing at runtime to figure out
which capabilities are supported

With the current set of depends we end up with -lsasl2  -lssl  -llz4
-lm -lzstd  -lcrypto  -lz  -ldl -lpthread -lrt as build flags.","Build based on latest tag, as recommended by the packaging guidelines
pkgver() will automatically override the pkgver value (but a static
value must be defined either way)
Do a complete build enabling all supported compression algorithms
SSL/TLS for encryption and SASL for authentication. The dependencies
are small enough that there's not much of a point in having them as
optional dependencies, they're likely already installed on the target
system and it avoids a bunch of guessing at runtime to figure out
which capabilities are supported

With the current set of depends we end up with -lsasl2  -lssl  -llz4
-lm -lzstd  -lcrypto  -lz  -ldl -lpthread -lrt as build flags.",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2197,2019-01-23T11:03:24Z,2019-01-23T12:01:08Z,2019-01-23T12:24:16Z,MERGED,True,28,3,1,https://github.com/daenney,packaging: Fix the Arch Linux PKGBUILD,1,[],https://github.com/edenhill/librdkafka/pull/2197,https://github.com/edenhill,2,https://github.com/edenhill/librdkafka/pull/2197#issuecomment-456776267,"Build based on latest tag, as recommended by the packaging guidelines
pkgver() will automatically override the pkgver value (but a static
value must be defined either way)
Do a complete build enabling all supported compression algorithms
SSL/TLS for encryption and SASL for authentication. The dependencies
are small enough that there's not much of a point in having them as
optional dependencies, they're likely already installed on the target
system and it avoids a bunch of guessing at runtime to figure out
which capabilities are supported

With the current set of depends we end up with -lsasl2  -lssl  -llz4
-lm -lzstd  -lcrypto  -lz  -ldl -lpthread -lrt as build flags.","Great stuff, thanks! :)",True,{'THUMBS_UP': ['https://github.com/daenney']}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2200,2019-01-24T00:34:44Z,2019-03-11T09:11:35Z,2019-03-13T09:09:29Z,MERGED,True,83,6,5,https://github.com/Oxymoron79,CMake: Enable linking with an external LZ4 library,10,[],https://github.com/edenhill/librdkafka/pull/2200,https://github.com/Oxymoron79,1,https://github.com/edenhill/librdkafka/pull/2200,"As mentioned in my last PR (#2075 (comment)) this PR adds a CMake option ENABLE_LZ4_EXT to enable optional linking with an external LZ4 library.
It uses the CMake find module for LZ4 from https://github.com/Kitware/VTK/blob/master/CMake/FindLZ4.cmake to find the linker options for the LZ4 library.
The librdkafka pkg-config file and CMake package are updated to reflect the option.
Also, a package version file is generated for the librdkafka CMake package (it is optional, but good practice IMO).","As mentioned in my last PR (#2075 (comment)) this PR adds a CMake option ENABLE_LZ4_EXT to enable optional linking with an external LZ4 library.
It uses the CMake find module for LZ4 from https://github.com/Kitware/VTK/blob/master/CMake/FindLZ4.cmake to find the linker options for the LZ4 library.
The librdkafka pkg-config file and CMake package are updated to reflect the option.
Also, a package version file is generated for the librdkafka CMake package (it is optional, but good practice IMO).",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2200,2019-01-24T00:34:44Z,2019-03-11T09:11:35Z,2019-03-13T09:09:29Z,MERGED,True,83,6,5,https://github.com/Oxymoron79,CMake: Enable linking with an external LZ4 library,10,[],https://github.com/edenhill/librdkafka/pull/2200,https://github.com/Oxymoron79,2,https://github.com/edenhill/librdkafka/pull/2200#issuecomment-457023293,"As mentioned in my last PR (#2075 (comment)) this PR adds a CMake option ENABLE_LZ4_EXT to enable optional linking with an external LZ4 library.
It uses the CMake find module for LZ4 from https://github.com/Kitware/VTK/blob/master/CMake/FindLZ4.cmake to find the linker options for the LZ4 library.
The librdkafka pkg-config file and CMake package are updated to reflect the option.
Also, a package version file is generated for the librdkafka CMake package (it is optional, but good practice IMO).",The doozer/target/cmake-xenial-amd64 check failed because the build machine does not have a LZ4 library installed but the ENABLE_LZ4_EXT option defaults to ON (like the respective mklove configure file option).,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2200,2019-01-24T00:34:44Z,2019-03-11T09:11:35Z,2019-03-13T09:09:29Z,MERGED,True,83,6,5,https://github.com/Oxymoron79,CMake: Enable linking with an external LZ4 library,10,[],https://github.com/edenhill/librdkafka/pull/2200,https://github.com/edenhill,3,https://github.com/edenhill/librdkafka/pull/2200#issuecomment-457101470,"As mentioned in my last PR (#2075 (comment)) this PR adds a CMake option ENABLE_LZ4_EXT to enable optional linking with an external LZ4 library.
It uses the CMake find module for LZ4 from https://github.com/Kitware/VTK/blob/master/CMake/FindLZ4.cmake to find the linker options for the LZ4 library.
The librdkafka pkg-config file and CMake package are updated to reflect the option.
Also, a package version file is generated for the librdkafka CMake package (it is optional, but good practice IMO).","ENABLE_LZ4_EXT makes mklove check for lz4, not require it (which might be questionable, but that's another discussion). I think we should try to get the same behaviour in CMake, if possible. I.e., check and disable if not found.",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2200,2019-01-24T00:34:44Z,2019-03-11T09:11:35Z,2019-03-13T09:09:29Z,MERGED,True,83,6,5,https://github.com/Oxymoron79,CMake: Enable linking with an external LZ4 library,10,[],https://github.com/edenhill/librdkafka/pull/2200,https://github.com/edenhill,4,https://github.com/edenhill/librdkafka/pull/2200#issuecomment-457115639,"As mentioned in my last PR (#2075 (comment)) this PR adds a CMake option ENABLE_LZ4_EXT to enable optional linking with an external LZ4 library.
It uses the CMake find module for LZ4 from https://github.com/Kitware/VTK/blob/master/CMake/FindLZ4.cmake to find the linker options for the LZ4 library.
The librdkafka pkg-config file and CMake package are updated to reflect the option.
Also, a package version file is generated for the librdkafka CMake package (it is optional, but good practice IMO).",@ruslo @raulbocanegra Would you mind help reviewing this?,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2200,2019-01-24T00:34:44Z,2019-03-11T09:11:35Z,2019-03-13T09:09:29Z,MERGED,True,83,6,5,https://github.com/Oxymoron79,CMake: Enable linking with an external LZ4 library,10,[],https://github.com/edenhill/librdkafka/pull/2200,https://github.com/Oxymoron79,5,https://github.com/edenhill/librdkafka/pull/2200#issuecomment-457358461,"As mentioned in my last PR (#2075 (comment)) this PR adds a CMake option ENABLE_LZ4_EXT to enable optional linking with an external LZ4 library.
It uses the CMake find module for LZ4 from https://github.com/Kitware/VTK/blob/master/CMake/FindLZ4.cmake to find the linker options for the LZ4 library.
The librdkafka pkg-config file and CMake package are updated to reflect the option.
Also, a package version file is generated for the librdkafka CMake package (it is optional, but good practice IMO).","ENABLE_LZ4_EXT makes mklove check for lz4, not require it (which might be questionable, but that's another discussion). I think we should try to get the same behaviour in CMake, if possible. I.e., check and disable if not found.

I have updated the CMake files accordingly.",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2200,2019-01-24T00:34:44Z,2019-03-11T09:11:35Z,2019-03-13T09:09:29Z,MERGED,True,83,6,5,https://github.com/Oxymoron79,CMake: Enable linking with an external LZ4 library,10,[],https://github.com/edenhill/librdkafka/pull/2200,https://github.com/edenhill,6,https://github.com/edenhill/librdkafka/pull/2200#issuecomment-471459506,"As mentioned in my last PR (#2075 (comment)) this PR adds a CMake option ENABLE_LZ4_EXT to enable optional linking with an external LZ4 library.
It uses the CMake find module for LZ4 from https://github.com/Kitware/VTK/blob/master/CMake/FindLZ4.cmake to find the linker options for the LZ4 library.
The librdkafka pkg-config file and CMake package are updated to reflect the option.
Also, a package version file is generated for the librdkafka CMake package (it is optional, but good practice IMO).",Thank you!,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2203,2019-01-25T10:25:20Z,2019-03-12T07:47:20Z,2019-03-12T07:47:24Z,MERGED,True,10,2,1,https://github.com/SkyToGround,Minor improvements to documentation.,1,[],https://github.com/edenhill/librdkafka/pull/2203,https://github.com/SkyToGround,1,https://github.com/edenhill/librdkafka/pull/2203,I clarified the documentation slightly as we had a bug caused by delete being called on a TopicMetadataVector*.,I clarified the documentation slightly as we had a bug caused by delete being called on a TopicMetadataVector*.,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2203,2019-01-25T10:25:20Z,2019-03-12T07:47:20Z,2019-03-12T07:47:24Z,MERGED,True,10,2,1,https://github.com/SkyToGround,Minor improvements to documentation.,1,[],https://github.com/edenhill/librdkafka/pull/2203,https://github.com/edenhill,2,https://github.com/edenhill/librdkafka/pull/2203#issuecomment-471891805,I clarified the documentation slightly as we had a bug caused by delete being called on a TopicMetadataVector*.,Thank you!,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2206,2019-01-29T09:08:59Z,2021-03-25T08:32:24Z,2021-03-25T08:32:24Z,CLOSED,False,8,4,3,https://github.com/ignitenet-martynas,Pass member_id to rkas_get_metadata_cb(),2,[],https://github.com/edenhill/librdkafka/pull/2206,https://github.com/ignitenet-martynas,1,https://github.com/edenhill/librdkafka/pull/2206,"I am currently working on adding support for the sticky assignment strategy to librdkafka in my fork, and that implies being able to persist the current assignment of partitions to a consumer as userData. I'm using the rkas_on_assignment_cb() callback to store the assignment for a particular group member and then look it up and serialize and return it from rkas_get_metadata_cb().
However, to be able to look up the current assignment for a particular consumer I need to identify it and currently no argument is passed to rkas_get_metadata_cb() that can be used to identify the consumer that the metadata is being assembled for.
Therefore I added member_id as the last argument of rkas_get_metadata_cb() and changed rd_kafka_JoinGroupRequest() to pass it.
This should not be introducing any binary or source code incompatibilities because rkas_get_metadata_cb() is only used internally within the library.","I am currently working on adding support for the sticky assignment strategy to librdkafka in my fork, and that implies being able to persist the current assignment of partitions to a consumer as userData. I'm using the rkas_on_assignment_cb() callback to store the assignment for a particular group member and then look it up and serialize and return it from rkas_get_metadata_cb().
However, to be able to look up the current assignment for a particular consumer I need to identify it and currently no argument is passed to rkas_get_metadata_cb() that can be used to identify the consumer that the metadata is being assembled for.
Therefore I added member_id as the last argument of rkas_get_metadata_cb() and changed rd_kafka_JoinGroupRequest() to pass it.
This should not be introducing any binary or source code incompatibilities because rkas_get_metadata_cb() is only used internally within the library.",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2206,2019-01-29T09:08:59Z,2021-03-25T08:32:24Z,2021-03-25T08:32:24Z,CLOSED,False,8,4,3,https://github.com/ignitenet-martynas,Pass member_id to rkas_get_metadata_cb(),2,[],https://github.com/edenhill/librdkafka/pull/2206,https://github.com/edenhill,2,https://github.com/edenhill/librdkafka/pull/2206#issuecomment-477906573,"I am currently working on adding support for the sticky assignment strategy to librdkafka in my fork, and that implies being able to persist the current assignment of partitions to a consumer as userData. I'm using the rkas_on_assignment_cb() callback to store the assignment for a particular group member and then look it up and serialize and return it from rkas_get_metadata_cb().
However, to be able to look up the current assignment for a particular consumer I need to identify it and currently no argument is passed to rkas_get_metadata_cb() that can be used to identify the consumer that the metadata is being assembled for.
Therefore I added member_id as the last argument of rkas_get_metadata_cb() and changed rd_kafka_JoinGroupRequest() to pass it.
This should not be introducing any binary or source code incompatibilities because rkas_get_metadata_cb() is only used internally within the library.",Can you elaborate on how you are using the member id in your custom assignor?,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2206,2019-01-29T09:08:59Z,2021-03-25T08:32:24Z,2021-03-25T08:32:24Z,CLOSED,False,8,4,3,https://github.com/ignitenet-martynas,Pass member_id to rkas_get_metadata_cb(),2,[],https://github.com/edenhill/librdkafka/pull/2206,https://github.com/ignitenet-martynas,3,https://github.com/edenhill/librdkafka/pull/2206#issuecomment-478875952,"I am currently working on adding support for the sticky assignment strategy to librdkafka in my fork, and that implies being able to persist the current assignment of partitions to a consumer as userData. I'm using the rkas_on_assignment_cb() callback to store the assignment for a particular group member and then look it up and serialize and return it from rkas_get_metadata_cb().
However, to be able to look up the current assignment for a particular consumer I need to identify it and currently no argument is passed to rkas_get_metadata_cb() that can be used to identify the consumer that the metadata is being assembled for.
Therefore I added member_id as the last argument of rkas_get_metadata_cb() and changed rd_kafka_JoinGroupRequest() to pass it.
This should not be introducing any binary or source code incompatibilities because rkas_get_metadata_cb() is only used internally within the library.","I am generally trying to copy the Java implementation of sticky assignor at https://github.com/apache/kafka/blob/trunk/clients/src/main/java/org/apache/kafka/clients/consumer/StickyAssignor.java
Basically:

In rd_kafka_sticky_assignor_on_assignment_cb() I store a copy of a member's current assignment into a hash table keyed by member_id
In rd_kafka_sticky_assignor_get_metadata_cb() I look the stored copy up and then serialize it into the userData part of the metadata.

However, I do need the member_id passed into rd_kafka_sticky_assignor_get_metadata_cb() in order to do the lookup in step 2.",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2206,2019-01-29T09:08:59Z,2021-03-25T08:32:24Z,2021-03-25T08:32:24Z,CLOSED,False,8,4,3,https://github.com/ignitenet-martynas,Pass member_id to rkas_get_metadata_cb(),2,[],https://github.com/edenhill/librdkafka/pull/2206,https://github.com/edenhill,4,https://github.com/edenhill/librdkafka/pull/2206#issuecomment-478887919,"I am currently working on adding support for the sticky assignment strategy to librdkafka in my fork, and that implies being able to persist the current assignment of partitions to a consumer as userData. I'm using the rkas_on_assignment_cb() callback to store the assignment for a particular group member and then look it up and serialize and return it from rkas_get_metadata_cb().
However, to be able to look up the current assignment for a particular consumer I need to identify it and currently no argument is passed to rkas_get_metadata_cb() that can be used to identify the consumer that the metadata is being assembled for.
Therefore I added member_id as the last argument of rkas_get_metadata_cb() and changed rd_kafka_JoinGroupRequest() to pass it.
This should not be introducing any binary or source code incompatibilities because rkas_get_metadata_cb() is only used internally within the library.","Would you be up for implementing and contributing the Sticky Assignor in librdkafka?
Being fully compatible & identical to the Java counterpart.",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2206,2019-01-29T09:08:59Z,2021-03-25T08:32:24Z,2021-03-25T08:32:24Z,CLOSED,False,8,4,3,https://github.com/ignitenet-martynas,Pass member_id to rkas_get_metadata_cb(),2,[],https://github.com/edenhill/librdkafka/pull/2206,https://github.com/edenhill,5,https://github.com/edenhill/librdkafka/pull/2206#issuecomment-713490861,"I am currently working on adding support for the sticky assignment strategy to librdkafka in my fork, and that implies being able to persist the current assignment of partitions to a consumer as userData. I'm using the rkas_on_assignment_cb() callback to store the assignment for a particular group member and then look it up and serialize and return it from rkas_get_metadata_cb().
However, to be able to look up the current assignment for a particular consumer I need to identify it and currently no argument is passed to rkas_get_metadata_cb() that can be used to identify the consumer that the metadata is being assembled for.
Therefore I added member_id as the last argument of rkas_get_metadata_cb() and changed rd_kafka_JoinGroupRequest() to pass it.
This should not be introducing any binary or source code incompatibilities because rkas_get_metadata_cb() is only used internally within the library.","With the sticky assignor (et.al) now merged to master, would you like to revisit this?",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2209,2019-01-31T06:17:20Z,2019-01-31T06:21:26Z,2019-01-31T22:53:40Z,CLOSED,False,3,3,1,https://github.com/rogerclermont,Handle additional retvals from cnd_timedwait_abs(),1,[],https://github.com/edenhill/librdkafka/pull/2209,https://github.com/rogerclermont,1,https://github.com/edenhill/librdkafka/pull/2209,Handle additional retvals from cnd_timedwait_abs()  to avoid infinite loop.,Handle additional retvals from cnd_timedwait_abs()  to avoid infinite loop.,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2209,2019-01-31T06:17:20Z,2019-01-31T06:21:26Z,2019-01-31T22:53:40Z,CLOSED,False,3,3,1,https://github.com/rogerclermont,Handle additional retvals from cnd_timedwait_abs(),1,[],https://github.com/edenhill/librdkafka/pull/2209,https://github.com/rogerclermont,2,https://github.com/edenhill/librdkafka/pull/2209#issuecomment-459230250,Handle additional retvals from cnd_timedwait_abs()  to avoid infinite loop.,Apologies; I had intended to first create this on the fork.,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2211,2019-01-31T22:56:42Z,2019-02-06T13:28:33Z,2019-02-06T13:28:47Z,MERGED,True,3,3,1,https://github.com/rogerclermont,Handle additional retvals from cnd_timedwait_abs(),2,[],https://github.com/edenhill/librdkafka/pull/2211,https://github.com/rogerclermont,1,https://github.com/edenhill/librdkafka/pull/2211,Handle additional retvals from cnd_timedwait_abs() to avoid infinite loop.,Handle additional retvals from cnd_timedwait_abs() to avoid infinite loop.,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2211,2019-01-31T22:56:42Z,2019-02-06T13:28:33Z,2019-02-06T13:28:47Z,MERGED,True,3,3,1,https://github.com/rogerclermont,Handle additional retvals from cnd_timedwait_abs(),2,[],https://github.com/edenhill/librdkafka/pull/2211,https://github.com/edenhill,2,https://github.com/edenhill/librdkafka/pull/2211#issuecomment-461022813,Handle additional retvals from cnd_timedwait_abs() to avoid infinite loop.,Thank you!,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2216,2019-02-05T00:15:35Z,,2021-06-16T14:57:47Z,OPEN,False,1507,506,13,https://github.com/LavaSpider,Batch produce requests to multiple topics and partitions.,1,"['enhancement', 'producer', 'GREAT REPORT']",https://github.com/edenhill/librdkafka/pull/2216,https://github.com/LavaSpider,1,https://github.com/edenhill/librdkafka/pull/2216,"I know this is a large diff, the intent is to reduce the overall number of produce requests since it seems that there is a high overhead per request on the broker.
This code allows for producing to multiple topics and partitions within a single produce request. It does not currently address producing multiple message sets for the same toppar within the same request.
The batch producing has been thoroughly vetted, however it was originally written on top of rdkafka 0.11.4. I have rebased to pick up the sparse connection and idempotent producer changes, and re-run the latency measurements as shown below.
I added the config var ""produce.request.max.partitions"" to limit the number of toppars contained within one request. Setting this to 1 will mimic existing behavior.
I am attaching latency measurements from before and after the change producing 1,000,000 messages to a topic which has 200 partitions, and one which has 1,000 partitions. The message rate is 20,000 messages per second. The only change is switching out the code used to produce, all config values remain the same.
On the charts, the X axis is message number and the Y axis is time in ms.
For our test, we are running against a 4 node kafka cluster. The producer and consumer are on the same machine, so there should be no deviation of timestamps.
The charts are divided into seven sub-charts, which show the following:

Time between call to rd_kafka_produce until rdkafka message is created.
Time between 1 and message set being created.
Time between 2 and produce request being sent by producer in rd_kafka_send.
Time between 3 to fetch request received by consumer in rd_kafka_recv.
Time between 4 to call to rd_kafka_consume0.
Time between 5 and message handled by app code.
Total time between rd_kafka_produce to message consumed by consumer app.

Latency charts: 1 topic 200 partitions
BASELINE

BATCH

Latency charts: 1 topic 1000 partitions
BASELINE

BATCH","I know this is a large diff, the intent is to reduce the overall number of produce requests since it seems that there is a high overhead per request on the broker.
This code allows for producing to multiple topics and partitions within a single produce request. It does not currently address producing multiple message sets for the same toppar within the same request.
The batch producing has been thoroughly vetted, however it was originally written on top of rdkafka 0.11.4. I have rebased to pick up the sparse connection and idempotent producer changes, and re-run the latency measurements as shown below.
I added the config var ""produce.request.max.partitions"" to limit the number of toppars contained within one request. Setting this to 1 will mimic existing behavior.
I am attaching latency measurements from before and after the change producing 1,000,000 messages to a topic which has 200 partitions, and one which has 1,000 partitions. The message rate is 20,000 messages per second. The only change is switching out the code used to produce, all config values remain the same.
On the charts, the X axis is message number and the Y axis is time in ms.
For our test, we are running against a 4 node kafka cluster. The producer and consumer are on the same machine, so there should be no deviation of timestamps.
The charts are divided into seven sub-charts, which show the following:

Time between call to rd_kafka_produce until rdkafka message is created.
Time between 1 and message set being created.
Time between 2 and produce request being sent by producer in rd_kafka_send.
Time between 3 to fetch request received by consumer in rd_kafka_recv.
Time between 4 to call to rd_kafka_consume0.
Time between 5 and message handled by app code.
Total time between rd_kafka_produce to message consumed by consumer app.

Latency charts: 1 topic 200 partitions
BASELINE

BATCH

Latency charts: 1 topic 1000 partitions
BASELINE

BATCH",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2216,2019-02-05T00:15:35Z,,2021-06-16T14:57:47Z,OPEN,False,1507,506,13,https://github.com/LavaSpider,Batch produce requests to multiple topics and partitions.,1,"['enhancement', 'producer', 'GREAT REPORT']",https://github.com/edenhill/librdkafka/pull/2216,https://github.com/edenhill,2,https://github.com/edenhill/librdkafka/pull/2216#issuecomment-460550551,"I know this is a large diff, the intent is to reduce the overall number of produce requests since it seems that there is a high overhead per request on the broker.
This code allows for producing to multiple topics and partitions within a single produce request. It does not currently address producing multiple message sets for the same toppar within the same request.
The batch producing has been thoroughly vetted, however it was originally written on top of rdkafka 0.11.4. I have rebased to pick up the sparse connection and idempotent producer changes, and re-run the latency measurements as shown below.
I added the config var ""produce.request.max.partitions"" to limit the number of toppars contained within one request. Setting this to 1 will mimic existing behavior.
I am attaching latency measurements from before and after the change producing 1,000,000 messages to a topic which has 200 partitions, and one which has 1,000 partitions. The message rate is 20,000 messages per second. The only change is switching out the code used to produce, all config values remain the same.
On the charts, the X axis is message number and the Y axis is time in ms.
For our test, we are running against a 4 node kafka cluster. The producer and consumer are on the same machine, so there should be no deviation of timestamps.
The charts are divided into seven sub-charts, which show the following:

Time between call to rd_kafka_produce until rdkafka message is created.
Time between 1 and message set being created.
Time between 2 and produce request being sent by producer in rd_kafka_send.
Time between 3 to fetch request received by consumer in rd_kafka_recv.
Time between 4 to call to rd_kafka_consume0.
Time between 5 and message handled by app code.
Total time between rd_kafka_produce to message consumed by consumer app.

Latency charts: 1 topic 200 partitions
BASELINE

BATCH

Latency charts: 1 topic 1000 partitions
BASELINE

BATCH","Wow! This is great, the new latency distribution looks very compelling.
Let's revisit this as soon as v1.0.0 is out the door, I have some additional changes around batch handling that may come in handy for this PR as well.",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2216,2019-02-05T00:15:35Z,,2021-06-16T14:57:47Z,OPEN,False,1507,506,13,https://github.com/LavaSpider,Batch produce requests to multiple topics and partitions.,1,"['enhancement', 'producer', 'GREAT REPORT']",https://github.com/edenhill/librdkafka/pull/2216,https://github.com/write2jaydeep,3,https://github.com/edenhill/librdkafka/pull/2216#issuecomment-481593567,"I know this is a large diff, the intent is to reduce the overall number of produce requests since it seems that there is a high overhead per request on the broker.
This code allows for producing to multiple topics and partitions within a single produce request. It does not currently address producing multiple message sets for the same toppar within the same request.
The batch producing has been thoroughly vetted, however it was originally written on top of rdkafka 0.11.4. I have rebased to pick up the sparse connection and idempotent producer changes, and re-run the latency measurements as shown below.
I added the config var ""produce.request.max.partitions"" to limit the number of toppars contained within one request. Setting this to 1 will mimic existing behavior.
I am attaching latency measurements from before and after the change producing 1,000,000 messages to a topic which has 200 partitions, and one which has 1,000 partitions. The message rate is 20,000 messages per second. The only change is switching out the code used to produce, all config values remain the same.
On the charts, the X axis is message number and the Y axis is time in ms.
For our test, we are running against a 4 node kafka cluster. The producer and consumer are on the same machine, so there should be no deviation of timestamps.
The charts are divided into seven sub-charts, which show the following:

Time between call to rd_kafka_produce until rdkafka message is created.
Time between 1 and message set being created.
Time between 2 and produce request being sent by producer in rd_kafka_send.
Time between 3 to fetch request received by consumer in rd_kafka_recv.
Time between 4 to call to rd_kafka_consume0.
Time between 5 and message handled by app code.
Total time between rd_kafka_produce to message consumed by consumer app.

Latency charts: 1 topic 200 partitions
BASELINE

BATCH

Latency charts: 1 topic 1000 partitions
BASELINE

BATCH","Hi
is this PR merged with the master branch? how does it effective to use in production?",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2216,2019-02-05T00:15:35Z,,2021-06-16T14:57:47Z,OPEN,False,1507,506,13,https://github.com/LavaSpider,Batch produce requests to multiple topics and partitions.,1,"['enhancement', 'producer', 'GREAT REPORT']",https://github.com/edenhill/librdkafka/pull/2216,https://github.com/LavaSpider,4,https://github.com/edenhill/librdkafka/pull/2216#issuecomment-481832758,"I know this is a large diff, the intent is to reduce the overall number of produce requests since it seems that there is a high overhead per request on the broker.
This code allows for producing to multiple topics and partitions within a single produce request. It does not currently address producing multiple message sets for the same toppar within the same request.
The batch producing has been thoroughly vetted, however it was originally written on top of rdkafka 0.11.4. I have rebased to pick up the sparse connection and idempotent producer changes, and re-run the latency measurements as shown below.
I added the config var ""produce.request.max.partitions"" to limit the number of toppars contained within one request. Setting this to 1 will mimic existing behavior.
I am attaching latency measurements from before and after the change producing 1,000,000 messages to a topic which has 200 partitions, and one which has 1,000 partitions. The message rate is 20,000 messages per second. The only change is switching out the code used to produce, all config values remain the same.
On the charts, the X axis is message number and the Y axis is time in ms.
For our test, we are running against a 4 node kafka cluster. The producer and consumer are on the same machine, so there should be no deviation of timestamps.
The charts are divided into seven sub-charts, which show the following:

Time between call to rd_kafka_produce until rdkafka message is created.
Time between 1 and message set being created.
Time between 2 and produce request being sent by producer in rd_kafka_send.
Time between 3 to fetch request received by consumer in rd_kafka_recv.
Time between 4 to call to rd_kafka_consume0.
Time between 5 and message handled by app code.
Total time between rd_kafka_produce to message consumed by consumer app.

Latency charts: 1 topic 200 partitions
BASELINE

BATCH

Latency charts: 1 topic 1000 partitions
BASELINE

BATCH","No it has not yet been merged yet. It looks like I need to rebase it off of master.
This change mainly affects users who have a large number of topics/partitions they are producing to. In that case, this will significantly reduce the number of produce requests sent to a broker. There seems to be a per-request overhead on the kafka broker, so limiting the overall number of the requests improves individual message latency.",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2216,2019-02-05T00:15:35Z,,2021-06-16T14:57:47Z,OPEN,False,1507,506,13,https://github.com/LavaSpider,Batch produce requests to multiple topics and partitions.,1,"['enhancement', 'producer', 'GREAT REPORT']",https://github.com/edenhill/librdkafka/pull/2216,https://github.com/cjf2xn,5,https://github.com/edenhill/librdkafka/pull/2216#issuecomment-685927616,"I know this is a large diff, the intent is to reduce the overall number of produce requests since it seems that there is a high overhead per request on the broker.
This code allows for producing to multiple topics and partitions within a single produce request. It does not currently address producing multiple message sets for the same toppar within the same request.
The batch producing has been thoroughly vetted, however it was originally written on top of rdkafka 0.11.4. I have rebased to pick up the sparse connection and idempotent producer changes, and re-run the latency measurements as shown below.
I added the config var ""produce.request.max.partitions"" to limit the number of toppars contained within one request. Setting this to 1 will mimic existing behavior.
I am attaching latency measurements from before and after the change producing 1,000,000 messages to a topic which has 200 partitions, and one which has 1,000 partitions. The message rate is 20,000 messages per second. The only change is switching out the code used to produce, all config values remain the same.
On the charts, the X axis is message number and the Y axis is time in ms.
For our test, we are running against a 4 node kafka cluster. The producer and consumer are on the same machine, so there should be no deviation of timestamps.
The charts are divided into seven sub-charts, which show the following:

Time between call to rd_kafka_produce until rdkafka message is created.
Time between 1 and message set being created.
Time between 2 and produce request being sent by producer in rd_kafka_send.
Time between 3 to fetch request received by consumer in rd_kafka_recv.
Time between 4 to call to rd_kafka_consume0.
Time between 5 and message handled by app code.
Total time between rd_kafka_produce to message consumed by consumer app.

Latency charts: 1 topic 200 partitions
BASELINE

BATCH

Latency charts: 1 topic 1000 partitions
BASELINE

BATCH",Has there been any movement on this or a similar PR? My use case would benefit from this change.,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2216,2019-02-05T00:15:35Z,,2021-06-16T14:57:47Z,OPEN,False,1507,506,13,https://github.com/LavaSpider,Batch produce requests to multiple topics and partitions.,1,"['enhancement', 'producer', 'GREAT REPORT']",https://github.com/edenhill/librdkafka/pull/2216,https://github.com/LavaSpider,6,https://github.com/edenhill/librdkafka/pull/2216#issuecomment-686851235,"I know this is a large diff, the intent is to reduce the overall number of produce requests since it seems that there is a high overhead per request on the broker.
This code allows for producing to multiple topics and partitions within a single produce request. It does not currently address producing multiple message sets for the same toppar within the same request.
The batch producing has been thoroughly vetted, however it was originally written on top of rdkafka 0.11.4. I have rebased to pick up the sparse connection and idempotent producer changes, and re-run the latency measurements as shown below.
I added the config var ""produce.request.max.partitions"" to limit the number of toppars contained within one request. Setting this to 1 will mimic existing behavior.
I am attaching latency measurements from before and after the change producing 1,000,000 messages to a topic which has 200 partitions, and one which has 1,000 partitions. The message rate is 20,000 messages per second. The only change is switching out the code used to produce, all config values remain the same.
On the charts, the X axis is message number and the Y axis is time in ms.
For our test, we are running against a 4 node kafka cluster. The producer and consumer are on the same machine, so there should be no deviation of timestamps.
The charts are divided into seven sub-charts, which show the following:

Time between call to rd_kafka_produce until rdkafka message is created.
Time between 1 and message set being created.
Time between 2 and produce request being sent by producer in rd_kafka_send.
Time between 3 to fetch request received by consumer in rd_kafka_recv.
Time between 4 to call to rd_kafka_consume0.
Time between 5 and message handled by app code.
Total time between rd_kafka_produce to message consumed by consumer app.

Latency charts: 1 topic 200 partitions
BASELINE

BATCH

Latency charts: 1 topic 1000 partitions
BASELINE

BATCH","Sorry, this PR has been in limbo for a while. It was originally made
against an older version of rdkafka.

Since then support for idempotent produce requests was added. That addition
modified code in similar areas as this change and I had not had time to
rebase. The rebase is happening currently as we need the idempotent
producer behavior, so I should be able to update this PR soon.

On Wed, Sep 2, 2020 at 11:45 AM Chris Flood ***@***.***> wrote:
 Has there been any movement on this or a similar PR? My use case would
 benefit from this change.

 
 You are receiving this because you authored the thread.
 Reply to this email directly, view it on GitHub
 <#2216 (comment)>,
 or unsubscribe
 <https://github.com/notifications/unsubscribe-auth/AIKIFPJEFNBXK46KYTQPC2DSD2HFBANCNFSM4GUJGWJQ>
 .",True,{'THUMBS_UP': ['https://github.com/cjf2xn']}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2216,2019-02-05T00:15:35Z,,2021-06-16T14:57:47Z,OPEN,False,1507,506,13,https://github.com/LavaSpider,Batch produce requests to multiple topics and partitions.,1,"['enhancement', 'producer', 'GREAT REPORT']",https://github.com/edenhill/librdkafka/pull/2216,https://github.com/kphelps,7,https://github.com/edenhill/librdkafka/pull/2216#issuecomment-862452884,"I know this is a large diff, the intent is to reduce the overall number of produce requests since it seems that there is a high overhead per request on the broker.
This code allows for producing to multiple topics and partitions within a single produce request. It does not currently address producing multiple message sets for the same toppar within the same request.
The batch producing has been thoroughly vetted, however it was originally written on top of rdkafka 0.11.4. I have rebased to pick up the sparse connection and idempotent producer changes, and re-run the latency measurements as shown below.
I added the config var ""produce.request.max.partitions"" to limit the number of toppars contained within one request. Setting this to 1 will mimic existing behavior.
I am attaching latency measurements from before and after the change producing 1,000,000 messages to a topic which has 200 partitions, and one which has 1,000 partitions. The message rate is 20,000 messages per second. The only change is switching out the code used to produce, all config values remain the same.
On the charts, the X axis is message number and the Y axis is time in ms.
For our test, we are running against a 4 node kafka cluster. The producer and consumer are on the same machine, so there should be no deviation of timestamps.
The charts are divided into seven sub-charts, which show the following:

Time between call to rd_kafka_produce until rdkafka message is created.
Time between 1 and message set being created.
Time between 2 and produce request being sent by producer in rd_kafka_send.
Time between 3 to fetch request received by consumer in rd_kafka_recv.
Time between 4 to call to rd_kafka_consume0.
Time between 5 and message handled by app code.
Total time between rd_kafka_produce to message consumed by consumer app.

Latency charts: 1 topic 200 partitions
BASELINE

BATCH

Latency charts: 1 topic 1000 partitions
BASELINE

BATCH","Is there any plan to get this change across the finish line? If not, I could try to take a shot at rebasing this in the near future. I've been using rdkafka heavily on the consumer end, but this issue has been a blocker for migrating some high partition count use cases to use rdkafka's producer.",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2218,2019-02-06T16:44:54Z,2019-02-06T20:49:32Z,2019-02-06T20:49:35Z,MERGED,True,110,8,3,https://github.com/edenhill,Fix recent regression: messages in UA partition does not time out.,2,[],https://github.com/edenhill/librdkafka/pull/2218,https://github.com/edenhill,1,https://github.com/edenhill/librdkafka/pull/2218,,,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2221,2019-02-12T16:17:17Z,2019-02-21T15:32:07Z,2019-03-14T23:13:08Z,MERGED,True,25,0,4,https://github.com/Oxymoron79,CMake: Add check for C11 thread support.,1,[],https://github.com/edenhill/librdkafka/pull/2221,https://github.com/Oxymoron79,1,https://github.com/edenhill/librdkafka/pull/2221,"This PR adds a check in the CMake files to detect C11 thread support on the build host to enable the conditional compilation of tinycthread.
Glibc version 2.28 introduced support for C11 threads, which leads to similar problems as reported in #1998 on system that already use that Glibc version.","This PR adds a check in the CMake files to detect C11 thread support on the build host to enable the conditional compilation of tinycthread.
Glibc version 2.28 introduced support for C11 threads, which leads to similar problems as reported in #1998 on system that already use that Glibc version.",True,{'THUMBS_UP': ['https://github.com/lukasdurfina']}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2221,2019-02-12T16:17:17Z,2019-02-21T15:32:07Z,2019-03-14T23:13:08Z,MERGED,True,25,0,4,https://github.com/Oxymoron79,CMake: Add check for C11 thread support.,1,[],https://github.com/edenhill/librdkafka/pull/2221,https://github.com/edenhill,2,https://github.com/edenhill/librdkafka/pull/2221#issuecomment-465475765,"This PR adds a check in the CMake files to detect C11 thread support on the build host to enable the conditional compilation of tinycthread.
Glibc version 2.28 introduced support for C11 threads, which leads to similar problems as reported in #1998 on system that already use that Glibc version.","LGTM.
Is this good to go, @Oxymoron79 ?",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2221,2019-02-12T16:17:17Z,2019-02-21T15:32:07Z,2019-03-14T23:13:08Z,MERGED,True,25,0,4,https://github.com/Oxymoron79,CMake: Add check for C11 thread support.,1,[],https://github.com/edenhill/librdkafka/pull/2221,https://github.com/Oxymoron79,3,https://github.com/edenhill/librdkafka/pull/2221#issuecomment-465578368,"This PR adds a check in the CMake files to detect C11 thread support on the build host to enable the conditional compilation of tinycthread.
Glibc version 2.28 introduced support for C11 threads, which leads to similar problems as reported in #1998 on system that already use that Glibc version.","@edenhill Yes, it's good to go. The fix works on the systems that I tested.",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2221,2019-02-12T16:17:17Z,2019-02-21T15:32:07Z,2019-03-14T23:13:08Z,MERGED,True,25,0,4,https://github.com/Oxymoron79,CMake: Add check for C11 thread support.,1,[],https://github.com/edenhill/librdkafka/pull/2221,https://github.com/edenhill,4,https://github.com/edenhill/librdkafka/pull/2221#issuecomment-466043884,"This PR adds a check in the CMake files to detect C11 thread support on the build host to enable the conditional compilation of tinycthread.
Glibc version 2.28 introduced support for C11 threads, which leads to similar problems as reported in #1998 on system that already use that Glibc version.",Thank you!,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2221,2019-02-12T16:17:17Z,2019-02-21T15:32:07Z,2019-03-14T23:13:08Z,MERGED,True,25,0,4,https://github.com/Oxymoron79,CMake: Add check for C11 thread support.,1,[],https://github.com/edenhill/librdkafka/pull/2221,https://github.com/edenhill,5,https://github.com/edenhill/librdkafka/pull/2221#issuecomment-472756078,"This PR adds a check in the CMake files to detect C11 thread support on the build host to enable the conditional compilation of tinycthread.
Glibc version 2.28 introduced support for C11 threads, which leads to similar problems as reported in #1998 on system that already use that Glibc version.","@Oxymoron79 This is unrelated to this issue, but the CMake builds are breaking because of missing BUILT_WITH define:
this was recently added to mklove as a string containing the WITH_x build options used for the build (without the WITH_ prefix).
Do you have any idea how to do this with CMake and would you like to take a stab at it?
If it turns out to be complicated we could just set it to ""CMAKE"", it is only used in an initial debug message to show the build options.
https://doozer.io/edenhill/librdkafka/buildlog/7044",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2221,2019-02-12T16:17:17Z,2019-02-21T15:32:07Z,2019-03-14T23:13:08Z,MERGED,True,25,0,4,https://github.com/Oxymoron79,CMake: Add check for C11 thread support.,1,[],https://github.com/edenhill/librdkafka/pull/2221,https://github.com/Oxymoron79,6,https://github.com/edenhill/librdkafka/pull/2221#issuecomment-473100521,"This PR adds a check in the CMake files to detect C11 thread support on the build host to enable the conditional compilation of tinycthread.
Glibc version 2.28 introduced support for C11 threads, which leads to similar problems as reported in #1998 on system that already use that Glibc version.","@edenhill Sure, I will try to find a solution in the next days. From a first quick look at the buildlog, I think it won't be hard to fix.",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2230,2019-02-19T13:47:58Z,2019-02-20T08:31:20Z,2019-02-20T08:31:23Z,MERGED,True,47,12,5,https://github.com/edenhill,"Hide sparse connections, document debug contexts, misc warnings",3,[],https://github.com/edenhill/librdkafka/pull/2230,https://github.com/edenhill,1,https://github.com/edenhill/librdkafka/pull/2230,"We've had zero issues with sparse connections during the last 5 months, so we should remove the dual stack mode, but that's too big of a change this close to release.
To avoid breaking changes post v1.0.0 we hide the configuration property so it can be removed later.","We've had zero issues with sparse connections during the last 5 months, so we should remove the dual stack mode, but that's too big of a change this close to release.
To avoid breaking changes post v1.0.0 we hide the configuration property so it can be removed later.",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2232,2019-02-19T20:46:58Z,2019-02-20T08:29:15Z,2019-02-20T08:29:18Z,MERGED,True,4,3,1,https://github.com/edenhill,Make sure POLLOUT is always set during connection establishment (#2231),1,[],https://github.com/edenhill/librdkafka/pull/2232,https://github.com/edenhill,1,https://github.com/edenhill/librdkafka/pull/2232,,,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2237,2019-02-26T08:25:57Z,2019-03-04T11:01:30Z,2019-03-04T11:01:33Z,MERGED,True,1,1,1,https://github.com/xxfelixxx,Update rdkafkacpp.h to fix -Wpedantic warning,1,[],https://github.com/edenhill/librdkafka/pull/2237,https://github.com/xxfelixxx,1,https://github.com/edenhill/librdkafka/pull/2237,"Fix for:
In file included from test.cpp:54:0:
/opt/librdkafka/include/librdkafka/rdkafkacpp.h:79:2: warning: extra ; [-Wpedantic]
};
^","Fix for:
In file included from test.cpp:54:0:
/opt/librdkafka/include/librdkafka/rdkafkacpp.h:79:2: warning: extra ; [-Wpedantic]
};
^",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2237,2019-02-26T08:25:57Z,2019-03-04T11:01:30Z,2019-03-04T11:01:33Z,MERGED,True,1,1,1,https://github.com/xxfelixxx,Update rdkafkacpp.h to fix -Wpedantic warning,1,[],https://github.com/edenhill/librdkafka/pull/2237,https://github.com/cfinkenstadt,2,https://github.com/edenhill/librdkafka/pull/2237#issuecomment-467683607,"Fix for:
In file included from test.cpp:54:0:
/opt/librdkafka/include/librdkafka/rdkafkacpp.h:79:2: warning: extra ; [-Wpedantic]
};
^",,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2237,2019-02-26T08:25:57Z,2019-03-04T11:01:30Z,2019-03-04T11:01:33Z,MERGED,True,1,1,1,https://github.com/xxfelixxx,Update rdkafkacpp.h to fix -Wpedantic warning,1,[],https://github.com/edenhill/librdkafka/pull/2237,https://github.com/edenhill,3,https://github.com/edenhill/librdkafka/pull/2237#issuecomment-469211765,"Fix for:
In file included from test.cpp:54:0:
/opt/librdkafka/include/librdkafka/rdkafkacpp.h:79:2: warning: extra ; [-Wpedantic]
};
^",Thank you!,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2239,2019-02-27T17:15:07Z,2019-03-07T08:55:52Z,2019-03-07T08:55:56Z,MERGED,True,18,10,5,https://github.com/adeason,Always give a non-NULL result pointer to thrd_join,1,[],https://github.com/edenhill/librdkafka/pull/2239,https://github.com/adeason,1,https://github.com/edenhill/librdkafka/pull/2239,"With this commit, I can successfully run 'gmake -C tests run_local' on a solaris 11.4 machine; I haven't tested anything further than that.","With this commit, I can successfully run 'gmake -C tests run_local' on a solaris 11.4 machine; I haven't tested anything further than that.",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2239,2019-02-27T17:15:07Z,2019-03-07T08:55:52Z,2019-03-07T08:55:56Z,MERGED,True,18,10,5,https://github.com/adeason,Always give a non-NULL result pointer to thrd_join,1,[],https://github.com/edenhill/librdkafka/pull/2239,https://github.com/edenhill,2,https://github.com/edenhill/librdkafka/pull/2239#issuecomment-469210500,"With this commit, I can successfully run 'gmake -C tests run_local' on a solaris 11.4 machine; I haven't tested anything further than that.",Is this with C11 threads from libc? (grep WITH_C11THREADS Makefile.config == y) or the builtin one?,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2239,2019-02-27T17:15:07Z,2019-03-07T08:55:52Z,2019-03-07T08:55:56Z,MERGED,True,18,10,5,https://github.com/adeason,Always give a non-NULL result pointer to thrd_join,1,[],https://github.com/edenhill/librdkafka/pull/2239,https://github.com/adeason,3,https://github.com/edenhill/librdkafka/pull/2239#issuecomment-469339734,"With this commit, I can successfully run 'gmake -C tests run_local' on a solaris 11.4 machine; I haven't tested anything further than that.","On Mon, 04 Mar 2019 02:57:42 -0800 Magnus Edenhill ***@***.***> wrote:
 Is this with C11 threads from libc? (`grep WITH_C11THREADS Makefile.config` == y) or the builtin one?
This is the C11 threads from the system libc:

$ grep WITH_C11THREADS Makefile.config
WITH_C11THREADS=        y

-- 
Andrew Deason
adeason@dson.org",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2239,2019-02-27T17:15:07Z,2019-03-07T08:55:52Z,2019-03-07T08:55:56Z,MERGED,True,18,10,5,https://github.com/adeason,Always give a non-NULL result pointer to thrd_join,1,[],https://github.com/edenhill/librdkafka/pull/2239,https://github.com/edenhill,4,https://github.com/edenhill/librdkafka/pull/2239#issuecomment-470440447,"With this commit, I can successfully run 'gmake -C tests run_local' on a solaris 11.4 machine; I haven't tested anything further than that.",Awesome!,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2240,2019-02-27T17:17:12Z,2019-03-07T08:54:46Z,2019-03-07T08:54:50Z,MERGED,True,6,6,3,https://github.com/adeason,Use __sun for detecting Solaris,1,[],https://github.com/edenhill/librdkafka/pull/2240,https://github.com/adeason,1,https://github.com/edenhill/librdkafka/pull/2240,"Without this, I need to run ./configure with CPPFLAGS='-Dsun' for compilation to succeed on Solaris.","Without this, I need to run ./configure with CPPFLAGS='-Dsun' for compilation to succeed on Solaris.",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2240,2019-02-27T17:17:12Z,2019-03-07T08:54:46Z,2019-03-07T08:54:50Z,MERGED,True,6,6,3,https://github.com/adeason,Use __sun for detecting Solaris,1,[],https://github.com/edenhill/librdkafka/pull/2240,https://github.com/adeason,2,https://github.com/edenhill/librdkafka/pull/2240#issuecomment-468986735,"Without this, I need to run ./configure with CPPFLAGS='-Dsun' for compilation to succeed on Solaris.","The appveyor failure looks unrelated as far as I can tell; it's failing on a prereq download for building on windows, I think?",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2240,2019-02-27T17:17:12Z,2019-03-07T08:54:46Z,2019-03-07T08:54:50Z,MERGED,True,6,6,3,https://github.com/adeason,Use __sun for detecting Solaris,1,[],https://github.com/edenhill/librdkafka/pull/2240,https://github.com/edenhill,3,https://github.com/edenhill/librdkafka/pull/2240#issuecomment-469194019,"Without this, I need to run ./configure with CPPFLAGS='-Dsun' for compilation to succeed on Solaris.","I see other projects using both, #if defined(sun) || defined(__sun), is there an authoritative source which to use?",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2240,2019-02-27T17:17:12Z,2019-03-07T08:54:46Z,2019-03-07T08:54:50Z,MERGED,True,6,6,3,https://github.com/adeason,Use __sun for detecting Solaris,1,[],https://github.com/edenhill/librdkafka/pull/2240,https://github.com/adeason,4,https://github.com/edenhill/librdkafka/pull/2240#issuecomment-469349534,"Without this, I need to run ./configure with CPPFLAGS='-Dsun' for compilation to succeed on Solaris.","On Mon, 04 Mar 2019 02:07:29 -0800 Magnus Edenhill ***@***.***> wrote:
 I see other projects using both, `#if defined(sun) || defined(__sun)`,
 is there an authoritative source which to use?
I don't think so. Some hints, though:

In the man page for the Solaris native compiler (""Oracle Solaris
Studio""):
<https://docs.oracle.com/cd/E37069_01/html/E54439/uc-cc-1.html>, search
for ""-Dname[=def]"" to find the list of predefined symbols. 'sun' and
'__sun' are both in there, but near the end it mentions:
 If +p is used, sun, unix, sparc and i386 are not defined.
The +p option says it turns off ""non-standard preprocessor asserts"". So
from that, 'sun' is considered non-standard.

But I was using gcc (which I assumed was required for librdkafka). I
don't see any mention of these symbols specifically for gcc, but this
page has something relevant to say:
<https://gcc.gnu.org/onlinedocs/cpp/System-specific-Predefined-Macros.html>
 The C standard requires that all system-specific macros be part of the
 reserved namespace. All names which begin with two underscores, or an
 underscore and a capital letter, are reserved for the compiler and
 library to use as they wish. However, historically system-specific
 macros have had names with no special prefix; for instance, it is
 common to find unix defined on Unix systems. For all such macros, GCC
 provides a parallel macro with two underscores added at the beginning
 and the end. If unix is defined, __unix__ will be defined too. There
 will never be more than two underscores; the parallel of _mips is
 __mips__.
[...]
 We are slowly phasing out all predefined macros which are outside the
 reserved namespace. You should never use them in new programs, and we
 encourage you to correct older code to use the parallel macros
 whenever you find it.
So that page indicates 'sun' is nonstandard since it doesn't begin with
any underscores, and you should avoid nonstandard platform macros. But
it also says:
 We dont recommend you use the system-specific macros that are in the
 reserved namespace, either. It is better in the long run to check
 specifically for features you need, using a tool such as autoconf.
My guess is that 'sun' is the older symbol, and at some point Sun
switched to using '__sun' to try to be more standard and avoid naming
conflicts. But ""older"" here is quite old, like pre-Solaris SunOS era,
which would have already been obsolete in the 90s. So checking for 'sun'
feels like one of those ridiculous checks for compatibility with
30-year-old systems that are unlikely to ever encounter a modern release
of librdkafka; but I'm not sure, since it's hard to find detailed info.
And maybe 'sun' will go away at some point; right now I only see it
output by the Solaris compiler, and not by gcc.

I agree checking for both '__sun' and 'sun' is common, though. If you
prefer to check for both, I don't mind resubmitting doing that instead.

-- 
Andrew Deason
adeason@dson.org",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2240,2019-02-27T17:17:12Z,2019-03-07T08:54:46Z,2019-03-07T08:54:50Z,MERGED,True,6,6,3,https://github.com/adeason,Use __sun for detecting Solaris,1,[],https://github.com/edenhill/librdkafka/pull/2240,https://github.com/edenhill,5,https://github.com/edenhill/librdkafka/pull/2240#issuecomment-469415013,"Without this, I need to run ./configure with CPPFLAGS='-Dsun' for compilation to succeed on Solaris.","Fantastic research, thank you!
You have me convinced that we should be fine with just __sun.",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2240,2019-02-27T17:17:12Z,2019-03-07T08:54:46Z,2019-03-07T08:54:50Z,MERGED,True,6,6,3,https://github.com/adeason,Use __sun for detecting Solaris,1,[],https://github.com/edenhill/librdkafka/pull/2240,https://github.com/edenhill,6,https://github.com/edenhill/librdkafka/pull/2240#issuecomment-470440107,"Without this, I need to run ./configure with CPPFLAGS='-Dsun' for compilation to succeed on Solaris.",Thank you for this!,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2243,2019-03-06T22:27:47Z,2019-03-12T07:46:45Z,2019-03-12T07:46:45Z,MERGED,True,2,2,1,https://github.com/JonECG,Fix tinycthread const_seg/data_seg pragma mismatch,1,[],https://github.com/edenhill/librdkafka/pull/2243,https://github.com/JonECG,1,https://github.com/edenhill/librdkafka/pull/2243,Fixes a pragma leak in tincythread on windows that could poison a dll,Fixes a pragma leak in tincythread on windows that could poison a dll,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2244,2019-03-07T20:05:03Z,2019-03-12T07:50:48Z,2019-03-12T07:50:52Z,MERGED,True,1028,233,35,https://github.com/edenhill,Allow ./configure to auto-bootstrap dependencies,10,[],https://github.com/edenhill/librdkafka/pull/2244,https://github.com/edenhill,1,https://github.com/edenhill/librdkafka/pull/2244,".. and a bunch of build/packaging fixes to produce reliable builds, optionally statically linked.
Daughter projects should be adjusted to use ./configure --install-deps --source-deps-only .. rather than any apt-get install libssl-dev.. in their bootstrap scripts, since the latter is error prone.",".. and a bunch of build/packaging fixes to produce reliable builds, optionally statically linked.
Daughter projects should be adjusted to use ./configure --install-deps --source-deps-only .. rather than any apt-get install libssl-dev.. in their bootstrap scripts, since the latter is error prone.",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2248,2019-03-12T12:44:42Z,2019-03-19T14:21:55Z,2019-03-19T14:47:58Z,CLOSED,False,1,1,1,https://github.com/AkshayDubey29,Character Length for Broker,1,[],https://github.com/edenhill/librdkafka/pull/2248,https://github.com/AkshayDubey29,1,https://github.com/edenhill/librdkafka/pull/2248,Character Length for Broker to support more than 128 Characters lengths,Character Length for Broker to support more than 128 Characters lengths,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2250,2019-03-13T05:15:53Z,2019-03-14T08:08:17Z,2019-03-14T08:08:17Z,CLOSED,False,1,1,1,https://github.com/AkshayDubey29,Update rdkafka_proto.h,1,[],https://github.com/edenhill/librdkafka/pull/2250,https://github.com/AkshayDubey29,1,https://github.com/edenhill/librdkafka/pull/2250,,,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2250,2019-03-13T05:15:53Z,2019-03-14T08:08:17Z,2019-03-14T08:08:17Z,CLOSED,False,1,1,1,https://github.com/AkshayDubey29,Update rdkafka_proto.h,1,[],https://github.com/edenhill/librdkafka/pull/2250,https://github.com/AkshayDubey29,2,https://github.com/edenhill/librdkafka/pull/2250#issuecomment-472283078,,Kafka Broker name more than 128 Characters needs to be handled in FluentBit,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2250,2019-03-13T05:15:53Z,2019-03-14T08:08:17Z,2019-03-14T08:08:17Z,CLOSED,False,1,1,1,https://github.com/AkshayDubey29,Update rdkafka_proto.h,1,[],https://github.com/edenhill/librdkafka/pull/2250,https://github.com/cosmo0920,3,https://github.com/edenhill/librdkafka/pull/2250#issuecomment-472357171,,"@AkshayDubey29 Build is broken. :(
https://travis-ci.org/edenhill/librdkafka/jobs/505587672#L590",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2252,2019-03-13T11:33:30Z,2019-03-14T08:46:12Z,2019-03-14T08:46:16Z,MERGED,True,456,2,7,https://github.com/edenhill,Idempotent producer example ,3,[],https://github.com/edenhill/librdkafka/pull/2252,https://github.com/edenhill,1,https://github.com/edenhill/librdkafka/pull/2252,And sneaked in a release checksum script.,And sneaked in a release checksum script.,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2260,2019-03-19T11:14:56Z,2019-03-19T21:50:51Z,2020-05-08T08:58:39Z,MERGED,True,141,2,7,https://github.com/edenhill,Selectively include logical brokers in ERR__ALL_BROKERS_DOWN calculation (#2259),2,[],https://github.com/edenhill/librdkafka/pull/2260,https://github.com/edenhill,1,https://github.com/edenhill/librdkafka/pull/2260,..depending on if they have an address set or not.,..depending on if they have an address set or not.,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2260,2019-03-19T11:14:56Z,2019-03-19T21:50:51Z,2020-05-08T08:58:39Z,MERGED,True,141,2,7,https://github.com/edenhill,Selectively include logical brokers in ERR__ALL_BROKERS_DOWN calculation (#2259),2,[],https://github.com/edenhill/librdkafka/pull/2260,https://github.com/edenhill,2,https://github.com/edenhill/librdkafka/pull/2260#issuecomment-474391815,..depending on if they have an address set or not.,You're absolutely right. Added a test.,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2262,2019-03-21T22:02:32Z,2019-03-25T22:37:23Z,2019-03-26T14:28:02Z,MERGED,True,116,26,5,https://github.com/Oxymoron79,CMake: Generate BUILT_WITH define,4,[],https://github.com/edenhill/librdkafka/pull/2262,https://github.com/Oxymoron79,1,https://github.com/edenhill/librdkafka/pull/2262,"As mentioned in #2221 I have been working on generating the BUILT_WITH define directive in CMake.
I was able to replicate the generation of the BUILT_WITH list for most entries except GNULD and LDS, since I haven't found a proper way to do the necessary checks in CMake.
Also, the entries for the C and C++ compilers (GCC, GXX) are created from the CMake compiler ID strings for GNU, Clang and MSVC only and are not exported to config.h.
Are these restrictions acceptable for now?
Along the way, I also added the missing checks to enable the hdr histogram compilation and for the support of the CRC32C instruction.","As mentioned in #2221 I have been working on generating the BUILT_WITH define directive in CMake.
I was able to replicate the generation of the BUILT_WITH list for most entries except GNULD and LDS, since I haven't found a proper way to do the necessary checks in CMake.
Also, the entries for the C and C++ compilers (GCC, GXX) are created from the CMake compiler ID strings for GNU, Clang and MSVC only and are not exported to config.h.
Are these restrictions acceptable for now?
Along the way, I also added the missing checks to enable the hdr histogram compilation and for the support of the CRC32C instruction.",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2262,2019-03-21T22:02:32Z,2019-03-25T22:37:23Z,2019-03-26T14:28:02Z,MERGED,True,116,26,5,https://github.com/Oxymoron79,CMake: Generate BUILT_WITH define,4,[],https://github.com/edenhill/librdkafka/pull/2262,https://github.com/Oxymoron79,2,https://github.com/edenhill/librdkafka/pull/2262#issuecomment-476009943,"As mentioned in #2221 I have been working on generating the BUILT_WITH define directive in CMake.
I was able to replicate the generation of the BUILT_WITH list for most entries except GNULD and LDS, since I haven't found a proper way to do the necessary checks in CMake.
Also, the entries for the C and C++ compilers (GCC, GXX) are created from the CMake compiler ID strings for GNU, Clang and MSVC only and are not exported to config.h.
Are these restrictions acceptable for now?
Along the way, I also added the missing checks to enable the hdr histogram compilation and for the support of the CRC32C instruction.","Can you rebase on latest master (has a conflicting BUILT_WITH commit)?

Rebased and squashed review commits on master.",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2262,2019-03-21T22:02:32Z,2019-03-25T22:37:23Z,2019-03-26T14:28:02Z,MERGED,True,116,26,5,https://github.com/Oxymoron79,CMake: Generate BUILT_WITH define,4,[],https://github.com/edenhill/librdkafka/pull/2262,https://github.com/edenhill,3,https://github.com/edenhill/librdkafka/pull/2262#issuecomment-476404354,"As mentioned in #2221 I have been working on generating the BUILT_WITH define directive in CMake.
I was able to replicate the generation of the BUILT_WITH list for most entries except GNULD and LDS, since I haven't found a proper way to do the necessary checks in CMake.
Also, the entries for the C and C++ compilers (GCC, GXX) are created from the CMake compiler ID strings for GNU, Clang and MSVC only and are not exported to config.h.
Are these restrictions acceptable for now?
Along the way, I also added the missing checks to enable the hdr histogram compilation and for the support of the CRC32C instruction.",Thank you!,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2268,2019-03-28T10:25:50Z,2019-04-11T07:11:23Z,2019-04-11T07:11:23Z,CLOSED,False,1,5,1,https://github.com/kenneth-jia,rd_kafka_broker_handle_ApiVersion() should not call rd_kafka_broker_f,1,[],https://github.com/edenhill/librdkafka/pull/2268,https://github.com/kenneth-jia,1,https://github.com/edenhill/librdkafka/pull/2268,"Sometimes librdkafka would crash with
*** rdkafka_buf.c:201:rd_kafka_bufq_deq: assert: rd_atomic32_get(&rkbufq->rkbq_cnt) > 0 ***
(gdb) bt
#0  0x00007f09f26e2495 in raise () from /lib64/libc.so.6
#1  0x00007f09f26e3c75 in abort () from /lib64/libc.so.6
#2  0x00007f09f3410353 in rd_kafka_crash (file=file@entry=0x7f09f34caf7f ""rdkafka_buf.c"", line=line@entry=197, function=function@entry=0x7f09f34cb130 <__FUNCTION__.22201> ""rd_kafka_bufq_deq"", rk=rk@entry=0x0,
    reason=reason@entry=0x7f09f34caff8 ""assert: rd_atomic32_get(&rkbufq->rkbq_cnt) > 0"") at rdkafka.c:3432
#3  0x00007f09f343b975 in rd_kafka_bufq_deq (rkbufq=rkbufq@entry=0xb32180, rkbuf=rkbuf@entry=0xb36050) at rdkafka_buf.c:197
#4  0x00007f09f342300b in rd_kafka_broker_bufq_timeout_scan (rkb=rkb@entry=0xb31fb0, is_waitresp_q=is_waitresp_q@entry=0, rkbq=rkbq@entry=0xb32180, partial_cntp=partial_cntp@entry=0x7f09ee56125c,
    err=err@entry=RD_KAFKA_RESP_ERR__TIMED_OUT_QUEUE, now=now@entry=215591720139) at rdkafka_broker.c:558
#5  0x00007f09f34291ed in rd_kafka_broker_timeout_scan (now=215591720139, rkb=0xb31fb0) at rdkafka_broker.c:594
#6  rd_kafka_broker_serve (rkb=rkb@entry=0xb31fb0, abs_timeout=abs_timeout@entry=215591720118) at rdkafka_broker.c:2562
#7  0x00007f09f3429597 in rd_kafka_broker_ua_idle (rkb=rkb@entry=0xb31fb0, timeout_ms=<optimized out>, timeout_ms@entry=-1) at rdkafka_broker.c:2617
#8  0x00007f09f3429a24 in rd_kafka_broker_thread_main (arg=arg@entry=0xb31fb0) at rdkafka_broker.c:3552
#9  0x00007f09f34763d7 in _thrd_wrapper_function (aArg=<optimized out>) at tinycthread.c:583
#10 0x00007f09f2a4baa1 in start_thread () from /lib64/libpthread.so.0
#11 0x00007f09f2798bdd in clone () from /lib64/libc.so.6

(gdb) f 4
#4  0x00007f09f342300b in rd_kafka_broker_bufq_timeout_scan (rkb=rkb@entry=0xb31fb0, is_waitresp_q=is_waitresp_q@entry=0, rkbq=rkbq@entry=0xb32180, partial_cntp=partial_cntp@entry=0x7f09ee56125c,
err=err@entry=RD_KAFKA_RESP_ERR__TIMED_OUT_QUEUE, now=now@entry=215591720139) at rdkafka_broker.c:558

(gdb) p rkb->rkb_outbufs
$58 = {rkbq_bufs = {tqh_first = 0x0, tqh_last = 0x7f09ee560fd0}, rkbq_cnt = {val = 0}, rkbq_msg_cnt = {val = 0}}

(gdb) p &rkb->rkb_outbufs
$59 = (rd_kafka_bufq_t *) 0xb32180

(gdb) p cnt
$60 = 1

Here is how it happened,

rd_kafka_broker_bufq_timeout_scan will iterate each buf in rkb->rkb_outbufs. De-queue one and process one each time.
But, theres one kind of buf, whose callback is rd_kafka_broker_handle_ApiVersion(), and the callback would directly call rd_kafka_broker_failed() in advance.
Unfortunately, rd_kafka_broker_failed() will touch the rkb->rkb_outbufs (and rkb_waitresps) as well. It will clear the rkb->rkb_outbufs!
Now, the rkb_outbufs becomes empty, within the loop in TAILQ_FOREACH_SAFE(, rkb_outbufs, next) (see step 1). Since the next pointer previously saved could be not NULL, and loop would continue, thus call rd_kafka_bufq_deq() which triggered the assert statement.

So, ""rd_kafka_broker_handle_ApiVersion()"" should not call ""rd_kafka_broker_fail()"" directly, -- since it would clear the rkb_outbufs while it's still within the iterating loop","Sometimes librdkafka would crash with
*** rdkafka_buf.c:201:rd_kafka_bufq_deq: assert: rd_atomic32_get(&rkbufq->rkbq_cnt) > 0 ***
(gdb) bt
#0  0x00007f09f26e2495 in raise () from /lib64/libc.so.6
#1  0x00007f09f26e3c75 in abort () from /lib64/libc.so.6
#2  0x00007f09f3410353 in rd_kafka_crash (file=file@entry=0x7f09f34caf7f ""rdkafka_buf.c"", line=line@entry=197, function=function@entry=0x7f09f34cb130 <__FUNCTION__.22201> ""rd_kafka_bufq_deq"", rk=rk@entry=0x0,
    reason=reason@entry=0x7f09f34caff8 ""assert: rd_atomic32_get(&rkbufq->rkbq_cnt) > 0"") at rdkafka.c:3432
#3  0x00007f09f343b975 in rd_kafka_bufq_deq (rkbufq=rkbufq@entry=0xb32180, rkbuf=rkbuf@entry=0xb36050) at rdkafka_buf.c:197
#4  0x00007f09f342300b in rd_kafka_broker_bufq_timeout_scan (rkb=rkb@entry=0xb31fb0, is_waitresp_q=is_waitresp_q@entry=0, rkbq=rkbq@entry=0xb32180, partial_cntp=partial_cntp@entry=0x7f09ee56125c,
    err=err@entry=RD_KAFKA_RESP_ERR__TIMED_OUT_QUEUE, now=now@entry=215591720139) at rdkafka_broker.c:558
#5  0x00007f09f34291ed in rd_kafka_broker_timeout_scan (now=215591720139, rkb=0xb31fb0) at rdkafka_broker.c:594
#6  rd_kafka_broker_serve (rkb=rkb@entry=0xb31fb0, abs_timeout=abs_timeout@entry=215591720118) at rdkafka_broker.c:2562
#7  0x00007f09f3429597 in rd_kafka_broker_ua_idle (rkb=rkb@entry=0xb31fb0, timeout_ms=<optimized out>, timeout_ms@entry=-1) at rdkafka_broker.c:2617
#8  0x00007f09f3429a24 in rd_kafka_broker_thread_main (arg=arg@entry=0xb31fb0) at rdkafka_broker.c:3552
#9  0x00007f09f34763d7 in _thrd_wrapper_function (aArg=<optimized out>) at tinycthread.c:583
#10 0x00007f09f2a4baa1 in start_thread () from /lib64/libpthread.so.0
#11 0x00007f09f2798bdd in clone () from /lib64/libc.so.6

(gdb) f 4
#4  0x00007f09f342300b in rd_kafka_broker_bufq_timeout_scan (rkb=rkb@entry=0xb31fb0, is_waitresp_q=is_waitresp_q@entry=0, rkbq=rkbq@entry=0xb32180, partial_cntp=partial_cntp@entry=0x7f09ee56125c,
err=err@entry=RD_KAFKA_RESP_ERR__TIMED_OUT_QUEUE, now=now@entry=215591720139) at rdkafka_broker.c:558

(gdb) p rkb->rkb_outbufs
$58 = {rkbq_bufs = {tqh_first = 0x0, tqh_last = 0x7f09ee560fd0}, rkbq_cnt = {val = 0}, rkbq_msg_cnt = {val = 0}}

(gdb) p &rkb->rkb_outbufs
$59 = (rd_kafka_bufq_t *) 0xb32180

(gdb) p cnt
$60 = 1

Here is how it happened,

rd_kafka_broker_bufq_timeout_scan will iterate each buf in rkb->rkb_outbufs. De-queue one and process one each time.
But, theres one kind of buf, whose callback is rd_kafka_broker_handle_ApiVersion(), and the callback would directly call rd_kafka_broker_failed() in advance.
Unfortunately, rd_kafka_broker_failed() will touch the rkb->rkb_outbufs (and rkb_waitresps) as well. It will clear the rkb->rkb_outbufs!
Now, the rkb_outbufs becomes empty, within the loop in TAILQ_FOREACH_SAFE(, rkb_outbufs, next) (see step 1). Since the next pointer previously saved could be not NULL, and loop would continue, thus call rd_kafka_bufq_deq() which triggered the assert statement.

So, ""rd_kafka_broker_handle_ApiVersion()"" should not call ""rd_kafka_broker_fail()"" directly, -- since it would clear the rkb_outbufs while it's still within the iterating loop",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2268,2019-03-28T10:25:50Z,2019-04-11T07:11:23Z,2019-04-11T07:11:23Z,CLOSED,False,1,5,1,https://github.com/kenneth-jia,rd_kafka_broker_handle_ApiVersion() should not call rd_kafka_broker_f,1,[],https://github.com/edenhill/librdkafka/pull/2268,https://github.com/edenhill,2,https://github.com/edenhill/librdkafka/pull/2268#issuecomment-477905260,"Sometimes librdkafka would crash with
*** rdkafka_buf.c:201:rd_kafka_bufq_deq: assert: rd_atomic32_get(&rkbufq->rkbq_cnt) > 0 ***
(gdb) bt
#0  0x00007f09f26e2495 in raise () from /lib64/libc.so.6
#1  0x00007f09f26e3c75 in abort () from /lib64/libc.so.6
#2  0x00007f09f3410353 in rd_kafka_crash (file=file@entry=0x7f09f34caf7f ""rdkafka_buf.c"", line=line@entry=197, function=function@entry=0x7f09f34cb130 <__FUNCTION__.22201> ""rd_kafka_bufq_deq"", rk=rk@entry=0x0,
    reason=reason@entry=0x7f09f34caff8 ""assert: rd_atomic32_get(&rkbufq->rkbq_cnt) > 0"") at rdkafka.c:3432
#3  0x00007f09f343b975 in rd_kafka_bufq_deq (rkbufq=rkbufq@entry=0xb32180, rkbuf=rkbuf@entry=0xb36050) at rdkafka_buf.c:197
#4  0x00007f09f342300b in rd_kafka_broker_bufq_timeout_scan (rkb=rkb@entry=0xb31fb0, is_waitresp_q=is_waitresp_q@entry=0, rkbq=rkbq@entry=0xb32180, partial_cntp=partial_cntp@entry=0x7f09ee56125c,
    err=err@entry=RD_KAFKA_RESP_ERR__TIMED_OUT_QUEUE, now=now@entry=215591720139) at rdkafka_broker.c:558
#5  0x00007f09f34291ed in rd_kafka_broker_timeout_scan (now=215591720139, rkb=0xb31fb0) at rdkafka_broker.c:594
#6  rd_kafka_broker_serve (rkb=rkb@entry=0xb31fb0, abs_timeout=abs_timeout@entry=215591720118) at rdkafka_broker.c:2562
#7  0x00007f09f3429597 in rd_kafka_broker_ua_idle (rkb=rkb@entry=0xb31fb0, timeout_ms=<optimized out>, timeout_ms@entry=-1) at rdkafka_broker.c:2617
#8  0x00007f09f3429a24 in rd_kafka_broker_thread_main (arg=arg@entry=0xb31fb0) at rdkafka_broker.c:3552
#9  0x00007f09f34763d7 in _thrd_wrapper_function (aArg=<optimized out>) at tinycthread.c:583
#10 0x00007f09f2a4baa1 in start_thread () from /lib64/libpthread.so.0
#11 0x00007f09f2798bdd in clone () from /lib64/libc.so.6

(gdb) f 4
#4  0x00007f09f342300b in rd_kafka_broker_bufq_timeout_scan (rkb=rkb@entry=0xb31fb0, is_waitresp_q=is_waitresp_q@entry=0, rkbq=rkbq@entry=0xb32180, partial_cntp=partial_cntp@entry=0x7f09ee56125c,
err=err@entry=RD_KAFKA_RESP_ERR__TIMED_OUT_QUEUE, now=now@entry=215591720139) at rdkafka_broker.c:558

(gdb) p rkb->rkb_outbufs
$58 = {rkbq_bufs = {tqh_first = 0x0, tqh_last = 0x7f09ee560fd0}, rkbq_cnt = {val = 0}, rkbq_msg_cnt = {val = 0}}

(gdb) p &rkb->rkb_outbufs
$59 = (rd_kafka_bufq_t *) 0xb32180

(gdb) p cnt
$60 = 1

Here is how it happened,

rd_kafka_broker_bufq_timeout_scan will iterate each buf in rkb->rkb_outbufs. De-queue one and process one each time.
But, theres one kind of buf, whose callback is rd_kafka_broker_handle_ApiVersion(), and the callback would directly call rd_kafka_broker_failed() in advance.
Unfortunately, rd_kafka_broker_failed() will touch the rkb->rkb_outbufs (and rkb_waitresps) as well. It will clear the rkb->rkb_outbufs!
Now, the rkb_outbufs becomes empty, within the loop in TAILQ_FOREACH_SAFE(, rkb_outbufs, next) (see step 1). Since the next pointer previously saved could be not NULL, and loop would continue, thus call rd_kafka_bufq_deq() which triggered the assert statement.

So, ""rd_kafka_broker_handle_ApiVersion()"" should not call ""rd_kafka_broker_fail()"" directly, -- since it would clear the rkb_outbufs while it's still within the iterating loop","Great analysis of the issue!
But the proposed fix will not work, it will terminate the broker thread.
I think the best solution is to check the broker state in the ApiVersion handler and only call broker_fail() if the state is not RD_KAFKA_BROKER_STATE_DOWN.",True,{'CONFUSED': ['https://github.com/kenneth-jia']}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2268,2019-03-28T10:25:50Z,2019-04-11T07:11:23Z,2019-04-11T07:11:23Z,CLOSED,False,1,5,1,https://github.com/kenneth-jia,rd_kafka_broker_handle_ApiVersion() should not call rd_kafka_broker_f,1,[],https://github.com/edenhill/librdkafka/pull/2268,https://github.com/kenneth-jia,3,https://github.com/edenhill/librdkafka/pull/2268#issuecomment-478444615,"Sometimes librdkafka would crash with
*** rdkafka_buf.c:201:rd_kafka_bufq_deq: assert: rd_atomic32_get(&rkbufq->rkbq_cnt) > 0 ***
(gdb) bt
#0  0x00007f09f26e2495 in raise () from /lib64/libc.so.6
#1  0x00007f09f26e3c75 in abort () from /lib64/libc.so.6
#2  0x00007f09f3410353 in rd_kafka_crash (file=file@entry=0x7f09f34caf7f ""rdkafka_buf.c"", line=line@entry=197, function=function@entry=0x7f09f34cb130 <__FUNCTION__.22201> ""rd_kafka_bufq_deq"", rk=rk@entry=0x0,
    reason=reason@entry=0x7f09f34caff8 ""assert: rd_atomic32_get(&rkbufq->rkbq_cnt) > 0"") at rdkafka.c:3432
#3  0x00007f09f343b975 in rd_kafka_bufq_deq (rkbufq=rkbufq@entry=0xb32180, rkbuf=rkbuf@entry=0xb36050) at rdkafka_buf.c:197
#4  0x00007f09f342300b in rd_kafka_broker_bufq_timeout_scan (rkb=rkb@entry=0xb31fb0, is_waitresp_q=is_waitresp_q@entry=0, rkbq=rkbq@entry=0xb32180, partial_cntp=partial_cntp@entry=0x7f09ee56125c,
    err=err@entry=RD_KAFKA_RESP_ERR__TIMED_OUT_QUEUE, now=now@entry=215591720139) at rdkafka_broker.c:558
#5  0x00007f09f34291ed in rd_kafka_broker_timeout_scan (now=215591720139, rkb=0xb31fb0) at rdkafka_broker.c:594
#6  rd_kafka_broker_serve (rkb=rkb@entry=0xb31fb0, abs_timeout=abs_timeout@entry=215591720118) at rdkafka_broker.c:2562
#7  0x00007f09f3429597 in rd_kafka_broker_ua_idle (rkb=rkb@entry=0xb31fb0, timeout_ms=<optimized out>, timeout_ms@entry=-1) at rdkafka_broker.c:2617
#8  0x00007f09f3429a24 in rd_kafka_broker_thread_main (arg=arg@entry=0xb31fb0) at rdkafka_broker.c:3552
#9  0x00007f09f34763d7 in _thrd_wrapper_function (aArg=<optimized out>) at tinycthread.c:583
#10 0x00007f09f2a4baa1 in start_thread () from /lib64/libpthread.so.0
#11 0x00007f09f2798bdd in clone () from /lib64/libc.so.6

(gdb) f 4
#4  0x00007f09f342300b in rd_kafka_broker_bufq_timeout_scan (rkb=rkb@entry=0xb31fb0, is_waitresp_q=is_waitresp_q@entry=0, rkbq=rkbq@entry=0xb32180, partial_cntp=partial_cntp@entry=0x7f09ee56125c,
err=err@entry=RD_KAFKA_RESP_ERR__TIMED_OUT_QUEUE, now=now@entry=215591720139) at rdkafka_broker.c:558

(gdb) p rkb->rkb_outbufs
$58 = {rkbq_bufs = {tqh_first = 0x0, tqh_last = 0x7f09ee560fd0}, rkbq_cnt = {val = 0}, rkbq_msg_cnt = {val = 0}}

(gdb) p &rkb->rkb_outbufs
$59 = (rd_kafka_bufq_t *) 0xb32180

(gdb) p cnt
$60 = 1

Here is how it happened,

rd_kafka_broker_bufq_timeout_scan will iterate each buf in rkb->rkb_outbufs. De-queue one and process one each time.
But, theres one kind of buf, whose callback is rd_kafka_broker_handle_ApiVersion(), and the callback would directly call rd_kafka_broker_failed() in advance.
Unfortunately, rd_kafka_broker_failed() will touch the rkb->rkb_outbufs (and rkb_waitresps) as well. It will clear the rkb->rkb_outbufs!
Now, the rkb_outbufs becomes empty, within the loop in TAILQ_FOREACH_SAFE(, rkb_outbufs, next) (see step 1). Since the next pointer previously saved could be not NULL, and loop would continue, thus call rd_kafka_bufq_deq() which triggered the assert statement.

So, ""rd_kafka_broker_handle_ApiVersion()"" should not call ""rd_kafka_broker_fail()"" directly, -- since it would clear the rkb_outbufs while it's still within the iterating loop","The proposed fix was trying to call rd_kafka_broker_fail() asynchronously within function rd_kafka_broker_handle_ApiVersion(), thus rkb->rkb_outbufs would not be modified within the TAILQ_FOREACH_SAFE(xxx, rkb_outbufs, xxx) loop. I was trying to use op RD_KAFKA_OP_TERMINATE to trigger rd_kafka_broker_fail().
If we check the broker's state (whether is RD_KAFKA_BROKER_STATE_DOWN) before calling rd_kafka_broker_fail(), the problem still exists. (we can't guarantee the rkb_outbufs/rkb_waitresps be empty while broker's  RD_KAFKA_BROKER_STATE_DOWN) If the broker's state is not RD_KAFKA_BROKER_STATE_DOWN, a crash might still happen.
Maybe we could use another new op type to call rd_kafka_broker_fail() asynchronously?",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2268,2019-03-28T10:25:50Z,2019-04-11T07:11:23Z,2019-04-11T07:11:23Z,CLOSED,False,1,5,1,https://github.com/kenneth-jia,rd_kafka_broker_handle_ApiVersion() should not call rd_kafka_broker_f,1,[],https://github.com/edenhill/librdkafka/pull/2268,https://github.com/kenneth-jia,4,https://github.com/edenhill/librdkafka/pull/2268#issuecomment-479426218,"Sometimes librdkafka would crash with
*** rdkafka_buf.c:201:rd_kafka_bufq_deq: assert: rd_atomic32_get(&rkbufq->rkbq_cnt) > 0 ***
(gdb) bt
#0  0x00007f09f26e2495 in raise () from /lib64/libc.so.6
#1  0x00007f09f26e3c75 in abort () from /lib64/libc.so.6
#2  0x00007f09f3410353 in rd_kafka_crash (file=file@entry=0x7f09f34caf7f ""rdkafka_buf.c"", line=line@entry=197, function=function@entry=0x7f09f34cb130 <__FUNCTION__.22201> ""rd_kafka_bufq_deq"", rk=rk@entry=0x0,
    reason=reason@entry=0x7f09f34caff8 ""assert: rd_atomic32_get(&rkbufq->rkbq_cnt) > 0"") at rdkafka.c:3432
#3  0x00007f09f343b975 in rd_kafka_bufq_deq (rkbufq=rkbufq@entry=0xb32180, rkbuf=rkbuf@entry=0xb36050) at rdkafka_buf.c:197
#4  0x00007f09f342300b in rd_kafka_broker_bufq_timeout_scan (rkb=rkb@entry=0xb31fb0, is_waitresp_q=is_waitresp_q@entry=0, rkbq=rkbq@entry=0xb32180, partial_cntp=partial_cntp@entry=0x7f09ee56125c,
    err=err@entry=RD_KAFKA_RESP_ERR__TIMED_OUT_QUEUE, now=now@entry=215591720139) at rdkafka_broker.c:558
#5  0x00007f09f34291ed in rd_kafka_broker_timeout_scan (now=215591720139, rkb=0xb31fb0) at rdkafka_broker.c:594
#6  rd_kafka_broker_serve (rkb=rkb@entry=0xb31fb0, abs_timeout=abs_timeout@entry=215591720118) at rdkafka_broker.c:2562
#7  0x00007f09f3429597 in rd_kafka_broker_ua_idle (rkb=rkb@entry=0xb31fb0, timeout_ms=<optimized out>, timeout_ms@entry=-1) at rdkafka_broker.c:2617
#8  0x00007f09f3429a24 in rd_kafka_broker_thread_main (arg=arg@entry=0xb31fb0) at rdkafka_broker.c:3552
#9  0x00007f09f34763d7 in _thrd_wrapper_function (aArg=<optimized out>) at tinycthread.c:583
#10 0x00007f09f2a4baa1 in start_thread () from /lib64/libpthread.so.0
#11 0x00007f09f2798bdd in clone () from /lib64/libc.so.6

(gdb) f 4
#4  0x00007f09f342300b in rd_kafka_broker_bufq_timeout_scan (rkb=rkb@entry=0xb31fb0, is_waitresp_q=is_waitresp_q@entry=0, rkbq=rkbq@entry=0xb32180, partial_cntp=partial_cntp@entry=0x7f09ee56125c,
err=err@entry=RD_KAFKA_RESP_ERR__TIMED_OUT_QUEUE, now=now@entry=215591720139) at rdkafka_broker.c:558

(gdb) p rkb->rkb_outbufs
$58 = {rkbq_bufs = {tqh_first = 0x0, tqh_last = 0x7f09ee560fd0}, rkbq_cnt = {val = 0}, rkbq_msg_cnt = {val = 0}}

(gdb) p &rkb->rkb_outbufs
$59 = (rd_kafka_bufq_t *) 0xb32180

(gdb) p cnt
$60 = 1

Here is how it happened,

rd_kafka_broker_bufq_timeout_scan will iterate each buf in rkb->rkb_outbufs. De-queue one and process one each time.
But, theres one kind of buf, whose callback is rd_kafka_broker_handle_ApiVersion(), and the callback would directly call rd_kafka_broker_failed() in advance.
Unfortunately, rd_kafka_broker_failed() will touch the rkb->rkb_outbufs (and rkb_waitresps) as well. It will clear the rkb->rkb_outbufs!
Now, the rkb_outbufs becomes empty, within the loop in TAILQ_FOREACH_SAFE(, rkb_outbufs, next) (see step 1). Since the next pointer previously saved could be not NULL, and loop would continue, thus call rd_kafka_bufq_deq() which triggered the assert statement.

So, ""rd_kafka_broker_handle_ApiVersion()"" should not call ""rd_kafka_broker_fail()"" directly, -- since it would clear the rkb_outbufs while it's still within the iterating loop","Great analysis of the issue!
But the proposed fix will not work, it will terminate the broker thread.
I think the best solution is to check the broker state in the ApiVersion handler and only call broker_fail() if the state is not RD_KAFKA_BROKER_STATE_DOWN.

Sorry, I'm confused. :-(  The only place setting broker's state to be RD_KAFKA_BROKER_STATE_DOWN is within function rd_kafka_broker_fail(). That means the ApiVersion handler even don't need to call broker_fail() (again)?",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2274,2019-04-08T04:56:24Z,2019-04-16T06:41:14Z,2019-04-16T09:49:32Z,MERGED,True,0,3,1,https://github.com/myd7349,CMake: Remove redundant LIBRDKAFKACPP_EXPORTS definition,1,[],https://github.com/edenhill/librdkafka/pull/2274,https://github.com/myd7349,1,https://github.com/edenhill/librdkafka/pull/2274,"Hi! I am creating a portfile for librdkafka in vcpkg these days. And I encountered several unresolved external symbol errors related to those static const data members defined  in RdKafka::Topic when I built librdkafka as a shared library on Win32: microsoft/vcpkg#5921 (comment)
After some research, I found that this issue seems caused by a redundant LIBRDKAFKACPP_EXPORTS definition in this file: https://github.com/edenhill/librdkafka/blob/master/src-cpp/CMakeLists.txt#L37-L39
This redundant definition causes RD_EXPORT expanded as __declspec(dllexport) in a CMake-based project.
A small CMake project to help reproduce this problem: https://github.com/myd7349/Ongoing-Study/tree/master/cpp/CMake/vcpkg/librdkafka_pr_2274","Hi! I am creating a portfile for librdkafka in vcpkg these days. And I encountered several unresolved external symbol errors related to those static const data members defined  in RdKafka::Topic when I built librdkafka as a shared library on Win32: microsoft/vcpkg#5921 (comment)
After some research, I found that this issue seems caused by a redundant LIBRDKAFKACPP_EXPORTS definition in this file: https://github.com/edenhill/librdkafka/blob/master/src-cpp/CMakeLists.txt#L37-L39
This redundant definition causes RD_EXPORT expanded as __declspec(dllexport) in a CMake-based project.
A small CMake project to help reproduce this problem: https://github.com/myd7349/Ongoing-Study/tree/master/cpp/CMake/vcpkg/librdkafka_pr_2274",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2274,2019-04-08T04:56:24Z,2019-04-16T06:41:14Z,2019-04-16T09:49:32Z,MERGED,True,0,3,1,https://github.com/myd7349,CMake: Remove redundant LIBRDKAFKACPP_EXPORTS definition,1,[],https://github.com/edenhill/librdkafka/pull/2274,https://github.com/edenhill,2,https://github.com/edenhill/librdkafka/pull/2274#issuecomment-480907846,"Hi! I am creating a portfile for librdkafka in vcpkg these days. And I encountered several unresolved external symbol errors related to those static const data members defined  in RdKafka::Topic when I built librdkafka as a shared library on Win32: microsoft/vcpkg#5921 (comment)
After some research, I found that this issue seems caused by a redundant LIBRDKAFKACPP_EXPORTS definition in this file: https://github.com/edenhill/librdkafka/blob/master/src-cpp/CMakeLists.txt#L37-L39
This redundant definition causes RD_EXPORT expanded as __declspec(dllexport) in a CMake-based project.
A small CMake project to help reproduce this problem: https://github.com/myd7349/Ongoing-Study/tree/master/cpp/CMake/vcpkg/librdkafka_pr_2274","Hey @raulbocanegra, you added the lines in question (https://github.com/edenhill/librdkafka/blob/master/src-cpp/CMakeLists.txt#L37-L39).
Would you like to help review this PR?",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2274,2019-04-08T04:56:24Z,2019-04-16T06:41:14Z,2019-04-16T09:49:32Z,MERGED,True,0,3,1,https://github.com/myd7349,CMake: Remove redundant LIBRDKAFKACPP_EXPORTS definition,1,[],https://github.com/edenhill/librdkafka/pull/2274,https://github.com/rbocanegra,3,https://github.com/edenhill/librdkafka/pull/2274#issuecomment-482599158,"Hi! I am creating a portfile for librdkafka in vcpkg these days. And I encountered several unresolved external symbol errors related to those static const data members defined  in RdKafka::Topic when I built librdkafka as a shared library on Win32: microsoft/vcpkg#5921 (comment)
After some research, I found that this issue seems caused by a redundant LIBRDKAFKACPP_EXPORTS definition in this file: https://github.com/edenhill/librdkafka/blob/master/src-cpp/CMakeLists.txt#L37-L39
This redundant definition causes RD_EXPORT expanded as __declspec(dllexport) in a CMake-based project.
A small CMake project to help reproduce this problem: https://github.com/myd7349/Ongoing-Study/tree/master/cpp/CMake/vcpkg/librdkafka_pr_2274","Hi! Sadly I am not using C++ nor librdkafka at my company since months ago. Anyway, the change seems correct.",True,"{'THUMBS_UP': ['https://github.com/edenhill'], 'HEART': ['https://github.com/myd7349']}"
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2274,2019-04-08T04:56:24Z,2019-04-16T06:41:14Z,2019-04-16T09:49:32Z,MERGED,True,0,3,1,https://github.com/myd7349,CMake: Remove redundant LIBRDKAFKACPP_EXPORTS definition,1,[],https://github.com/edenhill/librdkafka/pull/2274,https://github.com/edenhill,4,https://github.com/edenhill/librdkafka/pull/2274#issuecomment-483531522,"Hi! I am creating a portfile for librdkafka in vcpkg these days. And I encountered several unresolved external symbol errors related to those static const data members defined  in RdKafka::Topic when I built librdkafka as a shared library on Win32: microsoft/vcpkg#5921 (comment)
After some research, I found that this issue seems caused by a redundant LIBRDKAFKACPP_EXPORTS definition in this file: https://github.com/edenhill/librdkafka/blob/master/src-cpp/CMakeLists.txt#L37-L39
This redundant definition causes RD_EXPORT expanded as __declspec(dllexport) in a CMake-based project.
A small CMake project to help reproduce this problem: https://github.com/myd7349/Ongoing-Study/tree/master/cpp/CMake/vcpkg/librdkafka_pr_2274",Thank you!,True,{'HEART': ['https://github.com/myd7349']}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2287,2019-04-18T18:47:10Z,2019-04-29T18:13:42Z,2019-04-29T18:26:22Z,MERGED,True,44,2,3,https://github.com/benesch,Update error response codes,1,[],https://github.com/edenhill/librdkafka/pull/2287,https://github.com/benesch,1,https://github.com/edenhill/librdkafka/pull/2287,Update to include error codes from v2.2 of Kafka.,Update to include error codes from v2.2 of Kafka.,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2287,2019-04-18T18:47:10Z,2019-04-29T18:13:42Z,2019-04-29T18:26:22Z,MERGED,True,44,2,3,https://github.com/benesch,Update error response codes,1,[],https://github.com/edenhill/librdkafka/pull/2287,https://github.com/benesch,2,https://github.com/edenhill/librdkafka/pull/2287#issuecomment-486227046,Update to include error codes from v2.2 of Kafka.,Good point! Done.,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2287,2019-04-18T18:47:10Z,2019-04-29T18:13:42Z,2019-04-29T18:26:22Z,MERGED,True,44,2,3,https://github.com/benesch,Update error response codes,1,[],https://github.com/edenhill/librdkafka/pull/2287,https://github.com/edenhill,3,https://github.com/edenhill/librdkafka/pull/2287#issuecomment-487686449,Update to include error codes from v2.2 of Kafka.,Thank you for this!,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2287,2019-04-18T18:47:10Z,2019-04-29T18:13:42Z,2019-04-29T18:26:22Z,MERGED,True,44,2,3,https://github.com/benesch,Update error response codes,1,[],https://github.com/edenhill/librdkafka/pull/2287,https://github.com/benesch,4,https://github.com/edenhill/librdkafka/pull/2287#issuecomment-487690878,Update to include error codes from v2.2 of Kafka.,Thanks for reviewing! Looking forward to the next release.,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2293,2019-04-23T14:49:09Z,2019-05-21T08:53:58Z,2020-05-08T08:58:51Z,MERGED,True,638,399,19,https://github.com/edenhill," SASL GSSAPI: Don't run kinit refresh for each broker, just per client instance",7,[],https://github.com/edenhill/librdkafka/pull/2293,https://github.com/edenhill,1,https://github.com/edenhill/librdkafka/pull/2293,"Changes in this PR:

run kinit acquire/refresh on the client level instead of for each broker thread.
changed the default sasl.kerberos.kinit.cmd to refresh or acquire a ticket, since cyrus-sasl will do neither for us.
refactor SASL framework for generic init/term/ready, which affects GSSAPI and OAUTHBEARER","Changes in this PR:

run kinit acquire/refresh on the client level instead of for each broker thread.
changed the default sasl.kerberos.kinit.cmd to refresh or acquire a ticket, since cyrus-sasl will do neither for us.
refactor SASL framework for generic init/term/ready, which affects GSSAPI and OAUTHBEARER",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2298,2019-04-25T01:46:48Z,2019-04-25T08:26:57Z,2019-05-07T02:16:54Z,CLOSED,False,5,5,4,https://github.com/kermitbu,trust the headers,1,[],https://github.com/edenhill/librdkafka/pull/2298,https://github.com/kermitbu,1,https://github.com/edenhill/librdkafka/pull/2298,,,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2298,2019-04-25T01:46:48Z,2019-04-25T08:26:57Z,2019-05-07T02:16:54Z,CLOSED,False,5,5,4,https://github.com/kermitbu,trust the headers,1,[],https://github.com/edenhill/librdkafka/pull/2298,https://github.com/edenhill,2,https://github.com/edenhill/librdkafka/pull/2298#issuecomment-486554676,,"I have no idea what this is for, can you please explain the reasoning behind this change?",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2298,2019-04-25T01:46:48Z,2019-04-25T08:26:57Z,2019-05-07T02:16:54Z,CLOSED,False,5,5,4,https://github.com/kermitbu,trust the headers,1,[],https://github.com/edenhill/librdkafka/pull/2298,https://github.com/kermitbu,3,https://github.com/edenhill/librdkafka/pull/2298#issuecomment-486565001,,"I have no idea what this is for, can you please explain the reasoning behind this change?

I don't want to malloc headers every time when produce data, but my modification method is not suitable and I am improving the method",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2298,2019-04-25T01:46:48Z,2019-04-25T08:26:57Z,2019-05-07T02:16:54Z,CLOSED,False,5,5,4,https://github.com/kermitbu,trust the headers,1,[],https://github.com/edenhill/librdkafka/pull/2298,https://github.com/edenhill,4,https://github.com/edenhill/librdkafka/pull/2298#issuecomment-486570391,,So just pass NULL as headers.,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2298,2019-04-25T01:46:48Z,2019-04-25T08:26:57Z,2019-05-07T02:16:54Z,CLOSED,False,5,5,4,https://github.com/kermitbu,trust the headers,1,[],https://github.com/edenhill/librdkafka/pull/2298,https://github.com/kermitbu,5,https://github.com/edenhill/librdkafka/pull/2298#issuecomment-486571333,,"But I want to pass the header for extra info, What should I do",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2298,2019-04-25T01:46:48Z,2019-04-25T08:26:57Z,2019-05-07T02:16:54Z,CLOSED,False,5,5,4,https://github.com/kermitbu,trust the headers,1,[],https://github.com/edenhill/librdkafka/pull/2298,https://github.com/edenhill,6,https://github.com/edenhill/librdkafka/pull/2298#issuecomment-486572694,,"You either pass the headers that should be added to the message, or you dont.
If your headers are identical for each message you will still need to duplicate them for each time you call produce().",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2298,2019-04-25T01:46:48Z,2019-04-25T08:26:57Z,2019-05-07T02:16:54Z,CLOSED,False,5,5,4,https://github.com/kermitbu,trust the headers,1,[],https://github.com/edenhill/librdkafka/pull/2298,https://github.com/kermitbu,7,https://github.com/edenhill/librdkafka/pull/2298#issuecomment-486574108,,"Well, I have to make a version for business needs.
malloc for each production data request is a time-consuming activity.",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2298,2019-04-25T01:46:48Z,2019-04-25T08:26:57Z,2019-05-07T02:16:54Z,CLOSED,False,5,5,4,https://github.com/kermitbu,trust the headers,1,[],https://github.com/edenhill/librdkafka/pull/2298,https://github.com/edenhill,8,https://github.com/edenhill/librdkafka/pull/2298#issuecomment-486583606,,"What rate are you producing messages at?
In your measurements, what is the performance drop or CPU cost of creating a new Headers object for each produce() call?",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2298,2019-04-25T01:46:48Z,2019-04-25T08:26:57Z,2019-05-07T02:16:54Z,CLOSED,False,5,5,4,https://github.com/kermitbu,trust the headers,1,[],https://github.com/edenhill/librdkafka/pull/2298,https://github.com/kermitbu,9,https://github.com/edenhill/librdkafka/pull/2298#issuecomment-486585712,,"Message generation rates are around 1.4 million per second, and reducing malloc times is a point of optimization based on past experience",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2298,2019-04-25T01:46:48Z,2019-04-25T08:26:57Z,2019-05-07T02:16:54Z,CLOSED,False,5,5,4,https://github.com/kermitbu,trust the headers,1,[],https://github.com/edenhill/librdkafka/pull/2298,https://github.com/edenhill,10,https://github.com/edenhill/librdkafka/pull/2298#issuecomment-486627237,,And your headers are identical for each message?,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2298,2019-04-25T01:46:48Z,2019-04-25T08:26:57Z,2019-05-07T02:16:54Z,CLOSED,False,5,5,4,https://github.com/kermitbu,trust the headers,1,[],https://github.com/edenhill/librdkafka/pull/2298,https://github.com/kermitbu,11,https://github.com/edenhill/librdkafka/pull/2298#issuecomment-486637499,,"Each message has a different header, which I want to use to log breakpoint information when an exception occurs. The breakpoint information is not the same as offset, and the breakpoint information is a two-dimensional data.
My business is to collect 1.4 million data per second from upstream, and then write these data into multiple partitions under multiple topics. When there is an exception, I need to get the maximum number of multiple partitions in Kafka. Is there any good way?",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2298,2019-04-25T01:46:48Z,2019-04-25T08:26:57Z,2019-05-07T02:16:54Z,CLOSED,False,5,5,4,https://github.com/kermitbu,trust the headers,1,[],https://github.com/edenhill/librdkafka/pull/2298,https://github.com/kermitbu,12,https://github.com/edenhill/librdkafka/pull/2298#issuecomment-486639337,,"I have considered recording breakpoint information into another topic separately before, but I think it is equivalent to sending data twice and involves transaction processing, so I don't want to consider such a scheme.",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2298,2019-04-25T01:46:48Z,2019-04-25T08:26:57Z,2019-05-07T02:16:54Z,CLOSED,False,5,5,4,https://github.com/kermitbu,trust the headers,1,[],https://github.com/edenhill/librdkafka/pull/2298,https://github.com/edenhill,13,https://github.com/edenhill/librdkafka/pull/2298#issuecomment-486653324,,"So if each message has unique headers they would need to be separately allocated anyway, since there is no point in reusing them.",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2298,2019-04-25T01:46:48Z,2019-04-25T08:26:57Z,2019-05-07T02:16:54Z,CLOSED,False,5,5,4,https://github.com/kermitbu,trust the headers,1,[],https://github.com/edenhill/librdkafka/pull/2298,https://github.com/edenhill,14,https://github.com/edenhill/librdkafka/pull/2298#issuecomment-486653630,,"You can use metadata() to retrieve the number of partitions for a topic.
But it is better to rely on the partitioner to distribute messages among the existing partitions.",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2298,2019-04-25T01:46:48Z,2019-04-25T08:26:57Z,2019-05-07T02:16:54Z,CLOSED,False,5,5,4,https://github.com/kermitbu,trust the headers,1,[],https://github.com/edenhill/librdkafka/pull/2298,https://github.com/kermitbu,15,https://github.com/edenhill/librdkafka/pull/2298#issuecomment-486883915,,"Each message has a unique header, but it takes time for each header to reapply for memory. My idea is that the memory of this header is reused and used again after sending. This is controlled by the user.
In addition, if I want to reuse the memory of value and control the application and release of memory by myself, and I don't want to make a copy of rdkafka, can I set the msgflags parameter of rd_kafka_produce to 0?
I learned that the metadata information included all partition information for each topic, but I could not find what I wanted",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2298,2019-04-25T01:46:48Z,2019-04-25T08:26:57Z,2019-05-07T02:16:54Z,CLOSED,False,5,5,4,https://github.com/kermitbu,trust the headers,1,[],https://github.com/edenhill/librdkafka/pull/2298,https://github.com/edenhill,16,https://github.com/edenhill/librdkafka/pull/2298#issuecomment-487611522,,"The msgflags only applies to the produced value, not key or headers.",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2298,2019-04-25T01:46:48Z,2019-04-25T08:26:57Z,2019-05-07T02:16:54Z,CLOSED,False,5,5,4,https://github.com/kermitbu,trust the headers,1,[],https://github.com/edenhill/librdkafka/pull/2298,https://github.com/kermitbu,17,https://github.com/edenhill/librdkafka/pull/2298#issuecomment-489486178,,Thanks for your reply. I plan to use jemalloc to replace the malloc of glibc,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2301,2019-04-25T20:28:33Z,2019-04-26T08:35:16Z,2019-04-26T08:35:25Z,MERGED,True,67,0,3,https://github.com/edenhill,Added rd_kafka_conf() to retrieve the client's configuration object,1,[],https://github.com/edenhill/librdkafka/pull/2301,https://github.com/edenhill,1,https://github.com/edenhill/librdkafka/pull/2301,,,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2301,2019-04-25T20:28:33Z,2019-04-26T08:35:16Z,2019-04-26T08:35:25Z,MERGED,True,67,0,3,https://github.com/edenhill,Added rd_kafka_conf() to retrieve the client's configuration object,1,[],https://github.com/edenhill/librdkafka/pull/2301,https://github.com/mhowlett,2,https://github.com/edenhill/librdkafka/pull/2301#issuecomment-486896817,,"can this be used to get default values? (without knowing, that's what i'd assume the main use of this would be?) if so test for that would be good. regardless, LGTM.",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2301,2019-04-25T20:28:33Z,2019-04-26T08:35:16Z,2019-04-26T08:35:25Z,MERGED,True,67,0,3,https://github.com/edenhill,Added rd_kafka_conf() to retrieve the client's configuration object,1,[],https://github.com/edenhill/librdkafka/pull/2301,https://github.com/edenhill,3,https://github.com/edenhill/librdkafka/pull/2301#issuecomment-486969021,,"To get default values you would instantiate a fresh new config object with rd_kafka_conf_new().
This is for retrieving the values that are actually used by a client instance, including such values that have been automatically adjusted (e.g., for idempotence).
The need for this came up with the SASL OAUTHBEARER token refresh callback that may need to use other configuration parameters for its authentication.",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2305,2019-04-29T16:59:14Z,2019-08-12T08:41:52Z,2019-08-12T08:41:56Z,MERGED,True,5,1,1,https://github.com/Whissi,configure: Add option to disable automagic dependency on zstd,1,[],https://github.com/edenhill/librdkafka/pull/2305,https://github.com/Whissi,1,https://github.com/edenhill/librdkafka/pull/2305,"This commit will add an option which will allow you to explicit disable
zstd usage.","This commit will add an option which will allow you to explicit disable
zstd usage.",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2305,2019-04-29T16:59:14Z,2019-08-12T08:41:52Z,2019-08-12T08:41:56Z,MERGED,True,5,1,1,https://github.com/Whissi,configure: Add option to disable automagic dependency on zstd,1,[],https://github.com/edenhill/librdkafka/pull/2305,https://github.com/benesch,2,https://github.com/edenhill/librdkafka/pull/2305#issuecomment-518704501,"This commit will add an option which will allow you to explicit disable
zstd usage.","Not the PR author, but I'm interested in seeing this merged! Please see my response above. I actually think it would make sense to merge this as is for consistency with the other mklove lib modules and then if you'd like I can submit a follow-on PR to remove the default expansion across all of the mklove lib modules.",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2305,2019-04-29T16:59:14Z,2019-08-12T08:41:52Z,2019-08-12T08:41:56Z,MERGED,True,5,1,1,https://github.com/Whissi,configure: Add option to disable automagic dependency on zstd,1,[],https://github.com/edenhill/librdkafka/pull/2305,https://github.com/edenhill,3,https://github.com/edenhill/librdkafka/pull/2305#issuecomment-520342114,"This commit will add an option which will allow you to explicit disable
zstd usage.",Thank you!,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2307,2019-05-01T02:30:44Z,2019-05-06T15:51:11Z,2019-05-07T04:28:35Z,MERGED,True,207,4,4,https://github.com/myd7349,CMake: Take care of zstd static lib on Win32,5,[],https://github.com/edenhill/librdkafka/pull/2307,https://github.com/myd7349,1,https://github.com/edenhill/librdkafka/pull/2307,"When built as a static library on Win32, zstd has output names like zstd_static.lib (Release) and zstd_staticd.lib (Debug):
https://github.com/facebook/zstd/blob/69baaee3e42f90dedea2c946bc19bfeac4e782ee/build/cmake/lib/CMakeLists.txt#L141
and find_library(ZSTD zstd) will not work in such cases.","When built as a static library on Win32, zstd has output names like zstd_static.lib (Release) and zstd_staticd.lib (Debug):
https://github.com/facebook/zstd/blob/69baaee3e42f90dedea2c946bc19bfeac4e782ee/build/cmake/lib/CMakeLists.txt#L141
and find_library(ZSTD zstd) will not work in such cases.",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2307,2019-05-01T02:30:44Z,2019-05-06T15:51:11Z,2019-05-07T04:28:35Z,MERGED,True,207,4,4,https://github.com/myd7349,CMake: Take care of zstd static lib on Win32,5,[],https://github.com/edenhill/librdkafka/pull/2307,https://github.com/edenhill,2,https://github.com/edenhill/librdkafka/pull/2307#issuecomment-489671445,"When built as a static library on Win32, zstd has output names like zstd_static.lib (Release) and zstd_staticd.lib (Debug):
https://github.com/facebook/zstd/blob/69baaee3e42f90dedea2c946bc19bfeac4e782ee/build/cmake/lib/CMakeLists.txt#L141
and find_library(ZSTD zstd) will not work in such cases.",thank you!,True,{'HEART': ['https://github.com/myd7349']}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2307,2019-05-01T02:30:44Z,2019-05-06T15:51:11Z,2019-05-07T04:28:35Z,MERGED,True,207,4,4,https://github.com/myd7349,CMake: Take care of zstd static lib on Win32,5,[],https://github.com/edenhill/librdkafka/pull/2307,https://github.com/myd7349,3,https://github.com/edenhill/librdkafka/pull/2307#issuecomment-489903345,"When built as a static library on Win32, zstd has output names like zstd_static.lib (Release) and zstd_staticd.lib (Debug):
https://github.com/facebook/zstd/blob/69baaee3e42f90dedea2c946bc19bfeac4e782ee/build/cmake/lib/CMakeLists.txt#L141
and find_library(ZSTD zstd) will not work in such cases.","Hi! @edenhill Thanks!
BTW, the PR id in the commit message seems to be wrong.",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2309,2019-05-02T20:14:55Z,2019-06-03T21:16:10Z,2020-05-08T08:58:59Z,MERGED,True,3716,921,40,https://github.com/edenhill,Continuing on PR #1882: SSL cert verification and in-memory keys,14,[],https://github.com/edenhill/librdkafka/pull/2309,https://github.com/edenhill,1,https://github.com/edenhill/librdkafka/pull/2309,"This PR is a continuation on @noahdav's work on PR #1882.
Summary of changes compared to #1882:

The retrieve_cb() has been removed in favour of using standard configuration properties. The reasoning behind this s simplification, reduced code size, cross-language maintenance (some high-level languages, such as Go, can't use callbacks in a good way), and memory ownership issues (having the callback return cert memory to librdkafka in an effective and allocator-agnostic way is tricky).
Refactored the code and made it more in line with the librdkafka style guide.
Values of security-sensitive configuration properties are now cleared from memory when the value is no longer used, or on rd_kafka_t destruction. This is used for ssl.certificate.pem, ssl.key.pem and ssl.key.password.
The ssl_cert_verify_cb was changed to provide more information to the callback, including the full X509_STORE_CTX.
Add integration tests.

To be done:

Rework the Windows SSL cert store example to its own file.","This PR is a continuation on @noahdav's work on PR #1882.
Summary of changes compared to #1882:

The retrieve_cb() has been removed in favour of using standard configuration properties. The reasoning behind this s simplification, reduced code size, cross-language maintenance (some high-level languages, such as Go, can't use callbacks in a good way), and memory ownership issues (having the callback return cert memory to librdkafka in an effective and allocator-agnostic way is tricky).
Refactored the code and made it more in line with the librdkafka style guide.
Values of security-sensitive configuration properties are now cleared from memory when the value is no longer used, or on rd_kafka_t destruction. This is used for ssl.certificate.pem, ssl.key.pem and ssl.key.password.
The ssl_cert_verify_cb was changed to provide more information to the callback, including the full X509_STORE_CTX.
Add integration tests.

To be done:

Rework the Windows SSL cert store example to its own file.",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2309,2019-05-02T20:14:55Z,2019-06-03T21:16:10Z,2020-05-08T08:58:59Z,MERGED,True,3716,921,40,https://github.com/edenhill,Continuing on PR #1882: SSL cert verification and in-memory keys,14,[],https://github.com/edenhill/librdkafka/pull/2309,https://github.com/noahdav,2,https://github.com/edenhill/librdkafka/pull/2309#issuecomment-488827442,"This PR is a continuation on @noahdav's work on PR #1882.
Summary of changes compared to #1882:

The retrieve_cb() has been removed in favour of using standard configuration properties. The reasoning behind this s simplification, reduced code size, cross-language maintenance (some high-level languages, such as Go, can't use callbacks in a good way), and memory ownership issues (having the callback return cert memory to librdkafka in an effective and allocator-agnostic way is tricky).
Refactored the code and made it more in line with the librdkafka style guide.
Values of security-sensitive configuration properties are now cleared from memory when the value is no longer used, or on rd_kafka_t destruction. This is used for ssl.certificate.pem, ssl.key.pem and ssl.key.password.
The ssl_cert_verify_cb was changed to provide more information to the callback, including the full X509_STORE_CTX.
Add integration tests.

To be done:

Rework the Windows SSL cert store example to its own file.",@edenhill Thanks for the work on this PR.  I really appreciate it.  Due to the size of the change I would like to test this a little bit before I review.  I should be able to test by end of week.  Hope that works for your schedule.,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2309,2019-05-02T20:14:55Z,2019-06-03T21:16:10Z,2020-05-08T08:58:59Z,MERGED,True,3716,921,40,https://github.com/edenhill,Continuing on PR #1882: SSL cert verification and in-memory keys,14,[],https://github.com/edenhill/librdkafka/pull/2309,https://github.com/edenhill,3,https://github.com/edenhill/librdkafka/pull/2309#issuecomment-489013928,"This PR is a continuation on @noahdav's work on PR #1882.
Summary of changes compared to #1882:

The retrieve_cb() has been removed in favour of using standard configuration properties. The reasoning behind this s simplification, reduced code size, cross-language maintenance (some high-level languages, such as Go, can't use callbacks in a good way), and memory ownership issues (having the callback return cert memory to librdkafka in an effective and allocator-agnostic way is tricky).
Refactored the code and made it more in line with the librdkafka style guide.
Values of security-sensitive configuration properties are now cleared from memory when the value is no longer used, or on rd_kafka_t destruction. This is used for ssl.certificate.pem, ssl.key.pem and ssl.key.password.
The ssl_cert_verify_cb was changed to provide more information to the callback, including the full X509_STORE_CTX.
Add integration tests.

To be done:

Rework the Windows SSL cert store example to its own file.","Re PEM format:
I too stumbled upon the PEM issue as I was diving into the example code.
What's the format of the certificates in the cert store? DER?
If so we could provide rd_kafka_conf_set_ssl_key() and set_certificate(), set_ca(), etc.
Another question:
OpenSSL does not make use of the Windows cert store for finding CA/root certs.
But from what I understand this should be fairly simple to add to librdkafka, populating the OpenSSL cert store with all the Root certificates from the windows store.
Do you think this is a good idea? And if so, should we do it by default if no ssl.ca.xxxx is specified? (as it works on Linux/osx which sets up the default ca path if no ca is specified).",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2309,2019-05-02T20:14:55Z,2019-06-03T21:16:10Z,2020-05-08T08:58:59Z,MERGED,True,3716,921,40,https://github.com/edenhill,Continuing on PR #1882: SSL cert verification and in-memory keys,14,[],https://github.com/edenhill/librdkafka/pull/2309,https://github.com/noahdav,4,https://github.com/edenhill/librdkafka/pull/2309#issuecomment-489130993,"This PR is a continuation on @noahdav's work on PR #1882.
Summary of changes compared to #1882:

The retrieve_cb() has been removed in favour of using standard configuration properties. The reasoning behind this s simplification, reduced code size, cross-language maintenance (some high-level languages, such as Go, can't use callbacks in a good way), and memory ownership issues (having the callback return cert memory to librdkafka in an effective and allocator-agnostic way is tricky).
Refactored the code and made it more in line with the librdkafka style guide.
Values of security-sensitive configuration properties are now cleared from memory when the value is no longer used, or on rd_kafka_t destruction. This is used for ssl.certificate.pem, ssl.key.pem and ssl.key.password.
The ssl_cert_verify_cb was changed to provide more information to the callback, including the full X509_STORE_CTX.
Add integration tests.

To be done:

Rework the Windows SSL cert store example to its own file.","@edenhill thanks for your work on this.
Good point with the root store.  I think it would make sense to do this by default when no ssl.ca is specified.  In our case this seems to work without this but I think it would be a good idea to do.
When I did the original PR on this I was using the PFX format as that is what the windows cert store exports.  This has the public and private key in the buffer.
To convert to a PFX file from the public and private keys I do the following where client.key is the private key and client.pem is the public key
openssl.exe pkcs12 -export -out cert.pfx -inkey client.key -in client.pem
To verify that the pfx file has the private key I can use openssl to do this too.
openssl pkcs12 -in cert.pfx -nocerts -out priv.pem -nodes
The original PR I did assumed that the private key was in PFX format which looking back at this is probably not a good assumption for all cases.  On windows I can get the PFX as a buffer which OpenSSL can parse form PFXExportCertStoreEx.  I think if there was a way to specify the PFX to librdkafka that would be the best option here.  Then we can set the private key as we did before.
EVP_PKEY* pkey;
X509 *cert;
BIO* bio = BIO_new_mem_buf(pPfx, cbPfx);
PKCS12 *pkcs12 = d2i_PKCS12_bio(bio, nullptr);
PKCS12_parse(pkcs12, get_password(), &pkey, &cert, NULL))
SSL_CTX_use_PrivateKey(ctx, pkey);
SSL_CTX_check_private_key(ctx);
Once there is a way to specify the PFX to librdkafka I can test the verify callback.  If it has the full peer certificate I should be validate it as I did previously.",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2309,2019-05-02T20:14:55Z,2019-06-03T21:16:10Z,2020-05-08T08:58:59Z,MERGED,True,3716,921,40,https://github.com/edenhill,Continuing on PR #1882: SSL cert verification and in-memory keys,14,[],https://github.com/edenhill/librdkafka/pull/2309,https://github.com/edenhill,5,https://github.com/edenhill/librdkafka/pull/2309#issuecomment-489141884,"This PR is a continuation on @noahdav's work on PR #1882.
Summary of changes compared to #1882:

The retrieve_cb() has been removed in favour of using standard configuration properties. The reasoning behind this s simplification, reduced code size, cross-language maintenance (some high-level languages, such as Go, can't use callbacks in a good way), and memory ownership issues (having the callback return cert memory to librdkafka in an effective and allocator-agnostic way is tricky).
Refactored the code and made it more in line with the librdkafka style guide.
Values of security-sensitive configuration properties are now cleared from memory when the value is no longer used, or on rd_kafka_t destruction. This is used for ssl.certificate.pem, ssl.key.pem and ssl.key.password.
The ssl_cert_verify_cb was changed to provide more information to the callback, including the full X509_STORE_CTX.
Add integration tests.

To be done:

Rework the Windows SSL cert store example to its own file.","So it sounds like we should support PEM, DER and PKCS12 for at least the client's public and private keys, yeah?",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2309,2019-05-02T20:14:55Z,2019-06-03T21:16:10Z,2020-05-08T08:58:59Z,MERGED,True,3716,921,40,https://github.com/edenhill,Continuing on PR #1882: SSL cert verification and in-memory keys,14,[],https://github.com/edenhill/librdkafka/pull/2309,https://github.com/noahdav,6,https://github.com/edenhill/librdkafka/pull/2309#issuecomment-489209569,"This PR is a continuation on @noahdav's work on PR #1882.
Summary of changes compared to #1882:

The retrieve_cb() has been removed in favour of using standard configuration properties. The reasoning behind this s simplification, reduced code size, cross-language maintenance (some high-level languages, such as Go, can't use callbacks in a good way), and memory ownership issues (having the callback return cert memory to librdkafka in an effective and allocator-agnostic way is tricky).
Refactored the code and made it more in line with the librdkafka style guide.
Values of security-sensitive configuration properties are now cleared from memory when the value is no longer used, or on rd_kafka_t destruction. This is used for ssl.certificate.pem, ssl.key.pem and ssl.key.password.
The ssl_cert_verify_cb was changed to provide more information to the callback, including the full X509_STORE_CTX.
Add integration tests.

To be done:

Rework the Windows SSL cert store example to its own file.","Yes PKCS12 will work for our scenarios.  Although it contains the public and private keys we only need it to parse the private key.  The public key can still be in PEM format if that simplifies anything.
Once we get this working I can test the validation callback too.",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2309,2019-05-02T20:14:55Z,2019-06-03T21:16:10Z,2020-05-08T08:58:59Z,MERGED,True,3716,921,40,https://github.com/edenhill,Continuing on PR #1882: SSL cert verification and in-memory keys,14,[],https://github.com/edenhill/librdkafka/pull/2309,https://github.com/LighthouseJ,7,https://github.com/edenhill/librdkafka/pull/2309#issuecomment-489608238,"This PR is a continuation on @noahdav's work on PR #1882.
Summary of changes compared to #1882:

The retrieve_cb() has been removed in favour of using standard configuration properties. The reasoning behind this s simplification, reduced code size, cross-language maintenance (some high-level languages, such as Go, can't use callbacks in a good way), and memory ownership issues (having the callback return cert memory to librdkafka in an effective and allocator-agnostic way is tricky).
Refactored the code and made it more in line with the librdkafka style guide.
Values of security-sensitive configuration properties are now cleared from memory when the value is no longer used, or on rd_kafka_t destruction. This is used for ssl.certificate.pem, ssl.key.pem and ssl.key.password.
The ssl_cert_verify_cb was changed to provide more information to the callback, including the full X509_STORE_CTX.
Add integration tests.

To be done:

Rework the Windows SSL cert store example to its own file.","OpenSSL does not make use of the Windows cert store for finding CA/root certs.

Core OpenSSL yes, but this is not technically true.  OpenSSL has an implementation for it using the OpenSSL dynamic engine interface.  The engine is called ""capi"", which stands for Windows Crypto API, and it provides the glue (on Windows) to make Windows Crypto calls to access Windows cert stores, show Windows prompt for choosing certificates, access hardware devices, etc...",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2309,2019-05-02T20:14:55Z,2019-06-03T21:16:10Z,2020-05-08T08:58:59Z,MERGED,True,3716,921,40,https://github.com/edenhill,Continuing on PR #1882: SSL cert verification and in-memory keys,14,[],https://github.com/edenhill/librdkafka/pull/2309,https://github.com/edenhill,8,https://github.com/edenhill/librdkafka/pull/2309#issuecomment-489631873,"This PR is a continuation on @noahdav's work on PR #1882.
Summary of changes compared to #1882:

The retrieve_cb() has been removed in favour of using standard configuration properties. The reasoning behind this s simplification, reduced code size, cross-language maintenance (some high-level languages, such as Go, can't use callbacks in a good way), and memory ownership issues (having the callback return cert memory to librdkafka in an effective and allocator-agnostic way is tricky).
Refactored the code and made it more in line with the librdkafka style guide.
Values of security-sensitive configuration properties are now cleared from memory when the value is no longer used, or on rd_kafka_t destruction. This is used for ssl.certificate.pem, ssl.key.pem and ssl.key.password.
The ssl_cert_verify_cb was changed to provide more information to the callback, including the full X509_STORE_CTX.
Add integration tests.

To be done:

Rework the Windows SSL cert store example to its own file.","@LighthouseJ Interesting, have you had any luck using it?",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2309,2019-05-02T20:14:55Z,2019-06-03T21:16:10Z,2020-05-08T08:58:59Z,MERGED,True,3716,921,40,https://github.com/edenhill,Continuing on PR #1882: SSL cert verification and in-memory keys,14,[],https://github.com/edenhill/librdkafka/pull/2309,https://github.com/LighthouseJ,9,https://github.com/edenhill/librdkafka/pull/2309#issuecomment-489642251,"This PR is a continuation on @noahdav's work on PR #1882.
Summary of changes compared to #1882:

The retrieve_cb() has been removed in favour of using standard configuration properties. The reasoning behind this s simplification, reduced code size, cross-language maintenance (some high-level languages, such as Go, can't use callbacks in a good way), and memory ownership issues (having the callback return cert memory to librdkafka in an effective and allocator-agnostic way is tricky).
Refactored the code and made it more in line with the librdkafka style guide.
Values of security-sensitive configuration properties are now cleared from memory when the value is no longer used, or on rd_kafka_t destruction. This is used for ssl.certificate.pem, ssl.key.pem and ssl.key.password.
The ssl_cert_verify_cb was changed to provide more information to the callback, including the full X509_STORE_CTX.
Add integration tests.

To be done:

Rework the Windows SSL cert store example to its own file.","@edenhill Yes, actually I've needed a similar change to what @noahdav did in the original PR specifically to do w/ Windows Crypto API but @noahdav did a good job at capturing a lot of what I needed, mainly the plumbing in librdkafka that I needed, I just needed to add a few things in, but unfortunately needed a little hack. :)
I was anticipating making a PR on github for folks w/ changes I need once this change is settled and merged in.
One notable difference and reason for the hack is that in circumstance where Windows will not let you have the private key because it's stored on a hardware device (so not read from a PEM file), Windows gives you a realistic OpenSSL-looking thing but you cannot serialize it to a BIO for example or do parsing on it, and instead you can only use it.  The calling code in rdkafka_transport.c that was there attempted to parse it and it effectively taint the blackbox private key object, so I had to short-circuit some of the code to just use the object w/o a parse.
Once this is merged and I can find the time, I can make a PR with this change in possibly a less hacky way. :)",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2309,2019-05-02T20:14:55Z,2019-06-03T21:16:10Z,2020-05-08T08:58:59Z,MERGED,True,3716,921,40,https://github.com/edenhill,Continuing on PR #1882: SSL cert verification and in-memory keys,14,[],https://github.com/edenhill/librdkafka/pull/2309,https://github.com/noahdav,10,https://github.com/edenhill/librdkafka/pull/2309#issuecomment-489649946,"This PR is a continuation on @noahdav's work on PR #1882.
Summary of changes compared to #1882:

The retrieve_cb() has been removed in favour of using standard configuration properties. The reasoning behind this s simplification, reduced code size, cross-language maintenance (some high-level languages, such as Go, can't use callbacks in a good way), and memory ownership issues (having the callback return cert memory to librdkafka in an effective and allocator-agnostic way is tricky).
Refactored the code and made it more in line with the librdkafka style guide.
Values of security-sensitive configuration properties are now cleared from memory when the value is no longer used, or on rd_kafka_t destruction. This is used for ssl.certificate.pem, ssl.key.pem and ssl.key.password.
The ssl_cert_verify_cb was changed to provide more information to the callback, including the full X509_STORE_CTX.
Add integration tests.

To be done:

Rework the Windows SSL cert store example to its own file.","I originally looked at capi but did not think it was a good idea to require the implementation of the callbacks to need to deal with both OpenSSL and CryptAPI.  I only wanted to have each part of the code to deal with a single dependency and normalize it to OpenSSL since that is what librdkafka already does.
I am all for someone implementing the capi glue to librdkafka in a way that abstracts it and can handle the keys.  In our scenario the certificates are in the windows cert store but we do not interact with the windows cert store at all.  They are manged, rotated, etc by another service and we have an API which will eventually give us the PCCERT_CONTEXT.  Since we only have the PCCERT_CONTEXT I still need a way to convert this back to OpenSSL.  I think that the PKCS12 is the best way to handle this since I can convert back to the private key for OpenSSL.  If someone has a better option I am all for exploring another way.
@edenhill are you still planning on adding back the PKCS12 support or is this something I can add back?
I cannot use this PR as it currently stands for our scenario without this support.",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2309,2019-05-02T20:14:55Z,2019-06-03T21:16:10Z,2020-05-08T08:58:59Z,MERGED,True,3716,921,40,https://github.com/edenhill,Continuing on PR #1882: SSL cert verification and in-memory keys,14,[],https://github.com/edenhill/librdkafka/pull/2309,https://github.com/edenhill,11,https://github.com/edenhill/librdkafka/pull/2309#issuecomment-489652568,"This PR is a continuation on @noahdav's work on PR #1882.
Summary of changes compared to #1882:

The retrieve_cb() has been removed in favour of using standard configuration properties. The reasoning behind this s simplification, reduced code size, cross-language maintenance (some high-level languages, such as Go, can't use callbacks in a good way), and memory ownership issues (having the callback return cert memory to librdkafka in an effective and allocator-agnostic way is tricky).
Refactored the code and made it more in line with the librdkafka style guide.
Values of security-sensitive configuration properties are now cleared from memory when the value is no longer used, or on rd_kafka_t destruction. This is used for ssl.certificate.pem, ssl.key.pem and ssl.key.password.
The ssl_cert_verify_cb was changed to provide more information to the callback, including the full X509_STORE_CTX.
Add integration tests.

To be done:

Rework the Windows SSL cert store example to its own file.","@noahdav I'm working on adding specific config setters for the different format types, e.g.:
rd_kafka_conf_set_ssl_privatekey_pkcs12(..), but also keep the PEM-based configuration property since that will be useful for the high-level client bindings.",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2309,2019-05-02T20:14:55Z,2019-06-03T21:16:10Z,2020-05-08T08:58:59Z,MERGED,True,3716,921,40,https://github.com/edenhill,Continuing on PR #1882: SSL cert verification and in-memory keys,14,[],https://github.com/edenhill/librdkafka/pull/2309,https://github.com/noahdav,12,https://github.com/edenhill/librdkafka/pull/2309#issuecomment-489654696,"This PR is a continuation on @noahdav's work on PR #1882.
Summary of changes compared to #1882:

The retrieve_cb() has been removed in favour of using standard configuration properties. The reasoning behind this s simplification, reduced code size, cross-language maintenance (some high-level languages, such as Go, can't use callbacks in a good way), and memory ownership issues (having the callback return cert memory to librdkafka in an effective and allocator-agnostic way is tricky).
Refactored the code and made it more in line with the librdkafka style guide.
Values of security-sensitive configuration properties are now cleared from memory when the value is no longer used, or on rd_kafka_t destruction. This is used for ssl.certificate.pem, ssl.key.pem and ssl.key.password.
The ssl_cert_verify_cb was changed to provide more information to the callback, including the full X509_STORE_CTX.
Add integration tests.

To be done:

Rework the Windows SSL cert store example to its own file.",@edenhill That will be great.  I will test it once you have the changes.  Thank you for all of your support on getting the PR completed.,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2309,2019-05-02T20:14:55Z,2019-06-03T21:16:10Z,2020-05-08T08:58:59Z,MERGED,True,3716,921,40,https://github.com/edenhill,Continuing on PR #1882: SSL cert verification and in-memory keys,14,[],https://github.com/edenhill/librdkafka/pull/2309,https://github.com/edenhill,13,https://github.com/edenhill/librdkafka/pull/2309#issuecomment-490861452,"This PR is a continuation on @noahdav's work on PR #1882.
Summary of changes compared to #1882:

The retrieve_cb() has been removed in favour of using standard configuration properties. The reasoning behind this s simplification, reduced code size, cross-language maintenance (some high-level languages, such as Go, can't use callbacks in a good way), and memory ownership issues (having the callback return cert memory to librdkafka in an effective and allocator-agnostic way is tricky).
Refactored the code and made it more in line with the librdkafka style guide.
Values of security-sensitive configuration properties are now cleared from memory when the value is no longer used, or on rd_kafka_t destruction. This is used for ssl.certificate.pem, ssl.key.pem and ssl.key.password.
The ssl_cert_verify_cb was changed to provide more information to the callback, including the full X509_STORE_CTX.
Add integration tests.

To be done:

Rework the Windows SSL cert store example to its own file.","FYI: I gave OpenSSL's CAPI engine (Windows Crypto engine) a try and read thru its source; it does not provide a root CA certificate store, but it does seem to provide client certificate lookups.
I'm not sure the added dependency of capi.dll adds enough value for it to be included, so I'll skip it for now.
As for the CA root certs, as part of this PR I'll build that into librdkafka when on Windows and no ssl.ca.location has been specified.",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2309,2019-05-02T20:14:55Z,2019-06-03T21:16:10Z,2020-05-08T08:58:59Z,MERGED,True,3716,921,40,https://github.com/edenhill,Continuing on PR #1882: SSL cert verification and in-memory keys,14,[],https://github.com/edenhill/librdkafka/pull/2309,https://github.com/noahdav,14,https://github.com/edenhill/librdkafka/pull/2309#issuecomment-491101518,"This PR is a continuation on @noahdav's work on PR #1882.
Summary of changes compared to #1882:

The retrieve_cb() has been removed in favour of using standard configuration properties. The reasoning behind this s simplification, reduced code size, cross-language maintenance (some high-level languages, such as Go, can't use callbacks in a good way), and memory ownership issues (having the callback return cert memory to librdkafka in an effective and allocator-agnostic way is tricky).
Refactored the code and made it more in line with the librdkafka style guide.
Values of security-sensitive configuration properties are now cleared from memory when the value is no longer used, or on rd_kafka_t destruction. This is used for ssl.certificate.pem, ssl.key.pem and ssl.key.password.
The ssl_cert_verify_cb was changed to provide more information to the callback, including the full X509_STORE_CTX.
Add integration tests.

To be done:

Rework the Windows SSL cert store example to its own file.",@edenhill The API looks good to me however it is failing to parse the private key.  PKCS12_parse is failing.  I am trying to debug what is going on here to understand why this is now failing.  I will let you know what I find out.,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2309,2019-05-02T20:14:55Z,2019-06-03T21:16:10Z,2020-05-08T08:58:59Z,MERGED,True,3716,921,40,https://github.com/edenhill,Continuing on PR #1882: SSL cert verification and in-memory keys,14,[],https://github.com/edenhill/librdkafka/pull/2309,https://github.com/edenhill,15,https://github.com/edenhill/librdkafka/pull/2309#issuecomment-491265709,"This PR is a continuation on @noahdav's work on PR #1882.
Summary of changes compared to #1882:

The retrieve_cb() has been removed in favour of using standard configuration properties. The reasoning behind this s simplification, reduced code size, cross-language maintenance (some high-level languages, such as Go, can't use callbacks in a good way), and memory ownership issues (having the callback return cert memory to librdkafka in an effective and allocator-agnostic way is tricky).
Refactored the code and made it more in line with the librdkafka style guide.
Values of security-sensitive configuration properties are now cleared from memory when the value is no longer used, or on rd_kafka_t destruction. This is used for ssl.certificate.pem, ssl.key.pem and ssl.key.password.
The ssl_cert_verify_cb was changed to provide more information to the callback, including the full X509_STORE_CTX.
Add integration tests.

To be done:

Rework the Windows SSL cert store example to its own file.","@LighthouseJ

One notable difference and reason for the hack is that in circumstance where Windows will not let you have the private key because it's stored on a hardware device (so not read from a PEM file), Windows gives you a realistic OpenSSL-looking thing but you cannot serialize it to a BIO for example or do parsing on it, ..

I think we should provide a method to retrieve the SSL_CTX object from a librdkafka rd_kafka_t handle to allow deeper application integration with OpenSSL. That'd work for your use-case, right?",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2309,2019-05-02T20:14:55Z,2019-06-03T21:16:10Z,2020-05-08T08:58:59Z,MERGED,True,3716,921,40,https://github.com/edenhill,Continuing on PR #1882: SSL cert verification and in-memory keys,14,[],https://github.com/edenhill/librdkafka/pull/2309,https://github.com/edenhill,16,https://github.com/edenhill/librdkafka/pull/2309#issuecomment-491266391,"This PR is a continuation on @noahdav's work on PR #1882.
Summary of changes compared to #1882:

The retrieve_cb() has been removed in favour of using standard configuration properties. The reasoning behind this s simplification, reduced code size, cross-language maintenance (some high-level languages, such as Go, can't use callbacks in a good way), and memory ownership issues (having the callback return cert memory to librdkafka in an effective and allocator-agnostic way is tricky).
Refactored the code and made it more in line with the librdkafka style guide.
Values of security-sensitive configuration properties are now cleared from memory when the value is no longer used, or on rd_kafka_t destruction. This is used for ssl.certificate.pem, ssl.key.pem and ssl.key.password.
The ssl_cert_verify_cb was changed to provide more information to the callback, including the full X509_STORE_CTX.
Add integration tests.

To be done:

Rework the Windows SSL cert store example to its own file.","@noahdav I (force) pushed a couple of changes (including CA certs are now automatically read from the Root store), see examples/win_ssl_cert_store.cpp which is your original example extracted to its own file and reworked for the new API.
I can't get it to parse the private key from a PKCS#12 store, the OpenSSL error stack points to this line:
https://github.com/openssl/openssl/blob/OpenSSL_1_0_2-stable/crypto/pkcs12/p12_mutl.c#L90
But maybe I'm just using a lousy key (that I found in my key store..:) )",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2309,2019-05-02T20:14:55Z,2019-06-03T21:16:10Z,2020-05-08T08:58:59Z,MERGED,True,3716,921,40,https://github.com/edenhill,Continuing on PR #1882: SSL cert verification and in-memory keys,14,[],https://github.com/edenhill/librdkafka/pull/2309,https://github.com/edenhill,17,https://github.com/edenhill/librdkafka/pull/2309#issuecomment-491266533,"This PR is a continuation on @noahdav's work on PR #1882.
Summary of changes compared to #1882:

The retrieve_cb() has been removed in favour of using standard configuration properties. The reasoning behind this s simplification, reduced code size, cross-language maintenance (some high-level languages, such as Go, can't use callbacks in a good way), and memory ownership issues (having the callback return cert memory to librdkafka in an effective and allocator-agnostic way is tricky).
Refactored the code and made it more in line with the librdkafka style guide.
Values of security-sensitive configuration properties are now cleared from memory when the value is no longer used, or on rd_kafka_t destruction. This is used for ssl.certificate.pem, ssl.key.pem and ssl.key.password.
The ssl_cert_verify_cb was changed to provide more information to the callback, including the full X509_STORE_CTX.
Add integration tests.

To be done:

Rework the Windows SSL cert store example to its own file.","Extra bonus, added ssl.endpoint.identification.algorithm=https",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2309,2019-05-02T20:14:55Z,2019-06-03T21:16:10Z,2020-05-08T08:58:59Z,MERGED,True,3716,921,40,https://github.com/edenhill,Continuing on PR #1882: SSL cert verification and in-memory keys,14,[],https://github.com/edenhill/librdkafka/pull/2309,https://github.com/noahdav,18,https://github.com/edenhill/librdkafka/pull/2309#issuecomment-491283929,"This PR is a continuation on @noahdav's work on PR #1882.
Summary of changes compared to #1882:

The retrieve_cb() has been removed in favour of using standard configuration properties. The reasoning behind this s simplification, reduced code size, cross-language maintenance (some high-level languages, such as Go, can't use callbacks in a good way), and memory ownership issues (having the callback return cert memory to librdkafka in an effective and allocator-agnostic way is tricky).
Refactored the code and made it more in line with the librdkafka style guide.
Values of security-sensitive configuration properties are now cleared from memory when the value is no longer used, or on rd_kafka_t destruction. This is used for ssl.certificate.pem, ssl.key.pem and ssl.key.password.
The ssl_cert_verify_cb was changed to provide more information to the callback, including the full X509_STORE_CTX.
Add integration tests.

To be done:

Rework the Windows SSL cert store example to its own file.",Hmm. That is the same error I am seeing in OpenSSL as well. I am not sure what the difference is here. My original code worked with the callbacks but for some reason this is not working. I do not see any differences in how you are parsing the pkcs12 vs what I originally did. I will dig into it more today to see if I can find anything out.,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2309,2019-05-02T20:14:55Z,2019-06-03T21:16:10Z,2020-05-08T08:58:59Z,MERGED,True,3716,921,40,https://github.com/edenhill,Continuing on PR #1882: SSL cert verification and in-memory keys,14,[],https://github.com/edenhill/librdkafka/pull/2309,https://github.com/edenhill,19,https://github.com/edenhill/librdkafka/pull/2309#issuecomment-491295944,"This PR is a continuation on @noahdav's work on PR #1882.
Summary of changes compared to #1882:

The retrieve_cb() has been removed in favour of using standard configuration properties. The reasoning behind this s simplification, reduced code size, cross-language maintenance (some high-level languages, such as Go, can't use callbacks in a good way), and memory ownership issues (having the callback return cert memory to librdkafka in an effective and allocator-agnostic way is tricky).
Refactored the code and made it more in line with the librdkafka style guide.
Values of security-sensitive configuration properties are now cleared from memory when the value is no longer used, or on rd_kafka_t destruction. This is used for ssl.certificate.pem, ssl.key.pem and ssl.key.password.
The ssl_cert_verify_cb was changed to provide more information to the callback, including the full X509_STORE_CTX.
Add integration tests.

To be done:

Rework the Windows SSL cert store example to its own file.",@noahdav Thank you!,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2309,2019-05-02T20:14:55Z,2019-06-03T21:16:10Z,2020-05-08T08:58:59Z,MERGED,True,3716,921,40,https://github.com/edenhill,Continuing on PR #1882: SSL cert verification and in-memory keys,14,[],https://github.com/edenhill/librdkafka/pull/2309,https://github.com/noahdav,20,https://github.com/edenhill/librdkafka/pull/2309#issuecomment-491425151,"This PR is a continuation on @noahdav's work on PR #1882.
Summary of changes compared to #1882:

The retrieve_cb() has been removed in favour of using standard configuration properties. The reasoning behind this s simplification, reduced code size, cross-language maintenance (some high-level languages, such as Go, can't use callbacks in a good way), and memory ownership issues (having the callback return cert memory to librdkafka in an effective and allocator-agnostic way is tricky).
Refactored the code and made it more in line with the librdkafka style guide.
Values of security-sensitive configuration properties are now cleared from memory when the value is no longer used, or on rd_kafka_t destruction. This is used for ssl.certificate.pem, ssl.key.pem and ssl.key.password.
The ssl_cert_verify_cb was changed to provide more information to the callback, including the full X509_STORE_CTX.
Add integration tests.

To be done:

Rework the Windows SSL cert store example to its own file.","@edenhill I finally figured out the issue here.  It looks like OpenSSL is not initialized yet since rd_kafka_ssl_init is not yet called.  Therefore we did not call SSL_library_init() so when we are trying to parse the pkcs12 it fails to find the digest algorithm.  When I originally made this change using the callbacks they were only called after OpenSSL was initialized so we would not see this issue.  I would assume since we are not properly initializing OpenSSL that many other thing can fail unexpectedly as well.
I will spend some time to familiarize myself with when rd_kafka_ssl_init gets called to see if there is a way to initialize OpenSSL prior to calling rd_kafka_conf_set_ssl_cert or for that matter any of the OpenSSL config methods.  Please let me know if you have thoughts to how this can be done.",True,{'THUMBS_UP': ['https://github.com/raj1003']}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2309,2019-05-02T20:14:55Z,2019-06-03T21:16:10Z,2020-05-08T08:58:59Z,MERGED,True,3716,921,40,https://github.com/edenhill,Continuing on PR #1882: SSL cert verification and in-memory keys,14,[],https://github.com/edenhill/librdkafka/pull/2309,https://github.com/edenhill,21,https://github.com/edenhill/librdkafka/pull/2309#issuecomment-491434558,"This PR is a continuation on @noahdav's work on PR #1882.
Summary of changes compared to #1882:

The retrieve_cb() has been removed in favour of using standard configuration properties. The reasoning behind this s simplification, reduced code size, cross-language maintenance (some high-level languages, such as Go, can't use callbacks in a good way), and memory ownership issues (having the callback return cert memory to librdkafka in an effective and allocator-agnostic way is tricky).
Refactored the code and made it more in line with the librdkafka style guide.
Values of security-sensitive configuration properties are now cleared from memory when the value is no longer used, or on rd_kafka_t destruction. This is used for ssl.certificate.pem, ssl.key.pem and ssl.key.password.
The ssl_cert_verify_cb was changed to provide more information to the callback, including the full X509_STORE_CTX.
Add integration tests.

To be done:

Rework the Windows SSL cert store example to its own file.","@noahdav Aah, good find! Makes total sense. I'll push a fix for this soon.",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2309,2019-05-02T20:14:55Z,2019-06-03T21:16:10Z,2020-05-08T08:58:59Z,MERGED,True,3716,921,40,https://github.com/edenhill,Continuing on PR #1882: SSL cert verification and in-memory keys,14,[],https://github.com/edenhill/librdkafka/pull/2309,https://github.com/noahdav,22,https://github.com/edenhill/librdkafka/pull/2309#issuecomment-491440119,"This PR is a continuation on @noahdav's work on PR #1882.
Summary of changes compared to #1882:

The retrieve_cb() has been removed in favour of using standard configuration properties. The reasoning behind this s simplification, reduced code size, cross-language maintenance (some high-level languages, such as Go, can't use callbacks in a good way), and memory ownership issues (having the callback return cert memory to librdkafka in an effective and allocator-agnostic way is tricky).
Refactored the code and made it more in line with the librdkafka style guide.
Values of security-sensitive configuration properties are now cleared from memory when the value is no longer used, or on rd_kafka_t destruction. This is used for ssl.certificate.pem, ssl.key.pem and ssl.key.password.
The ssl_cert_verify_cb was changed to provide more information to the callback, including the full X509_STORE_CTX.
Add integration tests.

To be done:

Rework the Windows SSL cert store example to its own file.","After working around this initialization issue I am trying to test my verification callback.  So far I see a couple of issues.  First off in my test I am using a self signed cert.  So the first call to the rd_kafka_transport_ssl_cert_verify_cb function will call my callback but it does not have the context to understand why preverify_ok is 0.  In this case it is due to X509_V_ERR_SELF_SIGNED_CERT_IN_CHAIN.
Once I masked the X509_V_ERR_SELF_SIGNED_CERT_IN_CHAIN in the callback and return 1 in rd_kafka_transport_ssl_verify is SSL_get_verify_result is still returning the original error (X509_V_ERR_SELF_SIGNED_CERT_IN_CHAIN) so the verification still fails.  Do you know how to have the callback mask the error in this case?
I also think we should change the callback to pass a function table so that the callback can get information about the cert/validation process.  We can always extend it as needed but as long as the x509_ctx is passed to the callback they can parse it as currently implemented.  I assume each of the function pointers in the table would require the x509_ctx to be passed in anyhow so this would be needed.  I think this approach is better than offloading this to the callback as it make the callback not coupled to OpenSSL (librdkafka version dependent).  In our case we have no OpenSSL as we only use windows CryptAPI.  I do not think it is reasonable to ask implementer to take on an OpenSSL dependency in their application code if not necessary.  Also if for some reason we do not provide everything needed in rd_kafka_cert_verify_ft the context is still provided to the callback.
struct rd_kafka_cert_verify_ft {
error_code;
depth;
verification_result;
context;
...
}",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2309,2019-05-02T20:14:55Z,2019-06-03T21:16:10Z,2020-05-08T08:58:59Z,MERGED,True,3716,921,40,https://github.com/edenhill,Continuing on PR #1882: SSL cert verification and in-memory keys,14,[],https://github.com/edenhill/librdkafka/pull/2309,https://github.com/noahdav,23,https://github.com/edenhill/librdkafka/pull/2309#issuecomment-491468566,"This PR is a continuation on @noahdav's work on PR #1882.
Summary of changes compared to #1882:

The retrieve_cb() has been removed in favour of using standard configuration properties. The reasoning behind this s simplification, reduced code size, cross-language maintenance (some high-level languages, such as Go, can't use callbacks in a good way), and memory ownership issues (having the callback return cert memory to librdkafka in an effective and allocator-agnostic way is tricky).
Refactored the code and made it more in line with the librdkafka style guide.
Values of security-sensitive configuration properties are now cleared from memory when the value is no longer used, or on rd_kafka_t destruction. This is used for ssl.certificate.pem, ssl.key.pem and ssl.key.password.
The ssl_cert_verify_cb was changed to provide more information to the callback, including the full X509_STORE_CTX.
Add integration tests.

To be done:

Rework the Windows SSL cert store example to its own file.",@edenhill Thanks for the push to init OpenSSL.  That did the trick.  Back to my other issue.  We need a way to handle additional checks.  Since in this case the X509_V_ERR_SELF_SIGNED_CERT_IN_CHAIN error is failing for me I need a good way to call X509_STORE_CTX_set_error to success in this case.  I think this too could be added to the rd_kafka_cert_verify_ft that I mentioned above.  This would allow callbacks to be abstracted from OpenSSL.  Also the callback needs to be able to set X509_STORE_CTX_set_error in the case of a failure too.,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2309,2019-05-02T20:14:55Z,2019-06-03T21:16:10Z,2020-05-08T08:58:59Z,MERGED,True,3716,921,40,https://github.com/edenhill,Continuing on PR #1882: SSL cert verification and in-memory keys,14,[],https://github.com/edenhill/librdkafka/pull/2309,https://github.com/noahdav,24,https://github.com/edenhill/librdkafka/pull/2309#issuecomment-491569252,"This PR is a continuation on @noahdav's work on PR #1882.
Summary of changes compared to #1882:

The retrieve_cb() has been removed in favour of using standard configuration properties. The reasoning behind this s simplification, reduced code size, cross-language maintenance (some high-level languages, such as Go, can't use callbacks in a good way), and memory ownership issues (having the callback return cert memory to librdkafka in an effective and allocator-agnostic way is tricky).
Refactored the code and made it more in line with the librdkafka style guide.
Values of security-sensitive configuration properties are now cleared from memory when the value is no longer used, or on rd_kafka_t destruction. This is used for ssl.certificate.pem, ssl.key.pem and ssl.key.password.
The ssl_cert_verify_cb was changed to provide more information to the callback, including the full X509_STORE_CTX.
Add integration tests.

To be done:

Rework the Windows SSL cert store example to its own file.",@edenhill I went ahead and implemented the callbacks I was talking about.  It needs documentation comments and style fixes per the style guidelines.  However I wanted to get your thoughts on this type of change.  Please take a look at noahdav:ssl_cert_cb to see the changes and you can incorporate them in this PR.  I think for the callback to have the context to truly provide this validation check it needs this info at a minimum but will probably need additional callbacks as we discover them.  Please let me know your thoughts?,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2309,2019-05-02T20:14:55Z,2019-06-03T21:16:10Z,2020-05-08T08:58:59Z,MERGED,True,3716,921,40,https://github.com/edenhill,Continuing on PR #1882: SSL cert verification and in-memory keys,14,[],https://github.com/edenhill/librdkafka/pull/2309,https://github.com/LighthouseJ,25,https://github.com/edenhill/librdkafka/pull/2309#issuecomment-491864228,"This PR is a continuation on @noahdav's work on PR #1882.
Summary of changes compared to #1882:

The retrieve_cb() has been removed in favour of using standard configuration properties. The reasoning behind this s simplification, reduced code size, cross-language maintenance (some high-level languages, such as Go, can't use callbacks in a good way), and memory ownership issues (having the callback return cert memory to librdkafka in an effective and allocator-agnostic way is tricky).
Refactored the code and made it more in line with the librdkafka style guide.
Values of security-sensitive configuration properties are now cleared from memory when the value is no longer used, or on rd_kafka_t destruction. This is used for ssl.certificate.pem, ssl.key.pem and ssl.key.password.
The ssl_cert_verify_cb was changed to provide more information to the callback, including the full X509_STORE_CTX.
Add integration tests.

To be done:

Rework the Windows SSL cert store example to its own file.","@LighthouseJ

One notable difference and reason for the hack is that in circumstance where Windows will not let you have the private key because it's stored on a hardware device (so not read from a PEM file), Windows gives you a realistic OpenSSL-looking thing but you cannot serialize it to a BIO for example or do parsing on it, ..

I think we should provide a method to retrieve the SSL_CTX object from a librdkafka rd_kafka_t handle to allow deeper application integration with OpenSSL. That'd work for your use-case, right?

Not sure, but you might have a better idea because I have not had a chance to dig into this PR's worth of evolved work.
So the initial attempt by @noahdav was designed to put the entire cert into the buffer that's returning, and then use a sequence of calls to parse out and use certificates:
BIO_new_mem_buf (put bytes into BIO)
d2i_PKCS12_bio (convert BIO to PKCS12)
PKCS12_parse (extract private key)
and finally:
SSL_CTX_use_PrivateKey (use the private key)
to read and use the key in the SSL context.
This wasn't suitable to my use case because I could not possess the bytes to the private key, but got a pointer to them in memory.
I used CAPI along these lines:
    ENGINE *    ptr_engine;
    X509    *   ptr_client_cert;
    EVP_PKEY *  ptr_private_key;
...
    ENGINE_load_builtin_engines();
    ENGINE_load_dynamic();
    ERR_clear_error();
    ptr_engine = ENGINE_by_id(""dynamic"");
...
    ENGINE_ctrl_cmd_string(ptr_engine, ""SO_PATH"", ""./capi.dll"", 0);
    ENGINE_ctrl_cmd_string(ptr_engine, ""ID"", ""capi"", 0);
    ENGINE_ctrl_cmd_string(ptr_engine, ""LIST_ADD"", ""1"", 0);
    ENGINE_ctrl_cmd_string(ptr_engine, ""LOAD"", NULL, 0);

    ENGINE_init(ptr_engine);
...

ssize_t 
GetCAPICertificatesCb::ssl_cert_retrieve_cb(Type type, char **buffer, std::string &errstr)
{
    ...
    // only want to do this once, or a limited time if the load fails or needs to be re-read
    int load_result =
        ENGINE_load_ssl_client_cert(ptr_engine, NULL, NULL,
                &ptr_client_cert, &ptr_private_key,
                NULL, NULL, NULL);
    ...
    switch (type)
    {
    case CERTIFICATE_PRIVATE_KEY:
    {

        if (ptr_private_key)
        {

            // copies the pointer to the private key to the first character
            *buffer = (char *)ptr_private_key;
            return 1;

        break;
    } // case CERTIFICATE_PRIVATE_KEY:
    ...
    } // switch (type)

See the relevant call:
https://github.com/openssl/openssl/blob/master/engines/e_capi.c#L140
https://github.com/openssl/openssl/blob/master/engines/e_capi.c#L1691
I added a patch to @noahdav's original PR to look for buffer size of exactly 1 and then run this code:
if (len == 1) // *buffer points to a prepared EVP_PKEY pointer loses efficacy when serialized to a buffer
{
    rd_kafka_dbg(rk, SECURITY, ""SSL"",
        ""Using an opaque EVP_PKEY pointer as the private key"");
    EVP_PKEY * pkey = (EVP_PKEY *)buffer;

    r = SSL_CTX_use_PrivateKey(ctx, pkey);

    if (r != 1) {
        return -1;
    }

    rd_kafka_dbg(rk, SECURITY, ""SSL"",
        ""Checking client private key"");

    r = SSL_CTX_check_private_key(ctx);

    if (r != 1) {
        return -1;
    }
}
else
// do the original use case...

I just needed to bind the EVP_PKEY (which is prepared by Windows) to the SSL_CTX, in other words, only the last step above.
I realize that you're offering to give the SSL_CTX to users to use, which would smooth this out, but this is a little bit of what I did to skirt around the interface a little.  I did not look into this PR to see if this, or maybe making an EVP_PKEY an optional return, is a better choice.",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2309,2019-05-02T20:14:55Z,2019-06-03T21:16:10Z,2020-05-08T08:58:59Z,MERGED,True,3716,921,40,https://github.com/edenhill,Continuing on PR #1882: SSL cert verification and in-memory keys,14,[],https://github.com/edenhill/librdkafka/pull/2309,https://github.com/edenhill,26,https://github.com/edenhill/librdkafka/pull/2309#issuecomment-492616544,"This PR is a continuation on @noahdav's work on PR #1882.
Summary of changes compared to #1882:

The retrieve_cb() has been removed in favour of using standard configuration properties. The reasoning behind this s simplification, reduced code size, cross-language maintenance (some high-level languages, such as Go, can't use callbacks in a good way), and memory ownership issues (having the callback return cert memory to librdkafka in an effective and allocator-agnostic way is tricky).
Refactored the code and made it more in line with the librdkafka style guide.
Values of security-sensitive configuration properties are now cleared from memory when the value is no longer used, or on rd_kafka_t destruction. This is used for ssl.certificate.pem, ssl.key.pem and ssl.key.password.
The ssl_cert_verify_cb was changed to provide more information to the callback, including the full X509_STORE_CTX.
Add integration tests.

To be done:

Rework the Windows SSL cert store example to its own file.","@noahdav
I'm not sure it is a great idea to override OpenSSL's verification, I was under the impression that our verify callback was for additional verification.
For the self signed certs itself, you could set the cert as the ssl.ca.location to circumvent that, right?
As for the function table, I see what you are getting at, and the only thing that seems to be missing from today's signature is the per-depth error, the depth itself and the x509 are already provided.
Adding it as a function table in a public structure is unfortunately not extensible, since any change to that struct would break the ABI compat guarantees librdkafka provides.
We would have to make it an opaque/privately-defined object type with a lot of boiler plate for a feature that will not be that widely used. I'd like to avoid that if possible.
I feel that it might be easier to add the per-depth error code as an argument to the verify callback (possibly replacing the preverify_ok? since it makes it redundant).
@LighthouseJ
The interface you are using, retrieve_cb, has been removed in favour of conf setters.
While it would be easy to provide a CERT_ENC_EVP_PKEY type  for conf->set_ssl_cert() it would not really work in practice since OpenSSL does not provide any ABI guarantees: if the application uses a slightly different version, or possibly even build, of OpenSSL, the passed EVP_PKEY may not be compatible with the OpenSSL version librdkafka is using.
So I suggest you convert the PKEY to PEM or PKCS#12 and pass that set_ssl_cert()",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2309,2019-05-02T20:14:55Z,2019-06-03T21:16:10Z,2020-05-08T08:58:59Z,MERGED,True,3716,921,40,https://github.com/edenhill,Continuing on PR #1882: SSL cert verification and in-memory keys,14,[],https://github.com/edenhill/librdkafka/pull/2309,https://github.com/LighthouseJ,27,https://github.com/edenhill/librdkafka/pull/2309#issuecomment-492635854,"This PR is a continuation on @noahdav's work on PR #1882.
Summary of changes compared to #1882:

The retrieve_cb() has been removed in favour of using standard configuration properties. The reasoning behind this s simplification, reduced code size, cross-language maintenance (some high-level languages, such as Go, can't use callbacks in a good way), and memory ownership issues (having the callback return cert memory to librdkafka in an effective and allocator-agnostic way is tricky).
Refactored the code and made it more in line with the librdkafka style guide.
Values of security-sensitive configuration properties are now cleared from memory when the value is no longer used, or on rd_kafka_t destruction. This is used for ssl.certificate.pem, ssl.key.pem and ssl.key.password.
The ssl_cert_verify_cb was changed to provide more information to the callback, including the full X509_STORE_CTX.
Add integration tests.

To be done:

Rework the Windows SSL cert store example to its own file.","@LighthouseJ
The interface you are using, retrieve_cb, has been removed in favour of conf setters.

Good to know!

While it would be easy to provide a CERT_ENC_EVP_PKEY type for conf->set_ssl_cert() it would not really work in practice since OpenSSL does not provide any ABI guarantees: if the application uses a slightly different version, or possibly even build, of OpenSSL, the passed EVP_PKEY may not be compatible with the OpenSSL version librdkafka is using.

While I agree with this statement in general, I think for the purposes that librdkafka uses OpenSSL, it is quite stable, at least as far as librdkafka uses OpenSSL.
Part of the stability that CAPI inherits comes from the OpenSSL dynamic engine which is a C-function pointer system that keeps the OpenSSL engine interface stable.
I'm able to build librdkafka w/ OpenSSL 1.1.0g (previously 1.0.1 line as well) and use it without issue, so I believe 1.1.0 branch works fine w/ librdkafka from a librdkafka pre-1.0.0 release tag.
Could you please include the CERT_ENC_EVP_PKEY as a valid type of cert to return anyway, and I can try and use it to see if it fits my purpose?

So I suggest you convert the PKEY to PEM or PKCS#12 and pass that set_ssl_cert()

I did look into this before I made the ugly patch. I think when I found the right sequence of commands, I got no errors, but I got a zero-length buffer where the private key was supposed to be, so I thought that either Windows or OpenSSL was not allowing me to extract a private key as a method of protecting the integrity of the hardware device.",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2309,2019-05-02T20:14:55Z,2019-06-03T21:16:10Z,2020-05-08T08:58:59Z,MERGED,True,3716,921,40,https://github.com/edenhill,Continuing on PR #1882: SSL cert verification and in-memory keys,14,[],https://github.com/edenhill/librdkafka/pull/2309,https://github.com/edenhill,28,https://github.com/edenhill/librdkafka/pull/2309#issuecomment-492676621,"This PR is a continuation on @noahdav's work on PR #1882.
Summary of changes compared to #1882:

The retrieve_cb() has been removed in favour of using standard configuration properties. The reasoning behind this s simplification, reduced code size, cross-language maintenance (some high-level languages, such as Go, can't use callbacks in a good way), and memory ownership issues (having the callback return cert memory to librdkafka in an effective and allocator-agnostic way is tricky).
Refactored the code and made it more in line with the librdkafka style guide.
Values of security-sensitive configuration properties are now cleared from memory when the value is no longer used, or on rd_kafka_t destruction. This is used for ssl.certificate.pem, ssl.key.pem and ssl.key.password.
The ssl_cert_verify_cb was changed to provide more information to the callback, including the full X509_STORE_CTX.
Add integration tests.

To be done:

Rework the Windows SSL cert store example to its own file.","I think for the purposes that librdkafka uses OpenSSL, it is quite stable, at least as far as librdkafka uses OpenSSL.

As long as the OpenSSL project does not guarantee ABI stability we can't, and shouldn't, make any such assumptions. This will lead to bugs that are extremely difficult to track down.
Convert the ENV_PKEY to PEM or PKCS should be fairly straight forward with libssl.",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2309,2019-05-02T20:14:55Z,2019-06-03T21:16:10Z,2020-05-08T08:58:59Z,MERGED,True,3716,921,40,https://github.com/edenhill,Continuing on PR #1882: SSL cert verification and in-memory keys,14,[],https://github.com/edenhill/librdkafka/pull/2309,https://github.com/noahdav,29,https://github.com/edenhill/librdkafka/pull/2309#issuecomment-492688454,"This PR is a continuation on @noahdav's work on PR #1882.
Summary of changes compared to #1882:

The retrieve_cb() has been removed in favour of using standard configuration properties. The reasoning behind this s simplification, reduced code size, cross-language maintenance (some high-level languages, such as Go, can't use callbacks in a good way), and memory ownership issues (having the callback return cert memory to librdkafka in an effective and allocator-agnostic way is tricky).
Refactored the code and made it more in line with the librdkafka style guide.
Values of security-sensitive configuration properties are now cleared from memory when the value is no longer used, or on rd_kafka_t destruction. This is used for ssl.certificate.pem, ssl.key.pem and ssl.key.password.
The ssl_cert_verify_cb was changed to provide more information to the callback, including the full X509_STORE_CTX.
Add integration tests.

To be done:

Rework the Windows SSL cert store example to its own file.","@edenhill The problem with using the x509 that the callback passes today is that we are now requiring an application to take a hard dependency on OpenSSL.  Many windows applications do are using the Win32 Crypt API(s) instead.  Actually to make matters worse it is the same version that librdkafka uses so any application using a different version of OpenSSL is also unable to use this.  I think we really should not leak OpenSSL constructs to the callbacks in users applications.  Using ssl.ca.location will not work with certs in the windows store.  Most of our certs are trusted but even when they are self signed in some cases there is additional validation which is performed.  The application also needs the ability to set any error context when the validation fails.  This is currently missing from the API without using OpenSSL in the callback.
Why is it breaking to add a function to the end of the function table.  A struct can be extended as long as you add it to the end.  Also we can #ifdef depending on the librdkafka version if necessary.  If this approach will not work then can we add a control function to perform these operations?
int ssl_verify_control(void* context, int32_t op, void** buffer, size_t bufferSize) {
switch(op) {
case GET_CURRENT_ERROR:
...
break;
case SET_CURRENT_ERROR:
...
break;

case GET_DEPTH:
 ...
break;

...

}
return 1;
}",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2309,2019-05-02T20:14:55Z,2019-06-03T21:16:10Z,2020-05-08T08:58:59Z,MERGED,True,3716,921,40,https://github.com/edenhill,Continuing on PR #1882: SSL cert verification and in-memory keys,14,[],https://github.com/edenhill/librdkafka/pull/2309,https://github.com/edenhill,30,https://github.com/edenhill/librdkafka/pull/2309#issuecomment-492693969,"This PR is a continuation on @noahdav's work on PR #1882.
Summary of changes compared to #1882:

The retrieve_cb() has been removed in favour of using standard configuration properties. The reasoning behind this s simplification, reduced code size, cross-language maintenance (some high-level languages, such as Go, can't use callbacks in a good way), and memory ownership issues (having the callback return cert memory to librdkafka in an effective and allocator-agnostic way is tricky).
Refactored the code and made it more in line with the librdkafka style guide.
Values of security-sensitive configuration properties are now cleared from memory when the value is no longer used, or on rd_kafka_t destruction. This is used for ssl.certificate.pem, ssl.key.pem and ssl.key.password.
The ssl_cert_verify_cb was changed to provide more information to the callback, including the full X509_STORE_CTX.
Add integration tests.

To be done:

Rework the Windows SSL cert store example to its own file.","@noahdav Yeah, I agree, I'll take out the x509 parameter from the verify callback.
As for public structs, an application needs to be recompiled if the struct layout changes, thus breaking the ABI compatibility.
And I don't think a function table is really necessary, seeing how the only missing information is the per-cert error code which we can add to the verify_cb in some form.

The application also needs the ability to set any error context when the validation fails.

Why does the certificate verification need to be more fine grained than success or failure?

Using ssl.ca.location will not work with certs in the windows store.

Right, but if we allowed CA certs to be specified with set_ssl_cert  it would work, right?
Also, if the CA cert is in the Root store librdkafka will pick it up automatically now.",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2309,2019-05-02T20:14:55Z,2019-06-03T21:16:10Z,2020-05-08T08:58:59Z,MERGED,True,3716,921,40,https://github.com/edenhill,Continuing on PR #1882: SSL cert verification and in-memory keys,14,[],https://github.com/edenhill/librdkafka/pull/2309,https://github.com/noahdav,31,https://github.com/edenhill/librdkafka/pull/2309#issuecomment-492702763,"This PR is a continuation on @noahdav's work on PR #1882.
Summary of changes compared to #1882:

The retrieve_cb() has been removed in favour of using standard configuration properties. The reasoning behind this s simplification, reduced code size, cross-language maintenance (some high-level languages, such as Go, can't use callbacks in a good way), and memory ownership issues (having the callback return cert memory to librdkafka in an effective and allocator-agnostic way is tricky).
Refactored the code and made it more in line with the librdkafka style guide.
Values of security-sensitive configuration properties are now cleared from memory when the value is no longer used, or on rd_kafka_t destruction. This is used for ssl.certificate.pem, ssl.key.pem and ssl.key.password.
The ssl_cert_verify_cb was changed to provide more information to the callback, including the full X509_STORE_CTX.
Add integration tests.

To be done:

Rework the Windows SSL cert store example to its own file.","@edenhill  After the callback the call to SSL_get_verify_result is what librdkafka is using to determine success/failure.  Therefore we need to set the error so that this function will propagate the failure form the callback.
The callback needs the ability to retrieve any error and depth so that it can perform additional checks based upon the error which occurred.
Adding the root CA may help in some of the cases but in many cases there is additional cert checks we want to apply.  This still needs to be done somehow.  I think it is simplest to delegate this to the callback to determine the logic of what success/failure is.",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2309,2019-05-02T20:14:55Z,2019-06-03T21:16:10Z,2020-05-08T08:58:59Z,MERGED,True,3716,921,40,https://github.com/edenhill,Continuing on PR #1882: SSL cert verification and in-memory keys,14,[],https://github.com/edenhill/librdkafka/pull/2309,https://github.com/edenhill,32,https://github.com/edenhill/librdkafka/pull/2309#issuecomment-492705574,"This PR is a continuation on @noahdav's work on PR #1882.
Summary of changes compared to #1882:

The retrieve_cb() has been removed in favour of using standard configuration properties. The reasoning behind this s simplification, reduced code size, cross-language maintenance (some high-level languages, such as Go, can't use callbacks in a good way), and memory ownership issues (having the callback return cert memory to librdkafka in an effective and allocator-agnostic way is tricky).
Refactored the code and made it more in line with the librdkafka style guide.
Values of security-sensitive configuration properties are now cleared from memory when the value is no longer used, or on rd_kafka_t destruction. This is used for ssl.certificate.pem, ssl.key.pem and ssl.key.password.
The ssl_cert_verify_cb was changed to provide more information to the callback, including the full X509_STORE_CTX.
Add integration tests.

To be done:

Rework the Windows SSL cert store example to its own file.","The callback needs the ability to retrieve any error and depth so that it can perform additional checks based upon the error which occurred.

Since the callback is called once for each depth, does it really need to be able to extract errors for other depths than it is currently being called for?
As for error reporting, the user shall set an error string if the callback returns false, and that error string will be logged by librdkafka.",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2309,2019-05-02T20:14:55Z,2019-06-03T21:16:10Z,2020-05-08T08:58:59Z,MERGED,True,3716,921,40,https://github.com/edenhill,Continuing on PR #1882: SSL cert verification and in-memory keys,14,[],https://github.com/edenhill/librdkafka/pull/2309,https://github.com/noahdav,33,https://github.com/edenhill/librdkafka/pull/2309#issuecomment-492709273,"This PR is a continuation on @noahdav's work on PR #1882.
Summary of changes compared to #1882:

The retrieve_cb() has been removed in favour of using standard configuration properties. The reasoning behind this s simplification, reduced code size, cross-language maintenance (some high-level languages, such as Go, can't use callbacks in a good way), and memory ownership issues (having the callback return cert memory to librdkafka in an effective and allocator-agnostic way is tricky).
Refactored the code and made it more in line with the librdkafka style guide.
Values of security-sensitive configuration properties are now cleared from memory when the value is no longer used, or on rd_kafka_t destruction. This is used for ssl.certificate.pem, ssl.key.pem and ssl.key.password.
The ssl_cert_verify_cb was changed to provide more information to the callback, including the full X509_STORE_CTX.
Add integration tests.

To be done:

Rework the Windows SSL cert store example to its own file.",@edenhill I agree.  The callback only needs the current depth/error.  As for error reporting to OpenSSL I think we should have a way to set the OpenSSL error.  What is you objection to allowing the callback to set this error?,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2309,2019-05-02T20:14:55Z,2019-06-03T21:16:10Z,2020-05-08T08:58:59Z,MERGED,True,3716,921,40,https://github.com/edenhill,Continuing on PR #1882: SSL cert verification and in-memory keys,14,[],https://github.com/edenhill/librdkafka/pull/2309,https://github.com/edenhill,34,https://github.com/edenhill/librdkafka/pull/2309#issuecomment-492711260,"This PR is a continuation on @noahdav's work on PR #1882.
Summary of changes compared to #1882:

The retrieve_cb() has been removed in favour of using standard configuration properties. The reasoning behind this s simplification, reduced code size, cross-language maintenance (some high-level languages, such as Go, can't use callbacks in a good way), and memory ownership issues (having the callback return cert memory to librdkafka in an effective and allocator-agnostic way is tricky).
Refactored the code and made it more in line with the librdkafka style guide.
Values of security-sensitive configuration properties are now cleared from memory when the value is no longer used, or on rd_kafka_t destruction. This is used for ssl.certificate.pem, ssl.key.pem and ssl.key.password.
The ssl_cert_verify_cb was changed to provide more information to the callback, including the full X509_STORE_CTX.
Add integration tests.

To be done:

Rework the Windows SSL cert store example to its own file.","What is you objection to allowing the callback to set this error?

There are two issues with it:

X509_STORE_CTX_set_error() expects an SSL / libcrypto error code - this leaks the abstraction that we're trying to provide, this means an application will need to include openssl/err.h, et.al, to set this to a meaningful value.
the ctx error will just show up as a generic and weirdly formatted openssl error (because they are all weirdly formatted :) ) - while the errstr could provide some actual information on the verification error, not just the kind of error.

So I don't think it adds much value.",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2309,2019-05-02T20:14:55Z,2019-06-03T21:16:10Z,2020-05-08T08:58:59Z,MERGED,True,3716,921,40,https://github.com/edenhill,Continuing on PR #1882: SSL cert verification and in-memory keys,14,[],https://github.com/edenhill/librdkafka/pull/2309,https://github.com/noahdav,35,https://github.com/edenhill/librdkafka/pull/2309#issuecomment-492817251,"This PR is a continuation on @noahdav's work on PR #1882.
Summary of changes compared to #1882:

The retrieve_cb() has been removed in favour of using standard configuration properties. The reasoning behind this s simplification, reduced code size, cross-language maintenance (some high-level languages, such as Go, can't use callbacks in a good way), and memory ownership issues (having the callback return cert memory to librdkafka in an effective and allocator-agnostic way is tricky).
Refactored the code and made it more in line with the librdkafka style guide.
Values of security-sensitive configuration properties are now cleared from memory when the value is no longer used, or on rd_kafka_t destruction. This is used for ssl.certificate.pem, ssl.key.pem and ssl.key.password.
The ssl_cert_verify_cb was changed to provide more information to the callback, including the full X509_STORE_CTX.
Add integration tests.

To be done:

Rework the Windows SSL cert store example to its own file.","@edenhill Your concerns are valid.  I understand and after thinking more about it I agree with what you are saying.  I think it would be simplest to allow a self signed cert if the client asks for it to be ignored.  Maybe we can add a flags to the config?  Otherwise we can add the root ca if we can support adding it to set_ssl_cert.
I tested this and other than the self signed cert it seems to be working.  When the verify callback returns false we keep retrying which causes the callback to get called over and over in the connect loop.  This does not seem like a good idea but it does eventually times out.  I think we should try to understand if the fail to connect is due to verify failure and in this case do not try to reconnect.",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2309,2019-05-02T20:14:55Z,2019-06-03T21:16:10Z,2020-05-08T08:58:59Z,MERGED,True,3716,921,40,https://github.com/edenhill,Continuing on PR #1882: SSL cert verification and in-memory keys,14,[],https://github.com/edenhill/librdkafka/pull/2309,https://github.com/edenhill,36,https://github.com/edenhill/librdkafka/pull/2309#issuecomment-492820899,"This PR is a continuation on @noahdav's work on PR #1882.
Summary of changes compared to #1882:

The retrieve_cb() has been removed in favour of using standard configuration properties. The reasoning behind this s simplification, reduced code size, cross-language maintenance (some high-level languages, such as Go, can't use callbacks in a good way), and memory ownership issues (having the callback return cert memory to librdkafka in an effective and allocator-agnostic way is tricky).
Refactored the code and made it more in line with the librdkafka style guide.
Values of security-sensitive configuration properties are now cleared from memory when the value is no longer used, or on rd_kafka_t destruction. This is used for ssl.certificate.pem, ssl.key.pem and ssl.key.password.
The ssl_cert_verify_cb was changed to provide more information to the callback, including the full X509_STORE_CTX.
Add integration tests.

To be done:

Rework the Windows SSL cert store example to its own file.","I think it would be simplest to allow a self signed cert if the client asks for it to be ignored.

I looked at SSL_CTX_set_verify() again and it actually allows us to override the preverify_ok failure that you are seeing, so if we just provide the error to the verify callback  you can check if it is the self-signed-cert error and then return ok rather than fail (from preverify_ok). But we might need to clear the error on the context, I'll look into it.

Maybe we can add a flags to the config? Otherwise we can add the root ca if we can support adding it to set_ssl_cert.

But this is probably a better approach, I'll add support for setting the CA certificates using set_ssl_cert().  I'll add DER and PKCS#12/PFX support for CAs, that should be enough, right?",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2309,2019-05-02T20:14:55Z,2019-06-03T21:16:10Z,2020-05-08T08:58:59Z,MERGED,True,3716,921,40,https://github.com/edenhill,Continuing on PR #1882: SSL cert verification and in-memory keys,14,[],https://github.com/edenhill/librdkafka/pull/2309,https://github.com/edenhill,37,https://github.com/edenhill/librdkafka/pull/2309#issuecomment-492821221,"This PR is a continuation on @noahdav's work on PR #1882.
Summary of changes compared to #1882:

The retrieve_cb() has been removed in favour of using standard configuration properties. The reasoning behind this s simplification, reduced code size, cross-language maintenance (some high-level languages, such as Go, can't use callbacks in a good way), and memory ownership issues (having the callback return cert memory to librdkafka in an effective and allocator-agnostic way is tricky).
Refactored the code and made it more in line with the librdkafka style guide.
Values of security-sensitive configuration properties are now cleared from memory when the value is no longer used, or on rd_kafka_t destruction. This is used for ssl.certificate.pem, ssl.key.pem and ssl.key.password.
The ssl_cert_verify_cb was changed to provide more information to the callback, including the full X509_STORE_CTX.
Add integration tests.

To be done:

Rework the Windows SSL cert store example to its own file.","I think we should try to understand if the fail to connect is due to verify failure and in this case do not try to reconnect.

Yeah, librdkafka currently treats all errors as temporary. We're looking into enriching the error handling with indication if an error is temporary or permanent to allow the application to take proper action.",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2309,2019-05-02T20:14:55Z,2019-06-03T21:16:10Z,2020-05-08T08:58:59Z,MERGED,True,3716,921,40,https://github.com/edenhill,Continuing on PR #1882: SSL cert verification and in-memory keys,14,[],https://github.com/edenhill/librdkafka/pull/2309,https://github.com/edenhill,38,https://github.com/edenhill/librdkafka/pull/2309#issuecomment-492822189,"This PR is a continuation on @noahdav's work on PR #1882.
Summary of changes compared to #1882:

The retrieve_cb() has been removed in favour of using standard configuration properties. The reasoning behind this s simplification, reduced code size, cross-language maintenance (some high-level languages, such as Go, can't use callbacks in a good way), and memory ownership issues (having the callback return cert memory to librdkafka in an effective and allocator-agnostic way is tricky).
Refactored the code and made it more in line with the librdkafka style guide.
Values of security-sensitive configuration properties are now cleared from memory when the value is no longer used, or on rd_kafka_t destruction. This is used for ssl.certificate.pem, ssl.key.pem and ssl.key.password.
The ssl_cert_verify_cb was changed to provide more information to the callback, including the full X509_STORE_CTX.
Add integration tests.

To be done:

Rework the Windows SSL cert store example to its own file.","One open question is how we pass the OpenSSL error code to the verify_cb.

Unchanged, people will have to look up the int define in openssl/err.h (et.al)?
Add librdkafka defines for a few ""well known"", I dont even know where to begin
Convert them to a text string and let the application strcmp/strstr()?",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2309,2019-05-02T20:14:55Z,2019-06-03T21:16:10Z,2020-05-08T08:58:59Z,MERGED,True,3716,921,40,https://github.com/edenhill,Continuing on PR #1882: SSL cert verification and in-memory keys,14,[],https://github.com/edenhill/librdkafka/pull/2309,https://github.com/noahdav,39,https://github.com/edenhill/librdkafka/pull/2309#issuecomment-492837875,"This PR is a continuation on @noahdav's work on PR #1882.
Summary of changes compared to #1882:

The retrieve_cb() has been removed in favour of using standard configuration properties. The reasoning behind this s simplification, reduced code size, cross-language maintenance (some high-level languages, such as Go, can't use callbacks in a good way), and memory ownership issues (having the callback return cert memory to librdkafka in an effective and allocator-agnostic way is tricky).
Refactored the code and made it more in line with the librdkafka style guide.
Values of security-sensitive configuration properties are now cleared from memory when the value is no longer used, or on rd_kafka_t destruction. This is used for ssl.certificate.pem, ssl.key.pem and ssl.key.password.
The ssl_cert_verify_cb was changed to provide more information to the callback, including the full X509_STORE_CTX.
Add integration tests.

To be done:

Rework the Windows SSL cert store example to its own file.","@edenhill Thanks so much for your work on this.  I really appreciate it.

Yeah, librdkafka currently treats all errors as temporary. We're looking into enriching the error handling with indication if an error is temporary or permanent to allow the application to take proper action.

As long as we have the same behavior as rest of librdkafka I guess that is good enough for now.

One open question is how we pass the OpenSSL error code to the verify_cb.

As much as I hate to admit it I think that using the OpenSSL error codes is the way to go.  This will allow an application to either define the ones they care about or use OpenSSL defines in err.h/x509_vfy.h/etc.  Although it is not pretty I think it is much better then string parsing to figure out the errors.

I looked at SSL_CTX_set_verify() again and it actually allows us to override the preverify_ok failure that you are seeing, so if we just provide the error to the verify callback you can check if it is the self-signed-cert error and then return ok rather than fail (from preverify_ok). But we might need to clear the error on the context, I'll look into it.

I looked at this too and we will need a way to clear the error in the context otherwise SSL_CTX_set_verify() will return the original error.  I was using X509_STORE_CTX_set_error to do this in the function table.  If you have another way to do this so that the application does not have a OpenSSL link time dependency that would be great.

I'll add support for setting the CA certificates using set_ssl_cert(). I'll add DER and PKCS#12/PFX support for CAs, that should be enough, right?

Yes as long as you support PKCS#12/PFX we can convert all of our certs into this.",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2309,2019-05-02T20:14:55Z,2019-06-03T21:16:10Z,2020-05-08T08:58:59Z,MERGED,True,3716,921,40,https://github.com/edenhill,Continuing on PR #1882: SSL cert verification and in-memory keys,14,[],https://github.com/edenhill/librdkafka/pull/2309,https://github.com/edenhill,40,https://github.com/edenhill/librdkafka/pull/2309#issuecomment-493004367,"This PR is a continuation on @noahdav's work on PR #1882.
Summary of changes compared to #1882:

The retrieve_cb() has been removed in favour of using standard configuration properties. The reasoning behind this s simplification, reduced code size, cross-language maintenance (some high-level languages, such as Go, can't use callbacks in a good way), and memory ownership issues (having the callback return cert memory to librdkafka in an effective and allocator-agnostic way is tricky).
Refactored the code and made it more in line with the librdkafka style guide.
Values of security-sensitive configuration properties are now cleared from memory when the value is no longer used, or on rd_kafka_t destruction. This is used for ssl.certificate.pem, ssl.key.pem and ssl.key.password.
The ssl_cert_verify_cb was changed to provide more information to the callback, including the full X509_STORE_CTX.
Add integration tests.

To be done:

Rework the Windows SSL cert store example to its own file.","@noahdav Can you pull and try out the latest version?
The verify_cb changed: x509 and preverify_ok are gone, instead we have an in/out x509_error that you can use to change or clear the X509 CTX error.
CA loading from PKCS, DER and PEM was also added.",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2309,2019-05-02T20:14:55Z,2019-06-03T21:16:10Z,2020-05-08T08:58:59Z,MERGED,True,3716,921,40,https://github.com/edenhill,Continuing on PR #1882: SSL cert verification and in-memory keys,14,[],https://github.com/edenhill/librdkafka/pull/2309,https://github.com/noahdav,41,https://github.com/edenhill/librdkafka/pull/2309#issuecomment-493108216,"This PR is a continuation on @noahdav's work on PR #1882.
Summary of changes compared to #1882:

The retrieve_cb() has been removed in favour of using standard configuration properties. The reasoning behind this s simplification, reduced code size, cross-language maintenance (some high-level languages, such as Go, can't use callbacks in a good way), and memory ownership issues (having the callback return cert memory to librdkafka in an effective and allocator-agnostic way is tricky).
Refactored the code and made it more in line with the librdkafka style guide.
Values of security-sensitive configuration properties are now cleared from memory when the value is no longer used, or on rd_kafka_t destruction. This is used for ssl.certificate.pem, ssl.key.pem and ssl.key.password.
The ssl_cert_verify_cb was changed to provide more information to the callback, including the full X509_STORE_CTX.
Add integration tests.

To be done:

Rework the Windows SSL cert store example to its own file.",@edenhill Thanks for your work on this.  I am really excited to get this all complete.  I will take another look at this today.,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2309,2019-05-02T20:14:55Z,2019-06-03T21:16:10Z,2020-05-08T08:58:59Z,MERGED,True,3716,921,40,https://github.com/edenhill,Continuing on PR #1882: SSL cert verification and in-memory keys,14,[],https://github.com/edenhill/librdkafka/pull/2309,https://github.com/noahdav,42,https://github.com/edenhill/librdkafka/pull/2309#issuecomment-493167524,"This PR is a continuation on @noahdav's work on PR #1882.
Summary of changes compared to #1882:

The retrieve_cb() has been removed in favour of using standard configuration properties. The reasoning behind this s simplification, reduced code size, cross-language maintenance (some high-level languages, such as Go, can't use callbacks in a good way), and memory ownership issues (having the callback return cert memory to librdkafka in an effective and allocator-agnostic way is tricky).
Refactored the code and made it more in line with the librdkafka style guide.
Values of security-sensitive configuration properties are now cleared from memory when the value is no longer used, or on rd_kafka_t destruction. This is used for ssl.certificate.pem, ssl.key.pem and ssl.key.password.
The ssl_cert_verify_cb was changed to provide more information to the callback, including the full X509_STORE_CTX.
Add integration tests.

To be done:

Rework the Windows SSL cert store example to its own file.","@edenhill This will work for my scenario.  Thank you for all of your help on this feature.  This will be great to get it in.
I am seeing is that when I am just testing on my localmachine the callback gets called multiple times.  I am setting the metadata.broker.list to localhost:9093.  Then in rd_kafka_transport_io_event the rd_kafka_transport_t already has two entries in it.  One for the localhost (broker_id = -1) and one for :9093 (broker_id = 0).  Since there are two entries this causes the callback to get called two times (for each cert in the chain).  I do not see where the dns lookup is occurring but it seems that this should not create the extra connection in this case.  I can work around this by ignoring when broker_id < 0 but I think this should not create the extra connection.
On a simpler note you only allow the callback to clear the cert error.  Should we allow them to set the error too?  If not I am ok with this but we should have another flag that if set clears error.  Then have the current error field only and input not in out.  I do not think that this is clear that on input the x509_error param can have any value but on output only 0 has any effect.",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2309,2019-05-02T20:14:55Z,2019-06-03T21:16:10Z,2020-05-08T08:58:59Z,MERGED,True,3716,921,40,https://github.com/edenhill,Continuing on PR #1882: SSL cert verification and in-memory keys,14,[],https://github.com/edenhill/librdkafka/pull/2309,https://github.com/edenhill,43,https://github.com/edenhill/librdkafka/pull/2309#issuecomment-493172348,"This PR is a continuation on @noahdav's work on PR #1882.
Summary of changes compared to #1882:

The retrieve_cb() has been removed in favour of using standard configuration properties. The reasoning behind this s simplification, reduced code size, cross-language maintenance (some high-level languages, such as Go, can't use callbacks in a good way), and memory ownership issues (having the callback return cert memory to librdkafka in an effective and allocator-agnostic way is tricky).
Refactored the code and made it more in line with the librdkafka style guide.
Values of security-sensitive configuration properties are now cleared from memory when the value is no longer used, or on rd_kafka_t destruction. This is used for ssl.certificate.pem, ssl.key.pem and ssl.key.password.
The ssl_cert_verify_cb was changed to provide more information to the callback, including the full X509_STORE_CTX.
Add integration tests.

To be done:

Rework the Windows SSL cert store example to its own file.","librdkafka will map a bootstrap broker to a broker reported through cluster metadata if the hostname and port matches exactly, but it will not do so based on what the hostname resolves to since DNS entries may change or resolve to multiple addresses, so the behaviour you are describing is expected.
You can set the context error by updating the error code in x509_error and returning 0/false from the verify_cb.",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2309,2019-05-02T20:14:55Z,2019-06-03T21:16:10Z,2020-05-08T08:58:59Z,MERGED,True,3716,921,40,https://github.com/edenhill,Continuing on PR #1882: SSL cert verification and in-memory keys,14,[],https://github.com/edenhill/librdkafka/pull/2309,https://github.com/noahdav,44,https://github.com/edenhill/librdkafka/pull/2309#issuecomment-493177287,"This PR is a continuation on @noahdav's work on PR #1882.
Summary of changes compared to #1882:

The retrieve_cb() has been removed in favour of using standard configuration properties. The reasoning behind this s simplification, reduced code size, cross-language maintenance (some high-level languages, such as Go, can't use callbacks in a good way), and memory ownership issues (having the callback return cert memory to librdkafka in an effective and allocator-agnostic way is tricky).
Refactored the code and made it more in line with the librdkafka style guide.
Values of security-sensitive configuration properties are now cleared from memory when the value is no longer used, or on rd_kafka_t destruction. This is used for ssl.certificate.pem, ssl.key.pem and ssl.key.password.
The ssl_cert_verify_cb was changed to provide more information to the callback, including the full X509_STORE_CTX.
Add integration tests.

To be done:

Rework the Windows SSL cert store example to its own file.",@edenhill thanks you are right.  I missed that part of returning false that will set error.  This seems good to me.,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2309,2019-05-02T20:14:55Z,2019-06-03T21:16:10Z,2020-05-08T08:58:59Z,MERGED,True,3716,921,40,https://github.com/edenhill,Continuing on PR #1882: SSL cert verification and in-memory keys,14,[],https://github.com/edenhill/librdkafka/pull/2309,https://github.com/noahdav,45,https://github.com/edenhill/librdkafka/pull/2309#issuecomment-493194381,"This PR is a continuation on @noahdav's work on PR #1882.
Summary of changes compared to #1882:

The retrieve_cb() has been removed in favour of using standard configuration properties. The reasoning behind this s simplification, reduced code size, cross-language maintenance (some high-level languages, such as Go, can't use callbacks in a good way), and memory ownership issues (having the callback return cert memory to librdkafka in an effective and allocator-agnostic way is tricky).
Refactored the code and made it more in line with the librdkafka style guide.
Values of security-sensitive configuration properties are now cleared from memory when the value is no longer used, or on rd_kafka_t destruction. This is used for ssl.certificate.pem, ssl.key.pem and ssl.key.password.
The ssl_cert_verify_cb was changed to provide more information to the callback, including the full X509_STORE_CTX.
Add integration tests.

To be done:

Rework the Windows SSL cert store example to its own file.","@edenhill It looks like I spoke too soon.  In rd_kafka_conf_set_ssl_cert the map needs to initialize the ca cert.
rd_kafka_cert_t **cert_map[RD_KAFKA_CERT__CNT] = {
[RD_KAFKA_CERT_PUBLIC_KEY] = &conf->ssl.cert,
[RD_KAFKA_CERT_PRIVATE_KEY] = &conf->ssl.key,
[RD_KAFKA_CERT_CA] = &conf->ssl.ca
};
After making this change I am still getting error  X509_V_ERR_SELF_SIGNED_CERT_IN_CHAIN in the callback.  Any idea what I need to do in order to get OpenSSL to recognize the self signed ca?",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2309,2019-05-02T20:14:55Z,2019-06-03T21:16:10Z,2020-05-08T08:58:59Z,MERGED,True,3716,921,40,https://github.com/edenhill,Continuing on PR #1882: SSL cert verification and in-memory keys,14,[],https://github.com/edenhill/librdkafka/pull/2309,https://github.com/edenhill,46,https://github.com/edenhill/librdkafka/pull/2309#issuecomment-493197146,"This PR is a continuation on @noahdav's work on PR #1882.
Summary of changes compared to #1882:

The retrieve_cb() has been removed in favour of using standard configuration properties. The reasoning behind this s simplification, reduced code size, cross-language maintenance (some high-level languages, such as Go, can't use callbacks in a good way), and memory ownership issues (having the callback return cert memory to librdkafka in an effective and allocator-agnostic way is tricky).
Refactored the code and made it more in line with the librdkafka style guide.
Values of security-sensitive configuration properties are now cleared from memory when the value is no longer used, or on rd_kafka_t destruction. This is used for ssl.certificate.pem, ssl.key.pem and ssl.key.password.
The ssl_cert_verify_cb was changed to provide more information to the callback, including the full X509_STORE_CTX.
Add integration tests.

To be done:

Rework the Windows SSL cert store example to its own file.","Good catch!
What's your config?",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2309,2019-05-02T20:14:55Z,2019-06-03T21:16:10Z,2020-05-08T08:58:59Z,MERGED,True,3716,921,40,https://github.com/edenhill,Continuing on PR #1882: SSL cert verification and in-memory keys,14,[],https://github.com/edenhill/librdkafka/pull/2309,https://github.com/noahdav,47,https://github.com/edenhill/librdkafka/pull/2309#issuecomment-493200141,"This PR is a continuation on @noahdav's work on PR #1882.
Summary of changes compared to #1882:

The retrieve_cb() has been removed in favour of using standard configuration properties. The reasoning behind this s simplification, reduced code size, cross-language maintenance (some high-level languages, such as Go, can't use callbacks in a good way), and memory ownership issues (having the callback return cert memory to librdkafka in an effective and allocator-agnostic way is tricky).
Refactored the code and made it more in line with the librdkafka style guide.
Values of security-sensitive configuration properties are now cleared from memory when the value is no longer used, or on rd_kafka_t destruction. This is used for ssl.certificate.pem, ssl.key.pem and ssl.key.password.
The ssl_cert_verify_cb was changed to provide more information to the callback, including the full X509_STORE_CTX.
Add integration tests.

To be done:

Rework the Windows SSL cert store example to its own file.","I just tried to use set_ssl_cert to with CERT_CA CERT_ENC_PEM which returned success.  The pem file is the ca cert that I used to generate the certificate public and private keys.  Is there something else that I am missing here?
security.protocol=ssl
ssl.certificate.pem=...
ssl.key.password=...
set_ssl_cert with CERT_PRIVATE_KEY CERT_ENC_PKCS12
enable.ssl.certificate.verification=true
ssl_cert_verify_cb = verify_cb",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2309,2019-05-02T20:14:55Z,2019-06-03T21:16:10Z,2020-05-08T08:58:59Z,MERGED,True,3716,921,40,https://github.com/edenhill,Continuing on PR #1882: SSL cert verification and in-memory keys,14,[],https://github.com/edenhill/librdkafka/pull/2309,https://github.com/edenhill,48,https://github.com/edenhill/librdkafka/pull/2309#issuecomment-493207491,"This PR is a continuation on @noahdav's work on PR #1882.
Summary of changes compared to #1882:

The retrieve_cb() has been removed in favour of using standard configuration properties. The reasoning behind this s simplification, reduced code size, cross-language maintenance (some high-level languages, such as Go, can't use callbacks in a good way), and memory ownership issues (having the callback return cert memory to librdkafka in an effective and allocator-agnostic way is tricky).
Refactored the code and made it more in line with the librdkafka style guide.
Values of security-sensitive configuration properties are now cleared from memory when the value is no longer used, or on rd_kafka_t destruction. This is used for ssl.certificate.pem, ssl.key.pem and ssl.key.password.
The ssl_cert_verify_cb was changed to provide more information to the callback, including the full X509_STORE_CTX.
Add integration tests.

To be done:

Rework the Windows SSL cert store example to its own file.","So the thing is that CERT_CA, ssl.ca.pem AND PUBLIC_KEY and ssl.certificate.* all map to SSL_CTX_use_certificate(), which seems to be a singletonish thing in the context, i.e., sub-sequent calls will replace the previous one.
I'm not really sure if I should be using another API, possibly SSL_CTX_add_extra_chain_cert(), but that should only be needed if there is indeed a chain of certs that need to be added.
In your case it sounds like you only need to add one certificate.
Can you try commenting out your ssl.certificate.pem config and see if it helps, since that overwrites the CA?",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2309,2019-05-02T20:14:55Z,2019-06-03T21:16:10Z,2020-05-08T08:58:59Z,MERGED,True,3716,921,40,https://github.com/edenhill,Continuing on PR #1882: SSL cert verification and in-memory keys,14,[],https://github.com/edenhill/librdkafka/pull/2309,https://github.com/noahdav,49,https://github.com/edenhill/librdkafka/pull/2309#issuecomment-493212414,"This PR is a continuation on @noahdav's work on PR #1882.
Summary of changes compared to #1882:

The retrieve_cb() has been removed in favour of using standard configuration properties. The reasoning behind this s simplification, reduced code size, cross-language maintenance (some high-level languages, such as Go, can't use callbacks in a good way), and memory ownership issues (having the callback return cert memory to librdkafka in an effective and allocator-agnostic way is tricky).
Refactored the code and made it more in line with the librdkafka style guide.
Values of security-sensitive configuration properties are now cleared from memory when the value is no longer used, or on rd_kafka_t destruction. This is used for ssl.certificate.pem, ssl.key.pem and ssl.key.password.
The ssl_cert_verify_cb was changed to provide more information to the callback, including the full X509_STORE_CTX.
Add integration tests.

To be done:

Rework the Windows SSL cert store example to its own file.",I did that but then the public key cannot be found which causes create of the procurer to fail.  I can certainly work around this in the callback by setting the error back to 0 but not the most elegant solution.  Looking at the docs I think you are right that if we want to support this so a cert can be added with its CA we need to add it using  SSL_CTX_add_extra_chain_cer or SSL_CTX_add1_chain_cert.,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2309,2019-05-02T20:14:55Z,2019-06-03T21:16:10Z,2020-05-08T08:58:59Z,MERGED,True,3716,921,40,https://github.com/edenhill,Continuing on PR #1882: SSL cert verification and in-memory keys,14,[],https://github.com/edenhill/librdkafka/pull/2309,https://github.com/edenhill,50,https://github.com/edenhill/librdkafka/pull/2309#issuecomment-493971720,"This PR is a continuation on @noahdav's work on PR #1882.
Summary of changes compared to #1882:

The retrieve_cb() has been removed in favour of using standard configuration properties. The reasoning behind this s simplification, reduced code size, cross-language maintenance (some high-level languages, such as Go, can't use callbacks in a good way), and memory ownership issues (having the callback return cert memory to librdkafka in an effective and allocator-agnostic way is tricky).
Refactored the code and made it more in line with the librdkafka style guide.
Values of security-sensitive configuration properties are now cleared from memory when the value is no longer used, or on rd_kafka_t destruction. This is used for ssl.certificate.pem, ssl.key.pem and ssl.key.password.
The ssl_cert_verify_cb was changed to provide more information to the callback, including the full X509_STORE_CTX.
Add integration tests.

To be done:

Rework the Windows SSL cert store example to its own file.","Okay @noahdav, this should now be fixed, can you give it another spin?",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2309,2019-05-02T20:14:55Z,2019-06-03T21:16:10Z,2020-05-08T08:58:59Z,MERGED,True,3716,921,40,https://github.com/edenhill,Continuing on PR #1882: SSL cert verification and in-memory keys,14,[],https://github.com/edenhill/librdkafka/pull/2309,https://github.com/noahdav,51,https://github.com/edenhill/librdkafka/pull/2309#issuecomment-494141316,"This PR is a continuation on @noahdav's work on PR #1882.
Summary of changes compared to #1882:

The retrieve_cb() has been removed in favour of using standard configuration properties. The reasoning behind this s simplification, reduced code size, cross-language maintenance (some high-level languages, such as Go, can't use callbacks in a good way), and memory ownership issues (having the callback return cert memory to librdkafka in an effective and allocator-agnostic way is tricky).
Refactored the code and made it more in line with the librdkafka style guide.
Values of security-sensitive configuration properties are now cleared from memory when the value is no longer used, or on rd_kafka_t destruction. This is used for ssl.certificate.pem, ssl.key.pem and ssl.key.password.
The ssl_cert_verify_cb was changed to provide more information to the callback, including the full X509_STORE_CTX.
Add integration tests.

To be done:

Rework the Windows SSL cert store example to its own file.",@edenhill  What should the expected behavior be here.  I set the ca cert using set_ssl_cert.  However I am still seeing the validate callback being called with error code X509_V_ERR_SELF_SIGNED_CERT_IN_CHAIN.,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2309,2019-05-02T20:14:55Z,2019-06-03T21:16:10Z,2020-05-08T08:58:59Z,MERGED,True,3716,921,40,https://github.com/edenhill,Continuing on PR #1882: SSL cert verification and in-memory keys,14,[],https://github.com/edenhill/librdkafka/pull/2309,https://github.com/edenhill,52,https://github.com/edenhill/librdkafka/pull/2309#issuecomment-494260020,"This PR is a continuation on @noahdav's work on PR #1882.
Summary of changes compared to #1882:

The retrieve_cb() has been removed in favour of using standard configuration properties. The reasoning behind this s simplification, reduced code size, cross-language maintenance (some high-level languages, such as Go, can't use callbacks in a good way), and memory ownership issues (having the callback return cert memory to librdkafka in an effective and allocator-agnostic way is tricky).
Refactored the code and made it more in line with the librdkafka style guide.
Values of security-sensitive configuration properties are now cleared from memory when the value is no longer used, or on rd_kafka_t destruction. This is used for ssl.certificate.pem, ssl.key.pem and ssl.key.password.
The ssl_cert_verify_cb was changed to provide more information to the callback, including the full X509_STORE_CTX.
Add integration tests.

To be done:

Rework the Windows SSL cert store example to its own file.","@noahdav Can you export your private key and the root CA to files and then use openssl command line to verify correctness?
openssl verify -CAfile yourca.pem yourkey.pem",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2309,2019-05-02T20:14:55Z,2019-06-03T21:16:10Z,2020-05-08T08:58:59Z,MERGED,True,3716,921,40,https://github.com/edenhill,Continuing on PR #1882: SSL cert verification and in-memory keys,14,[],https://github.com/edenhill/librdkafka/pull/2309,https://github.com/noahdav,53,https://github.com/edenhill/librdkafka/pull/2309#issuecomment-494586511,"This PR is a continuation on @noahdav's work on PR #1882.
Summary of changes compared to #1882:

The retrieve_cb() has been removed in favour of using standard configuration properties. The reasoning behind this s simplification, reduced code size, cross-language maintenance (some high-level languages, such as Go, can't use callbacks in a good way), and memory ownership issues (having the callback return cert memory to librdkafka in an effective and allocator-agnostic way is tricky).
Refactored the code and made it more in line with the librdkafka style guide.
Values of security-sensitive configuration properties are now cleared from memory when the value is no longer used, or on rd_kafka_t destruction. This is used for ssl.certificate.pem, ssl.key.pem and ssl.key.password.
The ssl_cert_verify_cb was changed to provide more information to the callback, including the full X509_STORE_CTX.
Add integration tests.

To be done:

Rework the Windows SSL cert store example to its own file.","@edenhill OpenSSL verify works fine.  OpenSSL returns client.pem: OK when I use the public key of the ca and the cert public key.
In my testing I do the following:
Set ca public key in PEM format via set_ssl_cert()
ssl.certificate.pem=
ssl.key.password=
Set private key in PFX/PKCS#12 format via set_ssl_cert()
enable.ssl.certificate.verification=true
ssl_cert_verify_cb=<callback
When the callback gets invoked the error is still 19(X509_V_ERR_SELF_SIGNED_CERT_IN_CHAIN).",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2309,2019-05-02T20:14:55Z,2019-06-03T21:16:10Z,2020-05-08T08:58:59Z,MERGED,True,3716,921,40,https://github.com/edenhill,Continuing on PR #1882: SSL cert verification and in-memory keys,14,[],https://github.com/edenhill/librdkafka/pull/2309,https://github.com/edenhill,54,https://github.com/edenhill/librdkafka/pull/2309#issuecomment-494952607,"This PR is a continuation on @noahdav's work on PR #1882.
Summary of changes compared to #1882:

The retrieve_cb() has been removed in favour of using standard configuration properties. The reasoning behind this s simplification, reduced code size, cross-language maintenance (some high-level languages, such as Go, can't use callbacks in a good way), and memory ownership issues (having the callback return cert memory to librdkafka in an effective and allocator-agnostic way is tricky).
Refactored the code and made it more in line with the librdkafka style guide.
Values of security-sensitive configuration properties are now cleared from memory when the value is no longer used, or on rd_kafka_t destruction. This is used for ssl.certificate.pem, ssl.key.pem and ssl.key.password.
The ssl_cert_verify_cb was changed to provide more information to the callback, including the full X509_STORE_CTX.
Add integration tests.

To be done:

Rework the Windows SSL cert store example to its own file.","If you specify the client's public key as both certificate.pem and CA, then it is effectively a self-signed certificate.
You should have a root-CA cert (by your own authority or a proper one) signing your priv/pub key, right?",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2309,2019-05-02T20:14:55Z,2019-06-03T21:16:10Z,2020-05-08T08:58:59Z,MERGED,True,3716,921,40,https://github.com/edenhill,Continuing on PR #1882: SSL cert verification and in-memory keys,14,[],https://github.com/edenhill/librdkafka/pull/2309,https://github.com/LighthouseJ,55,https://github.com/edenhill/librdkafka/pull/2309#issuecomment-494966807,"This PR is a continuation on @noahdav's work on PR #1882.
Summary of changes compared to #1882:

The retrieve_cb() has been removed in favour of using standard configuration properties. The reasoning behind this s simplification, reduced code size, cross-language maintenance (some high-level languages, such as Go, can't use callbacks in a good way), and memory ownership issues (having the callback return cert memory to librdkafka in an effective and allocator-agnostic way is tricky).
Refactored the code and made it more in line with the librdkafka style guide.
Values of security-sensitive configuration properties are now cleared from memory when the value is no longer used, or on rd_kafka_t destruction. This is used for ssl.certificate.pem, ssl.key.pem and ssl.key.password.
The ssl_cert_verify_cb was changed to provide more information to the callback, including the full X509_STORE_CTX.
Add integration tests.

To be done:

Rework the Windows SSL cert store example to its own file.","If you specify the client's public key as both certificate.pem and CA, then it is effectively a self-signed certificate.

IMHO, this isn't technically true. A self-signed CA has the same subject and issuer DN, and is of course a CA.
Help for X509_V_ERR_SELF_SIGNED_CERT_IN_CHAIN says:

X509_V_ERR_SELF_SIGNED_CERT_IN_CHAIN: self signed certificate in certificate chain
the certificate chain could be built up using the untrusted certificates but the root could not be found  locally.

I think it's okay but not ideal to send the self-signed cert in the chain, but the primary cause for the verification error is if the remote end did not trust the CA.  The certs you send over the wire is untrusted, and the CA(s) need to be trusted via -CAfile or -CAdir (or JSSE CA truststore) first.
I typically have a self-signed root CA, which signs intermediate CA, which signs server certs and client certs per application.  I then create cert chains (which are ordered!) with intermediate and leaf cert, which avoids that verify failure as long as the peer trusts my CA files.
I create my own CA's and certs for various purposes using this guide:
https://jamielinux.com/docs/openssl-certificate-authority/",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2309,2019-05-02T20:14:55Z,2019-06-03T21:16:10Z,2020-05-08T08:58:59Z,MERGED,True,3716,921,40,https://github.com/edenhill,Continuing on PR #1882: SSL cert verification and in-memory keys,14,[],https://github.com/edenhill/librdkafka/pull/2309,https://github.com/edenhill,56,https://github.com/edenhill/librdkafka/pull/2309#issuecomment-494968170,"This PR is a continuation on @noahdav's work on PR #1882.
Summary of changes compared to #1882:

The retrieve_cb() has been removed in favour of using standard configuration properties. The reasoning behind this s simplification, reduced code size, cross-language maintenance (some high-level languages, such as Go, can't use callbacks in a good way), and memory ownership issues (having the callback return cert memory to librdkafka in an effective and allocator-agnostic way is tricky).
Refactored the code and made it more in line with the librdkafka style guide.
Values of security-sensitive configuration properties are now cleared from memory when the value is no longer used, or on rd_kafka_t destruction. This is used for ssl.certificate.pem, ssl.key.pem and ssl.key.password.
The ssl_cert_verify_cb was changed to provide more information to the callback, including the full X509_STORE_CTX.
Add integration tests.

To be done:

Rework the Windows SSL cert store example to its own file.","Oh, sorry. The CA cert you pass to librdkafka is for verifying the broker certificate, not the client's",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2309,2019-05-02T20:14:55Z,2019-06-03T21:16:10Z,2020-05-08T08:58:59Z,MERGED,True,3716,921,40,https://github.com/edenhill,Continuing on PR #1882: SSL cert verification and in-memory keys,14,[],https://github.com/edenhill/librdkafka/pull/2309,https://github.com/noahdav,57,https://github.com/edenhill/librdkafka/pull/2309#issuecomment-494970573,"This PR is a continuation on @noahdav's work on PR #1882.
Summary of changes compared to #1882:

The retrieve_cb() has been removed in favour of using standard configuration properties. The reasoning behind this s simplification, reduced code size, cross-language maintenance (some high-level languages, such as Go, can't use callbacks in a good way), and memory ownership issues (having the callback return cert memory to librdkafka in an effective and allocator-agnostic way is tricky).
Refactored the code and made it more in line with the librdkafka style guide.
Values of security-sensitive configuration properties are now cleared from memory when the value is no longer used, or on rd_kafka_t destruction. This is used for ssl.certificate.pem, ssl.key.pem and ssl.key.password.
The ssl_cert_verify_cb was changed to provide more information to the callback, including the full X509_STORE_CTX.
Add integration tests.

To be done:

Rework the Windows SSL cert store example to its own file.","If you specify the client's public key as both certificate.pem and CA, then it is effectively a self-signed certificate.

I can try this but I thouhght I should be using the root-CA.

You should have a root-CA cert (by your own authority or a proper one) signing your priv/pub key, right?

Yes I have a ca-cert which I used to sign both client and broker certs.",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2309,2019-05-02T20:14:55Z,2019-06-03T21:16:10Z,2020-05-08T08:58:59Z,MERGED,True,3716,921,40,https://github.com/edenhill,Continuing on PR #1882: SSL cert verification and in-memory keys,14,[],https://github.com/edenhill/librdkafka/pull/2309,https://github.com/edenhill,58,https://github.com/edenhill/librdkafka/pull/2309#issuecomment-495318031,"This PR is a continuation on @noahdav's work on PR #1882.
Summary of changes compared to #1882:

The retrieve_cb() has been removed in favour of using standard configuration properties. The reasoning behind this s simplification, reduced code size, cross-language maintenance (some high-level languages, such as Go, can't use callbacks in a good way), and memory ownership issues (having the callback return cert memory to librdkafka in an effective and allocator-agnostic way is tricky).
Refactored the code and made it more in line with the librdkafka style guide.
Values of security-sensitive configuration properties are now cleared from memory when the value is no longer used, or on rd_kafka_t destruction. This is used for ssl.certificate.pem, ssl.key.pem and ssl.key.password.
The ssl_cert_verify_cb was changed to provide more information to the callback, including the full X509_STORE_CTX.
Add integration tests.

To be done:

Rework the Windows SSL cert store example to its own file.","@noahdav
Okay, so if you have a CA-cert that was used to sign the client's key and broker's certificate then you should be fine with setting:

ssl.key.* or CERT_PRIVATE_KEY = the client's private key
ssl.certificate.* or CERT_PUBILIC_KEY` = the client's public key / certificate
ssl.ca.* or CERT_CA = the ca-cert you used to sign the client and broker certificates

I'm guessing you are only using set_ssl_cert(), and not mixing these properties, right?
What encodings are you using for these three?
Can you verify the above?",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2309,2019-05-02T20:14:55Z,2019-06-03T21:16:10Z,2020-05-08T08:58:59Z,MERGED,True,3716,921,40,https://github.com/edenhill,Continuing on PR #1882: SSL cert verification and in-memory keys,14,[],https://github.com/edenhill/librdkafka/pull/2309,https://github.com/edenhill,59,https://github.com/edenhill/librdkafka/pull/2309#issuecomment-495580691,"This PR is a continuation on @noahdav's work on PR #1882.
Summary of changes compared to #1882:

The retrieve_cb() has been removed in favour of using standard configuration properties. The reasoning behind this s simplification, reduced code size, cross-language maintenance (some high-level languages, such as Go, can't use callbacks in a good way), and memory ownership issues (having the callback return cert memory to librdkafka in an effective and allocator-agnostic way is tricky).
Refactored the code and made it more in line with the librdkafka style guide.
Values of security-sensitive configuration properties are now cleared from memory when the value is no longer used, or on rd_kafka_t destruction. This is used for ssl.certificate.pem, ssl.key.pem and ssl.key.password.
The ssl_cert_verify_cb was changed to provide more information to the callback, including the full X509_STORE_CTX.
Add integration tests.

To be done:

Rework the Windows SSL cert store example to its own file.","Okay @noahdav, I think I've got this sorted out now.
OpenSSL has a bunch of different, but similarily named, certificate stores, and I was using the wrong one.
Please give the latest version of this branch a try and report back please!",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2309,2019-05-02T20:14:55Z,2019-06-03T21:16:10Z,2020-05-08T08:58:59Z,MERGED,True,3716,921,40,https://github.com/edenhill,Continuing on PR #1882: SSL cert verification and in-memory keys,14,[],https://github.com/edenhill/librdkafka/pull/2309,https://github.com/noahdav,60,https://github.com/edenhill/librdkafka/pull/2309#issuecomment-495782905,"This PR is a continuation on @noahdav's work on PR #1882.
Summary of changes compared to #1882:

The retrieve_cb() has been removed in favour of using standard configuration properties. The reasoning behind this s simplification, reduced code size, cross-language maintenance (some high-level languages, such as Go, can't use callbacks in a good way), and memory ownership issues (having the callback return cert memory to librdkafka in an effective and allocator-agnostic way is tricky).
Refactored the code and made it more in line with the librdkafka style guide.
Values of security-sensitive configuration properties are now cleared from memory when the value is no longer used, or on rd_kafka_t destruction. This is used for ssl.certificate.pem, ssl.key.pem and ssl.key.password.
The ssl_cert_verify_cb was changed to provide more information to the callback, including the full X509_STORE_CTX.
Add integration tests.

To be done:

Rework the Windows SSL cert store example to its own file.",@edenhill That fixed my issue.  I am now able to validate the self signed cert.  Thanks again for all of your help with this.  I am excited to get this merge to master soon.  I will review one more time early next week if that works for you.  I took a look and did not see anything related to this in the latest commits.  Can you also point me to the commit which fixed this so I can see what actual changed?,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2309,2019-05-02T20:14:55Z,2019-06-03T21:16:10Z,2020-05-08T08:58:59Z,MERGED,True,3716,921,40,https://github.com/edenhill,Continuing on PR #1882: SSL cert verification and in-memory keys,14,[],https://github.com/edenhill/librdkafka/pull/2309,https://github.com/edenhill,61,https://github.com/edenhill/librdkafka/pull/2309#issuecomment-495805296,"This PR is a continuation on @noahdav's work on PR #1882.
Summary of changes compared to #1882:

The retrieve_cb() has been removed in favour of using standard configuration properties. The reasoning behind this s simplification, reduced code size, cross-language maintenance (some high-level languages, such as Go, can't use callbacks in a good way), and memory ownership issues (having the callback return cert memory to librdkafka in an effective and allocator-agnostic way is tricky).
Refactored the code and made it more in line with the librdkafka style guide.
Values of security-sensitive configuration properties are now cleared from memory when the value is no longer used, or on rd_kafka_t destruction. This is used for ssl.certificate.pem, ssl.key.pem and ssl.key.password.
The ssl_cert_verify_cb was changed to provide more information to the callback, including the full X509_STORE_CTX.
Add integration tests.

To be done:

Rework the Windows SSL cert store example to its own file.","@noahdav That's fantastic news, thank you!
We're now passing a X509_STORE to SSL_CTX_set_cert_store() (which is used for  verifying the peer cert):
https://github.com/edenhill/librdkafka/pull/2309/files#diff-c5104996425e1a3e383d42a77a8124ccR725",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2309,2019-05-02T20:14:55Z,2019-06-03T21:16:10Z,2020-05-08T08:58:59Z,MERGED,True,3716,921,40,https://github.com/edenhill,Continuing on PR #1882: SSL cert verification and in-memory keys,14,[],https://github.com/edenhill/librdkafka/pull/2309,https://github.com/noahdav,62,https://github.com/edenhill/librdkafka/pull/2309#issuecomment-495813927,"This PR is a continuation on @noahdav's work on PR #1882.
Summary of changes compared to #1882:

The retrieve_cb() has been removed in favour of using standard configuration properties. The reasoning behind this s simplification, reduced code size, cross-language maintenance (some high-level languages, such as Go, can't use callbacks in a good way), and memory ownership issues (having the callback return cert memory to librdkafka in an effective and allocator-agnostic way is tricky).
Refactored the code and made it more in line with the librdkafka style guide.
Values of security-sensitive configuration properties are now cleared from memory when the value is no longer used, or on rd_kafka_t destruction. This is used for ssl.certificate.pem, ssl.key.pem and ssl.key.password.
The ssl_cert_verify_cb was changed to provide more information to the callback, including the full X509_STORE_CTX.
Add integration tests.

To be done:

Rework the Windows SSL cert store example to its own file.",@edenhill Thank  you for all your help with this.  I will do a full of this early next week and let you know if I see anything.,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2309,2019-05-02T20:14:55Z,2019-06-03T21:16:10Z,2020-05-08T08:58:59Z,MERGED,True,3716,921,40,https://github.com/edenhill,Continuing on PR #1882: SSL cert verification and in-memory keys,14,[],https://github.com/edenhill/librdkafka/pull/2309,https://github.com/wesnerm,63,https://github.com/edenhill/librdkafka/pull/2309#issuecomment-496310152,"This PR is a continuation on @noahdav's work on PR #1882.
Summary of changes compared to #1882:

The retrieve_cb() has been removed in favour of using standard configuration properties. The reasoning behind this s simplification, reduced code size, cross-language maintenance (some high-level languages, such as Go, can't use callbacks in a good way), and memory ownership issues (having the callback return cert memory to librdkafka in an effective and allocator-agnostic way is tricky).
Refactored the code and made it more in line with the librdkafka style guide.
Values of security-sensitive configuration properties are now cleared from memory when the value is no longer used, or on rd_kafka_t destruction. This is used for ssl.certificate.pem, ssl.key.pem and ssl.key.password.
The ssl_cert_verify_cb was changed to provide more information to the callback, including the full X509_STORE_CTX.
Add integration tests.

To be done:

Rework the Windows SSL cert store example to its own file.","From issue #2319.

Sorry, for the lack of response. I haven't had to opportunity to test it.
Since I get my packages from NuGet, I need to figure out the interaction between package management, wrapper libraries (Confluent.NET) and private versions of packages from pull requests.
The PR does address other features that I have been interested in such as in-memory keys and the use of Windows roots.
When I verify this works or doesn't work, I will send a message to both this issue and the PR.
Most likely, I will only able to get to this after the PR is merged.
Thanks for your work.",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2309,2019-05-02T20:14:55Z,2019-06-03T21:16:10Z,2020-05-08T08:58:59Z,MERGED,True,3716,921,40,https://github.com/edenhill,Continuing on PR #1882: SSL cert verification and in-memory keys,14,[],https://github.com/edenhill/librdkafka/pull/2309,https://github.com/edenhill,64,https://github.com/edenhill/librdkafka/pull/2309#issuecomment-496847619,"This PR is a continuation on @noahdav's work on PR #1882.
Summary of changes compared to #1882:

The retrieve_cb() has been removed in favour of using standard configuration properties. The reasoning behind this s simplification, reduced code size, cross-language maintenance (some high-level languages, such as Go, can't use callbacks in a good way), and memory ownership issues (having the callback return cert memory to librdkafka in an effective and allocator-agnostic way is tricky).
Refactored the code and made it more in line with the librdkafka style guide.
Values of security-sensitive configuration properties are now cleared from memory when the value is no longer used, or on rd_kafka_t destruction. This is used for ssl.certificate.pem, ssl.key.pem and ssl.key.password.
The ssl_cert_verify_cb was changed to provide more information to the callback, including the full X509_STORE_CTX.
Add integration tests.

To be done:

Rework the Windows SSL cert store example to its own file.","Thanks for the review @mhowlett !
@noahdav As soon as you give the thumps up we'll merge this.",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2309,2019-05-02T20:14:55Z,2019-06-03T21:16:10Z,2020-05-08T08:58:59Z,MERGED,True,3716,921,40,https://github.com/edenhill,Continuing on PR #1882: SSL cert verification and in-memory keys,14,[],https://github.com/edenhill/librdkafka/pull/2309,https://github.com/noahdav,65,https://github.com/edenhill/librdkafka/pull/2309#issuecomment-497044599,"This PR is a continuation on @noahdav's work on PR #1882.
Summary of changes compared to #1882:

The retrieve_cb() has been removed in favour of using standard configuration properties. The reasoning behind this s simplification, reduced code size, cross-language maintenance (some high-level languages, such as Go, can't use callbacks in a good way), and memory ownership issues (having the callback return cert memory to librdkafka in an effective and allocator-agnostic way is tricky).
Refactored the code and made it more in line with the librdkafka style guide.
Values of security-sensitive configuration properties are now cleared from memory when the value is no longer used, or on rd_kafka_t destruction. This is used for ssl.certificate.pem, ssl.key.pem and ssl.key.password.
The ssl_cert_verify_cb was changed to provide more information to the callback, including the full X509_STORE_CTX.
Add integration tests.

To be done:

Rework the Windows SSL cert store example to its own file.",@edenhill Thanks for all your help getting this completed.  This will be great to get into next release.,True,"{'THUMBS_UP': ['https://github.com/raj1003', 'https://github.com/edenhill']}"
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2309,2019-05-02T20:14:55Z,2019-06-03T21:16:10Z,2020-05-08T08:58:59Z,MERGED,True,3716,921,40,https://github.com/edenhill,Continuing on PR #1882: SSL cert verification and in-memory keys,14,[],https://github.com/edenhill/librdkafka/pull/2309,https://github.com/edenhill,66,https://github.com/edenhill/librdkafka/pull/2309#issuecomment-497630794,"This PR is a continuation on @noahdav's work on PR #1882.
Summary of changes compared to #1882:

The retrieve_cb() has been removed in favour of using standard configuration properties. The reasoning behind this s simplification, reduced code size, cross-language maintenance (some high-level languages, such as Go, can't use callbacks in a good way), and memory ownership issues (having the callback return cert memory to librdkafka in an effective and allocator-agnostic way is tricky).
Refactored the code and made it more in line with the librdkafka style guide.
Values of security-sensitive configuration properties are now cleared from memory when the value is no longer used, or on rd_kafka_t destruction. This is used for ssl.certificate.pem, ssl.key.pem and ssl.key.password.
The ssl_cert_verify_cb was changed to provide more information to the callback, including the full X509_STORE_CTX.
Add integration tests.

To be done:

Rework the Windows SSL cert store example to its own file.",@noahdav Did you get a chance to review?,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2309,2019-05-02T20:14:55Z,2019-06-03T21:16:10Z,2020-05-08T08:58:59Z,MERGED,True,3716,921,40,https://github.com/edenhill,Continuing on PR #1882: SSL cert verification and in-memory keys,14,[],https://github.com/edenhill/librdkafka/pull/2309,https://github.com/noahdav,67,https://github.com/edenhill/librdkafka/pull/2309#issuecomment-497736066,"This PR is a continuation on @noahdav's work on PR #1882.
Summary of changes compared to #1882:

The retrieve_cb() has been removed in favour of using standard configuration properties. The reasoning behind this s simplification, reduced code size, cross-language maintenance (some high-level languages, such as Go, can't use callbacks in a good way), and memory ownership issues (having the callback return cert memory to librdkafka in an effective and allocator-agnostic way is tricky).
Refactored the code and made it more in line with the librdkafka style guide.
Values of security-sensitive configuration properties are now cleared from memory when the value is no longer used, or on rd_kafka_t destruction. This is used for ssl.certificate.pem, ssl.key.pem and ssl.key.password.
The ssl_cert_verify_cb was changed to provide more information to the callback, including the full X509_STORE_CTX.
Add integration tests.

To be done:

Rework the Windows SSL cert store example to its own file.",@edenhill yes it looks good.  I did already approve the PR.,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2309,2019-05-02T20:14:55Z,2019-06-03T21:16:10Z,2020-05-08T08:58:59Z,MERGED,True,3716,921,40,https://github.com/edenhill,Continuing on PR #1882: SSL cert verification and in-memory keys,14,[],https://github.com/edenhill/librdkafka/pull/2309,https://github.com/edenhill,68,https://github.com/edenhill/librdkafka/pull/2309#issuecomment-498365533,"This PR is a continuation on @noahdav's work on PR #1882.
Summary of changes compared to #1882:

The retrieve_cb() has been removed in favour of using standard configuration properties. The reasoning behind this s simplification, reduced code size, cross-language maintenance (some high-level languages, such as Go, can't use callbacks in a good way), and memory ownership issues (having the callback return cert memory to librdkafka in an effective and allocator-agnostic way is tricky).
Refactored the code and made it more in line with the librdkafka style guide.
Values of security-sensitive configuration properties are now cleared from memory when the value is no longer used, or on rd_kafka_t destruction. This is used for ssl.certificate.pem, ssl.key.pem and ssl.key.password.
The ssl_cert_verify_cb was changed to provide more information to the callback, including the full X509_STORE_CTX.
Add integration tests.

To be done:

Rework the Windows SSL cert store example to its own file.","@noahdav Oh, sorry, thanks!",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2309,2019-05-02T20:14:55Z,2019-06-03T21:16:10Z,2020-05-08T08:58:59Z,MERGED,True,3716,921,40,https://github.com/edenhill,Continuing on PR #1882: SSL cert verification and in-memory keys,14,[],https://github.com/edenhill/librdkafka/pull/2309,https://github.com/edenhill,69,https://github.com/edenhill/librdkafka/pull/2309#issuecomment-498430375,"This PR is a continuation on @noahdav's work on PR #1882.
Summary of changes compared to #1882:

The retrieve_cb() has been removed in favour of using standard configuration properties. The reasoning behind this s simplification, reduced code size, cross-language maintenance (some high-level languages, such as Go, can't use callbacks in a good way), and memory ownership issues (having the callback return cert memory to librdkafka in an effective and allocator-agnostic way is tricky).
Refactored the code and made it more in line with the librdkafka style guide.
Values of security-sensitive configuration properties are now cleared from memory when the value is no longer used, or on rd_kafka_t destruction. This is used for ssl.certificate.pem, ssl.key.pem and ssl.key.password.
The ssl_cert_verify_cb was changed to provide more information to the callback, including the full X509_STORE_CTX.
Add integration tests.

To be done:

Rework the Windows SSL cert store example to its own file.","@noahdav
Thanks a lot for all the time and effort you've spent on this PR, from the code contribution, to reviews discussions, testing, and verification, it has been hugely valuable! 
We're looking to make a release including this changes before end of June.",True,{'THUMBS_UP': ['https://github.com/LighthouseJ']}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2309,2019-05-02T20:14:55Z,2019-06-03T21:16:10Z,2020-05-08T08:58:59Z,MERGED,True,3716,921,40,https://github.com/edenhill,Continuing on PR #1882: SSL cert verification and in-memory keys,14,[],https://github.com/edenhill/librdkafka/pull/2309,https://github.com/noahdav,70,https://github.com/edenhill/librdkafka/pull/2309#issuecomment-498442419,"This PR is a continuation on @noahdav's work on PR #1882.
Summary of changes compared to #1882:

The retrieve_cb() has been removed in favour of using standard configuration properties. The reasoning behind this s simplification, reduced code size, cross-language maintenance (some high-level languages, such as Go, can't use callbacks in a good way), and memory ownership issues (having the callback return cert memory to librdkafka in an effective and allocator-agnostic way is tricky).
Refactored the code and made it more in line with the librdkafka style guide.
Values of security-sensitive configuration properties are now cleared from memory when the value is no longer used, or on rd_kafka_t destruction. This is used for ssl.certificate.pem, ssl.key.pem and ssl.key.password.
The ssl_cert_verify_cb was changed to provide more information to the callback, including the full X509_STORE_CTX.
Add integration tests.

To be done:

Rework the Windows SSL cert store example to its own file.",@edenhill My pleasure.  I am excited to have this in the community release.  Thank you too for all your help with this.,True,"{'HEART': ['https://github.com/edenhill', 'https://github.com/uniqueumang'], 'THUMBS_UP': ['https://github.com/LighthouseJ']}"
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2309,2019-05-02T20:14:55Z,2019-06-03T21:16:10Z,2020-05-08T08:58:59Z,MERGED,True,3716,921,40,https://github.com/edenhill,Continuing on PR #1882: SSL cert verification and in-memory keys,14,[],https://github.com/edenhill/librdkafka/pull/2309,https://github.com/larsbrekken,71,https://github.com/edenhill/librdkafka/pull/2309#issuecomment-517803316,"This PR is a continuation on @noahdav's work on PR #1882.
Summary of changes compared to #1882:

The retrieve_cb() has been removed in favour of using standard configuration properties. The reasoning behind this s simplification, reduced code size, cross-language maintenance (some high-level languages, such as Go, can't use callbacks in a good way), and memory ownership issues (having the callback return cert memory to librdkafka in an effective and allocator-agnostic way is tricky).
Refactored the code and made it more in line with the librdkafka style guide.
Values of security-sensitive configuration properties are now cleared from memory when the value is no longer used, or on rd_kafka_t destruction. This is used for ssl.certificate.pem, ssl.key.pem and ssl.key.password.
The ssl_cert_verify_cb was changed to provide more information to the callback, including the full X509_STORE_CTX.
Add integration tests.

To be done:

Rework the Windows SSL cert store example to its own file.","Hi guys, this functionality has worked great for us when running our code from a Windows Service with extensive permissions, however when running it from a web site with a default App Pool Identity user, we get the following permissions-related error:
DEBUG|[thrd:app]: librdkafka built with OpenSSL version 0x1000212f 
DEBUG|[thrd:app]: Failed to open Windows certificate Root store: Access is denied...: falling back to OpenSSL default CA paths

I am not familiar with the Windows system calls for accessing the store, but in the docs I see a mention of a store called ""Ca"" in addition to the ""Root"" store that librdkafka uses.
https://docs.microsoft.com/en-us/windows/win32/api/wincrypt/nf-wincrypt-certopenstore
That page also mentions a simpler call, and the example on that page uses the ""Ca"" store.
Again, not an expert on this and the ""Root"" store might be the right one for librdkafka.
Unfortunately the permissions issue is blocking us from rolling out with the new functionality and have to stick with referencing a CA file on disk.",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2309,2019-05-02T20:14:55Z,2019-06-03T21:16:10Z,2020-05-08T08:58:59Z,MERGED,True,3716,921,40,https://github.com/edenhill,Continuing on PR #1882: SSL cert verification and in-memory keys,14,[],https://github.com/edenhill/librdkafka/pull/2309,https://github.com/edenhill,72,https://github.com/edenhill/librdkafka/pull/2309#issuecomment-517805353,"This PR is a continuation on @noahdav's work on PR #1882.
Summary of changes compared to #1882:

The retrieve_cb() has been removed in favour of using standard configuration properties. The reasoning behind this s simplification, reduced code size, cross-language maintenance (some high-level languages, such as Go, can't use callbacks in a good way), and memory ownership issues (having the callback return cert memory to librdkafka in an effective and allocator-agnostic way is tricky).
Refactored the code and made it more in line with the librdkafka style guide.
Values of security-sensitive configuration properties are now cleared from memory when the value is no longer used, or on rd_kafka_t destruction. This is used for ssl.certificate.pem, ssl.key.pem and ssl.key.password.
The ssl_cert_verify_cb was changed to provide more information to the callback, including the full X509_STORE_CTX.
Add integration tests.

To be done:

Rework the Windows SSL cert store example to its own file.","@larsbrekken Try changing this line to ""Ca"" and see if it helps:
https://github.com/edenhill/librdkafka/blob/master/src/rdkafka_ssl.c#L658
If so we can make the store name configurable.",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2309,2019-05-02T20:14:55Z,2019-06-03T21:16:10Z,2020-05-08T08:58:59Z,MERGED,True,3716,921,40,https://github.com/edenhill,Continuing on PR #1882: SSL cert verification and in-memory keys,14,[],https://github.com/edenhill/librdkafka/pull/2309,https://github.com/noahdav,73,https://github.com/edenhill/librdkafka/pull/2309#issuecomment-526480917,"This PR is a continuation on @noahdav's work on PR #1882.
Summary of changes compared to #1882:

The retrieve_cb() has been removed in favour of using standard configuration properties. The reasoning behind this s simplification, reduced code size, cross-language maintenance (some high-level languages, such as Go, can't use callbacks in a good way), and memory ownership issues (having the callback return cert memory to librdkafka in an effective and allocator-agnostic way is tricky).
Refactored the code and made it more in line with the librdkafka style guide.
Values of security-sensitive configuration properties are now cleared from memory when the value is no longer used, or on rd_kafka_t destruction. This is used for ssl.certificate.pem, ssl.key.pem and ssl.key.password.
The ssl_cert_verify_cb was changed to provide more information to the callback, including the full X509_STORE_CTX.
Add integration tests.

To be done:

Rework the Windows SSL cert store example to its own file.","@LighthouseJ Can you share what you did to get the capi working?  We have a new requirement which does not allow us to export the private key to feed into OpenSSL.  Therefore we need to use capi to access the cert store directly.  I have not seen many examples of using this so I was hoping to see what you did in order to get this working.  We will also need to wire this up into librdkafka.  I am hoping to build on top of what you have already done.
Thanks",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2309,2019-05-02T20:14:55Z,2019-06-03T21:16:10Z,2020-05-08T08:58:59Z,MERGED,True,3716,921,40,https://github.com/edenhill,Continuing on PR #1882: SSL cert verification and in-memory keys,14,[],https://github.com/edenhill/librdkafka/pull/2309,https://github.com/LighthouseJ,74,https://github.com/edenhill/librdkafka/pull/2309#issuecomment-526919510,"This PR is a continuation on @noahdav's work on PR #1882.
Summary of changes compared to #1882:

The retrieve_cb() has been removed in favour of using standard configuration properties. The reasoning behind this s simplification, reduced code size, cross-language maintenance (some high-level languages, such as Go, can't use callbacks in a good way), and memory ownership issues (having the callback return cert memory to librdkafka in an effective and allocator-agnostic way is tricky).
Refactored the code and made it more in line with the librdkafka style guide.
Values of security-sensitive configuration properties are now cleared from memory when the value is no longer used, or on rd_kafka_t destruction. This is used for ssl.certificate.pem, ssl.key.pem and ssl.key.password.
The ssl_cert_verify_cb was changed to provide more information to the callback, including the full X509_STORE_CTX.
Add integration tests.

To be done:

Rework the Windows SSL cert store example to its own file.","@noahdav I was sure that I shared details on this... but I can't find it in this PR or your original.  I'm not at work now so I can't provide details now, but I will soon.
I vaguely outlined what you're looking for in this comment:
#2309 (comment)
In your PR, you allowed a user who has access to a soft cert to read and provide it to librdkafka.  With a smartcard or softcerts in Windows cert manager, you don't have access to read the private key, but with OpenSSL (and Windows CAPI), you can get an opaque object that points to the private key, and then use it within librdkafka.  My hack was to your original changeset, where the retrieve_cb could return a special payload result, instead of a PEM file, is a pointer to the OpenSSL private key opaque object.  The library was slightly modified to also look for a special (short) payload, and if found, cast as a pointer, do a check for validity, then call openssl to use it as the private key.
The essential change needed to librdkafka would be to modify the current function where retrieve_cb() used to be to return different types of private keys, so not just a PEM cert in a byte buffer, but also a private key object (a native OpenSSL object?)
Give me a few days and I'll reply with info on how to query capi w/ capi ui chooser dialog (happens in my implementation of your retrieve_cb()) and what I did to wire it into rdkafka to use it when authenticating with openssl.
I know retrieve_cb() was not kept in the final changes by Magnus, butI haven't looked into how difficult it would be to port the change that we probably both need into current rdkafka, but I hope to give you enough info to try and make it in.",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2309,2019-05-02T20:14:55Z,2019-06-03T21:16:10Z,2020-05-08T08:58:59Z,MERGED,True,3716,921,40,https://github.com/edenhill,Continuing on PR #1882: SSL cert verification and in-memory keys,14,[],https://github.com/edenhill/librdkafka/pull/2309,https://github.com/LighthouseJ,75,https://github.com/edenhill/librdkafka/pull/2309#issuecomment-528546693,"This PR is a continuation on @noahdav's work on PR #1882.
Summary of changes compared to #1882:

The retrieve_cb() has been removed in favour of using standard configuration properties. The reasoning behind this s simplification, reduced code size, cross-language maintenance (some high-level languages, such as Go, can't use callbacks in a good way), and memory ownership issues (having the callback return cert memory to librdkafka in an effective and allocator-agnostic way is tricky).
Refactored the code and made it more in line with the librdkafka style guide.
Values of security-sensitive configuration properties are now cleared from memory when the value is no longer used, or on rd_kafka_t destruction. This is used for ssl.certificate.pem, ssl.key.pem and ssl.key.password.
The ssl_cert_verify_cb was changed to provide more information to the callback, including the full X509_STORE_CTX.
Add integration tests.

To be done:

Rework the Windows SSL cert store example to its own file.","@noahdav
Here's the critical part.  In your branch, in the rd_kafka_transport_set_client_certificates() function, right after you call the user callback and you added:
BIO* bio = BIO_new_mem_buf(buffer, (long)len);
if (bio) {
    PKCS12 *p12 = d2i_PKCS12_bio(bio, NULL);
    ...
    < read file into BIO, use as private key if parse OK >
} else
    rd_snprintf(errstr, errstr_size,
        ""Failed to initialize BIO"");

Before that part, I patched it to do this:
if (len == 1) // *buffer points to a prepared EVP_PKEY pointer loses efficacy when serialized to a buffer
{
    rd_kafka_dbg(rk, SECURITY, ""SSL"",
        ""Using an opaque EVP_PKEY pointer as the private key"");

    EVP_PKEY * pkey = (EVP_PKEY *)buffer;

    r = SSL_CTX_use_PrivateKey(ctx, pkey);

    if (r != 1) { return -1; }

    rd_kafka_dbg(rk, SECURITY, ""SSL"",
        ""Checking client private key"");

    r = SSL_CTX_check_private_key(ctx);
    if (r != 1) { return -1; }
				
    rd_kafka_dbg(rk, SECURITY, ""SSL"", ""Private key OK"");
} // if (len == 1)
else // Assume the user passed a larger buffer and read normally
{ // do the rest of your code
    if (bio) {
        PKCS12 *p12 = d2i_PKCS12_bio(bio, NULL);
        ...
        < read file into BIO, use as private key if parse OK >
    } else
        rd_snprintf(errstr, errstr_size,
            ""Failed to initialize BIO"");

Here's how I initialize OpenSSL engine w/ capi:
// types in header implementation of your RdKafka::SslCertificateRetrieveCb class
    ENGINE *    ptr_engine;

    X509    *   ptr_client_cert;

    EVP_PKEY *  ptr_private_key;
// code to initialize in .cpp file
    ENGINE_load_builtin_engines();
    ENGINE_load_dynamic();
    ERR_clear_error();
    ptr_engine = ENGINE_by_id(""dynamic"");

    if (ptr_engine)
    {
        ENGINE_ctrl_cmd_string(ptr_engine, ""SO_PATH"", ""./capi.dll"", 0);
        ENGINE_ctrl_cmd_string(ptr_engine, ""ID"", ""capi"", 0);
        ENGINE_ctrl_cmd_string(ptr_engine, ""LIST_ADD"", ""1"", 0);
        ENGINE_ctrl_cmd_string(ptr_engine, ""LOAD"", NULL, 0);

        ENGINE_init(ptr_engine);

        // now loaded
    }
    else
    { // failed to load the dynamic engine
    }

I do a one-time lazy init at the top of your ssl_cert_retrieve_cb:
    if ((ptr_client_cert == NULL) || (ptr_private_key == NULL))
    {
        int load_result =
            ENGINE_load_ssl_client_cert(ptr_engine, NULL, NULL,
                &ptr_client_cert, &ptr_private_key,
                NULL, NULL, NULL);
        // log load_result to help diagnose

        if (load_result != 0) // 0 == user canceled, 1 == user selected a cert
        {
            // Loading user certificate info OK
        }
    }

And in the body of ssl_cert_retrieve_cb, I do this:
    switch (type)
    {
    case CERTIFICATE_PUBLIC_KEY:
    {
        if (ptr_client_cert)
        {
            // Loading public key...

            // need to convert the provided public key ""internal"" openssl format
            // to 'encoded' DER-formatted certificate
            int size_encoded =
                i2d_X509(ptr_client_cert, (unsigned char **)buffer);

            // Encoded <size_encoded> bytes into the outgoing buffer

            return (size_t)size_encoded;
        }
        else
        {
            // Unable to load public key because the client certificate was invalid
        }

        break;
    } // case CERTIFICATE_PUBLIC_KEY:
    case CERTIFICATE_PRIVATE_KEY:
    {
        if (ptr_private_key)
        {
            // Pushing private key pointer to kafka library

            // copies the pointer to the private key to the first character
            *buffer = (char *)ptr_private_key;
            return 1;
        }
        else
        {
            // Unable to find the private key
        }

        break;
    } // case CERTIFICATE_PRIVATE_KEY:

    case CERTIFICATE_PRIVATE_KEY_PASS:
        // do nothing
        break;
        
    } // switch (type)
    // default case -- return blanks!
    *buffer = NULL;
    return 0;
}

And there's appropriate cleanup...
ENGINE_load_ssl_client_cert(...) was the site where the user was asked for their cert, and saves what they chose to the member variables, and pointers to client cert and key are passed to the patch of librdkafka to use as-is without passing through BIO or any conversion.
To get the same level of function, you'll have to compile openssl with capi engine, and put capi.dll in the same directory as the executable running per the SO_PATH used.  Also, the certificate chooser dialog is not shown by default, you have to compile openssl w/ OPENSSL_CAPIENG_DIALOG preprocessor defined else capi will just choose the first cert/key pair found which may not be what you want.",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2309,2019-05-02T20:14:55Z,2019-06-03T21:16:10Z,2020-05-08T08:58:59Z,MERGED,True,3716,921,40,https://github.com/edenhill,Continuing on PR #1882: SSL cert verification and in-memory keys,14,[],https://github.com/edenhill/librdkafka/pull/2309,https://github.com/noahdav,76,https://github.com/edenhill/librdkafka/pull/2309#issuecomment-528574104,"This PR is a continuation on @noahdav's work on PR #1882.
Summary of changes compared to #1882:

The retrieve_cb() has been removed in favour of using standard configuration properties. The reasoning behind this s simplification, reduced code size, cross-language maintenance (some high-level languages, such as Go, can't use callbacks in a good way), and memory ownership issues (having the callback return cert memory to librdkafka in an effective and allocator-agnostic way is tricky).
Refactored the code and made it more in line with the librdkafka style guide.
Values of security-sensitive configuration properties are now cleared from memory when the value is no longer used, or on rd_kafka_t destruction. This is used for ssl.certificate.pem, ssl.key.pem and ssl.key.password.
The ssl_cert_verify_cb was changed to provide more information to the callback, including the full X509_STORE_CTX.
Add integration tests.

To be done:

Rework the Windows SSL cert store example to its own file.",@LighthouseJ Thank you for this.  It looks very helpful.  I will go through this and let you know what I come up with.,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2310,2019-05-07T10:31:15Z,2019-05-07T11:14:16Z,2019-05-07T11:15:09Z,MERGED,True,1,4,2,https://github.com/myd7349,Minor fixes for Win32,1,[],https://github.com/edenhill/librdkafka/pull/2310,https://github.com/myd7349,1,https://github.com/edenhill/librdkafka/pull/2310,"Remove redundant LIBRDKAFKACPP_EXPORTS definition in tests missed in #2274
I want to apologize for it since I should have fixed it in #2274. But I didn't run the tests before: https://github.com/myd7349/vcpkg/blob/8aeafa192dc621f17be56108cabcc556fe6522b9/ports/librdkafka/portfile.cmake#L32, so I just missed it at that time.
We can reproduce the link error caused by this definition on Win32 by:
mkdir build
cd build
cmake ..
cmake --build .



Fix C4996 warning related to strdup
When I was trying to build librdkafka on UWP, a C4996 error is reported by VS:

e:\vcpkg\buildtrees\librdkafka\src\a34ca9b746-a24f4e04b2\src\rdkafka_queue.h(652): error C4996: 'strdup': The POSIX name for this item is deprecated. Instead, use the ISO C and C++ conformant name: _strdup. See online help for details. (compiling source file

Though the UWP build is still failed for some other reasons(install-x86-uwp-dbg-out.log), I think this fix is reasonable since all other places in the code use rd_strdup instead of strdup.","Remove redundant LIBRDKAFKACPP_EXPORTS definition in tests missed in #2274
I want to apologize for it since I should have fixed it in #2274. But I didn't run the tests before: https://github.com/myd7349/vcpkg/blob/8aeafa192dc621f17be56108cabcc556fe6522b9/ports/librdkafka/portfile.cmake#L32, so I just missed it at that time.
We can reproduce the link error caused by this definition on Win32 by:
mkdir build
cd build
cmake ..
cmake --build .



Fix C4996 warning related to strdup
When I was trying to build librdkafka on UWP, a C4996 error is reported by VS:

e:\vcpkg\buildtrees\librdkafka\src\a34ca9b746-a24f4e04b2\src\rdkafka_queue.h(652): error C4996: 'strdup': The POSIX name for this item is deprecated. Instead, use the ISO C and C++ conformant name: _strdup. See online help for details. (compiling source file

Though the UWP build is still failed for some other reasons(install-x86-uwp-dbg-out.log), I think this fix is reasonable since all other places in the code use rd_strdup instead of strdup.",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2310,2019-05-07T10:31:15Z,2019-05-07T11:14:16Z,2019-05-07T11:15:09Z,MERGED,True,1,4,2,https://github.com/myd7349,Minor fixes for Win32,1,[],https://github.com/edenhill/librdkafka/pull/2310,https://github.com/edenhill,2,https://github.com/edenhill/librdkafka/pull/2310#issuecomment-490036272,"Remove redundant LIBRDKAFKACPP_EXPORTS definition in tests missed in #2274
I want to apologize for it since I should have fixed it in #2274. But I didn't run the tests before: https://github.com/myd7349/vcpkg/blob/8aeafa192dc621f17be56108cabcc556fe6522b9/ports/librdkafka/portfile.cmake#L32, so I just missed it at that time.
We can reproduce the link error caused by this definition on Win32 by:
mkdir build
cd build
cmake ..
cmake --build .



Fix C4996 warning related to strdup
When I was trying to build librdkafka on UWP, a C4996 error is reported by VS:

e:\vcpkg\buildtrees\librdkafka\src\a34ca9b746-a24f4e04b2\src\rdkafka_queue.h(652): error C4996: 'strdup': The POSIX name for this item is deprecated. Instead, use the ISO C and C++ conformant name: _strdup. See online help for details. (compiling source file

Though the UWP build is still failed for some other reasons(install-x86-uwp-dbg-out.log), I think this fix is reasonable since all other places in the code use rd_strdup instead of strdup.",Thanks!,True,{'HEART': ['https://github.com/myd7349']}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2311,2019-05-07T12:06:03Z,2019-05-07T12:56:11Z,2019-05-07T13:10:59Z,MERGED,True,8,2,1,https://github.com/souradeep100,#1550 fix,1,[],https://github.com/edenhill/librdkafka/pull/2311,https://github.com/souradeep100,1,https://github.com/edenhill/librdkafka/pull/2311,"Fix for Misleading ""All messages delivered!"" message. Earlier pull request was from a old fork , so created a fresh fork and
commiting.","Fix for Misleading ""All messages delivered!"" message. Earlier pull request was from a old fork , so created a fresh fork and
commiting.",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2311,2019-05-07T12:06:03Z,2019-05-07T12:56:11Z,2019-05-07T13:10:59Z,MERGED,True,8,2,1,https://github.com/souradeep100,#1550 fix,1,[],https://github.com/edenhill/librdkafka/pull/2311,https://github.com/edenhill,2,https://github.com/edenhill/librdkafka/pull/2311#issuecomment-490067723,"Fix for Misleading ""All messages delivered!"" message. Earlier pull request was from a old fork , so created a fresh fork and
commiting.",Thank you!,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2311,2019-05-07T12:06:03Z,2019-05-07T12:56:11Z,2019-05-07T13:10:59Z,MERGED,True,8,2,1,https://github.com/souradeep100,#1550 fix,1,[],https://github.com/edenhill/librdkafka/pull/2311,https://github.com/souradeep100,3,https://github.com/edenhill/librdkafka/pull/2311#issuecomment-490072875,"Fix for Misleading ""All messages delivered!"" message. Earlier pull request was from a old fork , so created a fresh fork and
commiting.",thanks :),True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2313,2019-05-07T13:22:11Z,2019-05-07T13:29:32Z,2019-05-07T13:29:45Z,CLOSED,False,10,2,2,https://github.com/souradeep100,#1974 fix,2,[],https://github.com/edenhill/librdkafka/pull/2313,https://github.com/souradeep100,1,https://github.com/edenhill/librdkafka/pull/2313,https://github.com/edenhill/librdkafka/commits?author=souradeep100,https://github.com/edenhill/librdkafka/commits?author=souradeep100,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2313,2019-05-07T13:22:11Z,2019-05-07T13:29:32Z,2019-05-07T13:29:45Z,CLOSED,False,10,2,2,https://github.com/souradeep100,#1974 fix,2,[],https://github.com/edenhill/librdkafka/pull/2313,https://github.com/edenhill,2,https://github.com/edenhill/librdkafka/pull/2313#issuecomment-490079506,https://github.com/edenhill/librdkafka/commits?author=souradeep100,"This PR includes some stuff from #1550 which has already been merged, remove that and rebase off latest master",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2313,2019-05-07T13:22:11Z,2019-05-07T13:29:32Z,2019-05-07T13:29:45Z,CLOSED,False,10,2,2,https://github.com/souradeep100,#1974 fix,2,[],https://github.com/edenhill/librdkafka/pull/2313,https://github.com/souradeep100,3,https://github.com/edenhill/librdkafka/pull/2313#issuecomment-490079907,https://github.com/edenhill/librdkafka/commits?author=souradeep100,I will do that . Thanks,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2314,2019-05-08T05:52:52Z,2019-05-08T11:28:15Z,2019-05-08T11:42:42Z,MERGED,True,13,0,1,https://github.com/souradeep100,#1974 fix,5,[],https://github.com/edenhill/librdkafka/pull/2314,https://github.com/souradeep100,1,https://github.com/edenhill/librdkafka/pull/2314,Resynced the fork and created new pull request for the bug #1974,Resynced the fork and created new pull request for the bug #1974,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2314,2019-05-08T05:52:52Z,2019-05-08T11:28:15Z,2019-05-08T11:42:42Z,MERGED,True,13,0,1,https://github.com/souradeep100,#1974 fix,5,[],https://github.com/edenhill/librdkafka/pull/2314,https://github.com/souradeep100,2,https://github.com/edenhill/librdkafka/pull/2314#issuecomment-490377810,Resynced the fork and created new pull request for the bug #1974,"Sure , will do and update.

On Wed, May 8, 2019 at 12:48 PM Magnus Edenhill ***@***.***> wrote:
 ***@***.**** requested changes on this pull request.

 Good find!
 Can you fix this for all these Request functions for:
 CreateTopics, DeleteTopics, CreatePartitions, AlterConfigs,
 DescribeConfigs,
 ------------------------------

 In src/rdkafka_request.c
 <#2314 (comment)>:

 > @@ -3208,6 +3208,7 @@ rd_kafka_DescribeConfigsRequest (rd_kafka_broker_t *rkb,
          if (rd_list_cnt(configs) == 0) {
                  rd_snprintf(errstr, errstr_size,
                              ""No config resources specified"");
 +		rd_kafka_replyq_destroy(&replyq);

 Please use 8 whitespace as indent, not tabs.

 
 You are receiving this because you authored the thread.
 Reply to this email directly, view it on GitHub
 <#2314 (review)>,
 or mute the thread
 <https://github.com/notifications/unsubscribe-auth/ACW5ORLYM56FLTSU7UPIONDPUJ5FDANCNFSM4HLOTFNQ>
 .


-- 
Thanks & Regards,
Souradeep
Mob: 09663082628
Bangalore, India",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2314,2019-05-08T05:52:52Z,2019-05-08T11:28:15Z,2019-05-08T11:42:42Z,MERGED,True,13,0,1,https://github.com/souradeep100,#1974 fix,5,[],https://github.com/edenhill/librdkafka/pull/2314,https://github.com/souradeep100,3,https://github.com/edenhill/librdkafka/pull/2314#issuecomment-490414118,Resynced the fork and created new pull request for the bug #1974,added the changes .,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2314,2019-05-08T05:52:52Z,2019-05-08T11:28:15Z,2019-05-08T11:42:42Z,MERGED,True,13,0,1,https://github.com/souradeep100,#1974 fix,5,[],https://github.com/edenhill/librdkafka/pull/2314,https://github.com/edenhill,4,https://github.com/edenhill/librdkafka/pull/2314#issuecomment-490450092,Resynced the fork and created new pull request for the bug #1974,Thanks alot!,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2314,2019-05-08T05:52:52Z,2019-05-08T11:28:15Z,2019-05-08T11:42:42Z,MERGED,True,13,0,1,https://github.com/souradeep100,#1974 fix,5,[],https://github.com/edenhill/librdkafka/pull/2314,https://github.com/souradeep100,5,https://github.com/edenhill/librdkafka/pull/2314#issuecomment-490453606,Resynced the fork and created new pull request for the bug #1974,:),True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2316,2019-05-08T08:29:59Z,2019-05-09T09:50:38Z,2019-05-22T19:57:30Z,MERGED,True,25,7,3,https://github.com/edenhill,Don't let active logical broker connections hinder new connects (#2266),1,[],https://github.com/edenhill/librdkafka/pull/2316,https://github.com/edenhill,1,https://github.com/edenhill/librdkafka/pull/2316,"The logical broker connections (such as to the group coordinator)
are reserved for specific use and shall not be reused for things like
Metadata requests. The code to lookup a usable broker to send
Metadata requests understood this, but the code to select a broker
to create a new connection to if no usable connections are
found (sparse connnections) did not understand this and already
thought there was an active connection (the logical broker connection)
and refused to set up a new one.
This could lead to consumers stalling consumption if it was fetching
from a single broker and that broker went down, no new connection
to the cluster would be made to refresh metadata.
This fixes #2266","The logical broker connections (such as to the group coordinator)
are reserved for specific use and shall not be reused for things like
Metadata requests. The code to lookup a usable broker to send
Metadata requests understood this, but the code to select a broker
to create a new connection to if no usable connections are
found (sparse connnections) did not understand this and already
thought there was an active connection (the logical broker connection)
and refused to set up a new one.
This could lead to consumers stalling consumption if it was fetching
from a single broker and that broker went down, no new connection
to the cluster would be made to refresh metadata.
This fixes #2266",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2316,2019-05-08T08:29:59Z,2019-05-09T09:50:38Z,2019-05-22T19:57:30Z,MERGED,True,25,7,3,https://github.com/edenhill,Don't let active logical broker connections hinder new connects (#2266),1,[],https://github.com/edenhill/librdkafka/pull/2316,https://github.com/dubee,2,https://github.com/edenhill/librdkafka/pull/2316#issuecomment-494913341,"The logical broker connections (such as to the group coordinator)
are reserved for specific use and shall not be reused for things like
Metadata requests. The code to lookup a usable broker to send
Metadata requests understood this, but the code to select a broker
to create a new connection to if no usable connections are
found (sparse connnections) did not understand this and already
thought there was an active connection (the logical broker connection)
and refused to set up a new one.
This could lead to consumers stalling consumption if it was fetching
from a single broker and that broker went down, no new connection
to the cluster would be made to refresh metadata.
This fixes #2266","@edenhill, this seems like a fairly important fix. Are you going to release 1.0.1 with this fix in it soon?
Curious if this bug would cause poll() to block indefinitely?",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2316,2019-05-08T08:29:59Z,2019-05-09T09:50:38Z,2019-05-22T19:57:30Z,MERGED,True,25,7,3,https://github.com/edenhill,Don't let active logical broker connections hinder new connects (#2266),1,[],https://github.com/edenhill/librdkafka/pull/2316,https://github.com/edenhill,3,https://github.com/edenhill/librdkafka/pull/2316#issuecomment-494947347,"The logical broker connections (such as to the group coordinator)
are reserved for specific use and shall not be reused for things like
Metadata requests. The code to lookup a usable broker to send
Metadata requests understood this, but the code to select a broker
to create a new connection to if no usable connections are
found (sparse connnections) did not understand this and already
thought there was an active connection (the logical broker connection)
and refused to set up a new one.
This could lead to consumers stalling consumption if it was fetching
from a single broker and that broker went down, no new connection
to the cluster would be made to refresh metadata.
This fixes #2266","Yes, it is part of the v1.0.1 release, v1.0.1-RC1 was tagged yesterday, we're looking to do the final release later this week.
This bug could cause a consumer to not consume any messages, effectively making consumer_poll() block indefinitely",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2323,2019-05-11T06:52:29Z,,2019-08-01T09:50:06Z,OPEN,False,41,47,9,https://github.com/myd7349,CMake: Fix configuration on Win32,1,[],https://github.com/edenhill/librdkafka/pull/2323,https://github.com/myd7349,1,https://github.com/edenhill/librdkafka/pull/2323,"On Windows with MSVC, this line:
check_library_exists(m pow """" WITH_HDRHISTOGRAM)
causes WITH_HDRHISTOGRAM set to OFF.
As a result:

When specifying WITHOUT_WIN32_CONFIG=ON, the histogram source will not be built.
When specifying WITHOUT_WIN32_CONFIG=OFF, there will be link erros since WITH_HISTOGRAM is defined as 1 in win32_config.h

References:
https://stackoverflow.com/questions/32816646/can-cmake-detect-if-i-need-to-link-to-libm-when-using-pow-in-c","On Windows with MSVC, this line:
check_library_exists(m pow """" WITH_HDRHISTOGRAM)
causes WITH_HDRHISTOGRAM set to OFF.
As a result:

When specifying WITHOUT_WIN32_CONFIG=ON, the histogram source will not be built.
When specifying WITHOUT_WIN32_CONFIG=OFF, there will be link erros since WITH_HISTOGRAM is defined as 1 in win32_config.h

References:
https://stackoverflow.com/questions/32816646/can-cmake-detect-if-i-need-to-link-to-libm-when-using-pow-in-c",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2323,2019-05-11T06:52:29Z,,2019-08-01T09:50:06Z,OPEN,False,41,47,9,https://github.com/myd7349,CMake: Fix configuration on Win32,1,[],https://github.com/edenhill/librdkafka/pull/2323,https://github.com/myd7349,2,https://github.com/edenhill/librdkafka/pull/2323#issuecomment-491485618,"On Windows with MSVC, this line:
check_library_exists(m pow """" WITH_HDRHISTOGRAM)
causes WITH_HDRHISTOGRAM set to OFF.
As a result:

When specifying WITHOUT_WIN32_CONFIG=ON, the histogram source will not be built.
When specifying WITHOUT_WIN32_CONFIG=OFF, there will be link erros since WITH_HISTOGRAM is defined as 1 in win32_config.h

References:
https://stackoverflow.com/questions/32816646/can-cmake-detect-if-i-need-to-link-to-libm-when-using-pow-in-c","The configuration step on Win32 through CMake still have room for improvement. For example, the meaning of WITHOUT_WIN32_CONFIG is confusing
The doc string of this option is:

Avoid including win32_config.h on cmake builds

but rd.h will always include this header when built with MSVC.
I have a more aggressive patch here: myd7349@24965ee
If this patch is preferable, I'd like to add more commits to this PR.",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2323,2019-05-11T06:52:29Z,,2019-08-01T09:50:06Z,OPEN,False,41,47,9,https://github.com/myd7349,CMake: Fix configuration on Win32,1,[],https://github.com/edenhill/librdkafka/pull/2323,https://github.com/edenhill,3,https://github.com/edenhill/librdkafka/pull/2323#issuecomment-494299875,"On Windows with MSVC, this line:
check_library_exists(m pow """" WITH_HDRHISTOGRAM)
causes WITH_HDRHISTOGRAM set to OFF.
As a result:

When specifying WITHOUT_WIN32_CONFIG=ON, the histogram source will not be built.
When specifying WITHOUT_WIN32_CONFIG=OFF, there will be link erros since WITH_HISTOGRAM is defined as 1 in win32_config.h

References:
https://stackoverflow.com/questions/32816646/can-cmake-detect-if-i-need-to-link-to-libm-when-using-pow-in-c","I think I like the more aggressive approach, better to get this all right in one go.",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2323,2019-05-11T06:52:29Z,,2019-08-01T09:50:06Z,OPEN,False,41,47,9,https://github.com/myd7349,CMake: Fix configuration on Win32,1,[],https://github.com/edenhill/librdkafka/pull/2323,https://github.com/myd7349,4,https://github.com/edenhill/librdkafka/pull/2323#issuecomment-494372057,"On Windows with MSVC, this line:
check_library_exists(m pow """" WITH_HDRHISTOGRAM)
causes WITH_HDRHISTOGRAM set to OFF.
As a result:

When specifying WITHOUT_WIN32_CONFIG=ON, the histogram source will not be built.
When specifying WITHOUT_WIN32_CONFIG=OFF, there will be link erros since WITH_HISTOGRAM is defined as 1 in win32_config.h

References:
https://stackoverflow.com/questions/32816646/can-cmake-detect-if-i-need-to-link-to-libm-when-using-pow-in-c",Here is a related discussion about libm: mesonbuild/meson#5390 (comment),True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2323,2019-05-11T06:52:29Z,,2019-08-01T09:50:06Z,OPEN,False,41,47,9,https://github.com/myd7349,CMake: Fix configuration on Win32,1,[],https://github.com/edenhill/librdkafka/pull/2323,https://github.com/myd7349,5,https://github.com/edenhill/librdkafka/pull/2323#issuecomment-517131994,"On Windows with MSVC, this line:
check_library_exists(m pow """" WITH_HDRHISTOGRAM)
causes WITH_HDRHISTOGRAM set to OFF.
As a result:

When specifying WITHOUT_WIN32_CONFIG=ON, the histogram source will not be built.
When specifying WITHOUT_WIN32_CONFIG=OFF, there will be link erros since WITH_HISTOGRAM is defined as 1 in win32_config.h

References:
https://stackoverflow.com/questions/32816646/can-cmake-detect-if-i-need-to-link-to-libm-when-using-pow-in-c","snappy support doesn't work out-of-box, reported by @rotrida:
microsoft/vcpkg#7456
To enable it, we have to pass an extra option:
cmake -DWITH_SNAPPY=ON

the problem is that, WITH_SNAPPY is not an option defined like this:
option(WITH_SNAPPY ...)",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2323,2019-05-11T06:52:29Z,,2019-08-01T09:50:06Z,OPEN,False,41,47,9,https://github.com/myd7349,CMake: Fix configuration on Win32,1,[],https://github.com/edenhill/librdkafka/pull/2323,https://github.com/edenhill,6,https://github.com/edenhill/librdkafka/pull/2323#issuecomment-517213390,"On Windows with MSVC, this line:
check_library_exists(m pow """" WITH_HDRHISTOGRAM)
causes WITH_HDRHISTOGRAM set to OFF.
As a result:

When specifying WITHOUT_WIN32_CONFIG=ON, the histogram source will not be built.
When specifying WITHOUT_WIN32_CONFIG=OFF, there will be link erros since WITH_HISTOGRAM is defined as 1 in win32_config.h

References:
https://stackoverflow.com/questions/32816646/can-cmake-detect-if-i-need-to-link-to-libm-when-using-pow-in-c",@myd7349 WITH_SNAPPY should always be set to 1 in CMakeLists.txt,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2326,2019-05-14T07:45:49Z,2020-05-08T08:54:52Z,2020-05-08T08:54:52Z,CLOSED,False,10,4,1,https://github.com/kenneth-jia,rd_kafka_broker_handle_ApiVersion() should not call rd_kafka_broker_fail within bufq_timeout_scan,1,[],https://github.com/edenhill/librdkafka/pull/2326,https://github.com/kenneth-jia,1,https://github.com/edenhill/librdkafka/pull/2326,"Sometimes librdkafka would crash with
*** rdkafka_buf.c:201:rd_kafka_bufq_deq: assert: rd_atomic32_get(&rkbufq->rkbq_cnt) > 0 ***
(gdb) bt
#0  0x00007f09f26e2495 in raise () from /lib64/libc.so.6
#1  0x00007f09f26e3c75 in abort () from /lib64/libc.so.6
#2  0x00007f09f3410353 in rd_kafka_crash (file=file@entry=0x7f09f34caf7f ""rdkafka_buf.c"", line=line@entry=197, function=function@entry=0x7f09f34cb130 <__FUNCTION__.22201> ""rd_kafka_bufq_deq"", rk=rk@entry=0x0,
    reason=reason@entry=0x7f09f34caff8 ""assert: rd_atomic32_get(&rkbufq->rkbq_cnt) > 0"") at rdkafka.c:3432
#3  0x00007f09f343b975 in rd_kafka_bufq_deq (rkbufq=rkbufq@entry=0xb32180, rkbuf=rkbuf@entry=0xb36050) at rdkafka_buf.c:197
#4  0x00007f09f342300b in rd_kafka_broker_bufq_timeout_scan (rkb=rkb@entry=0xb31fb0, is_waitresp_q=is_waitresp_q@entry=0, rkbq=rkbq@entry=0xb32180, partial_cntp=partial_cntp@entry=0x7f09ee56125c,
    err=err@entry=RD_KAFKA_RESP_ERR__TIMED_OUT_QUEUE, now=now@entry=215591720139) at rdkafka_broker.c:558
#5  0x00007f09f34291ed in rd_kafka_broker_timeout_scan (now=215591720139, rkb=0xb31fb0) at rdkafka_broker.c:594
#6  rd_kafka_broker_serve (rkb=rkb@entry=0xb31fb0, abs_timeout=abs_timeout@entry=215591720118) at rdkafka_broker.c:2562
#7  0x00007f09f3429597 in rd_kafka_broker_ua_idle (rkb=rkb@entry=0xb31fb0, timeout_ms=<optimized out>, timeout_ms@entry=-1) at rdkafka_broker.c:2617
#8  0x00007f09f3429a24 in rd_kafka_broker_thread_main (arg=arg@entry=0xb31fb0) at rdkafka_broker.c:3552
#9  0x00007f09f34763d7 in _thrd_wrapper_function (aArg=<optimized out>) at tinycthread.c:583
#10 0x00007f09f2a4baa1 in start_thread () from /lib64/libpthread.so.0
#11 0x00007f09f2798bdd in clone () from /lib64/libc.so.6

(gdb) f 4
#4  0x00007f09f342300b in rd_kafka_broker_bufq_timeout_scan (rkb=rkb@entry=0xb31fb0, is_waitresp_q=is_waitresp_q@entry=0, rkbq=rkbq@entry=0xb32180, partial_cntp=partial_cntp@entry=0x7f09ee56125c,
err=err@entry=RD_KAFKA_RESP_ERR__TIMED_OUT_QUEUE, now=now@entry=215591720139) at rdkafka_broker.c:558

(gdb) p rkb->rkb_outbufs
$58 = {rkbq_bufs = {tqh_first = 0x0, tqh_last = 0x7f09ee560fd0}, rkbq_cnt = {val = 0}, rkbq_msg_cnt = {val = 0}}

(gdb) p &rkb->rkb_outbufs
$59 = (rd_kafka_bufq_t *) 0xb32180

(gdb) p cnt
$60 = 1

Here is how it happened,

rd_kafka_broker_bufq_timeout_scan will iterate each buf in rkb->rkb_outbufs. De-queue one and process one each time.
But, theres one kind of buf, whose callback is rd_kafka_broker_handle_ApiVersion(), and the callback would directly call rd_kafka_broker_failed() in advance.
Unfortunately, rd_kafka_broker_failed() will touch the rkb->rkb_outbufs (and rkb_waitresps) as well. It will clear the rkb->rkb_outbufs!
Now, the rkb_outbufs becomes empty, within the loop in TAILQ_FOREACH_SAFE(, rkb_outbufs, next) (see step 1). Since the next pointer previously saved could be not NULL, and loop would continue, thus call rd_kafka_bufq_deq() which triggered the assert statement.

So, ""rd_kafka_broker_handle_ApiVersion()"" should not always call ""rd_kafka_broker_fail()"", -- we should use the error code (RD_KAFKA_RESP_ERR__TIMED_OUT_QUEUE) to determin whether it happens within the rd_kafka_broker_timeout_scan iterating loop

THE FOLLOWING DISCLAIMER APPLIES TO ALL SOFTWARE CODE AND OTHER MATERIALS CONTRIBUTED IN CONNECTION WITH THIS SOFTWARE:
THIS SOFTWARE IS LICENSED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS AS IS AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE AND ANY WARRANTY OF NON-INFRINGEMENT, ARE DISCLAIMED.  IN NO EVENT SHALL THE COPYRIGHT OWNER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.  THIS SOFTWARE MAY BE REDISTRIBUTED TO OTHERS ONLY BY EFFECTIVELY USING THIS OR ANOTHER EQUIVALENT DISCLAIMER IN ADDITION TO ANY OTHER REQUIRED LICENSE TERMS.
ONLY THE SOFTWARE CODE AND OTHER MATERIALS CONTRIBUTED IN CONNECTION WITH THIS SOFTWARE, IF ANY, THAT ARE ATTACHED TO (OR OTHERWISE ACCOMPANY) THIS SUBMISSION (AND ORDINARY COURSE CONTRIBUTIONS OF FUTURES PATCHES THERETO) ARE TO BE CONSIDERED A CONTRIBUTION.  NO OTHER SOFTWARE CODE OR MATERIALS ARE A CONTRIBUTION.","Sometimes librdkafka would crash with
*** rdkafka_buf.c:201:rd_kafka_bufq_deq: assert: rd_atomic32_get(&rkbufq->rkbq_cnt) > 0 ***
(gdb) bt
#0  0x00007f09f26e2495 in raise () from /lib64/libc.so.6
#1  0x00007f09f26e3c75 in abort () from /lib64/libc.so.6
#2  0x00007f09f3410353 in rd_kafka_crash (file=file@entry=0x7f09f34caf7f ""rdkafka_buf.c"", line=line@entry=197, function=function@entry=0x7f09f34cb130 <__FUNCTION__.22201> ""rd_kafka_bufq_deq"", rk=rk@entry=0x0,
    reason=reason@entry=0x7f09f34caff8 ""assert: rd_atomic32_get(&rkbufq->rkbq_cnt) > 0"") at rdkafka.c:3432
#3  0x00007f09f343b975 in rd_kafka_bufq_deq (rkbufq=rkbufq@entry=0xb32180, rkbuf=rkbuf@entry=0xb36050) at rdkafka_buf.c:197
#4  0x00007f09f342300b in rd_kafka_broker_bufq_timeout_scan (rkb=rkb@entry=0xb31fb0, is_waitresp_q=is_waitresp_q@entry=0, rkbq=rkbq@entry=0xb32180, partial_cntp=partial_cntp@entry=0x7f09ee56125c,
    err=err@entry=RD_KAFKA_RESP_ERR__TIMED_OUT_QUEUE, now=now@entry=215591720139) at rdkafka_broker.c:558
#5  0x00007f09f34291ed in rd_kafka_broker_timeout_scan (now=215591720139, rkb=0xb31fb0) at rdkafka_broker.c:594
#6  rd_kafka_broker_serve (rkb=rkb@entry=0xb31fb0, abs_timeout=abs_timeout@entry=215591720118) at rdkafka_broker.c:2562
#7  0x00007f09f3429597 in rd_kafka_broker_ua_idle (rkb=rkb@entry=0xb31fb0, timeout_ms=<optimized out>, timeout_ms@entry=-1) at rdkafka_broker.c:2617
#8  0x00007f09f3429a24 in rd_kafka_broker_thread_main (arg=arg@entry=0xb31fb0) at rdkafka_broker.c:3552
#9  0x00007f09f34763d7 in _thrd_wrapper_function (aArg=<optimized out>) at tinycthread.c:583
#10 0x00007f09f2a4baa1 in start_thread () from /lib64/libpthread.so.0
#11 0x00007f09f2798bdd in clone () from /lib64/libc.so.6

(gdb) f 4
#4  0x00007f09f342300b in rd_kafka_broker_bufq_timeout_scan (rkb=rkb@entry=0xb31fb0, is_waitresp_q=is_waitresp_q@entry=0, rkbq=rkbq@entry=0xb32180, partial_cntp=partial_cntp@entry=0x7f09ee56125c,
err=err@entry=RD_KAFKA_RESP_ERR__TIMED_OUT_QUEUE, now=now@entry=215591720139) at rdkafka_broker.c:558

(gdb) p rkb->rkb_outbufs
$58 = {rkbq_bufs = {tqh_first = 0x0, tqh_last = 0x7f09ee560fd0}, rkbq_cnt = {val = 0}, rkbq_msg_cnt = {val = 0}}

(gdb) p &rkb->rkb_outbufs
$59 = (rd_kafka_bufq_t *) 0xb32180

(gdb) p cnt
$60 = 1

Here is how it happened,

rd_kafka_broker_bufq_timeout_scan will iterate each buf in rkb->rkb_outbufs. De-queue one and process one each time.
But, theres one kind of buf, whose callback is rd_kafka_broker_handle_ApiVersion(), and the callback would directly call rd_kafka_broker_failed() in advance.
Unfortunately, rd_kafka_broker_failed() will touch the rkb->rkb_outbufs (and rkb_waitresps) as well. It will clear the rkb->rkb_outbufs!
Now, the rkb_outbufs becomes empty, within the loop in TAILQ_FOREACH_SAFE(, rkb_outbufs, next) (see step 1). Since the next pointer previously saved could be not NULL, and loop would continue, thus call rd_kafka_bufq_deq() which triggered the assert statement.

So, ""rd_kafka_broker_handle_ApiVersion()"" should not always call ""rd_kafka_broker_fail()"", -- we should use the error code (RD_KAFKA_RESP_ERR__TIMED_OUT_QUEUE) to determin whether it happens within the rd_kafka_broker_timeout_scan iterating loop

THE FOLLOWING DISCLAIMER APPLIES TO ALL SOFTWARE CODE AND OTHER MATERIALS CONTRIBUTED IN CONNECTION WITH THIS SOFTWARE:
THIS SOFTWARE IS LICENSED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS AS IS AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE AND ANY WARRANTY OF NON-INFRINGEMENT, ARE DISCLAIMED.  IN NO EVENT SHALL THE COPYRIGHT OWNER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.  THIS SOFTWARE MAY BE REDISTRIBUTED TO OTHERS ONLY BY EFFECTIVELY USING THIS OR ANOTHER EQUIVALENT DISCLAIMER IN ADDITION TO ANY OTHER REQUIRED LICENSE TERMS.
ONLY THE SOFTWARE CODE AND OTHER MATERIALS CONTRIBUTED IN CONNECTION WITH THIS SOFTWARE, IF ANY, THAT ARE ATTACHED TO (OR OTHERWISE ACCOMPANY) THIS SUBMISSION (AND ORDINARY COURSE CONTRIBUTIONS OF FUTURES PATCHES THERETO) ARE TO BE CONSIDERED A CONTRIBUTION.  NO OTHER SOFTWARE CODE OR MATERIALS ARE A CONTRIBUTION.",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2326,2019-05-14T07:45:49Z,2020-05-08T08:54:52Z,2020-05-08T08:54:52Z,CLOSED,False,10,4,1,https://github.com/kenneth-jia,rd_kafka_broker_handle_ApiVersion() should not call rd_kafka_broker_fail within bufq_timeout_scan,1,[],https://github.com/edenhill/librdkafka/pull/2326,https://github.com/kenneth-jia,2,https://github.com/edenhill/librdkafka/pull/2326#issuecomment-492119219,"Sometimes librdkafka would crash with
*** rdkafka_buf.c:201:rd_kafka_bufq_deq: assert: rd_atomic32_get(&rkbufq->rkbq_cnt) > 0 ***
(gdb) bt
#0  0x00007f09f26e2495 in raise () from /lib64/libc.so.6
#1  0x00007f09f26e3c75 in abort () from /lib64/libc.so.6
#2  0x00007f09f3410353 in rd_kafka_crash (file=file@entry=0x7f09f34caf7f ""rdkafka_buf.c"", line=line@entry=197, function=function@entry=0x7f09f34cb130 <__FUNCTION__.22201> ""rd_kafka_bufq_deq"", rk=rk@entry=0x0,
    reason=reason@entry=0x7f09f34caff8 ""assert: rd_atomic32_get(&rkbufq->rkbq_cnt) > 0"") at rdkafka.c:3432
#3  0x00007f09f343b975 in rd_kafka_bufq_deq (rkbufq=rkbufq@entry=0xb32180, rkbuf=rkbuf@entry=0xb36050) at rdkafka_buf.c:197
#4  0x00007f09f342300b in rd_kafka_broker_bufq_timeout_scan (rkb=rkb@entry=0xb31fb0, is_waitresp_q=is_waitresp_q@entry=0, rkbq=rkbq@entry=0xb32180, partial_cntp=partial_cntp@entry=0x7f09ee56125c,
    err=err@entry=RD_KAFKA_RESP_ERR__TIMED_OUT_QUEUE, now=now@entry=215591720139) at rdkafka_broker.c:558
#5  0x00007f09f34291ed in rd_kafka_broker_timeout_scan (now=215591720139, rkb=0xb31fb0) at rdkafka_broker.c:594
#6  rd_kafka_broker_serve (rkb=rkb@entry=0xb31fb0, abs_timeout=abs_timeout@entry=215591720118) at rdkafka_broker.c:2562
#7  0x00007f09f3429597 in rd_kafka_broker_ua_idle (rkb=rkb@entry=0xb31fb0, timeout_ms=<optimized out>, timeout_ms@entry=-1) at rdkafka_broker.c:2617
#8  0x00007f09f3429a24 in rd_kafka_broker_thread_main (arg=arg@entry=0xb31fb0) at rdkafka_broker.c:3552
#9  0x00007f09f34763d7 in _thrd_wrapper_function (aArg=<optimized out>) at tinycthread.c:583
#10 0x00007f09f2a4baa1 in start_thread () from /lib64/libpthread.so.0
#11 0x00007f09f2798bdd in clone () from /lib64/libc.so.6

(gdb) f 4
#4  0x00007f09f342300b in rd_kafka_broker_bufq_timeout_scan (rkb=rkb@entry=0xb31fb0, is_waitresp_q=is_waitresp_q@entry=0, rkbq=rkbq@entry=0xb32180, partial_cntp=partial_cntp@entry=0x7f09ee56125c,
err=err@entry=RD_KAFKA_RESP_ERR__TIMED_OUT_QUEUE, now=now@entry=215591720139) at rdkafka_broker.c:558

(gdb) p rkb->rkb_outbufs
$58 = {rkbq_bufs = {tqh_first = 0x0, tqh_last = 0x7f09ee560fd0}, rkbq_cnt = {val = 0}, rkbq_msg_cnt = {val = 0}}

(gdb) p &rkb->rkb_outbufs
$59 = (rd_kafka_bufq_t *) 0xb32180

(gdb) p cnt
$60 = 1

Here is how it happened,

rd_kafka_broker_bufq_timeout_scan will iterate each buf in rkb->rkb_outbufs. De-queue one and process one each time.
But, theres one kind of buf, whose callback is rd_kafka_broker_handle_ApiVersion(), and the callback would directly call rd_kafka_broker_failed() in advance.
Unfortunately, rd_kafka_broker_failed() will touch the rkb->rkb_outbufs (and rkb_waitresps) as well. It will clear the rkb->rkb_outbufs!
Now, the rkb_outbufs becomes empty, within the loop in TAILQ_FOREACH_SAFE(, rkb_outbufs, next) (see step 1). Since the next pointer previously saved could be not NULL, and loop would continue, thus call rd_kafka_bufq_deq() which triggered the assert statement.

So, ""rd_kafka_broker_handle_ApiVersion()"" should not always call ""rd_kafka_broker_fail()"", -- we should use the error code (RD_KAFKA_RESP_ERR__TIMED_OUT_QUEUE) to determin whether it happens within the rd_kafka_broker_timeout_scan iterating loop

THE FOLLOWING DISCLAIMER APPLIES TO ALL SOFTWARE CODE AND OTHER MATERIALS CONTRIBUTED IN CONNECTION WITH THIS SOFTWARE:
THIS SOFTWARE IS LICENSED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS AS IS AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE AND ANY WARRANTY OF NON-INFRINGEMENT, ARE DISCLAIMED.  IN NO EVENT SHALL THE COPYRIGHT OWNER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.  THIS SOFTWARE MAY BE REDISTRIBUTED TO OTHERS ONLY BY EFFECTIVELY USING THIS OR ANOTHER EQUIVALENT DISCLAIMER IN ADDITION TO ANY OTHER REQUIRED LICENSE TERMS.
ONLY THE SOFTWARE CODE AND OTHER MATERIALS CONTRIBUTED IN CONNECTION WITH THIS SOFTWARE, IF ANY, THAT ARE ATTACHED TO (OR OTHERWISE ACCOMPANY) THIS SUBMISSION (AND ORDINARY COURSE CONTRIBUTIONS OF FUTURES PATCHES THERETO) ARE TO BE CONSIDERED A CONTRIBUTION.  NO OTHER SOFTWARE CODE OR MATERIALS ARE A CONTRIBUTION.","I submitted an similar PR before, however, I'd like to use this new one instead, -- added some disclaimer (applying to Firm's rule) and changed the previous solution a bit.",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2326,2019-05-14T07:45:49Z,2020-05-08T08:54:52Z,2020-05-08T08:54:52Z,CLOSED,False,10,4,1,https://github.com/kenneth-jia,rd_kafka_broker_handle_ApiVersion() should not call rd_kafka_broker_fail within bufq_timeout_scan,1,[],https://github.com/edenhill/librdkafka/pull/2326,https://github.com/edenhill,3,https://github.com/edenhill/librdkafka/pull/2326#issuecomment-492948390,"Sometimes librdkafka would crash with
*** rdkafka_buf.c:201:rd_kafka_bufq_deq: assert: rd_atomic32_get(&rkbufq->rkbq_cnt) > 0 ***
(gdb) bt
#0  0x00007f09f26e2495 in raise () from /lib64/libc.so.6
#1  0x00007f09f26e3c75 in abort () from /lib64/libc.so.6
#2  0x00007f09f3410353 in rd_kafka_crash (file=file@entry=0x7f09f34caf7f ""rdkafka_buf.c"", line=line@entry=197, function=function@entry=0x7f09f34cb130 <__FUNCTION__.22201> ""rd_kafka_bufq_deq"", rk=rk@entry=0x0,
    reason=reason@entry=0x7f09f34caff8 ""assert: rd_atomic32_get(&rkbufq->rkbq_cnt) > 0"") at rdkafka.c:3432
#3  0x00007f09f343b975 in rd_kafka_bufq_deq (rkbufq=rkbufq@entry=0xb32180, rkbuf=rkbuf@entry=0xb36050) at rdkafka_buf.c:197
#4  0x00007f09f342300b in rd_kafka_broker_bufq_timeout_scan (rkb=rkb@entry=0xb31fb0, is_waitresp_q=is_waitresp_q@entry=0, rkbq=rkbq@entry=0xb32180, partial_cntp=partial_cntp@entry=0x7f09ee56125c,
    err=err@entry=RD_KAFKA_RESP_ERR__TIMED_OUT_QUEUE, now=now@entry=215591720139) at rdkafka_broker.c:558
#5  0x00007f09f34291ed in rd_kafka_broker_timeout_scan (now=215591720139, rkb=0xb31fb0) at rdkafka_broker.c:594
#6  rd_kafka_broker_serve (rkb=rkb@entry=0xb31fb0, abs_timeout=abs_timeout@entry=215591720118) at rdkafka_broker.c:2562
#7  0x00007f09f3429597 in rd_kafka_broker_ua_idle (rkb=rkb@entry=0xb31fb0, timeout_ms=<optimized out>, timeout_ms@entry=-1) at rdkafka_broker.c:2617
#8  0x00007f09f3429a24 in rd_kafka_broker_thread_main (arg=arg@entry=0xb31fb0) at rdkafka_broker.c:3552
#9  0x00007f09f34763d7 in _thrd_wrapper_function (aArg=<optimized out>) at tinycthread.c:583
#10 0x00007f09f2a4baa1 in start_thread () from /lib64/libpthread.so.0
#11 0x00007f09f2798bdd in clone () from /lib64/libc.so.6

(gdb) f 4
#4  0x00007f09f342300b in rd_kafka_broker_bufq_timeout_scan (rkb=rkb@entry=0xb31fb0, is_waitresp_q=is_waitresp_q@entry=0, rkbq=rkbq@entry=0xb32180, partial_cntp=partial_cntp@entry=0x7f09ee56125c,
err=err@entry=RD_KAFKA_RESP_ERR__TIMED_OUT_QUEUE, now=now@entry=215591720139) at rdkafka_broker.c:558

(gdb) p rkb->rkb_outbufs
$58 = {rkbq_bufs = {tqh_first = 0x0, tqh_last = 0x7f09ee560fd0}, rkbq_cnt = {val = 0}, rkbq_msg_cnt = {val = 0}}

(gdb) p &rkb->rkb_outbufs
$59 = (rd_kafka_bufq_t *) 0xb32180

(gdb) p cnt
$60 = 1

Here is how it happened,

rd_kafka_broker_bufq_timeout_scan will iterate each buf in rkb->rkb_outbufs. De-queue one and process one each time.
But, theres one kind of buf, whose callback is rd_kafka_broker_handle_ApiVersion(), and the callback would directly call rd_kafka_broker_failed() in advance.
Unfortunately, rd_kafka_broker_failed() will touch the rkb->rkb_outbufs (and rkb_waitresps) as well. It will clear the rkb->rkb_outbufs!
Now, the rkb_outbufs becomes empty, within the loop in TAILQ_FOREACH_SAFE(, rkb_outbufs, next) (see step 1). Since the next pointer previously saved could be not NULL, and loop would continue, thus call rd_kafka_bufq_deq() which triggered the assert statement.

So, ""rd_kafka_broker_handle_ApiVersion()"" should not always call ""rd_kafka_broker_fail()"", -- we should use the error code (RD_KAFKA_RESP_ERR__TIMED_OUT_QUEUE) to determin whether it happens within the rd_kafka_broker_timeout_scan iterating loop

THE FOLLOWING DISCLAIMER APPLIES TO ALL SOFTWARE CODE AND OTHER MATERIALS CONTRIBUTED IN CONNECTION WITH THIS SOFTWARE:
THIS SOFTWARE IS LICENSED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS AS IS AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE AND ANY WARRANTY OF NON-INFRINGEMENT, ARE DISCLAIMED.  IN NO EVENT SHALL THE COPYRIGHT OWNER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.  THIS SOFTWARE MAY BE REDISTRIBUTED TO OTHERS ONLY BY EFFECTIVELY USING THIS OR ANOTHER EQUIVALENT DISCLAIMER IN ADDITION TO ANY OTHER REQUIRED LICENSE TERMS.
ONLY THE SOFTWARE CODE AND OTHER MATERIALS CONTRIBUTED IN CONNECTION WITH THIS SOFTWARE, IF ANY, THAT ARE ATTACHED TO (OR OTHERWISE ACCOMPANY) THIS SUBMISSION (AND ORDINARY COURSE CONTRIBUTIONS OF FUTURES PATCHES THERETO) ARE TO BE CONSIDERED A CONTRIBUTION.  NO OTHER SOFTWARE CODE OR MATERIALS ARE A CONTRIBUTION.","Thank you for this PR.
I'm not sure what to do with the disclaimer. Any code contributions to librdkafka effectively transfer ownership of the contribution from the contributor to the librdkafka project maintainers.
librdkafka itself is already covered by the 2-clause BSD license and its disclaimer.",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2326,2019-05-14T07:45:49Z,2020-05-08T08:54:52Z,2020-05-08T08:54:52Z,CLOSED,False,10,4,1,https://github.com/kenneth-jia,rd_kafka_broker_handle_ApiVersion() should not call rd_kafka_broker_fail within bufq_timeout_scan,1,[],https://github.com/edenhill/librdkafka/pull/2326,https://github.com/kenneth-jia,4,https://github.com/edenhill/librdkafka/pull/2326#issuecomment-493286252,"Sometimes librdkafka would crash with
*** rdkafka_buf.c:201:rd_kafka_bufq_deq: assert: rd_atomic32_get(&rkbufq->rkbq_cnt) > 0 ***
(gdb) bt
#0  0x00007f09f26e2495 in raise () from /lib64/libc.so.6
#1  0x00007f09f26e3c75 in abort () from /lib64/libc.so.6
#2  0x00007f09f3410353 in rd_kafka_crash (file=file@entry=0x7f09f34caf7f ""rdkafka_buf.c"", line=line@entry=197, function=function@entry=0x7f09f34cb130 <__FUNCTION__.22201> ""rd_kafka_bufq_deq"", rk=rk@entry=0x0,
    reason=reason@entry=0x7f09f34caff8 ""assert: rd_atomic32_get(&rkbufq->rkbq_cnt) > 0"") at rdkafka.c:3432
#3  0x00007f09f343b975 in rd_kafka_bufq_deq (rkbufq=rkbufq@entry=0xb32180, rkbuf=rkbuf@entry=0xb36050) at rdkafka_buf.c:197
#4  0x00007f09f342300b in rd_kafka_broker_bufq_timeout_scan (rkb=rkb@entry=0xb31fb0, is_waitresp_q=is_waitresp_q@entry=0, rkbq=rkbq@entry=0xb32180, partial_cntp=partial_cntp@entry=0x7f09ee56125c,
    err=err@entry=RD_KAFKA_RESP_ERR__TIMED_OUT_QUEUE, now=now@entry=215591720139) at rdkafka_broker.c:558
#5  0x00007f09f34291ed in rd_kafka_broker_timeout_scan (now=215591720139, rkb=0xb31fb0) at rdkafka_broker.c:594
#6  rd_kafka_broker_serve (rkb=rkb@entry=0xb31fb0, abs_timeout=abs_timeout@entry=215591720118) at rdkafka_broker.c:2562
#7  0x00007f09f3429597 in rd_kafka_broker_ua_idle (rkb=rkb@entry=0xb31fb0, timeout_ms=<optimized out>, timeout_ms@entry=-1) at rdkafka_broker.c:2617
#8  0x00007f09f3429a24 in rd_kafka_broker_thread_main (arg=arg@entry=0xb31fb0) at rdkafka_broker.c:3552
#9  0x00007f09f34763d7 in _thrd_wrapper_function (aArg=<optimized out>) at tinycthread.c:583
#10 0x00007f09f2a4baa1 in start_thread () from /lib64/libpthread.so.0
#11 0x00007f09f2798bdd in clone () from /lib64/libc.so.6

(gdb) f 4
#4  0x00007f09f342300b in rd_kafka_broker_bufq_timeout_scan (rkb=rkb@entry=0xb31fb0, is_waitresp_q=is_waitresp_q@entry=0, rkbq=rkbq@entry=0xb32180, partial_cntp=partial_cntp@entry=0x7f09ee56125c,
err=err@entry=RD_KAFKA_RESP_ERR__TIMED_OUT_QUEUE, now=now@entry=215591720139) at rdkafka_broker.c:558

(gdb) p rkb->rkb_outbufs
$58 = {rkbq_bufs = {tqh_first = 0x0, tqh_last = 0x7f09ee560fd0}, rkbq_cnt = {val = 0}, rkbq_msg_cnt = {val = 0}}

(gdb) p &rkb->rkb_outbufs
$59 = (rd_kafka_bufq_t *) 0xb32180

(gdb) p cnt
$60 = 1

Here is how it happened,

rd_kafka_broker_bufq_timeout_scan will iterate each buf in rkb->rkb_outbufs. De-queue one and process one each time.
But, theres one kind of buf, whose callback is rd_kafka_broker_handle_ApiVersion(), and the callback would directly call rd_kafka_broker_failed() in advance.
Unfortunately, rd_kafka_broker_failed() will touch the rkb->rkb_outbufs (and rkb_waitresps) as well. It will clear the rkb->rkb_outbufs!
Now, the rkb_outbufs becomes empty, within the loop in TAILQ_FOREACH_SAFE(, rkb_outbufs, next) (see step 1). Since the next pointer previously saved could be not NULL, and loop would continue, thus call rd_kafka_bufq_deq() which triggered the assert statement.

So, ""rd_kafka_broker_handle_ApiVersion()"" should not always call ""rd_kafka_broker_fail()"", -- we should use the error code (RD_KAFKA_RESP_ERR__TIMED_OUT_QUEUE) to determin whether it happens within the rd_kafka_broker_timeout_scan iterating loop

THE FOLLOWING DISCLAIMER APPLIES TO ALL SOFTWARE CODE AND OTHER MATERIALS CONTRIBUTED IN CONNECTION WITH THIS SOFTWARE:
THIS SOFTWARE IS LICENSED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS AS IS AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE AND ANY WARRANTY OF NON-INFRINGEMENT, ARE DISCLAIMED.  IN NO EVENT SHALL THE COPYRIGHT OWNER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.  THIS SOFTWARE MAY BE REDISTRIBUTED TO OTHERS ONLY BY EFFECTIVELY USING THIS OR ANOTHER EQUIVALENT DISCLAIMER IN ADDITION TO ANY OTHER REQUIRED LICENSE TERMS.
ONLY THE SOFTWARE CODE AND OTHER MATERIALS CONTRIBUTED IN CONNECTION WITH THIS SOFTWARE, IF ANY, THAT ARE ATTACHED TO (OR OTHERWISE ACCOMPANY) THIS SUBMISSION (AND ORDINARY COURSE CONTRIBUTIONS OF FUTURES PATCHES THERETO) ARE TO BE CONSIDERED A CONTRIBUTION.  NO OTHER SOFTWARE CODE OR MATERIALS ARE A CONTRIBUTION.","Thank you for this PR.
I'm not sure what to do with the disclaimer. Any code contributions to librdkafka effectively transfer ownership of the contribution from the contributor to the librdkafka project maintainers.
librdkafka itself is already covered by the 2-clause BSD license and its disclaimer.

@edenhill ,
Sorry for putting a large paragraph of the disclaimer at the very beginning to scare people.  I came back to my colleges who handled similar issues before, and, I just edited the PR, -- putting the disclaimer at the end, -- it seems like a good best practice.
Anyway, this kind of disclaimer has no harm, -- just recap those legal things. It's just our firm's rules. (normally, our developers might not notice these different licences for kinds of oss projects, -- putting the disclaimer everywhere just to avoid any trouble)",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2326,2019-05-14T07:45:49Z,2020-05-08T08:54:52Z,2020-05-08T08:54:52Z,CLOSED,False,10,4,1,https://github.com/kenneth-jia,rd_kafka_broker_handle_ApiVersion() should not call rd_kafka_broker_fail within bufq_timeout_scan,1,[],https://github.com/edenhill/librdkafka/pull/2326,https://github.com/edenhill,5,https://github.com/edenhill/librdkafka/pull/2326#issuecomment-625714550,"Sometimes librdkafka would crash with
*** rdkafka_buf.c:201:rd_kafka_bufq_deq: assert: rd_atomic32_get(&rkbufq->rkbq_cnt) > 0 ***
(gdb) bt
#0  0x00007f09f26e2495 in raise () from /lib64/libc.so.6
#1  0x00007f09f26e3c75 in abort () from /lib64/libc.so.6
#2  0x00007f09f3410353 in rd_kafka_crash (file=file@entry=0x7f09f34caf7f ""rdkafka_buf.c"", line=line@entry=197, function=function@entry=0x7f09f34cb130 <__FUNCTION__.22201> ""rd_kafka_bufq_deq"", rk=rk@entry=0x0,
    reason=reason@entry=0x7f09f34caff8 ""assert: rd_atomic32_get(&rkbufq->rkbq_cnt) > 0"") at rdkafka.c:3432
#3  0x00007f09f343b975 in rd_kafka_bufq_deq (rkbufq=rkbufq@entry=0xb32180, rkbuf=rkbuf@entry=0xb36050) at rdkafka_buf.c:197
#4  0x00007f09f342300b in rd_kafka_broker_bufq_timeout_scan (rkb=rkb@entry=0xb31fb0, is_waitresp_q=is_waitresp_q@entry=0, rkbq=rkbq@entry=0xb32180, partial_cntp=partial_cntp@entry=0x7f09ee56125c,
    err=err@entry=RD_KAFKA_RESP_ERR__TIMED_OUT_QUEUE, now=now@entry=215591720139) at rdkafka_broker.c:558
#5  0x00007f09f34291ed in rd_kafka_broker_timeout_scan (now=215591720139, rkb=0xb31fb0) at rdkafka_broker.c:594
#6  rd_kafka_broker_serve (rkb=rkb@entry=0xb31fb0, abs_timeout=abs_timeout@entry=215591720118) at rdkafka_broker.c:2562
#7  0x00007f09f3429597 in rd_kafka_broker_ua_idle (rkb=rkb@entry=0xb31fb0, timeout_ms=<optimized out>, timeout_ms@entry=-1) at rdkafka_broker.c:2617
#8  0x00007f09f3429a24 in rd_kafka_broker_thread_main (arg=arg@entry=0xb31fb0) at rdkafka_broker.c:3552
#9  0x00007f09f34763d7 in _thrd_wrapper_function (aArg=<optimized out>) at tinycthread.c:583
#10 0x00007f09f2a4baa1 in start_thread () from /lib64/libpthread.so.0
#11 0x00007f09f2798bdd in clone () from /lib64/libc.so.6

(gdb) f 4
#4  0x00007f09f342300b in rd_kafka_broker_bufq_timeout_scan (rkb=rkb@entry=0xb31fb0, is_waitresp_q=is_waitresp_q@entry=0, rkbq=rkbq@entry=0xb32180, partial_cntp=partial_cntp@entry=0x7f09ee56125c,
err=err@entry=RD_KAFKA_RESP_ERR__TIMED_OUT_QUEUE, now=now@entry=215591720139) at rdkafka_broker.c:558

(gdb) p rkb->rkb_outbufs
$58 = {rkbq_bufs = {tqh_first = 0x0, tqh_last = 0x7f09ee560fd0}, rkbq_cnt = {val = 0}, rkbq_msg_cnt = {val = 0}}

(gdb) p &rkb->rkb_outbufs
$59 = (rd_kafka_bufq_t *) 0xb32180

(gdb) p cnt
$60 = 1

Here is how it happened,

rd_kafka_broker_bufq_timeout_scan will iterate each buf in rkb->rkb_outbufs. De-queue one and process one each time.
But, theres one kind of buf, whose callback is rd_kafka_broker_handle_ApiVersion(), and the callback would directly call rd_kafka_broker_failed() in advance.
Unfortunately, rd_kafka_broker_failed() will touch the rkb->rkb_outbufs (and rkb_waitresps) as well. It will clear the rkb->rkb_outbufs!
Now, the rkb_outbufs becomes empty, within the loop in TAILQ_FOREACH_SAFE(, rkb_outbufs, next) (see step 1). Since the next pointer previously saved could be not NULL, and loop would continue, thus call rd_kafka_bufq_deq() which triggered the assert statement.

So, ""rd_kafka_broker_handle_ApiVersion()"" should not always call ""rd_kafka_broker_fail()"", -- we should use the error code (RD_KAFKA_RESP_ERR__TIMED_OUT_QUEUE) to determin whether it happens within the rd_kafka_broker_timeout_scan iterating loop

THE FOLLOWING DISCLAIMER APPLIES TO ALL SOFTWARE CODE AND OTHER MATERIALS CONTRIBUTED IN CONNECTION WITH THIS SOFTWARE:
THIS SOFTWARE IS LICENSED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS AS IS AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE AND ANY WARRANTY OF NON-INFRINGEMENT, ARE DISCLAIMED.  IN NO EVENT SHALL THE COPYRIGHT OWNER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.  THIS SOFTWARE MAY BE REDISTRIBUTED TO OTHERS ONLY BY EFFECTIVELY USING THIS OR ANOTHER EQUIVALENT DISCLAIMER IN ADDITION TO ANY OTHER REQUIRED LICENSE TERMS.
ONLY THE SOFTWARE CODE AND OTHER MATERIALS CONTRIBUTED IN CONNECTION WITH THIS SOFTWARE, IF ANY, THAT ARE ATTACHED TO (OR OTHERWISE ACCOMPANY) THIS SUBMISSION (AND ORDINARY COURSE CONTRIBUTIONS OF FUTURES PATCHES THERETO) ARE TO BE CONSIDERED A CONTRIBUTION.  NO OTHER SOFTWARE CODE OR MATERIALS ARE A CONTRIBUTION.",Fixed in #2877,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2337,2019-05-24T17:48:33Z,2019-07-08T19:01:30Z,2019-07-08T19:01:30Z,CLOSED,False,1,1,1,https://github.com/meridional,fix off by one issue in rd_kafka_offsets_store,1,[],https://github.com/edenhill/librdkafka/pull/2337,https://github.com/meridional,1,https://github.com/edenhill/librdkafka/pull/2337,"There is a discrepancy between rd_kafka_offsets_store and rd_kafka_offset_store where only the latter automatically increment the offset by 1 and it's probably the desired the behavior in most scenarios.
This PR basically lets rd_kafka_offsets_store to do the same.
Please let me know if there is rationale behind the discrepancy or if this is indeed a bug.
More context:
I encountered this issue when running a go consumer, and used consumer.StoreOffsets which calls rd_kafka_offsets_store. And it came as a surprise to me that restarting an up-to-date consumer would replay the latest message.
Will add tests if necessary.","There is a discrepancy between rd_kafka_offsets_store and rd_kafka_offset_store where only the latter automatically increment the offset by 1 and it's probably the desired the behavior in most scenarios.
This PR basically lets rd_kafka_offsets_store to do the same.
Please let me know if there is rationale behind the discrepancy or if this is indeed a bug.
More context:
I encountered this issue when running a go consumer, and used consumer.StoreOffsets which calls rd_kafka_offsets_store. And it came as a surprise to me that restarting an up-to-date consumer would replay the latest message.
Will add tests if necessary.",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2337,2019-05-24T17:48:33Z,2019-07-08T19:01:30Z,2019-07-08T19:01:30Z,CLOSED,False,1,1,1,https://github.com/meridional,fix off by one issue in rd_kafka_offsets_store,1,[],https://github.com/edenhill/librdkafka/pull/2337,https://github.com/edenhill,2,https://github.com/edenhill/librdkafka/pull/2337#issuecomment-495753127,"There is a discrepancy between rd_kafka_offsets_store and rd_kafka_offset_store where only the latter automatically increment the offset by 1 and it's probably the desired the behavior in most scenarios.
This PR basically lets rd_kafka_offsets_store to do the same.
Please let me know if there is rationale behind the discrepancy or if this is indeed a bug.
More context:
I encountered this issue when running a go consumer, and used consumer.StoreOffsets which calls rd_kafka_offsets_store. And it came as a surprise to me that restarting an up-to-date consumer would replay the latest message.
Will add tests if necessary.","Yeah this is a lapse in the implementation, but we can't change it since it would break existing applications.
We should update the documentation to point out the difference though, and deprecate rd_kafka_offset_store().",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2340,2019-05-27T10:32:22Z,,2020-04-07T12:44:19Z,OPEN,False,72,33,2,https://github.com/myd7349,snappy-c: Fix MSVC build errors on ARM,1,"['workaround-provided', 'wait-info']",https://github.com/edenhill/librdkafka/pull/2340,https://github.com/myd7349,1,https://github.com/edenhill/librdkafka/pull/2340,"The original implementations of get_unaligned_memcpy and put_unaligned_memcpy use
these GCC extensions:

typeof
statements expressions

which will cause build errors on Windows running on ARM.
This fixes #2324 .","The original implementations of get_unaligned_memcpy and put_unaligned_memcpy use
these GCC extensions:

typeof
statements expressions

which will cause build errors on Windows running on ARM.
This fixes #2324 .",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2340,2019-05-27T10:32:22Z,,2020-04-07T12:44:19Z,OPEN,False,72,33,2,https://github.com/myd7349,snappy-c: Fix MSVC build errors on ARM,1,"['workaround-provided', 'wait-info']",https://github.com/edenhill/librdkafka/pull/2340,https://github.com/myd7349,2,https://github.com/edenhill/librdkafka/pull/2340#issuecomment-496178346,"The original implementations of get_unaligned_memcpy and put_unaligned_memcpy use
these GCC extensions:

typeof
statements expressions

which will cause build errors on Windows running on ARM.
This fixes #2324 .",https://dev.azure.com/vcpkg/public/_build/results?buildId=696,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2340,2019-05-27T10:32:22Z,,2020-04-07T12:44:19Z,OPEN,False,72,33,2,https://github.com/myd7349,snappy-c: Fix MSVC build errors on ARM,1,"['workaround-provided', 'wait-info']",https://github.com/edenhill/librdkafka/pull/2340,https://github.com/edenhill,3,https://github.com/edenhill/librdkafka/pull/2340#issuecomment-496182087,"The original implementations of get_unaligned_memcpy and put_unaligned_memcpy use
these GCC extensions:

typeof
statements expressions

which will cause build errors on Windows running on ARM.
This fixes #2324 .","Looks good 
Did you run the test suite, in particular the 0017 compression test?",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2340,2019-05-27T10:32:22Z,,2020-04-07T12:44:19Z,OPEN,False,72,33,2,https://github.com/myd7349,snappy-c: Fix MSVC build errors on ARM,1,"['workaround-provided', 'wait-info']",https://github.com/edenhill/librdkafka/pull/2340,https://github.com/myd7349,4,https://github.com/edenhill/librdkafka/pull/2340#issuecomment-496187540,"The original implementations of get_unaligned_memcpy and put_unaligned_memcpy use
these GCC extensions:

typeof
statements expressions

which will cause build errors on Windows running on ARM.
This fixes #2324 .","@edenhill I have turned on tests this time: microsoft/vcpkg@8b64e98
When it comes to ARM, current CI system of vcpkg will only test arm64-windows, arm-uwp.
arm-windows, however, will not be tested.",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2340,2019-05-27T10:32:22Z,,2020-04-07T12:44:19Z,OPEN,False,72,33,2,https://github.com/myd7349,snappy-c: Fix MSVC build errors on ARM,1,"['workaround-provided', 'wait-info']",https://github.com/edenhill/librdkafka/pull/2340,https://github.com/myd7349,5,https://github.com/edenhill/librdkafka/pull/2340#issuecomment-496189316,"The original implementations of get_unaligned_memcpy and put_unaligned_memcpy use
these GCC extensions:

typeof
statements expressions

which will cause build errors on Windows running on ARM.
This fixes #2324 .","This PR seems change too much thing. There is also a more short fix for arm64-windows:
diff --git a/src/snappy_compat.h b/src/snappy_compat.h
index 77606552..8c7d3b43 100644
--- a/src/snappy_compat.h
+++ b/src/snappy_compat.h
@@ -56,8 +56,10 @@ struct iovec {
 #define put_unaligned_direct(v,x) (*(x) = (v))

 // Potentially unaligned loads and stores.
-// x86 and PowerPC can simply do these loads and stores native.
-#if defined(__i386__) || defined(__x86_64__) || defined(__powerpc__) || defined(_M_IX86) || defined(_M_X64) || defined(_M_AMD64)
+// x86, PowerPC, and ARM64 can simply do these loads and stores native.
+#if defined(__i386__) || defined(__x86_64__) || defined(__powerpc__) || \
+       defined(_M_IX86) || defined(_M_X64) || defined(_M_AMD64) || \
+       defined(__aarch64__)

 #define get_unaligned get_unaligned_direct
 #define put_unaligned put_unaligned_direct
If 32-bit ARM + windows is not that popular, we may use this patch instead.",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2340,2019-05-27T10:32:22Z,,2020-04-07T12:44:19Z,OPEN,False,72,33,2,https://github.com/myd7349,snappy-c: Fix MSVC build errors on ARM,1,"['workaround-provided', 'wait-info']",https://github.com/edenhill/librdkafka/pull/2340,https://github.com/edenhill,6,https://github.com/edenhill/librdkafka/pull/2340#issuecomment-496193250,"The original implementations of get_unaligned_memcpy and put_unaligned_memcpy use
these GCC extensions:

typeof
statements expressions

which will cause build errors on Windows running on ARM.
This fixes #2324 .","Okay, let's do this; create a new PR that has the minimal arm64 patch only, and we'll merge that one.
We'll leave this PR open so if there's demand for arm32 support we'll merge it later.
Thanks",True,{'HEART': ['https://github.com/myd7349']}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2341,2019-05-27T12:37:04Z,2019-05-27T15:57:50Z,2019-05-27T23:29:41Z,MERGED,True,4,2,1,https://github.com/myd7349,snappy-c: ARM64 supports direct unaligned loads/stores,1,[],https://github.com/edenhill/librdkafka/pull/2341,https://github.com/myd7349,1,https://github.com/edenhill/librdkafka/pull/2341,https://github.com/google/snappy/blob/1.1.7/snappy-stubs-internal.h#L113-L114,https://github.com/google/snappy/blob/1.1.7/snappy-stubs-internal.h#L113-L114,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2341,2019-05-27T12:37:04Z,2019-05-27T15:57:50Z,2019-05-27T23:29:41Z,MERGED,True,4,2,1,https://github.com/myd7349,snappy-c: ARM64 supports direct unaligned loads/stores,1,[],https://github.com/edenhill/librdkafka/pull/2341,https://github.com/myd7349,2,https://github.com/edenhill/librdkafka/pull/2341#issuecomment-496198975,https://github.com/google/snappy/blob/1.1.7/snappy-stubs-internal.h#L113-L114,"This partially fixes #2324 , as discussed in #2340 .",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2341,2019-05-27T12:37:04Z,2019-05-27T15:57:50Z,2019-05-27T23:29:41Z,MERGED,True,4,2,1,https://github.com/myd7349,snappy-c: ARM64 supports direct unaligned loads/stores,1,[],https://github.com/edenhill/librdkafka/pull/2341,https://github.com/edenhill,3,https://github.com/edenhill/librdkafka/pull/2341#issuecomment-496254681,https://github.com/google/snappy/blob/1.1.7/snappy-stubs-internal.h#L113-L114,Thank you!,True,{'HEART': ['https://github.com/myd7349']}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2348,2019-06-02T20:39:15Z,2019-06-10T06:35:56Z,2019-06-10T06:36:00Z,MERGED,True,1,0,1,https://github.com/SahilKang,Add cl-rdkafka to language bindings,2,[],https://github.com/edenhill/librdkafka/pull/2348,https://github.com/SahilKang,1,https://github.com/edenhill/librdkafka/pull/2348,I've created and released a Common Lisp library on top of librdkafka and would like to list it on the README.,I've created and released a Common Lisp library on top of librdkafka and would like to list it on the README.,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2348,2019-06-02T20:39:15Z,2019-06-10T06:35:56Z,2019-06-10T06:36:00Z,MERGED,True,1,0,1,https://github.com/SahilKang,Add cl-rdkafka to language bindings,2,[],https://github.com/edenhill/librdkafka/pull/2348,https://github.com/edenhill,2,https://github.com/edenhill/librdkafka/pull/2348#issuecomment-500308269,I've created and released a Common Lisp library on top of librdkafka and would like to list it on the README.,Thank you!,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2349,2019-06-03T09:24:36Z,2019-07-12T13:02:17Z,2019-07-12T13:02:23Z,MERGED,True,9,8,1,https://github.com/martinivanov,Fix wrong legacy batch absolute offset calculation,1,[],https://github.com/edenhill/librdkafka/pull/2349,https://github.com/martinivanov,1,https://github.com/edenhill/librdkafka/pull/2349,"Currently, the number of messages in a legacy (v0, v1) message set is used to derive the base offset of the messages. Messages in a message set are not guaranteed to have a consecutive relative offset, which leads to message offsets being overridden by librdkafka and eventually lost to the consumer. The PR changes the usage of the number of messages in the message set to the relative offset of the last message in the set for the calculation of the base offset, which removes the possible inconsistency.","Currently, the number of messages in a legacy (v0, v1) message set is used to derive the base offset of the messages. Messages in a message set are not guaranteed to have a consecutive relative offset, which leads to message offsets being overridden by librdkafka and eventually lost to the consumer. The PR changes the usage of the number of messages in the message set to the relative offset of the last message in the set for the calculation of the base offset, which removes the possible inconsistency.",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2349,2019-06-03T09:24:36Z,2019-07-12T13:02:17Z,2019-07-12T13:02:23Z,MERGED,True,9,8,1,https://github.com/martinivanov,Fix wrong legacy batch absolute offset calculation,1,[],https://github.com/edenhill/librdkafka/pull/2349,https://github.com/martinivanov,2,https://github.com/edenhill/librdkafka/pull/2349#issuecomment-501639167,"Currently, the number of messages in a legacy (v0, v1) message set is used to derive the base offset of the messages. Messages in a message set are not guaranteed to have a consecutive relative offset, which leads to message offsets being overridden by librdkafka and eventually lost to the consumer. The PR changes the usage of the number of messages in the message set to the relative offset of the last message in the set for the calculation of the base offset, which removes the possible inconsistency.","Here is the equivalent Java client code:
https://github.com/apache/kafka/blob/c09e25fac2aaea61af892ae3e5273679a4bdbc7d/clients/src/main/java/org/apache/kafka/common/record/AbstractLegacyRecordBatch.java#L379
https://github.com/apache/kafka/blob/c09e25fac2aaea61af892ae3e5273679a4bdbc7d/clients/src/main/java/org/apache/kafka/common/record/AbstractLegacyRecordBatch.java#L400",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2349,2019-06-03T09:24:36Z,2019-07-12T13:02:17Z,2019-07-12T13:02:23Z,MERGED,True,9,8,1,https://github.com/martinivanov,Fix wrong legacy batch absolute offset calculation,1,[],https://github.com/edenhill/librdkafka/pull/2349,https://github.com/dkholod,3,https://github.com/edenhill/librdkafka/pull/2349#issuecomment-510876950,"Currently, the number of messages in a legacy (v0, v1) message set is used to derive the base offset of the messages. Messages in a message set are not guaranteed to have a consecutive relative offset, which leads to message offsets being overridden by librdkafka and eventually lost to the consumer. The PR changes the usage of the number of messages in the message set to the relative offset of the last message in the set for the calculation of the base offset, which removes the possible inconsistency.","Hi guys.
On topics with a lot of changes and compaction we often see the same issues. Messages are lost.
Is it any plans for a fix?",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2349,2019-06-03T09:24:36Z,2019-07-12T13:02:17Z,2019-07-12T13:02:23Z,MERGED,True,9,8,1,https://github.com/martinivanov,Fix wrong legacy batch absolute offset calculation,1,[],https://github.com/edenhill/librdkafka/pull/2349,https://github.com/edenhill,4,https://github.com/edenhill/librdkafka/pull/2349#issuecomment-510878021,"Currently, the number of messages in a legacy (v0, v1) message set is used to derive the base offset of the messages. Messages in a message set are not guaranteed to have a consecutive relative offset, which leads to message offsets being overridden by librdkafka and eventually lost to the consumer. The PR changes the usage of the number of messages in the message set to the relative offset of the last message in the set for the calculation of the base offset, which removes the possible inconsistency.",Thank you!,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2352,2019-06-04T22:21:11Z,2019-07-12T12:51:39Z,2019-07-12T12:51:39Z,CLOSED,False,1401,84,34,https://github.com/mhowlett,consumer isolation.level implementation,13,[],https://github.com/edenhill/librdkafka/pull/2352,https://github.com/mhowlett,1,https://github.com/edenhill/librdkafka/pull/2352,"not ready to merge (e.g. not tests!), but opening now for your early feedback @edenhill - thanks!
update: now ready for final review.","not ready to merge (e.g. not tests!), but opening now for your early feedback @edenhill - thanks!
update: now ready for final review.",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2352,2019-06-04T22:21:11Z,2019-07-12T12:51:39Z,2019-07-12T12:51:39Z,CLOSED,False,1401,84,34,https://github.com/mhowlett,consumer isolation.level implementation,13,[],https://github.com/edenhill/librdkafka/pull/2352,https://github.com/mhowlett,2,https://github.com/edenhill/librdkafka/pull/2352#issuecomment-504717283,"not ready to merge (e.g. not tests!), but opening now for your early feedback @edenhill - thanks!
update: now ready for final review.","mhowlett/isolation1 has been rebased to master, squashed and conflicts fixed, but github isn't picking that up here. may need to open another PR.",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2358,2019-06-12T22:12:38Z,2019-06-18T09:11:37Z,2019-06-18T09:11:41Z,MERGED,True,16,0,4,https://github.com/billygout,CMake: Add check for GNU thread name setting support.,1,[],https://github.com/edenhill/librdkafka/pull/2358,https://github.com/billygout,1,https://github.com/edenhill/librdkafka/pull/2358,"Tested by building with cmake with and without successful compile of [by introducing a typo into]  packaging/cmake/try_compile/pthread_setname_gnu_test.c, and checking by running gdb on one of the examples/ apps and checking the difference in the output of disassemble thrd_setname.","Tested by building with cmake with and without successful compile of [by introducing a typo into]  packaging/cmake/try_compile/pthread_setname_gnu_test.c, and checking by running gdb on one of the examples/ apps and checking the difference in the output of disassemble thrd_setname.",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2358,2019-06-12T22:12:38Z,2019-06-18T09:11:37Z,2019-06-18T09:11:41Z,MERGED,True,16,0,4,https://github.com/billygout,CMake: Add check for GNU thread name setting support.,1,[],https://github.com/edenhill/librdkafka/pull/2358,https://github.com/edenhill,2,https://github.com/edenhill/librdkafka/pull/2358#issuecomment-501623652,"Tested by building with cmake with and without successful compile of [by introducing a typo into]  packaging/cmake/try_compile/pthread_setname_gnu_test.c, and checking by running gdb on one of the examples/ apps and checking the difference in the output of disassemble thrd_setname.","Think you forgot the compile test file itself:
CMake Error at CMakeLists.txt:15 (add_executable):
  Cannot find source file:

    /project/repo/checkout/packaging/cmake/try_compile/pthread_setname_gnu_test.c",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2358,2019-06-12T22:12:38Z,2019-06-18T09:11:37Z,2019-06-18T09:11:41Z,MERGED,True,16,0,4,https://github.com/billygout,CMake: Add check for GNU thread name setting support.,1,[],https://github.com/edenhill/librdkafka/pull/2358,https://github.com/billygout,3,https://github.com/edenhill/librdkafka/pull/2358#issuecomment-501727981,"Tested by building with cmake with and without successful compile of [by introducing a typo into]  packaging/cmake/try_compile/pthread_setname_gnu_test.c, and checking by running gdb on one of the examples/ apps and checking the difference in the output of disassemble thrd_setname.",Doh! Thanks,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2358,2019-06-12T22:12:38Z,2019-06-18T09:11:37Z,2019-06-18T09:11:41Z,MERGED,True,16,0,4,https://github.com/billygout,CMake: Add check for GNU thread name setting support.,1,[],https://github.com/edenhill/librdkafka/pull/2358,https://github.com/billygout,4,https://github.com/edenhill/librdkafka/pull/2358#issuecomment-501777008,"Tested by building with cmake with and without successful compile of [by introducing a typo into]  packaging/cmake/try_compile/pthread_setname_gnu_test.c, and checking by running gdb on one of the examples/ apps and checking the difference in the output of disassemble thrd_setname.","fixed,rebased,force-pushed",True,{'THUMBS_UP': ['https://github.com/edenhill']}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2358,2019-06-12T22:12:38Z,2019-06-18T09:11:37Z,2019-06-18T09:11:41Z,MERGED,True,16,0,4,https://github.com/billygout,CMake: Add check for GNU thread name setting support.,1,[],https://github.com/edenhill/librdkafka/pull/2358,https://github.com/edenhill,5,https://github.com/edenhill/librdkafka/pull/2358#issuecomment-503018847,"Tested by building with cmake with and without successful compile of [by introducing a typo into]  packaging/cmake/try_compile/pthread_setname_gnu_test.c, and checking by running gdb on one of the examples/ apps and checking the difference in the output of disassemble thrd_setname.",Thank you!,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2360,2019-06-13T10:38:33Z,2019-06-13T13:34:33Z,2019-06-17T15:59:20Z,MERGED,True,12,9,3,https://github.com/edenhill,Disable kinit refresh when sasl.kerberos.min.time.before.relogin=0,1,[],https://github.com/edenhill/librdkafka/pull/2360,https://github.com/edenhill,1,https://github.com/edenhill/librdkafka/pull/2360,,,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2362,2019-06-14T12:56:22Z,2019-06-17T18:14:30Z,2019-06-17T18:14:35Z,MERGED,True,9,9,1,https://github.com/MrAnno,"README: Fix broken link, use HTTPS links",2,[],https://github.com/edenhill/librdkafka/pull/2362,https://github.com/MrAnno,1,https://github.com/edenhill/librdkafka/pull/2362,,,True,"{'ROCKET': ['https://github.com/Kokan'], 'EYES': ['https://github.com/Kokan']}"
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2362,2019-06-14T12:56:22Z,2019-06-17T18:14:30Z,2019-06-17T18:14:35Z,MERGED,True,9,9,1,https://github.com/MrAnno,"README: Fix broken link, use HTTPS links",2,[],https://github.com/edenhill/librdkafka/pull/2362,https://github.com/edenhill,2,https://github.com/edenhill/librdkafka/pull/2362#issuecomment-502793677,,Thank you!,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2364,2019-06-14T16:10:00Z,2019-06-17T18:05:04Z,2019-06-17T18:05:09Z,MERGED,True,4,4,3,https://github.com/sarkanyi,Bump message.timeout.ms limit to INT32_MAX,1,[],https://github.com/edenhill/librdkafka/pull/2364,https://github.com/sarkanyi,1,https://github.com/edenhill/librdkafka/pull/2364,Because 0 is broken and some people need more than just 15 minutes.,Because 0 is broken and some people need more than just 15 minutes.,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2364,2019-06-14T16:10:00Z,2019-06-17T18:05:04Z,2019-06-17T18:05:09Z,MERGED,True,4,4,3,https://github.com/sarkanyi,Bump message.timeout.ms limit to INT32_MAX,1,[],https://github.com/edenhill/librdkafka/pull/2364,https://github.com/edenhill,2,https://github.com/edenhill/librdkafka/pull/2364#issuecomment-502790336,Because 0 is broken and some people need more than just 15 minutes.,Thank you!,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2365,2019-06-17T16:09:18Z,2019-06-18T14:27:53Z,2019-06-18T14:27:56Z,MERGED,True,79,17,4,https://github.com/edenhill,max.poll.interval.ms: properly handle blocking calls,1,[],https://github.com/edenhill/librdkafka/pull/2365,https://github.com/edenhill,1,https://github.com/edenhill/librdkafka/pull/2365,"Previous to this fix the app_polled time was only updated upon
entry to a (possibly) blocking function, such as consumer_poll().
If the call had a timeout > max.poll.interval.ms and there were no messages
for at least max.poll.interval.ms, the max poll check would kick in and
evict the consumer from the group due to inactivity.
We now signal to app_polled that we're in a blocking call
which prevents the max poll check to fire.","Previous to this fix the app_polled time was only updated upon
entry to a (possibly) blocking function, such as consumer_poll().
If the call had a timeout > max.poll.interval.ms and there were no messages
for at least max.poll.interval.ms, the max poll check would kick in and
evict the consumer from the group due to inactivity.
We now signal to app_polled that we're in a blocking call
which prevents the max poll check to fire.",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2366,2019-06-18T06:10:33Z,2019-06-18T07:56:23Z,2019-06-18T07:56:29Z,MERGED,True,1,1,1,https://github.com/sarkanyi,Missed a conversion while raising message_timeout_ms's limit,1,[],https://github.com/edenhill/librdkafka/pull/2366,https://github.com/sarkanyi,1,https://github.com/edenhill/librdkafka/pull/2366,,,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2366,2019-06-18T06:10:33Z,2019-06-18T07:56:23Z,2019-06-18T07:56:29Z,MERGED,True,1,1,1,https://github.com/sarkanyi,Missed a conversion while raising message_timeout_ms's limit,1,[],https://github.com/edenhill/librdkafka/pull/2366,https://github.com/edenhill,2,https://github.com/edenhill/librdkafka/pull/2366#issuecomment-502991880,,,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2370,2019-06-18T11:37:18Z,2019-06-18T14:28:17Z,2019-06-18T14:28:20Z,MERGED,True,12,9,5,https://github.com/edenhill,Reverse openssl requirement on v1.0.2,2,[],https://github.com/edenhill/librdkafka/pull/2370,https://github.com/edenhill,1,https://github.com/edenhill/librdkafka/pull/2370,Ubuntu 14.04 is still on v1.0.1,Ubuntu 14.04 is still on v1.0.1,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2372,2019-06-19T14:50:11Z,2019-07-08T18:50:48Z,2019-07-08T18:50:52Z,MERGED,True,419,174,51,https://github.com/edenhill,tests: added quick mode (-Q) to allow running integration tests on CI,1,[],https://github.com/edenhill/librdkafka/pull/2372,https://github.com/edenhill,1,https://github.com/edenhill/librdkafka/pull/2372,,,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2372,2019-06-19T14:50:11Z,2019-07-08T18:50:48Z,2019-07-08T18:50:52Z,MERGED,True,419,174,51,https://github.com/edenhill,tests: added quick mode (-Q) to allow running integration tests on CI,1,[],https://github.com/edenhill/librdkafka/pull/2372,https://github.com/rnpridgeon,2,https://github.com/edenhill/librdkafka/pull/2372#issuecomment-503623591,,Is it worth making note of the admin client chages in the commit message? Other than that lgtm,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2377,2019-06-23T16:22:38Z,2019-08-14T08:09:37Z,2019-08-14T12:57:40Z,MERGED,True,1891,30,19,https://github.com/mhowlett,Consumer transaction support,10,[],https://github.com/edenhill/librdkafka/pull/2377,https://github.com/mhowlett,1,https://github.com/edenhill/librdkafka/pull/2377,same as #2352 - opening again as github isn't updating the PR to reflect rebase to master / conflict resolution / commit squash.,same as #2352 - opening again as github isn't updating the PR to reflect rebase to master / conflict resolution / commit squash.,True,"{'HEART': ['https://github.com/maks-rafalko'], 'THUMBS_UP': ['https://github.com/maks-rafalko']}"
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2377,2019-06-23T16:22:38Z,2019-08-14T08:09:37Z,2019-08-14T12:57:40Z,MERGED,True,1891,30,19,https://github.com/mhowlett,Consumer transaction support,10,[],https://github.com/edenhill/librdkafka/pull/2377,https://github.com/edenhill,2,https://github.com/edenhill/librdkafka/pull/2377#issuecomment-510823394,same as #2352 - opening again as github isn't updating the PR to reflect rebase to master / conflict resolution / commit squash.,There's a couple of compilation warnings on the Appveyor builders that needs to be addressed.,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2377,2019-06-23T16:22:38Z,2019-08-14T08:09:37Z,2019-08-14T12:57:40Z,MERGED,True,1891,30,19,https://github.com/mhowlett,Consumer transaction support,10,[],https://github.com/edenhill/librdkafka/pull/2377,https://github.com/fmd,3,https://github.com/edenhill/librdkafka/pull/2377#issuecomment-520398844,same as #2352 - opening again as github isn't updating the PR to reflect rebase to master / conflict resolution / commit squash.,"Is there any likely movement on this @mhowlett, @edenhill? It seems very close",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2377,2019-06-23T16:22:38Z,2019-08-14T08:09:37Z,2019-08-14T12:57:40Z,MERGED,True,1891,30,19,https://github.com/mhowlett,Consumer transaction support,10,[],https://github.com/edenhill/librdkafka/pull/2377,https://github.com/edenhill,4,https://github.com/edenhill/librdkafka/pull/2377#issuecomment-521147158,same as #2352 - opening again as github isn't updating the PR to reflect rebase to master / conflict resolution / commit squash.,Merged!  Great work on this @mhowlett !,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2377,2019-06-23T16:22:38Z,2019-08-14T08:09:37Z,2019-08-14T12:57:40Z,MERGED,True,1891,30,19,https://github.com/mhowlett,Consumer transaction support,10,[],https://github.com/edenhill/librdkafka/pull/2377,https://github.com/maks-rafalko,5,https://github.com/edenhill/librdkafka/pull/2377#issuecomment-521230732,same as #2352 - opening again as github isn't updating the PR to reflect rebase to master / conflict resolution / commit squash.,"Do you have an idea when it can be released? Was waiting exactly for this PR ;)
Great work guys!",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2377,2019-06-23T16:22:38Z,2019-08-14T08:09:37Z,2019-08-14T12:57:40Z,MERGED,True,1891,30,19,https://github.com/mhowlett,Consumer transaction support,10,[],https://github.com/edenhill/librdkafka/pull/2377,https://github.com/fmd,6,https://github.com/edenhill/librdkafka/pull/2377#issuecomment-521234699,same as #2352 - opening again as github isn't updating the PR to reflect rebase to master / conflict resolution / commit squash.,Amazing!,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2378,2019-06-24T08:31:28Z,2019-07-08T18:53:02Z,2019-07-08T18:53:08Z,MERGED,True,8,9,1,https://github.com/binary85,Fixed socket recv error handling for MSVC build,2,[],https://github.com/edenhill/librdkafka/pull/2378,https://github.com/binary85,1,https://github.com/edenhill/librdkafka/pull/2378,"Added socket recv error handling for MSVC build.
There's no returns when recv returns SOCKET_ERROR and error code is not WSAEWOULDBLOCK.
It causes crashes(calls rd_buf_write with size -1).","Added socket recv error handling for MSVC build.
There's no returns when recv returns SOCKET_ERROR and error code is not WSAEWOULDBLOCK.
It causes crashes(calls rd_buf_write with size -1).",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2378,2019-06-24T08:31:28Z,2019-07-08T18:53:02Z,2019-07-08T18:53:08Z,MERGED,True,8,9,1,https://github.com/binary85,Fixed socket recv error handling for MSVC build,2,[],https://github.com/edenhill/librdkafka/pull/2378,https://github.com/edenhill,2,https://github.com/edenhill/librdkafka/pull/2378#issuecomment-509348091,"Added socket recv error handling for MSVC build.
There's no returns when recv returns SOCKET_ERROR and error code is not WSAEWOULDBLOCK.
It causes crashes(calls rd_buf_write with size -1).",Thank you!,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2379,2019-06-24T08:44:39Z,2019-07-12T12:47:30Z,2019-07-12T12:47:34Z,MERGED,True,6,4,1,https://github.com/binary85,Prevents destroy toppar when retry OffsetRequest,3,[],https://github.com/edenhill/librdkafka/pull/2379,https://github.com/binary85,1,https://github.com/edenhill/librdkafka/pull/2379,"Fix for #2373.
Call rd_kafka_handle_Offset after checking op version and only when err is not RD_KAFKA_RESP_ERR__OUTDATED.","Fix for #2373.
Call rd_kafka_handle_Offset after checking op version and only when err is not RD_KAFKA_RESP_ERR__OUTDATED.",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2379,2019-06-24T08:44:39Z,2019-07-12T12:47:30Z,2019-07-12T12:47:34Z,MERGED,True,6,4,1,https://github.com/binary85,Prevents destroy toppar when retry OffsetRequest,3,[],https://github.com/edenhill/librdkafka/pull/2379,https://github.com/edenhill,2,https://github.com/edenhill/librdkafka/pull/2379#issuecomment-510873513,"Fix for #2373.
Call rd_kafka_handle_Offset after checking op version and only when err is not RD_KAFKA_RESP_ERR__OUTDATED.",Thank you!,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2386,2019-07-01T16:13:39Z,2019-07-02T19:30:23Z,2019-07-02T19:30:28Z,MERGED,True,2,1,1,https://github.com/jspdt0,Fix message timeout check,1,[],https://github.com/edenhill/librdkafka/pull/2386,https://github.com/jspdt0,1,https://github.com/edenhill/librdkafka/pull/2386,"This check was added in the 1.0 release, but it ignores the special value of message_timeout_ms = 0, which indicates an infinite timeout. This patch makes the timeout check treat the special value appropriately.","This check was added in the 1.0 release, but it ignores the special value of message_timeout_ms = 0, which indicates an infinite timeout. This patch makes the timeout check treat the special value appropriately.",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2386,2019-07-01T16:13:39Z,2019-07-02T19:30:23Z,2019-07-02T19:30:28Z,MERGED,True,2,1,1,https://github.com/jspdt0,Fix message timeout check,1,[],https://github.com/edenhill/librdkafka/pull/2386,https://github.com/edenhill,2,https://github.com/edenhill/librdkafka/pull/2386#issuecomment-507814016,"This check was added in the 1.0 release, but it ignores the special value of message_timeout_ms = 0, which indicates an infinite timeout. This patch makes the timeout check treat the special value appropriately.",Thank you!,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2393,2019-07-04T18:25:13Z,2019-07-07T14:13:28Z,2019-07-07T14:13:33Z,MERGED,True,14,0,1,https://github.com/damour,Fill metadata in rd_kafka_committed & rd_kafka_offsets_for_times.,2,[],https://github.com/edenhill/librdkafka/pull/2393,https://github.com/damour,1,https://github.com/edenhill/librdkafka/pull/2393,Currently rd_kafka_committed returns list with empty  metadata:,Currently rd_kafka_committed returns list with empty  metadata:,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2393,2019-07-04T18:25:13Z,2019-07-07T14:13:28Z,2019-07-07T14:13:33Z,MERGED,True,14,0,1,https://github.com/damour,Fill metadata in rd_kafka_committed & rd_kafka_offsets_for_times.,2,[],https://github.com/edenhill/librdkafka/pull/2393,https://github.com/edenhill,2,https://github.com/edenhill/librdkafka/pull/2393#issuecomment-509002952,Currently rd_kafka_committed returns list with empty  metadata:,Thank you!,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2395,2019-07-07T17:23:01Z,2019-07-07T20:57:06Z,2019-07-07T20:57:09Z,MERGED,True,0,3,1,https://github.com/edenhill,Don't double the size of partition lists when size is specified.,1,[],https://github.com/edenhill/librdkafka/pull/2395,https://github.com/edenhill,1,https://github.com/edenhill/librdkafka/pull/2395,,,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2396,2019-07-08T09:34:02Z,2019-07-08T22:05:33Z,2020-05-08T08:59:03Z,MERGED,True,907,83,27,https://github.com/edenhill, Add on_thread_start and on_thread_exit interceptors,7,[],https://github.com/edenhill/librdkafka/pull/2396,https://github.com/edenhill,1,https://github.com/edenhill/librdkafka/pull/2396,CLIENTS-998,CLIENTS-998,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2398,2019-07-08T18:17:47Z,2019-07-08T21:42:58Z,2020-05-08T08:59:01Z,MERGED,True,261,75,14,https://github.com/edenhill, Add support for sub-millisecond linger.ms,3,[],https://github.com/edenhill/librdkafka/pull/2398,https://github.com/edenhill,1,https://github.com/edenhill/librdkafka/pull/2398,CLIENTS-998,CLIENTS-998,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2401,2019-07-09T09:14:12Z,2019-07-20T08:50:28Z,2019-07-22T14:51:48Z,MERGED,True,6,2,2,https://github.com/ngrande,Add documentation about ERR__PARTITION_EOF,2,[],https://github.com/edenhill/librdkafka/pull/2401,https://github.com/ngrande,1,https://github.com/edenhill/librdkafka/pull/2401,clarify default behavior of ERR__PARTITION_EOF even/error since v1.0.0,clarify default behavior of ERR__PARTITION_EOF even/error since v1.0.0,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2401,2019-07-09T09:14:12Z,2019-07-20T08:50:28Z,2019-07-22T14:51:48Z,MERGED,True,6,2,2,https://github.com/ngrande,Add documentation about ERR__PARTITION_EOF,2,[],https://github.com/edenhill/librdkafka/pull/2401,https://github.com/ngrande,2,https://github.com/edenhill/librdkafka/pull/2401#issuecomment-509562028,clarify default behavior of ERR__PARTITION_EOF even/error since v1.0.0,#2397,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2401,2019-07-09T09:14:12Z,2019-07-20T08:50:28Z,2019-07-22T14:51:48Z,MERGED,True,6,2,2,https://github.com/ngrande,Add documentation about ERR__PARTITION_EOF,2,[],https://github.com/edenhill/librdkafka/pull/2401,https://github.com/edenhill,3,https://github.com/edenhill/librdkafka/pull/2401#issuecomment-513450123,clarify default behavior of ERR__PARTITION_EOF even/error since v1.0.0,thank you!,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2405,2019-07-10T13:59:44Z,2021-03-23T18:16:15Z,2021-03-23T18:16:29Z,MERGED,True,158,40,19,https://github.com/sarkanyi,Be consistent with rd_malloc/rd_free combos.,1,[],https://github.com/edenhill/librdkafka/pull/2405,https://github.com/sarkanyi,1,https://github.com/edenhill/librdkafka/pull/2405,Sometimes rd_free is called on objects allocated with plain malloc. This is not consistent.,Sometimes rd_free is called on objects allocated with plain malloc. This is not consistent.,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2405,2019-07-10T13:59:44Z,2021-03-23T18:16:15Z,2021-03-23T18:16:29Z,MERGED,True,158,40,19,https://github.com/sarkanyi,Be consistent with rd_malloc/rd_free combos.,1,[],https://github.com/edenhill/librdkafka/pull/2405,https://github.com/sarkanyi,2,https://github.com/edenhill/librdkafka/pull/2405#issuecomment-510081819,Sometimes rd_free is called on objects allocated with plain malloc. This is not consistent.,"And I've found some more, updating soon.",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2405,2019-07-10T13:59:44Z,2021-03-23T18:16:15Z,2021-03-23T18:16:29Z,MERGED,True,158,40,19,https://github.com/sarkanyi,Be consistent with rd_malloc/rd_free combos.,1,[],https://github.com/edenhill/librdkafka/pull/2405,https://github.com/sarkanyi,3,https://github.com/edenhill/librdkafka/pull/2405#issuecomment-510121763,Sometimes rd_free is called on objects allocated with plain malloc. This is not consistent.,"Not sure why the unkpart test files, the commit has tehnically nothing to do with it.",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2405,2019-07-10T13:59:44Z,2021-03-23T18:16:15Z,2021-03-23T18:16:29Z,MERGED,True,158,40,19,https://github.com/sarkanyi,Be consistent with rd_malloc/rd_free combos.,1,[],https://github.com/edenhill/librdkafka/pull/2405,https://github.com/sarkanyi,4,https://github.com/edenhill/librdkafka/pull/2405#issuecomment-510125352,Sometimes rd_free is called on objects allocated with plain malloc. This is not consistent.,"I think the failure is related to the float-linger change, but have no time to investigate.",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2405,2019-07-10T13:59:44Z,2021-03-23T18:16:15Z,2021-03-23T18:16:29Z,MERGED,True,158,40,19,https://github.com/sarkanyi,Be consistent with rd_malloc/rd_free combos.,1,[],https://github.com/edenhill/librdkafka/pull/2405,https://github.com/edenhill,5,https://github.com/edenhill/librdkafka/pull/2405#issuecomment-805123607,Sometimes rd_free is called on objects allocated with plain malloc. This is not consistent.,Thanks for your perseverence!,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2406,2019-07-10T22:48:44Z,,2020-04-07T12:42:40Z,OPEN,False,1,1,1,https://github.com/PlacidBox,Fix UB left shift in zigzag encoding,1,[],https://github.com/edenhill/librdkafka/pull/2406,https://github.com/PlacidBox,1,https://github.com/edenhill/librdkafka/pull/2406,"Fixes an issue in #2403
Neither latest gcc/clang seem to have codegen differences based
on this change, and are unlikely ever to since left shift of
negative isn't UB in c++20. It's still worth fixing to get
less print outs when running with fsanitize=undefined","Fixes an issue in #2403
Neither latest gcc/clang seem to have codegen differences based
on this change, and are unlikely ever to since left shift of
negative isn't UB in c++20. It's still worth fixing to get
less print outs when running with fsanitize=undefined",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2406,2019-07-10T22:48:44Z,,2020-04-07T12:42:40Z,OPEN,False,1,1,1,https://github.com/PlacidBox,Fix UB left shift in zigzag encoding,1,[],https://github.com/edenhill/librdkafka/pull/2406,https://github.com/PlacidBox,2,https://github.com/edenhill/librdkafka/pull/2406#issuecomment-510280111,"Fixes an issue in #2403
Neither latest gcc/clang seem to have codegen differences based
on this change, and are unlikely ever to since left shift of
negative isn't UB in c++20. It's still worth fixing to get
less print outs when running with fsanitize=undefined",Looks like the 0002_unkpart test is failing in other PRs?,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2407,2019-07-11T00:31:57Z,2019-08-14T08:46:29Z,2019-08-15T04:36:31Z,MERGED,True,5,2,2,https://github.com/PlacidBox,Fix UBSan errors,3,[],https://github.com/edenhill/librdkafka/pull/2407,https://github.com/PlacidBox,1,https://github.com/edenhill/librdkafka/pull/2407,"rd_tmpabuf_write is sometimes called with a NULL buffer to clone,
leading to UB when calling memcpy.
It doesn't look like the func could just return NULL in this case
either. Almost all the callers ignore the possibility of NULL, or
are interpreting NULL as an allocation failure.","rd_tmpabuf_write is sometimes called with a NULL buffer to clone,
leading to UB when calling memcpy.
It doesn't look like the func could just return NULL in this case
either. Almost all the callers ignore the possibility of NULL, or
are interpreting NULL as an allocation failure.",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2407,2019-07-11T00:31:57Z,2019-08-14T08:46:29Z,2019-08-15T04:36:31Z,MERGED,True,5,2,2,https://github.com/PlacidBox,Fix UBSan errors,3,[],https://github.com/edenhill/librdkafka/pull/2407,https://github.com/PlacidBox,2,https://github.com/edenhill/librdkafka/pull/2407#issuecomment-510286153,"rd_tmpabuf_write is sometimes called with a NULL buffer to clone,
leading to UB when calling memcpy.
It doesn't look like the func could just return NULL in this case
either. Almost all the callers ignore the possibility of NULL, or
are interpreting NULL as an allocation failure.","bundled in the last fix for #2403, since it's an easy one",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2407,2019-07-11T00:31:57Z,2019-08-14T08:46:29Z,2019-08-15T04:36:31Z,MERGED,True,5,2,2,https://github.com/PlacidBox,Fix UBSan errors,3,[],https://github.com/edenhill/librdkafka/pull/2407,https://github.com/edenhill,3,https://github.com/edenhill/librdkafka/pull/2407#issuecomment-510376586,"rd_tmpabuf_write is sometimes called with a NULL buffer to clone,
leading to UB when calling memcpy.
It doesn't look like the func could just return NULL in this case
either. Almost all the callers ignore the possibility of NULL, or
are interpreting NULL as an allocation failure.","These are false positives, because why would memcpy or qsort read at any pointer address (null or not) if the size of the buffer or array is zero?",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2407,2019-07-11T00:31:57Z,2019-08-14T08:46:29Z,2019-08-15T04:36:31Z,MERGED,True,5,2,2,https://github.com/PlacidBox,Fix UBSan errors,3,[],https://github.com/edenhill/librdkafka/pull/2407,https://github.com/PlacidBox,4,https://github.com/edenhill/librdkafka/pull/2407#issuecomment-510410415,"rd_tmpabuf_write is sometimes called with a NULL buffer to clone,
leading to UB when calling memcpy.
It doesn't look like the func could just return NULL in this case
either. Almost all the callers ignore the possibility of NULL, or
are interpreting NULL as an allocation failure.","They're definitely not false positives, the compiler is very much free to optimise around the values being non-null, for example: https://godbolt.org/z/g1Lg0q. Note that there is no code path in foo_bad that calls bar, even if you pass in s=NULL count=0.
Sometimes you end up with time travel too.
I have no idea why memcpy and qsort don't allow NULL args when the length is 0, but the unfortunate reality is that they don't, and you end up with UBSan triggers on them that need to be fixed.
An alternative fix would also be to guarantee that a lot of the internal lists and buffers are never nullable, but i'm not familiar enough with the code base to make that far reaching a change.",True,{'HEART': ['https://github.com/edenhill']}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2407,2019-07-11T00:31:57Z,2019-08-14T08:46:29Z,2019-08-15T04:36:31Z,MERGED,True,5,2,2,https://github.com/PlacidBox,Fix UBSan errors,3,[],https://github.com/edenhill/librdkafka/pull/2407,https://github.com/edenhill,5,https://github.com/edenhill/librdkafka/pull/2407#issuecomment-510412769,"rd_tmpabuf_write is sometimes called with a NULL buffer to clone,
leading to UB when calling memcpy.
It doesn't look like the func could just return NULL in this case
either. Almost all the callers ignore the possibility of NULL, or
are interpreting NULL as an allocation failure.","Okay, fair enough.
Let's fix the places where it passes NULL, preferably with something like:
if (likely(src != NULL))
 memcpy(dst, src, srclen);",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2407,2019-07-11T00:31:57Z,2019-08-14T08:46:29Z,2019-08-15T04:36:31Z,MERGED,True,5,2,2,https://github.com/PlacidBox,Fix UBSan errors,3,[],https://github.com/edenhill/librdkafka/pull/2407,https://github.com/PlacidBox,6,https://github.com/edenhill/librdkafka/pull/2407#issuecomment-515278896,"rd_tmpabuf_write is sometimes called with a NULL buffer to clone,
leading to UB when calling memcpy.
It doesn't look like the func could just return NULL in this case
either. Almost all the callers ignore the possibility of NULL, or
are interpreting NULL as an allocation failure.","Sorry about the delay, was on other projects for a bit
Do you want me to rebase #2406 on to this PR? It's the same category of issue",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2407,2019-07-11T00:31:57Z,2019-08-14T08:46:29Z,2019-08-15T04:36:31Z,MERGED,True,5,2,2,https://github.com/PlacidBox,Fix UBSan errors,3,[],https://github.com/edenhill/librdkafka/pull/2407,https://github.com/edenhill,7,https://github.com/edenhill/librdkafka/pull/2407#issuecomment-521159025,"rd_tmpabuf_write is sometimes called with a NULL buffer to clone,
leading to UB when calling memcpy.
It doesn't look like the func could just return NULL in this case
either. Almost all the callers ignore the possibility of NULL, or
are interpreting NULL as an allocation failure.",Thank you for this!,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2409,2019-07-11T12:30:45Z,,2022-04-26T06:41:08Z,OPEN,False,168,36,9,https://github.com/sarkanyi,Add API to replace rd_kafka's default allocator,1,[],https://github.com/edenhill/librdkafka/pull/2409,https://github.com/sarkanyi,1,https://github.com/edenhill/librdkafka/pull/2409,Add API to replace rd_kafka's default allocator with a custom implementation,Add API to replace rd_kafka's default allocator with a custom implementation,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2409,2019-07-11T12:30:45Z,,2022-04-26T06:41:08Z,OPEN,False,168,36,9,https://github.com/sarkanyi,Add API to replace rd_kafka's default allocator,1,[],https://github.com/edenhill/librdkafka/pull/2409,https://github.com/sarkanyi,2,https://github.com/edenhill/librdkafka/pull/2409#issuecomment-512151821,Add API to replace rd_kafka's default allocator with a custom implementation,I think I covered all requests. Could you please re-check and merge if ok? Also please check and merge: #2405 because it's related. I can rebase both if needed,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2409,2019-07-11T12:30:45Z,,2022-04-26T06:41:08Z,OPEN,False,168,36,9,https://github.com/sarkanyi,Add API to replace rd_kafka's default allocator,1,[],https://github.com/edenhill/librdkafka/pull/2409,https://github.com/edenhill,3,https://github.com/edenhill/librdkafka/pull/2409#issuecomment-512177860,Add API to replace rd_kafka's default allocator with a custom implementation,"Will re-review soon, but this will not be merged until after the next release.",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2409,2019-07-11T12:30:45Z,,2022-04-26T06:41:08Z,OPEN,False,168,36,9,https://github.com/sarkanyi,Add API to replace rd_kafka's default allocator,1,[],https://github.com/edenhill/librdkafka/pull/2409,https://github.com/edenhill,4,https://github.com/edenhill/librdkafka/pull/2409#issuecomment-806465503,Add API to replace rd_kafka's default allocator with a custom implementation,"With your PR merged to add mem_malloc() et.al. I'm guessing this PR is no longer needed, an application can use LD_PRELOAD.. to replace the rd_malloc()* calls, right?",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2409,2019-07-11T12:30:45Z,,2022-04-26T06:41:08Z,OPEN,False,168,36,9,https://github.com/sarkanyi,Add API to replace rd_kafka's default allocator,1,[],https://github.com/edenhill/librdkafka/pull/2409,https://github.com/sarkanyi,5,https://github.com/edenhill/librdkafka/pull/2409#issuecomment-806490318,Add API to replace rd_kafka's default allocator with a custom implementation,"With your PR merged to add mem_malloc() et.al. I'm guessing this PR is no longer needed, an application can use LD_PRELOAD.. to replace the rd_malloc()* calls, right?

To be honest it would be preferable to have an API, either this - I'll change to have rd_str_dup & co also added, or function hooks something like:
""typeof (&rd_malloc) malloc_fn = rd_malloc_hook ? rd_malloc_hook : malloc;
void *p = malloc_fn(sz);"" -set via a setter function?",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2409,2019-07-11T12:30:45Z,,2022-04-26T06:41:08Z,OPEN,False,168,36,9,https://github.com/sarkanyi,Add API to replace rd_kafka's default allocator,1,[],https://github.com/edenhill/librdkafka/pull/2409,https://github.com/sarkanyi,6,https://github.com/edenhill/librdkafka/pull/2409#issuecomment-806494813,Add API to replace rd_kafka's default allocator with a custom implementation,"Most big libraries use some sort of API to set the custom allocator, ie: jansson has void json_set_alloc_funcs(json_malloc_t malloc_fn, json_free_t free_fn), it makes it more explicit. LD_PRELOAD is quite invasive, even if we'd overload rd_funcs",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2409,2019-07-11T12:30:45Z,,2022-04-26T06:41:08Z,OPEN,False,168,36,9,https://github.com/sarkanyi,Add API to replace rd_kafka's default allocator,1,[],https://github.com/edenhill/librdkafka/pull/2409,https://github.com/edenhill,7,https://github.com/edenhill/librdkafka/pull/2409#issuecomment-814146261,Add API to replace rd_kafka's default allocator with a custom implementation,"Okay, a single function to set the allocators once on program startup before calling any other librdkafka function (in particular before global_init is called) should be fine and avoid the need for locking.",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2409,2019-07-11T12:30:45Z,,2022-04-26T06:41:08Z,OPEN,False,168,36,9,https://github.com/sarkanyi,Add API to replace rd_kafka's default allocator,1,[],https://github.com/edenhill/librdkafka/pull/2409,https://github.com/sarkanyi,8,https://github.com/edenhill/librdkafka/pull/2409#issuecomment-883103866,Add API to replace rd_kafka's default allocator with a custom implementation,"Hi, is this still on track for 1.8? Do you need me to do anything else on it?",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2409,2019-07-11T12:30:45Z,,2022-04-26T06:41:08Z,OPEN,False,168,36,9,https://github.com/sarkanyi,Add API to replace rd_kafka's default allocator,1,[],https://github.com/edenhill/librdkafka/pull/2409,https://github.com/sarkanyi,9,https://github.com/edenhill/librdkafka/pull/2409#issuecomment-1109407591,Add API to replace rd_kafka's default allocator with a custom implementation,@edenhill won't this change make it into 1.9.0? Thanks,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2413,2019-07-15T08:19:55Z,2020-03-27T13:59:07Z,2020-03-27T13:59:12Z,MERGED,True,626,91,38,https://github.com/edenhill,Static library bundles,14,[],https://github.com/edenhill/librdkafka/pull/2413,https://github.com/edenhill,1,https://github.com/edenhill/librdkafka/pull/2413,"Build static library bundles containing all external dependencies of librdkafka (or as many as possible).
This is to be used with confluent-kafka-go client (demo here).
Related Go PR:
confluentinc/confluent-kafka-go#355","Build static library bundles containing all external dependencies of librdkafka (or as many as possible).
This is to be used with confluent-kafka-go client (demo here).
Related Go PR:
confluentinc/confluent-kafka-go#355",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2413,2019-07-15T08:19:55Z,2020-03-27T13:59:07Z,2020-03-27T13:59:12Z,MERGED,True,626,91,38,https://github.com/edenhill,Static library bundles,14,[],https://github.com/edenhill/librdkafka/pull/2413,https://github.com/mikekamornikov,2,https://github.com/edenhill/librdkafka/pull/2413#issuecomment-561613698,"Build static library bundles containing all external dependencies of librdkafka (or as many as possible).
This is to be used with confluent-kafka-go client (demo here).
Related Go PR:
confluentinc/confluent-kafka-go#355",@edenhill What is the status of this one? Should we expect it (and confluentinc/confluent-kafka-go#355) to me merged anytime soon? The thing should simplify a lot bazel builds. Currently I have to build librdkafka separately via rules_foreign_cc and do some magic to tie it to confluent-kafka-go. I can't cross compile the result this way (bazel complains about missing toolchain). And for me it means a switch to some pure go client.,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2416,2019-07-17T20:23:56Z,2019-08-15T09:25:27Z,2019-08-15T09:25:27Z,MERGED,True,4,2,2,https://github.com/mhowlett,escape || inside markdown table,3,[],https://github.com/edenhill/librdkafka/pull/2416,https://github.com/mhowlett,1,https://github.com/edenhill/librdkafka/pull/2416,alternatively could surround the default value in back ticks.,alternatively could surround the default value in back ticks.,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2420,2019-07-18T20:17:17Z,2019-07-19T09:37:13Z,2019-07-29T17:59:33Z,MERGED,True,1,2,1,https://github.com/hunterjackson,use hostname when validating hostname,1,[],https://github.com/edenhill/librdkafka/pull/2420,https://github.com/hunterjackson,1,https://github.com/edenhill/librdkafka/pull/2420,See issue #2419 for more background,See issue #2419 for more background,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2420,2019-07-18T20:17:17Z,2019-07-19T09:37:13Z,2019-07-29T17:59:33Z,MERGED,True,1,2,1,https://github.com/hunterjackson,use hostname when validating hostname,1,[],https://github.com/edenhill/librdkafka/pull/2420,https://github.com/edenhill,2,https://github.com/edenhill/librdkafka/pull/2420#issuecomment-513164625,See issue #2419 for more background,Thank you!,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2420,2019-07-18T20:17:17Z,2019-07-19T09:37:13Z,2019-07-29T17:59:33Z,MERGED,True,1,2,1,https://github.com/hunterjackson,use hostname when validating hostname,1,[],https://github.com/edenhill/librdkafka/pull/2420,https://github.com/hunterjackson,3,https://github.com/edenhill/librdkafka/pull/2420#issuecomment-513865632,See issue #2419 for more background,Do you have an ETA on when this will make it into a release?,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2420,2019-07-18T20:17:17Z,2019-07-19T09:37:13Z,2019-07-29T17:59:33Z,MERGED,True,1,2,1,https://github.com/hunterjackson,use hostname when validating hostname,1,[],https://github.com/edenhill/librdkafka/pull/2420,https://github.com/edenhill,4,https://github.com/edenhill/librdkafka/pull/2420#issuecomment-516098247,See issue #2419 for more background,mid/end august,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2421,2019-07-18T21:33:10Z,2019-07-30T08:18:42Z,2019-07-30T08:18:47Z,MERGED,True,13,6,1,https://github.com/hannip,don't fail kinit for ECHILD,5,[],https://github.com/edenhill/librdkafka/pull/2421,https://github.com/hannip,1,https://github.com/edenhill/librdkafka/pull/2421,When librdkafka is used as a plugin and there is a signal handler that ignores ECHILD for plugins the kinit system call returns a -1 with errno ECHILD.  This can be safely ignored and just means the command finished before the wait for it could be issued.  This fixes an issue with pmacct using librdkafka as a plugin.,When librdkafka is used as a plugin and there is a signal handler that ignores ECHILD for plugins the kinit system call returns a -1 with errno ECHILD.  This can be safely ignored and just means the command finished before the wait for it could be issued.  This fixes an issue with pmacct using librdkafka as a plugin.,True,{'THUMBS_UP': ['https://github.com/ymzong']}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2421,2019-07-18T21:33:10Z,2019-07-30T08:18:42Z,2019-07-30T08:18:47Z,MERGED,True,13,6,1,https://github.com/hannip,don't fail kinit for ECHILD,5,[],https://github.com/edenhill/librdkafka/pull/2421,https://github.com/edenhill,2,https://github.com/edenhill/librdkafka/pull/2421#issuecomment-516316546,When librdkafka is used as a plugin and there is a signal handler that ignores ECHILD for plugins the kinit system call returns a -1 with errno ECHILD.  This can be safely ignored and just means the command finished before the wait for it could be issued.  This fixes an issue with pmacct using librdkafka as a plugin.,Thank you!,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2427,2019-07-24T10:12:35Z,2019-07-24T10:24:35Z,2020-05-08T08:59:04Z,CLOSED,False,1,1,1,https://github.com/edenhill,test markdown double pipery,2,[],https://github.com/edenhill/librdkafka/pull/2427,https://github.com/edenhill,1,https://github.com/edenhill/librdkafka/pull/2427,,,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2428,2019-07-24T11:03:12Z,2019-07-25T08:23:15Z,2019-07-25T08:23:19Z,MERGED,True,1,1,1,https://github.com/njzcx,fix#2425 rdkafka_example_cpp -L to list metadata can't assign topic,1,[],https://github.com/edenhill/librdkafka/pull/2428,https://github.com/njzcx,1,https://github.com/edenhill/librdkafka/pull/2428,"./rdkafka_example_cpp -L -b 172.17.0.2:9092 -t test_topic
./rdkafka_example_cpp -L -b 172.17.0.2:9092
execute above two commands, the result is the same. The reason is topic!=NULL should be !topic in 661 line","./rdkafka_example_cpp -L -b 172.17.0.2:9092 -t test_topic
./rdkafka_example_cpp -L -b 172.17.0.2:9092
execute above two commands, the result is the same. The reason is topic!=NULL should be !topic in 661 line",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2428,2019-07-24T11:03:12Z,2019-07-25T08:23:15Z,2019-07-25T08:23:19Z,MERGED,True,1,1,1,https://github.com/njzcx,fix#2425 rdkafka_example_cpp -L to list metadata can't assign topic,1,[],https://github.com/edenhill/librdkafka/pull/2428,https://github.com/edenhill,2,https://github.com/edenhill/librdkafka/pull/2428#issuecomment-514948202,"./rdkafka_example_cpp -L -b 172.17.0.2:9092 -t test_topic
./rdkafka_example_cpp -L -b 172.17.0.2:9092
execute above two commands, the result is the same. The reason is topic!=NULL should be !topic in 661 line",Thank you!,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2430,2019-07-25T10:36:31Z,2019-08-01T12:01:44Z,2019-08-01T12:01:44Z,CLOSED,False,6,2,1,https://github.com/kenneth-jia,Fix rd_kafka_handle_OffsetCommit,1,[],https://github.com/edenhill/librdkafka/pull/2430,https://github.com/kenneth-jia,1,https://github.com/edenhill/librdkafka/pull/2430,"To avoid consumer be stuck while closing if no response for outgoing OffsetCommit.
To reproduce the problem,

Start a consumer. (enable.auto.offset.store=false, enable.auto.commit=false, auto.commit.interval.ms=0)
Let consumer poll some messages.
Block all kafka brokers. (kill -STOP xxx. note: here we keep blocking it)
Let consumer commit these offsets (for previously polled messages)
Call rdkafka_consumer_close, and wait...wait...  It just could NEVER be finished!

Root cause:
Since the OffsetCommitRequest is in the output queue, it would keep retrying (incr_retry is 0) and refuse admitting failure. Thus, the rkcg_wait_commit_cnt could never become 0!

THE FOLLOWING DISCLAIMER APPLIES TO ALL SOFTWARE CODE AND OTHER MATERIALS CONTRIBUTED IN CONNECTION WITH THIS SOFTWARE:
THIS SOFTWARE IS LICENSED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS AS IS AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE AND ANY WARRANTY OF NON-INFRINGEMENT, ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT OWNER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE. THIS SOFTWARE MAY BE REDISTRIBUTED TO OTHERS ONLY BY EFFECTIVELY USING THIS OR ANOTHER EQUIVALENT DISCLAIMER IN ADDITION TO ANY OTHER REQUIRED LICENSE TERMS.
ONLY THE SOFTWARE CODE AND OTHER MATERIALS CONTRIBUTED IN CONNECTION WITH THIS SOFTWARE, IF ANY, THAT ARE ATTACHED TO (OR OTHERWISE ACCOMPANY) THIS SUBMISSION (AND ORDINARY COURSE CONTRIBUTIONS OF FUTURES PATCHES THERETO) ARE TO BE CONSIDERED A CONTRIBUTION. NO OTHER SOFTWARE CODE OR MATERIALS ARE A CONTRIBUTION.","To avoid consumer be stuck while closing if no response for outgoing OffsetCommit.
To reproduce the problem,

Start a consumer. (enable.auto.offset.store=false, enable.auto.commit=false, auto.commit.interval.ms=0)
Let consumer poll some messages.
Block all kafka brokers. (kill -STOP xxx. note: here we keep blocking it)
Let consumer commit these offsets (for previously polled messages)
Call rdkafka_consumer_close, and wait...wait...  It just could NEVER be finished!

Root cause:
Since the OffsetCommitRequest is in the output queue, it would keep retrying (incr_retry is 0) and refuse admitting failure. Thus, the rkcg_wait_commit_cnt could never become 0!

THE FOLLOWING DISCLAIMER APPLIES TO ALL SOFTWARE CODE AND OTHER MATERIALS CONTRIBUTED IN CONNECTION WITH THIS SOFTWARE:
THIS SOFTWARE IS LICENSED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS AS IS AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE AND ANY WARRANTY OF NON-INFRINGEMENT, ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT OWNER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE. THIS SOFTWARE MAY BE REDISTRIBUTED TO OTHERS ONLY BY EFFECTIVELY USING THIS OR ANOTHER EQUIVALENT DISCLAIMER IN ADDITION TO ANY OTHER REQUIRED LICENSE TERMS.
ONLY THE SOFTWARE CODE AND OTHER MATERIALS CONTRIBUTED IN CONNECTION WITH THIS SOFTWARE, IF ANY, THAT ARE ATTACHED TO (OR OTHERWISE ACCOMPANY) THIS SUBMISSION (AND ORDINARY COURSE CONTRIBUTIONS OF FUTURES PATCHES THERETO) ARE TO BE CONSIDERED A CONTRIBUTION. NO OTHER SOFTWARE CODE OR MATERIALS ARE A CONTRIBUTION.",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2430,2019-07-25T10:36:31Z,2019-08-01T12:01:44Z,2019-08-01T12:01:44Z,CLOSED,False,6,2,1,https://github.com/kenneth-jia,Fix rd_kafka_handle_OffsetCommit,1,[],https://github.com/edenhill/librdkafka/pull/2430,https://github.com/edenhill,2,https://github.com/edenhill/librdkafka/pull/2430#issuecomment-517255728,"To avoid consumer be stuck while closing if no response for outgoing OffsetCommit.
To reproduce the problem,

Start a consumer. (enable.auto.offset.store=false, enable.auto.commit=false, auto.commit.interval.ms=0)
Let consumer poll some messages.
Block all kafka brokers. (kill -STOP xxx. note: here we keep blocking it)
Let consumer commit these offsets (for previously polled messages)
Call rdkafka_consumer_close, and wait...wait...  It just could NEVER be finished!

Root cause:
Since the OffsetCommitRequest is in the output queue, it would keep retrying (incr_retry is 0) and refuse admitting failure. Thus, the rkcg_wait_commit_cnt could never become 0!

THE FOLLOWING DISCLAIMER APPLIES TO ALL SOFTWARE CODE AND OTHER MATERIALS CONTRIBUTED IN CONNECTION WITH THIS SOFTWARE:
THIS SOFTWARE IS LICENSED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS AS IS AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE AND ANY WARRANTY OF NON-INFRINGEMENT, ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT OWNER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE. THIS SOFTWARE MAY BE REDISTRIBUTED TO OTHERS ONLY BY EFFECTIVELY USING THIS OR ANOTHER EQUIVALENT DISCLAIMER IN ADDITION TO ANY OTHER REQUIRED LICENSE TERMS.
ONLY THE SOFTWARE CODE AND OTHER MATERIALS CONTRIBUTED IN CONNECTION WITH THIS SOFTWARE, IF ANY, THAT ARE ATTACHED TO (OR OTHERWISE ACCOMPANY) THIS SUBMISSION (AND ORDINARY COURSE CONTRIBUTIONS OF FUTURES PATCHES THERETO) ARE TO BE CONSIDERED A CONTRIBUTION. NO OTHER SOFTWARE CODE OR MATERIALS ARE A CONTRIBUTION.","Thanks for your contribution, this has been fixed in #2440",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2433,2019-07-26T15:54:56Z,2019-07-29T16:10:49Z,2019-07-29T16:10:57Z,MERGED,True,5,1,1,https://github.com/salisbury-espinosa,fix for processing of enable.ssl.certificate.verification,5,[],https://github.com/edenhill/librdkafka/pull/2433,https://github.com/salisbury-espinosa,1,https://github.com/edenhill/librdkafka/pull/2433,"ignoring result of SSL_get_verify_result if enable_verify == false
lil chore: using SSL_VERIFY_NONE instead of 0 in SSL_CTX_set_verify
the problem this patch closes is:
if I have a self-signed and expired certificate, then I cant get the software to work with the option enable.ssl.certificate.verification = false, because the rd_kafka_transport_ssl_verify function will always return -1 and the process will always try to reconnect","ignoring result of SSL_get_verify_result if enable_verify == false
lil chore: using SSL_VERIFY_NONE instead of 0 in SSL_CTX_set_verify
the problem this patch closes is:
if I have a self-signed and expired certificate, then I cant get the software to work with the option enable.ssl.certificate.verification = false, because the rd_kafka_transport_ssl_verify function will always return -1 and the process will always try to reconnect",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2433,2019-07-26T15:54:56Z,2019-07-29T16:10:49Z,2019-07-29T16:10:57Z,MERGED,True,5,1,1,https://github.com/salisbury-espinosa,fix for processing of enable.ssl.certificate.verification,5,[],https://github.com/edenhill/librdkafka/pull/2433,https://github.com/edenhill,2,https://github.com/edenhill/librdkafka/pull/2433#issuecomment-516056952,"ignoring result of SSL_get_verify_result if enable_verify == false
lil chore: using SSL_VERIFY_NONE instead of 0 in SSL_CTX_set_verify
the problem this patch closes is:
if I have a self-signed and expired certificate, then I cant get the software to work with the option enable.ssl.certificate.verification = false, because the rd_kafka_transport_ssl_verify function will always return -1 and the process will always try to reconnect",Thank you for your contribution!,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2440,2019-07-30T09:00:05Z,2019-08-01T12:00:57Z,2019-08-01T12:01:00Z,MERGED,True,7,1,2,https://github.com/edenhill,Defer commit on transport error to avoid consumer_close hang,1,[],https://github.com/edenhill/librdkafka/pull/2440,https://github.com/edenhill,1,https://github.com/edenhill/librdkafka/pull/2440,,,True,{'THUMBS_UP': ['https://github.com/kenneth-jia']}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2440,2019-07-30T09:00:05Z,2019-08-01T12:00:57Z,2019-08-01T12:01:00Z,MERGED,True,7,1,2,https://github.com/edenhill,Defer commit on transport error to avoid consumer_close hang,1,[],https://github.com/edenhill/librdkafka/pull/2440,https://github.com/kenneth-jia,2,https://github.com/edenhill/librdkafka/pull/2440#issuecomment-516366318,,"It works, thank a lot!",True,{'THUMBS_UP': ['https://github.com/edenhill']}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2441,2019-07-30T13:44:23Z,2019-08-01T12:37:52Z,2019-08-01T12:37:54Z,MERGED,True,270,70,9,https://github.com/edenhill,Add support for KIP-152 authentication errors,1,[],https://github.com/edenhill/librdkafka/pull/2441,https://github.com/edenhill,1,https://github.com/edenhill/librdkafka/pull/2441,,,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2442,2019-07-30T14:05:26Z,2019-08-14T08:42:58Z,2019-08-14T08:43:02Z,MERGED,True,27,2,2,https://github.com/vsakharuk,Fix memory leak in cpp:Headers,1,[],https://github.com/edenhill/librdkafka/pull/2442,https://github.com/vsakharuk,1,https://github.com/edenhill/librdkafka/pull/2442,"Change-Id: Ib0aab2e663a6ec63426605f4bfb527a6ee95fd94
#2431
This proposed fix for the Headers memory leaks in some case.
Have a nice day.","Change-Id: Ib0aab2e663a6ec63426605f4bfb527a6ee95fd94
#2431
This proposed fix for the Headers memory leaks in some case.
Have a nice day.",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2442,2019-07-30T14:05:26Z,2019-08-14T08:42:58Z,2019-08-14T08:43:02Z,MERGED,True,27,2,2,https://github.com/vsakharuk,Fix memory leak in cpp:Headers,1,[],https://github.com/edenhill/librdkafka/pull/2442,https://github.com/edenhill,2,https://github.com/edenhill/librdkafka/pull/2442#issuecomment-521157876,"Change-Id: Ib0aab2e663a6ec63426605f4bfb527a6ee95fd94
#2431
This proposed fix for the Headers memory leaks in some case.
Have a nice day.",Thank you!,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2444,2019-07-31T08:06:58Z,2019-08-01T11:15:07Z,2019-08-01T11:15:09Z,MERGED,True,67,60,17,https://github.com/edenhill,Fix compare overflows (#2443),2,[],https://github.com/edenhill/librdkafka/pull/2444,https://github.com/edenhill,1,https://github.com/edenhill/librdkafka/pull/2444,,,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2446,2019-08-01T12:24:29Z,2019-08-01T12:47:14Z,2019-08-01T12:47:17Z,MERGED,True,19,12,3,https://github.com/edenhill,Increase queue.buffering.max.kbytes max to INT_MAX,1,[],https://github.com/edenhill/librdkafka/pull/2446,https://github.com/edenhill,1,https://github.com/edenhill/librdkafka/pull/2446,for confluentinc/confluent-kafka-go#356,for confluentinc/confluent-kafka-go#356,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2452,2019-08-02T21:28:39Z,,2020-10-21T11:06:59Z,OPEN,False,8,3,2,https://github.com/yhager,allow compilation with a non-standard cyrus-sasl,1,[],https://github.com/edenhill/librdkafka/pull/2452,https://github.com/yhager,1,https://github.com/edenhill/librdkafka/pull/2452,"This should fix #2451. Notice that I updated the minimum cmake requirement, so I can use the IMPORTED_TARGET functionality. I found that to be the easiest solution, and cmake 3.6 should be widely available.","This should fix #2451. Notice that I updated the minimum cmake requirement, so I can use the IMPORTED_TARGET functionality. I found that to be the easiest solution, and cmake 3.6 should be widely available.",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2452,2019-08-02T21:28:39Z,,2020-10-21T11:06:59Z,OPEN,False,8,3,2,https://github.com/yhager,allow compilation with a non-standard cyrus-sasl,1,[],https://github.com/edenhill/librdkafka/pull/2452,https://github.com/edenhill,2,https://github.com/edenhill/librdkafka/pull/2452#issuecomment-520349323,"This should fix #2451. Notice that I updated the minimum cmake requirement, so I can use the IMPORTED_TARGET functionality. I found that to be the easiest solution, and cmake 3.6 should be widely available.","Since this breaks Ubuntu <=16.04 builds, which is in wide use, I can't accept this change at this point.",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2452,2019-08-02T21:28:39Z,,2020-10-21T11:06:59Z,OPEN,False,8,3,2,https://github.com/yhager,allow compilation with a non-standard cyrus-sasl,1,[],https://github.com/edenhill/librdkafka/pull/2452,https://github.com/edenhill,3,https://github.com/edenhill/librdkafka/pull/2452#issuecomment-713491538,"This should fix #2451. Notice that I updated the minimum cmake requirement, so I can use the IMPORTED_TARGET functionality. I found that to be the easiest solution, and cmake 3.6 should be widely available.",Is this PR still required after @benesch 's changes?,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2454,2019-08-04T19:40:40Z,2019-08-12T09:02:39Z,2019-08-12T09:02:44Z,MERGED,True,1,1,1,https://github.com/rixed,Minor correction in documentation,1,[],https://github.com/edenhill/librdkafka/pull/2454,https://github.com/rixed,1,https://github.com/edenhill/librdkafka/pull/2454,,,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2454,2019-08-04T19:40:40Z,2019-08-12T09:02:39Z,2019-08-12T09:02:44Z,MERGED,True,1,1,1,https://github.com/rixed,Minor correction in documentation,1,[],https://github.com/edenhill/librdkafka/pull/2454,https://github.com/edenhill,2,https://github.com/edenhill/librdkafka/pull/2454#issuecomment-520347946,,Thank you!,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2457,2019-08-06T03:09:51Z,2019-08-14T08:21:44Z,2019-08-14T18:31:28Z,MERGED,True,32,0,6,https://github.com/benesch,Set thread names on macOS,1,[],https://github.com/edenhill/librdkafka/pull/2457,https://github.com/benesch,1,https://github.com/edenhill/librdkafka/pull/2457,"Like Linux, macOS supports the nonstandard pthread_setname_np, though
the signature is slightly different. Learn to detect and use macOS
variants to, to make multithreaded debugging on macOS a bit easier.","Like Linux, macOS supports the nonstandard pthread_setname_np, though
the signature is slightly different. Learn to detect and use macOS
variants to, to make multithreaded debugging on macOS a bit easier.",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2457,2019-08-06T03:09:51Z,2019-08-14T08:21:44Z,2019-08-14T18:31:28Z,MERGED,True,32,0,6,https://github.com/benesch,Set thread names on macOS,1,[],https://github.com/edenhill/librdkafka/pull/2457,https://github.com/edenhill,2,https://github.com/edenhill/librdkafka/pull/2457#issuecomment-520346188,"Like Linux, macOS supports the nonstandard pthread_setname_np, though
the signature is slightly different. Learn to detect and use macOS
variants to, to make multithreaded debugging on macOS a bit easier.","Seems to be a missing include or feature define for the declaration:
https://travis-ci.org/edenhill/librdkafka/jobs/568176835#L230",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2457,2019-08-06T03:09:51Z,2019-08-14T08:21:44Z,2019-08-14T18:31:28Z,MERGED,True,32,0,6,https://github.com/benesch,Set thread names on macOS,1,[],https://github.com/edenhill/librdkafka/pull/2457,https://github.com/benesch,3,https://github.com/edenhill/librdkafka/pull/2457#issuecomment-520935322,"Like Linux, macOS supports the nonstandard pthread_setname_np, though
the signature is slightly different. Learn to detect and use macOS
variants to, to make multithreaded debugging on macOS a bit easier.","Seems to be a missing include or feature define for the declaration:
https://travis-ci.org/edenhill/librdkafka/jobs/568176835#L230

Indeed, looks like _DARWIN_C_SOURCE needs to be set. Note that simply setting it before including pthread.h is insufficient, as it needs to be set for all system headers or none of them (or else various things go wrong), so I added the define for it in rdkafka.h.
Also, heads up: the latest revision of this PR adds the same modifications to the CMake build system.",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2457,2019-08-06T03:09:51Z,2019-08-14T08:21:44Z,2019-08-14T18:31:28Z,MERGED,True,32,0,6,https://github.com/benesch,Set thread names on macOS,1,[],https://github.com/edenhill/librdkafka/pull/2457,https://github.com/edenhill,4,https://github.com/edenhill/librdkafka/pull/2457#issuecomment-521150834,"Like Linux, macOS supports the nonstandard pthread_setname_np, though
the signature is slightly different. Learn to detect and use macOS
variants to, to make multithreaded debugging on macOS a bit easier.",Thank you! ,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2457,2019-08-06T03:09:51Z,2019-08-14T08:21:44Z,2019-08-14T18:31:28Z,MERGED,True,32,0,6,https://github.com/benesch,Set thread names on macOS,1,[],https://github.com/edenhill/librdkafka/pull/2457,https://github.com/benesch,5,https://github.com/edenhill/librdkafka/pull/2457#issuecomment-521364470,"Like Linux, macOS supports the nonstandard pthread_setname_np, though
the signature is slightly different. Learn to detect and use macOS
variants to, to make multithreaded debugging on macOS a bit easier.","Awesome, thanks @edenhill!",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2459,2019-08-07T18:14:33Z,2019-08-12T08:43:25Z,2019-08-12T08:43:25Z,MERGED,True,8,2,1,https://github.com/ybyzek,DEVX-974: Cross link librdkafka binding repos to examples repo for Cloud,1,[],https://github.com/edenhill/librdkafka/pull/2459,https://github.com/ybyzek,1,https://github.com/edenhill/librdkafka/pull/2459,"cc @rspurgeon , @edenhill","cc @rspurgeon , @edenhill",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2465,2019-08-09T18:31:35Z,2019-08-12T18:21:48Z,2019-08-12T18:21:48Z,CLOSED,False,6,4,1,https://github.com/lizthegrey,"Fix deadlock between rd_kafka_curr_msgs_{add,sub}",1,[],https://github.com/edenhill/librdkafka/pull/2465,https://github.com/lizthegrey,1,https://github.com/edenhill/librdkafka/pull/2465,"If rk_curr_msgs.cnt is 0 and cnt is more than rk_curr_msgs.max_cnt, this method will otherwise never return, because rd_kafka_curr_msgs_sub ran too fast.
#2464
confluentinc/confluent-kafka-go-dev#2","If rk_curr_msgs.cnt is 0 and cnt is more than rk_curr_msgs.max_cnt, this method will otherwise never return, because rd_kafka_curr_msgs_sub ran too fast.
#2464
confluentinc/confluent-kafka-go-dev#2",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2468,2019-08-12T13:19:40Z,2019-08-13T07:44:07Z,2019-08-13T07:44:10Z,MERGED,True,10,47,3,https://github.com/edenhill,Avoid max.poll.interval tracking when in Producer mode,2,[],https://github.com/edenhill/librdkafka/pull/2468,https://github.com/edenhill,1,https://github.com/edenhill/librdkafka/pull/2468,,,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2468,2019-08-12T13:19:40Z,2019-08-13T07:44:07Z,2019-08-13T07:44:10Z,MERGED,True,10,47,3,https://github.com/edenhill,Avoid max.poll.interval tracking when in Producer mode,2,[],https://github.com/edenhill/librdkafka/pull/2468,https://github.com/mhowlett,2,https://github.com/edenhill/librdkafka/pull/2468#issuecomment-520617822,,lgtm,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2471,2019-08-13T11:05:47Z,2019-08-14T08:17:02Z,2019-08-14T08:17:05Z,MERGED,True,118,27,4,https://github.com/edenhill,Producer: Messages were not timed out for leader-less partitions,1,[],https://github.com/edenhill/librdkafka/pull/2471,https://github.com/edenhill,1,https://github.com/edenhill/librdkafka/pull/2471,"confluentinc/confluent-kafka-dotnet#1027
This also improves the granularity of message timeout scans to
match the actual message timeout. (#2202)
The minimum effective message timeout will still be 1000ms, but the
granularity after the initial 1000ms is now on millisecond precision.","confluentinc/confluent-kafka-dotnet#1027
This also improves the granularity of message timeout scans to
match the actual message timeout. (#2202)
The minimum effective message timeout will still be 1000ms, but the
granularity after the initial 1000ms is now on millisecond precision.",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2473,2019-08-14T09:51:32Z,2019-08-16T09:38:46Z,2019-08-16T09:38:49Z,MERGED,True,82,37,5,https://github.com/edenhill,Make rd_kafka_pause|resume_partitions() synchronous (#2455),1,[],https://github.com/edenhill/librdkafka/pull/2473,https://github.com/edenhill,1,https://github.com/edenhill/librdkafka/pull/2473,"This makes sure that a consumer_poll() call after pause() will not
return any messages.","This makes sure that a consumer_poll() call after pause() will not
return any messages.",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2476,2019-08-15T15:14:19Z,2019-08-16T09:14:26Z,2019-08-16T09:14:28Z,MERGED,True,49,16,6,https://github.com/edenhill,Refresh broker list even if no topics to refresh (#2466),2,[],https://github.com/edenhill/librdkafka/pull/2476,https://github.com/edenhill,1,https://github.com/edenhill/librdkafka/pull/2476,,,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2476,2019-08-15T15:14:19Z,2019-08-16T09:14:26Z,2019-08-16T09:14:28Z,MERGED,True,49,16,6,https://github.com/edenhill,Refresh broker list even if no topics to refresh (#2466),2,[],https://github.com/edenhill/librdkafka/pull/2476,https://github.com/mhowlett,2,https://github.com/edenhill/librdkafka/pull/2476#issuecomment-521717046,,lgtm. probably worth reworking the doc string a bit i think.,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2477,2019-08-15T22:57:04Z,2019-08-16T08:05:29Z,2019-08-16T08:05:30Z,MERGED,True,2,2,1,https://github.com/mhowlett,Fix fetch request rkbuf size,1,[],https://github.com/edenhill/librdkafka/pull/2477,https://github.com/mhowlett,1,https://github.com/edenhill/librdkafka/pull/2477,,,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2479,2019-08-16T10:31:10Z,2019-08-19T17:36:00Z,2019-08-19T17:36:03Z,MERGED,True,8,10,1,https://github.com/edenhill,Tidy up rd_kafka_new() conf freeing to make code clearer (#2478),1,[],https://github.com/edenhill/librdkafka/pull/2479,https://github.com/edenhill,1,https://github.com/edenhill/librdkafka/pull/2479,"This is not a problem in practice, but the code was wrong, so fixed it.","This is not a problem in practice, but the code was wrong, so fixed it.",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2479,2019-08-16T10:31:10Z,2019-08-19T17:36:00Z,2019-08-19T17:36:03Z,MERGED,True,8,10,1,https://github.com/edenhill,Tidy up rd_kafka_new() conf freeing to make code clearer (#2478),1,[],https://github.com/edenhill/librdkafka/pull/2479,https://github.com/mhowlett,2,https://github.com/edenhill/librdkafka/pull/2479#issuecomment-522638576,"This is not a problem in practice, but the code was wrong, so fixed it.",for reference: #2478,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2482,2019-08-19T08:50:09Z,2019-08-19T15:47:03Z,2019-08-19T15:47:06Z,MERGED,True,50,19,7,https://github.com/edenhill,Fix memory leak in aborted transactions,5,[],https://github.com/edenhill/librdkafka/pull/2482,https://github.com/edenhill,1,https://github.com/edenhill/librdkafka/pull/2482,,,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2483,2019-08-19T09:15:42Z,2019-08-19T20:43:38Z,2019-08-19T20:43:41Z,MERGED,True,110,52,6,https://github.com/edenhill,Producer and consumer optimizations,2,['enhancement'],https://github.com/edenhill/librdkafka/pull/2483,https://github.com/edenhill,1,https://github.com/edenhill/librdkafka/pull/2483,,,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2484,2019-08-20T17:38:23Z,2019-09-09T23:36:18Z,2019-09-09T23:36:19Z,CLOSED,False,9,13,3,https://github.com/mhowlett,Cleanup rd_kafka_toppar_leader_update,2,[],https://github.com/edenhill/librdkafka/pull/2484,https://github.com/mhowlett,1,https://github.com/edenhill/librdkafka/pull/2484,"additionally, the reason for the call to rd_kafka_toppar_broker_delegate in the case had_leader is false is not obvious to me (something to do with reference counting?). I assume it's necessary since the logic goes out of the way to make sure it happens, but i feel a comment is needed (or probably better, something refactored).","additionally, the reason for the call to rd_kafka_toppar_broker_delegate in the case had_leader is false is not obvious to me (something to do with reference counting?). I assume it's necessary since the logic goes out of the way to make sure it happens, but i feel a comment is needed (or probably better, something refactored).",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2484,2019-08-20T17:38:23Z,2019-09-09T23:36:18Z,2019-09-09T23:36:19Z,CLOSED,False,9,13,3,https://github.com/mhowlett,Cleanup rd_kafka_toppar_leader_update,2,[],https://github.com/edenhill/librdkafka/pull/2484,https://github.com/mhowlett,2,https://github.com/edenhill/librdkafka/pull/2484#issuecomment-523647066,"additionally, the reason for the call to rd_kafka_toppar_broker_delegate in the case had_leader is false is not obvious to me (something to do with reference counting?). I assume it's necessary since the logic goes out of the way to make sure it happens, but i feel a comment is needed (or probably better, something refactored).","if these won't make it into 1.2, suggest closing this and i'll incorporate into the KIP-392 PR since i'm making additional overlapping changes.",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2484,2019-08-20T17:38:23Z,2019-09-09T23:36:18Z,2019-09-09T23:36:19Z,CLOSED,False,9,13,3,https://github.com/mhowlett,Cleanup rd_kafka_toppar_leader_update,2,[],https://github.com/edenhill/librdkafka/pull/2484,https://github.com/edenhill,3,https://github.com/edenhill/librdkafka/pull/2484#issuecomment-523768451,"additionally, the reason for the call to rd_kafka_toppar_broker_delegate in the case had_leader is false is not obvious to me (something to do with reference counting?). I assume it's necessary since the logic goes out of the way to make sure it happens, but i feel a comment is needed (or probably better, something refactored).","1.2 is in code freeze, so let's punt this for fff",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2484,2019-08-20T17:38:23Z,2019-09-09T23:36:18Z,2019-09-09T23:36:19Z,CLOSED,False,9,13,3,https://github.com/mhowlett,Cleanup rd_kafka_toppar_leader_update,2,[],https://github.com/edenhill/librdkafka/pull/2484,https://github.com/mhowlett,4,https://github.com/edenhill/librdkafka/pull/2484#issuecomment-529707187,"additionally, the reason for the call to rd_kafka_toppar_broker_delegate in the case had_leader is false is not obvious to me (something to do with reference counting?). I assume it's necessary since the logic goes out of the way to make sure it happens, but i feel a comment is needed (or probably better, something refactored).",superseded by #2518,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2485,2019-08-21T10:56:59Z,2019-10-16T09:18:13Z,2019-10-16T09:18:16Z,MERGED,True,161,99,26,https://github.com/edenhill,Fixes for issues found by Coverity static code analysis,13,[],https://github.com/edenhill/librdkafka/pull/2485,https://github.com/edenhill,1,https://github.com/edenhill/librdkafka/pull/2485,"https://scan.coverity.com/projects/edenhill-librdkafka?tab=overview
Let's get this merged after 1.2.0","https://scan.coverity.com/projects/edenhill-librdkafka?tab=overview
Let's get this merged after 1.2.0",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2494,2019-08-26T08:43:04Z,2019-09-23T07:43:30Z,2019-09-23T07:43:32Z,MERGED,True,7,5,1,https://github.com/edenhill,Add more autoconf compatibility options to ignore,1,[],https://github.com/edenhill/librdkafka/pull/2494,https://github.com/edenhill,1,https://github.com/edenhill/librdkafka/pull/2494,for confluentinc/confluent-kafka-go#339,for confluentinc/confluent-kafka-go#339,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2495,2019-08-27T21:27:42Z,2019-08-28T06:26:38Z,2019-08-28T06:26:38Z,MERGED,True,2,2,2,https://github.com/mhowlett,Changed isolation.level default to read_committed,1,[],https://github.com/edenhill/librdkafka/pull/2495,https://github.com/mhowlett,1,https://github.com/edenhill/librdkafka/pull/2495,,,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2498,2019-08-29T17:52:58Z,2019-08-30T06:57:39Z,2019-08-30T06:57:39Z,MERGED,True,2,2,1,https://github.com/mhowlett,Fixed off-by-one error in aborted txn check,1,[],https://github.com/edenhill/librdkafka/pull/2498,https://github.com/mhowlett,1,https://github.com/edenhill/librdkafka/pull/2498,"It is valid for a transaction to both start and end at an abort transaction marker, the check did not previously allow for that case.","It is valid for a transaction to both start and end at an abort transaction marker, the check did not previously allow for that case.",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2503,2019-08-30T16:11:52Z,2019-08-31T08:15:29Z,2019-08-31T08:15:36Z,MERGED,True,28,17,1,https://github.com/edenhill,Fix consumer protocol parse error when using aborted transactions,1,[],https://github.com/edenhill/librdkafka/pull/2503,https://github.com/edenhill,1,https://github.com/edenhill/librdkafka/pull/2503,"When control messages were skipped the remaining buffer was not consumed
which lead to the messageset parser reading the remaining bytes as
messages, which in best case resulted in parse errors.
There's also some tiny cosmetic changes to keep under 80 cols.","When control messages were skipped the remaining buffer was not consumed
which lead to the messageset parser reading the remaining bytes as
messages, which in best case resulted in parse errors.
There's also some tiny cosmetic changes to keep under 80 cols.",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2504,2019-08-31T10:37:18Z,2019-09-04T07:40:03Z,2019-09-04T07:40:05Z,MERGED,True,162,60,5,https://github.com/edenhill,Fix Doxygen warnings,2,[],https://github.com/edenhill/librdkafka/pull/2504,https://github.com/edenhill,1,https://github.com/edenhill/librdkafka/pull/2504,Required for cp doc warning->error transition,Required for cp doc warning->error transition,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2511,2019-09-05T18:24:31Z,2019-09-06T07:54:49Z,2019-09-06T07:54:54Z,MERGED,True,2,2,1,https://github.com/nicklauslittle,Fix VS 2017 build,1,[],https://github.com/edenhill/librdkafka/pull/2511,https://github.com/nicklauslittle,1,https://github.com/edenhill/librdkafka/pull/2511,"This replaces a couple c boolean values (true, false) with the built-in rdkafka boolean values (rd_true, rd_false).","This replaces a couple c boolean values (true, false) with the built-in rdkafka boolean values (rd_true, rd_false).",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2511,2019-09-05T18:24:31Z,2019-09-06T07:54:49Z,2019-09-06T07:54:54Z,MERGED,True,2,2,1,https://github.com/nicklauslittle,Fix VS 2017 build,1,[],https://github.com/edenhill/librdkafka/pull/2511,https://github.com/edenhill,2,https://github.com/edenhill/librdkafka/pull/2511#issuecomment-528752800,"This replaces a couple c boolean values (true, false) with the built-in rdkafka boolean values (rd_true, rd_false).",Thank you!,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2513,2019-09-07T09:01:28Z,2019-10-16T15:11:14Z,2019-10-16T15:11:17Z,MERGED,True,95,49,8,https://github.com/edenhill,Less strict message.max.bytes check for individual messages (#993),1,[],https://github.com/edenhill/librdkafka/pull/2513,https://github.com/edenhill,1,https://github.com/edenhill/librdkafka/pull/2513,"Since the final request size can't be known at produce() time
we allow ProduceRequests larger than message.max.bytes (overshot by at
most one message) and instead rely on the broker enforcing the
MessageSet size.","Since the final request size can't be known at produce() time
we allow ProduceRequests larger than message.max.bytes (overshot by at
most one message) and instead rely on the broker enforcing the
MessageSet size.",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2513,2019-09-07T09:01:28Z,2019-10-16T15:11:14Z,2019-10-16T15:11:17Z,MERGED,True,95,49,8,https://github.com/edenhill,Less strict message.max.bytes check for individual messages (#993),1,[],https://github.com/edenhill/librdkafka/pull/2513,https://github.com/edenhill,2,https://github.com/edenhill/librdkafka/pull/2513#issuecomment-541539023,"Since the final request size can't be known at produce() time
we allow ProduceRequests larger than message.max.bytes (overshot by at
most one message) and instead rely on the broker enforcing the
MessageSet size.","Fixed comments, added test. Please re-review",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2518,2019-09-09T23:31:43Z,2019-11-15T12:30:13Z,2019-11-25T22:21:19Z,MERGED,True,5832,455,53,https://github.com/mhowlett,Fetch from follower,19,[],https://github.com/edenhill/librdkafka/pull/2518,https://github.com/mhowlett,1,https://github.com/edenhill/librdkafka/pull/2518,,,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2518,2019-09-09T23:31:43Z,2019-11-15T12:30:13Z,2019-11-25T22:21:19Z,MERGED,True,5832,455,53,https://github.com/mhowlett,Fetch from follower,19,[],https://github.com/edenhill/librdkafka/pull/2518,https://github.com/mhowlett,2,https://github.com/edenhill/librdkafka/pull/2518#issuecomment-529735893,,"it looks like there's a problem with rapidjson & I've also made a minor change locally to trivup, but these are small things and this is otherwise ready to review.",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2520,2019-09-10T20:02:40Z,2019-09-12T19:12:53Z,2019-09-12T19:12:56Z,MERGED,True,24,9,3,https://github.com/edenhill,Rate limit IO-based queue wakeups to linger.ms (#2509),1,[],https://github.com/edenhill/librdkafka/pull/2520,https://github.com/edenhill,1,https://github.com/edenhill/librdkafka/pull/2520,,,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2520,2019-09-10T20:02:40Z,2019-09-12T19:12:53Z,2019-09-12T19:12:56Z,MERGED,True,24,9,3,https://github.com/edenhill,Rate limit IO-based queue wakeups to linger.ms (#2509),1,[],https://github.com/edenhill/librdkafka/pull/2520,https://github.com/shanson7,2,https://github.com/edenhill/librdkafka/pull/2520#issuecomment-530408893,,Is the 0086_purge_remote test failure related to this change? Should I hold off on applying this patch for further testing on my side?,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2520,2019-09-10T20:02:40Z,2019-09-12T19:12:53Z,2019-09-12T19:12:56Z,MERGED,True,24,9,3,https://github.com/edenhill,Rate limit IO-based queue wakeups to linger.ms (#2509),1,[],https://github.com/edenhill/librdkafka/pull/2520,https://github.com/edenhill,3,https://github.com/edenhill/librdkafka/pull/2520#issuecomment-530426925,,"@shanson7 It is a real problem. The patch will need to be modified, in its current form it rate limits all queue wakeups, not just due to msg enqueuing.",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2520,2019-09-10T20:02:40Z,2019-09-12T19:12:53Z,2019-09-12T19:12:56Z,MERGED,True,24,9,3,https://github.com/edenhill,Rate limit IO-based queue wakeups to linger.ms (#2509),1,[],https://github.com/edenhill/librdkafka/pull/2520,https://github.com/shanson7,4,https://github.com/edenhill/librdkafka/pull/2520#issuecomment-530476957,,"If it's not too problematic to get the config element there, I think it should probably happen in rd_kafka_toppar_enq_msg since that's where the decision is made for queue_len == 1 as well.",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2520,2019-09-10T20:02:40Z,2019-09-12T19:12:53Z,2019-09-12T19:12:56Z,MERGED,True,24,9,3,https://github.com/edenhill,Rate limit IO-based queue wakeups to linger.ms (#2509),1,[],https://github.com/edenhill/librdkafka/pull/2520,https://github.com/edenhill,5,https://github.com/edenhill/librdkafka/pull/2520#issuecomment-530740050,,"@shanson7 Yep , the rate-limiting is now limited to toppar_enq_msg only.",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2520,2019-09-10T20:02:40Z,2019-09-12T19:12:53Z,2019-09-12T19:12:56Z,MERGED,True,24,9,3,https://github.com/edenhill,Rate limit IO-based queue wakeups to linger.ms (#2509),1,[],https://github.com/edenhill/librdkafka/pull/2520,https://github.com/edenhill,6,https://github.com/edenhill/librdkafka/pull/2520#issuecomment-530785806,,ping @mhowlett @rnpridgeon : re-review please,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2520,2019-09-10T20:02:40Z,2019-09-12T19:12:53Z,2019-09-12T19:12:56Z,MERGED,True,24,9,3,https://github.com/edenhill,Rate limit IO-based queue wakeups to linger.ms (#2509),1,[],https://github.com/edenhill/librdkafka/pull/2520,https://github.com/mhowlett,7,https://github.com/edenhill/librdkafka/pull/2520#issuecomment-530891470,,"yes, lgtm",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2525,2019-09-13T14:42:40Z,2019-11-12T11:43:52Z,2019-11-12T11:43:58Z,MERGED,True,582,25,18,https://github.com/rnpridgeon,Kip 345: Static group membership ,7,[],https://github.com/edenhill/librdkafka/pull/2525,https://github.com/rnpridgeon,1,https://github.com/edenhill/librdkafka/pull/2525,,,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2525,2019-09-13T14:42:40Z,2019-11-12T11:43:52Z,2019-11-12T11:43:58Z,MERGED,True,582,25,18,https://github.com/rnpridgeon,Kip 345: Static group membership ,7,[],https://github.com/edenhill/librdkafka/pull/2525,https://github.com/rnpridgeon,2,https://github.com/edenhill/librdkafka/pull/2525#issuecomment-531317919,,"oddy enough I'm unable to reproduce the 0050_subscribe_adds
[0050_subscribe_adds         / 20.764s] Closing consumer
[0050_subscribe_adds         / 20.769s] CONSUMER.CLOSE: duration 5.200ms
[0050_subscribe_adds         / 20.769s] 0050_subscribe_adds: duration 20769.172ms
[0050_subscribe_adds         / 20.769s] ================= Test 0050_subscribe_adds PASSED =================
[                      / 21.044s] ALL-TESTS: duration 21044.305ms
TEST 20190913130820 (bare) SUMMARY
#==================================================================#
|                                    |     PASSED |  21.044s |
| 0050_subscribe_adds                      |     PASSED |  20.769s |
#==================================================================#
[                      / 21.045s] # Test report written to test_report_20190913130820.json
[                      / 21.045s] 0 thread(s) in use by librdkafka
[                      / 21.045s]
============== ALL TESTS PASSED ==============

./merged in bare mode PASSED!",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2525,2019-09-13T14:42:40Z,2019-11-12T11:43:52Z,2019-11-12T11:43:58Z,MERGED,True,582,25,18,https://github.com/rnpridgeon,Kip 345: Static group membership ,7,[],https://github.com/edenhill/librdkafka/pull/2525,https://github.com/edenhill,3,https://github.com/edenhill/librdkafka/pull/2525#issuecomment-531766060,,re 0050_subscribe_adds: if you find an intermittent test failure you can use TESTS=0050 ./until-fail.sh bare to try to reproduce it.,True,{'THUMBS_UP': ['https://github.com/rnpridgeon']}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2525,2019-09-13T14:42:40Z,2019-11-12T11:43:52Z,2019-11-12T11:43:58Z,MERGED,True,582,25,18,https://github.com/rnpridgeon,Kip 345: Static group membership ,7,[],https://github.com/edenhill/librdkafka/pull/2525,https://github.com/edenhill,4,https://github.com/edenhill/librdkafka/pull/2525#issuecomment-542556916,,Also update the protocol versions and supported KIPs in INTRODUCTION.md,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2525,2019-09-13T14:42:40Z,2019-11-12T11:43:52Z,2019-11-12T11:43:58Z,MERGED,True,582,25,18,https://github.com/rnpridgeon,Kip 345: Static group membership ,7,[],https://github.com/edenhill/librdkafka/pull/2525,https://github.com/edenhill,5,https://github.com/edenhill/librdkafka/pull/2525#issuecomment-542601992,,Bump to latest Kafka version in .travis.yml since your test depends on >=2.3.0,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2528,2019-09-16T11:10:31Z,2019-09-23T07:40:44Z,2019-09-23T07:40:47Z,MERGED,True,470,121,6,https://github.com/edenhill,Fix msgq (re)insertion code to avoid O(N^2) insert sort operations on retry (#2508),1,[],https://github.com/edenhill/librdkafka/pull/2528,https://github.com/edenhill,1,https://github.com/edenhill/librdkafka/pull/2528,"The msgq insert code now properly handles interleaved and overlapping
message range inserts, which may occur during Producer retries for
high-throughput applications.","The msgq insert code now properly handles interleaved and overlapping
message range inserts, which may occur during Producer retries for
high-throughput applications.",True,{'THUMBS_UP': ['https://github.com/kenneth-jia']}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2528,2019-09-16T11:10:31Z,2019-09-23T07:40:44Z,2019-09-23T07:40:47Z,MERGED,True,470,121,6,https://github.com/edenhill,Fix msgq (re)insertion code to avoid O(N^2) insert sort operations on retry (#2508),1,[],https://github.com/edenhill/librdkafka/pull/2528,https://github.com/edenhill,2,https://github.com/edenhill/librdkafka/pull/2528#issuecomment-532139860,"The msgq insert code now properly handles interleaved and overlapping
message range inserts, which may occur during Producer retries for
high-throughput applications.",This will not make v1.2.0.,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2528,2019-09-16T11:10:31Z,2019-09-23T07:40:44Z,2019-09-23T07:40:47Z,MERGED,True,470,121,6,https://github.com/edenhill,Fix msgq (re)insertion code to avoid O(N^2) insert sort operations on retry (#2508),1,[],https://github.com/edenhill/librdkafka/pull/2528,https://github.com/sarkanyi,3,https://github.com/edenhill/librdkafka/pull/2528#issuecomment-532525354,"The msgq insert code now properly handles interleaved and overlapping
message range inserts, which may occur during Producer retries for
high-throughput applications.","Looks great, no issues with it so far!",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2528,2019-09-16T11:10:31Z,2019-09-23T07:40:44Z,2019-09-23T07:40:47Z,MERGED,True,470,121,6,https://github.com/edenhill,Fix msgq (re)insertion code to avoid O(N^2) insert sort operations on retry (#2508),1,[],https://github.com/edenhill/librdkafka/pull/2528,https://github.com/kenneth-jia,4,https://github.com/edenhill/librdkafka/pull/2528#issuecomment-532546703,"The msgq insert code now properly handles interleaved and overlapping
message range inserts, which may occur during Producer retries for
high-throughput applications.","This will not make v1.2.0.

The on-coming release v1.2.0 would not include the fix? It's such a pity.
During our throughput test (with EOS enabled), we found that once the kafka cluster stuck  for a few seconds, the cpu rate for some internal broker thread would reach nearly 100%!",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2528,2019-09-16T11:10:31Z,2019-09-23T07:40:44Z,2019-09-23T07:40:47Z,MERGED,True,470,121,6,https://github.com/edenhill,Fix msgq (re)insertion code to avoid O(N^2) insert sort operations on retry (#2508),1,[],https://github.com/edenhill/librdkafka/pull/2528,https://github.com/sarkanyi,5,https://github.com/edenhill/librdkafka/pull/2528#issuecomment-532558809,"The msgq insert code now properly handles interleaved and overlapping
message range inserts, which may occur during Producer retries for
high-throughput applications.",@kenneth-jia For our use I've applied this on top of 1.1.0 in my fork and will use that until we have a stable release. Maybe you can do the same or something similar. Also @edenhill said that there is a chance of an 1.2.1 in a short while after 1.2.0 with this fix in it.,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2528,2019-09-16T11:10:31Z,2019-09-23T07:40:44Z,2019-09-23T07:40:47Z,MERGED,True,470,121,6,https://github.com/edenhill,Fix msgq (re)insertion code to avoid O(N^2) insert sort operations on retry (#2508),1,[],https://github.com/edenhill/librdkafka/pull/2528,https://github.com/kenneth-jia,6,https://github.com/edenhill/librdkafka/pull/2528#issuecomment-532565349,"The msgq insert code now properly handles interleaved and overlapping
message range inserts, which may occur during Producer retries for
high-throughput applications.","@kenneth-jia For our use I've applied this on top of 1.1.0 in my fork and will use that until we have a stable release. Maybe you can do the same or something similar. Also @edenhill said that there is a chance of an 1.2.1 in a short while after 1.2.0 with this fix in it.

Sounds good. Thanks :-)",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2531,2019-09-19T22:27:21Z,2019-09-25T05:04:26Z,2019-11-26T18:41:40Z,CLOSED,False,9,3,4,https://github.com/mezzi,Include rack field in broker metadata,7,[],https://github.com/edenhill/librdkafka/pull/2531,https://github.com/mezzi,1,https://github.com/edenhill/librdkafka/pull/2531,"Added the ""rack"" field to broker metadata struct.","Added the ""rack"" field to broker metadata struct.",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2535,2019-09-24T15:31:16Z,,2020-07-15T20:29:01Z,OPEN,False,6,5,1,https://github.com/zilder,Prevent infinite loop in rd_kafka_query_watermark_offsets(),1,[],https://github.com/edenhill/librdkafka/pull/2535,https://github.com/zilder,1,https://github.com/edenhill/librdkafka/pull/2535,"Hi,
we've discovered an strange issue recently in one of our programs that use librdkafka. We found that program stuck in rd_kafka_query_watermark_offsets(). Here is a truncated stacktrace:
#0  0x00007f018c4b196a in pthread_cond_timedwait@@GLIBC_2.3.2 () from /lib64/libpthread.so.0
#1  0x00007f018b10ee59 in cnd_timedwait (cond=cond@entry=0x55d599512918, 
    mtx=mtx@entry=0x55d5995128f0, ts=ts@entry=0x7fffd3c3ea50) at tinycthread.c:462
#2  0x00007f018b10f273 in cnd_timedwait_abs (cnd=cnd@entry=0x55d599512918, 
    mtx=mtx@entry=0x55d5995128f0, tspec=tspec@entry=0x7fffd3c3ea50)
    at tinycthread_extra.c:100
#3  0x00007f018b0d8dff in rd_kafka_q_serve (rkq=rkq@entry=0x55d5995128f0, 
    timeout_ms=timeout_ms@entry=100, max_cnt=max_cnt@entry=0, 
    cb_type=cb_type@entry=RD_KAFKA_Q_CB_CALLBACK, 
    callback=callback@entry=0x7f018b0a7090 <rd_kafka_poll_cb>, opaque=opaque@entry=0x0)
    at rdkafka_queue.h:475
#4  0x00007f018b0a637e in rd_kafka_query_watermark_offsets (rk=<optimized out>, 
    topic=<optimized out>, partition=partition@entry=0, low=low@entry=0x7fffd3c3ecc0, 
    high=high@entry=0x7fffd3c3ecc8, timeout_ms=timeout_ms@entry=1000) at rdkafka.c:3102
...

According to our sysops it might be because kafka's advertised.listeners parameter was pointing to the host that at the moment was unreachable. Anyway I attached with gdb and saw that execution was trapped in an infinite loop in rd_kafka_query_watermark_offsets():
        /* Wait for reply (or timeout) */
        while (state.err == RD_KAFKA_RESP_ERR__IN_PROGRESS &&
               rd_kafka_q_serve(rkq, 100, 0, RD_KAFKA_Q_CB_CALLBACK,
                                rd_kafka_poll_cb, NULL) !=
               RD_KAFKA_OP_RES_YIELD)
                ;
I don't understand much of what's going on under the hood, but from looking at the rd_kafka_q_serve() function's code I assume it should return an integer value signifying the number of processed messages and zero in case of timeout. And therefore it's unclear why return value is compared to RD_KAFKA_OP_RES_YIELD which does not make sense to me. In our case this comparison failed all the time as rd_kafka_q_serve() returned zero and execution proceeded to the next iteration and so on. To me it looks like we should actually check for zero and set an error code if this condition satisfied. In this PR I tried to implement this idea and it fixed the initial issue in our case. I would be glad to hear your thoughts on this. Thanks!
ps: here is a sample code I used for tests","Hi,
we've discovered an strange issue recently in one of our programs that use librdkafka. We found that program stuck in rd_kafka_query_watermark_offsets(). Here is a truncated stacktrace:
#0  0x00007f018c4b196a in pthread_cond_timedwait@@GLIBC_2.3.2 () from /lib64/libpthread.so.0
#1  0x00007f018b10ee59 in cnd_timedwait (cond=cond@entry=0x55d599512918, 
    mtx=mtx@entry=0x55d5995128f0, ts=ts@entry=0x7fffd3c3ea50) at tinycthread.c:462
#2  0x00007f018b10f273 in cnd_timedwait_abs (cnd=cnd@entry=0x55d599512918, 
    mtx=mtx@entry=0x55d5995128f0, tspec=tspec@entry=0x7fffd3c3ea50)
    at tinycthread_extra.c:100
#3  0x00007f018b0d8dff in rd_kafka_q_serve (rkq=rkq@entry=0x55d5995128f0, 
    timeout_ms=timeout_ms@entry=100, max_cnt=max_cnt@entry=0, 
    cb_type=cb_type@entry=RD_KAFKA_Q_CB_CALLBACK, 
    callback=callback@entry=0x7f018b0a7090 <rd_kafka_poll_cb>, opaque=opaque@entry=0x0)
    at rdkafka_queue.h:475
#4  0x00007f018b0a637e in rd_kafka_query_watermark_offsets (rk=<optimized out>, 
    topic=<optimized out>, partition=partition@entry=0, low=low@entry=0x7fffd3c3ecc0, 
    high=high@entry=0x7fffd3c3ecc8, timeout_ms=timeout_ms@entry=1000) at rdkafka.c:3102
...

According to our sysops it might be because kafka's advertised.listeners parameter was pointing to the host that at the moment was unreachable. Anyway I attached with gdb and saw that execution was trapped in an infinite loop in rd_kafka_query_watermark_offsets():
        /* Wait for reply (or timeout) */
        while (state.err == RD_KAFKA_RESP_ERR__IN_PROGRESS &&
               rd_kafka_q_serve(rkq, 100, 0, RD_KAFKA_Q_CB_CALLBACK,
                                rd_kafka_poll_cb, NULL) !=
               RD_KAFKA_OP_RES_YIELD)
                ;
I don't understand much of what's going on under the hood, but from looking at the rd_kafka_q_serve() function's code I assume it should return an integer value signifying the number of processed messages and zero in case of timeout. And therefore it's unclear why return value is compared to RD_KAFKA_OP_RES_YIELD which does not make sense to me. In our case this comparison failed all the time as rd_kafka_q_serve() returned zero and execution proceeded to the next iteration and so on. To me it looks like we should actually check for zero and set an error code if this condition satisfied. In this PR I tried to implement this idea and it fixed the initial issue in our case. I would be glad to hear your thoughts on this. Thanks!
ps: here is a sample code I used for tests",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2535,2019-09-24T15:31:16Z,,2020-07-15T20:29:01Z,OPEN,False,6,5,1,https://github.com/zilder,Prevent infinite loop in rd_kafka_query_watermark_offsets(),1,[],https://github.com/edenhill/librdkafka/pull/2535,https://github.com/edenhill,2,https://github.com/edenhill/librdkafka/pull/2535#issuecomment-582332487,"Hi,
we've discovered an strange issue recently in one of our programs that use librdkafka. We found that program stuck in rd_kafka_query_watermark_offsets(). Here is a truncated stacktrace:
#0  0x00007f018c4b196a in pthread_cond_timedwait@@GLIBC_2.3.2 () from /lib64/libpthread.so.0
#1  0x00007f018b10ee59 in cnd_timedwait (cond=cond@entry=0x55d599512918, 
    mtx=mtx@entry=0x55d5995128f0, ts=ts@entry=0x7fffd3c3ea50) at tinycthread.c:462
#2  0x00007f018b10f273 in cnd_timedwait_abs (cnd=cnd@entry=0x55d599512918, 
    mtx=mtx@entry=0x55d5995128f0, tspec=tspec@entry=0x7fffd3c3ea50)
    at tinycthread_extra.c:100
#3  0x00007f018b0d8dff in rd_kafka_q_serve (rkq=rkq@entry=0x55d5995128f0, 
    timeout_ms=timeout_ms@entry=100, max_cnt=max_cnt@entry=0, 
    cb_type=cb_type@entry=RD_KAFKA_Q_CB_CALLBACK, 
    callback=callback@entry=0x7f018b0a7090 <rd_kafka_poll_cb>, opaque=opaque@entry=0x0)
    at rdkafka_queue.h:475
#4  0x00007f018b0a637e in rd_kafka_query_watermark_offsets (rk=<optimized out>, 
    topic=<optimized out>, partition=partition@entry=0, low=low@entry=0x7fffd3c3ecc0, 
    high=high@entry=0x7fffd3c3ecc8, timeout_ms=timeout_ms@entry=1000) at rdkafka.c:3102
...

According to our sysops it might be because kafka's advertised.listeners parameter was pointing to the host that at the moment was unreachable. Anyway I attached with gdb and saw that execution was trapped in an infinite loop in rd_kafka_query_watermark_offsets():
        /* Wait for reply (or timeout) */
        while (state.err == RD_KAFKA_RESP_ERR__IN_PROGRESS &&
               rd_kafka_q_serve(rkq, 100, 0, RD_KAFKA_Q_CB_CALLBACK,
                                rd_kafka_poll_cb, NULL) !=
               RD_KAFKA_OP_RES_YIELD)
                ;
I don't understand much of what's going on under the hood, but from looking at the rd_kafka_q_serve() function's code I assume it should return an integer value signifying the number of processed messages and zero in case of timeout. And therefore it's unclear why return value is compared to RD_KAFKA_OP_RES_YIELD which does not make sense to me. In our case this comparison failed all the time as rd_kafka_q_serve() returned zero and execution proceeded to the next iteration and so on. To me it looks like we should actually check for zero and set an error code if this condition satisfied. In this PR I tried to implement this idea and it fixed the initial issue in our case. I would be glad to hear your thoughts on this. Thanks!
ps: here is a sample code I used for tests",There should be an added testcase to test 0031 to verify this fix.,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2535,2019-09-24T15:31:16Z,,2020-07-15T20:29:01Z,OPEN,False,6,5,1,https://github.com/zilder,Prevent infinite loop in rd_kafka_query_watermark_offsets(),1,[],https://github.com/edenhill/librdkafka/pull/2535,https://github.com/kondetibharat,3,https://github.com/edenhill/librdkafka/pull/2535#issuecomment-658987204,"Hi,
we've discovered an strange issue recently in one of our programs that use librdkafka. We found that program stuck in rd_kafka_query_watermark_offsets(). Here is a truncated stacktrace:
#0  0x00007f018c4b196a in pthread_cond_timedwait@@GLIBC_2.3.2 () from /lib64/libpthread.so.0
#1  0x00007f018b10ee59 in cnd_timedwait (cond=cond@entry=0x55d599512918, 
    mtx=mtx@entry=0x55d5995128f0, ts=ts@entry=0x7fffd3c3ea50) at tinycthread.c:462
#2  0x00007f018b10f273 in cnd_timedwait_abs (cnd=cnd@entry=0x55d599512918, 
    mtx=mtx@entry=0x55d5995128f0, tspec=tspec@entry=0x7fffd3c3ea50)
    at tinycthread_extra.c:100
#3  0x00007f018b0d8dff in rd_kafka_q_serve (rkq=rkq@entry=0x55d5995128f0, 
    timeout_ms=timeout_ms@entry=100, max_cnt=max_cnt@entry=0, 
    cb_type=cb_type@entry=RD_KAFKA_Q_CB_CALLBACK, 
    callback=callback@entry=0x7f018b0a7090 <rd_kafka_poll_cb>, opaque=opaque@entry=0x0)
    at rdkafka_queue.h:475
#4  0x00007f018b0a637e in rd_kafka_query_watermark_offsets (rk=<optimized out>, 
    topic=<optimized out>, partition=partition@entry=0, low=low@entry=0x7fffd3c3ecc0, 
    high=high@entry=0x7fffd3c3ecc8, timeout_ms=timeout_ms@entry=1000) at rdkafka.c:3102
...

According to our sysops it might be because kafka's advertised.listeners parameter was pointing to the host that at the moment was unreachable. Anyway I attached with gdb and saw that execution was trapped in an infinite loop in rd_kafka_query_watermark_offsets():
        /* Wait for reply (or timeout) */
        while (state.err == RD_KAFKA_RESP_ERR__IN_PROGRESS &&
               rd_kafka_q_serve(rkq, 100, 0, RD_KAFKA_Q_CB_CALLBACK,
                                rd_kafka_poll_cb, NULL) !=
               RD_KAFKA_OP_RES_YIELD)
                ;
I don't understand much of what's going on under the hood, but from looking at the rd_kafka_q_serve() function's code I assume it should return an integer value signifying the number of processed messages and zero in case of timeout. And therefore it's unclear why return value is compared to RD_KAFKA_OP_RES_YIELD which does not make sense to me. In our case this comparison failed all the time as rd_kafka_q_serve() returned zero and execution proceeded to the next iteration and so on. To me it looks like we should actually check for zero and set an error code if this condition satisfied. In this PR I tried to implement this idea and it fixed the initial issue in our case. I would be glad to hear your thoughts on this. Thanks!
ps: here is a sample code I used for tests","@edenhill, checking to get a traction on this issue. We are having same issue where the call to 'rd_kafka_query_watermark_offsets' get stuck, because the topic / partition leader moved to a new broker, and the old broker is down for maintenance.
As you can see from the logs, broker 22 went down for maintenance during the 'query_watermark_offsets"" call. Partitions switched to broker 23, but still method is trying to query using the down broker instead of switching to new broker. Also, method takes a timeout.. so should have exited.
Wondering why the above proposed change not adopted?
PreciseTimeStamp	Pid	Tid		Message
2020-07-12 13:10:16.9671238	57212	68108	Message: [thrd:main]: Topic TMNode [2]: leader is down: re-query
2020-07-12 13:10:16.9712443	57212	68108	Message: [thrd:main]: TMContainer [8]: broker 111.11.11.11:9092/22 no longer leader
2020-07-12 13:10:16.9712644	57212	68108	Message: [thrd:main]: Migrating topic TMContainer [8] 00000214CA31D4F0 from 111.11.11.11:9092/22 to 100.89.21.29:9092/23 (sending PARTITION_LEAVE to 111.11.11.11:9092/22)
2020-07-12 13:10:16.9713601	57212	17932	Message: [thrd:111.11.11.11:9092/22]: 111.11.11.11:9092/22: Topic TMContainer [8] in state active at offset 172086 (0/100000 msgs, 0/1048576 kb queued, opv 2) is not fetchable: forced removal
2020-07-12 13:10:16.9713733	57212	17932	Message: [thrd:111.11.11.11:9092/22]: 111.11.11.11:9092/22: Removed TMContainer [8] from fetch list (1 entries, opv 2)
2020-07-12 13:10:16.9713853	57212	17932	Message: [thrd:111.11.11.11:9092/22]: 111.11.11.11:9092/22: Topic TMContainer [8]: leaving broker (0 messages in xmitq, next leader 100.89.21.29:9092/23, rktp 00000214CA31D4F0)
2020-07-12 13:10:16.9714301	57212	68108	Message: [thrd:main]: Topic TMNode [2] migrated from broker 22 to 23
2020-07-12 13:10:16.9714815	57212	68108	Message: [thrd:main]: TMNode [2]: delegate to broker 100.89.21.29:9092/23 (rktp 00000214C8788D30, term 0, ref 5, remove 0)
2020-07-12 13:10:16.9714980	57212	68108	Message: [thrd:main]: TMNode [2]: broker 111.11.11.11:9092/22 no longer leader
2020-07-12 13:10:16.9715092	57212	68108	Message: [thrd:main]: TMNode [2]: broker 100.89.21.29:9092/23 is now leader for partition with 0 messages (0 bytes) queued
2020-07-12 13:10:16.9715187	57212	68108	Message: [thrd:main]: Migrating topic TMNode [2] 00000214C8788D30 from 111.11.11.11:9092/22 to 100.89.21.29:9092/23 (sending PARTITION_LEAVE to 111.11.11.11:9092/22)
2020-07-12 13:10:16.9715580	57212	17932	Message: [thrd:111.11.11.11:9092/22]: 111.11.11.11:9092/22: Topic TMNode [2] in state active at offset 1047218 (0/100000 msgs, 0/1048576 kb queued, opv 2) is not fetchable: forced removal
2020-07-12 13:10:16.9715696	57212	17932	Message: [thrd:111.11.11.11:9092/22]: 111.11.11.11:9092/22: Removed TMNode [2] from fetch list (0 entries, opv 2)
2020-07-12 13:10:16.9715836	57212	17932	Message: [thrd:111.11.11.11:9092/22]: 111.11.11.11:9092/22: Topic TMNode [2]: leaving broker (0 messages in xmitq, next leader 100.89.21.29:9092/23, rktp 00000214C8788D30)
2020-07-12 13:10:16.9716146	57212	61928	Message: [thrd:100.89.21.29:9092/23]: 100.89.21.29:9092/23: Topic TMNode [2]: joining broker (rktp 00000214C8788D30, 0 message(s) queued)
2020-07-12 13:10:16.9716278	57212	61928	Message: [thrd:100.89.21.29:9092/23]: 100.89.21.29:9092/23: Topic TMNode [2] in state active at offset 1047218 (0/100000 msgs, 0/1048576 kb queued, opv 2) is fetchable:
2020-07-12 13:10:16.9716391	57212	61928	Message: [thrd:100.89.21.29:9092/23]: 100.89.21.29:9092/23: Added TMNode [2] to fetch list (3 entries, opv 2, 0 messages queued)
2020-07-12 13:10:30.9974966	57212	17932	Message: [thrd:111.11.11.11:9092/22]: 111.11.11.11:9092/22: failed: err: Local: Broker transport failure: (errno: Unknown error)
2020-07-12 13:10:30.9975363	57212	17932	Message: [thrd:111.11.11.11:9092/22]: 111.11.11.11:9092/22: Broker changed state CONNECT -> DOWN
2020-07-12 13:10:30.9975883	57212	65716	Message: [thrd:app]: 111.11.11.11:9092/22: OffsetRequest failed: Local: Broker transport failure: actions Retry
2020-07-12 13:10:30.9976362	57212	65716	Message: [thrd:app]: 111.11.11.11:9092/22: OffsetRequest failed: Local: Broker transport failure: actions Retry
2020-07-12 13:10:32.0130724	57212	17932	Message: [thrd:111.11.11.11:9092/22]: 111.11.11.11:9092/22: Broker changed state DOWN -> INIT
2020-07-12 13:10:32.0130905	57212	17932	Message: [thrd:111.11.11.11:9092/22]: 111.11.11.11:9092/22: Broker changed state INIT -> TRY_CONNECT
2020-07-12 13:10:32.0130985	57212	17932	Message: [thrd:111.11.11.11:9092/22]: 111.11.11.11:9092/22: Moved 2 retry buffer(s) to output queue
2020-07-12 13:10:33.0285730	57212	17932	Message: [thrd:111.11.11.11:9092/22]: 111.11.11.11:9092/22: broker in state TRY_CONNECT connecting
2020-07-12 13:10:33.0285836	57212	17932	Message: [thrd:111.11.11.11:9092/22]: 111.11.11.11:9092/22: Broker changed state TRY_CONNECT -> CONNECT
2020-07-12 13:10:33.0286753	57212	17932	Message: [thrd:111.11.11.11:9092/22]: 111.11.11.11:9092/22: Connecting to ipv4#111.11.11.11:9092 (plaintext) with socket 8800
2020-07-12 13:10:54.0897990	57212	17932	Message: [thrd:111.11.11.11:9092/22]: 111.11.11.11:9092/22: failed: err: Local: Broker transport failure: (errno: Unknown error)
2020-07-12 13:10:54.0898375	57212	17932	Message: [thrd:111.11.11.11:9092/22]: 111.11.11.11:9092/22: Connect to ipv4#111.11.11.11:9092 failed: Unknown error (after 21061ms in state CONNECT)
2020-07-12 13:10:54.0898461	57212	17932	Message: [thrd:111.11.11.11:9092/22]: 111.11.11.11:9092/22: Broker changed state CONNECT -> DOWN
2020-07-12 13:10:54.0898901	57212	65716	Message: [thrd:app]: 111.11.11.11:9092/22: OffsetRequest failed: Local: Broker transport failure: actions Retry
2020-07-12 13:10:54.0899362	57212	65716	Message: [thrd:app]: 111.11.11.11:9092/22: OffsetRequest failed: Local: Broker transport failure: actions Retry
2020-07-12 13:10:55.1053765	57212	17932	Message: [thrd:111.11.11.11:9092/22]: 111.11.11.11:9092/22: Broker changed state DOWN -> INIT
2020-07-12 13:10:55.1053872	57212	17932	Message: [thrd:111.11.11.11:9092/22]: 111.11.11.11:9092/22: Broker changed state INIT -> TRY_CONNECT
2020-07-12 13:10:55.1053954	57212	17932	Message: [thrd:111.11.11.11:9092/22]: 111.11.11.11:9092/22: Moved 2 retry buffer(s) to output queue
2020-07-12 13:10:56.1209043	57212	17932	Message: [thrd:111.11.11.11:9092/22]: 111.11.11.11:9092/22: broker in state TRY_CONNECT connecting
2020-07-12 13:10:56.1209212	57212	17932	Message: [thrd:111.11.11.11:9092/22]: 111.11.11.11:9092/22: Broker changed state TRY_CONNECT -> CONNECT
2020-07-12 13:10:56.1210161	57212	17932	Message: [thrd:111.11.11.11:9092/22]: 111.11.11.11:9092/22: Connecting to ipv4#111.11.11.11:9092 (plaintext) with socket 7220
2020-07-12 13:11:17.1821828	57212	17932	Message: [thrd:111.11.11.11:9092/22]: 111.11.11.11:9092/22: failed: err: Local: Broker transport failure: (errno: Unknown error)
2020-07-12 13:11:17.1822228	57212	17932	Message: [thrd:111.11.11.11:9092/22]: 111.11.11.11:9092/22: Connect to ipv4#111.11.11.11:9092 failed: Unknown error (after 21061ms in state CONNECT)
2020-07-12 13:11:17.1822313	57212	17932	Message: [thrd:111.11.11.11:9092/22]: 111.11.11.11:9092/22: Broker changed state CONNECT -> DOWN
2020-07-12 13:11:17.1822748	57212	65716	Message: [thrd:app]: 111.11.11.11:9092/22: OffsetRequest failed: Local: Broker transport failure: actions Retry
2020-07-12 13:11:17.1823217	57212	65716	Message: [thrd:app]: 111.11.11.11:9092/22: OffsetRequest failed: Local: Broker transport failure: actions Retry
2020-07-12 13:11:18.1976734	57212	17932	Message: [thrd:111.11.11.11:9092/22]: 111.11.11.11:9092/22: Broker changed state DOWN -> INIT
2020-07-12 13:11:18.1976837	57212	17932	Message: [thrd:111.11.11.11:9092/22]: 111.11.11.11:9092/22: Broker changed state INIT -> TRY_CONNECT
2020-07-12 13:11:18.1976914	57212	17932	Message: [thrd:111.11.11.11:9092/22]: 111.11.11.11:9092/22: Moved 2 retry buffer(s) to output queue
2020-07-12 13:11:19.2133371	57212	17932	Message: [thrd:111.11.11.11:9092/22]: 111.11.11.11:9092/22: broker in state TRY_CONNECT connecting
2020-07-12 13:11:19.2133472	57212	17932	Message: [thrd:111.11.11.11:9092/22]: 111.11.11.11:9092/22: Broker changed state TRY_CONNECT -> CONNECT
2020-07-12 13:11:19.2134261	57212	17932	Message: [thrd:111.11.11.11:9092/22]: 111.11.11.11:9092/22: Connecting to ipv4#111.11.11.11:9092 (plaintext) with socket 72288
2020-07-12 13:11:40.2588812	57212	17932	Message: [thrd:111.11.11.11:9092/22]: 111.11.11.11:9092/22: failed: err: Local: Broker transport failure: (errno: Unknown error)
2020-07-12 13:11:40.2589210	57212	17932	Message: [thrd:111.11.11.11:9092/22]: 111.11.11.11:9092/22: Connect to ipv4#111.11.11.11:9092 failed: Unknown error (after 21045ms in state CONNECT)
2020-07-12 13:11:40.2589291	57212	17932	Message: [thrd:111.11.11.11:9092/22]: 111.11.11.11:9092/22: Broker changed state CONNECT -> DOWN
2020-07-12 13:11:40.2589723	57212	65716	Message: [thrd:app]: 111.11.11.11:9092/22: OffsetRequest failed: Local: Broker transport failure: actions Retry
2020-07-12 13:11:40.2590186	57212	65716	Message: [thrd:app]: 111.11.11.11:9092/22: OffsetRequest failed: Local: Broker transport failure: actions Retry
2020-07-12 13:11:41.2587734	57212	17932	Message: [thrd:111.11.11.11:9092/22]: 111.11.11.11:9092/22: Moved 2 retry buffer(s) to output queue
2020-07-12 13:11:41.2589526	57212	17932	Message: [thrd:111.11.11.11:9092/22]: 111.11.11.11:9092/22: Broker changed state DOWN -> INIT
2020-07-12 13:12:42.1769763	57212	17932	ST1SPFA919BB8E1	AzPubSub: Level: Warning, Message: [thrd:111.11.11.11:9092/22]: 111.11.11.11:9092/22: Timed out 0 in-flight, 0 retry-queued, 2 out-queue, 0 partially-sent requests
2020-07-12 13:12:42.1769973	57212	65716	Message: [thrd:app]: 111.11.11.11:9092/22: OffsetRequest failed: Local: Timed out in queue: actions Retry
2020-07-12 13:12:42.1770423	57212	65716	Message: [thrd:app]: 111.11.11.11:9092/22: OffsetRequest failed: Local: Timed out in queue: actions Retry
2020-07-12 13:12:43.1768838	57212	17932	Message: [thrd:111.11.11.11:9092/22]: 111.11.11.11:9092/22: Broker changed state INIT -> TRY_CONNECT
2020-07-12 13:12:43.1768951	57212	17932	Message: [thrd:111.11.11.11:9092/22]: 111.11.11.11:9092/22: Moved 2 retry buffer(s) to output queue
2020-07-12 13:12:43.1769054	57212	17932	Message: [thrd:111.11.11.11:9092/22]: 111.11.11.11:9092/22: broker in state TRY_CONNECT connecting
2020-07-12 13:12:43.1769121	57212	17932	Message: [thrd:111.11.11.11:9092/22]: 111.11.11.11:9092/22: Broker changed state TRY_CONNECT -> CONNECT
2020-07-12 13:12:43.1769978	57212	17932	Message: [thrd:111.11.11.11:9092/22]: 111.11.11.11:9092/22: Connecting to ipv4#111.11.11.11:9092 (plaintext) with socket 72256
2020-07-12 13:13:04.2068078	57212	17932	Message: [thrd:111.11.11.11:9092/22]: 111.11.11.11:9092/22: failed: err: Local: Broker transport failure: (errno: Unknown error)
2020-07-12 13:13:04.2068460	57212	17932	Message: [thrd:111.11.11.11:9092/22]: 111.11.11.11:9092/22: Connect to ipv4#111.11.11.11:9092 failed: Unknown error (after 21029ms in state CONNECT)
2020-07-12 13:13:04.2068552	57212	17932	Message: [thrd:111.11.11.11:9092/22]: 111.11.11.11:9092/22: Broker changed state CONNECT -> DOWN
2020-07-12 13:13:04.2069004	57212	65716	Message: [thrd:app]: 111.11.11.11:9092/22: OffsetRequest failed: Local: Broker transport failure: actions Retry
2020-07-12 13:13:04.2069483	57212	65716	Message: [thrd:app]: 111.11.11.11:9092/22: OffsetRequest failed: Local: Broker transport failure: actions Retry
2020-07-12 13:13:05.2224585	57212	17932	Message: [thrd:111.11.11.11:9092/22]: 111.11.11.11:9092/22: Broker changed state DOWN -> INIT
2020-07-12 13:13:05.2224695	57212	17932	Message: [thrd:111.11.11.11:9092/22]: 111.11.11.11:9092/22: Broker changed state INIT -> TRY_CONNECT
2020-07-12 13:13:05.2224805	57212	17932	Message: [thrd:111.11.11.11:9092/22]: 111.11.11.11:9092/22: Moved 2 retry buffer(s) to output queue
2020-07-12 13:13:06.2379348	57212	17932	Message: [thrd:111.11.11.11:9092/22]: 111.11.11.11:9092/22: broker in state TRY_CONNECT connecting
2020-07-12 13:13:06.2379455	57212	17932	Message: [thrd:111.11.11.11:9092/22]: 111.11.11.11:9092/22: Broker changed state TRY_CONNECT -> CONNECT
2020-07-12 13:13:06.2380237	57212	17932	Message: [thrd:111.11.11.11:9092/22]: 111.11.11.11:9092/22: Connecting to ipv4#111.11.11.11:9092 (plaintext) with socket 34608
2020-07-12 13:13:27.2679049	57212	17932	Message: [thrd:111.11.11.11:9092/22]: 111.11.11.11:9092/22: failed: err: Local: Broker transport failure: (errno: Unknown error)
2020-07-12 13:13:27.2679436	57212	17932	Message: [thrd:111.11.11.11:9092/22]: 111.11.11.11:9092/22: Connect to ipv4#111.11.11.11:9092 failed: Unknown error (after 21029ms in state CONNECT)
2020-07-12 13:13:27.2679514	57212	17932	Message: [thrd:111.11.11.11:9092/22]: 111.11.11.11:9092/22: Broker changed state CONNECT -> DOWN
2020-07-12 13:13:27.2679989	57212	65716	Message: [thrd:app]: 111.11.11.11:9092/22: OffsetRequest failed: Local: Broker transport failure: actions Retry
2020-07-12 13:13:27.2680470	57212	65716	Message: [thrd:app]: 111.11.11.11:9092/22: OffsetRequest failed: Local: Broker transport failure: actions Retry
2020-07-12 13:13:28.2678549	57212	17932	Message: [thrd:111.11.11.11:9092/22]: 111.11.11.11:9092/22: Moved 2 retry buffer(s) to output queue
2020-07-12 13:13:28.2679737	57212	17932	Message: [thrd:111.11.11.11:9092/22]: 111.11.11.11:9092/22: Broker changed state DOWN -> INIT
2020-07-12 13:13:55.4857274	57212	68108	Message: [thrd:main]: 111.11.11.12:9092/18: OffsetRequest (v0, opv 0) for 1 topic(s) and 1 partition(s)
2020-07-12 13:14:29.1859411	57212	17932	ST1SPFA919BB8E1	AzPubSub: Level: Warning, Message: [thrd:111.11.11.11:9092/22]: 111.11.11.11:9092/22: Timed out 0 in-flight, 0 retry-queued, 2 out-queue, 0 partially-sent requests
2020-07-12 13:14:29.1859725	57212	65716	Message: [thrd:app]: 111.11.11.11:9092/22: OffsetRequest failed: Local: Timed out in queue: actions Retry
2020-07-12 13:14:29.1860193	57212	65716	Message: [thrd:app]: 111.11.11.11:9092/22: OffsetRequest failed: Local: Timed out in queue: actions Retry
2020-07-12 13:14:30.1858930	57212	17932	Message: [thrd:111.11.11.11:9092/22]: 111.11.11.11:9092/22: Broker changed state INIT -> TRY_CONNECT
2020-07-12 13:14:30.1859154	57212	17932	Message: [thrd:111.11.11.11:9092/22]: 111.11.11.11:9092/22: Moved 2 retry buffer(s) to output queue
2020-07-12 13:14:30.1859337	57212	17932	Message: [thrd:111.11.11.11:9092/22]: 111.11.11.11:9092/22: broker in state TRY_CONNECT connecting
2020-07-12 13:14:30.1859452	57212	17932	Message: [thrd:111.11.11.11:9092/22]: 111.11.11.11:9092/22: Broker changed state TRY_CONNECT -> CONNECT
2020-07-12 13:14:30.1860664	57212	17932	Message: [thrd:111.11.11.11:9092/22]: 111.11.11.11:9092/22: Connecting to ipv4#111.11.11.11:9092 (plaintext) with socket 73808
2020-07-12 13:14:51.2315394	57212	17932	Message: [thrd:111.11.11.11:9092/22]: 111.11.11.11:9092/22: failed: err: Local: Broker transport failure: (errno: Unknown error)
2020-07-12 13:14:51.2315938	57212	17932	Message: [thrd:111.11.11.11:9092/22]: 111.11.11.11:9092/22: Connect to ipv4#111.11.11.11:9092 failed: Unknown error (after 21045ms in state CONNECT)
2020-07-12 13:14:51.2316147	57212	17932	Message: [thrd:111.11.11.11:9092/22]: 111.11.11.11:9092/22: Broker changed state CONNECT -> DOWN
2020-07-12 13:14:51.2316855	57212	65716	Message: [thrd:app]: 111.11.11.11:9092/22: OffsetRequest failed: Local: Broker transport failure: actions Retry
2020-07-12 13:14:51.2317611	57212	65716	Message: [thrd:app]: 111.11.11.11:9092/22: OffsetRequest failed: Local: Broker transport failure: actions Retry
2020-07-12 13:14:52.2314715	57212	17932	Message: [thrd:111.11.11.11:9092/22]: 111.11.11.11:9092/22: Moved 2 retry buffer(s) to output queue
2020-07-12 13:14:52.2316432	57212	17932	Message: [thrd:111.11.11.11:9092/22]: 111.11.11.11:9092/22: Broker changed state DOWN -> INIT",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2537,2019-09-25T10:04:49Z,2019-09-26T16:53:40Z,2019-09-26T16:53:43Z,MERGED,True,535,173,13,https://github.com/edenhill,Improve test build and run instructions,2,[],https://github.com/edenhill/librdkafka/pull/2537,https://github.com/edenhill,1,https://github.com/edenhill/librdkafka/pull/2537,,,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2537,2019-09-25T10:04:49Z,2019-09-26T16:53:40Z,2019-09-26T16:53:43Z,MERGED,True,535,173,13,https://github.com/edenhill,Improve test build and run instructions,2,[],https://github.com/edenhill/librdkafka/pull/2537,https://github.com/edenhill,2,https://github.com/edenhill/librdkafka/pull/2537#issuecomment-535378897,,Please give this a final glance before merging,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2540,2019-09-26T08:52:49Z,2019-09-26T19:14:29Z,2019-09-26T20:03:29Z,MERGED,True,425,138,3,https://github.com/edenhill,Add supported KIPs and ApiVersions to manual,2,[],https://github.com/edenhill/librdkafka/pull/2540,https://github.com/edenhill,1,https://github.com/edenhill/librdkafka/pull/2540,...removed some outdated benchmarks and transferred some wiki pages to the manual.,...removed some outdated benchmarks and transferred some wiki pages to the manual.,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2541,2019-09-26T16:42:04Z,2019-09-26T16:53:52Z,2019-09-26T16:53:56Z,MERGED,True,11,4,1,https://github.com/edenhill,configure: added --disable-c11threads to avoid using libc-provided C11 threads,1,[],https://github.com/edenhill/librdkafka/pull/2541,https://github.com/edenhill,1,https://github.com/edenhill/librdkafka/pull/2541,.. since they pose problems with TSAN,.. since they pose problems with TSAN,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2546,2019-10-01T17:18:33Z,,2020-04-07T12:38:36Z,OPEN,False,130,0,1,https://github.com/piotrsmolinski,Support for logging TLS session keys using SSLKEYLOGFILE environment variable,2,[],https://github.com/edenhill/librdkafka/pull/2546,https://github.com/piotrsmolinski,1,https://github.com/edenhill/librdkafka/pull/2546,"SSLKEYLOGFILE environment variable is standard way of declaring the file to write the TLS session keys. This mechanism is used by major web browsers as well as by cURL and other projects. The session key logging is a powerful mechanism to debug network services in such tools as Wireshark.
In Java the same solution is achieved using jSslKeyLog agent (https://svn.code.sf.net/p/jsslkeylog/code). The patch provides the feature to be available in librdkafka based Kafka applications.
The code in the PR was written similar way as the same feature in cURL.","SSLKEYLOGFILE environment variable is standard way of declaring the file to write the TLS session keys. This mechanism is used by major web browsers as well as by cURL and other projects. The session key logging is a powerful mechanism to debug network services in such tools as Wireshark.
In Java the same solution is achieved using jSslKeyLog agent (https://svn.code.sf.net/p/jsslkeylog/code). The patch provides the feature to be available in librdkafka based Kafka applications.
The code in the PR was written similar way as the same feature in cURL.",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2546,2019-10-01T17:18:33Z,,2020-04-07T12:38:36Z,OPEN,False,130,0,1,https://github.com/piotrsmolinski,Support for logging TLS session keys using SSLKEYLOGFILE environment variable,2,[],https://github.com/edenhill/librdkafka/pull/2546,https://github.com/edenhill,2,https://github.com/edenhill/librdkafka/pull/2546#issuecomment-538887351,"SSLKEYLOGFILE environment variable is standard way of declaring the file to write the TLS session keys. This mechanism is used by major web browsers as well as by cURL and other projects. The session key logging is a powerful mechanism to debug network services in such tools as Wireshark.
In Java the same solution is achieved using jSslKeyLog agent (https://svn.code.sf.net/p/jsslkeylog/code). The patch provides the feature to be available in librdkafka based Kafka applications.
The code in the PR was written similar way as the same feature in cURL.",Please follow the coding style guide: https://github.com/edenhill/librdkafka/blob/master/CONTRIBUTING.md#librdkafka-c-style-guide,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2548,2019-10-03T08:13:29Z,2020-02-05T10:32:21Z,2020-02-05T10:32:28Z,MERGED,True,1,1,1,https://github.com/hanickadot,Fix clang warning [-Wstring-plus-int],1,[],https://github.com/edenhill/librdkafka/pull/2548,https://github.com/hanickadot,1,https://github.com/edenhill/librdkafka/pull/2548,"This fix will silence clang warning [-Wstring-plus-int]
librdkafka/src/rdkafka.c:369:2: warning: adding 'int' to a string does not append to the string [-Wstring-plus-int]
        _ERR_DESC(RD_KAFKA_RESP_ERR__BEGIN, NULL),
        ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
librdkafka/src/rdkafka.c:366:53: note: expanded from macro '_ERR_DESC'
        [ENUM - RD_KAFKA_RESP_ERR__BEGIN] = { ENUM, # ENUM + 18/*pfx*/, DESC }
                                                    ~~~~~~~^~~~
librdkafka/src/rdkafka.c:369:2: note: use array indexing to silence this warning","This fix will silence clang warning [-Wstring-plus-int]
librdkafka/src/rdkafka.c:369:2: warning: adding 'int' to a string does not append to the string [-Wstring-plus-int]
        _ERR_DESC(RD_KAFKA_RESP_ERR__BEGIN, NULL),
        ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
librdkafka/src/rdkafka.c:366:53: note: expanded from macro '_ERR_DESC'
        [ENUM - RD_KAFKA_RESP_ERR__BEGIN] = { ENUM, # ENUM + 18/*pfx*/, DESC }
                                                    ~~~~~~~^~~~
librdkafka/src/rdkafka.c:369:2: note: use array indexing to silence this warning",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2548,2019-10-03T08:13:29Z,2020-02-05T10:32:21Z,2020-02-05T10:32:28Z,MERGED,True,1,1,1,https://github.com/hanickadot,Fix clang warning [-Wstring-plus-int],1,[],https://github.com/edenhill/librdkafka/pull/2548,https://github.com/edenhill,2,https://github.com/edenhill/librdkafka/pull/2548#issuecomment-582343407,"This fix will silence clang warning [-Wstring-plus-int]
librdkafka/src/rdkafka.c:369:2: warning: adding 'int' to a string does not append to the string [-Wstring-plus-int]
        _ERR_DESC(RD_KAFKA_RESP_ERR__BEGIN, NULL),
        ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
librdkafka/src/rdkafka.c:366:53: note: expanded from macro '_ERR_DESC'
        [ENUM - RD_KAFKA_RESP_ERR__BEGIN] = { ENUM, # ENUM + 18/*pfx*/, DESC }
                                                    ~~~~~~~^~~~
librdkafka/src/rdkafka.c:369:2: note: use array indexing to silence this warning",Thank you!,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2551,2019-10-04T17:24:28Z,2019-10-04T18:05:01Z,2019-10-04T18:12:05Z,MERGED,True,25,4,1,https://github.com/edenhill,Properly handle new Kafka-framed SASL GSSAPI frame semantics on Win32/SSPI (#2542),1,[],https://github.com/edenhill/librdkafka/pull/2551,https://github.com/edenhill,1,https://github.com/edenhill/librdkafka/pull/2551,"Manually verified with Windows AD, big kudos to Lord Security @rnpridgeon","Manually verified with Windows AD, big kudos to Lord Security @rnpridgeon",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2553,2019-10-07T00:50:26Z,2020-04-29T06:24:56Z,2020-05-20T00:16:36Z,MERGED,True,267,185,56,https://github.com/ed-alertedh,Patches for MinGW-w64,13,[],https://github.com/edenhill/librdkafka/pull/2553,https://github.com/ed-alertedh,1,https://github.com/edenhill/librdkafka/pull/2553,"This set of changes allows me to build librdkafka with a MinGW-w64 cross-compiler and the existing CMake configuration. The main changes were:

Check for __MINGW32__ (MinGW-w64 defines this for both 32-bit and 64-bit targets) in addition to _MSC_VER
Use __thread rather than __declspec(thread) for MinGW.
Change the case of a few headers so that it works on a Linux host with a case-sensitive filesystem.
Define WINVER and _WIN32_WINNT to match the Windows 8.1 target set in the .vcxproj files

All the test cases that are enabled for Windows passed when I ran it. I had some trouble testing that the Visual Studio build still works because the OpenSSL lib you are linking with has moved to VS 2017 in the latest builds and they do not retain old builds for security reasons. It still compiles in VS 2015 but the link fails due to mismatched runtime libs.
If you are interested in merging this I'm very willing to make any necessary revisions. E.g. you might prefer to create a new define rather than repeating defined(_MSC_VER) || defined(__MINGW32__) everywhere. I also have a docker container with the cross-compiler toolchain I can contribute to help you build the tests.
edit:

Defined UNICODE to eliminate bug with mismatched char* and wchar_t*
Added a couple of pointer casts to silence GCC warnings about LONG* vs int32_t* (they are the same size under MinGW-w64, as they are with MSVC)","This set of changes allows me to build librdkafka with a MinGW-w64 cross-compiler and the existing CMake configuration. The main changes were:

Check for __MINGW32__ (MinGW-w64 defines this for both 32-bit and 64-bit targets) in addition to _MSC_VER
Use __thread rather than __declspec(thread) for MinGW.
Change the case of a few headers so that it works on a Linux host with a case-sensitive filesystem.
Define WINVER and _WIN32_WINNT to match the Windows 8.1 target set in the .vcxproj files

All the test cases that are enabled for Windows passed when I ran it. I had some trouble testing that the Visual Studio build still works because the OpenSSL lib you are linking with has moved to VS 2017 in the latest builds and they do not retain old builds for security reasons. It still compiles in VS 2015 but the link fails due to mismatched runtime libs.
If you are interested in merging this I'm very willing to make any necessary revisions. E.g. you might prefer to create a new define rather than repeating defined(_MSC_VER) || defined(__MINGW32__) everywhere. I also have a docker container with the cross-compiler toolchain I can contribute to help you build the tests.
edit:

Defined UNICODE to eliminate bug with mismatched char* and wchar_t*
Added a couple of pointer casts to silence GCC warnings about LONG* vs int32_t* (they are the same size under MinGW-w64, as they are with MSVC)",True,"{'HOORAY': ['https://github.com/ryanmickler', 'https://github.com/XhstormR']}"
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2553,2019-10-07T00:50:26Z,2020-04-29T06:24:56Z,2020-05-20T00:16:36Z,MERGED,True,267,185,56,https://github.com/ed-alertedh,Patches for MinGW-w64,13,[],https://github.com/edenhill/librdkafka/pull/2553,https://github.com/edenhill,2,https://github.com/edenhill/librdkafka/pull/2553#issuecomment-538864274,"This set of changes allows me to build librdkafka with a MinGW-w64 cross-compiler and the existing CMake configuration. The main changes were:

Check for __MINGW32__ (MinGW-w64 defines this for both 32-bit and 64-bit targets) in addition to _MSC_VER
Use __thread rather than __declspec(thread) for MinGW.
Change the case of a few headers so that it works on a Linux host with a case-sensitive filesystem.
Define WINVER and _WIN32_WINNT to match the Windows 8.1 target set in the .vcxproj files

All the test cases that are enabled for Windows passed when I ran it. I had some trouble testing that the Visual Studio build still works because the OpenSSL lib you are linking with has moved to VS 2017 in the latest builds and they do not retain old builds for security reasons. It still compiles in VS 2015 but the link fails due to mismatched runtime libs.
If you are interested in merging this I'm very willing to make any necessary revisions. E.g. you might prefer to create a new define rather than repeating defined(_MSC_VER) || defined(__MINGW32__) everywhere. I also have a docker container with the cross-compiler toolchain I can contribute to help you build the tests.
edit:

Defined UNICODE to eliminate bug with mismatched char* and wchar_t*
Added a couple of pointer casts to silence GCC warnings about LONG* vs int32_t* (they are the same size under MinGW-w64, as they are with MSVC)","Thanks for your contribution!
I think it does make sense to create a ""WITH_WIN32"" define to replace the MSC_VER or MINGW checks.",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2553,2019-10-07T00:50:26Z,2020-04-29T06:24:56Z,2020-05-20T00:16:36Z,MERGED,True,267,185,56,https://github.com/ed-alertedh,Patches for MinGW-w64,13,[],https://github.com/edenhill/librdkafka/pull/2553,https://github.com/ed-alertedh,3,https://github.com/edenhill/librdkafka/pull/2553#issuecomment-539278162,"This set of changes allows me to build librdkafka with a MinGW-w64 cross-compiler and the existing CMake configuration. The main changes were:

Check for __MINGW32__ (MinGW-w64 defines this for both 32-bit and 64-bit targets) in addition to _MSC_VER
Use __thread rather than __declspec(thread) for MinGW.
Change the case of a few headers so that it works on a Linux host with a case-sensitive filesystem.
Define WINVER and _WIN32_WINNT to match the Windows 8.1 target set in the .vcxproj files

All the test cases that are enabled for Windows passed when I ran it. I had some trouble testing that the Visual Studio build still works because the OpenSSL lib you are linking with has moved to VS 2017 in the latest builds and they do not retain old builds for security reasons. It still compiles in VS 2015 but the link fails due to mismatched runtime libs.
If you are interested in merging this I'm very willing to make any necessary revisions. E.g. you might prefer to create a new define rather than repeating defined(_MSC_VER) || defined(__MINGW32__) everywhere. I also have a docker container with the cross-compiler toolchain I can contribute to help you build the tests.
edit:

Defined UNICODE to eliminate bug with mismatched char* and wchar_t*
Added a couple of pointer casts to silence GCC warnings about LONG* vs int32_t* (they are the same size under MinGW-w64, as they are with MSVC)","Not at all, thanks for providing such a useful library!
I've just refactored to use WITH_WIN32 instead (there are a couple of special cases where it still checks for MSVC vs MinGW). Having taken a look at your current CI setup I think it might be easiest to take advantage of the ""Visual Studio 2013"" Appveyor image you are using already. It has CMake and various MinGW-w64 versions installed already so it should be possible to build with CMake and run tests in there. I'm happy to take a crack at setting that up if you'd like.",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2553,2019-10-07T00:50:26Z,2020-04-29T06:24:56Z,2020-05-20T00:16:36Z,MERGED,True,267,185,56,https://github.com/ed-alertedh,Patches for MinGW-w64,13,[],https://github.com/edenhill/librdkafka/pull/2553,https://github.com/neptoess,4,https://github.com/edenhill/librdkafka/pull/2553#issuecomment-571093648,"This set of changes allows me to build librdkafka with a MinGW-w64 cross-compiler and the existing CMake configuration. The main changes were:

Check for __MINGW32__ (MinGW-w64 defines this for both 32-bit and 64-bit targets) in addition to _MSC_VER
Use __thread rather than __declspec(thread) for MinGW.
Change the case of a few headers so that it works on a Linux host with a case-sensitive filesystem.
Define WINVER and _WIN32_WINNT to match the Windows 8.1 target set in the .vcxproj files

All the test cases that are enabled for Windows passed when I ran it. I had some trouble testing that the Visual Studio build still works because the OpenSSL lib you are linking with has moved to VS 2017 in the latest builds and they do not retain old builds for security reasons. It still compiles in VS 2015 but the link fails due to mismatched runtime libs.
If you are interested in merging this I'm very willing to make any necessary revisions. E.g. you might prefer to create a new define rather than repeating defined(_MSC_VER) || defined(__MINGW32__) everywhere. I also have a docker container with the cross-compiler toolchain I can contribute to help you build the tests.
edit:

Defined UNICODE to eliminate bug with mismatched char* and wchar_t*
Added a couple of pointer casts to silence GCC warnings about LONG* vs int32_t* (they are the same size under MinGW-w64, as they are with MSVC)","@ed-alertedh,
Does using _WIN32 instead of the the custom WITH_WIN32 work for this?
The only change Im concerned with is using __thread instead of __declspec(thread). Both of these are compiler-specific, so it may make sense to explicitly check MSVC or MinGW in this case (maybe fallback to _Thread_local if both MSVC and MinGW checks fail? Assumes whatever compiler this is supports C11 though). Thoughts?",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2553,2019-10-07T00:50:26Z,2020-04-29T06:24:56Z,2020-05-20T00:16:36Z,MERGED,True,267,185,56,https://github.com/ed-alertedh,Patches for MinGW-w64,13,[],https://github.com/edenhill/librdkafka/pull/2553,https://github.com/ed-alertedh,5,https://github.com/edenhill/librdkafka/pull/2553#issuecomment-571357778,"This set of changes allows me to build librdkafka with a MinGW-w64 cross-compiler and the existing CMake configuration. The main changes were:

Check for __MINGW32__ (MinGW-w64 defines this for both 32-bit and 64-bit targets) in addition to _MSC_VER
Use __thread rather than __declspec(thread) for MinGW.
Change the case of a few headers so that it works on a Linux host with a case-sensitive filesystem.
Define WINVER and _WIN32_WINNT to match the Windows 8.1 target set in the .vcxproj files

All the test cases that are enabled for Windows passed when I ran it. I had some trouble testing that the Visual Studio build still works because the OpenSSL lib you are linking with has moved to VS 2017 in the latest builds and they do not retain old builds for security reasons. It still compiles in VS 2015 but the link fails due to mismatched runtime libs.
If you are interested in merging this I'm very willing to make any necessary revisions. E.g. you might prefer to create a new define rather than repeating defined(_MSC_VER) || defined(__MINGW32__) everywhere. I also have a docker container with the cross-compiler toolchain I can contribute to help you build the tests.
edit:

Defined UNICODE to eliminate bug with mismatched char* and wchar_t*
Added a couple of pointer casts to silence GCC warnings about LONG* vs int32_t* (they are the same size under MinGW-w64, as they are with MSVC)","Hmm, it's been a while since I looked at this. @neptoess as far as I can tell, _WIN32 is compiler-specific and is only defined by MSVC [1], hence why I was originally checking for _MSC_VER or __MINGW32__. It could be that your cgo build system is defining some extra symbols for compatibility.
I agree that this PR could still be improved. Last time I worked on this I opted to make the build system define WITH_WIN32 since there didn't seem to be a common header included by everything. But I hadn't really considered consumers of the public header, so really the solution is probably to include something like this everywhere:
#if defined(_MSC_VER) || defined(__MINGW32__)
#define WITH_WIN32
#endif

This is a little fragile though, because if you forgot to include it in one file it will build incorrectly. That was the other motivation for defining WITH_WIN32 in the build system.
With respect to  __thread, vs __declspec(thread), I believe I already put compiler-specific checks in there out of necessity. Not being super familiar with C11 I didn't know _Thread_local was a fallback option, I guess it can't hurt to add that.
[1] http://mingw.5.n7.nabble.com/difference-between-WIN32-and-WIN32-macro-tp10055p10057.html
""As far as I know, WIN32/_WIN32 are defines that are builtins of the
Microsoft compiler.""",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2553,2019-10-07T00:50:26Z,2020-04-29T06:24:56Z,2020-05-20T00:16:36Z,MERGED,True,267,185,56,https://github.com/ed-alertedh,Patches for MinGW-w64,13,[],https://github.com/edenhill/librdkafka/pull/2553,https://github.com/neptoess,6,https://github.com/edenhill/librdkafka/pull/2553#issuecomment-571388855,"This set of changes allows me to build librdkafka with a MinGW-w64 cross-compiler and the existing CMake configuration. The main changes were:

Check for __MINGW32__ (MinGW-w64 defines this for both 32-bit and 64-bit targets) in addition to _MSC_VER
Use __thread rather than __declspec(thread) for MinGW.
Change the case of a few headers so that it works on a Linux host with a case-sensitive filesystem.
Define WINVER and _WIN32_WINNT to match the Windows 8.1 target set in the .vcxproj files

All the test cases that are enabled for Windows passed when I ran it. I had some trouble testing that the Visual Studio build still works because the OpenSSL lib you are linking with has moved to VS 2017 in the latest builds and they do not retain old builds for security reasons. It still compiles in VS 2015 but the link fails due to mismatched runtime libs.
If you are interested in merging this I'm very willing to make any necessary revisions. E.g. you might prefer to create a new define rather than repeating defined(_MSC_VER) || defined(__MINGW32__) everywhere. I also have a docker container with the cross-compiler toolchain I can contribute to help you build the tests.
edit:

Defined UNICODE to eliminate bug with mismatched char* and wchar_t*
Added a couple of pointer casts to silence GCC warnings about LONG* vs int32_t* (they are the same size under MinGW-w64, as they are with MSVC)","@ed-alertedh,
If I run use the MinGW environment and run gcc -E -dM test.c I see _WIN32. Can you try this with the Linux mingw gcc? I agree that we dont want to rely on the build system defining macros.
Regarding __thread vs __declspec, I think you got that covered pretty well. No need to bring _Thread_local into it.
I think were fairly safe either way we handle MSVC / coexistence (as long as we dont rely on the build system) since:

The Windows binaries for rdkafka are compiled with MSVC, and both options leave that build working
Regardless of how we handle MinGW, I highly doubt a 3rd compiler targeting Windows will enter the mix.

Ill close my PR since this one getting merged will solve the same problem.",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2553,2019-10-07T00:50:26Z,2020-04-29T06:24:56Z,2020-05-20T00:16:36Z,MERGED,True,267,185,56,https://github.com/ed-alertedh,Patches for MinGW-w64,13,[],https://github.com/edenhill/librdkafka/pull/2553,https://github.com/ed-alertedh,7,https://github.com/edenhill/librdkafka/pull/2553#issuecomment-571394063,"This set of changes allows me to build librdkafka with a MinGW-w64 cross-compiler and the existing CMake configuration. The main changes were:

Check for __MINGW32__ (MinGW-w64 defines this for both 32-bit and 64-bit targets) in addition to _MSC_VER
Use __thread rather than __declspec(thread) for MinGW.
Change the case of a few headers so that it works on a Linux host with a case-sensitive filesystem.
Define WINVER and _WIN32_WINNT to match the Windows 8.1 target set in the .vcxproj files

All the test cases that are enabled for Windows passed when I ran it. I had some trouble testing that the Visual Studio build still works because the OpenSSL lib you are linking with has moved to VS 2017 in the latest builds and they do not retain old builds for security reasons. It still compiles in VS 2015 but the link fails due to mismatched runtime libs.
If you are interested in merging this I'm very willing to make any necessary revisions. E.g. you might prefer to create a new define rather than repeating defined(_MSC_VER) || defined(__MINGW32__) everywhere. I also have a docker container with the cross-compiler toolchain I can contribute to help you build the tests.
edit:

Defined UNICODE to eliminate bug with mismatched char* and wchar_t*
Added a couple of pointer casts to silence GCC warnings about LONG* vs int32_t* (they are the same size under MinGW-w64, as they are with MSVC)","@neptoess my apologies, you're right!
# /usr/local/bin/x86_64-w64-mingw32-gcc  -E -dM test.c
[...]
#define _WIN32 1
[...]
#define _WIN64 1
[...]

That makes things much cleaner, good find. This is still something we're planning to use whether it gets merged or not, so I'll aim to merge the latest changes from master and refactor to use _WIN32 some time in the next week or so.",True,{'THUMBS_UP': ['https://github.com/neptoess']}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2553,2019-10-07T00:50:26Z,2020-04-29T06:24:56Z,2020-05-20T00:16:36Z,MERGED,True,267,185,56,https://github.com/ed-alertedh,Patches for MinGW-w64,13,[],https://github.com/edenhill/librdkafka/pull/2553,https://github.com/ed-alertedh,8,https://github.com/edenhill/librdkafka/pull/2553#issuecomment-582767114,"This set of changes allows me to build librdkafka with a MinGW-w64 cross-compiler and the existing CMake configuration. The main changes were:

Check for __MINGW32__ (MinGW-w64 defines this for both 32-bit and 64-bit targets) in addition to _MSC_VER
Use __thread rather than __declspec(thread) for MinGW.
Change the case of a few headers so that it works on a Linux host with a case-sensitive filesystem.
Define WINVER and _WIN32_WINNT to match the Windows 8.1 target set in the .vcxproj files

All the test cases that are enabled for Windows passed when I ran it. I had some trouble testing that the Visual Studio build still works because the OpenSSL lib you are linking with has moved to VS 2017 in the latest builds and they do not retain old builds for security reasons. It still compiles in VS 2015 but the link fails due to mismatched runtime libs.
If you are interested in merging this I'm very willing to make any necessary revisions. E.g. you might prefer to create a new define rather than repeating defined(_MSC_VER) || defined(__MINGW32__) everywhere. I also have a docker container with the cross-compiler toolchain I can contribute to help you build the tests.
edit:

Defined UNICODE to eliminate bug with mismatched char* and wchar_t*
Added a couple of pointer casts to silence GCC warnings about LONG* vs int32_t* (they are the same size under MinGW-w64, as they are with MSVC)","I've made a start at building it in Travis using CMake and a cross-compiler. Would you prefer it to use mklove instead? The CMake settings I have configured in .travis.yml worked locally in an Ubuntu Trusty VM, but I ran out of time today to get it building in Travis.
The older version of MinGW-w64 included with Trusty seems to be missing ctime_s and ctime_r so I have put in an alternate implementation for now - not sure if a non-english locale might break it with weekday and month names that aren't three chars long. In my codebase I am using a more recent version of MinGW-w64 and I did not seem to have issues using ctime_s.",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2553,2019-10-07T00:50:26Z,2020-04-29T06:24:56Z,2020-05-20T00:16:36Z,MERGED,True,267,185,56,https://github.com/ed-alertedh,Patches for MinGW-w64,13,[],https://github.com/edenhill/librdkafka/pull/2553,https://github.com/edenhill,9,https://github.com/edenhill/librdkafka/pull/2553#issuecomment-582775648,"This set of changes allows me to build librdkafka with a MinGW-w64 cross-compiler and the existing CMake configuration. The main changes were:

Check for __MINGW32__ (MinGW-w64 defines this for both 32-bit and 64-bit targets) in addition to _MSC_VER
Use __thread rather than __declspec(thread) for MinGW.
Change the case of a few headers so that it works on a Linux host with a case-sensitive filesystem.
Define WINVER and _WIN32_WINNT to match the Windows 8.1 target set in the .vcxproj files

All the test cases that are enabled for Windows passed when I ran it. I had some trouble testing that the Visual Studio build still works because the OpenSSL lib you are linking with has moved to VS 2017 in the latest builds and they do not retain old builds for security reasons. It still compiles in VS 2015 but the link fails due to mismatched runtime libs.
If you are interested in merging this I'm very willing to make any necessary revisions. E.g. you might prefer to create a new define rather than repeating defined(_MSC_VER) || defined(__MINGW32__) everywhere. I also have a docker container with the cross-compiler toolchain I can contribute to help you build the tests.
edit:

Defined UNICODE to eliminate bug with mismatched char* and wchar_t*
Added a couple of pointer casts to silence GCC warnings about LONG* vs int32_t* (they are the same size under MinGW-w64, as they are with MSVC)","I recommend you use CMake for mingw build, suspecting that mklove will require quite a bit of modifications to fully support mingw.
It would be good if the mingw build was using a (standard) docker image, have a look at packaging/tools/build-...sh for various platforms.",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2553,2019-10-07T00:50:26Z,2020-04-29T06:24:56Z,2020-05-20T00:16:36Z,MERGED,True,267,185,56,https://github.com/ed-alertedh,Patches for MinGW-w64,13,[],https://github.com/edenhill/librdkafka/pull/2553,https://github.com/ed-alertedh,10,https://github.com/edenhill/librdkafka/pull/2553#issuecomment-583240813,"This set of changes allows me to build librdkafka with a MinGW-w64 cross-compiler and the existing CMake configuration. The main changes were:

Check for __MINGW32__ (MinGW-w64 defines this for both 32-bit and 64-bit targets) in addition to _MSC_VER
Use __thread rather than __declspec(thread) for MinGW.
Change the case of a few headers so that it works on a Linux host with a case-sensitive filesystem.
Define WINVER and _WIN32_WINNT to match the Windows 8.1 target set in the .vcxproj files

All the test cases that are enabled for Windows passed when I ran it. I had some trouble testing that the Visual Studio build still works because the OpenSSL lib you are linking with has moved to VS 2017 in the latest builds and they do not retain old builds for security reasons. It still compiles in VS 2015 but the link fails due to mismatched runtime libs.
If you are interested in merging this I'm very willing to make any necessary revisions. E.g. you might prefer to create a new define rather than repeating defined(_MSC_VER) || defined(__MINGW32__) everywhere. I also have a docker container with the cross-compiler toolchain I can contribute to help you build the tests.
edit:

Defined UNICODE to eliminate bug with mismatched char* and wchar_t*
Added a couple of pointer casts to silence GCC warnings about LONG* vs int32_t* (they are the same size under MinGW-w64, as they are with MSVC)","I spent some time today working on the CI build for this with mixed results. The history for this branch is getting pretty long so at a later point I can rebase if you like.
I found that the Trusty MinGW package is a little old and WSAPoll isn't properly defined in the headers. I had no problems building locally in the ubuntu:xenial docker container but for some reason the Travis xenial environment still didn't work for me and neither did bionic - unsure if there is some weird caching going on or what.
For now I have added a packaging script to build inside a container which seems to be working. The bincrafters/docker-mingw-gcc7 image doesn't seem to be maintained though so the perhaps it will be better to just use a debian/ubuntu image and install the mingw packages in the packaging script.",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2553,2019-10-07T00:50:26Z,2020-04-29T06:24:56Z,2020-05-20T00:16:36Z,MERGED,True,267,185,56,https://github.com/ed-alertedh,Patches for MinGW-w64,13,[],https://github.com/edenhill/librdkafka/pull/2553,https://github.com/ed-alertedh,11,https://github.com/edenhill/librdkafka/pull/2553#issuecomment-584482845,"This set of changes allows me to build librdkafka with a MinGW-w64 cross-compiler and the existing CMake configuration. The main changes were:

Check for __MINGW32__ (MinGW-w64 defines this for both 32-bit and 64-bit targets) in addition to _MSC_VER
Use __thread rather than __declspec(thread) for MinGW.
Change the case of a few headers so that it works on a Linux host with a case-sensitive filesystem.
Define WINVER and _WIN32_WINNT to match the Windows 8.1 target set in the .vcxproj files

All the test cases that are enabled for Windows passed when I ran it. I had some trouble testing that the Visual Studio build still works because the OpenSSL lib you are linking with has moved to VS 2017 in the latest builds and they do not retain old builds for security reasons. It still compiles in VS 2015 but the link fails due to mismatched runtime libs.
If you are interested in merging this I'm very willing to make any necessary revisions. E.g. you might prefer to create a new define rather than repeating defined(_MSC_VER) || defined(__MINGW32__) everywhere. I also have a docker container with the cross-compiler toolchain I can contribute to help you build the tests.
edit:

Defined UNICODE to eliminate bug with mismatched char* and wchar_t*
Added a couple of pointer casts to silence GCC warnings about LONG* vs int32_t* (they are the same size under MinGW-w64, as they are with MSVC)","OK, I have builds working now in:

Travis bionic env
ubuntu:bionic docker container as another packaging script that uploads artifacts
Travis windows env (using MSYS2 to provide cmake+make+mingw-w64)

I also have the tests running and passing in the Travis windows env (same set you are running for the VS build in appveyor).
Do you think you would distribute binaries for this configuration? I'm happy to keep building from source but if so, I'm wondering if it would be better to upload artifacts for the windows build since that is now the one that is tested. At the very least I think it would be nice to leave one linux cross-compile in there to check that it still builds in that configuration (main difference being the case-sensitive filesystem).",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2553,2019-10-07T00:50:26Z,2020-04-29T06:24:56Z,2020-05-20T00:16:36Z,MERGED,True,267,185,56,https://github.com/ed-alertedh,Patches for MinGW-w64,13,[],https://github.com/edenhill/librdkafka/pull/2553,https://github.com/neptoess,12,https://github.com/edenhill/librdkafka/pull/2553#issuecomment-612095693,"This set of changes allows me to build librdkafka with a MinGW-w64 cross-compiler and the existing CMake configuration. The main changes were:

Check for __MINGW32__ (MinGW-w64 defines this for both 32-bit and 64-bit targets) in addition to _MSC_VER
Use __thread rather than __declspec(thread) for MinGW.
Change the case of a few headers so that it works on a Linux host with a case-sensitive filesystem.
Define WINVER and _WIN32_WINNT to match the Windows 8.1 target set in the .vcxproj files

All the test cases that are enabled for Windows passed when I ran it. I had some trouble testing that the Visual Studio build still works because the OpenSSL lib you are linking with has moved to VS 2017 in the latest builds and they do not retain old builds for security reasons. It still compiles in VS 2015 but the link fails due to mismatched runtime libs.
If you are interested in merging this I'm very willing to make any necessary revisions. E.g. you might prefer to create a new define rather than repeating defined(_MSC_VER) || defined(__MINGW32__) everywhere. I also have a docker container with the cross-compiler toolchain I can contribute to help you build the tests.
edit:

Defined UNICODE to eliminate bug with mismatched char* and wchar_t*
Added a couple of pointer casts to silence GCC warnings about LONG* vs int32_t* (they are the same size under MinGW-w64, as they are with MSVC)","Anything I can do to help here? I see some of the tests failed in travis, but I'm not sure if that's the only thing preventing this from being merged.
EDIT: I was just looking through the confluent-kafka-go repo, and realized that, if this gets merged, we can build a librdkafka_windows.a and greatly simplify using confluent-kafka-go on Windows (I know it's not officially supported, but you can do this today by jumping through a few hoops to get MinGW gcc to link to the MSVC-built DLL)",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2553,2019-10-07T00:50:26Z,2020-04-29T06:24:56Z,2020-05-20T00:16:36Z,MERGED,True,267,185,56,https://github.com/ed-alertedh,Patches for MinGW-w64,13,[],https://github.com/edenhill/librdkafka/pull/2553,https://github.com/edenhill,13,https://github.com/edenhill/librdkafka/pull/2553#issuecomment-613273537,"This set of changes allows me to build librdkafka with a MinGW-w64 cross-compiler and the existing CMake configuration. The main changes were:

Check for __MINGW32__ (MinGW-w64 defines this for both 32-bit and 64-bit targets) in addition to _MSC_VER
Use __thread rather than __declspec(thread) for MinGW.
Change the case of a few headers so that it works on a Linux host with a case-sensitive filesystem.
Define WINVER and _WIN32_WINNT to match the Windows 8.1 target set in the .vcxproj files

All the test cases that are enabled for Windows passed when I ran it. I had some trouble testing that the Visual Studio build still works because the OpenSSL lib you are linking with has moved to VS 2017 in the latest builds and they do not retain old builds for security reasons. It still compiles in VS 2015 but the link fails due to mismatched runtime libs.
If you are interested in merging this I'm very willing to make any necessary revisions. E.g. you might prefer to create a new define rather than repeating defined(_MSC_VER) || defined(__MINGW32__) everywhere. I also have a docker container with the cross-compiler toolchain I can contribute to help you build the tests.
edit:

Defined UNICODE to eliminate bug with mismatched char* and wchar_t*
Added a couple of pointer casts to silence GCC warnings about LONG* vs int32_t* (they are the same size under MinGW-w64, as they are with MSVC)","Sorry for dropping the ball on this one, let's schedule it for the upcoming v1.5.0 release.
One thing that needs to be addressed is SSL support, a prebuilt librdkafka without SSL is not that useful for most people in this age.
Rumour has it that the Shining light OpenSSL builds that we're using in MSVC should work with MinGW as well, so might want to give that a stab?
I'd also like to see the build commands in .travis.yml be turned into cmd/bat files in packaging/mingw.. to keep .travis.yml cleaner and compartmentalize the mingw build logic to its own directory.",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2553,2019-10-07T00:50:26Z,2020-04-29T06:24:56Z,2020-05-20T00:16:36Z,MERGED,True,267,185,56,https://github.com/ed-alertedh,Patches for MinGW-w64,13,[],https://github.com/edenhill/librdkafka/pull/2553,https://github.com/edenhill,14,https://github.com/edenhill/librdkafka/pull/2553#issuecomment-613273625,"This set of changes allows me to build librdkafka with a MinGW-w64 cross-compiler and the existing CMake configuration. The main changes were:

Check for __MINGW32__ (MinGW-w64 defines this for both 32-bit and 64-bit targets) in addition to _MSC_VER
Use __thread rather than __declspec(thread) for MinGW.
Change the case of a few headers so that it works on a Linux host with a case-sensitive filesystem.
Define WINVER and _WIN32_WINNT to match the Windows 8.1 target set in the .vcxproj files

All the test cases that are enabled for Windows passed when I ran it. I had some trouble testing that the Visual Studio build still works because the OpenSSL lib you are linking with has moved to VS 2017 in the latest builds and they do not retain old builds for security reasons. It still compiles in VS 2015 but the link fails due to mismatched runtime libs.
If you are interested in merging this I'm very willing to make any necessary revisions. E.g. you might prefer to create a new define rather than repeating defined(_MSC_VER) || defined(__MINGW32__) everywhere. I also have a docker container with the cross-compiler toolchain I can contribute to help you build the tests.
edit:

Defined UNICODE to eliminate bug with mismatched char* and wchar_t*
Added a couple of pointer casts to silence GCC warnings about LONG* vs int32_t* (they are the same size under MinGW-w64, as they are with MSVC)",Also make sure to rebase this on latest master.,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2553,2019-10-07T00:50:26Z,2020-04-29T06:24:56Z,2020-05-20T00:16:36Z,MERGED,True,267,185,56,https://github.com/ed-alertedh,Patches for MinGW-w64,13,[],https://github.com/edenhill/librdkafka/pull/2553,https://github.com/ed-alertedh,15,https://github.com/edenhill/librdkafka/pull/2553#issuecomment-616886674,"This set of changes allows me to build librdkafka with a MinGW-w64 cross-compiler and the existing CMake configuration. The main changes were:

Check for __MINGW32__ (MinGW-w64 defines this for both 32-bit and 64-bit targets) in addition to _MSC_VER
Use __thread rather than __declspec(thread) for MinGW.
Change the case of a few headers so that it works on a Linux host with a case-sensitive filesystem.
Define WINVER and _WIN32_WINNT to match the Windows 8.1 target set in the .vcxproj files

All the test cases that are enabled for Windows passed when I ran it. I had some trouble testing that the Visual Studio build still works because the OpenSSL lib you are linking with has moved to VS 2017 in the latest builds and they do not retain old builds for security reasons. It still compiles in VS 2015 but the link fails due to mismatched runtime libs.
If you are interested in merging this I'm very willing to make any necessary revisions. E.g. you might prefer to create a new define rather than repeating defined(_MSC_VER) || defined(__MINGW32__) everywhere. I also have a docker container with the cross-compiler toolchain I can contribute to help you build the tests.
edit:

Defined UNICODE to eliminate bug with mismatched char* and wchar_t*
Added a couple of pointer casts to silence GCC warnings about LONG* vs int32_t* (they are the same size under MinGW-w64, as they are with MSVC)","@edenhill OK, sounds good. I'll do my best to prioritise this as you're clearly very busy! To give me a rough idea, what kind of target date are we talking here?
MinGW can't link with MSVC .lib static libs, but I could try dynamically linking with the DLLs in the Shining Light distribution. If the goal is to use the same OpenSSL library between the MSVC and MinGW librdkafka builds though, there will be a little extra work required to upgrade your version of Visual Studio: ""September 13, 2018 - Visual Studio 2017 is being used for builds meaning new runtime requirements"". I can't download the old versions of the libs living on your MSVC build instance - the Shining Light page has an amusing security-oriented explanation why.",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2553,2019-10-07T00:50:26Z,2020-04-29T06:24:56Z,2020-05-20T00:16:36Z,MERGED,True,267,185,56,https://github.com/ed-alertedh,Patches for MinGW-w64,13,[],https://github.com/edenhill/librdkafka/pull/2553,https://github.com/neptoess,16,https://github.com/edenhill/librdkafka/pull/2553#issuecomment-616894676,"This set of changes allows me to build librdkafka with a MinGW-w64 cross-compiler and the existing CMake configuration. The main changes were:

Check for __MINGW32__ (MinGW-w64 defines this for both 32-bit and 64-bit targets) in addition to _MSC_VER
Use __thread rather than __declspec(thread) for MinGW.
Change the case of a few headers so that it works on a Linux host with a case-sensitive filesystem.
Define WINVER and _WIN32_WINNT to match the Windows 8.1 target set in the .vcxproj files

All the test cases that are enabled for Windows passed when I ran it. I had some trouble testing that the Visual Studio build still works because the OpenSSL lib you are linking with has moved to VS 2017 in the latest builds and they do not retain old builds for security reasons. It still compiles in VS 2015 but the link fails due to mismatched runtime libs.
If you are interested in merging this I'm very willing to make any necessary revisions. E.g. you might prefer to create a new define rather than repeating defined(_MSC_VER) || defined(__MINGW32__) everywhere. I also have a docker container with the cross-compiler toolchain I can contribute to help you build the tests.
edit:

Defined UNICODE to eliminate bug with mismatched char* and wchar_t*
Added a couple of pointer casts to silence GCC warnings about LONG* vs int32_t* (they are the same size under MinGW-w64, as they are with MSVC)","MSYS2 distributes a MinGW version of openssl via pacman (https://packages.msys2.org/package/mingw-w64-x86_64-openssl), and it's up to date. It does include some DLLs, but there are no dependencies on the Visual C++ redistributables, so I strongly suspect they're using MinGW to build them.",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2553,2019-10-07T00:50:26Z,2020-04-29T06:24:56Z,2020-05-20T00:16:36Z,MERGED,True,267,185,56,https://github.com/ed-alertedh,Patches for MinGW-w64,13,[],https://github.com/edenhill/librdkafka/pull/2553,https://github.com/ed-alertedh,17,https://github.com/edenhill/librdkafka/pull/2553#issuecomment-616904813,"This set of changes allows me to build librdkafka with a MinGW-w64 cross-compiler and the existing CMake configuration. The main changes were:

Check for __MINGW32__ (MinGW-w64 defines this for both 32-bit and 64-bit targets) in addition to _MSC_VER
Use __thread rather than __declspec(thread) for MinGW.
Change the case of a few headers so that it works on a Linux host with a case-sensitive filesystem.
Define WINVER and _WIN32_WINNT to match the Windows 8.1 target set in the .vcxproj files

All the test cases that are enabled for Windows passed when I ran it. I had some trouble testing that the Visual Studio build still works because the OpenSSL lib you are linking with has moved to VS 2017 in the latest builds and they do not retain old builds for security reasons. It still compiles in VS 2015 but the link fails due to mismatched runtime libs.
If you are interested in merging this I'm very willing to make any necessary revisions. E.g. you might prefer to create a new define rather than repeating defined(_MSC_VER) || defined(__MINGW32__) everywhere. I also have a docker container with the cross-compiler toolchain I can contribute to help you build the tests.
edit:

Defined UNICODE to eliminate bug with mismatched char* and wchar_t*
Added a couple of pointer casts to silence GCC warnings about LONG* vs int32_t* (they are the same size under MinGW-w64, as they are with MSVC)","@neptoess I thought maybe the Shining Light build integrated with the Windows certificate store but it doesn't seem like it, so I think using the MSYS2 package is probably an easier option to set up and keep up-to-date. Only disadvantage I can see is differing versions/builds of OpenSSL might add more maintenance burden, but realistically it seems like majority of users will keep using the MSVC build.
edit: Just saw your comment about this potentially allowing for easier golang support on Windows. I guess in that case this build configuration could end up with a decent user base!",True,{'THUMBS_UP': ['https://github.com/neptoess']}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2553,2019-10-07T00:50:26Z,2020-04-29T06:24:56Z,2020-05-20T00:16:36Z,MERGED,True,267,185,56,https://github.com/ed-alertedh,Patches for MinGW-w64,13,[],https://github.com/edenhill/librdkafka/pull/2553,https://github.com/edenhill,18,https://github.com/edenhill/librdkafka/pull/2553#issuecomment-616981602,"This set of changes allows me to build librdkafka with a MinGW-w64 cross-compiler and the existing CMake configuration. The main changes were:

Check for __MINGW32__ (MinGW-w64 defines this for both 32-bit and 64-bit targets) in addition to _MSC_VER
Use __thread rather than __declspec(thread) for MinGW.
Change the case of a few headers so that it works on a Linux host with a case-sensitive filesystem.
Define WINVER and _WIN32_WINNT to match the Windows 8.1 target set in the .vcxproj files

All the test cases that are enabled for Windows passed when I ran it. I had some trouble testing that the Visual Studio build still works because the OpenSSL lib you are linking with has moved to VS 2017 in the latest builds and they do not retain old builds for security reasons. It still compiles in VS 2015 but the link fails due to mismatched runtime libs.
If you are interested in merging this I'm very willing to make any necessary revisions. E.g. you might prefer to create a new define rather than repeating defined(_MSC_VER) || defined(__MINGW32__) everywhere. I also have a docker container with the cross-compiler toolchain I can contribute to help you build the tests.
edit:

Defined UNICODE to eliminate bug with mismatched char* and wchar_t*
Added a couple of pointer casts to silence GCC warnings about LONG* vs int32_t* (they are the same size under MinGW-w64, as they are with MSVC)",Using the msys2 openssl package sounds like a good idea ,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2553,2019-10-07T00:50:26Z,2020-04-29T06:24:56Z,2020-05-20T00:16:36Z,MERGED,True,267,185,56,https://github.com/ed-alertedh,Patches for MinGW-w64,13,[],https://github.com/edenhill/librdkafka/pull/2553,https://github.com/ed-alertedh,19,https://github.com/edenhill/librdkafka/pull/2553#issuecomment-618265658,"This set of changes allows me to build librdkafka with a MinGW-w64 cross-compiler and the existing CMake configuration. The main changes were:

Check for __MINGW32__ (MinGW-w64 defines this for both 32-bit and 64-bit targets) in addition to _MSC_VER
Use __thread rather than __declspec(thread) for MinGW.
Change the case of a few headers so that it works on a Linux host with a case-sensitive filesystem.
Define WINVER and _WIN32_WINNT to match the Windows 8.1 target set in the .vcxproj files

All the test cases that are enabled for Windows passed when I ran it. I had some trouble testing that the Visual Studio build still works because the OpenSSL lib you are linking with has moved to VS 2017 in the latest builds and they do not retain old builds for security reasons. It still compiles in VS 2015 but the link fails due to mismatched runtime libs.
If you are interested in merging this I'm very willing to make any necessary revisions. E.g. you might prefer to create a new define rather than repeating defined(_MSC_VER) || defined(__MINGW32__) everywhere. I also have a docker container with the cross-compiler toolchain I can contribute to help you build the tests.
edit:

Defined UNICODE to eliminate bug with mismatched char* and wchar_t*
Added a couple of pointer casts to silence GCC warnings about LONG* vs int32_t* (they are the same size under MinGW-w64, as they are with MSVC)","OK, hopefully it's close now. I haven't forgotten about rebasing but have been holding off on that because I tend to generate a lot of small commits!
Probably the major questions left are whether to get rid of the Linux cross-compile (which was mainly intended as a ""smoke test"") and whether to continue installing libs with MSYS2/pacman or consider switching to choco.",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2553,2019-10-07T00:50:26Z,2020-04-29T06:24:56Z,2020-05-20T00:16:36Z,MERGED,True,267,185,56,https://github.com/ed-alertedh,Patches for MinGW-w64,13,[],https://github.com/edenhill/librdkafka/pull/2553,https://github.com/neptoess,20,https://github.com/edenhill/librdkafka/pull/2553#issuecomment-618383549,"This set of changes allows me to build librdkafka with a MinGW-w64 cross-compiler and the existing CMake configuration. The main changes were:

Check for __MINGW32__ (MinGW-w64 defines this for both 32-bit and 64-bit targets) in addition to _MSC_VER
Use __thread rather than __declspec(thread) for MinGW.
Change the case of a few headers so that it works on a Linux host with a case-sensitive filesystem.
Define WINVER and _WIN32_WINNT to match the Windows 8.1 target set in the .vcxproj files

All the test cases that are enabled for Windows passed when I ran it. I had some trouble testing that the Visual Studio build still works because the OpenSSL lib you are linking with has moved to VS 2017 in the latest builds and they do not retain old builds for security reasons. It still compiles in VS 2015 but the link fails due to mismatched runtime libs.
If you are interested in merging this I'm very willing to make any necessary revisions. E.g. you might prefer to create a new define rather than repeating defined(_MSC_VER) || defined(__MINGW32__) everywhere. I also have a docker container with the cross-compiler toolchain I can contribute to help you build the tests.
edit:

Defined UNICODE to eliminate bug with mismatched char* and wchar_t*
Added a couple of pointer casts to silence GCC warnings about LONG* vs int32_t* (they are the same size under MinGW-w64, as they are with MSVC)","Probably the major questions left are whether to get rid of the Linux cross-compile (which was mainly intended as a ""smoke test"")

Since the artifacts coming out of the build are the same as the Windows MinGW build, and I think MSYS2's package manager makes wrangling dependencies more straightforward than cross-compiling everything from a Linux host, I don't see a downside to removing the Linux cross-compile build.

and whether to continue installing libs with MSYS2/pacman or consider switching to choco.

choco is mostly focused on automating application installs. NuGet is the system most often used for development package management, but, either way, neither is really focused on functionality / compatibility with the MinGW tooling. The MinGW packages from MSYS2 are actually intended to be used by the MinGW tooling, and are tested against it.
tl;dr I see no upside to using choco instead of pacman",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2553,2019-10-07T00:50:26Z,2020-04-29T06:24:56Z,2020-05-20T00:16:36Z,MERGED,True,267,185,56,https://github.com/ed-alertedh,Patches for MinGW-w64,13,[],https://github.com/edenhill/librdkafka/pull/2553,https://github.com/ed-alertedh,21,https://github.com/edenhill/librdkafka/pull/2553#issuecomment-618790634,"This set of changes allows me to build librdkafka with a MinGW-w64 cross-compiler and the existing CMake configuration. The main changes were:

Check for __MINGW32__ (MinGW-w64 defines this for both 32-bit and 64-bit targets) in addition to _MSC_VER
Use __thread rather than __declspec(thread) for MinGW.
Change the case of a few headers so that it works on a Linux host with a case-sensitive filesystem.
Define WINVER and _WIN32_WINNT to match the Windows 8.1 target set in the .vcxproj files

All the test cases that are enabled for Windows passed when I ran it. I had some trouble testing that the Visual Studio build still works because the OpenSSL lib you are linking with has moved to VS 2017 in the latest builds and they do not retain old builds for security reasons. It still compiles in VS 2015 but the link fails due to mismatched runtime libs.
If you are interested in merging this I'm very willing to make any necessary revisions. E.g. you might prefer to create a new define rather than repeating defined(_MSC_VER) || defined(__MINGW32__) everywhere. I also have a docker container with the cross-compiler toolchain I can contribute to help you build the tests.
edit:

Defined UNICODE to eliminate bug with mismatched char* and wchar_t*
Added a couple of pointer casts to silence GCC warnings about LONG* vs int32_t* (they are the same size under MinGW-w64, as they are with MSVC)",I've rebased/squashed onto latest master. The linux cross-compile is gone. I tend to agree that MSYS2 is a better way to manage the library dependencies than choco in this case. Let me know if you want anything else revised.,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2553,2019-10-07T00:50:26Z,2020-04-29T06:24:56Z,2020-05-20T00:16:36Z,MERGED,True,267,185,56,https://github.com/ed-alertedh,Patches for MinGW-w64,13,[],https://github.com/edenhill/librdkafka/pull/2553,https://github.com/ed-alertedh,22,https://github.com/edenhill/librdkafka/pull/2553#issuecomment-620394629,"This set of changes allows me to build librdkafka with a MinGW-w64 cross-compiler and the existing CMake configuration. The main changes were:

Check for __MINGW32__ (MinGW-w64 defines this for both 32-bit and 64-bit targets) in addition to _MSC_VER
Use __thread rather than __declspec(thread) for MinGW.
Change the case of a few headers so that it works on a Linux host with a case-sensitive filesystem.
Define WINVER and _WIN32_WINNT to match the Windows 8.1 target set in the .vcxproj files

All the test cases that are enabled for Windows passed when I ran it. I had some trouble testing that the Visual Studio build still works because the OpenSSL lib you are linking with has moved to VS 2017 in the latest builds and they do not retain old builds for security reasons. It still compiles in VS 2015 but the link fails due to mismatched runtime libs.
If you are interested in merging this I'm very willing to make any necessary revisions. E.g. you might prefer to create a new define rather than repeating defined(_MSC_VER) || defined(__MINGW32__) everywhere. I also have a docker container with the cross-compiler toolchain I can contribute to help you build the tests.
edit:

Defined UNICODE to eliminate bug with mismatched char* and wchar_t*
Added a couple of pointer casts to silence GCC warnings about LONG* vs int32_t* (they are the same size under MinGW-w64, as they are with MSVC)","@edenhill OK, I think I've addressed all your comments. I'm struggling to explain the error in this build. It think it might somehow be due to a bad/stale cache because the error relates to a codepath that shouldn't be active with _WIN32 defined. It builds fine for me locally in MSYS2 shell on Windows.
Unsure if I needed to do more when I added Windows to the matrix to ensure caching works correctly? I'm not super familiar with Travis CI.",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2553,2019-10-07T00:50:26Z,2020-04-29T06:24:56Z,2020-05-20T00:16:36Z,MERGED,True,267,185,56,https://github.com/ed-alertedh,Patches for MinGW-w64,13,[],https://github.com/edenhill/librdkafka/pull/2553,https://github.com/edenhill,23,https://github.com/edenhill/librdkafka/pull/2553#issuecomment-620418995,"This set of changes allows me to build librdkafka with a MinGW-w64 cross-compiler and the existing CMake configuration. The main changes were:

Check for __MINGW32__ (MinGW-w64 defines this for both 32-bit and 64-bit targets) in addition to _MSC_VER
Use __thread rather than __declspec(thread) for MinGW.
Change the case of a few headers so that it works on a Linux host with a case-sensitive filesystem.
Define WINVER and _WIN32_WINNT to match the Windows 8.1 target set in the .vcxproj files

All the test cases that are enabled for Windows passed when I ran it. I had some trouble testing that the Visual Studio build still works because the OpenSSL lib you are linking with has moved to VS 2017 in the latest builds and they do not retain old builds for security reasons. It still compiles in VS 2015 but the link fails due to mismatched runtime libs.
If you are interested in merging this I'm very willing to make any necessary revisions. E.g. you might prefer to create a new define rather than repeating defined(_MSC_VER) || defined(__MINGW32__) everywhere. I also have a docker container with the cross-compiler toolchain I can contribute to help you build the tests.
edit:

Defined UNICODE to eliminate bug with mismatched char* and wchar_t*
Added a couple of pointer casts to silence GCC warnings about LONG* vs int32_t* (they are the same size under MinGW-w64, as they are with MSVC)","Do you mean this error?

C:\Users\travis\build\edenhill\librdkafka\tests\test.c:3903:11: warning: implicit declaration of function 'WIFSIGNALED' [-Wimplicit-function-declaration]

Don't see why that ifdef-guard doesnt work",True,{'THUMBS_UP': ['https://github.com/ed-alertedh']}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2553,2019-10-07T00:50:26Z,2020-04-29T06:24:56Z,2020-05-20T00:16:36Z,MERGED,True,267,185,56,https://github.com/ed-alertedh,Patches for MinGW-w64,13,[],https://github.com/edenhill/librdkafka/pull/2553,https://github.com/ed-alertedh,24,https://github.com/edenhill/librdkafka/pull/2553#issuecomment-620437862,"This set of changes allows me to build librdkafka with a MinGW-w64 cross-compiler and the existing CMake configuration. The main changes were:

Check for __MINGW32__ (MinGW-w64 defines this for both 32-bit and 64-bit targets) in addition to _MSC_VER
Use __thread rather than __declspec(thread) for MinGW.
Change the case of a few headers so that it works on a Linux host with a case-sensitive filesystem.
Define WINVER and _WIN32_WINNT to match the Windows 8.1 target set in the .vcxproj files

All the test cases that are enabled for Windows passed when I ran it. I had some trouble testing that the Visual Studio build still works because the OpenSSL lib you are linking with has moved to VS 2017 in the latest builds and they do not retain old builds for security reasons. It still compiles in VS 2015 but the link fails due to mismatched runtime libs.
If you are interested in merging this I'm very willing to make any necessary revisions. E.g. you might prefer to create a new define rather than repeating defined(_MSC_VER) || defined(__MINGW32__) everywhere. I also have a docker container with the cross-compiler toolchain I can contribute to help you build the tests.
edit:

Defined UNICODE to eliminate bug with mismatched char* and wchar_t*
Added a couple of pointer casts to silence GCC warnings about LONG* vs int32_t* (they are the same size under MinGW-w64, as they are with MSVC)","In my use-case, I wanted to keep all my CI running in Linux on our existing CircleCI plan so I prefer to cross-compile my application with MinGW. So first and foremost, this PR makes the rdkafka.h header work on MinGW. This should allow dynamic linking with your current MSVC dll builds.
But it wasn't a lot more effort to make the whole thing compile on MinGW, so I thought it was worth it. Being able to statically link librdkafka is convenient for me and I can't do that with the MSVC build.
It sounded like all of this would also enable confluent-kafka-go to build on windows without patches, either linked dynamically against your MSVC builds or linked statically (which is standard for golang programs).
edit: I have to finish up for the day but it looks like I accidentally cancelled my test build with cache disabled -_-",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2553,2019-10-07T00:50:26Z,2020-04-29T06:24:56Z,2020-05-20T00:16:36Z,MERGED,True,267,185,56,https://github.com/ed-alertedh,Patches for MinGW-w64,13,[],https://github.com/edenhill/librdkafka/pull/2553,https://github.com/edenhill,25,https://github.com/edenhill/librdkafka/pull/2553#issuecomment-620463534,"This set of changes allows me to build librdkafka with a MinGW-w64 cross-compiler and the existing CMake configuration. The main changes were:

Check for __MINGW32__ (MinGW-w64 defines this for both 32-bit and 64-bit targets) in addition to _MSC_VER
Use __thread rather than __declspec(thread) for MinGW.
Change the case of a few headers so that it works on a Linux host with a case-sensitive filesystem.
Define WINVER and _WIN32_WINNT to match the Windows 8.1 target set in the .vcxproj files

All the test cases that are enabled for Windows passed when I ran it. I had some trouble testing that the Visual Studio build still works because the OpenSSL lib you are linking with has moved to VS 2017 in the latest builds and they do not retain old builds for security reasons. It still compiles in VS 2015 but the link fails due to mismatched runtime libs.
If you are interested in merging this I'm very willing to make any necessary revisions. E.g. you might prefer to create a new define rather than repeating defined(_MSC_VER) || defined(__MINGW32__) everywhere. I also have a docker container with the cross-compiler toolchain I can contribute to help you build the tests.
edit:

Defined UNICODE to eliminate bug with mismatched char* and wchar_t*
Added a couple of pointer casts to silence GCC warnings about LONG* vs int32_t* (they are the same size under MinGW-w64, as they are with MSVC)","Thank you for the explanation.
Some follow up questions:

Can mingw dynamic libraries be linked by msvc?
Can mingw static libraries be linked by msvc/vs?
Can mingw dynamic libraries depend on msvc DLLs?",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2553,2019-10-07T00:50:26Z,2020-04-29T06:24:56Z,2020-05-20T00:16:36Z,MERGED,True,267,185,56,https://github.com/ed-alertedh,Patches for MinGW-w64,13,[],https://github.com/edenhill/librdkafka/pull/2553,https://github.com/neptoess,26,https://github.com/edenhill/librdkafka/pull/2553#issuecomment-620605638,"This set of changes allows me to build librdkafka with a MinGW-w64 cross-compiler and the existing CMake configuration. The main changes were:

Check for __MINGW32__ (MinGW-w64 defines this for both 32-bit and 64-bit targets) in addition to _MSC_VER
Use __thread rather than __declspec(thread) for MinGW.
Change the case of a few headers so that it works on a Linux host with a case-sensitive filesystem.
Define WINVER and _WIN32_WINNT to match the Windows 8.1 target set in the .vcxproj files

All the test cases that are enabled for Windows passed when I ran it. I had some trouble testing that the Visual Studio build still works because the OpenSSL lib you are linking with has moved to VS 2017 in the latest builds and they do not retain old builds for security reasons. It still compiles in VS 2015 but the link fails due to mismatched runtime libs.
If you are interested in merging this I'm very willing to make any necessary revisions. E.g. you might prefer to create a new define rather than repeating defined(_MSC_VER) || defined(__MINGW32__) everywhere. I also have a docker container with the cross-compiler toolchain I can contribute to help you build the tests.
edit:

Defined UNICODE to eliminate bug with mismatched char* and wchar_t*
Added a couple of pointer casts to silence GCC warnings about LONG* vs int32_t* (they are the same size under MinGW-w64, as they are with MSVC)","@edenhill
All answers assume pure C code. C++ compiler ABIs are a mess. clang has this problem solved (they implemented the ability to output in the same ABI as Visual C++), but gcc does not.


Can mingw dynamic libraries be linked by msvc?


Yes. Even if the import libraries generated by MinGW couldn't be linked by Visual Studio, Visual Studio will generate import libraries based on a DLLs exports if you try to link to the .dll file directly.


Can mingw static libraries be linked by msvc/vs?


Yes, in practice. However, for this question and the previous one, we should probably consider something. Currently, there appears to be link compatibility between objects built with MinGW and those built with MSVC, and this has been the case for well over a decade as far as I know. That said, I can find no evidence that link compatibility with MSVC is a project goal of MinGW, So we're in the clear now, but it's probably best that we don't knowingly do anything in the code that breaks the ability to build with Visual Studio.


Can mingw dynamic libraries depend on msvc DLLs?


Yes, but things can get weird here once you try to distribute those DLLs. It's fairly common knowledge that software built with MSVC will rely on its runtime, so it's on the user to distribute that runtime. I noticed librdkafka.redist actually ships the runtime DLLs directly in the NuGet package. I'm not positive whether this complies with the runtime's license, but the vast majority of Windows software ships with the version of Microsoft's Visual C++ redistributable installer matching the compiler (https://support.microsoft.com/en-us/help/2977003/the-latest-supported-visual-c-downloads).
The same restrictions will apply to your MinGW dynamic library if it depends on libraries built with MSVC (newer than v6.0). Everything will build fine, but silently fail at runtime on the client machine if you forget to distribute the Visual C++ runtime.",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2553,2019-10-07T00:50:26Z,2020-04-29T06:24:56Z,2020-05-20T00:16:36Z,MERGED,True,267,185,56,https://github.com/ed-alertedh,Patches for MinGW-w64,13,[],https://github.com/edenhill/librdkafka/pull/2553,https://github.com/neptoess,27,https://github.com/edenhill/librdkafka/pull/2553#issuecomment-620614478,"This set of changes allows me to build librdkafka with a MinGW-w64 cross-compiler and the existing CMake configuration. The main changes were:

Check for __MINGW32__ (MinGW-w64 defines this for both 32-bit and 64-bit targets) in addition to _MSC_VER
Use __thread rather than __declspec(thread) for MinGW.
Change the case of a few headers so that it works on a Linux host with a case-sensitive filesystem.
Define WINVER and _WIN32_WINNT to match the Windows 8.1 target set in the .vcxproj files

All the test cases that are enabled for Windows passed when I ran it. I had some trouble testing that the Visual Studio build still works because the OpenSSL lib you are linking with has moved to VS 2017 in the latest builds and they do not retain old builds for security reasons. It still compiles in VS 2015 but the link fails due to mismatched runtime libs.
If you are interested in merging this I'm very willing to make any necessary revisions. E.g. you might prefer to create a new define rather than repeating defined(_MSC_VER) || defined(__MINGW32__) everywhere. I also have a docker container with the cross-compiler toolchain I can contribute to help you build the tests.
edit:

Defined UNICODE to eliminate bug with mismatched char* and wchar_t*
Added a couple of pointer casts to silence GCC warnings about LONG* vs int32_t* (they are the same size under MinGW-w64, as they are with MSVC)","@edenhill

Can you educate me on when and how the MinGW builds are useful, compared to the msvc nuget package? I'm not questioning the need, just want to understand since my knowledge of Windows runtimes are so-and-so.

The big benefit for me is, same as @ed-alertedh , being able to use rdkafka.h with MinGW, which allows me to use confluent-kafka-go on Windows (cgo doesn't support Microsoft's compilers). However, benefits for the overall project might be:

No need to maintain a separate Visual Studio build for the C library (which right now is stuck on VS2013 and can't use mainline OpenSSL I believe).
Opens up the possibility of static linking on Windows (which has huge potential for streamlining confluent-kafka-go on Windows)
Lets us take advantage of the MSYS2 distribution's packages for pulling in dependencies like OpenSSL
librdkafka.redist can stop shipping libzstd.dll and zlib.dll
Allows the MSYS2 community to create a librdkafka package, so users can develop Windows programs that use Kafka without Visual Studio.

There would be more, but the C++ part will have to be built by the same compiler the end user plans on using, so we'll have to keep Visual Studio around for that. However, if the only thing we need Visual Studio to build is the C++ code, we could consider removing C++ support from librdkafka.redist, and migrating it to vcpkg, which would be much easier to maintain as new Visual Studio versions are released.",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2553,2019-10-07T00:50:26Z,2020-04-29T06:24:56Z,2020-05-20T00:16:36Z,MERGED,True,267,185,56,https://github.com/ed-alertedh,Patches for MinGW-w64,13,[],https://github.com/edenhill/librdkafka/pull/2553,https://github.com/ed-alertedh,28,https://github.com/edenhill/librdkafka/pull/2553#issuecomment-620993878,"This set of changes allows me to build librdkafka with a MinGW-w64 cross-compiler and the existing CMake configuration. The main changes were:

Check for __MINGW32__ (MinGW-w64 defines this for both 32-bit and 64-bit targets) in addition to _MSC_VER
Use __thread rather than __declspec(thread) for MinGW.
Change the case of a few headers so that it works on a Linux host with a case-sensitive filesystem.
Define WINVER and _WIN32_WINNT to match the Windows 8.1 target set in the .vcxproj files

All the test cases that are enabled for Windows passed when I ran it. I had some trouble testing that the Visual Studio build still works because the OpenSSL lib you are linking with has moved to VS 2017 in the latest builds and they do not retain old builds for security reasons. It still compiles in VS 2015 but the link fails due to mismatched runtime libs.
If you are interested in merging this I'm very willing to make any necessary revisions. E.g. you might prefer to create a new define rather than repeating defined(_MSC_VER) || defined(__MINGW32__) everywhere. I also have a docker container with the cross-compiler toolchain I can contribute to help you build the tests.
edit:

Defined UNICODE to eliminate bug with mismatched char* and wchar_t*
Added a couple of pointer casts to silence GCC warnings about LONG* vs int32_t* (they are the same size under MinGW-w64, as they are with MSVC)","I don't think I have much to add to @neptoess 's answers to the questions. I'll admit I wasn't aware that MinGW happens to be link-compatible with MSVC for static C libs, but I agree that it seems safer not to mix compilers when linking statically. I'm not sure if I would recommend switching your main Windows build over to MinGW given you still need VS to build the C++ lib (as pointed out, MinGW and MSVC are not ABI compatible for C++).
I've fixed that build error now. I was ""chasing ghosts"" trying to turn caching on and off. The actual fix was to patch another instance of _MSC_VER that was recently introduced on master (I forgot that the PR builds run after merging with master).",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2553,2019-10-07T00:50:26Z,2020-04-29T06:24:56Z,2020-05-20T00:16:36Z,MERGED,True,267,185,56,https://github.com/ed-alertedh,Patches for MinGW-w64,13,[],https://github.com/edenhill/librdkafka/pull/2553,https://github.com/edenhill,29,https://github.com/edenhill/librdkafka/pull/2553#issuecomment-621007885,"This set of changes allows me to build librdkafka with a MinGW-w64 cross-compiler and the existing CMake configuration. The main changes were:

Check for __MINGW32__ (MinGW-w64 defines this for both 32-bit and 64-bit targets) in addition to _MSC_VER
Use __thread rather than __declspec(thread) for MinGW.
Change the case of a few headers so that it works on a Linux host with a case-sensitive filesystem.
Define WINVER and _WIN32_WINNT to match the Windows 8.1 target set in the .vcxproj files

All the test cases that are enabled for Windows passed when I ran it. I had some trouble testing that the Visual Studio build still works because the OpenSSL lib you are linking with has moved to VS 2017 in the latest builds and they do not retain old builds for security reasons. It still compiles in VS 2015 but the link fails due to mismatched runtime libs.
If you are interested in merging this I'm very willing to make any necessary revisions. E.g. you might prefer to create a new define rather than repeating defined(_MSC_VER) || defined(__MINGW32__) everywhere. I also have a docker container with the cross-compiler toolchain I can contribute to help you build the tests.
edit:

Defined UNICODE to eliminate bug with mismatched char* and wchar_t*
Added a couple of pointer casts to silence GCC warnings about LONG* vs int32_t* (they are the same size under MinGW-w64, as they are with MSVC)","Thank you both for the answers, getting a static win32 lib into confluent-kafka-go would be huge.",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2553,2019-10-07T00:50:26Z,2020-04-29T06:24:56Z,2020-05-20T00:16:36Z,MERGED,True,267,185,56,https://github.com/ed-alertedh,Patches for MinGW-w64,13,[],https://github.com/edenhill/librdkafka/pull/2553,https://github.com/ed-alertedh,30,https://github.com/edenhill/librdkafka/pull/2553#issuecomment-621012890,"This set of changes allows me to build librdkafka with a MinGW-w64 cross-compiler and the existing CMake configuration. The main changes were:

Check for __MINGW32__ (MinGW-w64 defines this for both 32-bit and 64-bit targets) in addition to _MSC_VER
Use __thread rather than __declspec(thread) for MinGW.
Change the case of a few headers so that it works on a Linux host with a case-sensitive filesystem.
Define WINVER and _WIN32_WINNT to match the Windows 8.1 target set in the .vcxproj files

All the test cases that are enabled for Windows passed when I ran it. I had some trouble testing that the Visual Studio build still works because the OpenSSL lib you are linking with has moved to VS 2017 in the latest builds and they do not retain old builds for security reasons. It still compiles in VS 2015 but the link fails due to mismatched runtime libs.
If you are interested in merging this I'm very willing to make any necessary revisions. E.g. you might prefer to create a new define rather than repeating defined(_MSC_VER) || defined(__MINGW32__) everywhere. I also have a docker container with the cross-compiler toolchain I can contribute to help you build the tests.
edit:

Defined UNICODE to eliminate bug with mismatched char* and wchar_t*
Added a couple of pointer casts to silence GCC warnings about LONG* vs int32_t* (they are the same size under MinGW-w64, as they are with MSVC)","Yep, ready to merge!",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2553,2019-10-07T00:50:26Z,2020-04-29T06:24:56Z,2020-05-20T00:16:36Z,MERGED,True,267,185,56,https://github.com/ed-alertedh,Patches for MinGW-w64,13,[],https://github.com/edenhill/librdkafka/pull/2553,https://github.com/edenhill,31,https://github.com/edenhill/librdkafka/pull/2553#issuecomment-621013675,"This set of changes allows me to build librdkafka with a MinGW-w64 cross-compiler and the existing CMake configuration. The main changes were:

Check for __MINGW32__ (MinGW-w64 defines this for both 32-bit and 64-bit targets) in addition to _MSC_VER
Use __thread rather than __declspec(thread) for MinGW.
Change the case of a few headers so that it works on a Linux host with a case-sensitive filesystem.
Define WINVER and _WIN32_WINNT to match the Windows 8.1 target set in the .vcxproj files

All the test cases that are enabled for Windows passed when I ran it. I had some trouble testing that the Visual Studio build still works because the OpenSSL lib you are linking with has moved to VS 2017 in the latest builds and they do not retain old builds for security reasons. It still compiles in VS 2015 but the link fails due to mismatched runtime libs.
If you are interested in merging this I'm very willing to make any necessary revisions. E.g. you might prefer to create a new define rather than repeating defined(_MSC_VER) || defined(__MINGW32__) everywhere. I also have a docker container with the cross-compiler toolchain I can contribute to help you build the tests.
edit:

Defined UNICODE to eliminate bug with mismatched char* and wchar_t*
Added a couple of pointer casts to silence GCC warnings about LONG* vs int32_t* (they are the same size under MinGW-w64, as they are with MSVC)","Superb, thank you!",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2553,2019-10-07T00:50:26Z,2020-04-29T06:24:56Z,2020-05-20T00:16:36Z,MERGED,True,267,185,56,https://github.com/ed-alertedh,Patches for MinGW-w64,13,[],https://github.com/edenhill/librdkafka/pull/2553,https://github.com/ed-alertedh,32,https://github.com/edenhill/librdkafka/pull/2553#issuecomment-621015462,"This set of changes allows me to build librdkafka with a MinGW-w64 cross-compiler and the existing CMake configuration. The main changes were:

Check for __MINGW32__ (MinGW-w64 defines this for both 32-bit and 64-bit targets) in addition to _MSC_VER
Use __thread rather than __declspec(thread) for MinGW.
Change the case of a few headers so that it works on a Linux host with a case-sensitive filesystem.
Define WINVER and _WIN32_WINNT to match the Windows 8.1 target set in the .vcxproj files

All the test cases that are enabled for Windows passed when I ran it. I had some trouble testing that the Visual Studio build still works because the OpenSSL lib you are linking with has moved to VS 2017 in the latest builds and they do not retain old builds for security reasons. It still compiles in VS 2015 but the link fails due to mismatched runtime libs.
If you are interested in merging this I'm very willing to make any necessary revisions. E.g. you might prefer to create a new define rather than repeating defined(_MSC_VER) || defined(__MINGW32__) everywhere. I also have a docker container with the cross-compiler toolchain I can contribute to help you build the tests.
edit:

Defined UNICODE to eliminate bug with mismatched char* and wchar_t*
Added a couple of pointer casts to silence GCC warnings about LONG* vs int32_t* (they are the same size under MinGW-w64, as they are with MSVC)","Likewise, thanks! Also, thanks for your reviews and your input @neptoess",True,"{'HOORAY': ['https://github.com/neptoess', 'https://github.com/ryanmickler']}"
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2553,2019-10-07T00:50:26Z,2020-04-29T06:24:56Z,2020-05-20T00:16:36Z,MERGED,True,267,185,56,https://github.com/ed-alertedh,Patches for MinGW-w64,13,[],https://github.com/edenhill/librdkafka/pull/2553,https://github.com/edenhill,33,https://github.com/edenhill/librdkafka/pull/2553#issuecomment-630739849,"This set of changes allows me to build librdkafka with a MinGW-w64 cross-compiler and the existing CMake configuration. The main changes were:

Check for __MINGW32__ (MinGW-w64 defines this for both 32-bit and 64-bit targets) in addition to _MSC_VER
Use __thread rather than __declspec(thread) for MinGW.
Change the case of a few headers so that it works on a Linux host with a case-sensitive filesystem.
Define WINVER and _WIN32_WINNT to match the Windows 8.1 target set in the .vcxproj files

All the test cases that are enabled for Windows passed when I ran it. I had some trouble testing that the Visual Studio build still works because the OpenSSL lib you are linking with has moved to VS 2017 in the latest builds and they do not retain old builds for security reasons. It still compiles in VS 2015 but the link fails due to mismatched runtime libs.
If you are interested in merging this I'm very willing to make any necessary revisions. E.g. you might prefer to create a new define rather than repeating defined(_MSC_VER) || defined(__MINGW32__) everywhere. I also have a docker container with the cross-compiler toolchain I can contribute to help you build the tests.
edit:

Defined UNICODE to eliminate bug with mismatched char* and wchar_t*
Added a couple of pointer casts to silence GCC warnings about LONG* vs int32_t* (they are the same size under MinGW-w64, as they are with MSVC)","We're seeing recurring mingw installation failures on travis:
https://travis-ci.org/github/edenhill/librdkafka/jobs/688737080
@ed-alertedh Can you take a look?",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2553,2019-10-07T00:50:26Z,2020-04-29T06:24:56Z,2020-05-20T00:16:36Z,MERGED,True,267,185,56,https://github.com/ed-alertedh,Patches for MinGW-w64,13,[],https://github.com/edenhill/librdkafka/pull/2553,https://github.com/neptoess,34,https://github.com/edenhill/librdkafka/pull/2553#issuecomment-630812478,"This set of changes allows me to build librdkafka with a MinGW-w64 cross-compiler and the existing CMake configuration. The main changes were:

Check for __MINGW32__ (MinGW-w64 defines this for both 32-bit and 64-bit targets) in addition to _MSC_VER
Use __thread rather than __declspec(thread) for MinGW.
Change the case of a few headers so that it works on a Linux host with a case-sensitive filesystem.
Define WINVER and _WIN32_WINNT to match the Windows 8.1 target set in the .vcxproj files

All the test cases that are enabled for Windows passed when I ran it. I had some trouble testing that the Visual Studio build still works because the OpenSSL lib you are linking with has moved to VS 2017 in the latest builds and they do not retain old builds for security reasons. It still compiles in VS 2015 but the link fails due to mismatched runtime libs.
If you are interested in merging this I'm very willing to make any necessary revisions. E.g. you might prefer to create a new define rather than repeating defined(_MSC_VER) || defined(__MINGW32__) everywhere. I also have a docker container with the cross-compiler toolchain I can contribute to help you build the tests.
edit:

Defined UNICODE to eliminate bug with mismatched char* and wchar_t*
Added a couple of pointer casts to silence GCC warnings about LONG* vs int32_t* (they are the same size under MinGW-w64, as they are with MSVC)","@edenhill / @ed-alertedh ,
Looks like we're not the only ones running into this https://chocolatey.org/packages/msys2#discussion
I'm thinking we can get away with adding pacman -Sy --noconfirm pacman before the choco upgrade in travis-before-install.sh, i.e. change
        choco uninstall -y mingw
        choco upgrade --no-progress -y msys2
        export msys2='cmd //C RefreshEnv.cmd '
        export msys2+='& set MSYS=winsymlinks:nativestrict '
        export msys2+='& C:\\tools\\msys64\\msys2_shell.cmd -defterm -no-start'
        export mingw64=""$msys2 -mingw64 -full-path -here -c ""\""\$@""\"" --""
        export msys2+="" -msys2 -c ""\""\$@""\"" --""
        $msys2 pacman --sync --noconfirm --needed mingw-w64-x86_64-toolchain mingw-w64-x86_64-cmake mingw-w64-x86_64-openssl mingw-w64-x86_64-cyrus-sasl

to
        choco uninstall -y mingw
        export msys2='cmd //C RefreshEnv.cmd '
        export msys2+='& set MSYS=winsymlinks:nativestrict '
        export msys2+='& C:\\tools\\msys64\\msys2_shell.cmd -defterm -no-start'
        export mingw64=""$msys2 -mingw64 -full-path -here -c ""\""\$@""\"" --""
        export msys2+="" -msys2 -c ""\""\$@""\"" --""
        $msys2 pacman -Sy --noconfirm pacman
        choco upgrade --no-progress -y msys2
        $msys2 pacman --sync --noconfirm --needed mingw-w64-x86_64-toolchain mingw-w64-x86_64-cmake mingw-w64-x86_64-openssl mingw-w64-x86_64-cyrus-sasl",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2553,2019-10-07T00:50:26Z,2020-04-29T06:24:56Z,2020-05-20T00:16:36Z,MERGED,True,267,185,56,https://github.com/ed-alertedh,Patches for MinGW-w64,13,[],https://github.com/edenhill/librdkafka/pull/2553,https://github.com/edenhill,35,https://github.com/edenhill/librdkafka/pull/2553#issuecomment-630816181,"This set of changes allows me to build librdkafka with a MinGW-w64 cross-compiler and the existing CMake configuration. The main changes were:

Check for __MINGW32__ (MinGW-w64 defines this for both 32-bit and 64-bit targets) in addition to _MSC_VER
Use __thread rather than __declspec(thread) for MinGW.
Change the case of a few headers so that it works on a Linux host with a case-sensitive filesystem.
Define WINVER and _WIN32_WINNT to match the Windows 8.1 target set in the .vcxproj files

All the test cases that are enabled for Windows passed when I ran it. I had some trouble testing that the Visual Studio build still works because the OpenSSL lib you are linking with has moved to VS 2017 in the latest builds and they do not retain old builds for security reasons. It still compiles in VS 2015 but the link fails due to mismatched runtime libs.
If you are interested in merging this I'm very willing to make any necessary revisions. E.g. you might prefer to create a new define rather than repeating defined(_MSC_VER) || defined(__MINGW32__) everywhere. I also have a docker container with the cross-compiler toolchain I can contribute to help you build the tests.
edit:

Defined UNICODE to eliminate bug with mismatched char* and wchar_t*
Added a couple of pointer casts to silence GCC warnings about LONG* vs int32_t* (they are the same size under MinGW-w64, as they are with MSVC)","Sounds good, wanna PR?",True,{'THUMBS_UP': ['https://github.com/neptoess']}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2553,2019-10-07T00:50:26Z,2020-04-29T06:24:56Z,2020-05-20T00:16:36Z,MERGED,True,267,185,56,https://github.com/ed-alertedh,Patches for MinGW-w64,13,[],https://github.com/edenhill/librdkafka/pull/2553,https://github.com/neptoess,36,https://github.com/edenhill/librdkafka/pull/2553#issuecomment-630824603,"This set of changes allows me to build librdkafka with a MinGW-w64 cross-compiler and the existing CMake configuration. The main changes were:

Check for __MINGW32__ (MinGW-w64 defines this for both 32-bit and 64-bit targets) in addition to _MSC_VER
Use __thread rather than __declspec(thread) for MinGW.
Change the case of a few headers so that it works on a Linux host with a case-sensitive filesystem.
Define WINVER and _WIN32_WINNT to match the Windows 8.1 target set in the .vcxproj files

All the test cases that are enabled for Windows passed when I ran it. I had some trouble testing that the Visual Studio build still works because the OpenSSL lib you are linking with has moved to VS 2017 in the latest builds and they do not retain old builds for security reasons. It still compiles in VS 2015 but the link fails due to mismatched runtime libs.
If you are interested in merging this I'm very willing to make any necessary revisions. E.g. you might prefer to create a new define rather than repeating defined(_MSC_VER) || defined(__MINGW32__) everywhere. I also have a docker container with the cross-compiler toolchain I can contribute to help you build the tests.
edit:

Defined UNICODE to eliminate bug with mismatched char* and wchar_t*
Added a couple of pointer casts to silence GCC warnings about LONG* vs int32_t* (they are the same size under MinGW-w64, as they are with MSVC)",#2892,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2553,2019-10-07T00:50:26Z,2020-04-29T06:24:56Z,2020-05-20T00:16:36Z,MERGED,True,267,185,56,https://github.com/ed-alertedh,Patches for MinGW-w64,13,[],https://github.com/edenhill/librdkafka/pull/2553,https://github.com/ed-alertedh,37,https://github.com/edenhill/librdkafka/pull/2553#issuecomment-631159048,"This set of changes allows me to build librdkafka with a MinGW-w64 cross-compiler and the existing CMake configuration. The main changes were:

Check for __MINGW32__ (MinGW-w64 defines this for both 32-bit and 64-bit targets) in addition to _MSC_VER
Use __thread rather than __declspec(thread) for MinGW.
Change the case of a few headers so that it works on a Linux host with a case-sensitive filesystem.
Define WINVER and _WIN32_WINNT to match the Windows 8.1 target set in the .vcxproj files

All the test cases that are enabled for Windows passed when I ran it. I had some trouble testing that the Visual Studio build still works because the OpenSSL lib you are linking with has moved to VS 2017 in the latest builds and they do not retain old builds for security reasons. It still compiles in VS 2015 but the link fails due to mismatched runtime libs.
If you are interested in merging this I'm very willing to make any necessary revisions. E.g. you might prefer to create a new define rather than repeating defined(_MSC_VER) || defined(__MINGW32__) everywhere. I also have a docker container with the cross-compiler toolchain I can contribute to help you build the tests.
edit:

Defined UNICODE to eliminate bug with mismatched char* and wchar_t*
Added a couple of pointer casts to silence GCC warnings about LONG* vs int32_t* (they are the same size under MinGW-w64, as they are with MSVC)","Looks like you resolved it while I was asleep, happy to help in future if needed though!",True,{'THUMBS_UP': ['https://github.com/neptoess']}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2558,2019-10-08T15:39:38Z,2019-10-09T11:02:51Z,2019-10-09T11:02:51Z,CLOSED,False,5,5,1,https://github.com/rnpridgeon,run_test.sh README requires with executable,1,[],https://github.com/edenhill/librdkafka/pull/2558,https://github.com/rnpridgeon,1,https://github.com/edenhill/librdkafka/pull/2558,"run_test.sh requires the executable. Alternatively it can be updated to assume ""merged"" from the current working directory.","run_test.sh requires the executable. Alternatively it can be updated to assume ""merged"" from the current working directory.",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2559,2019-10-09T13:21:00Z,2019-10-11T10:35:28Z,2019-10-11T10:35:35Z,MERGED,True,583,66,12,https://github.com/edenhill,Reorganized examples and added cleaner consumer and producer examples,2,[],https://github.com/edenhill/librdkafka/pull/2559,https://github.com/edenhill,1,https://github.com/edenhill/librdkafka/pull/2559,Also removed the zookeeper example since it is of no use anymore.,Also removed the zookeeper example since it is of no use anymore.,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2560,2019-10-09T13:57:14Z,2019-10-11T06:47:47Z,2019-10-11T06:47:49Z,MERGED,True,2,2,2,https://github.com/edenhill,Fix warnings from newer GCC,1,[],https://github.com/edenhill/librdkafka/pull/2560,https://github.com/edenhill,1,https://github.com/edenhill/librdkafka/pull/2560,,,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2562,2019-10-10T09:02:17Z,2019-10-11T06:46:57Z,2019-10-11T06:46:59Z,MERGED,True,4,4,2,https://github.com/edenhill,Fix assert when using LZ4 with pre-ApiVersionRequest brokers,2,[],https://github.com/edenhill/librdkafka/pull/2562,https://github.com/edenhill,1,https://github.com/edenhill/librdkafka/pull/2562,..and an unrelated example fix.,..and an unrelated example fix.,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2569,2019-10-12T15:10:17Z,2019-10-16T12:56:48Z,2019-11-21T13:04:09Z,MERGED,True,12,0,1,https://github.com/maparent,do not call timespec_get on mac,1,[],https://github.com/edenhill/librdkafka/pull/2569,https://github.com/maparent,1,https://github.com/edenhill/librdkafka/pull/2569,"rdtime.h avoids use of timespec on macos in general, but it was introduced in #7ee0fdcfd . With the latest 10.15 SDK, timespec_get is gone, and the library breaks.
This is a very simple correction, there may be a better way.
dyld: lazy symbol binding failed: Symbol not found: _timespec_get
...
Expected in: /usr/lib/libSystem.B.dylib```","rdtime.h avoids use of timespec on macos in general, but it was introduced in #7ee0fdcfd . With the latest 10.15 SDK, timespec_get is gone, and the library breaks.
This is a very simple correction, there may be a better way.
dyld: lazy symbol binding failed: Symbol not found: _timespec_get
...
Expected in: /usr/lib/libSystem.B.dylib```",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2569,2019-10-12T15:10:17Z,2019-10-16T12:56:48Z,2019-11-21T13:04:09Z,MERGED,True,12,0,1,https://github.com/maparent,do not call timespec_get on mac,1,[],https://github.com/edenhill/librdkafka/pull/2569,https://github.com/Aashna-Agrawal,2,https://github.com/edenhill/librdkafka/pull/2569#issuecomment-541388090,"rdtime.h avoids use of timespec on macos in general, but it was introduced in #7ee0fdcfd . With the latest 10.15 SDK, timespec_get is gone, and the library breaks.
This is a very simple correction, there may be a better way.
dyld: lazy symbol binding failed: Symbol not found: _timespec_get
...
Expected in: /usr/lib/libSystem.B.dylib```","Will it break with macOs 10.14.5 too? Also, could you provide me with the reference to  #7ee0fdcfd to see the change?",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2569,2019-10-12T15:10:17Z,2019-10-16T12:56:48Z,2019-11-21T13:04:09Z,MERGED,True,12,0,1,https://github.com/maparent,do not call timespec_get on mac,1,[],https://github.com/edenhill/librdkafka/pull/2569,https://github.com/maparent,3,https://github.com/edenhill/librdkafka/pull/2569#issuecomment-541390007,"rdtime.h avoids use of timespec on macos in general, but it was introduced in #7ee0fdcfd . With the latest 10.15 SDK, timespec_get is gone, and the library breaks.
This is a very simple correction, there may be a better way.
dyld: lazy symbol binding failed: Symbol not found: _timespec_get
...
Expected in: /usr/lib/libSystem.B.dylib```","It breaks in 10.14.5 if you install the latest XCode, which comes with 10.15 SDK.
I'm referring to this line which was introduced in v0.11.6. (note: this line was introduced later along the same pattern.)",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2569,2019-10-12T15:10:17Z,2019-10-16T12:56:48Z,2019-11-21T13:04:09Z,MERGED,True,12,0,1,https://github.com/maparent,do not call timespec_get on mac,1,[],https://github.com/edenhill/librdkafka/pull/2569,https://github.com/Aashna-Agrawal,4,https://github.com/edenhill/librdkafka/pull/2569#issuecomment-541414773,"rdtime.h avoids use of timespec on macos in general, but it was introduced in #7ee0fdcfd . With the latest 10.15 SDK, timespec_get is gone, and the library breaks.
This is a very simple correction, there may be a better way.
dyld: lazy symbol binding failed: Symbol not found: _timespec_get
...
Expected in: /usr/lib/libSystem.B.dylib```","I checked the versions of XCode on my system
On running the following commands, I got the responses as mentioned below:
$ which g++
/usr/bin/g++
$ g++ --version
Configured with: --prefix=/Library/Developer/CommandLineTools/usr --with-gxx-include-dir=/Library/Developer/CommandLineTools/SDKs/MacOSX10.14.sdk/usr/include/c++/4.2.1
Apple LLVM version 10.0.1 (clang-1001.0.46.4)
Target: x86_64-apple-darwin18.6.0
Thread model: posix
InstalledDir: /Library/Developer/CommandLineTools/usr/bin
$ xcode-select -v
xcode-select version 2354
$ xcode-select --install
xcode-select: error: command line tools are already installed, use ""Software Update"" to install updates
I am guessing here that the XCode is installed for the 10.14 SDK, but the error is seen as mentioned in  #2570",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2569,2019-10-12T15:10:17Z,2019-10-16T12:56:48Z,2019-11-21T13:04:09Z,MERGED,True,12,0,1,https://github.com/maparent,do not call timespec_get on mac,1,[],https://github.com/edenhill/librdkafka/pull/2569,https://github.com/maparent,5,https://github.com/edenhill/librdkafka/pull/2569#issuecomment-541415119,"rdtime.h avoids use of timespec on macos in general, but it was introduced in #7ee0fdcfd . With the latest 10.15 SDK, timespec_get is gone, and the library breaks.
This is a very simple correction, there may be a better way.
dyld: lazy symbol binding failed: Symbol not found: _timespec_get
...
Expected in: /usr/lib/libSystem.B.dylib```","Interesting. I have clang-1100.0.33.8 with XCode 11.1. Not sure that the version of xcode-select is relevant. So maybe the loss of librdkafka is older than the latest XCode, I had not been using it in some time. I saw it break while using kafkacat in homebrew. I also based my timeline on Blizzard/node-rdkafka#686",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2569,2019-10-12T15:10:17Z,2019-10-16T12:56:48Z,2019-11-21T13:04:09Z,MERGED,True,12,0,1,https://github.com/maparent,do not call timespec_get on mac,1,[],https://github.com/edenhill/librdkafka/pull/2569,https://github.com/Aashna-Agrawal,6,https://github.com/edenhill/librdkafka/pull/2569#issuecomment-542165322,"rdtime.h avoids use of timespec on macos in general, but it was introduced in #7ee0fdcfd . With the latest 10.15 SDK, timespec_get is gone, and the library breaks.
This is a very simple correction, there may be a better way.
dyld: lazy symbol binding failed: Symbol not found: _timespec_get
...
Expected in: /usr/lib/libSystem.B.dylib```",Changed the version of Xcode to XCode10. The issue still persists.,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2569,2019-10-12T15:10:17Z,2019-10-16T12:56:48Z,2019-11-21T13:04:09Z,MERGED,True,12,0,1,https://github.com/maparent,do not call timespec_get on mac,1,[],https://github.com/edenhill/librdkafka/pull/2569,https://github.com/maparent,7,https://github.com/edenhill/librdkafka/pull/2569#issuecomment-542189953,"rdtime.h avoids use of timespec on macos in general, but it was introduced in #7ee0fdcfd . With the latest 10.15 SDK, timespec_get is gone, and the library breaks.
This is a very simple correction, there may be a better way.
dyld: lazy symbol binding failed: Symbol not found: _timespec_get
...
Expected in: /usr/lib/libSystem.B.dylib```",And did you try using my branch? (a usual configure/make/make install should do it. uninstall from homebrew first if needed.),True,{'THUMBS_UP': ['https://github.com/Aashna-Agrawal']}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2569,2019-10-12T15:10:17Z,2019-10-16T12:56:48Z,2019-11-21T13:04:09Z,MERGED,True,12,0,1,https://github.com/maparent,do not call timespec_get on mac,1,[],https://github.com/edenhill/librdkafka/pull/2569,https://github.com/edenhill,8,https://github.com/edenhill/librdkafka/pull/2569#issuecomment-542686735,"rdtime.h avoids use of timespec on macos in general, but it was introduced in #7ee0fdcfd . With the latest 10.15 SDK, timespec_get is gone, and the library breaks.
This is a very simple correction, there may be a better way.
dyld: lazy symbol binding failed: Symbol not found: _timespec_get
...
Expected in: /usr/lib/libSystem.B.dylib```",Thank you!,True,{'HEART': ['https://github.com/maparent']}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2569,2019-10-12T15:10:17Z,2019-10-16T12:56:48Z,2019-11-21T13:04:09Z,MERGED,True,12,0,1,https://github.com/maparent,do not call timespec_get on mac,1,[],https://github.com/edenhill/librdkafka/pull/2569,https://github.com/Aashna-Agrawal,9,https://github.com/edenhill/librdkafka/pull/2569#issuecomment-543052203,"rdtime.h avoids use of timespec on macos in general, but it was introduced in #7ee0fdcfd . With the latest 10.15 SDK, timespec_get is gone, and the library breaks.
This is a very simple correction, there may be a better way.
dyld: lazy symbol binding failed: Symbol not found: _timespec_get
...
Expected in: /usr/lib/libSystem.B.dylib```","And did you try using my branch? (a usual configure/make/make install should do it. uninstall from homebrew first if needed.)

@maparent Yes, I did. It worked. Thanks!",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2569,2019-10-12T15:10:17Z,2019-10-16T12:56:48Z,2019-11-21T13:04:09Z,MERGED,True,12,0,1,https://github.com/maparent,do not call timespec_get on mac,1,[],https://github.com/edenhill/librdkafka/pull/2569,https://github.com/sweco-seponr,10,https://github.com/edenhill/librdkafka/pull/2569#issuecomment-543333972,"rdtime.h avoids use of timespec on macos in general, but it was introduced in #7ee0fdcfd . With the latest 10.15 SDK, timespec_get is gone, and the library breaks.
This is a very simple correction, there may be a better way.
dyld: lazy symbol binding failed: Symbol not found: _timespec_get
...
Expected in: /usr/lib/libSystem.B.dylib```",Any ideas on when to expect a homebrew upgrade will installed this bugfixed master?,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2569,2019-10-12T15:10:17Z,2019-10-16T12:56:48Z,2019-11-21T13:04:09Z,MERGED,True,12,0,1,https://github.com/maparent,do not call timespec_get on mac,1,[],https://github.com/edenhill/librdkafka/pull/2569,https://github.com/maparent,11,https://github.com/edenhill/librdkafka/pull/2569#issuecomment-543334716,"rdtime.h avoids use of timespec on macos in general, but it was introduced in #7ee0fdcfd . With the latest 10.15 SDK, timespec_get is gone, and the library breaks.
This is a very simple correction, there may be a better way.
dyld: lazy symbol binding failed: Symbol not found: _timespec_get
...
Expected in: /usr/lib/libSystem.B.dylib```","Not for me to say, but meanwhile you can brew install librdkafka --HEAD.
Edit: Note that it's unlikely brew will update until a new point release is done.",True,"{'THUMBS_UP': ['https://github.com/joosterman', 'https://github.com/chintanparikh', 'https://github.com/DaveWM', 'https://github.com/jaisonpjohn', 'https://github.com/brifordwylie', 'https://github.com/justinbellamy', 'https://github.com/BenEddy', 'https://github.com/statwonk', 'https://github.com/Ditofry', 'https://github.com/cowell21', 'https://github.com/andrewferrier', 'https://github.com/759803573', 'https://github.com/x3ro', 'https://github.com/zeikar', 'https://github.com/mgunter-pivotal', 'https://github.com/meetsitaram']}"
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2569,2019-10-12T15:10:17Z,2019-10-16T12:56:48Z,2019-11-21T13:04:09Z,MERGED,True,12,0,1,https://github.com/maparent,do not call timespec_get on mac,1,[],https://github.com/edenhill/librdkafka/pull/2569,https://github.com/edenhill,12,https://github.com/edenhill/librdkafka/pull/2569#issuecomment-543335249,"rdtime.h avoids use of timespec on macos in general, but it was introduced in #7ee0fdcfd . With the latest 10.15 SDK, timespec_get is gone, and the library breaks.
This is a very simple correction, there may be a better way.
dyld: lazy symbol binding failed: Symbol not found: _timespec_get
...
Expected in: /usr/lib/libSystem.B.dylib```","The brew package is either updated automatically or by community members, I haven't touched it in a long time.",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2569,2019-10-12T15:10:17Z,2019-10-16T12:56:48Z,2019-11-21T13:04:09Z,MERGED,True,12,0,1,https://github.com/maparent,do not call timespec_get on mac,1,[],https://github.com/edenhill/librdkafka/pull/2569,https://github.com/mgunter-pivotal,13,https://github.com/edenhill/librdkafka/pull/2569#issuecomment-556255070,"rdtime.h avoids use of timespec on macos in general, but it was introduced in #7ee0fdcfd . With the latest 10.15 SDK, timespec_get is gone, and the library breaks.
This is a very simple correction, there may be a better way.
dyld: lazy symbol binding failed: Symbol not found: _timespec_get
...
Expected in: /usr/lib/libSystem.B.dylib```","brew uninstall kafkacat
brew uninstall librdkafka
brew install librdkafka --HEAD
brew install kafkacat
#...and test to confirm it works... :-D
$kafkacat -b localhost:9094 -L
Metadata for all topics (from broker 0: localhost:9094/0):
1 brokers:
broker 0 at localhost:9094 (controller)
0 topics:",True,"{'THUMBS_UP': ['https://github.com/meetsitaram', 'https://github.com/godber', 'https://github.com/edenhill', 'https://github.com/sixleaves']}"
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2569,2019-10-12T15:10:17Z,2019-10-16T12:56:48Z,2019-11-21T13:04:09Z,MERGED,True,12,0,1,https://github.com/maparent,do not call timespec_get on mac,1,[],https://github.com/edenhill/librdkafka/pull/2569,https://github.com/maparent,14,https://github.com/edenhill/librdkafka/pull/2569#issuecomment-557075469,"rdtime.h avoids use of timespec on macos in general, but it was introduced in #7ee0fdcfd . With the latest 10.15 SDK, timespec_get is gone, and the library breaks.
This is a very simple correction, there may be a better way.
dyld: lazy symbol binding failed: Symbol not found: _timespec_get
...
Expected in: /usr/lib/libSystem.B.dylib```",New version with fix now in homebrew,True,{'THUMBS_UP': ['https://github.com/mgunter-pivotal']}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2572,2019-10-14T13:38:49Z,2019-10-16T12:56:05Z,2019-10-16T12:56:07Z,MERGED,True,490,43,14,https://github.com/edenhill,Monitor resource usage in tests,4,[],https://github.com/edenhill/librdkafka/pull/2572,https://github.com/edenhill,1,https://github.com/edenhill/librdkafka/pull/2572,"make rusage in tests/ will run the test suite with resource usage threshold checks, this surfaced a busy loop in the producer (test 0076)","make rusage in tests/ will run the test suite with resource usage threshold checks, this surfaced a busy loop in the producer (test 0076)",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2573,2019-10-15T09:54:23Z,2019-10-17T10:59:41Z,2019-10-17T10:59:43Z,MERGED,True,228,32,7,https://github.com/edenhill,Fix producer insert msgq regression in v1.2.1 (#2450),3,['bug'],https://github.com/edenhill/librdkafka/pull/2573,https://github.com/edenhill,1,https://github.com/edenhill/librdkafka/pull/2573,"The common case where the source queue is simply appended
to the destination queue was not optimized, causing O(n) scans
to find the insert position.
Issue introduced in v1.2.1","The common case where the source queue is simply appended
to the destination queue was not optimized, causing O(n) scans
to find the insert position.
Issue introduced in v1.2.1",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2573,2019-10-15T09:54:23Z,2019-10-17T10:59:41Z,2019-10-17T10:59:43Z,MERGED,True,228,32,7,https://github.com/edenhill,Fix producer insert msgq regression in v1.2.1 (#2450),3,['bug'],https://github.com/edenhill/librdkafka/pull/2573,https://github.com/rnpridgeon,2,https://github.com/edenhill/librdkafka/pull/2573#issuecomment-542733043,"The common case where the source queue is simply appended
to the destination queue was not optimized, causing O(n) scans
to find the insert position.
Issue introduced in v1.2.1","How confident are we that the us_per_msg max value is sufficiently tolerant of to avoid frequent test flakiness?
RDUT: FAIL: rdkafka_msg.c:1942: unittest_msgq_insert_each_sort: assert failed: !(us_per_msg > max_us_per_msg + 0.0001): maximum us/msg exceeded: 0.7500 > 0.3000 us/msg
Other than that LGTM",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2573,2019-10-15T09:54:23Z,2019-10-17T10:59:41Z,2019-10-17T10:59:43Z,MERGED,True,228,32,7,https://github.com/edenhill,Fix producer insert msgq regression in v1.2.1 (#2450),3,['bug'],https://github.com/edenhill/librdkafka/pull/2573,https://github.com/edenhill,3,https://github.com/edenhill/librdkafka/pull/2573#issuecomment-542734055,"The common case where the source queue is simply appended
to the destination queue was not optimized, causing O(n) scans
to find the insert position.
Issue introduced in v1.2.1","How confident are we that the us_per_msg max value is sufficiently tolerant of to avoid frequent test flakiness?

Since that unittest runs an isolated part of the code base it is not subject to external factors (such as networks), other than the system load.
Under virtualization, such as this CI failure, all bets are off though and this should be a warning, not a failure.",True,{'THUMBS_UP': ['https://github.com/rnpridgeon']}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2575,2019-10-16T07:07:49Z,2019-10-16T10:15:55Z,2019-10-16T10:15:58Z,MERGED,True,2,0,1,https://github.com/edenhill,configure: add --runstatedir for compatibility with autoconf,1,[],https://github.com/edenhill/librdkafka/pull/2575,https://github.com/edenhill,1,https://github.com/edenhill/librdkafka/pull/2575,"Needed by Debian packaging
Fixes this:
https://www.mail-archive.com/debian-devel-changes@lists.debian.org/msg620872.html","Needed by Debian packaging
Fixes this:
https://www.mail-archive.com/debian-devel-changes@lists.debian.org/msg620872.html",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2576,2019-10-17T07:22:30Z,2019-10-17T14:50:45Z,2019-10-17T14:50:48Z,MERGED,True,20,0,1,https://github.com/edenhill,Add warnings for inconsistent security configuration,1,[],https://github.com/edenhill/librdkafka/pull/2576,https://github.com/edenhill,1,https://github.com/edenhill/librdkafka/pull/2576,"This would've helped here:
confluentinc/confluent-kafka-dotnet#1068","This would've helped here:
confluentinc/confluent-kafka-dotnet#1068",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2577,2019-10-21T07:24:44Z,,2020-01-23T02:01:35Z,OPEN,False,578,26,9,https://github.com/mezzi,Proposal for broker rack metadata using private type,1,[],https://github.com/edenhill/librdkafka/pull/2577,https://github.com/mezzi,1,https://github.com/edenhill/librdkafka/pull/2577,"This proposed change intends to expose the rack field on broker metadata without breaking backwards ABI compatibility, by relying on private types and accessor methods.
The idea is that we can hang additional fields (i.e. rack) off of the char *host pointer on the public struct rd_kafka_metadata_broker type, as long as the host string is the first thing found at the memory location it points to. I'm introducing a new private type for extended broker metadata  (struct rd_kafka_metadata_broker_extended), which instead of the host field, contains a pointer to an extension struct.
typedef struct rd_kafka_metadata_broker_extended_s {
        int32_t                         id;     /**< Broker Id */
        rd_kafka_metadata_broker_ext_t  *ext;   /**< Pointer to extended broker information */
        int                             port;   /**< Broker listening port */
} rd_kafka_metadata_broker_extended_t;

typedef struct rd_kafka_metadata_broker_ext_s {
        char       host[256];    /**< Broker hostname */
        const char *rack;                          /**< Broker rack */
} rd_kafka_metadata_broker_ext_t;

When a metadata response is parsed, the broker metadata will be unmarshalled into an array of  rd_kafka_metadata_broker_extended_t structs, however the returned broker metadata will still appear to the application as struct rd_kafka_metadata_broker. Since the host and ext pointers are of the same size and the host is a char array directly within the extension struct, applications compiled against previous library versions will not know the difference and can simply read the host string from the memory allocated to ext and pointed to by host as before.
The tradeoff here is that we must impose a limit on the host name length. However, according to the research I've done, host names on Unix systems are limited to 253 chars (see: http://man7.org/linux/man-pages/man7/hostname.7.html), while the POSIX standard guarantees it not to exceed 255 chars (http://man7.org/linux/man-pages/man3/sysconf.3.html). Of course, this also means that we must allocated 256 bytes for each broker host name, regardless of the actual length. I believe this to be a small tradeoff, however there may be ways to optimize.
In order to access the extended broker metadata fields (i.e. rack), the application must obtain a handle to an opaque rd_kafka_metadata_broker_extended_s value by calling rd_kafka_metadata_broker_get(metadata, i), where metadata is the what's obtained from a metadata request and i is the index of the broker within the broker array. The following accessor methods may then be used to access the id, host, port and rack fields of the underlying value:
int32_t rd_kafka_metadata_broker_id (const rd_kafka_metadata_broker_extended_t *mdb);
const char *rd_kafka_metadata_broker_host (const rd_kafka_metadata_broker_extended_t *mdb);
int rd_kafka_metadata_broker_port (const rd_kafka_metadata_broker_extended_t *mdb);
const char *rd_kafka_metadata_broker_rack (const rd_kafka_metadata_broker_extended_t *mdb);

Please take a look and let me know if this seems like a sensible approach. I've included an integration test to test its validity. There is still some work to be done on the C++ implementation, but I wanted your feedback before continuing down this path. Looking forward to hearing your thoughts.","This proposed change intends to expose the rack field on broker metadata without breaking backwards ABI compatibility, by relying on private types and accessor methods.
The idea is that we can hang additional fields (i.e. rack) off of the char *host pointer on the public struct rd_kafka_metadata_broker type, as long as the host string is the first thing found at the memory location it points to. I'm introducing a new private type for extended broker metadata  (struct rd_kafka_metadata_broker_extended), which instead of the host field, contains a pointer to an extension struct.
typedef struct rd_kafka_metadata_broker_extended_s {
        int32_t                         id;     /**< Broker Id */
        rd_kafka_metadata_broker_ext_t  *ext;   /**< Pointer to extended broker information */
        int                             port;   /**< Broker listening port */
} rd_kafka_metadata_broker_extended_t;

typedef struct rd_kafka_metadata_broker_ext_s {
        char       host[256];    /**< Broker hostname */
        const char *rack;                          /**< Broker rack */
} rd_kafka_metadata_broker_ext_t;

When a metadata response is parsed, the broker metadata will be unmarshalled into an array of  rd_kafka_metadata_broker_extended_t structs, however the returned broker metadata will still appear to the application as struct rd_kafka_metadata_broker. Since the host and ext pointers are of the same size and the host is a char array directly within the extension struct, applications compiled against previous library versions will not know the difference and can simply read the host string from the memory allocated to ext and pointed to by host as before.
The tradeoff here is that we must impose a limit on the host name length. However, according to the research I've done, host names on Unix systems are limited to 253 chars (see: http://man7.org/linux/man-pages/man7/hostname.7.html), while the POSIX standard guarantees it not to exceed 255 chars (http://man7.org/linux/man-pages/man3/sysconf.3.html). Of course, this also means that we must allocated 256 bytes for each broker host name, regardless of the actual length. I believe this to be a small tradeoff, however there may be ways to optimize.
In order to access the extended broker metadata fields (i.e. rack), the application must obtain a handle to an opaque rd_kafka_metadata_broker_extended_s value by calling rd_kafka_metadata_broker_get(metadata, i), where metadata is the what's obtained from a metadata request and i is the index of the broker within the broker array. The following accessor methods may then be used to access the id, host, port and rack fields of the underlying value:
int32_t rd_kafka_metadata_broker_id (const rd_kafka_metadata_broker_extended_t *mdb);
const char *rd_kafka_metadata_broker_host (const rd_kafka_metadata_broker_extended_t *mdb);
int rd_kafka_metadata_broker_port (const rd_kafka_metadata_broker_extended_t *mdb);
const char *rd_kafka_metadata_broker_rack (const rd_kafka_metadata_broker_extended_t *mdb);

Please take a look and let me know if this seems like a sensible approach. I've included an integration test to test its validity. There is still some work to be done on the C++ implementation, but I wanted your feedback before continuing down this path. Looking forward to hearing your thoughts.",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2582,2019-10-22T19:18:45Z,2019-10-22T19:30:39Z,2019-10-23T03:58:52Z,CLOSED,False,1,1,1,https://github.com/virtual41tridiv,ADDED MY NAME,1,[],https://github.com/edenhill/librdkafka/pull/2582,https://github.com/virtual41tridiv,1,https://github.com/edenhill/librdkafka/pull/2582,,,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2583,2019-10-23T14:10:06Z,2019-10-24T12:29:11Z,2019-10-24T12:29:16Z,MERGED,True,6,3,1,https://github.com/edenhill,Fix strlcpy implementation,1,[],https://github.com/edenhill/librdkafka/pull/2583,https://github.com/edenhill,1,https://github.com/edenhill/librdkafka/pull/2583,,,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2585,2019-10-23T14:31:18Z,2020-02-05T08:57:54Z,2020-10-21T11:21:42Z,MERGED,True,3,3,1,https://github.com/mortymacs,Fix test rd_snprintf warnings #2584,3,[],https://github.com/edenhill/librdkafka/pull/2585,https://github.com/mortymacs,1,https://github.com/edenhill/librdkafka/pull/2585,Issue ID: #2584,Issue ID: #2584,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2585,2019-10-23T14:31:18Z,2020-02-05T08:57:54Z,2020-10-21T11:21:42Z,MERGED,True,3,3,1,https://github.com/mortymacs,Fix test rd_snprintf warnings #2584,3,[],https://github.com/edenhill/librdkafka/pull/2585,https://github.com/edenhill,2,https://github.com/edenhill/librdkafka/pull/2585#issuecomment-582305896,Issue ID: #2584,Thanks!,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2600,2019-11-01T10:36:21Z,2019-11-05T08:26:17Z,2019-11-05T08:26:20Z,MERGED,True,5240,2649,11,https://github.com/edenhill,Bump lz4 1.9.2 for CVE-2019-17543,1,[],https://github.com/edenhill/librdkafka/pull/2600,https://github.com/edenhill,1,https://github.com/edenhill/librdkafka/pull/2600,Fixes #2598,Fixes #2598,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2600,2019-11-01T10:36:21Z,2019-11-05T08:26:17Z,2019-11-05T08:26:20Z,MERGED,True,5240,2649,11,https://github.com/edenhill,Bump lz4 1.9.2 for CVE-2019-17543,1,[],https://github.com/edenhill/librdkafka/pull/2600,https://github.com/edenhill,2,https://github.com/edenhill/librdkafka/pull/2600#issuecomment-548745345,Fixes #2598,There is no need to review the lz4 code itself since it is a direct source copy.,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2602,2019-11-02T03:35:14Z,2021-03-23T06:14:16Z,2021-03-23T06:14:16Z,CLOSED,False,32358,5449,230,https://github.com/adinigam,Adding OpenSSL engine support in LibrdKafka for SSL connections,378,[],https://github.com/edenhill/librdkafka/pull/2602,https://github.com/adinigam,1,https://github.com/edenhill/librdkafka/pull/2602,"Currently LibrdKafka uses Openssl behind the scenes to establish SSL channel between broker and client.
To create the SSL channel, OpenSSL needs public and private keys. Today these keys are provided in byte* format to openssl. Byte* can either be generated by librdkafka or application itself. But ultimately to generate the byte* format, keys needs to be exported from where-ever it is residing.
In many scenarios exporting might not be an option since it might not be permitted by hardware/software, eg: if application is using non-exportable certificates in windows certificate store.
Good news is OpenSSL provides plugin model which is called engine. This engine can be used to interact with the storage and use the public and private keys for the certificate without exporting in byte* format. Such clients will have to write custom engines.
I am proposing to enhance LibrdKafka to support OpenSSL engine. Below are the changes I am taking up in this PR.


Add new parameter ""openssl.engine.location"" which will take the full path to the binary containing the openssl engine


This engine will be initialized if the parameter ""openssl.engine.location"" value is not null. Below is the initialization code in the method rd_kafka_ssl_ctx_init


Call engine method ENGINE_load_ssl_client_cert with appropriate params to fetch the pointers to the public and private keys. Key parameter is building and disposing the stack of ca-certs in rd_kafka_ssl_set_certs.


Update: I can see some old threads suggesting the usage of engine especially for non-exportable certificates. Great if someone knows and let me know why that idea was not taken forward?","Currently LibrdKafka uses Openssl behind the scenes to establish SSL channel between broker and client.
To create the SSL channel, OpenSSL needs public and private keys. Today these keys are provided in byte* format to openssl. Byte* can either be generated by librdkafka or application itself. But ultimately to generate the byte* format, keys needs to be exported from where-ever it is residing.
In many scenarios exporting might not be an option since it might not be permitted by hardware/software, eg: if application is using non-exportable certificates in windows certificate store.
Good news is OpenSSL provides plugin model which is called engine. This engine can be used to interact with the storage and use the public and private keys for the certificate without exporting in byte* format. Such clients will have to write custom engines.
I am proposing to enhance LibrdKafka to support OpenSSL engine. Below are the changes I am taking up in this PR.


Add new parameter ""openssl.engine.location"" which will take the full path to the binary containing the openssl engine


This engine will be initialized if the parameter ""openssl.engine.location"" value is not null. Below is the initialization code in the method rd_kafka_ssl_ctx_init


Call engine method ENGINE_load_ssl_client_cert with appropriate params to fetch the pointers to the public and private keys. Key parameter is building and disposing the stack of ca-certs in rd_kafka_ssl_set_certs.


Update: I can see some old threads suggesting the usage of engine especially for non-exportable certificates. Great if someone knows and let me know why that idea was not taken forward?",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2602,2019-11-02T03:35:14Z,2021-03-23T06:14:16Z,2021-03-23T06:14:16Z,CLOSED,False,32358,5449,230,https://github.com/adinigam,Adding OpenSSL engine support in LibrdKafka for SSL connections,378,[],https://github.com/edenhill/librdkafka/pull/2602,https://github.com/edenhill,2,https://github.com/edenhill/librdkafka/pull/2602#issuecomment-549689804,"Currently LibrdKafka uses Openssl behind the scenes to establish SSL channel between broker and client.
To create the SSL channel, OpenSSL needs public and private keys. Today these keys are provided in byte* format to openssl. Byte* can either be generated by librdkafka or application itself. But ultimately to generate the byte* format, keys needs to be exported from where-ever it is residing.
In many scenarios exporting might not be an option since it might not be permitted by hardware/software, eg: if application is using non-exportable certificates in windows certificate store.
Good news is OpenSSL provides plugin model which is called engine. This engine can be used to interact with the storage and use the public and private keys for the certificate without exporting in byte* format. Such clients will have to write custom engines.
I am proposing to enhance LibrdKafka to support OpenSSL engine. Below are the changes I am taking up in this PR.


Add new parameter ""openssl.engine.location"" which will take the full path to the binary containing the openssl engine


This engine will be initialized if the parameter ""openssl.engine.location"" value is not null. Below is the initialization code in the method rd_kafka_ssl_ctx_init


Call engine method ENGINE_load_ssl_client_cert with appropriate params to fetch the pointers to the public and private keys. Key parameter is building and disposing the stack of ca-certs in rd_kafka_ssl_set_certs.


Update: I can see some old threads suggesting the usage of engine especially for non-exportable certificates. Great if someone knows and let me know why that idea was not taken forward?","Thank you for this contribution!
The freature window for Q4 is closed so this will go in a Q1 release the earliest.
I'll start reviewing this in about a week or two.
In the meantime, make sure CIs pass (and does not have build warnings) and that the coding style is maintained.
Thank you",True,{'THUMBS_UP': ['https://github.com/adinigam']}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2602,2019-11-02T03:35:14Z,2021-03-23T06:14:16Z,2021-03-23T06:14:16Z,CLOSED,False,32358,5449,230,https://github.com/adinigam,Adding OpenSSL engine support in LibrdKafka for SSL connections,378,[],https://github.com/edenhill/librdkafka/pull/2602,https://github.com/adinigam,3,https://github.com/edenhill/librdkafka/pull/2602#issuecomment-564216549,"Currently LibrdKafka uses Openssl behind the scenes to establish SSL channel between broker and client.
To create the SSL channel, OpenSSL needs public and private keys. Today these keys are provided in byte* format to openssl. Byte* can either be generated by librdkafka or application itself. But ultimately to generate the byte* format, keys needs to be exported from where-ever it is residing.
In many scenarios exporting might not be an option since it might not be permitted by hardware/software, eg: if application is using non-exportable certificates in windows certificate store.
Good news is OpenSSL provides plugin model which is called engine. This engine can be used to interact with the storage and use the public and private keys for the certificate without exporting in byte* format. Such clients will have to write custom engines.
I am proposing to enhance LibrdKafka to support OpenSSL engine. Below are the changes I am taking up in this PR.


Add new parameter ""openssl.engine.location"" which will take the full path to the binary containing the openssl engine


This engine will be initialized if the parameter ""openssl.engine.location"" value is not null. Below is the initialization code in the method rd_kafka_ssl_ctx_init


Call engine method ENGINE_load_ssl_client_cert with appropriate params to fetch the pointers to the public and private keys. Key parameter is building and disposing the stack of ca-certs in rd_kafka_ssl_set_certs.


Update: I can see some old threads suggesting the usage of engine especially for non-exportable certificates. Great if someone knows and let me know why that idea was not taken forward?",@edenhill Any idea when you would start reviewing this ?,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2602,2019-11-02T03:35:14Z,2021-03-23T06:14:16Z,2021-03-23T06:14:16Z,CLOSED,False,32358,5449,230,https://github.com/adinigam,Adding OpenSSL engine support in LibrdKafka for SSL connections,378,[],https://github.com/edenhill/librdkafka/pull/2602,https://github.com/edenhill,4,https://github.com/edenhill/librdkafka/pull/2602#issuecomment-564262490,"Currently LibrdKafka uses Openssl behind the scenes to establish SSL channel between broker and client.
To create the SSL channel, OpenSSL needs public and private keys. Today these keys are provided in byte* format to openssl. Byte* can either be generated by librdkafka or application itself. But ultimately to generate the byte* format, keys needs to be exported from where-ever it is residing.
In many scenarios exporting might not be an option since it might not be permitted by hardware/software, eg: if application is using non-exportable certificates in windows certificate store.
Good news is OpenSSL provides plugin model which is called engine. This engine can be used to interact with the storage and use the public and private keys for the certificate without exporting in byte* format. Such clients will have to write custom engines.
I am proposing to enhance LibrdKafka to support OpenSSL engine. Below are the changes I am taking up in this PR.


Add new parameter ""openssl.engine.location"" which will take the full path to the binary containing the openssl engine


This engine will be initialized if the parameter ""openssl.engine.location"" value is not null. Below is the initialization code in the method rd_kafka_ssl_ctx_init


Call engine method ENGINE_load_ssl_client_cert with appropriate params to fetch the pointers to the public and private keys. Key parameter is building and disposing the stack of ca-certs in rd_kafka_ssl_set_certs.


Update: I can see some old threads suggesting the usage of engine especially for non-exportable certificates. Great if someone knows and let me know why that idea was not taken forward?","The current release plan is to get static group membership and transactions into the next release, which is planned for early Q1, and then start looking at this PR.
Thanks for your patience.",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2602,2019-11-02T03:35:14Z,2021-03-23T06:14:16Z,2021-03-23T06:14:16Z,CLOSED,False,32358,5449,230,https://github.com/adinigam,Adding OpenSSL engine support in LibrdKafka for SSL connections,378,[],https://github.com/edenhill/librdkafka/pull/2602,https://github.com/LighthouseJ,5,https://github.com/edenhill/librdkafka/pull/2602#issuecomment-569735327,"Currently LibrdKafka uses Openssl behind the scenes to establish SSL channel between broker and client.
To create the SSL channel, OpenSSL needs public and private keys. Today these keys are provided in byte* format to openssl. Byte* can either be generated by librdkafka or application itself. But ultimately to generate the byte* format, keys needs to be exported from where-ever it is residing.
In many scenarios exporting might not be an option since it might not be permitted by hardware/software, eg: if application is using non-exportable certificates in windows certificate store.
Good news is OpenSSL provides plugin model which is called engine. This engine can be used to interact with the storage and use the public and private keys for the certificate without exporting in byte* format. Such clients will have to write custom engines.
I am proposing to enhance LibrdKafka to support OpenSSL engine. Below are the changes I am taking up in this PR.


Add new parameter ""openssl.engine.location"" which will take the full path to the binary containing the openssl engine


This engine will be initialized if the parameter ""openssl.engine.location"" value is not null. Below is the initialization code in the method rd_kafka_ssl_ctx_init


Call engine method ENGINE_load_ssl_client_cert with appropriate params to fetch the pointers to the public and private keys. Key parameter is building and disposing the stack of ca-certs in rd_kafka_ssl_set_certs.


Update: I can see some old threads suggesting the usage of engine especially for non-exportable certificates. Great if someone knows and let me know why that idea was not taken forward?","Hello @adinigam I really like this implementation.  I've been tracking an earlier bit of work by @noahdav which allowed a user to provide PEM certs to librdkafka.  I (and he later) needed something closer to this, which uses the same dynamic engine but we needed CAPI.
I provided my modifications to his first cut at the effort, which did similar things but in a more minimal implementation to yours.  I outlined them here: #2309 (comment)
I think you hit on pretty much everything that we needed in addition, except a possibly useful addition would be to allow the user to provide the engine ID to load.  It's optional if the library has only one engine implementation, but it may be a small change and useful to someone.",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2602,2019-11-02T03:35:14Z,2021-03-23T06:14:16Z,2021-03-23T06:14:16Z,CLOSED,False,32358,5449,230,https://github.com/adinigam,Adding OpenSSL engine support in LibrdKafka for SSL connections,378,[],https://github.com/edenhill/librdkafka/pull/2602,https://github.com/adinigam,6,https://github.com/edenhill/librdkafka/pull/2602#issuecomment-586012261,"Currently LibrdKafka uses Openssl behind the scenes to establish SSL channel between broker and client.
To create the SSL channel, OpenSSL needs public and private keys. Today these keys are provided in byte* format to openssl. Byte* can either be generated by librdkafka or application itself. But ultimately to generate the byte* format, keys needs to be exported from where-ever it is residing.
In many scenarios exporting might not be an option since it might not be permitted by hardware/software, eg: if application is using non-exportable certificates in windows certificate store.
Good news is OpenSSL provides plugin model which is called engine. This engine can be used to interact with the storage and use the public and private keys for the certificate without exporting in byte* format. Such clients will have to write custom engines.
I am proposing to enhance LibrdKafka to support OpenSSL engine. Below are the changes I am taking up in this PR.


Add new parameter ""openssl.engine.location"" which will take the full path to the binary containing the openssl engine


This engine will be initialized if the parameter ""openssl.engine.location"" value is not null. Below is the initialization code in the method rd_kafka_ssl_ctx_init


Call engine method ENGINE_load_ssl_client_cert with appropriate params to fetch the pointers to the public and private keys. Key parameter is building and disposing the stack of ca-certs in rd_kafka_ssl_set_certs.


Update: I can see some old threads suggesting the usage of engine especially for non-exportable certificates. Great if someone knows and let me know why that idea was not taken forward?","@LighthouseJ Yes #2309 helped me get here.
Also i will add ssl.engine.id and ssl.engine.callback_data as parameters in my next iteration",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2602,2019-11-02T03:35:14Z,2021-03-23T06:14:16Z,2021-03-23T06:14:16Z,CLOSED,False,32358,5449,230,https://github.com/adinigam,Adding OpenSSL engine support in LibrdKafka for SSL connections,378,[],https://github.com/edenhill/librdkafka/pull/2602,https://github.com/adinigam,7,https://github.com/edenhill/librdkafka/pull/2602#issuecomment-623655975,"Currently LibrdKafka uses Openssl behind the scenes to establish SSL channel between broker and client.
To create the SSL channel, OpenSSL needs public and private keys. Today these keys are provided in byte* format to openssl. Byte* can either be generated by librdkafka or application itself. But ultimately to generate the byte* format, keys needs to be exported from where-ever it is residing.
In many scenarios exporting might not be an option since it might not be permitted by hardware/software, eg: if application is using non-exportable certificates in windows certificate store.
Good news is OpenSSL provides plugin model which is called engine. This engine can be used to interact with the storage and use the public and private keys for the certificate without exporting in byte* format. Such clients will have to write custom engines.
I am proposing to enhance LibrdKafka to support OpenSSL engine. Below are the changes I am taking up in this PR.


Add new parameter ""openssl.engine.location"" which will take the full path to the binary containing the openssl engine


This engine will be initialized if the parameter ""openssl.engine.location"" value is not null. Below is the initialization code in the method rd_kafka_ssl_ctx_init


Call engine method ENGINE_load_ssl_client_cert with appropriate params to fetch the pointers to the public and private keys. Key parameter is building and disposing the stack of ca-certs in rd_kafka_ssl_set_certs.


Update: I can see some old threads suggesting the usage of engine especially for non-exportable certificates. Great if someone knows and let me know why that idea was not taken forward?",@edenhill Approx when we can target this change ?,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2602,2019-11-02T03:35:14Z,2021-03-23T06:14:16Z,2021-03-23T06:14:16Z,CLOSED,False,32358,5449,230,https://github.com/adinigam,Adding OpenSSL engine support in LibrdKafka for SSL connections,378,[],https://github.com/edenhill/librdkafka/pull/2602,https://github.com/adinigam,8,https://github.com/edenhill/librdkafka/pull/2602#issuecomment-628827004,"Currently LibrdKafka uses Openssl behind the scenes to establish SSL channel between broker and client.
To create the SSL channel, OpenSSL needs public and private keys. Today these keys are provided in byte* format to openssl. Byte* can either be generated by librdkafka or application itself. But ultimately to generate the byte* format, keys needs to be exported from where-ever it is residing.
In many scenarios exporting might not be an option since it might not be permitted by hardware/software, eg: if application is using non-exportable certificates in windows certificate store.
Good news is OpenSSL provides plugin model which is called engine. This engine can be used to interact with the storage and use the public and private keys for the certificate without exporting in byte* format. Such clients will have to write custom engines.
I am proposing to enhance LibrdKafka to support OpenSSL engine. Below are the changes I am taking up in this PR.


Add new parameter ""openssl.engine.location"" which will take the full path to the binary containing the openssl engine


This engine will be initialized if the parameter ""openssl.engine.location"" value is not null. Below is the initialization code in the method rd_kafka_ssl_ctx_init


Call engine method ENGINE_load_ssl_client_cert with appropriate params to fetch the pointers to the public and private keys. Key parameter is building and disposing the stack of ca-certs in rd_kafka_ssl_set_certs.


Update: I can see some old threads suggesting the usage of engine especially for non-exportable certificates. Great if someone knows and let me know why that idea was not taken forward?",Thanks for review. I will get to it soon.,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2602,2019-11-02T03:35:14Z,2021-03-23T06:14:16Z,2021-03-23T06:14:16Z,CLOSED,False,32358,5449,230,https://github.com/adinigam,Adding OpenSSL engine support in LibrdKafka for SSL connections,378,[],https://github.com/edenhill/librdkafka/pull/2602,https://github.com/edenhill,9,https://github.com/edenhill/librdkafka/pull/2602#issuecomment-635209590,"Currently LibrdKafka uses Openssl behind the scenes to establish SSL channel between broker and client.
To create the SSL channel, OpenSSL needs public and private keys. Today these keys are provided in byte* format to openssl. Byte* can either be generated by librdkafka or application itself. But ultimately to generate the byte* format, keys needs to be exported from where-ever it is residing.
In many scenarios exporting might not be an option since it might not be permitted by hardware/software, eg: if application is using non-exportable certificates in windows certificate store.
Good news is OpenSSL provides plugin model which is called engine. This engine can be used to interact with the storage and use the public and private keys for the certificate without exporting in byte* format. Such clients will have to write custom engines.
I am proposing to enhance LibrdKafka to support OpenSSL engine. Below are the changes I am taking up in this PR.


Add new parameter ""openssl.engine.location"" which will take the full path to the binary containing the openssl engine


This engine will be initialized if the parameter ""openssl.engine.location"" value is not null. Below is the initialization code in the method rd_kafka_ssl_ctx_init


Call engine method ENGINE_load_ssl_client_cert with appropriate params to fetch the pointers to the public and private keys. Key parameter is building and disposing the stack of ca-certs in rd_kafka_ssl_set_certs.


Update: I can see some old threads suggesting the usage of engine especially for non-exportable certificates. Great if someone knows and let me know why that idea was not taken forward?","Let me know when all comments have been addressed and this is ready for review.
Also make sure to rebase it on latest master branch.",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2602,2019-11-02T03:35:14Z,2021-03-23T06:14:16Z,2021-03-23T06:14:16Z,CLOSED,False,32358,5449,230,https://github.com/adinigam,Adding OpenSSL engine support in LibrdKafka for SSL connections,378,[],https://github.com/edenhill/librdkafka/pull/2602,https://github.com/adinigam,10,https://github.com/edenhill/librdkafka/pull/2602#issuecomment-635228118,"Currently LibrdKafka uses Openssl behind the scenes to establish SSL channel between broker and client.
To create the SSL channel, OpenSSL needs public and private keys. Today these keys are provided in byte* format to openssl. Byte* can either be generated by librdkafka or application itself. But ultimately to generate the byte* format, keys needs to be exported from where-ever it is residing.
In many scenarios exporting might not be an option since it might not be permitted by hardware/software, eg: if application is using non-exportable certificates in windows certificate store.
Good news is OpenSSL provides plugin model which is called engine. This engine can be used to interact with the storage and use the public and private keys for the certificate without exporting in byte* format. Such clients will have to write custom engines.
I am proposing to enhance LibrdKafka to support OpenSSL engine. Below are the changes I am taking up in this PR.


Add new parameter ""openssl.engine.location"" which will take the full path to the binary containing the openssl engine


This engine will be initialized if the parameter ""openssl.engine.location"" value is not null. Below is the initialization code in the method rd_kafka_ssl_ctx_init


Call engine method ENGINE_load_ssl_client_cert with appropriate params to fetch the pointers to the public and private keys. Key parameter is building and disposing the stack of ca-certs in rd_kafka_ssl_set_certs.


Update: I can see some old threads suggesting the usage of engine especially for non-exportable certificates. Great if someone knows and let me know why that idea was not taken forward?","@edenhill I think i am done for now.
all existing comments are resolved, and i rebased it to master",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2602,2019-11-02T03:35:14Z,2021-03-23T06:14:16Z,2021-03-23T06:14:16Z,CLOSED,False,32358,5449,230,https://github.com/adinigam,Adding OpenSSL engine support in LibrdKafka for SSL connections,378,[],https://github.com/edenhill/librdkafka/pull/2602,https://github.com/edenhill,11,https://github.com/edenhill/librdkafka/pull/2602#issuecomment-635230753,"Currently LibrdKafka uses Openssl behind the scenes to establish SSL channel between broker and client.
To create the SSL channel, OpenSSL needs public and private keys. Today these keys are provided in byte* format to openssl. Byte* can either be generated by librdkafka or application itself. But ultimately to generate the byte* format, keys needs to be exported from where-ever it is residing.
In many scenarios exporting might not be an option since it might not be permitted by hardware/software, eg: if application is using non-exportable certificates in windows certificate store.
Good news is OpenSSL provides plugin model which is called engine. This engine can be used to interact with the storage and use the public and private keys for the certificate without exporting in byte* format. Such clients will have to write custom engines.
I am proposing to enhance LibrdKafka to support OpenSSL engine. Below are the changes I am taking up in this PR.


Add new parameter ""openssl.engine.location"" which will take the full path to the binary containing the openssl engine


This engine will be initialized if the parameter ""openssl.engine.location"" value is not null. Below is the initialization code in the method rd_kafka_ssl_ctx_init


Call engine method ENGINE_load_ssl_client_cert with appropriate params to fetch the pointers to the public and private keys. Key parameter is building and disposing the stack of ca-certs in rd_kafka_ssl_set_certs.


Update: I can see some old threads suggesting the usage of engine especially for non-exportable certificates. Great if someone knows and let me know why that idea was not taken forward?",Please address the CI failures,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2602,2019-11-02T03:35:14Z,2021-03-23T06:14:16Z,2021-03-23T06:14:16Z,CLOSED,False,32358,5449,230,https://github.com/adinigam,Adding OpenSSL engine support in LibrdKafka for SSL connections,378,[],https://github.com/edenhill/librdkafka/pull/2602,https://github.com/edenhill,12,https://github.com/edenhill/librdkafka/pull/2602#issuecomment-635230881,"Currently LibrdKafka uses Openssl behind the scenes to establish SSL channel between broker and client.
To create the SSL channel, OpenSSL needs public and private keys. Today these keys are provided in byte* format to openssl. Byte* can either be generated by librdkafka or application itself. But ultimately to generate the byte* format, keys needs to be exported from where-ever it is residing.
In many scenarios exporting might not be an option since it might not be permitted by hardware/software, eg: if application is using non-exportable certificates in windows certificate store.
Good news is OpenSSL provides plugin model which is called engine. This engine can be used to interact with the storage and use the public and private keys for the certificate without exporting in byte* format. Such clients will have to write custom engines.
I am proposing to enhance LibrdKafka to support OpenSSL engine. Below are the changes I am taking up in this PR.


Add new parameter ""openssl.engine.location"" which will take the full path to the binary containing the openssl engine


This engine will be initialized if the parameter ""openssl.engine.location"" value is not null. Below is the initialization code in the method rd_kafka_ssl_ctx_init


Call engine method ENGINE_load_ssl_client_cert with appropriate params to fetch the pointers to the public and private keys. Key parameter is building and disposing the stack of ca-certs in rd_kafka_ssl_set_certs.


Update: I can see some old threads suggesting the usage of engine especially for non-exportable certificates. Great if someone knows and let me know why that idea was not taken forward?",Also says there's a conflict ^,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2602,2019-11-02T03:35:14Z,2021-03-23T06:14:16Z,2021-03-23T06:14:16Z,CLOSED,False,32358,5449,230,https://github.com/adinigam,Adding OpenSSL engine support in LibrdKafka for SSL connections,378,[],https://github.com/edenhill/librdkafka/pull/2602,https://github.com/adinigam,13,https://github.com/edenhill/librdkafka/pull/2602#issuecomment-639351862,"Currently LibrdKafka uses Openssl behind the scenes to establish SSL channel between broker and client.
To create the SSL channel, OpenSSL needs public and private keys. Today these keys are provided in byte* format to openssl. Byte* can either be generated by librdkafka or application itself. But ultimately to generate the byte* format, keys needs to be exported from where-ever it is residing.
In many scenarios exporting might not be an option since it might not be permitted by hardware/software, eg: if application is using non-exportable certificates in windows certificate store.
Good news is OpenSSL provides plugin model which is called engine. This engine can be used to interact with the storage and use the public and private keys for the certificate without exporting in byte* format. Such clients will have to write custom engines.
I am proposing to enhance LibrdKafka to support OpenSSL engine. Below are the changes I am taking up in this PR.


Add new parameter ""openssl.engine.location"" which will take the full path to the binary containing the openssl engine


This engine will be initialized if the parameter ""openssl.engine.location"" value is not null. Below is the initialization code in the method rd_kafka_ssl_ctx_init


Call engine method ENGINE_load_ssl_client_cert with appropriate params to fetch the pointers to the public and private keys. Key parameter is building and disposing the stack of ca-certs in rd_kafka_ssl_set_certs.


Update: I can see some old threads suggesting the usage of engine especially for non-exportable certificates. Great if someone knows and let me know why that idea was not taken forward?","Let me know when all comments have been addressed and this is ready for review.
Also make sure to rebase it on latest master branch.

@edenhill Can you take another look.
Also note i have tested this only on windows.
Generally does PRs require testing on multiple OS before checkin?",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2602,2019-11-02T03:35:14Z,2021-03-23T06:14:16Z,2021-03-23T06:14:16Z,CLOSED,False,32358,5449,230,https://github.com/adinigam,Adding OpenSSL engine support in LibrdKafka for SSL connections,378,[],https://github.com/edenhill/librdkafka/pull/2602,https://github.com/ajbarb,14,https://github.com/edenhill/librdkafka/pull/2602#issuecomment-799562007,"Currently LibrdKafka uses Openssl behind the scenes to establish SSL channel between broker and client.
To create the SSL channel, OpenSSL needs public and private keys. Today these keys are provided in byte* format to openssl. Byte* can either be generated by librdkafka or application itself. But ultimately to generate the byte* format, keys needs to be exported from where-ever it is residing.
In many scenarios exporting might not be an option since it might not be permitted by hardware/software, eg: if application is using non-exportable certificates in windows certificate store.
Good news is OpenSSL provides plugin model which is called engine. This engine can be used to interact with the storage and use the public and private keys for the certificate without exporting in byte* format. Such clients will have to write custom engines.
I am proposing to enhance LibrdKafka to support OpenSSL engine. Below are the changes I am taking up in this PR.


Add new parameter ""openssl.engine.location"" which will take the full path to the binary containing the openssl engine


This engine will be initialized if the parameter ""openssl.engine.location"" value is not null. Below is the initialization code in the method rd_kafka_ssl_ctx_init


Call engine method ENGINE_load_ssl_client_cert with appropriate params to fetch the pointers to the public and private keys. Key parameter is building and disposing the stack of ca-certs in rd_kafka_ssl_set_certs.


Update: I can see some old threads suggesting the usage of engine especially for non-exportable certificates. Great if someone knows and let me know why that idea was not taken forward?","@edenhill, when do you think this can be merged in? Aditya is not working on this anymore. I can address any new feedback that.",True,{'CONFUSED': ['https://github.com/adinigam']}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2602,2019-11-02T03:35:14Z,2021-03-23T06:14:16Z,2021-03-23T06:14:16Z,CLOSED,False,32358,5449,230,https://github.com/adinigam,Adding OpenSSL engine support in LibrdKafka for SSL connections,378,[],https://github.com/edenhill/librdkafka/pull/2602,https://github.com/ajbarb,15,https://github.com/edenhill/librdkafka/pull/2602#issuecomment-799708456,"Currently LibrdKafka uses Openssl behind the scenes to establish SSL channel between broker and client.
To create the SSL channel, OpenSSL needs public and private keys. Today these keys are provided in byte* format to openssl. Byte* can either be generated by librdkafka or application itself. But ultimately to generate the byte* format, keys needs to be exported from where-ever it is residing.
In many scenarios exporting might not be an option since it might not be permitted by hardware/software, eg: if application is using non-exportable certificates in windows certificate store.
Good news is OpenSSL provides plugin model which is called engine. This engine can be used to interact with the storage and use the public and private keys for the certificate without exporting in byte* format. Such clients will have to write custom engines.
I am proposing to enhance LibrdKafka to support OpenSSL engine. Below are the changes I am taking up in this PR.


Add new parameter ""openssl.engine.location"" which will take the full path to the binary containing the openssl engine


This engine will be initialized if the parameter ""openssl.engine.location"" value is not null. Below is the initialization code in the method rd_kafka_ssl_ctx_init


Call engine method ENGINE_load_ssl_client_cert with appropriate params to fetch the pointers to the public and private keys. Key parameter is building and disposing the stack of ca-certs in rd_kafka_ssl_set_certs.


Update: I can see some old threads suggesting the usage of engine especially for non-exportable certificates. Great if someone knows and let me know why that idea was not taken forward?","@edenhill, when do you think this can be merged in? Aditya is not working on this anymore. I can address any new feedback that.

I spoke too soon without checking. It seems @adinigam is very interested to complete this and is willing to address any new CR comments. Please let us know if anything is needed to merge this.
Finishing this will really help us in making things easier with respect to merging the new librdkafka changes to our internal repo. We love to take on the improvements/fixes/features that you and community adds and also want to provide early feedback on it. Thank you!",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2602,2019-11-02T03:35:14Z,2021-03-23T06:14:16Z,2021-03-23T06:14:16Z,CLOSED,False,32358,5449,230,https://github.com/adinigam,Adding OpenSSL engine support in LibrdKafka for SSL connections,378,[],https://github.com/edenhill/librdkafka/pull/2602,https://github.com/edenhill,16,https://github.com/edenhill/librdkafka/pull/2602#issuecomment-800072397,"Currently LibrdKafka uses Openssl behind the scenes to establish SSL channel between broker and client.
To create the SSL channel, OpenSSL needs public and private keys. Today these keys are provided in byte* format to openssl. Byte* can either be generated by librdkafka or application itself. But ultimately to generate the byte* format, keys needs to be exported from where-ever it is residing.
In many scenarios exporting might not be an option since it might not be permitted by hardware/software, eg: if application is using non-exportable certificates in windows certificate store.
Good news is OpenSSL provides plugin model which is called engine. This engine can be used to interact with the storage and use the public and private keys for the certificate without exporting in byte* format. Such clients will have to write custom engines.
I am proposing to enhance LibrdKafka to support OpenSSL engine. Below are the changes I am taking up in this PR.


Add new parameter ""openssl.engine.location"" which will take the full path to the binary containing the openssl engine


This engine will be initialized if the parameter ""openssl.engine.location"" value is not null. Below is the initialization code in the method rd_kafka_ssl_ctx_init


Call engine method ENGINE_load_ssl_client_cert with appropriate params to fetch the pointers to the public and private keys. Key parameter is building and disposing the stack of ca-certs in rd_kafka_ssl_set_certs.


Update: I can see some old threads suggesting the usage of engine especially for non-exportable certificates. Great if someone knows and let me know why that idea was not taken forward?",Please add a test-case that attempts to set a non-exist SSL engine to see that the error handling works.,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2602,2019-11-02T03:35:14Z,2021-03-23T06:14:16Z,2021-03-23T06:14:16Z,CLOSED,False,32358,5449,230,https://github.com/adinigam,Adding OpenSSL engine support in LibrdKafka for SSL connections,378,[],https://github.com/edenhill/librdkafka/pull/2602,https://github.com/edenhill,17,https://github.com/edenhill/librdkafka/pull/2602#issuecomment-800072459,"Currently LibrdKafka uses Openssl behind the scenes to establish SSL channel between broker and client.
To create the SSL channel, OpenSSL needs public and private keys. Today these keys are provided in byte* format to openssl. Byte* can either be generated by librdkafka or application itself. But ultimately to generate the byte* format, keys needs to be exported from where-ever it is residing.
In many scenarios exporting might not be an option since it might not be permitted by hardware/software, eg: if application is using non-exportable certificates in windows certificate store.
Good news is OpenSSL provides plugin model which is called engine. This engine can be used to interact with the storage and use the public and private keys for the certificate without exporting in byte* format. Such clients will have to write custom engines.
I am proposing to enhance LibrdKafka to support OpenSSL engine. Below are the changes I am taking up in this PR.


Add new parameter ""openssl.engine.location"" which will take the full path to the binary containing the openssl engine


This engine will be initialized if the parameter ""openssl.engine.location"" value is not null. Below is the initialization code in the method rd_kafka_ssl_ctx_init


Call engine method ENGINE_load_ssl_client_cert with appropriate params to fetch the pointers to the public and private keys. Key parameter is building and disposing the stack of ca-certs in rd_kafka_ssl_set_certs.


Update: I can see some old threads suggesting the usage of engine especially for non-exportable certificates. Great if someone knows and let me know why that idea was not taken forward?",Also rebase on latest master.,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2602,2019-11-02T03:35:14Z,2021-03-23T06:14:16Z,2021-03-23T06:14:16Z,CLOSED,False,32358,5449,230,https://github.com/adinigam,Adding OpenSSL engine support in LibrdKafka for SSL connections,378,[],https://github.com/edenhill/librdkafka/pull/2602,https://github.com/ajbarb,18,https://github.com/edenhill/librdkafka/pull/2602#issuecomment-800531601,"Currently LibrdKafka uses Openssl behind the scenes to establish SSL channel between broker and client.
To create the SSL channel, OpenSSL needs public and private keys. Today these keys are provided in byte* format to openssl. Byte* can either be generated by librdkafka or application itself. But ultimately to generate the byte* format, keys needs to be exported from where-ever it is residing.
In many scenarios exporting might not be an option since it might not be permitted by hardware/software, eg: if application is using non-exportable certificates in windows certificate store.
Good news is OpenSSL provides plugin model which is called engine. This engine can be used to interact with the storage and use the public and private keys for the certificate without exporting in byte* format. Such clients will have to write custom engines.
I am proposing to enhance LibrdKafka to support OpenSSL engine. Below are the changes I am taking up in this PR.


Add new parameter ""openssl.engine.location"" which will take the full path to the binary containing the openssl engine


This engine will be initialized if the parameter ""openssl.engine.location"" value is not null. Below is the initialization code in the method rd_kafka_ssl_ctx_init


Call engine method ENGINE_load_ssl_client_cert with appropriate params to fetch the pointers to the public and private keys. Key parameter is building and disposing the stack of ca-certs in rd_kafka_ssl_set_certs.


Update: I can see some old threads suggesting the usage of engine especially for non-exportable certificates. Great if someone knows and let me know why that idea was not taken forward?","Also rebase on latest master.

Thanks for the comments. We will work on them.",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2605,2019-11-05T09:20:41Z,2020-02-05T08:33:17Z,2020-02-05T08:37:37Z,MERGED,True,9141,997,77,https://github.com/edenhill,"Transactional Producer (full EOS, KIP-98)",33,['enhancement'],https://github.com/edenhill/librdkafka/pull/2605,https://github.com/edenhill,1,https://github.com/edenhill/librdkafka/pull/2605,"This is work in progress.
Current status:

 Functionally complete - try it out in action!
 Functional tests are complete.
 Public API is settled.
 Proper error handling across, including controller switches during transactions
 API docs and manual are not done and will be finalized in a sub-sequent PR.

Review guide:

The transactional producer relies on the idempotent producer: to acquire the initial ProducerID, and for the message guarantees. Most fatal errors in the idempotent producer are only fatal to the current transaction when using the transactional producer, resulting in the ABORTABLE_ERROR state.
Most in-transaction errors, be they from the underlying idempotent producer, or failure to add offsets or partitions to the transaction, result in an Abortable Error, which is a transactional error state that requires the application to call abort_transaction() and then start over again with begin_transaction().
All the transactional logic is the transaction manager, rdkafka_txnmgr.c, with some leaky abstraction in rdkafka_idempotence.c for the interplay between the idempotent producer and the transaction manager.
The use of coord_req() (rdkafka_coord.c), which is an abstraction on top of coordinator brokers makes it easier to perform retries, etc. External community work on the Admin API has already started using it (based on the txns branch).
Matt's 0098 transactional consumer test has been extended to run identical tests with the Java txn producer and the librdkafka txn producer.
Ignore documentation FIXMEs for now, docs will be finalized after this PR has been merged.

Miscellaneous:

Adds a builtin mock cluster with error injection.
Manual unittest code coverage functionality as an alternative to object inspection.","This is work in progress.
Current status:

 Functionally complete - try it out in action!
 Functional tests are complete.
 Public API is settled.
 Proper error handling across, including controller switches during transactions
 API docs and manual are not done and will be finalized in a sub-sequent PR.

Review guide:

The transactional producer relies on the idempotent producer: to acquire the initial ProducerID, and for the message guarantees. Most fatal errors in the idempotent producer are only fatal to the current transaction when using the transactional producer, resulting in the ABORTABLE_ERROR state.
Most in-transaction errors, be they from the underlying idempotent producer, or failure to add offsets or partitions to the transaction, result in an Abortable Error, which is a transactional error state that requires the application to call abort_transaction() and then start over again with begin_transaction().
All the transactional logic is the transaction manager, rdkafka_txnmgr.c, with some leaky abstraction in rdkafka_idempotence.c for the interplay between the idempotent producer and the transaction manager.
The use of coord_req() (rdkafka_coord.c), which is an abstraction on top of coordinator brokers makes it easier to perform retries, etc. External community work on the Admin API has already started using it (based on the txns branch).
Matt's 0098 transactional consumer test has been extended to run identical tests with the Java txn producer and the librdkafka txn producer.
Ignore documentation FIXMEs for now, docs will be finalized after this PR has been merged.

Miscellaneous:

Adds a builtin mock cluster with error injection.
Manual unittest code coverage functionality as an alternative to object inspection.",True,"{'THUMBS_UP': ['https://github.com/Buttered', 'https://github.com/NikDevPHP', 'https://github.com/maks-rafalko']}"
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2605,2019-11-05T09:20:41Z,2020-02-05T08:33:17Z,2020-02-05T08:37:37Z,MERGED,True,9141,997,77,https://github.com/edenhill,"Transactional Producer (full EOS, KIP-98)",33,['enhancement'],https://github.com/edenhill/librdkafka/pull/2605,https://github.com/alex88,2,https://github.com/edenhill/librdkafka/pull/2605#issuecomment-582298447,"This is work in progress.
Current status:

 Functionally complete - try it out in action!
 Functional tests are complete.
 Public API is settled.
 Proper error handling across, including controller switches during transactions
 API docs and manual are not done and will be finalized in a sub-sequent PR.

Review guide:

The transactional producer relies on the idempotent producer: to acquire the initial ProducerID, and for the message guarantees. Most fatal errors in the idempotent producer are only fatal to the current transaction when using the transactional producer, resulting in the ABORTABLE_ERROR state.
Most in-transaction errors, be they from the underlying idempotent producer, or failure to add offsets or partitions to the transaction, result in an Abortable Error, which is a transactional error state that requires the application to call abort_transaction() and then start over again with begin_transaction().
All the transactional logic is the transaction manager, rdkafka_txnmgr.c, with some leaky abstraction in rdkafka_idempotence.c for the interplay between the idempotent producer and the transaction manager.
The use of coord_req() (rdkafka_coord.c), which is an abstraction on top of coordinator brokers makes it easier to perform retries, etc. External community work on the Admin API has already started using it (based on the txns branch).
Matt's 0098 transactional consumer test has been extended to run identical tests with the Java txn producer and the librdkafka txn producer.
Ignore documentation FIXMEs for now, docs will be finalized after this PR has been merged.

Miscellaneous:

Adds a builtin mock cluster with error injection.
Manual unittest code coverage functionality as an alternative to object inspection.", glad to see this merged! Thanks for the awesome work!,True,"{'HEART': ['https://github.com/edenhill', 'https://github.com/ervitis']}"
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2612,2019-11-08T10:59:00Z,2019-11-08T11:14:51Z,2019-11-08T11:14:53Z,MERGED,True,1,1,1,https://github.com/edenhill,Don't trigger error when broker hostname changes (#2591),1,[],https://github.com/edenhill/librdkafka/pull/2612,https://github.com/edenhill,1,https://github.com/edenhill/librdkafka/pull/2612,"Fixes #2591, et.al.","Fixes #2591, et.al.",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2613,2019-11-08T11:45:58Z,2019-11-11T08:30:25Z,2019-11-12T11:57:25Z,MERGED,True,26,6,3,https://github.com/edenhill,Print compression type per message-set when debug=msg,1,[],https://github.com/edenhill/librdkafka/pull/2613,https://github.com/edenhill,1,https://github.com/edenhill/librdkafka/pull/2613,This is handy,This is handy,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2619,2019-11-11T14:03:55Z,2019-11-15T11:54:08Z,2019-11-15T11:54:08Z,CLOSED,False,4174,224,40,https://github.com/edenhill,Mock Cluster,8,[],https://github.com/edenhill/librdkafka/pull/2619,https://github.com/edenhill,1,https://github.com/edenhill/librdkafka/pull/2619,,,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2619,2019-11-11T14:03:55Z,2019-11-15T11:54:08Z,2019-11-15T11:54:08Z,CLOSED,False,4174,224,40,https://github.com/edenhill,Mock Cluster,8,[],https://github.com/edenhill/librdkafka/pull/2619,https://github.com/edenhill,2,https://github.com/edenhill/librdkafka/pull/2619#issuecomment-554331714,,This was integrated into mhowlett/fff,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2620,2019-11-11T21:55:46Z,2021-03-09T14:07:18Z,2021-03-09T14:07:25Z,MERGED,True,15,8,2,https://github.com/chrisbeard,Provide RdKafka::Handle* to OAuthBearerTokenRefreshCb callback (#2607),5,[],https://github.com/edenhill/librdkafka/pull/2620,https://github.com/chrisbeard,1,https://github.com/edenhill/librdkafka/pull/2620,"This change provides  RdKafka::OAuthBearerTokenRefreshCb::oauthbearer_token_refresh_cb() with an RdKafka::Handle*. This gives the callback the proper RdKafka::Handle to call oauthbearer_set_token()/oauthbearer_set_token_failure().
I've also updated the docs to more explicitly refer to the RdKafka::Handle methods.
Addresses issue: #2607","This change provides  RdKafka::OAuthBearerTokenRefreshCb::oauthbearer_token_refresh_cb() with an RdKafka::Handle*. This gives the callback the proper RdKafka::Handle to call oauthbearer_set_token()/oauthbearer_set_token_failure().
I've also updated the docs to more explicitly refer to the RdKafka::Handle methods.
Addresses issue: #2607",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2620,2019-11-11T21:55:46Z,2021-03-09T14:07:18Z,2021-03-09T14:07:25Z,MERGED,True,15,8,2,https://github.com/chrisbeard,Provide RdKafka::Handle* to OAuthBearerTokenRefreshCb callback (#2607),5,[],https://github.com/edenhill/librdkafka/pull/2620,https://github.com/chrisbeard,2,https://github.com/edenhill/librdkafka/pull/2620#issuecomment-558277854,"This change provides  RdKafka::OAuthBearerTokenRefreshCb::oauthbearer_token_refresh_cb() with an RdKafka::Handle*. This gives the callback the proper RdKafka::Handle to call oauthbearer_set_token()/oauthbearer_set_token_failure().
I've also updated the docs to more explicitly refer to the RdKafka::Handle methods.
Addresses issue: #2607","@edenhill It doesn't looks like the travis-ci failure is related as far as I can tell, is this okay?",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2620,2019-11-11T21:55:46Z,2021-03-09T14:07:18Z,2021-03-09T14:07:25Z,MERGED,True,15,8,2,https://github.com/chrisbeard,Provide RdKafka::Handle* to OAuthBearerTokenRefreshCb callback (#2607),5,[],https://github.com/edenhill/librdkafka/pull/2620,https://github.com/edenhill,3,https://github.com/edenhill/librdkafka/pull/2620#issuecomment-793944327,"This change provides  RdKafka::OAuthBearerTokenRefreshCb::oauthbearer_token_refresh_cb() with an RdKafka::Handle*. This gives the callback the proper RdKafka::Handle to call oauthbearer_set_token()/oauthbearer_set_token_failure().
I've also updated the docs to more explicitly refer to the RdKafka::Handle methods.
Addresses issue: #2607","Thank you, @chrisbeard !",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2624,2019-11-15T15:32:45Z,2019-11-15T17:08:56Z,2019-11-15T17:09:00Z,MERGED,True,1,0,1,https://github.com/edenhill,Missing stdarg.h for va_list in rdkafka_mock.c (#2623),1,[],https://github.com/edenhill/librdkafka/pull/2624,https://github.com/edenhill,1,https://github.com/edenhill/librdkafka/pull/2624,,,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2627,2019-11-18T09:36:57Z,2019-11-18T10:02:31Z,2019-11-18T10:02:34Z,MERGED,True,2,2,1,https://github.com/edenhill,Fix multiple-typedef warnings for mock interface,1,[],https://github.com/edenhill/librdkafka/pull/2627,https://github.com/edenhill,1,https://github.com/edenhill/librdkafka/pull/2627,,,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2635,2019-11-25T16:21:15Z,2019-12-11T15:19:49Z,2019-12-11T15:19:53Z,MERGED,True,470,111,10,https://github.com/edenhill,Static consumer fencing and fatal consumer errors,5,[],https://github.com/edenhill/librdkafka/pull/2635,https://github.com/edenhill,1,https://github.com/edenhill/librdkafka/pull/2635,"This PR introduces fatal errors on the consumer (previously only used on the idempotent producer), for use when a static group member is fenced off by a newer instance.
There's also a stability fix for the KIP-345 test suite
The lower timeout for when a group rebalance will occur can't be known from the test since it relies on the time of the latest SyncGroup or Heartbeat request.
This PR allows a larger low end timespan to accomodate for this uncertainty.
Also did some minor refactoring to help catch the issue, which I think might be useful to keep.","This PR introduces fatal errors on the consumer (previously only used on the idempotent producer), for use when a static group member is fenced off by a newer instance.
There's also a stability fix for the KIP-345 test suite
The lower timeout for when a group rebalance will occur can't be known from the test since it relies on the time of the latest SyncGroup or Heartbeat request.
This PR allows a larger low end timespan to accomodate for this uncertainty.
Also did some minor refactoring to help catch the issue, which I think might be useful to keep.",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2635,2019-11-25T16:21:15Z,2019-12-11T15:19:49Z,2019-12-11T15:19:53Z,MERGED,True,470,111,10,https://github.com/edenhill,Static consumer fencing and fatal consumer errors,5,[],https://github.com/edenhill/librdkafka/pull/2635,https://github.com/edenhill,2,https://github.com/edenhill/librdkafka/pull/2635#issuecomment-564115747,"This PR introduces fatal errors on the consumer (previously only used on the idempotent producer), for use when a static group member is fenced off by a newer instance.
There's also a stability fix for the KIP-345 test suite
The lower timeout for when a group rebalance will occur can't be known from the test since it relies on the time of the latest SyncGroup or Heartbeat request.
This PR allows a larger low end timespan to accomodate for this uncertainty.
Also did some minor refactoring to help catch the issue, which I think might be useful to keep.",@rnpridgeon @mhowlett Please give this PR a complete re-review since it now also contains fenced error handling.,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2636,2019-11-26T14:20:39Z,2020-07-02T13:58:43Z,2020-07-02T13:58:43Z,CLOSED,False,54,4,7,https://github.com/bdaheb,Add getaddrinfo callback,1,[],https://github.com/edenhill/librdkafka/pull/2636,https://github.com/bdaheb,1,https://github.com/edenhill/librdkafka/pull/2636,"Here is a proposal for a callback to getaddrinfo(). This is extension to librdkafka existing socket() and connect() callbacks.
This callback is used to control the whole connection process starting from getaddrinfo_cb then socket_cb and finally connect_cb.
In details, it provides rd_kafka_conf_set_getaddrinfo_cb() to add the getaddrinfo_cb callback.
Moreover, the typical usage is for a router that has 2 wan interfaces to reach the kafka broker. Each wan interface is in a different network namespace and can be up or down.
Since, the kafka client is launched in the root namespace, it needs to be called (via getaddrinfo_cb()) in order to resolve the broker name in a network namespace where the wan interface is up.
Otherwise, if the kafka client starts in a network namespace where the wan interface is down, it gets stuck there retrying getaddrinfo() indefinitely without success.","Here is a proposal for a callback to getaddrinfo(). This is extension to librdkafka existing socket() and connect() callbacks.
This callback is used to control the whole connection process starting from getaddrinfo_cb then socket_cb and finally connect_cb.
In details, it provides rd_kafka_conf_set_getaddrinfo_cb() to add the getaddrinfo_cb callback.
Moreover, the typical usage is for a router that has 2 wan interfaces to reach the kafka broker. Each wan interface is in a different network namespace and can be up or down.
Since, the kafka client is launched in the root namespace, it needs to be called (via getaddrinfo_cb()) in order to resolve the broker name in a network namespace where the wan interface is up.
Otherwise, if the kafka client starts in a network namespace where the wan interface is down, it gets stuck there retrying getaddrinfo() indefinitely without success.",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2636,2019-11-26T14:20:39Z,2020-07-02T13:58:43Z,2020-07-02T13:58:43Z,CLOSED,False,54,4,7,https://github.com/bdaheb,Add getaddrinfo callback,1,[],https://github.com/edenhill/librdkafka/pull/2636,https://github.com/edenhill,2,https://github.com/edenhill/librdkafka/pull/2636#issuecomment-653022298,"Here is a proposal for a callback to getaddrinfo(). This is extension to librdkafka existing socket() and connect() callbacks.
This callback is used to control the whole connection process starting from getaddrinfo_cb then socket_cb and finally connect_cb.
In details, it provides rd_kafka_conf_set_getaddrinfo_cb() to add the getaddrinfo_cb callback.
Moreover, the typical usage is for a router that has 2 wan interfaces to reach the kafka broker. Each wan interface is in a different network namespace and can be up or down.
Since, the kafka client is launched in the root namespace, it needs to be called (via getaddrinfo_cb()) in order to resolve the broker name in a network namespace where the wan interface is up.
Otherwise, if the kafka client starts in a network namespace where the wan interface is down, it gets stuck there retrying getaddrinfo() indefinitely without success.","Thanks for your contribution.
While I understand the use of this it still a very niche use-case and the changes exposes the netdb interface in rdkafka.h, which will have portability concerns, so I'll close this PR until there's bigger demand for this feature.
Thank you.",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2638,2019-11-27T16:13:02Z,2019-11-28T12:58:55Z,2019-11-28T12:58:58Z,MERGED,True,2,16,3,https://github.com/edenhill,Hide KIP-345 static group membership feature (group.instance.id),1,[],https://github.com/edenhill/librdkafka/pull/2638,https://github.com/edenhill,1,https://github.com/edenhill/librdkafka/pull/2638,..and mark as experimental and unsupported.,..and mark as experimental and unsupported.,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2640,2019-12-02T05:20:57Z,2020-05-04T07:55:21Z,2020-05-04T07:55:31Z,MERGED,True,50,40,6,https://github.com/benesch,Fail configuration when requested library isn't present ,2,[],https://github.com/edenhill/librdkafka/pull/2640,https://github.com/benesch,1,https://github.com/edenhill/librdkafka/pull/2640,"Two commits in here. Messages reproduced below. The tl;dr is this fixes edenhill/mklove#21 (which was spawned from #556). It seems like the change to configure.base should be incorporated into mklove, but it seems like more mklove development happens in this repository than there?

Fail configuration when requested library isn't present
If the user specifies --enable-XXX, fail configuration if XXX isn't
present. The prior behavior required the user to manually inspect the
configure output to determine whether the feature was actually enabled.
This was particularly problematic with downstream build systems, like
Cargo with rust-rdkafka, where the configure output is hidden from users
if configuration is successful.
The default behavior remains unchanged; if a user does not explicitly
request a feature with --enable-XXX nor dis-request it with
--disable-XXX, and the feature is enabled by default, then it is not be
an error if the feature cannot be found.
Note that this commit additionally excludes the ENABLE_* options from
the confugration summary, as they were misleading. For example, if a
user passed --enable-zstd, but zstd was not found on the system, the
configuration summary would report ""ENABLE_ZSTD y""--merely reporting
that the user had requested zstd support, not that zstd support would
actually be available in the built library. (If the user does desire
that information, they can find it in the BUILT_WITH line, which
includes only the features that were actually possible to enable.)

Allow disabling zlib when building with mklove
For symmetry with the CMake build, which allows disabling zlib
explicitly.","Two commits in here. Messages reproduced below. The tl;dr is this fixes edenhill/mklove#21 (which was spawned from #556). It seems like the change to configure.base should be incorporated into mklove, but it seems like more mklove development happens in this repository than there?

Fail configuration when requested library isn't present
If the user specifies --enable-XXX, fail configuration if XXX isn't
present. The prior behavior required the user to manually inspect the
configure output to determine whether the feature was actually enabled.
This was particularly problematic with downstream build systems, like
Cargo with rust-rdkafka, where the configure output is hidden from users
if configuration is successful.
The default behavior remains unchanged; if a user does not explicitly
request a feature with --enable-XXX nor dis-request it with
--disable-XXX, and the feature is enabled by default, then it is not be
an error if the feature cannot be found.
Note that this commit additionally excludes the ENABLE_* options from
the confugration summary, as they were misleading. For example, if a
user passed --enable-zstd, but zstd was not found on the system, the
configuration summary would report ""ENABLE_ZSTD y""--merely reporting
that the user had requested zstd support, not that zstd support would
actually be available in the built library. (If the user does desire
that information, they can find it in the BUILT_WITH line, which
includes only the features that were actually possible to enable.)

Allow disabling zlib when building with mklove
For symmetry with the CMake build, which allows disabling zlib
explicitly.",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2640,2019-12-02T05:20:57Z,2020-05-04T07:55:21Z,2020-05-04T07:55:31Z,MERGED,True,50,40,6,https://github.com/benesch,Fail configuration when requested library isn't present ,2,[],https://github.com/edenhill/librdkafka/pull/2640,https://github.com/edenhill,2,https://github.com/edenhill/librdkafka/pull/2640#issuecomment-621684634,"Two commits in here. Messages reproduced below. The tl;dr is this fixes edenhill/mklove#21 (which was spawned from #556). It seems like the change to configure.base should be incorporated into mklove, but it seems like more mklove development happens in this repository than there?

Fail configuration when requested library isn't present
If the user specifies --enable-XXX, fail configuration if XXX isn't
present. The prior behavior required the user to manually inspect the
configure output to determine whether the feature was actually enabled.
This was particularly problematic with downstream build systems, like
Cargo with rust-rdkafka, where the configure output is hidden from users
if configuration is successful.
The default behavior remains unchanged; if a user does not explicitly
request a feature with --enable-XXX nor dis-request it with
--disable-XXX, and the feature is enabled by default, then it is not be
an error if the feature cannot be found.
Note that this commit additionally excludes the ENABLE_* options from
the confugration summary, as they were misleading. For example, if a
user passed --enable-zstd, but zstd was not found on the system, the
configuration summary would report ""ENABLE_ZSTD y""--merely reporting
that the user had requested zstd support, not that zstd support would
actually be available in the built library. (If the user does desire
that information, they can find it in the BUILT_WITH line, which
includes only the features that were actually possible to enable.)

Allow disabling zlib when building with mklove
For symmetry with the CMake build, which allows disabling zlib
explicitly.","Sorry for the long delay, can you rebase this on latest master?",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2640,2019-12-02T05:20:57Z,2020-05-04T07:55:21Z,2020-05-04T07:55:31Z,MERGED,True,50,40,6,https://github.com/benesch,Fail configuration when requested library isn't present ,2,[],https://github.com/edenhill/librdkafka/pull/2640,https://github.com/benesch,3,https://github.com/edenhill/librdkafka/pull/2640#issuecomment-621887487,"Two commits in here. Messages reproduced below. The tl;dr is this fixes edenhill/mklove#21 (which was spawned from #556). It seems like the change to configure.base should be incorporated into mklove, but it seems like more mklove development happens in this repository than there?

Fail configuration when requested library isn't present
If the user specifies --enable-XXX, fail configuration if XXX isn't
present. The prior behavior required the user to manually inspect the
configure output to determine whether the feature was actually enabled.
This was particularly problematic with downstream build systems, like
Cargo with rust-rdkafka, where the configure output is hidden from users
if configuration is successful.
The default behavior remains unchanged; if a user does not explicitly
request a feature with --enable-XXX nor dis-request it with
--disable-XXX, and the feature is enabled by default, then it is not be
an error if the feature cannot be found.
Note that this commit additionally excludes the ENABLE_* options from
the confugration summary, as they were misleading. For example, if a
user passed --enable-zstd, but zstd was not found on the system, the
configuration summary would report ""ENABLE_ZSTD y""--merely reporting
that the user had requested zstd support, not that zstd support would
actually be available in the built library. (If the user does desire
that information, they can find it in the BUILT_WITH line, which
includes only the features that were actually possible to enable.)

Allow disabling zlib when building with mklove
For symmetry with the CMake build, which allows disabling zlib
explicitly.","No problem, done!",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2640,2019-12-02T05:20:57Z,2020-05-04T07:55:21Z,2020-05-04T07:55:31Z,MERGED,True,50,40,6,https://github.com/benesch,Fail configuration when requested library isn't present ,2,[],https://github.com/edenhill/librdkafka/pull/2640,https://github.com/benesch,4,https://github.com/edenhill/librdkafka/pull/2640#issuecomment-621953311,"Two commits in here. Messages reproduced below. The tl;dr is this fixes edenhill/mklove#21 (which was spawned from #556). It seems like the change to configure.base should be incorporated into mklove, but it seems like more mklove development happens in this repository than there?

Fail configuration when requested library isn't present
If the user specifies --enable-XXX, fail configuration if XXX isn't
present. The prior behavior required the user to manually inspect the
configure output to determine whether the feature was actually enabled.
This was particularly problematic with downstream build systems, like
Cargo with rust-rdkafka, where the configure output is hidden from users
if configuration is successful.
The default behavior remains unchanged; if a user does not explicitly
request a feature with --enable-XXX nor dis-request it with
--disable-XXX, and the feature is enabled by default, then it is not be
an error if the feature cannot be found.
Note that this commit additionally excludes the ENABLE_* options from
the confugration summary, as they were misleading. For example, if a
user passed --enable-zstd, but zstd was not found on the system, the
configuration summary would report ""ENABLE_ZSTD y""--merely reporting
that the user had requested zstd support, not that zstd support would
actually be available in the built library. (If the user does desire
that information, they can find it in the BUILT_WITH line, which
includes only the features that were actually possible to enable.)

Allow disabling zlib when building with mklove
For symmetry with the CMake build, which allows disabling zlib
explicitly.","Reworking this to avoid the potential for typos makes a lot of sense! I wasn't quite sure how to apply your suggestions exactly as written, so a took a stab at capturing their spirit in the way that made sense to me. Toggle options that should be a tristate now set their default value to ""d"". Then the decision on what to do has four branches:

n -> don't perform the check, leave the library disabled
y -> fail the configure if the library is missing
d -> disable the library and continue if the library is missing
* -> internal mklove error, explode

I wasn't sure how to wrap that logic up in a helper function, because handling the n case requires returning from the function early.
By the way, if it would be easier for you, feel free to just take over this PR and adapt it to your liking!",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2640,2019-12-02T05:20:57Z,2020-05-04T07:55:21Z,2020-05-04T07:55:31Z,MERGED,True,50,40,6,https://github.com/benesch,Fail configuration when requested library isn't present ,2,[],https://github.com/edenhill/librdkafka/pull/2640,https://github.com/edenhill,5,https://github.com/edenhill/librdkafka/pull/2640#issuecomment-623314665,"Two commits in here. Messages reproduced below. The tl;dr is this fixes edenhill/mklove#21 (which was spawned from #556). It seems like the change to configure.base should be incorporated into mklove, but it seems like more mklove development happens in this repository than there?

Fail configuration when requested library isn't present
If the user specifies --enable-XXX, fail configuration if XXX isn't
present. The prior behavior required the user to manually inspect the
configure output to determine whether the feature was actually enabled.
This was particularly problematic with downstream build systems, like
Cargo with rust-rdkafka, where the configure output is hidden from users
if configuration is successful.
The default behavior remains unchanged; if a user does not explicitly
request a feature with --enable-XXX nor dis-request it with
--disable-XXX, and the feature is enabled by default, then it is not be
an error if the feature cannot be found.
Note that this commit additionally excludes the ENABLE_* options from
the confugration summary, as they were misleading. For example, if a
user passed --enable-zstd, but zstd was not found on the system, the
configuration summary would report ""ENABLE_ZSTD y""--merely reporting
that the user had requested zstd support, not that zstd support would
actually be available in the built library. (If the user does desire
that information, they can find it in the BUILT_WITH line, which
includes only the features that were actually possible to enable.)

Allow disabling zlib when building with mklove
For symmetry with the CMake build, which allows disabling zlib
explicitly.","This is great stuff, thank you!",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2641,2019-12-02T05:41:03Z,2020-03-11T15:46:17Z,2020-03-11T15:46:22Z,MERGED,True,4,0,1,https://github.com/benesch,Support finding headers in nonstandard directories in CMake build,1,[],https://github.com/edenhill/librdkafka/pull/2641,https://github.com/benesch,1,https://github.com/edenhill/librdkafka/pull/2641,"CMake may discover libraries, like zlib/openssl/libsasl2 in nonstandard
directories, e.g., if the user has specified the path to a custom build
of one of these libraries via CMAKE_PREFIX_PATH. In these cases, both the
library search paths and the header search paths must be updated, but
the CMake build system was forgetting to update the header search paths
for all libraries besides zstd.
Fix #2451.
Supersedes #2452.","CMake may discover libraries, like zlib/openssl/libsasl2 in nonstandard
directories, e.g., if the user has specified the path to a custom build
of one of these libraries via CMAKE_PREFIX_PATH. In these cases, both the
library search paths and the header search paths must be updated, but
the CMake build system was forgetting to update the header search paths
for all libraries besides zstd.
Fix #2451.
Supersedes #2452.",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2641,2019-12-02T05:41:03Z,2020-03-11T15:46:17Z,2020-03-11T15:46:22Z,MERGED,True,4,0,1,https://github.com/benesch,Support finding headers in nonstandard directories in CMake build,1,[],https://github.com/edenhill/librdkafka/pull/2641,https://github.com/edenhill,2,https://github.com/edenhill/librdkafka/pull/2641#issuecomment-582774870,"CMake may discover libraries, like zlib/openssl/libsasl2 in nonstandard
directories, e.g., if the user has specified the path to a custom build
of one of these libraries via CMAKE_PREFIX_PATH. In these cases, both the
library search paths and the header search paths must be updated, but
the CMake build system was forgetting to update the header search paths
for all libraries besides zstd.
Fix #2451.
Supersedes #2452.",Is this good to go?,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2641,2019-12-02T05:41:03Z,2020-03-11T15:46:17Z,2020-03-11T15:46:22Z,MERGED,True,4,0,1,https://github.com/benesch,Support finding headers in nonstandard directories in CMake build,1,[],https://github.com/edenhill/librdkafka/pull/2641,https://github.com/benesch,3,https://github.com/edenhill/librdkafka/pull/2641#issuecomment-584222433,"CMake may discover libraries, like zlib/openssl/libsasl2 in nonstandard
directories, e.g., if the user has specified the path to a custom build
of one of these libraries via CMAKE_PREFIX_PATH. In these cases, both the
library search paths and the header search paths must be updated, but
the CMake build system was forgetting to update the header search paths
for all libraries besides zstd.
Fix #2451.
Supersedes #2452.","Yep, definitely!",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2641,2019-12-02T05:41:03Z,2020-03-11T15:46:17Z,2020-03-11T15:46:22Z,MERGED,True,4,0,1,https://github.com/benesch,Support finding headers in nonstandard directories in CMake build,1,[],https://github.com/edenhill/librdkafka/pull/2641,https://github.com/edenhill,4,https://github.com/edenhill/librdkafka/pull/2641#issuecomment-597710182,"CMake may discover libraries, like zlib/openssl/libsasl2 in nonstandard
directories, e.g., if the user has specified the path to a custom build
of one of these libraries via CMAKE_PREFIX_PATH. In these cases, both the
library search paths and the header search paths must be updated, but
the CMake build system was forgetting to update the header search paths
for all libraries besides zstd.
Fix #2451.
Supersedes #2452.",Thank you for this!,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2642,2019-12-02T13:12:47Z,2019-12-02T18:40:09Z,2019-12-02T18:40:12Z,MERGED,True,82,37,12,https://github.com/edenhill,"consumer_lag fix, and some test compat fixes",5,[],https://github.com/edenhill/librdkafka/pull/2642,https://github.com/edenhill,1,https://github.com/edenhill/librdkafka/pull/2642,,,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2645,2019-12-03T17:05:32Z,2019-12-05T09:54:25Z,2019-12-05T09:54:25Z,MERGED,True,4,4,3,https://github.com/mhowlett,remove XML tag entity from config docs,1,[],https://github.com/edenhill/librdkafka/pull/2645,https://github.com/mhowlett,1,https://github.com/edenhill/librdkafka/pull/2645,"this is not quite as good, but means i don't need to make special cases in C#.","this is not quite as good, but means i don't need to make special cases in C#.",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2646,2019-12-04T05:55:36Z,,2022-02-17T07:05:57Z,OPEN,False,133,37,13,https://github.com/goldenbull,Update librdkafka.redist.targets,3,[],https://github.com/edenhill/librdkafka/pull/2646,https://github.com/goldenbull,1,https://github.com/edenhill/librdkafka/pull/2646,add librdkafkacpp.lib into VisualStudio targets file,add librdkafkacpp.lib into VisualStudio targets file,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2646,2019-12-04T05:55:36Z,,2022-02-17T07:05:57Z,OPEN,False,133,37,13,https://github.com/goldenbull,Update librdkafka.redist.targets,3,[],https://github.com/edenhill/librdkafka/pull/2646,https://github.com/edenhill,2,https://github.com/edenhill/librdkafka/pull/2646#issuecomment-561515953,add librdkafkacpp.lib into VisualStudio targets file,"I'm not sure we want C applications to link to the CPP library, which I assume this is doing.",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2646,2019-12-04T05:55:36Z,,2022-02-17T07:05:57Z,OPEN,False,133,37,13,https://github.com/goldenbull,Update librdkafka.redist.targets,3,[],https://github.com/edenhill/librdkafka/pull/2646,https://github.com/goldenbull,3,https://github.com/edenhill/librdkafka/pull/2646#issuecomment-562942434,add librdkafkacpp.lib into VisualStudio targets file,"maybe my knowledge of MSVC compiler is out dated, but I guess C application will not be linked to the cpp lib file if no function is used in the lib, compiler will ignore the cpp lib file even it's included in project setting.",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2650,2019-12-10T03:57:59Z,2020-05-11T11:17:20Z,2020-05-11T11:17:21Z,CLOSED,False,173,9,5,https://github.com/wilmai,cgrp: Roundrobin assignor handles unsubscribed topics (#2121),1,[],https://github.com/edenhill/librdkafka/pull/2650,https://github.com/wilmai,1,https://github.com/edenhill/librdkafka/pull/2650,"Check next roundrobin consumer is in the topic's eligible list,
otherwise select the next eligible consumer.
Previous code overflowed members[next] and crashed.","Check next roundrobin consumer is in the topic's eligible list,
otherwise select the next eligible consumer.
Previous code overflowed members[next] and crashed.",True,"{'THUMBS_UP': ['https://github.com/takaomag', 'https://github.com/mateuszmrozewski']}"
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2650,2019-12-10T03:57:59Z,2020-05-11T11:17:20Z,2020-05-11T11:17:21Z,CLOSED,False,173,9,5,https://github.com/wilmai,cgrp: Roundrobin assignor handles unsubscribed topics (#2121),1,[],https://github.com/edenhill/librdkafka/pull/2650,https://github.com/wilmai,2,https://github.com/edenhill/librdkafka/pull/2650#issuecomment-577001505,"Check next roundrobin consumer is in the topic's eligible list,
otherwise select the next eligible consumer.
Previous code overflowed members[next] and crashed.","The failing test is due to a hardcoded value in 0052-msg_timestamps.c
https://github.com/edenhill/librdkafka/blame/master/tests/0052-msg_timestamps.c#L48",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2650,2019-12-10T03:57:59Z,2020-05-11T11:17:20Z,2020-05-11T11:17:21Z,CLOSED,False,173,9,5,https://github.com/wilmai,cgrp: Roundrobin assignor handles unsubscribed topics (#2121),1,[],https://github.com/edenhill/librdkafka/pull/2650,https://github.com/edenhill,3,https://github.com/edenhill/librdkafka/pull/2650#issuecomment-581821175,"Check next roundrobin consumer is in the topic's eligible list,
otherwise select the next eligible consumer.
Previous code overflowed members[next] and crashed.","Can you rebase this on latest master:
$ git checkout master
$ git fetch edenhill  # where edenhill is your remote for edenhill/librdkafka.git
$ git checkout fix-roundrobin
$ git rebase edenhill/master
$ make
$ cd tests
$ make
$ git push --force origin fix-roundrobin",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2650,2019-12-10T03:57:59Z,2020-05-11T11:17:20Z,2020-05-11T11:17:21Z,CLOSED,False,173,9,5,https://github.com/wilmai,cgrp: Roundrobin assignor handles unsubscribed topics (#2121),1,[],https://github.com/edenhill/librdkafka/pull/2650,https://github.com/wilmai,4,https://github.com/edenhill/librdkafka/pull/2650#issuecomment-585006505,"Check next roundrobin consumer is in the topic's eligible list,
otherwise select the next eligible consumer.
Previous code overflowed members[next] and crashed.",rebased and fixed up @edenhill,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2650,2019-12-10T03:57:59Z,2020-05-11T11:17:20Z,2020-05-11T11:17:21Z,CLOSED,False,173,9,5,https://github.com/wilmai,cgrp: Roundrobin assignor handles unsubscribed topics (#2121),1,[],https://github.com/edenhill/librdkafka/pull/2650,https://github.com/edenhill,5,https://github.com/edenhill/librdkafka/pull/2650#issuecomment-626640098,"Check next roundrobin consumer is in the topic's eligible list,
otherwise select the next eligible consumer.
Previous code overflowed members[next] and crashed.","Thanks for your contribution, this was fixed in 8ba5df7",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2652,2019-12-10T14:19:54Z,2019-12-10T18:58:16Z,2019-12-10T18:58:19Z,MERGED,True,3,3,2,https://github.com/edenhill,Fix 0101 test (when using SASL) and fix AK release-candidate test regression,2,[],https://github.com/edenhill/librdkafka/pull/2652,https://github.com/edenhill,1,https://github.com/edenhill/librdkafka/pull/2652,,,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2654,2019-12-11T01:18:31Z,2019-12-11T08:22:33Z,2019-12-11T08:22:39Z,MERGED,True,1,1,1,https://github.com/ffosilva,gen-ssl-certs: fix rdkafka client cert. creation,1,[],https://github.com/edenhill/librdkafka/pull/2654,https://github.com/ffosilva,1,https://github.com/edenhill/librdkafka/pull/2654,"To reproduce this bug, you should execute in this order:


create a new ca-cert
./gen-ssl-certs.sh ca ca-cert dummy-cn


try to create a client certificate & key without keytool:
./gen-ssl-certs.sh client ca-cert dummy_client_cert_ dummy_client


You will get this error message:
fopen:No such file or directory:../crypto/bio/bss_file.c:72:fopen('ca-cert.srl','r')
After this fix, it should works like expected.","To reproduce this bug, you should execute in this order:


create a new ca-cert
./gen-ssl-certs.sh ca ca-cert dummy-cn


try to create a client certificate & key without keytool:
./gen-ssl-certs.sh client ca-cert dummy_client_cert_ dummy_client


You will get this error message:
fopen:No such file or directory:../crypto/bio/bss_file.c:72:fopen('ca-cert.srl','r')
After this fix, it should works like expected.",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2654,2019-12-11T01:18:31Z,2019-12-11T08:22:33Z,2019-12-11T08:22:39Z,MERGED,True,1,1,1,https://github.com/ffosilva,gen-ssl-certs: fix rdkafka client cert. creation,1,[],https://github.com/edenhill/librdkafka/pull/2654,https://github.com/edenhill,2,https://github.com/edenhill/librdkafka/pull/2654#issuecomment-564431131,"To reproduce this bug, you should execute in this order:


create a new ca-cert
./gen-ssl-certs.sh ca ca-cert dummy-cn


try to create a client certificate & key without keytool:
./gen-ssl-certs.sh client ca-cert dummy_client_cert_ dummy_client


You will get this error message:
fopen:No such file or directory:../crypto/bio/bss_file.c:72:fopen('ca-cert.srl','r')
After this fix, it should works like expected.",Thank you!,True,{'THUMBS_UP': ['https://github.com/ffosilva']}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2657,2019-12-15T04:07:34Z,2019-12-15T11:53:06Z,2019-12-15T11:53:12Z,MERGED,True,1,1,1,https://github.com/nicklauslittle,Fix Coverity parse warnings,1,[],https://github.com/edenhill/librdkafka/pull/2657,https://github.com/nicklauslittle,1,https://github.com/edenhill/librdkafka/pull/2657,This fixes the parse warnings generated by Coverity during a scan.,This fixes the parse warnings generated by Coverity during a scan.,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2657,2019-12-15T04:07:34Z,2019-12-15T11:53:06Z,2019-12-15T11:53:12Z,MERGED,True,1,1,1,https://github.com/nicklauslittle,Fix Coverity parse warnings,1,[],https://github.com/edenhill/librdkafka/pull/2657,https://github.com/edenhill,2,https://github.com/edenhill/librdkafka/pull/2657#issuecomment-565802555,This fixes the parse warnings generated by Coverity during a scan.,Thank you!,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2659,2019-12-17T19:10:36Z,2020-09-14T22:47:37Z,2020-09-14T22:47:50Z,MERGED,True,22,11,4,https://github.com/gridaphobe,Improve pkg-config support,3,[],https://github.com/edenhill/librdkafka/pull/2659,https://github.com/gridaphobe,1,https://github.com/edenhill/librdkafka/pull/2659,"Add required libraries (zlib, lz4, zstd, openssl, sasl2) to the
pkg-config Requires field instead of to Libs if they were
discovered via pkg-config. This allows pkg-config (and other tools
built on top of the pkg-config metadata) to see librdkafka's full
dependency graph.


Prefix full paths to librdkafka.a with the pkg-config variable
${pc_sysrootdir}. pkg-config automatically applies this prefix to
paths passed to -I and -L, but nowhere else. This means that the
previous link line in the -static.pc files is inconsistent, and
would expand to
-L${pc_sysrootdir}${libdir} ${libdir}/librdkafka.a


#include <stddef.h> when checking for sasl so that detection via
pkg-config works. This is needed as sasl.h references some types
that are defined in stddef.h.","Add required libraries (zlib, lz4, zstd, openssl, sasl2) to the
pkg-config Requires field instead of to Libs if they were
discovered via pkg-config. This allows pkg-config (and other tools
built on top of the pkg-config metadata) to see librdkafka's full
dependency graph.


Prefix full paths to librdkafka.a with the pkg-config variable
${pc_sysrootdir}. pkg-config automatically applies this prefix to
paths passed to -I and -L, but nowhere else. This means that the
previous link line in the -static.pc files is inconsistent, and
would expand to
-L${pc_sysrootdir}${libdir} ${libdir}/librdkafka.a


#include <stddef.h> when checking for sasl so that detection via
pkg-config works. This is needed as sasl.h references some types
that are defined in stddef.h.",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2659,2019-12-17T19:10:36Z,2020-09-14T22:47:37Z,2020-09-14T22:47:50Z,MERGED,True,22,11,4,https://github.com/gridaphobe,Improve pkg-config support,3,[],https://github.com/edenhill/librdkafka/pull/2659,https://github.com/edenhill,2,https://github.com/edenhill/librdkafka/pull/2659#issuecomment-687117365,"Add required libraries (zlib, lz4, zstd, openssl, sasl2) to the
pkg-config Requires field instead of to Libs if they were
discovered via pkg-config. This allows pkg-config (and other tools
built on top of the pkg-config metadata) to see librdkafka's full
dependency graph.


Prefix full paths to librdkafka.a with the pkg-config variable
${pc_sysrootdir}. pkg-config automatically applies this prefix to
paths passed to -I and -L, but nowhere else. This means that the
previous link line in the -static.pc files is inconsistent, and
would expand to
-L${pc_sysrootdir}${libdir} ${libdir}/librdkafka.a


#include <stddef.h> when checking for sasl so that detection via
pkg-config works. This is needed as sasl.h references some types
that are defined in stddef.h.","@gridaphobe As I'm no expert in pkg-config (obviously), I'm relying on the assumption that you've tested this in the real world?
Can you rebase this on latest master and I'll review it, if you still think it should be added?",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2659,2019-12-17T19:10:36Z,2020-09-14T22:47:37Z,2020-09-14T22:47:50Z,MERGED,True,22,11,4,https://github.com/gridaphobe,Improve pkg-config support,3,[],https://github.com/edenhill/librdkafka/pull/2659,https://github.com/gridaphobe,3,https://github.com/edenhill/librdkafka/pull/2659#issuecomment-687129905,"Add required libraries (zlib, lz4, zstd, openssl, sasl2) to the
pkg-config Requires field instead of to Libs if they were
discovered via pkg-config. This allows pkg-config (and other tools
built on top of the pkg-config metadata) to see librdkafka's full
dependency graph.


Prefix full paths to librdkafka.a with the pkg-config variable
${pc_sysrootdir}. pkg-config automatically applies this prefix to
paths passed to -I and -L, but nowhere else. This means that the
previous link line in the -static.pc files is inconsistent, and
would expand to
-L${pc_sysrootdir}${libdir} ${libdir}/librdkafka.a


#include <stddef.h> when checking for sasl so that detection via
pkg-config works. This is needed as sasl.h references some types
that are defined in stddef.h.","I wouldn't call myself a pkg-config expert either, but I've tested these changes in the context of our internal package system at Bloomberg, where we have ~900 reverse dependencies of librdkafka, using both the regular and -static pc files from a variety of languages.
If you have contacts in the debian or redhat communities that handle packaging of librdkafka, I'd be happy for them to review this patch too!",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2659,2019-12-17T19:10:36Z,2020-09-14T22:47:37Z,2020-09-14T22:47:50Z,MERGED,True,22,11,4,https://github.com/gridaphobe,Improve pkg-config support,3,[],https://github.com/edenhill/librdkafka/pull/2659,https://github.com/edenhill,4,https://github.com/edenhill/librdkafka/pull/2659#issuecomment-687133803,"Add required libraries (zlib, lz4, zstd, openssl, sasl2) to the
pkg-config Requires field instead of to Libs if they were
discovered via pkg-config. This allows pkg-config (and other tools
built on top of the pkg-config metadata) to see librdkafka's full
dependency graph.


Prefix full paths to librdkafka.a with the pkg-config variable
${pc_sysrootdir}. pkg-config automatically applies this prefix to
paths passed to -I and -L, but nowhere else. This means that the
previous link line in the -static.pc files is inconsistent, and
would expand to
-L${pc_sysrootdir}${libdir} ${libdir}/librdkafka.a


#include <stddef.h> when checking for sasl so that detection via
pkg-config works. This is needed as sasl.h references some types
that are defined in stddef.h.",@paravoid @vincentbernat Could you take a quick look at this to see if it would pose problems for the Debian packages?,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2659,2019-12-17T19:10:36Z,2020-09-14T22:47:37Z,2020-09-14T22:47:50Z,MERGED,True,22,11,4,https://github.com/gridaphobe,Improve pkg-config support,3,[],https://github.com/edenhill/librdkafka/pull/2659,https://github.com/gridaphobe,5,https://github.com/edenhill/librdkafka/pull/2659#issuecomment-687151253,"Add required libraries (zlib, lz4, zstd, openssl, sasl2) to the
pkg-config Requires field instead of to Libs if they were
discovered via pkg-config. This allows pkg-config (and other tools
built on top of the pkg-config metadata) to see librdkafka's full
dependency graph.


Prefix full paths to librdkafka.a with the pkg-config variable
${pc_sysrootdir}. pkg-config automatically applies this prefix to
paths passed to -I and -L, but nowhere else. This means that the
previous link line in the -static.pc files is inconsistent, and
would expand to
-L${pc_sysrootdir}${libdir} ${libdir}/librdkafka.a


#include <stddef.h> when checking for sasl so that detection via
pkg-config works. This is needed as sasl.h references some types
that are defined in stddef.h.","For reference, here's the diff of the pc files that I get on my macbook.
 diff -ur old new
diff -ur old/rdkafka++-static.pc new/rdkafka++-static.pc
--- old/rdkafka++-static.pc	2020-09-04 09:35:01.000000000 -0400
+++ new/rdkafka++-static.pc	2020-09-04 09:35:45.000000000 -0400
@@ -5,5 +5,6 @@
 Name: librdkafka++-static
 Description: The Apache Kafka C/C++ library (static)
 Version: 1.5.0
+Requires: rdkafka-static
 Cflags: -I${includedir}
-Libs: -L${libdir} ${libdir}/librdkafka++-static.a -llz4 -lm -lzstd -lsasl2 -lssl -lcrypto -lz -ldl -lpthread
+Libs: -L${libdir} ${pc_sysrootdir}${libdir}/librdkafka++-static.a -lm -ldl -lpthread

diff -ur old/rdkafka++.pc new/rdkafka++.pc
--- old/rdkafka++.pc	2020-09-04 09:35:01.000000000 -0400
+++ new/rdkafka++.pc	2020-09-04 09:35:45.000000000 -0400
@@ -5,6 +5,7 @@
 Name: librdkafka++
 Description: The Apache Kafka C/C++ library
 Version: 1.5.0
+Requires.private: rdkafka
 Cflags: -I${includedir}
 Libs: -L${libdir} -lrdkafka++
-Libs.private: -L../src -lrdkafka
+Libs.private: -lm -ldl -lpthread

diff -ur old/rdkafka-static.pc new/rdkafka-static.pc
--- old/rdkafka-static.pc	2020-09-04 09:35:04.000000000 -0400
+++ new/rdkafka-static.pc	2020-09-04 09:35:48.000000000 -0400
@@ -5,5 +5,6 @@
 Name: librdkafka-static
 Description: The Apache Kafka C/C++ library (static)
 Version: 1.5.0
+Requires: zlib libcrypto libssl libzstd liblz4
 Cflags: -I${includedir}
-Libs: -L${libdir} ${libdir}/librdkafka-static.a -llz4 -lm -lzstd -lsasl2 -lssl -lcrypto -lz -ldl -lpthread
+Libs: -L${libdir} ${pc_sysrootdir}${libdir}/librdkafka-static.a -lm -ldl -lpthread

diff -ur old/rdkafka.pc new/rdkafka.pc
--- old/rdkafka.pc	2020-09-04 09:35:04.000000000 -0400
+++ new/rdkafka.pc	2020-09-04 09:35:48.000000000 -0400
@@ -5,6 +5,7 @@
 Name: librdkafka
 Description: The Apache Kafka C/C++ library
 Version: 1.5.0
+Requires.private: zlib libcrypto libssl libzstd liblz4
 Cflags: -I${includedir}
 Libs: -L${libdir} -lrdkafka
-Libs.private: -L/usr/local/Cellar/lz4/1.9.2/lib -llz4 -lm -L/usr/local/Cellar/zstd/1.4.5//lib -lzstd -lsasl2 -L/usr/local/Cellar/openssl@1.1/1.1.1g/lib -lssl -L/usr/local/Cellar/openssl@1.1/1.1.1g/lib -lcrypto -L/usr/local/Cellar/zlib/1.2.11/lib -lz -ldl -lpthread
+Libs.private: -lm -ldl -lpthread",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2659,2019-12-17T19:10:36Z,2020-09-14T22:47:37Z,2020-09-14T22:47:50Z,MERGED,True,22,11,4,https://github.com/gridaphobe,Improve pkg-config support,3,[],https://github.com/edenhill/librdkafka/pull/2659,https://github.com/edenhill,6,https://github.com/edenhill/librdkafka/pull/2659#issuecomment-688086933,"Add required libraries (zlib, lz4, zstd, openssl, sasl2) to the
pkg-config Requires field instead of to Libs if they were
discovered via pkg-config. This allows pkg-config (and other tools
built on top of the pkg-config metadata) to see librdkafka's full
dependency graph.


Prefix full paths to librdkafka.a with the pkg-config variable
${pc_sysrootdir}. pkg-config automatically applies this prefix to
paths passed to -I and -L, but nowhere else. This means that the
previous link line in the -static.pc files is inconsistent, and
would expand to
-L${pc_sysrootdir}${libdir} ${libdir}/librdkafka.a


#include <stddef.h> when checking for sasl so that detection via
pkg-config works. This is needed as sasl.h references some types
that are defined in stddef.h.",@remicollet Do these changes look ok from an RPM perspective?,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2659,2019-12-17T19:10:36Z,2020-09-14T22:47:37Z,2020-09-14T22:47:50Z,MERGED,True,22,11,4,https://github.com/gridaphobe,Improve pkg-config support,3,[],https://github.com/edenhill/librdkafka/pull/2659,https://github.com/vincentbernat,7,https://github.com/edenhill/librdkafka/pull/2659#issuecomment-688109897,"Add required libraries (zlib, lz4, zstd, openssl, sasl2) to the
pkg-config Requires field instead of to Libs if they were
discovered via pkg-config. This allows pkg-config (and other tools
built on top of the pkg-config metadata) to see librdkafka's full
dependency graph.


Prefix full paths to librdkafka.a with the pkg-config variable
${pc_sysrootdir}. pkg-config automatically applies this prefix to
paths passed to -I and -L, but nowhere else. This means that the
previous link line in the -static.pc files is inconsistent, and
would expand to
-L${pc_sysrootdir}${libdir} ${libdir}/librdkafka.a


#include <stddef.h> when checking for sasl so that detection via
pkg-config works. This is needed as sasl.h references some types
that are defined in stddef.h.","For Debian, I think the changes are fine.",True,{'THUMBS_UP': ['https://github.com/edenhill']}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2659,2019-12-17T19:10:36Z,2020-09-14T22:47:37Z,2020-09-14T22:47:50Z,MERGED,True,22,11,4,https://github.com/gridaphobe,Improve pkg-config support,3,[],https://github.com/edenhill/librdkafka/pull/2659,https://github.com/remicollet,8,https://github.com/edenhill/librdkafka/pull/2659#issuecomment-688151713,"Add required libraries (zlib, lz4, zstd, openssl, sasl2) to the
pkg-config Requires field instead of to Libs if they were
discovered via pkg-config. This allows pkg-config (and other tools
built on top of the pkg-config metadata) to see librdkafka's full
dependency graph.


Prefix full paths to librdkafka.a with the pkg-config variable
${pc_sysrootdir}. pkg-config automatically applies this prefix to
paths passed to -I and -L, but nowhere else. This means that the
previous link line in the -static.pc files is inconsistent, and
would expand to
-L${pc_sysrootdir}${libdir} ${libdir}/librdkafka.a


#include <stddef.h> when checking for sasl so that detection via
pkg-config works. This is needed as sasl.h references some types
that are defined in stddef.h.","Seems libsasl2 disappear from dependencies
BTW for RPM packaging, static/private stuff are not used (by Guidelines), so no real impact.
Notice: some old libsasl version doesn't provide the .pc file so cannot be used as ""requires"" (but very old version, such as 2.1.23 on RHEL / CentOS 6, which is close to its EOL), at least version >= 2.1.26 are ok)",True,{'THUMBS_UP': ['https://github.com/edenhill']}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2659,2019-12-17T19:10:36Z,2020-09-14T22:47:37Z,2020-09-14T22:47:50Z,MERGED,True,22,11,4,https://github.com/gridaphobe,Improve pkg-config support,3,[],https://github.com/edenhill/librdkafka/pull/2659,https://github.com/gridaphobe,9,https://github.com/edenhill/librdkafka/pull/2659#issuecomment-688361154,"Add required libraries (zlib, lz4, zstd, openssl, sasl2) to the
pkg-config Requires field instead of to Libs if they were
discovered via pkg-config. This allows pkg-config (and other tools
built on top of the pkg-config metadata) to see librdkafka's full
dependency graph.


Prefix full paths to librdkafka.a with the pkg-config variable
${pc_sysrootdir}. pkg-config automatically applies this prefix to
paths passed to -I and -L, but nowhere else. This means that the
previous link line in the -static.pc files is inconsistent, and
would expand to
-L${pc_sysrootdir}${libdir} ${libdir}/librdkafka.a


#include <stddef.h> when checking for sasl so that detection via
pkg-config works. This is needed as sasl.h references some types
that are defined in stddef.h.","Good catch @remicollet, it looks like the change I made to the include check for sasl.h was producing invalid C code somehow.
I also just noticed that when you --enable-static rdkafka now bundles the dependencies into librdkafka-static.a, which means we don't need to add them to the Requires field for rdkafka-static.pc.
I'll try to fix both issues tomorrow.",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2659,2019-12-17T19:10:36Z,2020-09-14T22:47:37Z,2020-09-14T22:47:50Z,MERGED,True,22,11,4,https://github.com/gridaphobe,Improve pkg-config support,3,[],https://github.com/edenhill/librdkafka/pull/2659,https://github.com/edenhill,10,https://github.com/edenhill/librdkafka/pull/2659#issuecomment-688362737,"Add required libraries (zlib, lz4, zstd, openssl, sasl2) to the
pkg-config Requires field instead of to Libs if they were
discovered via pkg-config. This allows pkg-config (and other tools
built on top of the pkg-config metadata) to see librdkafka's full
dependency graph.


Prefix full paths to librdkafka.a with the pkg-config variable
${pc_sysrootdir}. pkg-config automatically applies this prefix to
paths passed to -I and -L, but nowhere else. This means that the
previous link line in the -static.pc files is inconsistent, and
would expand to
-L${pc_sysrootdir}${libdir} ${libdir}/librdkafka.a


#include <stddef.h> when checking for sasl so that detection via
pkg-config works. This is needed as sasl.h references some types
that are defined in stddef.h.","I also just noticed that when you --enable-static rdkafka now bundles the dependencies into librdkafka-static.a

Only for the dependencies that are available as static libraries, the remaining deps (if any) still needs to be linked dynamically and thus listed in the .pc file.",True,{'THUMBS_UP': ['https://github.com/gridaphobe']}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2659,2019-12-17T19:10:36Z,2020-09-14T22:47:37Z,2020-09-14T22:47:50Z,MERGED,True,22,11,4,https://github.com/gridaphobe,Improve pkg-config support,3,[],https://github.com/edenhill/librdkafka/pull/2659,https://github.com/gridaphobe,11,https://github.com/edenhill/librdkafka/pull/2659#issuecomment-690797111,"Add required libraries (zlib, lz4, zstd, openssl, sasl2) to the
pkg-config Requires field instead of to Libs if they were
discovered via pkg-config. This allows pkg-config (and other tools
built on top of the pkg-config metadata) to see librdkafka's full
dependency graph.


Prefix full paths to librdkafka.a with the pkg-config variable
${pc_sysrootdir}. pkg-config automatically applies this prefix to
paths passed to -I and -L, but nowhere else. This means that the
previous link line in the -static.pc files is inconsistent, and
would expand to
-L${pc_sysrootdir}${libdir} ${libdir}/librdkafka.a


#include <stddef.h> when checking for sasl so that detection via
pkg-config works. This is needed as sasl.h references some types
that are defined in stddef.h.","Ok, I've fixed the sasl include check and removed libs bundled into the -static.a from the Requires clause. The diff (under --enable-static) is now:
diff -ur old/rdkafka++-static.pc new/rdkafka++-static.pc
--- old/rdkafka++-static.pc	2020-09-10 19:54:54.000000000 -0400
+++ new/rdkafka++-static.pc	2020-09-10 19:51:46.000000000 -0400
@@ -5,5 +5,6 @@
 Name: librdkafka++-static
 Description: The Apache Kafka C/C++ library (static)
 Version: 1.5.0
+Requires: rdkafka-static
 Cflags: -I${includedir}
-Libs: -L${libdir} ${libdir}/librdkafka++-static.a -lm -lsasl2 -ldl -lpthread
+Libs: -L${libdir} ${pc_sysrootdir}${libdir}/librdkafka++-static.a -lm -lsasl2 -ldl -lpthread

diff -ur old/rdkafka++.pc new/rdkafka++.pc
--- old/rdkafka++.pc	2020-09-10 19:54:54.000000000 -0400
+++ new/rdkafka++.pc	2020-09-10 19:51:46.000000000 -0400
@@ -5,6 +5,7 @@
 Name: librdkafka++
 Description: The Apache Kafka C/C++ library
 Version: 1.5.0
+Requires.private: rdkafka
 Cflags: -I${includedir}
 Libs: -L${libdir} -lrdkafka++
-Libs.private: -L../src -lrdkafka
+Libs.private: -lm -lsasl2 -ldl -lpthread

diff -ur old/rdkafka-static.pc new/rdkafka-static.pc
--- old/rdkafka-static.pc	2020-09-10 19:54:50.000000000 -0400
+++ new/rdkafka-static.pc	2020-09-10 19:51:50.000000000 -0400
@@ -5,5 +5,6 @@
 Name: librdkafka-static
 Description: The Apache Kafka C/C++ library (static)
 Version: 1.5.0
+Requires:
 Cflags: -I${includedir}
-Libs: -L${libdir} ${libdir}/librdkafka-static.a -lm -lsasl2 -ldl -lpthread
+Libs: -L${libdir} ${pc_sysrootdir}${libdir}/librdkafka-static.a -lm -lsasl2 -ldl -lpthread

diff -ur old/rdkafka.pc new/rdkafka.pc
--- old/rdkafka.pc	2020-09-10 19:54:50.000000000 -0400
+++ new/rdkafka.pc	2020-09-10 19:51:50.000000000 -0400
@@ -5,6 +5,7 @@
 Name: librdkafka
 Description: The Apache Kafka C/C++ library
 Version: 1.5.0
+Requires.private: zlib libcrypto libssl libzstd
 Cflags: -I${includedir}
 Libs: -L${libdir} -lrdkafka
-Libs.private: -lm /usr/local/Cellar/zstd/1.4.5//lib/libzstd.a -lsasl2 /usr/local/Cellar/openssl@1.1/1.1.1g/lib/libssl.a /usr/local/Cellar/openssl@1.1/1.1.1g/lib/libcrypto.a /usr/local/Cellar/zlib/1.2.11/lib/libz.a -ldl -lpthread
+Libs.private: -lm -lsasl2 -ldl -lpthread
And if I hide e.g. libzstd.a I get
diff -ur old/rdkafka-static.pc new/rdkafka-static.pc
--- old/rdkafka-static.pc	2020-09-10 19:54:50.000000000 -0400
+++ new/rdkafka-static.pc	2020-09-10 20:07:20.000000000 -0400
@@ -5,5 +5,6 @@
 Name: librdkafka-static
 Description: The Apache Kafka C/C++ library (static)
 Version: 1.5.0
+Requires: libzstd
 Cflags: -I${includedir}
-Libs: -L${libdir} ${libdir}/librdkafka-static.a -lm -lsasl2 -ldl -lpthread
+Libs: -L${libdir} ${pc_sysrootdir}${libdir}/librdkafka-static.a -lm -lsasl2 -ldl -lpthread

diff -ur old/rdkafka.pc new/rdkafka.pc
--- old/rdkafka.pc	2020-09-10 19:54:50.000000000 -0400
+++ new/rdkafka.pc	2020-09-10 20:07:20.000000000 -0400
@@ -5,6 +5,7 @@
 Name: librdkafka
 Description: The Apache Kafka C/C++ library
 Version: 1.5.0
+Requires.private: zlib libcrypto libssl libzstd
 Cflags: -I${includedir}
 Libs: -L${libdir} -lrdkafka
-Libs.private: -lm /usr/local/Cellar/zstd/1.4.5//lib/libzstd.a -lsasl2 /usr/local/Cellar/openssl@1.1/1.1.1g/lib/libssl.a /usr/local/Cellar/openssl@1.1/1.1.1g/lib/libcrypto.a /usr/local/Cellar/zlib/1.2.11/lib/libz.a -ldl -lpthread
+Libs.private: -lm -lsasl2 -ldl -lpthread
Note that libzstd is marked as a public dependency for rdkafka-static.pc since it's not bundled in librdkafka-static.a, but still as a private dependency for rdkafka.pc. This is still correct as librdkafka.so encodes its own dependency on zstd and we never expose zstd types, but librdkafka.a (not the static bundle) still needs to link against libzstd.a.",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2659,2019-12-17T19:10:36Z,2020-09-14T22:47:37Z,2020-09-14T22:47:50Z,MERGED,True,22,11,4,https://github.com/gridaphobe,Improve pkg-config support,3,[],https://github.com/edenhill/librdkafka/pull/2659,https://github.com/gridaphobe,12,https://github.com/edenhill/librdkafka/pull/2659#issuecomment-692275114,"Add required libraries (zlib, lz4, zstd, openssl, sasl2) to the
pkg-config Requires field instead of to Libs if they were
discovered via pkg-config. This allows pkg-config (and other tools
built on top of the pkg-config metadata) to see librdkafka's full
dependency graph.


Prefix full paths to librdkafka.a with the pkg-config variable
${pc_sysrootdir}. pkg-config automatically applies this prefix to
paths passed to -I and -L, but nowhere else. This means that the
previous link line in the -static.pc files is inconsistent, and
would expand to
-L${pc_sysrootdir}${libdir} ${libdir}/librdkafka.a


#include <stddef.h> when checking for sasl so that detection via
pkg-config works. This is needed as sasl.h references some types
that are defined in stddef.h.","I just did another full rebuild of our internal packages with the new pkgconfig files, and it looks good to me ",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2659,2019-12-17T19:10:36Z,2020-09-14T22:47:37Z,2020-09-14T22:47:50Z,MERGED,True,22,11,4,https://github.com/gridaphobe,Improve pkg-config support,3,[],https://github.com/edenhill/librdkafka/pull/2659,https://github.com/edenhill,13,https://github.com/edenhill/librdkafka/pull/2659#issuecomment-692357988,"Add required libraries (zlib, lz4, zstd, openssl, sasl2) to the
pkg-config Requires field instead of to Libs if they were
discovered via pkg-config. This allows pkg-config (and other tools
built on top of the pkg-config metadata) to see librdkafka's full
dependency graph.


Prefix full paths to librdkafka.a with the pkg-config variable
${pc_sysrootdir}. pkg-config automatically applies this prefix to
paths passed to -I and -L, but nowhere else. This means that the
previous link line in the -static.pc files is inconsistent, and
would expand to
-L${pc_sysrootdir}${libdir} ${libdir}/librdkafka.a


#include <stddef.h> when checking for sasl so that detection via
pkg-config works. This is needed as sasl.h references some types
that are defined in stddef.h.",Thank you @gridaphobe !,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2660,2019-12-19T12:02:03Z,2020-02-05T08:49:08Z,2020-02-05T15:13:07Z,MERGED,True,9,1,3,https://github.com/fboranek,fix: cleanup conf object if fails creating producer,1,[],https://github.com/edenhill/librdkafka/pull/2660,https://github.com/fboranek,1,https://github.com/edenhill/librdkafka/pull/2660,I notice there is memory leak in RdKafka::Producer::create if rd_kafka_new returns NULL.,I notice there is memory leak in RdKafka::Producer::create if rd_kafka_new returns NULL.,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2660,2019-12-19T12:02:03Z,2020-02-05T08:49:08Z,2020-02-05T15:13:07Z,MERGED,True,9,1,3,https://github.com/fboranek,fix: cleanup conf object if fails creating producer,1,[],https://github.com/edenhill/librdkafka/pull/2660,https://github.com/edenhill,2,https://github.com/edenhill/librdkafka/pull/2660#issuecomment-567494684,I notice there is memory leak in RdKafka::Producer::create if rd_kafka_new returns NULL.,"Great! Can you fix the same issues in ConsumerImpl.cpp, KafkaConsumerImpl.cpp?",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2660,2019-12-19T12:02:03Z,2020-02-05T08:49:08Z,2020-02-05T15:13:07Z,MERGED,True,9,1,3,https://github.com/fboranek,fix: cleanup conf object if fails creating producer,1,[],https://github.com/edenhill/librdkafka/pull/2660,https://github.com/fboranek,3,https://github.com/edenhill/librdkafka/pull/2660#issuecomment-567511750,I notice there is memory leak in RdKafka::Producer::create if rd_kafka_new returns NULL.,You are right. The same issue. Done.,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2660,2019-12-19T12:02:03Z,2020-02-05T08:49:08Z,2020-02-05T15:13:07Z,MERGED,True,9,1,3,https://github.com/fboranek,fix: cleanup conf object if fails creating producer,1,[],https://github.com/edenhill/librdkafka/pull/2660,https://github.com/edenhill,4,https://github.com/edenhill/librdkafka/pull/2660#issuecomment-582302709,I notice there is memory leak in RdKafka::Producer::create if rd_kafka_new returns NULL.,Thank you!,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2661,2019-12-19T23:56:36Z,2020-03-04T20:17:43Z,2020-03-04T20:17:43Z,CLOSED,False,58,12,6,https://github.com/mhowlett,Store offsets corresponding to ctrl messages,1,[],https://github.com/edenhill/librdkafka/pull/2661,https://github.com/mhowlett,1,https://github.com/edenhill/librdkafka/pull/2661,resolves: #2539,resolves: #2539,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2661,2019-12-19T23:56:36Z,2020-03-04T20:17:43Z,2020-03-04T20:17:43Z,CLOSED,False,58,12,6,https://github.com/mhowlett,Store offsets corresponding to ctrl messages,1,[],https://github.com/edenhill/librdkafka/pull/2661,https://github.com/edenhill,2,https://github.com/edenhill/librdkafka/pull/2661#issuecomment-576144709,resolves: #2539,Can you rebase this on latest txns branch?,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2661,2019-12-19T23:56:36Z,2020-03-04T20:17:43Z,2020-03-04T20:17:43Z,CLOSED,False,58,12,6,https://github.com/mhowlett,Store offsets corresponding to ctrl messages,1,[],https://github.com/edenhill/librdkafka/pull/2661,https://github.com/edenhill,3,https://github.com/edenhill/librdkafka/pull/2661#issuecomment-582301815,resolves: #2539,Can you rebase this on latest master?,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2661,2019-12-19T23:56:36Z,2020-03-04T20:17:43Z,2020-03-04T20:17:43Z,CLOSED,False,58,12,6,https://github.com/mhowlett,Store offsets corresponding to ctrl messages,1,[],https://github.com/edenhill/librdkafka/pull/2661,https://github.com/mhowlett,4,https://github.com/edenhill/librdkafka/pull/2661#issuecomment-587699508,resolves: #2539,"needs to go in in order for the v1.4.0 .net integration tests to pass. IIRC you'll need to check carefully, you'll have opinions.",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2661,2019-12-19T23:56:36Z,2020-03-04T20:17:43Z,2020-03-04T20:17:43Z,CLOSED,False,58,12,6,https://github.com/mhowlett,Store offsets corresponding to ctrl messages,1,[],https://github.com/edenhill/librdkafka/pull/2661,https://github.com/edenhill,5,https://github.com/edenhill/librdkafka/pull/2661#issuecomment-594812435,resolves: #2539,Continued in #2743,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2664,2019-12-26T03:08:56Z,2020-01-08T16:11:11Z,2020-01-08T16:11:17Z,MERGED,True,1,1,1,https://github.com/jediMunees,Issue-2368: Correct message timestamp type,1,[],https://github.com/edenhill/librdkafka/pull/2664,https://github.com/jediMunees,1,https://github.com/edenhill/librdkafka/pull/2664,"For topic configured with message.timestamp.type=LogAppendTime, type should be RD_KAFKA_TIMESTAMP_LOG_APPEND_TIME.
Issue reference: #2368","For topic configured with message.timestamp.type=LogAppendTime, type should be RD_KAFKA_TIMESTAMP_LOG_APPEND_TIME.
Issue reference: #2368",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2664,2019-12-26T03:08:56Z,2020-01-08T16:11:11Z,2020-01-08T16:11:17Z,MERGED,True,1,1,1,https://github.com/jediMunees,Issue-2368: Correct message timestamp type,1,[],https://github.com/edenhill/librdkafka/pull/2664,https://github.com/jediMunees,2,https://github.com/edenhill/librdkafka/pull/2664#issuecomment-568957170,"For topic configured with message.timestamp.type=LogAppendTime, type should be RD_KAFKA_TIMESTAMP_LOG_APPEND_TIME.
Issue reference: #2368",PR request for this issue: #2368,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2664,2019-12-26T03:08:56Z,2020-01-08T16:11:11Z,2020-01-08T16:11:17Z,MERGED,True,1,1,1,https://github.com/jediMunees,Issue-2368: Correct message timestamp type,1,[],https://github.com/edenhill/librdkafka/pull/2664,https://github.com/edenhill,3,https://github.com/edenhill/librdkafka/pull/2664#issuecomment-572140495,"For topic configured with message.timestamp.type=LogAppendTime, type should be RD_KAFKA_TIMESTAMP_LOG_APPEND_TIME.
Issue reference: #2368","Perfect, thank you!",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2670,2020-01-05T15:55:22Z,2020-01-07T01:18:42Z,2020-01-07T01:18:42Z,CLOSED,False,1,1,1,https://github.com/neptoess,Use _WIN32 preprocessor macro instead of _MSC_VER,1,[],https://github.com/edenhill/librdkafka/pull/2670,https://github.com/neptoess,1,https://github.com/edenhill/librdkafka/pull/2670,"_MSC_VER is only defined by MSVC, but other compilers, e.g. gcc from MinGW, can also be used to build librdkafka on Windows. _WIN32 is not compiler-specific.","_MSC_VER is only defined by MSVC, but other compilers, e.g. gcc from MinGW, can also be used to build librdkafka on Windows. _WIN32 is not compiler-specific.",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2670,2020-01-05T15:55:22Z,2020-01-07T01:18:42Z,2020-01-07T01:18:42Z,CLOSED,False,1,1,1,https://github.com/neptoess,Use _WIN32 preprocessor macro instead of _MSC_VER,1,[],https://github.com/edenhill/librdkafka/pull/2670,https://github.com/neptoess,2,https://github.com/edenhill/librdkafka/pull/2670#issuecomment-571088755,"_MSC_VER is only defined by MSVC, but other compilers, e.g. gcc from MinGW, can also be used to build librdkafka on Windows. _WIN32 is not compiler-specific.","It looks like #2553 is already pending and covers a lot more cases of _MSC_VER in the code. Im not using a cross compiler. I only went far enough to allow cgo to link to rdkafka when I reference confluent-kafka-go. However, it looks like the #ifdef checks in that PR are still compiler-based, instead of OS-target based. I still believe that PR may be the smarter one to merge, but I will propose that the author use the _WIN32 macro instead. If he agrees, I will close this PR.",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2673,2020-01-06T11:53:52Z,2020-02-05T08:45:54Z,2020-02-05T08:45:54Z,CLOSED,False,1,1,1,https://github.com/sknop,Corrected the reference to the broker/topic setting,1,[],https://github.com/edenhill/librdkafka/pull/2673,https://github.com/sknop,1,https://github.com/edenhill/librdkafka/pull/2673,"It is message.max.bytes, not max.message.bytes","It is message.max.bytes, not max.message.bytes",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2675,2020-01-06T17:46:48Z,2020-01-08T15:32:40Z,2020-01-08T15:32:40Z,MERGED,True,7,1,3,https://github.com/mhowlett,Added ERR__APPLICATION,1,[],https://github.com/edenhill/librdkafka/pull/2675,https://github.com/mhowlett,1,https://github.com/edenhill/librdkafka/pull/2675,"Along with adding support to the .NET binding for transactions, I'm adding a 'word count' example, which includes significant logic in the rebalance handlers. This has highlighted the need (or more big convenience really) for exceptions to be propagated through to the initiating function call (currently any unhandled exception in a handler will terminate the process). In order to do that, I need an error code to associate with application generated errors.","Along with adding support to the .NET binding for transactions, I'm adding a 'word count' example, which includes significant logic in the rebalance handlers. This has highlighted the need (or more big convenience really) for exceptions to be propagated through to the initiating function call (currently any unhandled exception in a handler will terminate the process). In order to do that, I need an error code to associate with application generated errors.",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2676,2020-01-06T22:35:01Z,2021-11-18T14:27:50Z,2021-11-18T14:36:40Z,MERGED,True,2586,23,17,https://github.com/emasab,"ACL Admin Apis: CreateAcls, DescribeAcls, DeleteAcls",3,[],https://github.com/edenhill/librdkafka/pull/2676,https://github.com/emasab,1,https://github.com/edenhill/librdkafka/pull/2676,"Hi, I've implemented the ACL admin apis.
The things that I've checked are:

regression tests with versions 0 and 1 of the API client side against versions 0 or 1 server side.
memory leaks with the valgrind mode
removed all warnings
formatted the changed code with the clang-format that I've created following the ""librdkafka C style guide"", using git clang-format-9 
added a make format target to format all the c, cpp and h files (but not run it)

In order to make the 0081-admin test work this change must be applied to trivup:
edenhill/trivup#10
I've tried to follow the existing coding style, naming and design but given that the change is not trivial there could be things that are missing or need some change. The formatting rules are open to any change that is needed, I haven't found a rule that adds spaces before parenthesis always except in function calls.","Hi, I've implemented the ACL admin apis.
The things that I've checked are:

regression tests with versions 0 and 1 of the API client side against versions 0 or 1 server side.
memory leaks with the valgrind mode
removed all warnings
formatted the changed code with the clang-format that I've created following the ""librdkafka C style guide"", using git clang-format-9 
added a make format target to format all the c, cpp and h files (but not run it)

In order to make the 0081-admin test work this change must be applied to trivup:
edenhill/trivup#10
I've tried to follow the existing coding style, naming and design but given that the change is not trivial there could be things that are missing or need some change. The formatting rules are open to any change that is needed, I haven't found a rule that adds spaces before parenthesis always except in function calls.",True,"{'THUMBS_UP': ['https://github.com/edenhill', 'https://github.com/ldom', 'https://github.com/Manicben', 'https://github.com/0x003e'], 'HEART': ['https://github.com/edenhill'], 'HOORAY': ['https://github.com/RPG-18']}"
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2676,2020-01-06T22:35:01Z,2021-11-18T14:27:50Z,2021-11-18T14:36:40Z,MERGED,True,2586,23,17,https://github.com/emasab,"ACL Admin Apis: CreateAcls, DescribeAcls, DeleteAcls",3,[],https://github.com/edenhill/librdkafka/pull/2676,https://github.com/emasab,2,https://github.com/edenhill/librdkafka/pull/2676#issuecomment-586494540,"Hi, I've implemented the ACL admin apis.
The things that I've checked are:

regression tests with versions 0 and 1 of the API client side against versions 0 or 1 server side.
memory leaks with the valgrind mode
removed all warnings
formatted the changed code with the clang-format that I've created following the ""librdkafka C style guide"", using git clang-format-9 
added a make format target to format all the c, cpp and h files (but not run it)

In order to make the 0081-admin test work this change must be applied to trivup:
edenhill/trivup#10
I've tried to follow the existing coding style, naming and design but given that the change is not trivial there could be things that are missing or need some change. The formatting rules are open to any change that is needed, I haven't found a rule that adds spaces before parenthesis always except in function calls.",rebased,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2676,2020-01-06T22:35:01Z,2021-11-18T14:27:50Z,2021-11-18T14:36:40Z,MERGED,True,2586,23,17,https://github.com/emasab,"ACL Admin Apis: CreateAcls, DescribeAcls, DeleteAcls",3,[],https://github.com/edenhill/librdkafka/pull/2676,https://github.com/emasab,3,https://github.com/edenhill/librdkafka/pull/2676#issuecomment-586597667,"Hi, I've implemented the ACL admin apis.
The things that I've checked are:

regression tests with versions 0 and 1 of the API client side against versions 0 or 1 server side.
memory leaks with the valgrind mode
removed all warnings
formatted the changed code with the clang-format that I've created following the ""librdkafka C style guide"", using git clang-format-9 
added a make format target to format all the c, cpp and h files (but not run it)

In order to make the 0081-admin test work this change must be applied to trivup:
edenhill/trivup#10
I've tried to follow the existing coding style, naming and design but given that the change is not trivial there could be things that are missing or need some change. The formatting rules are open to any change that is needed, I haven't found a rule that adds spaces before parenthesis always except in function calls.","Did you have time to check this one?
Best",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2676,2020-01-06T22:35:01Z,2021-11-18T14:27:50Z,2021-11-18T14:36:40Z,MERGED,True,2586,23,17,https://github.com/emasab,"ACL Admin Apis: CreateAcls, DescribeAcls, DeleteAcls",3,[],https://github.com/edenhill/librdkafka/pull/2676,https://github.com/emasab,4,https://github.com/edenhill/librdkafka/pull/2676#issuecomment-601120889,"Hi, I've implemented the ACL admin apis.
The things that I've checked are:

regression tests with versions 0 and 1 of the API client side against versions 0 or 1 server side.
memory leaks with the valgrind mode
removed all warnings
formatted the changed code with the clang-format that I've created following the ""librdkafka C style guide"", using git clang-format-9 
added a make format target to format all the c, cpp and h files (but not run it)

In order to make the 0081-admin test work this change must be applied to trivup:
edenhill/trivup#10
I've tried to follow the existing coding style, naming and design but given that the change is not trivial there could be things that are missing or need some change. The formatting rules are open to any change that is needed, I haven't found a rule that adds spaces before parenthesis always except in function calls.","rebased, any plans for this PR?",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2676,2020-01-06T22:35:01Z,2021-11-18T14:27:50Z,2021-11-18T14:36:40Z,MERGED,True,2586,23,17,https://github.com/emasab,"ACL Admin Apis: CreateAcls, DescribeAcls, DeleteAcls",3,[],https://github.com/edenhill/librdkafka/pull/2676,https://github.com/edenhill,5,https://github.com/edenhill/librdkafka/pull/2676#issuecomment-601127220,"Hi, I've implemented the ACL admin apis.
The things that I've checked are:

regression tests with versions 0 and 1 of the API client side against versions 0 or 1 server side.
memory leaks with the valgrind mode
removed all warnings
formatted the changed code with the clang-format that I've created following the ""librdkafka C style guide"", using git clang-format-9 
added a make format target to format all the c, cpp and h files (but not run it)

In order to make the 0081-admin test work this change must be applied to trivup:
edenhill/trivup#10
I've tried to follow the existing coding style, naming and design but given that the change is not trivial there could be things that are missing or need some change. The formatting rules are open to any change that is needed, I haven't found a rule that adds spaces before parenthesis always except in function calls.","Thank you for your patience, we'll be looking at this PR after v1.4.0 is released (soon!).",True,"{'HOORAY': ['https://github.com/emasab', 'https://github.com/noderat', 'https://github.com/leoluz']}"
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2676,2020-01-06T22:35:01Z,2021-11-18T14:27:50Z,2021-11-18T14:36:40Z,MERGED,True,2586,23,17,https://github.com/emasab,"ACL Admin Apis: CreateAcls, DescribeAcls, DeleteAcls",3,[],https://github.com/edenhill/librdkafka/pull/2676,https://github.com/emasab,6,https://github.com/edenhill/librdkafka/pull/2676#issuecomment-601135859,"Hi, I've implemented the ACL admin apis.
The things that I've checked are:

regression tests with versions 0 and 1 of the API client side against versions 0 or 1 server side.
memory leaks with the valgrind mode
removed all warnings
formatted the changed code with the clang-format that I've created following the ""librdkafka C style guide"", using git clang-format-9 
added a make format target to format all the c, cpp and h files (but not run it)

In order to make the 0081-admin test work this change must be applied to trivup:
edenhill/trivup#10
I've tried to follow the existing coding style, naming and design but given that the change is not trivial there could be things that are missing or need some change. The formatting rules are open to any change that is needed, I haven't found a rule that adds spaces before parenthesis always except in function calls.",Thanks for the update!,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2676,2020-01-06T22:35:01Z,2021-11-18T14:27:50Z,2021-11-18T14:36:40Z,MERGED,True,2586,23,17,https://github.com/emasab,"ACL Admin Apis: CreateAcls, DescribeAcls, DeleteAcls",3,[],https://github.com/edenhill/librdkafka/pull/2676,https://github.com/emasab,7,https://github.com/edenhill/librdkafka/pull/2676#issuecomment-646288093,"Hi, I've implemented the ACL admin apis.
The things that I've checked are:

regression tests with versions 0 and 1 of the API client side against versions 0 or 1 server side.
memory leaks with the valgrind mode
removed all warnings
formatted the changed code with the clang-format that I've created following the ""librdkafka C style guide"", using git clang-format-9 
added a make format target to format all the c, cpp and h files (but not run it)

In order to make the 0081-admin test work this change must be applied to trivup:
edenhill/trivup#10
I've tried to follow the existing coding style, naming and design but given that the change is not trivial there could be things that are missing or need some change. The formatting rules are open to any change that is needed, I haven't found a rule that adds spaces before parenthesis always except in function calls.","rebased and now, with the latest trivup, the tests are passing in travis",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2676,2020-01-06T22:35:01Z,2021-11-18T14:27:50Z,2021-11-18T14:36:40Z,MERGED,True,2586,23,17,https://github.com/emasab,"ACL Admin Apis: CreateAcls, DescribeAcls, DeleteAcls",3,[],https://github.com/edenhill/librdkafka/pull/2676,https://github.com/emasab,8,https://github.com/edenhill/librdkafka/pull/2676#issuecomment-748068266,"Hi, I've implemented the ACL admin apis.
The things that I've checked are:

regression tests with versions 0 and 1 of the API client side against versions 0 or 1 server side.
memory leaks with the valgrind mode
removed all warnings
formatted the changed code with the clang-format that I've created following the ""librdkafka C style guide"", using git clang-format-9 
added a make format target to format all the c, cpp and h files (but not run it)

In order to make the 0081-admin test work this change must be applied to trivup:
edenhill/trivup#10
I've tried to follow the existing coding style, naming and design but given that the change is not trivial there could be things that are missing or need some change. The formatting rules are open to any change that is needed, I haven't found a rule that adds spaces before parenthesis always except in function calls.","Hello, I've rebased this one after the addition of the RD_KAFKA_OP_DELETERECORDS, RD_KAFKA_OP_DELETEGROUPS,               RD_KAFKA_OP_DELETECONSUMERGROUPOFFSETS, RD_KAFKA_OP_ADMIN_FANOUT. There where different conflicts, even some that could cause bugs, for example same values of RD_KAFKA_EVENT_*_RESULT (I've changed those values). Please merge soon. If not in master, in the branch of the next release. Thanks",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2676,2020-01-06T22:35:01Z,2021-11-18T14:27:50Z,2021-11-18T14:36:40Z,MERGED,True,2586,23,17,https://github.com/emasab,"ACL Admin Apis: CreateAcls, DescribeAcls, DeleteAcls",3,[],https://github.com/edenhill/librdkafka/pull/2676,https://github.com/edenhill,9,https://github.com/edenhill/librdkafka/pull/2676#issuecomment-758500007,"Hi, I've implemented the ACL admin apis.
The things that I've checked are:

regression tests with versions 0 and 1 of the API client side against versions 0 or 1 server side.
memory leaks with the valgrind mode
removed all warnings
formatted the changed code with the clang-format that I've created following the ""librdkafka C style guide"", using git clang-format-9 
added a make format target to format all the c, cpp and h files (but not run it)

In order to make the 0081-admin test work this change must be applied to trivup:
edenhill/trivup#10
I've tried to follow the existing coding style, naming and design but given that the change is not trivial there could be things that are missing or need some change. The formatting rules are open to any change that is needed, I haven't found a rule that adds spaces before parenthesis always except in function calls.","Will address this after v1.6.0 is released, thanks for your patience.",True,{'THUMBS_UP': ['https://github.com/emasab']}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2676,2020-01-06T22:35:01Z,2021-11-18T14:27:50Z,2021-11-18T14:36:40Z,MERGED,True,2586,23,17,https://github.com/emasab,"ACL Admin Apis: CreateAcls, DescribeAcls, DeleteAcls",3,[],https://github.com/edenhill/librdkafka/pull/2676,https://github.com/yanivmn,10,https://github.com/edenhill/librdkafka/pull/2676#issuecomment-800073219,"Hi, I've implemented the ACL admin apis.
The things that I've checked are:

regression tests with versions 0 and 1 of the API client side against versions 0 or 1 server side.
memory leaks with the valgrind mode
removed all warnings
formatted the changed code with the clang-format that I've created following the ""librdkafka C style guide"", using git clang-format-9 
added a make format target to format all the c, cpp and h files (but not run it)

In order to make the 0081-admin test work this change must be applied to trivup:
edenhill/trivup#10
I've tried to follow the existing coding style, naming and design but given that the change is not trivial there could be things that are missing or need some change. The formatting rules are open to any change that is needed, I haven't found a rule that adds spaces before parenthesis always except in function calls.","Hi @edenhill, what is your plan regarding this PR?",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2676,2020-01-06T22:35:01Z,2021-11-18T14:27:50Z,2021-11-18T14:36:40Z,MERGED,True,2586,23,17,https://github.com/emasab,"ACL Admin Apis: CreateAcls, DescribeAcls, DeleteAcls",3,[],https://github.com/edenhill/librdkafka/pull/2676,https://github.com/edenhill,11,https://github.com/edenhill/librdkafka/pull/2676#issuecomment-829896148,"Hi, I've implemented the ACL admin apis.
The things that I've checked are:

regression tests with versions 0 and 1 of the API client side against versions 0 or 1 server side.
memory leaks with the valgrind mode
removed all warnings
formatted the changed code with the clang-format that I've created following the ""librdkafka C style guide"", using git clang-format-9 
added a make format target to format all the c, cpp and h files (but not run it)

In order to make the 0081-admin test work this change must be applied to trivup:
edenhill/trivup#10
I've tried to follow the existing coding style, naming and design but given that the change is not trivial there could be things that are missing or need some change. The formatting rules are open to any change that is needed, I haven't found a rule that adds spaces before parenthesis always except in function calls.","Sorry for the long delay, too much stuff to do, I'll revisit this after the 1.7.0 release.",True,"{'EYES': ['https://github.com/0x003e', 'https://github.com/dimpavloff']}"
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2676,2020-01-06T22:35:01Z,2021-11-18T14:27:50Z,2021-11-18T14:36:40Z,MERGED,True,2586,23,17,https://github.com/emasab,"ACL Admin Apis: CreateAcls, DescribeAcls, DeleteAcls",3,[],https://github.com/edenhill/librdkafka/pull/2676,https://github.com/dimpavloff,12,https://github.com/edenhill/librdkafka/pull/2676#issuecomment-904597990,"Hi, I've implemented the ACL admin apis.
The things that I've checked are:

regression tests with versions 0 and 1 of the API client side against versions 0 or 1 server side.
memory leaks with the valgrind mode
removed all warnings
formatted the changed code with the clang-format that I've created following the ""librdkafka C style guide"", using git clang-format-9 
added a make format target to format all the c, cpp and h files (but not run it)

In order to make the 0081-admin test work this change must be applied to trivup:
edenhill/trivup#10
I've tried to follow the existing coding style, naming and design but given that the change is not trivial there could be things that are missing or need some change. The formatting rules are open to any change that is needed, I haven't found a rule that adds spaces before parenthesis always except in function calls.","Hi, are there any plans to merge this for 1.8 or 1.9?",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2676,2020-01-06T22:35:01Z,2021-11-18T14:27:50Z,2021-11-18T14:36:40Z,MERGED,True,2586,23,17,https://github.com/emasab,"ACL Admin Apis: CreateAcls, DescribeAcls, DeleteAcls",3,[],https://github.com/edenhill/librdkafka/pull/2676,https://github.com/emasab,13,https://github.com/edenhill/librdkafka/pull/2676#issuecomment-907846910,"Hi, I've implemented the ACL admin apis.
The things that I've checked are:

regression tests with versions 0 and 1 of the API client side against versions 0 or 1 server side.
memory leaks with the valgrind mode
removed all warnings
formatted the changed code with the clang-format that I've created following the ""librdkafka C style guide"", using git clang-format-9 
added a make format target to format all the c, cpp and h files (but not run it)

In order to make the 0081-admin test work this change must be applied to trivup:
edenhill/trivup#10
I've tried to follow the existing coding style, naming and design but given that the change is not trivial there could be things that are missing or need some change. The formatting rules are open to any change that is needed, I haven't found a rule that adds spaces before parenthesis always except in function calls.","Hi, are you looking to auto generate the code from the protocol specification? In that case do you want to merge this one or will it be replaced by auto generated code?",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2676,2020-01-06T22:35:01Z,2021-11-18T14:27:50Z,2021-11-18T14:36:40Z,MERGED,True,2586,23,17,https://github.com/emasab,"ACL Admin Apis: CreateAcls, DescribeAcls, DeleteAcls",3,[],https://github.com/edenhill/librdkafka/pull/2676,https://github.com/SivolcC,14,https://github.com/edenhill/librdkafka/pull/2676#issuecomment-943206923,"Hi, I've implemented the ACL admin apis.
The things that I've checked are:

regression tests with versions 0 and 1 of the API client side against versions 0 or 1 server side.
memory leaks with the valgrind mode
removed all warnings
formatted the changed code with the clang-format that I've created following the ""librdkafka C style guide"", using git clang-format-9 
added a make format target to format all the c, cpp and h files (but not run it)

In order to make the 0081-admin test work this change must be applied to trivup:
edenhill/trivup#10
I've tried to follow the existing coding style, naming and design but given that the change is not trivial there could be things that are missing or need some change. The formatting rules are open to any change that is needed, I haven't found a rule that adds spaces before parenthesis always except in function calls.","Hello everyone,
Thanks for all the work you are putting in. I am very much interested in this feature to be available (and to be implemented afterwards in confluent-kafka go and python clients).
Is there any chance that we can expect it to be released with 1.8 or 1.9?
Cheers,",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2676,2020-01-06T22:35:01Z,2021-11-18T14:27:50Z,2021-11-18T14:36:40Z,MERGED,True,2586,23,17,https://github.com/emasab,"ACL Admin Apis: CreateAcls, DescribeAcls, DeleteAcls",3,[],https://github.com/edenhill/librdkafka/pull/2676,https://github.com/treydempsey,15,https://github.com/edenhill/librdkafka/pull/2676#issuecomment-970789017,"Hi, I've implemented the ACL admin apis.
The things that I've checked are:

regression tests with versions 0 and 1 of the API client side against versions 0 or 1 server side.
memory leaks with the valgrind mode
removed all warnings
formatted the changed code with the clang-format that I've created following the ""librdkafka C style guide"", using git clang-format-9 
added a make format target to format all the c, cpp and h files (but not run it)

In order to make the 0081-admin test work this change must be applied to trivup:
edenhill/trivup#10
I've tried to follow the existing coding style, naming and design but given that the change is not trivial there could be things that are missing or need some change. The formatting rules are open to any change that is needed, I haven't found a rule that adds spaces before parenthesis always except in function calls.",Can you share with us why this has been delayed so long? It seems like a high quality addition to the code base and something I and many others want.,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2676,2020-01-06T22:35:01Z,2021-11-18T14:27:50Z,2021-11-18T14:36:40Z,MERGED,True,2586,23,17,https://github.com/emasab,"ACL Admin Apis: CreateAcls, DescribeAcls, DeleteAcls",3,[],https://github.com/edenhill/librdkafka/pull/2676,https://github.com/edenhill,16,https://github.com/edenhill/librdkafka/pull/2676#issuecomment-971734189,"Hi, I've implemented the ACL admin apis.
The things that I've checked are:

regression tests with versions 0 and 1 of the API client side against versions 0 or 1 server side.
memory leaks with the valgrind mode
removed all warnings
formatted the changed code with the clang-format that I've created following the ""librdkafka C style guide"", using git clang-format-9 
added a make format target to format all the c, cpp and h files (but not run it)

In order to make the 0081-admin test work this change must be applied to trivup:
edenhill/trivup#10
I've tried to follow the existing coding style, naming and design but given that the change is not trivial there could be things that are missing or need some change. The formatting rules are open to any change that is needed, I haven't found a rule that adds spaces before parenthesis always except in function calls.",I took the liberty to rebase this on latest master and fix the style issues induced by the recent introduction of clang-formatting in the project.,True,{'THUMBS_UP': ['https://github.com/emasab']}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2676,2020-01-06T22:35:01Z,2021-11-18T14:27:50Z,2021-11-18T14:36:40Z,MERGED,True,2586,23,17,https://github.com/emasab,"ACL Admin Apis: CreateAcls, DescribeAcls, DeleteAcls",3,[],https://github.com/edenhill/librdkafka/pull/2676,https://github.com/edenhill,17,https://github.com/edenhill/librdkafka/pull/2676#issuecomment-971875851,"Hi, I've implemented the ACL admin apis.
The things that I've checked are:

regression tests with versions 0 and 1 of the API client side against versions 0 or 1 server side.
memory leaks with the valgrind mode
removed all warnings
formatted the changed code with the clang-format that I've created following the ""librdkafka C style guide"", using git clang-format-9 
added a make format target to format all the c, cpp and h files (but not run it)

In order to make the 0081-admin test work this change must be applied to trivup:
edenhill/trivup#10
I've tried to follow the existing coding style, naming and design but given that the change is not trivial there could be things that are missing or need some change. The formatting rules are open to any change that is needed, I haven't found a rule that adds spaces before parenthesis always except in function calls.","Will review soon. This is targeted for the next release.
Sorry for the delay..again, always.",True,"{'THUMBS_UP': ['https://github.com/RPG-18', 'https://github.com/emasab', 'https://github.com/SivolcC']}"
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2676,2020-01-06T22:35:01Z,2021-11-18T14:27:50Z,2021-11-18T14:36:40Z,MERGED,True,2586,23,17,https://github.com/emasab,"ACL Admin Apis: CreateAcls, DescribeAcls, DeleteAcls",3,[],https://github.com/edenhill/librdkafka/pull/2676,https://github.com/emasab,18,https://github.com/edenhill/librdkafka/pull/2676#issuecomment-972174339,"Hi, I've implemented the ACL admin apis.
The things that I've checked are:

regression tests with versions 0 and 1 of the API client side against versions 0 or 1 server side.
memory leaks with the valgrind mode
removed all warnings
formatted the changed code with the clang-format that I've created following the ""librdkafka C style guide"", using git clang-format-9 
added a make format target to format all the c, cpp and h files (but not run it)

In order to make the 0081-admin test work this change must be applied to trivup:
edenhill/trivup#10
I've tried to follow the existing coding style, naming and design but given that the change is not trivial there could be things that are missing or need some change. The formatting rules are open to any change that is needed, I haven't found a rule that adds spaces before parenthesis always except in function calls.","Thanks Magnus, don't worry I understand that as this is already available in the Java client and it's for administrative operations, has lower priority than other core apis. Hope this time it'll make it into the release, let me know if there are changes to make.",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2676,2020-01-06T22:35:01Z,2021-11-18T14:27:50Z,2021-11-18T14:36:40Z,MERGED,True,2586,23,17,https://github.com/emasab,"ACL Admin Apis: CreateAcls, DescribeAcls, DeleteAcls",3,[],https://github.com/edenhill/librdkafka/pull/2676,https://github.com/dimpavloff,19,https://github.com/edenhill/librdkafka/pull/2676#issuecomment-972923537,"Hi, I've implemented the ACL admin apis.
The things that I've checked are:

regression tests with versions 0 and 1 of the API client side against versions 0 or 1 server side.
memory leaks with the valgrind mode
removed all warnings
formatted the changed code with the clang-format that I've created following the ""librdkafka C style guide"", using git clang-format-9 
added a make format target to format all the c, cpp and h files (but not run it)

In order to make the 0081-admin test work this change must be applied to trivup:
edenhill/trivup#10
I've tried to follow the existing coding style, naming and design but given that the change is not trivial there could be things that are missing or need some change. The formatting rules are open to any change that is needed, I haven't found a rule that adds spaces before parenthesis always except in function calls.",Thanks @emasab and @edenhill for getting this merged!,True,{'HEART': ['https://github.com/emasab']}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2678,2020-01-08T10:12:55Z,2020-02-05T08:43:36Z,2020-02-05T08:43:42Z,MERGED,True,8,0,1,https://github.com/Eliyahu-Machluf,"Fix build of rdkafka_example project for windows, when using Visual Studio 2017/2019",1,[],https://github.com/edenhill/librdkafka/pull/2678,https://github.com/Eliyahu-Machluf,1,https://github.com/edenhill/librdkafka/pull/2678,"When building without the fix, you get an error:

1>------ Build started: Project: rdkafka_example, Configuration: Release x64 ------
1>C:\Program Files (x86)\Microsoft Visual Studio\2019\Professional\MSBuild\Microsoft\VC\v160\Microsoft.CppBuild.targets(379,5): error MSB8020: The build tools for v142 (Platform Toolset = 'v142') cannot be found. To build using the v142 build tools, please install v142 build tools.  Alternatively, you may upgrade to the current Visual Studio tools by selecting the Project menu or right-click the solution, and then selecting ""Retarget solution"".
1>Done building project ""rdkafka_example.vcxproj"" -- FAILED.

The error is NOT fixed, when re-targeting the solution.
The file common.vcxproj needs to contain the relevant platform toolset definitions, and this is what this commit does.","When building without the fix, you get an error:

1>------ Build started: Project: rdkafka_example, Configuration: Release x64 ------
1>C:\Program Files (x86)\Microsoft Visual Studio\2019\Professional\MSBuild\Microsoft\VC\v160\Microsoft.CppBuild.targets(379,5): error MSB8020: The build tools for v142 (Platform Toolset = 'v142') cannot be found. To build using the v142 build tools, please install v142 build tools.  Alternatively, you may upgrade to the current Visual Studio tools by selecting the Project menu or right-click the solution, and then selecting ""Retarget solution"".
1>Done building project ""rdkafka_example.vcxproj"" -- FAILED.

The error is NOT fixed, when re-targeting the solution.
The file common.vcxproj needs to contain the relevant platform toolset definitions, and this is what this commit does.",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2678,2020-01-08T10:12:55Z,2020-02-05T08:43:36Z,2020-02-05T08:43:42Z,MERGED,True,8,0,1,https://github.com/Eliyahu-Machluf,"Fix build of rdkafka_example project for windows, when using Visual Studio 2017/2019",1,[],https://github.com/edenhill/librdkafka/pull/2678,https://github.com/Eliyahu-Machluf,2,https://github.com/edenhill/librdkafka/pull/2678#issuecomment-572110735,"When building without the fix, you get an error:

1>------ Build started: Project: rdkafka_example, Configuration: Release x64 ------
1>C:\Program Files (x86)\Microsoft Visual Studio\2019\Professional\MSBuild\Microsoft\VC\v160\Microsoft.CppBuild.targets(379,5): error MSB8020: The build tools for v142 (Platform Toolset = 'v142') cannot be found. To build using the v142 build tools, please install v142 build tools.  Alternatively, you may upgrade to the current Visual Studio tools by selecting the Project menu or right-click the solution, and then selecting ""Retarget solution"".
1>Done building project ""rdkafka_example.vcxproj"" -- FAILED.

The error is NOT fixed, when re-targeting the solution.
The file common.vcxproj needs to contain the relevant platform toolset definitions, and this is what this commit does.",The check which failed (gcc - linux) is not relevant to the fix which relate to building librdkafka on windows using Visual Studio 2017/2019,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2678,2020-01-08T10:12:55Z,2020-02-05T08:43:36Z,2020-02-05T08:43:42Z,MERGED,True,8,0,1,https://github.com/Eliyahu-Machluf,"Fix build of rdkafka_example project for windows, when using Visual Studio 2017/2019",1,[],https://github.com/edenhill/librdkafka/pull/2678,https://github.com/edenhill,3,https://github.com/edenhill/librdkafka/pull/2678#issuecomment-582300663,"When building without the fix, you get an error:

1>------ Build started: Project: rdkafka_example, Configuration: Release x64 ------
1>C:\Program Files (x86)\Microsoft Visual Studio\2019\Professional\MSBuild\Microsoft\VC\v160\Microsoft.CppBuild.targets(379,5): error MSB8020: The build tools for v142 (Platform Toolset = 'v142') cannot be found. To build using the v142 build tools, please install v142 build tools.  Alternatively, you may upgrade to the current Visual Studio tools by selecting the Project menu or right-click the solution, and then selecting ""Retarget solution"".
1>Done building project ""rdkafka_example.vcxproj"" -- FAILED.

The error is NOT fixed, when re-targeting the solution.
The file common.vcxproj needs to contain the relevant platform toolset definitions, and this is what this commit does.","Perfect, thank you!",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2679,2020-01-08T13:50:27Z,2020-02-05T08:42:52Z,2020-02-05T08:42:57Z,MERGED,True,2,2,2,https://github.com/Eliyahu-Machluf,minor fix to rdkafka_example usage: add lz4 and zstd compression codec to usage.,1,[],https://github.com/edenhill/librdkafka/pull/2679,https://github.com/Eliyahu-Machluf,1,https://github.com/edenhill/librdkafka/pull/2679,update the usage with the compression codec of lz4 and zstd,update the usage with the compression codec of lz4 and zstd,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2679,2020-01-08T13:50:27Z,2020-02-05T08:42:52Z,2020-02-05T08:42:57Z,MERGED,True,2,2,2,https://github.com/Eliyahu-Machluf,minor fix to rdkafka_example usage: add lz4 and zstd compression codec to usage.,1,[],https://github.com/edenhill/librdkafka/pull/2679,https://github.com/Eliyahu-Machluf,2,https://github.com/edenhill/librdkafka/pull/2679#issuecomment-572110296,update the usage with the compression codec of lz4 and zstd,The check which failed is not relevant to the fix (simply fix of usage text),True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2679,2020-01-08T13:50:27Z,2020-02-05T08:42:52Z,2020-02-05T08:42:57Z,MERGED,True,2,2,2,https://github.com/Eliyahu-Machluf,minor fix to rdkafka_example usage: add lz4 and zstd compression codec to usage.,1,[],https://github.com/edenhill/librdkafka/pull/2679,https://github.com/edenhill,3,https://github.com/edenhill/librdkafka/pull/2679#issuecomment-582300395,update the usage with the compression codec of lz4 and zstd,Thank you!,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2680,2020-01-09T05:52:59Z,2020-01-09T10:28:32Z,2020-01-09T10:28:33Z,CLOSED,False,4,4,1,https://github.com/j1king,Delete trailing blanks in CONFIGURATION.md,1,[],https://github.com/edenhill/librdkafka/pull/2680,https://github.com/j1king,1,https://github.com/edenhill/librdkafka/pull/2680,"Deleted blanks making document ugly.

AS-IS

TO-BE","Deleted blanks making document ugly.

AS-IS

TO-BE",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2680,2020-01-09T05:52:59Z,2020-01-09T10:28:32Z,2020-01-09T10:28:33Z,CLOSED,False,4,4,1,https://github.com/j1king,Delete trailing blanks in CONFIGURATION.md,1,[],https://github.com/edenhill/librdkafka/pull/2680,https://github.com/edenhill,2,https://github.com/edenhill/librdkafka/pull/2680#issuecomment-572496132,"Deleted blanks making document ugly.

AS-IS

TO-BE","CONFIGURATION.md is programatically generated from rdkafka_conf.c and must not be edited directly.
I'm incorporating your suggestions in the upcoming transactions branch merge.
Thank you",True,{'THUMBS_UP': ['https://github.com/j1king']}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2683,2020-01-10T15:34:08Z,2020-01-12T22:34:59Z,2020-01-12T22:35:07Z,MERGED,True,2,2,2,https://github.com/jaspervandenberg,OpenBSD support,2,[],https://github.com/edenhill/librdkafka/pull/2683,https://github.com/jaspervandenberg,1,https://github.com/edenhill/librdkafka/pull/2683,"Both points that reference FreeBSD should also check for OpenBSD as OS.
I was able to compile the library on OpenBSD with these two minor changes.","Both points that reference FreeBSD should also check for OpenBSD as OS.
I was able to compile the library on OpenBSD with these two minor changes.",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2683,2020-01-10T15:34:08Z,2020-01-12T22:34:59Z,2020-01-12T22:35:07Z,MERGED,True,2,2,2,https://github.com/jaspervandenberg,OpenBSD support,2,[],https://github.com/edenhill/librdkafka/pull/2683,https://github.com/edenhill,2,https://github.com/edenhill/librdkafka/pull/2683#issuecomment-573464886,"Both points that reference FreeBSD should also check for OpenBSD as OS.
I was able to compile the library on OpenBSD with these two minor changes.",Thank you!,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2684,2020-01-10T23:16:08Z,2020-03-04T20:17:25Z,2020-03-04T20:17:25Z,CLOSED,False,1,1,1,https://github.com/mhowlett,Broaded check for no-op when no messages read,1,[],https://github.com/edenhill/librdkafka/pull/2684,https://github.com/mhowlett,1,https://github.com/edenhill/librdkafka/pull/2684,"Additionally, i think it's misleading to use RD_KAFKA_RESP_ERR_MSG_SIZE_TOO_LARGE in the error op below, as it's a broker error implying it originated from the broker. This had me confused for some time. I'll let you work out what to do with that.","Additionally, i think it's misleading to use RD_KAFKA_RESP_ERR_MSG_SIZE_TOO_LARGE in the error op below, as it's a broker error implying it originated from the broker. This had me confused for some time. I'll let you work out what to do with that.",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2684,2020-01-10T23:16:08Z,2020-03-04T20:17:25Z,2020-03-04T20:17:25Z,CLOSED,False,1,1,1,https://github.com/mhowlett,Broaded check for no-op when no messages read,1,[],https://github.com/edenhill/librdkafka/pull/2684,https://github.com/edenhill,2,https://github.com/edenhill/librdkafka/pull/2684#issuecomment-582300043,"Additionally, i think it's misleading to use RD_KAFKA_RESP_ERR_MSG_SIZE_TOO_LARGE in the error op below, as it's a broker error implying it originated from the broker. This had me confused for some time. I'll let you work out what to do with that.",Can you rebase this on latest master?,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2684,2020-01-10T23:16:08Z,2020-03-04T20:17:25Z,2020-03-04T20:17:25Z,CLOSED,False,1,1,1,https://github.com/mhowlett,Broaded check for no-op when no messages read,1,[],https://github.com/edenhill/librdkafka/pull/2684,https://github.com/mhowlett,3,https://github.com/edenhill/librdkafka/pull/2684#issuecomment-587706325,"Additionally, i think it's misleading to use RD_KAFKA_RESP_ERR_MSG_SIZE_TOO_LARGE in the error op below, as it's a broker error implying it originated from the broker. This had me confused for some time. I'll let you work out what to do with that.","this one is important (the symptom was the branch with the RD_KAFKA_RESP_ERR_MSG_SIZE_TOO_LARGE error was getting hit errantly). unfortunately, i can't remember / don't have a link to somewhere with more info for context.",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2684,2020-01-10T23:16:08Z,2020-03-04T20:17:25Z,2020-03-04T20:17:25Z,CLOSED,False,1,1,1,https://github.com/mhowlett,Broaded check for no-op when no messages read,1,[],https://github.com/edenhill/librdkafka/pull/2684,https://github.com/edenhill,4,https://github.com/edenhill/librdkafka/pull/2684#issuecomment-594390185,"Additionally, i think it's misleading to use RD_KAFKA_RESP_ERR_MSG_SIZE_TOO_LARGE in the error op below, as it's a broker error implying it originated from the broker. This had me confused for some time. I'll let you work out what to do with that.",Is this really needed when #2661 is fixed?,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2684,2020-01-10T23:16:08Z,2020-03-04T20:17:25Z,2020-03-04T20:17:25Z,CLOSED,False,1,1,1,https://github.com/mhowlett,Broaded check for no-op when no messages read,1,[],https://github.com/edenhill/librdkafka/pull/2684,https://github.com/edenhill,5,https://github.com/edenhill/librdkafka/pull/2684#issuecomment-594812303,"Additionally, i think it's misleading to use RD_KAFKA_RESP_ERR_MSG_SIZE_TOO_LARGE in the error op below, as it's a broker error implying it originated from the broker. This had me confused for some time. I'll let you work out what to do with that.",Fixed by #2743,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2686,2020-01-15T17:42:57Z,2020-02-04T09:45:31Z,2020-02-04T09:45:36Z,MERGED,True,14,14,7,https://github.com/pponnuvel,Fix format specifier for printing size_t,1,[],https://github.com/edenhill/librdkafka/pull/2686,https://github.com/pponnuvel,1,https://github.com/edenhill/librdkafka/pull/2686,"size_t being an unsigned type, '%zu' is the correct format specifier.
Using '%zd' is technically undefined behaviour (due to sign mismatch).","size_t being an unsigned type, '%zu' is the correct format specifier.
Using '%zd' is technically undefined behaviour (due to sign mismatch).",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2686,2020-01-15T17:42:57Z,2020-02-04T09:45:31Z,2020-02-04T09:45:36Z,MERGED,True,14,14,7,https://github.com/pponnuvel,Fix format specifier for printing size_t,1,[],https://github.com/edenhill/librdkafka/pull/2686,https://github.com/edenhill,2,https://github.com/edenhill/librdkafka/pull/2686#issuecomment-575083263,"size_t being an unsigned type, '%zu' is the correct format specifier.
Using '%zd' is technically undefined behaviour (due to sign mismatch).","Should use ""...%""PRIusz"".."" for portability",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2686,2020-01-15T17:42:57Z,2020-02-04T09:45:31Z,2020-02-04T09:45:36Z,MERGED,True,14,14,7,https://github.com/pponnuvel,Fix format specifier for printing size_t,1,[],https://github.com/edenhill/librdkafka/pull/2686,https://github.com/pponnuvel,3,https://github.com/edenhill/librdkafka/pull/2686#issuecomment-575107232,"size_t being an unsigned type, '%zu' is the correct format specifier.
Using '%zd' is technically undefined behaviour (due to sign mismatch).","PRIu macros are for fixed width integers (C99). Unless there's some non-standard platform/toolchain which doesn't support C99, '%zu' should work everywhere for size_t.
Checking MSVS, Visual studio 2019 documents it. Perhaps, older versions of Visual Studio and/or somewhere else it doesn't work?",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2686,2020-01-15T17:42:57Z,2020-02-04T09:45:31Z,2020-02-04T09:45:36Z,MERGED,True,14,14,7,https://github.com/pponnuvel,Fix format specifier for printing size_t,1,[],https://github.com/edenhill/librdkafka/pull/2686,https://github.com/edenhill,4,https://github.com/edenhill/librdkafka/pull/2686#issuecomment-575108909,"size_t being an unsigned type, '%zu' is the correct format specifier.
Using '%zd' is technically undefined behaviour (due to sign mismatch).","Older versions of MSVC use %Iu (as in i, not L). See rdwin32.h",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2686,2020-01-15T17:42:57Z,2020-02-04T09:45:31Z,2020-02-04T09:45:36Z,MERGED,True,14,14,7,https://github.com/pponnuvel,Fix format specifier for printing size_t,1,[],https://github.com/edenhill/librdkafka/pull/2686,https://github.com/pponnuvel,5,https://github.com/edenhill/librdkafka/pull/2686#issuecomment-575256804,"size_t being an unsigned type, '%zu' is the correct format specifier.
Using '%zd' is technically undefined behaviour (due to sign mismatch).","Ah, rdwin32.h and rdposix.h makes sense.
Modified to use the macro - left out the examples as that requires more headers from src/ (I can't test for all platforms).
On another note, it's probably OK now (at least for lack of z modifier support) given that it's been using zd currently (which wasn't supported on older Visual Studio).
I'll leave the above two your consideration. Thanks.",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2686,2020-01-15T17:42:57Z,2020-02-04T09:45:31Z,2020-02-04T09:45:36Z,MERGED,True,14,14,7,https://github.com/pponnuvel,Fix format specifier for printing size_t,1,[],https://github.com/edenhill/librdkafka/pull/2686,https://github.com/edenhill,6,https://github.com/edenhill/librdkafka/pull/2686#issuecomment-581824974,"size_t being an unsigned type, '%zu' is the correct format specifier.
Using '%zd' is technically undefined behaviour (due to sign mismatch).",Thank you!,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2688,2020-01-15T21:30:55Z,2020-02-04T09:48:41Z,2020-02-04T09:48:47Z,MERGED,True,31,31,12,https://github.com/pponnuvel,Use sig_atomic_t for vars modified in signal handlers,1,[],https://github.com/edenhill/librdkafka/pull/2688,https://github.com/pponnuvel,1,https://github.com/edenhill/librdkafka/pull/2688,"Another minor tweak to a type:
For proper atomicity (with respect to signal handler), the type of variables modified in signal handlers should be 'sig_atomic_t' with 'volatile' qualifier (modifying other types such as a plain 'int' is undefined behaviour even if it works as expected on most platforms). This is atomicity between main code & signal handler i.e. required even in single-threaded program.
In C++ files, used 0 for false and 1 for true respectively just to avoid any confusion and implicit conversions (which is otherwise fine).
sig_atomic_t type is available since C89, so should be reasonably portable.","Another minor tweak to a type:
For proper atomicity (with respect to signal handler), the type of variables modified in signal handlers should be 'sig_atomic_t' with 'volatile' qualifier (modifying other types such as a plain 'int' is undefined behaviour even if it works as expected on most platforms). This is atomicity between main code & signal handler i.e. required even in single-threaded program.
In C++ files, used 0 for false and 1 for true respectively just to avoid any confusion and implicit conversions (which is otherwise fine).
sig_atomic_t type is available since C89, so should be reasonably portable.",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2688,2020-01-15T21:30:55Z,2020-02-04T09:48:41Z,2020-02-04T09:48:47Z,MERGED,True,31,31,12,https://github.com/pponnuvel,Use sig_atomic_t for vars modified in signal handlers,1,[],https://github.com/edenhill/librdkafka/pull/2688,https://github.com/edenhill,2,https://github.com/edenhill/librdkafka/pull/2688#issuecomment-581826295,"Another minor tweak to a type:
For proper atomicity (with respect to signal handler), the type of variables modified in signal handlers should be 'sig_atomic_t' with 'volatile' qualifier (modifying other types such as a plain 'int' is undefined behaviour even if it works as expected on most platforms). This is atomicity between main code & signal handler i.e. required even in single-threaded program.
In C++ files, used 0 for false and 1 for true respectively just to avoid any confusion and implicit conversions (which is otherwise fine).
sig_atomic_t type is available since C89, so should be reasonably portable.",Thanks for this!,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2690,2020-01-17T06:31:52Z,2020-01-17T09:02:32Z,2020-01-17T09:02:38Z,MERGED,True,19,0,1,https://github.com/NancyLi1013,Add vcpkg installation instructions,4,[],https://github.com/edenhill/librdkafka/pull/2690,https://github.com/NancyLi1013,1,https://github.com/edenhill/librdkafka/pull/2690,"librdkafka is available as a port in vcpkg, a C++ library manager that simplifies installation for librdkafka and other project dependencies. Documenting the install process here will help users get started by providing a single set of commands to build librdkafka, ready to be included in their projects.
We also test whether our library ports build in various configurations (dynamic, static) on various platforms (OSX, Linux, Windows: x86, x64, arm) to keep a wide coverage for users.
I'm a maintainer for vcpkg, and here is what the port script looks like. We try to keep the library maintained as close as possible to the original library.","librdkafka is available as a port in vcpkg, a C++ library manager that simplifies installation for librdkafka and other project dependencies. Documenting the install process here will help users get started by providing a single set of commands to build librdkafka, ready to be included in their projects.
We also test whether our library ports build in various configurations (dynamic, static) on various platforms (OSX, Linux, Windows: x86, x64, arm) to keep a wide coverage for users.
I'm a maintainer for vcpkg, and here is what the port script looks like. We try to keep the library maintained as close as possible to the original library.",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2690,2020-01-17T06:31:52Z,2020-01-17T09:02:32Z,2020-01-17T09:02:38Z,MERGED,True,19,0,1,https://github.com/NancyLi1013,Add vcpkg installation instructions,4,[],https://github.com/edenhill/librdkafka/pull/2690,https://github.com/edenhill,2,https://github.com/edenhill/librdkafka/pull/2690#issuecomment-575513103,"librdkafka is available as a port in vcpkg, a C++ library manager that simplifies installation for librdkafka and other project dependencies. Documenting the install process here will help users get started by providing a single set of commands to build librdkafka, ready to be included in their projects.
We also test whether our library ports build in various configurations (dynamic, static) on various platforms (OSX, Linux, Windows: x86, x64, arm) to keep a wide coverage for users.
I'm a maintainer for vcpkg, and here is what the port script looks like. We try to keep the library maintained as close as possible to the original library.",It'd be good if the vcpkg was upgraded to the latest version (v1.3.0) before this PR gets merged.,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2690,2020-01-17T06:31:52Z,2020-01-17T09:02:32Z,2020-01-17T09:02:38Z,MERGED,True,19,0,1,https://github.com/NancyLi1013,Add vcpkg installation instructions,4,[],https://github.com/edenhill/librdkafka/pull/2690,https://github.com/NancyLi1013,3,https://github.com/edenhill/librdkafka/pull/2690#issuecomment-575527778,"librdkafka is available as a port in vcpkg, a C++ library manager that simplifies installation for librdkafka and other project dependencies. Documenting the install process here will help users get started by providing a single set of commands to build librdkafka, ready to be included in their projects.
We also test whether our library ports build in various configurations (dynamic, static) on various platforms (OSX, Linux, Windows: x86, x64, arm) to keep a wide coverage for users.
I'm a maintainer for vcpkg, and here is what the port script looks like. We try to keep the library maintained as close as possible to the original library.","Hi @edenhill thanks for your review and so quick feedback.
I have updated the changes you requested above. Please help check them again.
As for the upgrade to the latest version in vcpkg, we will try to update it later.
Thanks again for your help and support.",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2690,2020-01-17T06:31:52Z,2020-01-17T09:02:32Z,2020-01-17T09:02:38Z,MERGED,True,19,0,1,https://github.com/NancyLi1013,Add vcpkg installation instructions,4,[],https://github.com/edenhill/librdkafka/pull/2690,https://github.com/edenhill,4,https://github.com/edenhill/librdkafka/pull/2690#issuecomment-575537580,"librdkafka is available as a port in vcpkg, a C++ library manager that simplifies installation for librdkafka and other project dependencies. Documenting the install process here will help users get started by providing a single set of commands to build librdkafka, ready to be included in their projects.
We also test whether our library ports build in various configurations (dynamic, static) on various platforms (OSX, Linux, Windows: x86, x64, arm) to keep a wide coverage for users.
I'm a maintainer for vcpkg, and here is what the port script looks like. We try to keep the library maintained as close as possible to the original library.",Thank you!,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2697,2020-01-27T12:11:30Z,2020-02-03T13:23:24Z,2020-02-03T13:23:43Z,MERGED,True,22,15,2,https://github.com/dadufour,Fix test 0052 and rename test executable generated with CMake,6,[],https://github.com/edenhill/librdkafka/pull/2697,https://github.com/dadufour,1,https://github.com/edenhill/librdkafka/pull/2697,"Test 0052 is broken since 01JAN2020 - fix test by generating dynamically expected timestamps
Test executable generated by CMake was rdkafka_test while when generated through configure is test-runner. Rename rdkafka_test to test-runner to be consistent","Test 0052 is broken since 01JAN2020 - fix test by generating dynamically expected timestamps
Test executable generated by CMake was rdkafka_test while when generated through configure is test-runner. Rename rdkafka_test to test-runner to be consistent",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2697,2020-01-27T12:11:30Z,2020-02-03T13:23:24Z,2020-02-03T13:23:43Z,MERGED,True,22,15,2,https://github.com/dadufour,Fix test 0052 and rename test executable generated with CMake,6,[],https://github.com/edenhill/librdkafka/pull/2697,https://github.com/edenhill,2,https://github.com/edenhill/librdkafka/pull/2697#issuecomment-581411566,"Test 0052 is broken since 01JAN2020 - fix test by generating dynamically expected timestamps
Test executable generated by CMake was rdkafka_test while when generated through configure is test-runner. Rename rdkafka_test to test-runner to be consistent",Thank you for your contribution!,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2700,2020-02-03T14:24:59Z,2020-02-05T13:53:11Z,2020-02-05T13:56:02Z,MERGED,True,1818,270,34,https://github.com/edenhill,Enforce session.timeout.ms in the consumer itself (#2631),17,['bug'],https://github.com/edenhill/librdkafka/pull/2700,https://github.com/edenhill,1,https://github.com/edenhill/librdkafka/pull/2700,Also contains some low hanging bug fixes.,Also contains some low hanging bug fixes.,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2701,2020-02-04T17:53:41Z,2020-12-16T16:23:09Z,2020-12-16T16:23:25Z,MERGED,True,5314,551,49,https://github.com/gridaphobe,implement admin DeleteRecords & DeleteGroups APIs with fanout,20,[],https://github.com/edenhill/librdkafka/pull/2701,https://github.com/gridaphobe,1,https://github.com/edenhill/librdkafka/pull/2701,"Fixes #2056
This adds support for the DeleteRecords admin API. The request is mostly asynchronous -- there's a brief blocking period where we look up partition leaders -- and consolidates the per-broker responses into a single response that is sent back to the caller.
This PR supercedes #2065.","Fixes #2056
This adds support for the DeleteRecords admin API. The request is mostly asynchronous -- there's a brief blocking period where we look up partition leaders -- and consolidates the per-broker responses into a single response that is sent back to the caller.
This PR supercedes #2065.",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2701,2020-02-04T17:53:41Z,2020-12-16T16:23:09Z,2020-12-16T16:23:25Z,MERGED,True,5314,551,49,https://github.com/gridaphobe,implement admin DeleteRecords & DeleteGroups APIs with fanout,20,[],https://github.com/edenhill/librdkafka/pull/2701,https://github.com/gridaphobe,2,https://github.com/edenhill/librdkafka/pull/2701#issuecomment-582052097,"Fixes #2056
This adds support for the DeleteRecords admin API. The request is mostly asynchronous -- there's a brief blocking period where we look up partition leaders -- and consolidates the per-broker responses into a single response that is sent back to the caller.
This PR supercedes #2065.",Looks like the CI failure is due to a timing issue.,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2701,2020-02-04T17:53:41Z,2020-12-16T16:23:09Z,2020-12-16T16:23:25Z,MERGED,True,5314,551,49,https://github.com/gridaphobe,implement admin DeleteRecords & DeleteGroups APIs with fanout,20,[],https://github.com/edenhill/librdkafka/pull/2701,https://github.com/gridaphobe,3,https://github.com/edenhill/librdkafka/pull/2701#issuecomment-609862994,"Fixes #2056
This adds support for the DeleteRecords admin API. The request is mostly asynchronous -- there's a brief blocking period where we look up partition leaders -- and consolidates the per-broker responses into a single response that is sent back to the caller.
This PR supercedes #2065.",@edenhill I've implemented DeleteGroups as well and attached it to this PR since it uses the same underlying fanout mechanism for admin requests.,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2701,2020-02-04T17:53:41Z,2020-12-16T16:23:09Z,2020-12-16T16:23:25Z,MERGED,True,5314,551,49,https://github.com/gridaphobe,implement admin DeleteRecords & DeleteGroups APIs with fanout,20,[],https://github.com/edenhill/librdkafka/pull/2701,https://github.com/edenhill,4,https://github.com/edenhill/librdkafka/pull/2701#issuecomment-690263513,"Fixes #2056
This adds support for the DeleteRecords admin API. The request is mostly asynchronous -- there's a brief blocking period where we look up partition leaders -- and consolidates the per-broker responses into a single response that is sent back to the caller.
This PR supercedes #2065.","Let's leave the current blocking functionality as-is and get it merged to master and then work on making it non-blocking from there.
Can you rebase this PR on latest master?",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2701,2020-02-04T17:53:41Z,2020-12-16T16:23:09Z,2020-12-16T16:23:25Z,MERGED,True,5314,551,49,https://github.com/gridaphobe,implement admin DeleteRecords & DeleteGroups APIs with fanout,20,[],https://github.com/edenhill/librdkafka/pull/2701,https://github.com/edenhill,5,https://github.com/edenhill/librdkafka/pull/2701#issuecomment-690264307,"Fixes #2056
This adds support for the DeleteRecords admin API. The request is mostly asynchronous -- there's a brief blocking period where we look up partition leaders -- and consolidates the per-broker responses into a single response that is sent back to the caller.
This PR supercedes #2065.","Please address the other comments, rebase, and then we'll get this merged ",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2701,2020-02-04T17:53:41Z,2020-12-16T16:23:09Z,2020-12-16T16:23:25Z,MERGED,True,5314,551,49,https://github.com/gridaphobe,implement admin DeleteRecords & DeleteGroups APIs with fanout,20,[],https://github.com/edenhill/librdkafka/pull/2701,https://github.com/edenhill,6,https://github.com/edenhill/librdkafka/pull/2701#issuecomment-713499860,"Fixes #2056
This adds support for the DeleteRecords admin API. The request is mostly asynchronous -- there's a brief blocking period where we look up partition leaders -- and consolidates the per-broker responses into a single response that is sent back to the caller.
This PR supercedes #2065.","Now with v1.5.2 released we should try to get this into v1.6.0.
What's the current state of this PR?
If ready for review, please rebase on latest master.",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2701,2020-02-04T17:53:41Z,2020-12-16T16:23:09Z,2020-12-16T16:23:25Z,MERGED,True,5314,551,49,https://github.com/gridaphobe,implement admin DeleteRecords & DeleteGroups APIs with fanout,20,[],https://github.com/edenhill/librdkafka/pull/2701,https://github.com/gridaphobe,7,https://github.com/edenhill/librdkafka/pull/2701#issuecomment-713628958,"Fixes #2056
This adds support for the DeleteRecords admin API. The request is mostly asynchronous -- there's a brief blocking period where we look up partition leaders -- and consolidates the per-broker responses into a single response that is sent back to the caller.
This PR supercedes #2065.",@edenhill rebased and ready for another review!,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2701,2020-02-04T17:53:41Z,2020-12-16T16:23:09Z,2020-12-16T16:23:25Z,MERGED,True,5314,551,49,https://github.com/gridaphobe,implement admin DeleteRecords & DeleteGroups APIs with fanout,20,[],https://github.com/edenhill/librdkafka/pull/2701,https://github.com/edenhill,8,https://github.com/edenhill/librdkafka/pull/2701#issuecomment-734258961,"Fixes #2056
This adds support for the DeleteRecords admin API. The request is mostly asynchronous -- there's a brief blocking period where we look up partition leaders -- and consolidates the per-broker responses into a single response that is sent back to the caller.
This PR supercedes #2065.","I'm adding the async leader lookups to your branch, hope that's ok.",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2701,2020-02-04T17:53:41Z,2020-12-16T16:23:09Z,2020-12-16T16:23:25Z,MERGED,True,5314,551,49,https://github.com/gridaphobe,implement admin DeleteRecords & DeleteGroups APIs with fanout,20,[],https://github.com/edenhill/librdkafka/pull/2701,https://github.com/gridaphobe,9,https://github.com/edenhill/librdkafka/pull/2701#issuecomment-734368060,"Fixes #2056
This adds support for the DeleteRecords admin API. The request is mostly asynchronous -- there's a brief blocking period where we look up partition leaders -- and consolidates the per-broker responses into a single response that is sent back to the caller.
This PR supercedes #2065.","I'm adding the async leader lookups to your branch, hope that's ok.

Sounds good, thanks!!",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2701,2020-02-04T17:53:41Z,2020-12-16T16:23:09Z,2020-12-16T16:23:25Z,MERGED,True,5314,551,49,https://github.com/gridaphobe,implement admin DeleteRecords & DeleteGroups APIs with fanout,20,[],https://github.com/edenhill/librdkafka/pull/2701,https://github.com/edenhill,10,https://github.com/edenhill/librdkafka/pull/2701#issuecomment-743160641,"Fixes #2056
This adds support for the DeleteRecords admin API. The request is mostly asynchronous -- there's a brief blocking period where we look up partition leaders -- and consolidates the per-broker responses into a single response that is sent back to the caller.
This PR supercedes #2065.","If you shake the tree....
Found a bunch of AdminAPI and rko pitfalls and issues that was unearthed by the new broker target types, fixed them, made some minor API adjustments for DeleteRecords, added DeleteConsumerGroupOffsets while I was at it.
Eric, could you try this out in your environment and see that the new Admin APIs are to your liking.
I had to make the DeleteRecords a bit more complicated by adding a DeleteRecords_t* type, this is so that we can extend the calling convention without breaking the API in case the protocol request requires extra parameters in the future.",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2701,2020-02-04T17:53:41Z,2020-12-16T16:23:09Z,2020-12-16T16:23:25Z,MERGED,True,5314,551,49,https://github.com/gridaphobe,implement admin DeleteRecords & DeleteGroups APIs with fanout,20,[],https://github.com/edenhill/librdkafka/pull/2701,https://github.com/gridaphobe,11,https://github.com/edenhill/librdkafka/pull/2701#issuecomment-743222334,"Fixes #2056
This adds support for the DeleteRecords admin API. The request is mostly asynchronous -- there's a brief blocking period where we look up partition leaders -- and consolidates the per-broker responses into a single response that is sent back to the caller.
This PR supercedes #2065.","Awesome! The addition of the DeleteRecords_t type seems very sensible. I can't imagine that will cause us any problems, but I'll test the new branch later today or early next week to be sure.",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2701,2020-02-04T17:53:41Z,2020-12-16T16:23:09Z,2020-12-16T16:23:25Z,MERGED,True,5314,551,49,https://github.com/gridaphobe,implement admin DeleteRecords & DeleteGroups APIs with fanout,20,[],https://github.com/edenhill/librdkafka/pull/2701,https://github.com/gridaphobe,12,https://github.com/edenhill/librdkafka/pull/2701#issuecomment-745552552,"Fixes #2056
This adds support for the DeleteRecords admin API. The request is mostly asynchronous -- there's a brief blocking period where we look up partition leaders -- and consolidates the per-broker responses into a single response that is sent back to the caller.
This PR supercedes #2065.",@edenhill the API changes were easy to adapt to and everything looks good on our end!,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2701,2020-02-04T17:53:41Z,2020-12-16T16:23:09Z,2020-12-16T16:23:25Z,MERGED,True,5314,551,49,https://github.com/gridaphobe,implement admin DeleteRecords & DeleteGroups APIs with fanout,20,[],https://github.com/edenhill/librdkafka/pull/2701,https://github.com/edenhill,13,https://github.com/edenhill/librdkafka/pull/2701#issuecomment-746574806,"Fixes #2056
This adds support for the DeleteRecords admin API. The request is mostly asynchronous -- there's a brief blocking period where we look up partition leaders -- and consolidates the per-broker responses into a single response that is sent back to the caller.
This PR supercedes #2065.","Big thanks for this contribution, @gridaphobe ! ",True,"{'HOORAY': ['https://github.com/gridaphobe', 'https://github.com/mlongob', 'https://github.com/chrisbeard']}"
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2705,2020-02-06T17:35:17Z,2020-02-06T19:40:09Z,2020-02-06T19:40:14Z,MERGED,True,2,0,1,https://github.com/ahalam,Fix build break,1,[],https://github.com/edenhill/librdkafka/pull/2705,https://github.com/ahalam,1,https://github.com/edenhill/librdkafka/pull/2705,"Fixing build errors on debian:stretch
rdkafka_txnmgr.c: In function 'rd_kafka_txn_set_fatal_error':
rdkafka_txnmgr.c:245:9: warning: implicit declaration of function 'va_start' [-Wimplicit-function-declaration]
         va_start(ap, fmt);
         ^~~~~~~~
rdkafka_txnmgr.c:247:9: warning: implicit declaration of function 'va_end' [-Wimplicit-function-declaration]
         va_end(ap);
         ^~~~~~

which breaks the build
Creating shared library librdkafka++.so.1
gcc  -shared -Wl,-soname,librdkafka++.so.1 RdKafka.o ConfImpl.o HandleImpl.o ConsumerImpl.o ProducerImpl.o KafkaConsumerImpl.o TopicImpl.o TopicPartitionImpl.o MessageImpl.o HeadersImpl.o QueueImpl.o MetadataImpl.o -o librdkafka++.so.1 -L../src -lrdkafka -lstdc++
Creating static library librdkafka++.a
ar rcs librdkafka++.a RdKafka.o ConfImpl.o HandleImpl.o ConsumerImpl.o ProducerImpl.o KafkaConsumerImpl.o TopicImpl.o TopicPartitionImpl.o MessageImpl.o HeadersImpl.o QueueImpl.o MetadataImpl.o
Creating librdkafka++.so symlink
rm -f ""librdkafka++.so"" && ln -s ""librdkafka++.so.1"" ""librdkafka++.so""
Generating pkg-config file rdkafka++.pc
Generating pkg-config file rdkafka++-static.pc
Checking librdkafka++ integrity
librdkafka++.so.1              OK
librdkafka++.a                 OK
make[1]: Leaving directory '/go/librdkafka/src-cpp'
make -C examples
make[1]: Entering directory '/go/librdkafka/examples'
gcc -g -O2 -fPIC -Wall -Wsign-compare -Wfloat-equal -Wpointer-arith -Wcast-align -I../src rdkafka_example.c -o rdkafka_example  \
	../src/librdkafka.a -lm -ldl -lpthread -lrt
../src/librdkafka.a(rdkafka_txnmgr.o): In function `rd_kafka_txn_coord_set':
/go/librdkafka/src/rdkafka_txnmgr.c:2486: undefined reference to `va_start'
/go/librdkafka/src/rdkafka_txnmgr.c:2488: undefined reference to `va_end'
../src/librdkafka.a(rdkafka_txnmgr.o): In function `rd_kafka_txn_curr_api_reply':
/go/librdkafka/src/rdkafka_txnmgr.c:365: undefined reference to `va_start'
/go/librdkafka/src/rdkafka_txnmgr.c:367: undefined reference to `va_end'
../src/librdkafka.a(rdkafka_txnmgr.o): In function `rd_kafka_txn_set_fatal_error':
/go/librdkafka/src/rdkafka_txnmgr.c:245: undefined reference to `va_start'
/go/librdkafka/src/rdkafka_txnmgr.c:247: undefined reference to `va_end'
../src/librdkafka.a(rdkafka_txnmgr.o): In function `rd_kafka_txn_set_abortable_error':
/go/librdkafka/src/rdkafka_txnmgr.c:298: undefined reference to `va_start'
/go/librdkafka/src/rdkafka_txnmgr.c:300: undefined reference to `va_end'
collect2: error: ld returned 1 exit status
Makefile:18: recipe for target 'rdkafka_example' failed
make[1]: Leaving directory '/go/librdkafka/examples'
make[1]: *** [rdkafka_example] Error 1
make: *** [examples] Error 2
Makefile:44: recipe for target 'examples' failed
ERROR: Service 'xxxx-xxxxx' failed to build: The command '/bin/sh -c git clone https://github.com/edenhill/librdkafka.git && cd librdkafka && ./configure --prefix /usr && make && make install' returned a non-zero code: 2
Failed with exit code: 1

Exited with code exit status 1","Fixing build errors on debian:stretch
rdkafka_txnmgr.c: In function 'rd_kafka_txn_set_fatal_error':
rdkafka_txnmgr.c:245:9: warning: implicit declaration of function 'va_start' [-Wimplicit-function-declaration]
         va_start(ap, fmt);
         ^~~~~~~~
rdkafka_txnmgr.c:247:9: warning: implicit declaration of function 'va_end' [-Wimplicit-function-declaration]
         va_end(ap);
         ^~~~~~

which breaks the build
Creating shared library librdkafka++.so.1
gcc  -shared -Wl,-soname,librdkafka++.so.1 RdKafka.o ConfImpl.o HandleImpl.o ConsumerImpl.o ProducerImpl.o KafkaConsumerImpl.o TopicImpl.o TopicPartitionImpl.o MessageImpl.o HeadersImpl.o QueueImpl.o MetadataImpl.o -o librdkafka++.so.1 -L../src -lrdkafka -lstdc++
Creating static library librdkafka++.a
ar rcs librdkafka++.a RdKafka.o ConfImpl.o HandleImpl.o ConsumerImpl.o ProducerImpl.o KafkaConsumerImpl.o TopicImpl.o TopicPartitionImpl.o MessageImpl.o HeadersImpl.o QueueImpl.o MetadataImpl.o
Creating librdkafka++.so symlink
rm -f ""librdkafka++.so"" && ln -s ""librdkafka++.so.1"" ""librdkafka++.so""
Generating pkg-config file rdkafka++.pc
Generating pkg-config file rdkafka++-static.pc
Checking librdkafka++ integrity
librdkafka++.so.1              OK
librdkafka++.a                 OK
make[1]: Leaving directory '/go/librdkafka/src-cpp'
make -C examples
make[1]: Entering directory '/go/librdkafka/examples'
gcc -g -O2 -fPIC -Wall -Wsign-compare -Wfloat-equal -Wpointer-arith -Wcast-align -I../src rdkafka_example.c -o rdkafka_example  \
	../src/librdkafka.a -lm -ldl -lpthread -lrt
../src/librdkafka.a(rdkafka_txnmgr.o): In function `rd_kafka_txn_coord_set':
/go/librdkafka/src/rdkafka_txnmgr.c:2486: undefined reference to `va_start'
/go/librdkafka/src/rdkafka_txnmgr.c:2488: undefined reference to `va_end'
../src/librdkafka.a(rdkafka_txnmgr.o): In function `rd_kafka_txn_curr_api_reply':
/go/librdkafka/src/rdkafka_txnmgr.c:365: undefined reference to `va_start'
/go/librdkafka/src/rdkafka_txnmgr.c:367: undefined reference to `va_end'
../src/librdkafka.a(rdkafka_txnmgr.o): In function `rd_kafka_txn_set_fatal_error':
/go/librdkafka/src/rdkafka_txnmgr.c:245: undefined reference to `va_start'
/go/librdkafka/src/rdkafka_txnmgr.c:247: undefined reference to `va_end'
../src/librdkafka.a(rdkafka_txnmgr.o): In function `rd_kafka_txn_set_abortable_error':
/go/librdkafka/src/rdkafka_txnmgr.c:298: undefined reference to `va_start'
/go/librdkafka/src/rdkafka_txnmgr.c:300: undefined reference to `va_end'
collect2: error: ld returned 1 exit status
Makefile:18: recipe for target 'rdkafka_example' failed
make[1]: Leaving directory '/go/librdkafka/examples'
make[1]: *** [rdkafka_example] Error 1
make: *** [examples] Error 2
Makefile:44: recipe for target 'examples' failed
ERROR: Service 'xxxx-xxxxx' failed to build: The command '/bin/sh -c git clone https://github.com/edenhill/librdkafka.git && cd librdkafka && ./configure --prefix /usr && make && make install' returned a non-zero code: 2
Failed with exit code: 1

Exited with code exit status 1",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2705,2020-02-06T17:35:17Z,2020-02-06T19:40:09Z,2020-02-06T19:40:14Z,MERGED,True,2,0,1,https://github.com/ahalam,Fix build break,1,[],https://github.com/edenhill/librdkafka/pull/2705,https://github.com/edenhill,2,https://github.com/edenhill/librdkafka/pull/2705#issuecomment-583075285,"Fixing build errors on debian:stretch
rdkafka_txnmgr.c: In function 'rd_kafka_txn_set_fatal_error':
rdkafka_txnmgr.c:245:9: warning: implicit declaration of function 'va_start' [-Wimplicit-function-declaration]
         va_start(ap, fmt);
         ^~~~~~~~
rdkafka_txnmgr.c:247:9: warning: implicit declaration of function 'va_end' [-Wimplicit-function-declaration]
         va_end(ap);
         ^~~~~~

which breaks the build
Creating shared library librdkafka++.so.1
gcc  -shared -Wl,-soname,librdkafka++.so.1 RdKafka.o ConfImpl.o HandleImpl.o ConsumerImpl.o ProducerImpl.o KafkaConsumerImpl.o TopicImpl.o TopicPartitionImpl.o MessageImpl.o HeadersImpl.o QueueImpl.o MetadataImpl.o -o librdkafka++.so.1 -L../src -lrdkafka -lstdc++
Creating static library librdkafka++.a
ar rcs librdkafka++.a RdKafka.o ConfImpl.o HandleImpl.o ConsumerImpl.o ProducerImpl.o KafkaConsumerImpl.o TopicImpl.o TopicPartitionImpl.o MessageImpl.o HeadersImpl.o QueueImpl.o MetadataImpl.o
Creating librdkafka++.so symlink
rm -f ""librdkafka++.so"" && ln -s ""librdkafka++.so.1"" ""librdkafka++.so""
Generating pkg-config file rdkafka++.pc
Generating pkg-config file rdkafka++-static.pc
Checking librdkafka++ integrity
librdkafka++.so.1              OK
librdkafka++.a                 OK
make[1]: Leaving directory '/go/librdkafka/src-cpp'
make -C examples
make[1]: Entering directory '/go/librdkafka/examples'
gcc -g -O2 -fPIC -Wall -Wsign-compare -Wfloat-equal -Wpointer-arith -Wcast-align -I../src rdkafka_example.c -o rdkafka_example  \
	../src/librdkafka.a -lm -ldl -lpthread -lrt
../src/librdkafka.a(rdkafka_txnmgr.o): In function `rd_kafka_txn_coord_set':
/go/librdkafka/src/rdkafka_txnmgr.c:2486: undefined reference to `va_start'
/go/librdkafka/src/rdkafka_txnmgr.c:2488: undefined reference to `va_end'
../src/librdkafka.a(rdkafka_txnmgr.o): In function `rd_kafka_txn_curr_api_reply':
/go/librdkafka/src/rdkafka_txnmgr.c:365: undefined reference to `va_start'
/go/librdkafka/src/rdkafka_txnmgr.c:367: undefined reference to `va_end'
../src/librdkafka.a(rdkafka_txnmgr.o): In function `rd_kafka_txn_set_fatal_error':
/go/librdkafka/src/rdkafka_txnmgr.c:245: undefined reference to `va_start'
/go/librdkafka/src/rdkafka_txnmgr.c:247: undefined reference to `va_end'
../src/librdkafka.a(rdkafka_txnmgr.o): In function `rd_kafka_txn_set_abortable_error':
/go/librdkafka/src/rdkafka_txnmgr.c:298: undefined reference to `va_start'
/go/librdkafka/src/rdkafka_txnmgr.c:300: undefined reference to `va_end'
collect2: error: ld returned 1 exit status
Makefile:18: recipe for target 'rdkafka_example' failed
make[1]: Leaving directory '/go/librdkafka/examples'
make[1]: *** [rdkafka_example] Error 1
make: *** [examples] Error 2
Makefile:44: recipe for target 'examples' failed
ERROR: Service 'xxxx-xxxxx' failed to build: The command '/bin/sh -c git clone https://github.com/edenhill/librdkafka.git && cd librdkafka && ./configure --prefix /usr && make && make install' returned a non-zero code: 2
Failed with exit code: 1

Exited with code exit status 1",Thank you!,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2707,2020-02-10T16:02:34Z,2020-02-11T08:48:37Z,2020-02-11T08:49:01Z,MERGED,True,840,91,24,https://github.com/edenhill,KIP-511: client.software.name and client.software.version,11,[],https://github.com/edenhill/librdkafka/pull/2707,https://github.com/edenhill,1,https://github.com/edenhill/librdkafka/pull/2707,The JMX metrics read-out in test 0108 was verified with AK trunk as of now.,The JMX metrics read-out in test 0108 was verified with AK trunk as of now.,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2708,2020-02-11T12:03:31Z,2020-02-11T12:45:49Z,2020-02-11T12:45:52Z,MERGED,True,4,4,2,https://github.com/edenhill,SSL connection resets were not silenced by log.connection.close=false,2,[],https://github.com/edenhill/librdkafka/pull/2708,https://github.com/edenhill,1,https://github.com/edenhill/librdkafka/pull/2708,,,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2711,2020-02-13T12:50:02Z,2020-02-13T13:05:29Z,2020-02-13T13:05:31Z,MERGED,True,3,5,4,https://github.com/edenhill,Re-enable KIP-345,2,[],https://github.com/edenhill/librdkafka/pull/2711,https://github.com/edenhill,1,https://github.com/edenhill/librdkafka/pull/2711,"With consumer fencing fixed, show static consumer group config.","With consumer fencing fixed, show static consumer group config.",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2714,2020-02-13T21:01:25Z,2020-03-10T13:54:42Z,2020-03-10T13:54:48Z,MERGED,True,32,0,1,https://github.com/edenhill,Added SSL chapter to manual,1,[],https://github.com/edenhill/librdkafka/pull/2714,https://github.com/edenhill,1,https://github.com/edenhill/librdkafka/pull/2714,,,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2727,2020-02-20T13:17:53Z,2020-03-11T12:14:11Z,2020-03-11T12:14:11Z,CLOSED,False,9,0,1,https://github.com/mekleo,Fully support transactions in rd_kafka_produce_batch(),1,[],https://github.com/edenhill/librdkafka/pull/2727,https://github.com/mekleo,1,https://github.com/edenhill/librdkafka/pull/2727,"When messages were manually assigned to partitions (i.e. no partitioner invocation), rd_kafka_produce_batch() did not include them in the transaction. Consequently, rd_kafka_commit_transaction() failed to flush all outstanding messages.","When messages were manually assigned to partitions (i.e. no partitioner invocation), rd_kafka_produce_batch() did not include them in the transaction. Consequently, rd_kafka_commit_transaction() failed to flush all outstanding messages.",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2727,2020-02-20T13:17:53Z,2020-03-11T12:14:11Z,2020-03-11T12:14:11Z,CLOSED,False,9,0,1,https://github.com/mekleo,Fully support transactions in rd_kafka_produce_batch(),1,[],https://github.com/edenhill/librdkafka/pull/2727,https://github.com/edenhill,2,https://github.com/edenhill/librdkafka/pull/2727#issuecomment-595077706,"When messages were manually assigned to partitions (i.e. no partitioner invocation), rd_kafka_produce_batch() did not include them in the transaction. Consequently, rd_kafka_commit_transaction() failed to flush all outstanding messages.","Thanks for identifying this mistake!
I'm thinking we might want to move this logic to toppar_enq_msg() instead, need to give it a think-thru.",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2727,2020-02-20T13:17:53Z,2020-03-11T12:14:11Z,2020-03-11T12:14:11Z,CLOSED,False,9,0,1,https://github.com/mekleo,Fully support transactions in rd_kafka_produce_batch(),1,[],https://github.com/edenhill/librdkafka/pull/2727,https://github.com/edenhill,3,https://github.com/edenhill/librdkafka/pull/2727#issuecomment-597598090,"When messages were manually assigned to partitions (i.e. no partitioner invocation), rd_kafka_produce_batch() did not include them in the transaction. Consequently, rd_kafka_commit_transaction() failed to flush all outstanding messages.",Merged to master manually. Thank you for your contribution!,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2728,2020-02-21T12:42:33Z,2020-03-03T08:49:27Z,2020-03-03T08:49:33Z,MERGED,True,1628,608,24,https://github.com/edenhill,"Fixes for the transactional producer, et.al.",12,[],https://github.com/edenhill/librdkafka/pull/2728,https://github.com/edenhill,1,https://github.com/edenhill/librdkafka/pull/2728,Issues that surfaced during development of the Go client transactions.,Issues that surfaced during development of the Go client transactions.,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2728,2020-02-21T12:42:33Z,2020-03-03T08:49:27Z,2020-03-03T08:49:33Z,MERGED,True,1628,608,24,https://github.com/edenhill,"Fixes for the transactional producer, et.al.",12,[],https://github.com/edenhill/librdkafka/pull/2728,https://github.com/mhowlett,2,https://github.com/edenhill/librdkafka/pull/2728#issuecomment-593522109,Issues that surfaced during development of the Go client transactions.,LGTM still (with some minor commentary),True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2742,2020-03-04T09:03:24Z,2020-03-04T14:18:48Z,2020-03-04T14:18:51Z,MERGED,True,34,2,3,https://github.com/edenhill,Fix crash on metadata update (regression in txns PR),3,[],https://github.com/edenhill/librdkafka/pull/2742,https://github.com/edenhill,1,https://github.com/edenhill/librdkafka/pull/2742,"And bump travis integration tests to AK 2.4.0.
And attempt to get a backtrace if tests crash on travis.","And bump travis integration tests to AK 2.4.0.
And attempt to get a backtrace if tests crash on travis.",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2743,2020-03-04T15:44:37Z,2020-03-04T20:15:41Z,2020-03-04T20:15:45Z,MERGED,True,193,47,10,https://github.com/edenhill,Continuation of PR #2621 to fix txn consumer lag,3,[],https://github.com/edenhill/librdkafka/pull/2743,https://github.com/edenhill,1,https://github.com/edenhill/librdkafka/pull/2743,,,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2744,2020-03-04T16:43:00Z,2020-03-04T20:16:33Z,2020-03-06T20:15:51Z,MERGED,True,191,0,3,https://github.com/edenhill,Added consumer_group_metadata serdes to avoid bindings to hang on to object,1,[],https://github.com/edenhill/librdkafka/pull/2744,https://github.com/edenhill,1,https://github.com/edenhill/librdkafka/pull/2744,,,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2744,2020-03-04T16:43:00Z,2020-03-04T20:16:33Z,2020-03-06T20:15:51Z,MERGED,True,191,0,3,https://github.com/edenhill,Added consumer_group_metadata serdes to avoid bindings to hang on to object,1,[],https://github.com/edenhill/librdkafka/pull/2744,https://github.com/mhowlett,2,https://github.com/edenhill/librdkafka/pull/2744#issuecomment-594664887,,"LGTM. the magic thing seems a bit too paranoid, but i guess that fits with the general theme of the PR.
thanks!",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2744,2020-03-04T16:43:00Z,2020-03-04T20:16:33Z,2020-03-06T20:15:51Z,MERGED,True,191,0,3,https://github.com/edenhill,Added consumer_group_metadata serdes to avoid bindings to hang on to object,1,[],https://github.com/edenhill/librdkafka/pull/2744,https://github.com/edenhill,3,https://github.com/edenhill/librdkafka/pull/2744#issuecomment-594667068,,Paranoid? We need to identify the input as valid somehow.,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2744,2020-03-04T16:43:00Z,2020-03-04T20:16:33Z,2020-03-06T20:15:51Z,MERGED,True,191,0,3,https://github.com/edenhill,Added consumer_group_metadata serdes to avoid bindings to hang on to object,1,[],https://github.com/edenhill/librdkafka/pull/2744,https://github.com/mhowlett,4,https://github.com/edenhill/librdkafka/pull/2744#issuecomment-595943345,,"Paranoid? We need to identify the input as valid somehow.

it's externally visible, but effectively an internal detail, is where i was coming from",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2744,2020-03-04T16:43:00Z,2020-03-04T20:16:33Z,2020-03-06T20:15:51Z,MERGED,True,191,0,3,https://github.com/edenhill,Added consumer_group_metadata serdes to avoid bindings to hang on to object,1,[],https://github.com/edenhill/librdkafka/pull/2744,https://github.com/edenhill,5,https://github.com/edenhill/librdkafka/pull/2744#issuecomment-595947517,,"The magic is defined in an internal source file, not in the public API.",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2746,2020-03-05T16:02:56Z,2020-03-05T16:04:22Z,2020-03-05T16:04:25Z,MERGED,True,2,1,1,https://github.com/edenhill,consumer_group_metadata: allow empty group.ids,1,[],https://github.com/edenhill/librdkafka/pull/2746,https://github.com/edenhill,1,https://github.com/edenhill/librdkafka/pull/2746,Let the broker enforce it rather than us.,Let the broker enforce it rather than us.,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2748,2020-03-05T21:10:20Z,2020-03-06T12:25:28Z,2020-03-06T12:25:31Z,MERGED,True,9,5,1,https://github.com/edenhill,Wake up broker thread based on next request retry,1,[],https://github.com/edenhill/librdkafka/pull/2748,https://github.com/edenhill,1,https://github.com/edenhill/librdkafka/pull/2748,"Prior to this fix the next retry buf time was not considered for the
IO wakeup time, thus blocking up to 1s even if a retry was scheduled sooner
than that, such as is the case with the default retry time of 100ms.
Saw this in a transaction loop where the next transaction's AddOFfsetsToTxnRequest would need to be retried because the previous commit wasn't done, and the retry was scheduled for 1s rather than the configured 100ms.
With the fix in the retry of 100ms is honoured.","Prior to this fix the next retry buf time was not considered for the
IO wakeup time, thus blocking up to 1s even if a retry was scheduled sooner
than that, such as is the case with the default retry time of 100ms.
Saw this in a transaction loop where the next transaction's AddOFfsetsToTxnRequest would need to be retried because the previous commit wasn't done, and the retry was scheduled for 1s rather than the configured 100ms.
With the fix in the retry of 100ms is honoured.",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2749,2020-03-09T09:33:17Z,2020-03-09T10:43:37Z,2020-03-09T10:43:40Z,MERGED,True,12,1,2,https://github.com/edenhill,"Txn producer must bump epoch through broker, not locally.",2,[],https://github.com/edenhill/librdkafka/pull/2749,https://github.com/edenhill,1,https://github.com/edenhill/librdkafka/pull/2749,"Bump the epoch locally, which is okay for the idempotent producer, is not allowed for the transactional producer since the broker needs to track state of each pid+epoch.
However, this is not yet enforced by the broker which leads to inconsistent state and transactional.id states that go into a blocked unusable state.","Bump the epoch locally, which is okay for the idempotent producer, is not allowed for the transactional producer since the broker needs to track state of each pid+epoch.
However, this is not yet enforced by the broker which leads to inconsistent state and transactional.id states that go into a blocked unusable state.",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2750,2020-03-09T11:13:25Z,2020-03-10T15:44:08Z,2020-03-10T15:44:11Z,MERGED,True,76,16,2,https://github.com/edenhill,Fix offsets_for_times() returning INVALID_REQUEST (#2176),1,[],https://github.com/edenhill/librdkafka/pull/2750,https://github.com/edenhill,1,https://github.com/edenhill/librdkafka/pull/2750,"leaders_get() could add duplicate partitions in some cases which resulted
in broker returning INVALID_REQUEST for the duplicate partitions.","leaders_get() could add duplicate partitions in some cases which resulted
in broker returning INVALID_REQUEST for the duplicate partitions.",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2752,2020-03-10T13:22:51Z,2020-03-16T19:58:58Z,2020-03-16T19:59:07Z,MERGED,True,680,118,5,https://github.com/edenhill,Add transaction example,2,[],https://github.com/edenhill/librdkafka/pull/2752,https://github.com/edenhill,1,https://github.com/edenhill/librdkafka/pull/2752,,,True,{'THUMBS_UP': ['https://github.com/nick-zh']}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2752,2020-03-10T13:22:51Z,2020-03-16T19:58:58Z,2020-03-16T19:59:07Z,MERGED,True,680,118,5,https://github.com/edenhill,Add transaction example,2,[],https://github.com/edenhill/librdkafka/pull/2752,https://github.com/nick-zh,2,https://github.com/edenhill/librdkafka/pull/2752#issuecomment-599171229,,thanks very much!,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2753,2020-03-10T13:34:01Z,2020-03-11T12:15:48Z,2020-03-11T12:15:55Z,MERGED,True,293,9,14,https://github.com/Manicben,Add FNV-1a partitioner (#2724),1,[],https://github.com/edenhill/librdkafka/pull/2753,https://github.com/Manicben,1,https://github.com/edenhill/librdkafka/pull/2753,"Adds a new partitioner using the FNV-1a hashing algorithm, with some
tweaks to match Sarama's default hashing partitioner behaviour.
Main use case is for users switching from Sarama to librdkafka
(or confluent-kafka-go) and wanting to maintain ordering guarantees.
Test hashes were generated in the Go Playground using Go's hash/fnv lib.
Resolves #2724.
Didn't know whether it was OK to mention Sarama at all. Switching from it is the main reason for this, and the absolute value tweak to the hashing is required to be compatible with it.
Also, apologies for the whitespace changes, my IDE went ahead and trimmed all extra whitespace.","Adds a new partitioner using the FNV-1a hashing algorithm, with some
tweaks to match Sarama's default hashing partitioner behaviour.
Main use case is for users switching from Sarama to librdkafka
(or confluent-kafka-go) and wanting to maintain ordering guarantees.
Test hashes were generated in the Go Playground using Go's hash/fnv lib.
Resolves #2724.
Didn't know whether it was OK to mention Sarama at all. Switching from it is the main reason for this, and the absolute value tweak to the hashing is required to be compatible with it.
Also, apologies for the whitespace changes, my IDE went ahead and trimmed all extra whitespace.",True,"{'ROCKET': ['https://github.com/RichardoC', 'https://github.com/polycaster']}"
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2753,2020-03-10T13:34:01Z,2020-03-11T12:15:48Z,2020-03-11T12:15:55Z,MERGED,True,293,9,14,https://github.com/Manicben,Add FNV-1a partitioner (#2724),1,[],https://github.com/edenhill/librdkafka/pull/2753,https://github.com/Manicben,2,https://github.com/edenhill/librdkafka/pull/2753#issuecomment-597581103,"Adds a new partitioner using the FNV-1a hashing algorithm, with some
tweaks to match Sarama's default hashing partitioner behaviour.
Main use case is for users switching from Sarama to librdkafka
(or confluent-kafka-go) and wanting to maintain ordering guarantees.
Test hashes were generated in the Go Playground using Go's hash/fnv lib.
Resolves #2724.
Didn't know whether it was OK to mention Sarama at all. Switching from it is the main reason for this, and the absolute value tweak to the hashing is required to be compatible with it.
Also, apologies for the whitespace changes, my IDE went ahead and trimmed all extra whitespace.","Would this by any chance make it into 1.4.0? Would be amazing if it did, but I understand that it may be too late.",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2753,2020-03-10T13:34:01Z,2020-03-11T12:15:48Z,2020-03-11T12:15:55Z,MERGED,True,293,9,14,https://github.com/Manicben,Add FNV-1a partitioner (#2724),1,[],https://github.com/edenhill/librdkafka/pull/2753,https://github.com/edenhill,3,https://github.com/edenhill/librdkafka/pull/2753#issuecomment-597582713,"Adds a new partitioner using the FNV-1a hashing algorithm, with some
tweaks to match Sarama's default hashing partitioner behaviour.
Main use case is for users switching from Sarama to librdkafka
(or confluent-kafka-go) and wanting to maintain ordering guarantees.
Test hashes were generated in the Go Playground using Go's hash/fnv lib.
Resolves #2724.
Didn't know whether it was OK to mention Sarama at all. Switching from it is the main reason for this, and the absolute value tweak to the hashing is required to be compatible with it.
Also, apologies for the whitespace changes, my IDE went ahead and trimmed all extra whitespace.","It is an isolated feature that has good testing, so the plan is to include it in v1.4.0.",True,"{'HOORAY': ['https://github.com/Manicben', 'https://github.com/RichardoC']}"
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2753,2020-03-10T13:34:01Z,2020-03-11T12:15:48Z,2020-03-11T12:15:55Z,MERGED,True,293,9,14,https://github.com/Manicben,Add FNV-1a partitioner (#2724),1,[],https://github.com/edenhill/librdkafka/pull/2753,https://github.com/edenhill,4,https://github.com/edenhill/librdkafka/pull/2753#issuecomment-597598895,"Adds a new partitioner using the FNV-1a hashing algorithm, with some
tweaks to match Sarama's default hashing partitioner behaviour.
Main use case is for users switching from Sarama to librdkafka
(or confluent-kafka-go) and wanting to maintain ordering guarantees.
Test hashes were generated in the Go Playground using Go's hash/fnv lib.
Resolves #2724.
Didn't know whether it was OK to mention Sarama at all. Switching from it is the main reason for this, and the absolute value tweak to the hashing is required to be compatible with it.
Also, apologies for the whitespace changes, my IDE went ahead and trimmed all extra whitespace.",Thank you for a perfect PR!,True,"{'HOORAY': ['https://github.com/dimpavloff', 'https://github.com/perpetualjourney', 'https://github.com/RichardoC', 'https://github.com/polycaster', 'https://github.com/peterebden']}"
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2754,2020-03-10T16:58:24Z,2020-03-10T17:02:46Z,2020-03-10T20:23:29Z,MERGED,True,2,2,1,https://github.com/TimWSpence,Correct statistics names in docs,1,[],https://github.com/edenhill/librdkafka/pull/2754,https://github.com/TimWSpence,1,https://github.com/edenhill/librdkafka/pull/2754,"This isn't necessarily the correct solution - the current format is inconsistent, using rxbytes for brokers and partitions but rx_bytes at the top-level. But at the very least this will keep the documentation in line with the behaviour","This isn't necessarily the correct solution - the current format is inconsistent, using rxbytes for brokers and partitions but rx_bytes at the top-level. But at the very least this will keep the documentation in line with the behaviour",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2754,2020-03-10T16:58:24Z,2020-03-10T17:02:46Z,2020-03-10T20:23:29Z,MERGED,True,2,2,1,https://github.com/TimWSpence,Correct statistics names in docs,1,[],https://github.com/edenhill/librdkafka/pull/2754,https://github.com/edenhill,2,https://github.com/edenhill/librdkafka/pull/2754#issuecomment-597200345,"This isn't necessarily the correct solution - the current format is inconsistent, using rxbytes for brokers and partitions but rx_bytes at the top-level. But at the very least this will keep the documentation in line with the behaviour",Thank you!,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2754,2020-03-10T16:58:24Z,2020-03-10T17:02:46Z,2020-03-10T20:23:29Z,MERGED,True,2,2,1,https://github.com/TimWSpence,Correct statistics names in docs,1,[],https://github.com/edenhill/librdkafka/pull/2754,https://github.com/TimWSpence,3,https://github.com/edenhill/librdkafka/pull/2754#issuecomment-597296069,"This isn't necessarily the correct solution - the current format is inconsistent, using rxbytes for brokers and partitions but rx_bytes at the top-level. But at the very least this will keep the documentation in line with the behaviour",You're welcome! :),True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2757,2020-03-11T12:54:23Z,2020-03-12T07:44:25Z,2020-03-12T07:44:28Z,MERGED,True,90,2,2,https://github.com/edenhill,SASL SCRAM security fixes,3,[],https://github.com/edenhill/librdkafka/pull/2757,https://github.com/edenhill,1,https://github.com/edenhill/librdkafka/pull/2757,,,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2761,2020-03-12T10:03:46Z,2020-03-12T13:46:25Z,2020-03-12T13:46:30Z,MERGED,True,9,9,7,https://github.com/edenhill,"Re-enable static member test, and try to make KIP-511 test more reliable",5,[],https://github.com/edenhill/librdkafka/pull/2761,https://github.com/edenhill,1,https://github.com/edenhill/librdkafka/pull/2761,,,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2762,2020-03-12T11:09:34Z,2020-03-12T13:10:59Z,2020-03-12T13:11:03Z,MERGED,True,33,31,9,https://github.com/edenhill,Rename txn_abortable() to txn_requires_abort(),1,[],https://github.com/edenhill/librdkafka/pull/2762,https://github.com/edenhill,1,https://github.com/edenhill/librdkafka/pull/2762,,,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2763,2020-03-12T22:46:53Z,2020-03-13T09:36:18Z,2020-03-13T09:36:23Z,MERGED,True,2,2,2,https://github.com/cbarcenas,Fix lintian-detected misspellings,1,[],https://github.com/edenhill/librdkafka/pull/2763,https://github.com/cbarcenas,1,https://github.com/edenhill/librdkafka/pull/2763,"Just a minor fixup :) These misspellings generate warnings in lintian (Debian's package linter).
I: librdkafka1: spelling-error-in-binary usr/lib/x86_64-linux-gnu/librdkafka.so.1 offets offsets
I: librdkafka1: spelling-error-in-binary usr/lib/x86_64-linux-gnu/librdkafka.so.1 succesful successful","Just a minor fixup :) These misspellings generate warnings in lintian (Debian's package linter).
I: librdkafka1: spelling-error-in-binary usr/lib/x86_64-linux-gnu/librdkafka.so.1 offets offsets
I: librdkafka1: spelling-error-in-binary usr/lib/x86_64-linux-gnu/librdkafka.so.1 succesful successful",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2763,2020-03-12T22:46:53Z,2020-03-13T09:36:18Z,2020-03-13T09:36:23Z,MERGED,True,2,2,2,https://github.com/cbarcenas,Fix lintian-detected misspellings,1,[],https://github.com/edenhill/librdkafka/pull/2763,https://github.com/edenhill,2,https://github.com/edenhill/librdkafka/pull/2763#issuecomment-598633597,"Just a minor fixup :) These misspellings generate warnings in lintian (Debian's package linter).
I: librdkafka1: spelling-error-in-binary usr/lib/x86_64-linux-gnu/librdkafka.so.1 offets offsets
I: librdkafka1: spelling-error-in-binary usr/lib/x86_64-linux-gnu/librdkafka.so.1 succesful successful",Thank you!,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2766,2020-03-14T12:23:35Z,2020-03-16T07:38:55Z,2020-03-16T07:39:00Z,MERGED,True,2,2,1,https://github.com/litao3rd,Fix lintian-detected misspellings,1,[],https://github.com/edenhill/librdkafka/pull/2766,https://github.com/litao3rd,1,https://github.com/edenhill/librdkafka/pull/2766,"Just a minor fixup :) These misspellings generate warnings [1]
in lintian (Debian's package linter).
[1] https://lintian.debian.org/tags/spelling-error-in-binary.html","Just a minor fixup :) These misspellings generate warnings [1]
in lintian (Debian's package linter).
[1] https://lintian.debian.org/tags/spelling-error-in-binary.html",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2766,2020-03-14T12:23:35Z,2020-03-16T07:38:55Z,2020-03-16T07:39:00Z,MERGED,True,2,2,1,https://github.com/litao3rd,Fix lintian-detected misspellings,1,[],https://github.com/edenhill/librdkafka/pull/2766,https://github.com/edenhill,2,https://github.com/edenhill/librdkafka/pull/2766#issuecomment-599387760,"Just a minor fixup :) These misspellings generate warnings [1]
in lintian (Debian's package linter).
[1] https://lintian.debian.org/tags/spelling-error-in-binary.html",Thank you!,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2775,2020-03-20T13:33:08Z,2020-03-20T13:59:32Z,2020-03-20T13:59:35Z,MERGED,True,3,2,1,https://github.com/edenhill,Honour array size in rd_kafka_event_message_array() to avoid overflow,1,[],https://github.com/edenhill/librdkafka/pull/2775,https://github.com/edenhill,1,https://github.com/edenhill/librdkafka/pull/2775, (#2773), (#2773),True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2778,2020-03-23T16:18:59Z,2020-03-24T08:54:14Z,2020-03-24T08:54:14Z,CLOSED,False,0,1,1,https://github.com/jakesylvestre,Remove deprecated compression.type,1,[],https://github.com/edenhill/librdkafka/pull/2778,https://github.com/jakesylvestre,1,https://github.com/edenhill/librdkafka/pull/2778,,,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2778,2020-03-23T16:18:59Z,2020-03-24T08:54:14Z,2020-03-24T08:54:14Z,CLOSED,False,0,1,1,https://github.com/jakesylvestre,Remove deprecated compression.type,1,[],https://github.com/edenhill/librdkafka/pull/2778,https://github.com/edenhill,2,https://github.com/edenhill/librdkafka/pull/2778#issuecomment-603111886,,"compression.type works fine and must not be removed (since it would break existing applications):
 examples/rdkafka_example -b $BROKERS -P -t test -X compression.type=lz4
% Type stuff and hit enter to send
foo
% Sent 3 bytes to topic test partition -1
% Message delivered (3 bytes, offset 0, partition 0): foo",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2781,2020-03-25T08:24:43Z,2020-03-25T20:30:38Z,2020-03-25T20:30:43Z,MERGED,True,126,18,3,https://github.com/edenhill,max.poll.interval.ms should only be enforced when using subscribe(),1,['bug'],https://github.com/edenhill/librdkafka/pull/2781,https://github.com/edenhill,1,https://github.com/edenhill/librdkafka/pull/2781,"For confluentinc/confluent-kafka-dotnet#1220
This needs to go into v1.4","For confluentinc/confluent-kafka-dotnet#1220
This needs to go into v1.4",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2784,2020-03-27T17:02:07Z,2020-03-31T07:16:59Z,2020-03-31T07:17:02Z,MERGED,True,2,0,2,https://github.com/edenhill,Fix two test memory leaks,1,[],https://github.com/edenhill/librdkafka/pull/2784,https://github.com/edenhill,1,https://github.com/edenhill/librdkafka/pull/2784,,,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2788,2020-03-31T15:10:20Z,2020-03-31T15:24:12Z,2020-03-31T15:24:14Z,MERGED,True,4,3,3,https://github.com/edenhill,"Fixes for static library builds, exposed by python build tests",2,[],https://github.com/edenhill/librdkafka/pull/2788,https://github.com/edenhill,1,https://github.com/edenhill/librdkafka/pull/2788,,,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2792,2020-04-01T13:21:41Z,2020-04-01T13:30:53Z,2020-04-01T13:30:56Z,MERGED,True,5,5,3,https://github.com/edenhill,"configure: OpenSSL URLs changed, and propagate http error code when c",1,[],https://github.com/edenhill/librdkafka/pull/2792,https://github.com/edenhill,1,https://github.com/edenhill/librdkafka/pull/2792,url fails,url fails,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2798,2020-04-02T15:41:34Z,2020-04-02T15:46:24Z,2020-04-02T15:46:27Z,MERGED,True,5,2,1,https://github.com/edenhill,Trigger broker connection-retry (when down) if any requests are enqueued,1,['bug'],https://github.com/edenhill/librdkafka/pull/2798,https://github.com/edenhill,1,https://github.com/edenhill/librdkafka/pull/2798,,,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2799,2020-04-03T07:55:31Z,2020-04-03T14:51:14Z,2020-04-03T14:51:17Z,MERGED,True,30,1,4,https://github.com/edenhill,"Seed the PRNG by default, allow application to override with enable.random.seed=false (#2795)",1,[],https://github.com/edenhill/librdkafka/pull/2799,https://github.com/edenhill,1,https://github.com/edenhill/librdkafka/pull/2799,,,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2800,2020-04-05T00:31:53Z,2020-04-06T15:06:34Z,2020-04-06T15:06:34Z,CLOSED,False,39,38,6,https://github.com/mhowlett,Simplify group assignor init / config,3,[],https://github.com/edenhill/librdkafka/pull/2800,https://github.com/mhowlett,1,https://github.com/edenhill/librdkafka/pull/2800,"some things here seem unnecessarily complicated / general. am i missing anything important?
haven't tested this - if it looks right to you, will do that.","some things here seem unnecessarily complicated / general. am i missing anything important?
haven't tested this - if it looks right to you, will do that.",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2800,2020-04-05T00:31:53Z,2020-04-06T15:06:34Z,2020-04-06T15:06:34Z,CLOSED,False,39,38,6,https://github.com/mhowlett,Simplify group assignor init / config,3,[],https://github.com/edenhill/librdkafka/pull/2800,https://github.com/edenhill,2,https://github.com/edenhill/librdkafka/pull/2800#issuecomment-609396360,"some things here seem unnecessarily complicated / general. am i missing anything important?
haven't tested this - if it looks right to you, will do that.","The group protocol is generic and not specific to consumers (see kafka connect), the code reflects this and is prepared for supporting future protocol types as well as user-pluggable ones, which might have been a premature generalization, but I'm not sure what the benefit is of removing this generalization?
Also, make sure the tests pass locally, there are compilation warnings and test failures due to this change.",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2800,2020-04-05T00:31:53Z,2020-04-06T15:06:34Z,2020-04-06T15:06:34Z,CLOSED,False,39,38,6,https://github.com/mhowlett,Simplify group assignor init / config,3,[],https://github.com/edenhill/librdkafka/pull/2800,https://github.com/mhowlett,3,https://github.com/edenhill/librdkafka/pull/2800#issuecomment-609554762,"some things here seem unnecessarily complicated / general. am i missing anything important?
haven't tested this - if it looks right to you, will do that.","[note: deleted previous comment that demonstrated i didn't read yours]
yeah, agree that having the group.protocol.type config isn't a big deal and it's better not to remove it. I added some clarification to the docs in the current state of this PR instead of removing it.
the user-pluggable related code caused me some amount of wasted time, so i still think it's an improvement to either remove it or make it clearer (current PR state removes it).",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2800,2020-04-05T00:31:53Z,2020-04-06T15:06:34Z,2020-04-06T15:06:34Z,CLOSED,False,39,38,6,https://github.com/mhowlett,Simplify group assignor init / config,3,[],https://github.com/edenhill/librdkafka/pull/2800,https://github.com/edenhill,4,https://github.com/edenhill/librdkafka/pull/2800#issuecomment-609706497,"some things here seem unnecessarily complicated / general. am i missing anything important?
haven't tested this - if it looks right to you, will do that.","I don't really understand the reasoning behind these changes.
The code is prepared for (but does not yet expose) user-pluggable assignors, I do appreciate that unused abstractions can be removed to reduce complexity, but that should not be a reason in itself to change existing code that does not have issues, since any code change risks introducing bugs.
Also, the suggested patch breaks abstraction of an isolated part of the code (assignors_init()..).
I'd like to understand the reasoning for this change to understand why it is deemed warranted, what does it enable?",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2800,2020-04-05T00:31:53Z,2020-04-06T15:06:34Z,2020-04-06T15:06:34Z,CLOSED,False,39,38,6,https://github.com/mhowlett,Simplify group assignor init / config,3,[],https://github.com/edenhill/librdkafka/pull/2800,https://github.com/edenhill,5,https://github.com/edenhill/librdkafka/pull/2800#issuecomment-609835489,"some things here seem unnecessarily complicated / general. am i missing anything important?
haven't tested this - if it looks right to you, will do that.",See #2284,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2800,2020-04-05T00:31:53Z,2020-04-06T15:06:34Z,2020-04-06T15:06:34Z,CLOSED,False,39,38,6,https://github.com/mhowlett,Simplify group assignor init / config,3,[],https://github.com/edenhill/librdkafka/pull/2800,https://github.com/mhowlett,6,https://github.com/edenhill/librdkafka/pull/2800#issuecomment-609852108,"some things here seem unnecessarily complicated / general. am i missing anything important?
haven't tested this - if it looks right to you, will do that.","My reasoning was just reduce complexity in something that caused me to waste some time + make it clear in the user facing config that a property was currently pointless. Thanks for the context, makes sense!",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2801,2020-04-06T11:15:46Z,2020-04-07T20:08:26Z,2020-04-07T20:08:29Z,MERGED,True,11,7,1,https://github.com/edenhill,Align bundled c11 threads (tinycthreads) constants to glibc and musl (#2681),1,[],https://github.com/edenhill/librdkafka/pull/2801,https://github.com/edenhill,1,https://github.com/edenhill/librdkafka/pull/2801,,,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2805,2020-04-08T09:07:54Z,2020-04-08T10:17:53Z,2020-04-08T10:17:59Z,MERGED,True,5,4,1,https://github.com/theidexisted,Fix readme format,1,[],https://github.com/edenhill/librdkafka/pull/2805,https://github.com/theidexisted,1,https://github.com/edenhill/librdkafka/pull/2805,,,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2805,2020-04-08T09:07:54Z,2020-04-08T10:17:53Z,2020-04-08T10:17:59Z,MERGED,True,5,4,1,https://github.com/theidexisted,Fix readme format,1,[],https://github.com/edenhill/librdkafka/pull/2805,https://github.com/edenhill,2,https://github.com/edenhill/librdkafka/pull/2805#issuecomment-610875365,,Thank you!,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2810,2020-04-09T12:50:53Z,2020-04-16T14:33:54Z,2020-04-16T14:33:58Z,MERGED,True,99,114,3,https://github.com/edenhill,Doxygen formatting fixes,1,[],https://github.com/edenhill/librdkafka/pull/2810,https://github.com/edenhill,1,https://github.com/edenhill/librdkafka/pull/2810,"sections don't really work across functions as I had hoped.
A bunch of whitespace cleanups.","sections don't really work across functions as I had hoped.
A bunch of whitespace cleanups.",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2810,2020-04-09T12:50:53Z,2020-04-16T14:33:54Z,2020-04-16T14:33:58Z,MERGED,True,99,114,3,https://github.com/edenhill,Doxygen formatting fixes,1,[],https://github.com/edenhill/librdkafka/pull/2810,https://github.com/edenhill,2,https://github.com/edenhill/librdkafka/pull/2810#issuecomment-611510294,"sections don't really work across functions as I had hoped.
A bunch of whitespace cleanups.",For #2808,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2810,2020-04-09T12:50:53Z,2020-04-16T14:33:54Z,2020-04-16T14:33:58Z,MERGED,True,99,114,3,https://github.com/edenhill,Doxygen formatting fixes,1,[],https://github.com/edenhill/librdkafka/pull/2810,https://github.com/edenhill,3,https://github.com/edenhill/librdkafka/pull/2810#issuecomment-612883177,"sections don't really work across functions as I had hoped.
A bunch of whitespace cleanups.",Please review,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2815,2020-04-09T20:50:51Z,2020-04-10T09:37:08Z,2020-04-10T09:37:22Z,MERGED,True,1,1,1,https://github.com/ckb42,Fix return value of rd_kafka_test_fatal_error(),1,[],https://github.com/edenhill/librdkafka/pull/2815,https://github.com/ckb42,1,https://github.com/edenhill/librdkafka/pull/2815,"Fixed the inverted logic testing the return code of rd_kafka_set_fatal_error().   rd_kafka_set_fatal_error() returns 0 on the PREV_IN_PROGRESS error, which is the only error it returns.
Fixes #2811","Fixed the inverted logic testing the return code of rd_kafka_set_fatal_error().   rd_kafka_set_fatal_error() returns 0 on the PREV_IN_PROGRESS error, which is the only error it returns.
Fixes #2811",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2815,2020-04-09T20:50:51Z,2020-04-10T09:37:08Z,2020-04-10T09:37:22Z,MERGED,True,1,1,1,https://github.com/ckb42,Fix return value of rd_kafka_test_fatal_error(),1,[],https://github.com/edenhill/librdkafka/pull/2815,https://github.com/edenhill,2,https://github.com/edenhill/librdkafka/pull/2815#issuecomment-611960071,"Fixed the inverted logic testing the return code of rd_kafka_set_fatal_error().   rd_kafka_set_fatal_error() returns 0 on the PREV_IN_PROGRESS error, which is the only error it returns.
Fixes #2811",Thank you! Less is more :),True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2827,2020-04-16T11:09:46Z,2020-04-16T14:02:36Z,2020-04-16T14:02:44Z,MERGED,True,225,15,11,https://github.com/edenhill,Add `batch.size` producer configuration property (#638),2,[],https://github.com/edenhill/librdkafka/pull/2827,https://github.com/edenhill,1,https://github.com/edenhill/librdkafka/pull/2827,,,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2828,2020-04-16T13:39:43Z,2020-04-16T18:23:07Z,2020-04-16T18:23:10Z,MERGED,True,596,946,39,https://github.com/edenhill,Remove shared pointers,3,[],https://github.com/edenhill/librdkafka/pull/2828,https://github.com/edenhill,1,https://github.com/edenhill/librdkafka/pull/2828,"The shared pointers implementation (build-time opt-in) has been useful for
finding object reference leaks, but the added code complexity of casting
between the wrapping and original types is now more costly than what the
reaped benefits are.

This patch removes all traces of shared pointers, and renames
the internal rd_kafka_itopic_t type back to the public rd_kafka_topic_t.","The shared pointers implementation (build-time opt-in) has been useful for
finding object reference leaks, but the added code complexity of casting
between the wrapping and original types is now more costly than what the
reaped benefits are.

This patch removes all traces of shared pointers, and renames
the internal rd_kafka_itopic_t type back to the public rd_kafka_topic_t.",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2832,2020-04-17T16:11:41Z,2020-04-27T10:30:44Z,2020-04-27T10:30:47Z,MERGED,True,6,1,1,https://github.com/edenhill,Fix stack overwrite (of 1 byte) when SaslHandshake MechCnt is zero,1,[],https://github.com/edenhill/librdkafka/pull/2832,https://github.com/edenhill,1,https://github.com/edenhill/librdkafka/pull/2832,Saw this fly by once.,Saw this fly by once.,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2833,2020-04-17T21:07:09Z,2020-05-20T06:27:17Z,2020-05-20T06:27:25Z,MERGED,True,140,24,7,https://github.com/wolfchimneyrock,add rd_kafka_event_debug_contexts() for log events,4,[],https://github.com/edenhill/librdkafka/pull/2833,https://github.com/wolfchimneyrock,1,https://github.com/edenhill/librdkafka/pull/2833,"this additional callback will allow the client to dynamically
control which debug contexts are logged when using debug=all.
relates to #2804
create the config log_detailed_cb which is set using new function rd_kafka_conf_set_log_detailed_cb.  If the config is not set, then the callback set in log_cb will be used by default.  If the config is set, then log_detailed_cb will be called instead of any log_cb that is set.
Additionally, create new builtin handlers:

rd_kafka_log_detailed_print
rd_kafka_log_detailed_syslog

Which behave analogously to the existing log_cb builtin handlers.
Logging macros are modified to transparently handle this extra parameter.
A new member is appended to the log union of rd_kafka_op_s which contains an array of strings representing the debug contexts of the log event.","this additional callback will allow the client to dynamically
control which debug contexts are logged when using debug=all.
relates to #2804
create the config log_detailed_cb which is set using new function rd_kafka_conf_set_log_detailed_cb.  If the config is not set, then the callback set in log_cb will be used by default.  If the config is set, then log_detailed_cb will be called instead of any log_cb that is set.
Additionally, create new builtin handlers:

rd_kafka_log_detailed_print
rd_kafka_log_detailed_syslog

Which behave analogously to the existing log_cb builtin handlers.
Logging macros are modified to transparently handle this extra parameter.
A new member is appended to the log union of rd_kafka_op_s which contains an array of strings representing the debug contexts of the log event.",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2833,2020-04-17T21:07:09Z,2020-05-20T06:27:17Z,2020-05-20T06:27:25Z,MERGED,True,140,24,7,https://github.com/wolfchimneyrock,add rd_kafka_event_debug_contexts() for log events,4,[],https://github.com/edenhill/librdkafka/pull/2833,https://github.com/edenhill,2,https://github.com/edenhill/librdkafka/pull/2833#issuecomment-620045246,"this additional callback will allow the client to dynamically
control which debug contexts are logged when using debug=all.
relates to #2804
create the config log_detailed_cb which is set using new function rd_kafka_conf_set_log_detailed_cb.  If the config is not set, then the callback set in log_cb will be used by default.  If the config is set, then log_detailed_cb will be called instead of any log_cb that is set.
Additionally, create new builtin handlers:

rd_kafka_log_detailed_print
rd_kafka_log_detailed_syslog

Which behave analogously to the existing log_cb builtin handlers.
Logging macros are modified to transparently handle this extra parameter.
A new member is appended to the log union of rd_kafka_op_s which contains an array of strings representing the debug contexts of the log event.","Thank you for your efforts on this.
I'm not sure this is the direction we want to go though, to keep the API surface area down we try to craft more generic interfaces that can be extended over time to as need arise, especially so for the niche use cases like this one.
Taking this as an example it would be more appropriate to expose the debug contexts in the rd_kafka_event_t (RD_KAFKA_EVENT_LOG) interface  (and the corresponding C++ Event class).",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2833,2020-04-17T21:07:09Z,2020-05-20T06:27:17Z,2020-05-20T06:27:25Z,MERGED,True,140,24,7,https://github.com/wolfchimneyrock,add rd_kafka_event_debug_contexts() for log events,4,[],https://github.com/edenhill/librdkafka/pull/2833,https://github.com/wolfchimneyrock,3,https://github.com/edenhill/librdkafka/pull/2833#issuecomment-620111482,"this additional callback will allow the client to dynamically
control which debug contexts are logged when using debug=all.
relates to #2804
create the config log_detailed_cb which is set using new function rd_kafka_conf_set_log_detailed_cb.  If the config is not set, then the callback set in log_cb will be used by default.  If the config is set, then log_detailed_cb will be called instead of any log_cb that is set.
Additionally, create new builtin handlers:

rd_kafka_log_detailed_print
rd_kafka_log_detailed_syslog

Which behave analogously to the existing log_cb builtin handlers.
Logging macros are modified to transparently handle this extra parameter.
A new member is appended to the log union of rd_kafka_op_s which contains an array of strings representing the debug contexts of the log event.","Thank you for your efforts on this.
I'm not sure this is the direction we want to go though, to keep the API surface area down we try to craft more generic interfaces that can be extended over time to as need arise, especially so for the niche use cases like this one.
Taking this as an example it would be more appropriate to expose the debug contexts in the rd_kafka_event_t (RD_KAFKA_EVENT_LOG) interface (and the corresponding C++ Event class).

OK, i take it then I would leave rd_kafka_event_log() as-is and add something like rd_kafka_event_log_context() or rd_kafka_event_log_detail() ?
also, would you see in value in attaching the original rdkafka source file and line that logged via __FILE__ and __LINE__ macros?",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2833,2020-04-17T21:07:09Z,2020-05-20T06:27:17Z,2020-05-20T06:27:25Z,MERGED,True,140,24,7,https://github.com/wolfchimneyrock,add rd_kafka_event_debug_contexts() for log events,4,[],https://github.com/edenhill/librdkafka/pull/2833,https://github.com/edenhill,4,https://github.com/edenhill/librdkafka/pull/2833#issuecomment-620427043,"this additional callback will allow the client to dynamically
control which debug contexts are logged when using debug=all.
relates to #2804
create the config log_detailed_cb which is set using new function rd_kafka_conf_set_log_detailed_cb.  If the config is not set, then the callback set in log_cb will be used by default.  If the config is set, then log_detailed_cb will be called instead of any log_cb that is set.
Additionally, create new builtin handlers:

rd_kafka_log_detailed_print
rd_kafka_log_detailed_syslog

Which behave analogously to the existing log_cb builtin handlers.
Logging macros are modified to transparently handle this extra parameter.
A new member is appended to the log union of rd_kafka_op_s which contains an array of strings representing the debug contexts of the log event.","Yeah, I'd call it rd_kafka_event_debug_contexts() and I'd do the flags2str conversion in that method rather than in the op-creation.
I don't think file and line are needed, the debug lines are easy to track down through their format.",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2833,2020-04-17T21:07:09Z,2020-05-20T06:27:17Z,2020-05-20T06:27:25Z,MERGED,True,140,24,7,https://github.com/wolfchimneyrock,add rd_kafka_event_debug_contexts() for log events,4,[],https://github.com/edenhill/librdkafka/pull/2833,https://github.com/edenhill,5,https://github.com/edenhill/librdkafka/pull/2833#issuecomment-631264786,"this additional callback will allow the client to dynamically
control which debug contexts are logged when using debug=all.
relates to #2804
create the config log_detailed_cb which is set using new function rd_kafka_conf_set_log_detailed_cb.  If the config is not set, then the callback set in log_cb will be used by default.  If the config is set, then log_detailed_cb will be called instead of any log_cb that is set.
Additionally, create new builtin handlers:

rd_kafka_log_detailed_print
rd_kafka_log_detailed_syslog

Which behave analogously to the existing log_cb builtin handlers.
Logging macros are modified to transparently handle this extra parameter.
A new member is appended to the log union of rd_kafka_op_s which contains an array of strings representing the debug contexts of the log event.",Thank you!,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2836,2020-04-19T02:05:03Z,2020-04-20T07:09:35Z,2020-04-20T07:09:39Z,MERGED,True,24,8,1,https://github.com/benesch,Ensure CMake sets disabled defines to zero on Windows,1,[],https://github.com/edenhill/librdkafka/pull/2836,https://github.com/benesch,1,https://github.com/edenhill/librdkafka/pull/2836,"This patch fixes CMake builds on Windows, which have been broken since
v1.4.0.
The CMake build for Windows was setting options by defining or not
defining them, whereas for all other platforms options are defined to 1
or 0 to indicate set or not set.
In most code, options are checked with a preprocessor test like #if OPTION, where it doesn't matter if OPTION is undefined or set to zero;
both approaches omit the code within the #if. But c426b31 introduced
code that requires OPTION to actually be defined to an integral value.
This patch ensures that the CMake configuration sets options to 1/0 when
targeting Windows. Technically only ENABLE_DEVEL needs to have a 1/0
value, but it seemed more future-proof to apply the change proactively
to all options.","This patch fixes CMake builds on Windows, which have been broken since
v1.4.0.
The CMake build for Windows was setting options by defining or not
defining them, whereas for all other platforms options are defined to 1
or 0 to indicate set or not set.
In most code, options are checked with a preprocessor test like #if OPTION, where it doesn't matter if OPTION is undefined or set to zero;
both approaches omit the code within the #if. But c426b31 introduced
code that requires OPTION to actually be defined to an integral value.
This patch ensures that the CMake configuration sets options to 1/0 when
targeting Windows. Technically only ENABLE_DEVEL needs to have a 1/0
value, but it seemed more future-proof to apply the change proactively
to all options.",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2836,2020-04-19T02:05:03Z,2020-04-20T07:09:35Z,2020-04-20T07:09:39Z,MERGED,True,24,8,1,https://github.com/benesch,Ensure CMake sets disabled defines to zero on Windows,1,[],https://github.com/edenhill/librdkafka/pull/2836,https://github.com/edenhill,2,https://github.com/edenhill/librdkafka/pull/2836#issuecomment-616355196,"This patch fixes CMake builds on Windows, which have been broken since
v1.4.0.
The CMake build for Windows was setting options by defining or not
defining them, whereas for all other platforms options are defined to 1
or 0 to indicate set or not set.
In most code, options are checked with a preprocessor test like #if OPTION, where it doesn't matter if OPTION is undefined or set to zero;
both approaches omit the code within the #if. But c426b31 introduced
code that requires OPTION to actually be defined to an integral value.
This patch ensures that the CMake configuration sets options to 1/0 when
targeting Windows. Technically only ENABLE_DEVEL needs to have a 1/0
value, but it seemed more future-proof to apply the change proactively
to all options.",Thank you!,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2839,2020-04-21T13:28:31Z,2020-04-27T07:44:22Z,2020-04-28T08:03:45Z,MERGED,True,849,115,32,https://github.com/edenhill,Propagate consumer errors for unavailable subscribed topics ,6,[],https://github.com/edenhill/librdkafka/pull/2839,https://github.com/edenhill,1,https://github.com/edenhill/librdkafka/pull/2839,,,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2840,2020-04-21T13:30:08Z,2020-04-23T15:05:10Z,2020-07-06T13:31:17Z,MERGED,True,487,86,28,https://github.com/edenhill, Fix produce/consume hang after partition goes away and comes back,10,[],https://github.com/edenhill/librdkafka/pull/2840,https://github.com/edenhill,1,https://github.com/edenhill/librdkafka/pull/2840,"This is a regression from fetch-from-follower.
And a bunch of other fixes.","This is a regression from fetch-from-follower.
And a bunch of other fixes.",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2849,2020-04-27T07:45:32Z,2020-04-27T07:45:55Z,2020-04-27T07:45:55Z,CLOSED,False,1304,282,42,https://github.com/edenhill,Improve broker connection error messages and add SSL CA cert probing,14,[],https://github.com/edenhill/librdkafka/pull/2849,https://github.com/edenhill,1,https://github.com/edenhill/librdkafka/pull/2849,,,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2851,2020-04-27T09:25:20Z,2020-04-29T09:18:46Z,2020-04-29T09:18:49Z,MERGED,True,553,220,44,https://github.com/edenhill,"Improve connection error propagation, remove ApiVersion heuristics, and probe for CA cert path",8,[],https://github.com/edenhill/librdkafka/pull/2851,https://github.com/edenhill,1,https://github.com/edenhill/librdkafka/pull/2851,,,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2852,2020-04-27T12:49:20Z,2020-04-27T13:35:25Z,2020-04-27T13:35:36Z,MERGED,True,6,1,1,https://github.com/andrewthad,Clarify reuse restriction on arguments to rd_kafka_produce(),1,[],https://github.com/edenhill/librdkafka/pull/2852,https://github.com/andrewthad,1,https://github.com/edenhill/librdkafka/pull/2852,Resolves #2843,Resolves #2843,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2852,2020-04-27T12:49:20Z,2020-04-27T13:35:25Z,2020-04-27T13:35:36Z,MERGED,True,6,1,1,https://github.com/andrewthad,Clarify reuse restriction on arguments to rd_kafka_produce(),1,[],https://github.com/edenhill/librdkafka/pull/2852,https://github.com/andrewthad,2,https://github.com/edenhill/librdkafka/pull/2852#issuecomment-619982778,Resolves #2843,Amended the commit to address the issues discussed in the inline comments.,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2852,2020-04-27T12:49:20Z,2020-04-27T13:35:25Z,2020-04-27T13:35:36Z,MERGED,True,6,1,1,https://github.com/andrewthad,Clarify reuse restriction on arguments to rd_kafka_produce(),1,[],https://github.com/edenhill/librdkafka/pull/2852,https://github.com/edenhill,3,https://github.com/edenhill/librdkafka/pull/2852#issuecomment-619989989,Resolves #2843,Thank you!,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2856,2020-04-29T06:57:46Z,2020-04-29T18:33:34Z,2020-04-29T18:33:37Z,MERGED,True,6,0,1,https://github.com/edenhill,Speed up initial cgrp join state (#2735),1,[],https://github.com/edenhill/librdkafka/pull/2856,https://github.com/edenhill,1,https://github.com/edenhill/librdkafka/pull/2856,,,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2860,2020-04-30T07:35:40Z,2020-05-11T07:24:10Z,2020-05-11T07:24:14Z,MERGED,True,13,12,2,https://github.com/edenhill,"Export rd_kafka_message_errstr() as proper symbol, not inline (#2822)",1,[],https://github.com/edenhill/librdkafka/pull/2860,https://github.com/edenhill,1,https://github.com/edenhill/librdkafka/pull/2860,This allows non-C languages (like .NET!) to use it without the need of a C compiler.,This allows non-C languages (like .NET!) to use it without the need of a C compiler.,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2861,2020-04-30T15:21:36Z,2020-05-05T07:02:51Z,2020-05-08T06:37:37Z,MERGED,True,418,65,39,https://github.com/edenhill,Centos 8 RPMs for 1.4.x,6,[],https://github.com/edenhill/librdkafka/pull/2861,https://github.com/edenhill,1,https://github.com/edenhill/librdkafka/pull/2861,This is a backport of the work on master which also brings in Python3 support (since python is not a valid python2 executable on Centos 8).,This is a backport of the work on master which also brings in Python3 support (since python is not a valid python2 executable on Centos 8).,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2861,2020-04-30T15:21:36Z,2020-05-05T07:02:51Z,2020-05-08T06:37:37Z,MERGED,True,418,65,39,https://github.com/edenhill,Centos 8 RPMs for 1.4.x,6,[],https://github.com/edenhill/librdkafka/pull/2861,https://github.com/edenhill,2,https://github.com/edenhill/librdkafka/pull/2861#issuecomment-623671705,This is a backport of the work on master which also brings in Python3 support (since python is not a valid python2 executable on Centos 8).,"@andrewegel Addressed your comments, please do a final review.",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2861,2020-04-30T15:21:36Z,2020-05-05T07:02:51Z,2020-05-08T06:37:37Z,MERGED,True,418,65,39,https://github.com/edenhill,Centos 8 RPMs for 1.4.x,6,[],https://github.com/edenhill/librdkafka/pull/2861,https://github.com/andrewegel,3,https://github.com/edenhill/librdkafka/pull/2861#issuecomment-623813913,This is a backport of the work on master which also brings in Python3 support (since python is not a valid python2 executable on Centos 8).,"I tested the $(MOCK_OPTIONS) suggestion of sorts in the packaging job here: confluentinc/packaging#754 (with this branches changes + that suggestion in mu own fork).
aegelhofer@Andrew-Egelhofer's- ~ % aws s3 ls s3://jenkins-confluent-packages/PR-754/5/rpm/6.0/8/
                           PRE repodata/
2020-05-04 18:10:42     239004 avro-c-1.8.0_confluent6.0.0-0.1.SNAPSHOT.el8.src.rpm
2020-05-04 18:10:43      92700 avro-c-1.8.0_confluent6.0.0-0.1.SNAPSHOT.el8.x86_64.rpm
2020-05-04 18:10:43     273088 avro-c-debuginfo-1.8.0_confluent6.0.0-0.1.SNAPSHOT.el8.x86_64.rpm
2020-05-04 18:10:43     108824 avro-c-debugsource-1.8.0_confluent6.0.0-0.1.SNAPSHOT.el8.x86_64.rpm
2020-05-04 18:10:43     147292 avro-c-devel-1.8.0_confluent6.0.0-0.1.SNAPSHOT.el8.x86_64.rpm
2020-05-04 18:10:43     155004 avro-c-tools-1.8.0_confluent6.0.0-0.1.SNAPSHOT.el8.x86_64.rpm
2020-05-04 18:10:43     450328 avro-c-tools-debuginfo-1.8.0_confluent6.0.0-0.1.SNAPSHOT.el8.x86_64.rpm
2020-05-04 18:10:43    2742850 librdkafka-1.4.0_confluent6.0.0-0.1.SNAPSHOT.el8.src.rpm
2020-05-04 18:10:43    1132418 librdkafka-debugsource-1.4.0_confluent6.0.0-0.1.SNAPSHOT.el8.x86_64.rpm
2020-05-04 18:10:44    1144624 librdkafka-devel-1.4.0_confluent6.0.0-0.1.SNAPSHOT.el8.x86_64.rpm
2020-05-04 18:10:44     998147 librdkafka1-1.4.0_confluent6.0.0-0.1.SNAPSHOT.el8.x86_64.rpm
2020-05-04 18:10:44    4228674 librdkafka1-debuginfo-1.4.0_confluent6.0.0-0.1.SNAPSHOT.el8.x86_64.rpm",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2861,2020-04-30T15:21:36Z,2020-05-05T07:02:51Z,2020-05-08T06:37:37Z,MERGED,True,418,65,39,https://github.com/edenhill,Centos 8 RPMs for 1.4.x,6,[],https://github.com/edenhill/librdkafka/pull/2861,https://github.com/edenhill,4,https://github.com/edenhill/librdkafka/pull/2861#issuecomment-623888555,This is a backport of the work on master which also brings in Python3 support (since python is not a valid python2 executable on Centos 8).,Thank you @andrewegel !,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2862,2020-04-30T15:25:15Z,2020-05-11T07:20:50Z,2020-05-11T07:20:54Z,MERGED,True,252,34,14,https://github.com/edenhill,Add `topic.metadata.propagation.max.ms` to circumvent topic creation race (#2858),1,[],https://github.com/edenhill/librdkafka/pull/2862,https://github.com/edenhill,1,https://github.com/edenhill/librdkafka/pull/2862,cc @theduderog,cc @theduderog,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2862,2020-04-30T15:25:15Z,2020-05-11T07:20:50Z,2020-05-11T07:20:54Z,MERGED,True,252,34,14,https://github.com/edenhill,Add `topic.metadata.propagation.max.ms` to circumvent topic creation race (#2858),1,[],https://github.com/edenhill/librdkafka/pull/2862,https://github.com/theduderog,2,https://github.com/edenhill/librdkafka/pull/2862#issuecomment-622195236,cc @theduderog,Thank you!  This will be so nice. ,True,{'HEART': ['https://github.com/edenhill']}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2862,2020-04-30T15:25:15Z,2020-05-11T07:20:50Z,2020-05-11T07:20:54Z,MERGED,True,252,34,14,https://github.com/edenhill,Add `topic.metadata.propagation.max.ms` to circumvent topic creation race (#2858),1,[],https://github.com/edenhill/librdkafka/pull/2862,https://github.com/edenhill,3,https://github.com/edenhill/librdkafka/pull/2862#issuecomment-625659818,cc @theduderog,Please review.,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2865,2020-05-03T06:12:41Z,2020-05-04T07:15:56Z,2020-05-04T07:16:01Z,MERGED,True,3,0,1,https://github.com/benesch,Shave another 1s off cgrp initialization,1,['GREAT REPORT'],https://github.com/edenhill/librdkafka/pull/2865,https://github.com/benesch,1,https://github.com/edenhill/librdkafka/pull/2865,"Querying for the cgrp coordinator does not need to back off when there
are no brokers connected, as no requests were sent. This avoids an
accident of timing that would cause cgrp initialization to wait an extra
turn of the main rdkafka thread loop, which would add an unnecessary
second of latency.
Inspired by c64b652.

Details follow.
The logs that inspired this change are here: https://gist.github.com/benesch/787791fa96cc75660ecb29cd37d49ddc. Warning: there are some additional debugging log lines that I patched in myself.
The relevant lines are these:
May 03 00:40:46.553 DEBUG librdkafka: librdkafka: BRKMAIN [thrd:marmoset:9092/0]: marmoset:9092/0: Enter main broker thread
May 03 00:40:47.542 DEBUG librdkafka: librdkafka: CGRPQUERY [thrd:main]: localhost:9092/bootstrap: Group ""materialize-kafka-u1/u2"": querying for coordinator: intervaled in state query-coord    

Notice the nearly 1s gap between when the broker is connected, and when the cgrp actually starts querying for the coordinator! This turned out to be a really unfortunate artifact of timing.
The cgrp is created first, when there are no brokers. This request understandably fails:
May 03 00:40:46.544 DEBUG librdkafka: librdkafka: CGRPQUERY [thrd:main]: Group ""materialize-kafka-u1/u2"": no broker available for coordinator query: intervaled in state query-coord

The broker connection happens just milliseconds later:
May 03 00:40:46.551 DEBUG librdkafka: librdkafka: STATE [thrd:localhost:9092/bootstrap]: localhost:9092/bootstrap: Broker changed state APIVERSION_QUERY -> UP    

This immediately triggers a call to rd_kafka_cgrp_serve (log line is something.I added myself)
May 03 00:40:46.552 DEBUG librdkafka: librdkafka: CGRPLOG [thrd:main]: serving state query-coord  

but that call doesn't actually query for the coordinator, because we queried for the coordinator less than 500ms ago! We're then forced to wait not just 500ms, but a full 1000ms, because we need to wait out a full turn of the main rdkafka thread loop, and there are no events that wake it up before it hits the 1000ms max timeout.
The fix here is to reset the coord-query interval if there were no brokers availablethat operation wasn't expensive at all, so might as well try it again ASAP. And indeed, in combination with c64b652, this brings the consumer initialization time down to ~150ms, which is much more pleasant than the 5s+ we were seeing before.","Querying for the cgrp coordinator does not need to back off when there
are no brokers connected, as no requests were sent. This avoids an
accident of timing that would cause cgrp initialization to wait an extra
turn of the main rdkafka thread loop, which would add an unnecessary
second of latency.
Inspired by c64b652.

Details follow.
The logs that inspired this change are here: https://gist.github.com/benesch/787791fa96cc75660ecb29cd37d49ddc. Warning: there are some additional debugging log lines that I patched in myself.
The relevant lines are these:
May 03 00:40:46.553 DEBUG librdkafka: librdkafka: BRKMAIN [thrd:marmoset:9092/0]: marmoset:9092/0: Enter main broker thread
May 03 00:40:47.542 DEBUG librdkafka: librdkafka: CGRPQUERY [thrd:main]: localhost:9092/bootstrap: Group ""materialize-kafka-u1/u2"": querying for coordinator: intervaled in state query-coord    

Notice the nearly 1s gap between when the broker is connected, and when the cgrp actually starts querying for the coordinator! This turned out to be a really unfortunate artifact of timing.
The cgrp is created first, when there are no brokers. This request understandably fails:
May 03 00:40:46.544 DEBUG librdkafka: librdkafka: CGRPQUERY [thrd:main]: Group ""materialize-kafka-u1/u2"": no broker available for coordinator query: intervaled in state query-coord

The broker connection happens just milliseconds later:
May 03 00:40:46.551 DEBUG librdkafka: librdkafka: STATE [thrd:localhost:9092/bootstrap]: localhost:9092/bootstrap: Broker changed state APIVERSION_QUERY -> UP    

This immediately triggers a call to rd_kafka_cgrp_serve (log line is something.I added myself)
May 03 00:40:46.552 DEBUG librdkafka: librdkafka: CGRPLOG [thrd:main]: serving state query-coord  

but that call doesn't actually query for the coordinator, because we queried for the coordinator less than 500ms ago! We're then forced to wait not just 500ms, but a full 1000ms, because we need to wait out a full turn of the main rdkafka thread loop, and there are no events that wake it up before it hits the 1000ms max timeout.
The fix here is to reset the coord-query interval if there were no brokers availablethat operation wasn't expensive at all, so might as well try it again ASAP. And indeed, in combination with c64b652, this brings the consumer initialization time down to ~150ms, which is much more pleasant than the 5s+ we were seeing before.",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2865,2020-05-03T06:12:41Z,2020-05-04T07:15:56Z,2020-05-04T07:16:01Z,MERGED,True,3,0,1,https://github.com/benesch,Shave another 1s off cgrp initialization,1,['GREAT REPORT'],https://github.com/edenhill/librdkafka/pull/2865,https://github.com/edenhill,2,https://github.com/edenhill/librdkafka/pull/2865#issuecomment-623298468,"Querying for the cgrp coordinator does not need to back off when there
are no brokers connected, as no requests were sent. This avoids an
accident of timing that would cause cgrp initialization to wait an extra
turn of the main rdkafka thread loop, which would add an unnecessary
second of latency.
Inspired by c64b652.

Details follow.
The logs that inspired this change are here: https://gist.github.com/benesch/787791fa96cc75660ecb29cd37d49ddc. Warning: there are some additional debugging log lines that I patched in myself.
The relevant lines are these:
May 03 00:40:46.553 DEBUG librdkafka: librdkafka: BRKMAIN [thrd:marmoset:9092/0]: marmoset:9092/0: Enter main broker thread
May 03 00:40:47.542 DEBUG librdkafka: librdkafka: CGRPQUERY [thrd:main]: localhost:9092/bootstrap: Group ""materialize-kafka-u1/u2"": querying for coordinator: intervaled in state query-coord    

Notice the nearly 1s gap between when the broker is connected, and when the cgrp actually starts querying for the coordinator! This turned out to be a really unfortunate artifact of timing.
The cgrp is created first, when there are no brokers. This request understandably fails:
May 03 00:40:46.544 DEBUG librdkafka: librdkafka: CGRPQUERY [thrd:main]: Group ""materialize-kafka-u1/u2"": no broker available for coordinator query: intervaled in state query-coord

The broker connection happens just milliseconds later:
May 03 00:40:46.551 DEBUG librdkafka: librdkafka: STATE [thrd:localhost:9092/bootstrap]: localhost:9092/bootstrap: Broker changed state APIVERSION_QUERY -> UP    

This immediately triggers a call to rd_kafka_cgrp_serve (log line is something.I added myself)
May 03 00:40:46.552 DEBUG librdkafka: librdkafka: CGRPLOG [thrd:main]: serving state query-coord  

but that call doesn't actually query for the coordinator, because we queried for the coordinator less than 500ms ago! We're then forced to wait not just 500ms, but a full 1000ms, because we need to wait out a full turn of the main rdkafka thread loop, and there are no events that wake it up before it hits the 1000ms max timeout.
The fix here is to reset the coord-query interval if there were no brokers availablethat operation wasn't expensive at all, so might as well try it again ASAP. And indeed, in combination with c64b652, this brings the consumer initialization time down to ~150ms, which is much more pleasant than the 5s+ we were seeing before.",Superb! Thank you!,True,"{'HEART': ['https://github.com/benesch'], 'THUMBS_UP': ['https://github.com/TrevorDArcyEvans']}"
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2867,2020-05-04T14:20:43Z,2020-05-05T19:49:23Z,2020-05-05T19:49:26Z,MERGED,True,425,6,5,https://github.com/edenhill,Fix crash in roundrobin assignor for asymmetrical subscriptions (#2121),2,[],https://github.com/edenhill/librdkafka/pull/2867,https://github.com/edenhill,1,https://github.com/edenhill/librdkafka/pull/2867,This also adds declarative unit tests of the assignors.,This also adds declarative unit tests of the assignors.,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2871,2020-05-06T07:43:16Z,2020-05-08T06:27:44Z,2020-05-08T06:27:47Z,MERGED,True,2,6,1,https://github.com/edenhill,Always log broker errors (unless supressed) regardless of error callback,1,[],https://github.com/edenhill/librdkafka/pull/2871,https://github.com/edenhill,1,https://github.com/edenhill/librdkafka/pull/2871,@mhowlett was right,@mhowlett was right,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2872,2020-05-06T10:58:08Z,2020-05-11T07:22:53Z,2020-05-11T07:22:56Z,MERGED,True,8,5,5,https://github.com/edenhill,Avoid test timeouts on CI,2,[],https://github.com/edenhill/librdkafka/pull/2872,https://github.com/edenhill,1,https://github.com/edenhill/librdkafka/pull/2872,And bump version define to v1.5.0..,And bump version define to v1.5.0..,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2873,2020-05-06T11:42:16Z,2020-05-11T11:16:15Z,2020-05-11T11:16:18Z,MERGED,True,187,127,7,https://github.com/edenhill,Prefer least-idle connection,2,[],https://github.com/edenhill/librdkafka/pull/2873,https://github.com/edenhill,1,https://github.com/edenhill/librdkafka/pull/2873,,,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2874,2020-05-06T11:42:42Z,2020-05-11T07:24:27Z,2020-05-11T07:24:30Z,MERGED,True,15,8,3,https://github.com/edenhill,make: don't depend on linker script if not enabled,1,[],https://github.com/edenhill/librdkafka/pull/2874,https://github.com/edenhill,1,https://github.com/edenhill/librdkafka/pull/2874,"This prevents the shared libs to be re-generated on each 'make',
including 'make install', even if there were no changes, which in the case
of 'sudo make install' would create shared libs as the root user messing up the build dir permissions.","This prevents the shared libs to be re-generated on each 'make',
including 'make install', even if there were no changes, which in the case
of 'sudo make install' would create shared libs as the root user messing up the build dir permissions.",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2876,2020-05-07T20:22:22Z,2020-05-08T06:28:19Z,2020-05-08T06:28:22Z,MERGED,True,4,8,1,https://github.com/edenhill,Don't use cached END offset on offset reset (follow-up on #2782),1,[],https://github.com/edenhill/librdkafka/pull/2876,https://github.com/edenhill,1,https://github.com/edenhill/librdkafka/pull/2876,,,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2877,2020-05-08T08:53:38Z,2020-05-11T07:23:52Z,2020-05-11T07:23:56Z,MERGED,True,31,2,2,https://github.com/edenhill,Restart/abort req timeout scan if broker went down to avoid crash (#2326),1,['bug'],https://github.com/edenhill/librdkafka/pull/2877,https://github.com/edenhill,1,https://github.com/edenhill/librdkafka/pull/2877,,,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2881,2020-05-11T16:09:06Z,2020-05-12T15:16:20Z,2020-05-12T15:16:23Z,MERGED,True,28,27,2,https://github.com/edenhill,Support all Fetch versions 0..11 to support ZStd on AK 2.2.1 (#2880),1,[],https://github.com/edenhill/librdkafka/pull/2881,https://github.com/edenhill,1,https://github.com/edenhill/librdkafka/pull/2881,,,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2882,2020-05-12T07:38:01Z,2020-05-13T08:25:48Z,2020-05-14T05:08:18Z,MERGED,True,20,10,5,https://github.com/edenhill,Use CXX instead of CC to link C++ libs/programs so that -lstdc++ is not needed (#2878),1,[],https://github.com/edenhill/librdkafka/pull/2882,https://github.com/edenhill,1,https://github.com/edenhill/librdkafka/pull/2882,,,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2884,2020-05-13T14:01:22Z,2020-05-28T15:05:03Z,2020-05-28T15:10:16Z,MERGED,True,648,175,21,https://github.com/mhowlett,KIP-429: Incremental rebalancing,11,[],https://github.com/edenhill/librdkafka/pull/2884,https://github.com/mhowlett,1,https://github.com/edenhill/librdkafka/pull/2884,"This PR is just a start. It includes:

changes to allow for stateful assignors.
changes for KIP-54 except the actual assignor implementation.
owned partitions on the wire (not used by kip-54, but ready for kip-429).

I've explicitly considered/checked compatibility with the Java consumer as appropriate.","This PR is just a start. It includes:

changes to allow for stateful assignors.
changes for KIP-54 except the actual assignor implementation.
owned partitions on the wire (not used by kip-54, but ready for kip-429).

I've explicitly considered/checked compatibility with the Java consumer as appropriate.",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2884,2020-05-13T14:01:22Z,2020-05-28T15:05:03Z,2020-05-28T15:10:16Z,MERGED,True,648,175,21,https://github.com/mhowlett,KIP-429: Incremental rebalancing,11,[],https://github.com/edenhill/librdkafka/pull/2884,https://github.com/mhowlett,2,https://github.com/edenhill/librdkafka/pull/2884#issuecomment-632410434,"This PR is just a start. It includes:

changes to allow for stateful assignors.
changes for KIP-54 except the actual assignor implementation.
owned partitions on the wire (not used by kip-54, but ready for kip-429).

I've explicitly considered/checked compatibility with the Java consumer as appropriate.",this is ready for re-review. just tacked on supported protocols field as well.,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2884,2020-05-13T14:01:22Z,2020-05-28T15:05:03Z,2020-05-28T15:10:16Z,MERGED,True,648,175,21,https://github.com/mhowlett,KIP-429: Incremental rebalancing,11,[],https://github.com/edenhill/librdkafka/pull/2884,https://github.com/edenhill,3,https://github.com/edenhill/librdkafka/pull/2884#issuecomment-634777589,"This PR is just a start. It includes:

changes to allow for stateful assignors.
changes for KIP-54 except the actual assignor implementation.
owned partitions on the wire (not used by kip-54, but ready for kip-429).

I've explicitly considered/checked compatibility with the Java consumer as appropriate.","There is a build failure:
https://travis-ci.org/github/edenhill/librdkafka/jobs/691794391#L725",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2884,2020-05-13T14:01:22Z,2020-05-28T15:05:03Z,2020-05-28T15:10:16Z,MERGED,True,648,175,21,https://github.com/mhowlett,KIP-429: Incremental rebalancing,11,[],https://github.com/edenhill/librdkafka/pull/2884,https://github.com/edenhill,4,https://github.com/edenhill/librdkafka/pull/2884#issuecomment-635397888,"This PR is just a start. It includes:

changes to allow for stateful assignors.
changes for KIP-54 except the actual assignor implementation.
owned partitions on the wire (not used by kip-54, but ready for kip-429).

I've explicitly considered/checked compatibility with the Java consumer as appropriate.","This PR will need to be squashed due to unclean commit history, but that should be fine since it is basically one change",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2887,2020-05-14T00:31:00Z,2020-05-14T13:25:46Z,2020-05-14T13:32:00Z,MERGED,True,9,0,1,https://github.com/mpribble,mock: Make rdkafka_mock.h c++ compatible (#2885),1,[],https://github.com/edenhill/librdkafka/pull/2887,https://github.com/mpribble,1,https://github.com/edenhill/librdkafka/pull/2887,Allow rdkafka_mock.h to be included from c and c++ programs.,Allow rdkafka_mock.h to be included from c and c++ programs.,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2887,2020-05-14T00:31:00Z,2020-05-14T13:25:46Z,2020-05-14T13:32:00Z,MERGED,True,9,0,1,https://github.com/mpribble,mock: Make rdkafka_mock.h c++ compatible (#2885),1,[],https://github.com/edenhill/librdkafka/pull/2887,https://github.com/edenhill,2,https://github.com/edenhill/librdkafka/pull/2887#issuecomment-628633502,Allow rdkafka_mock.h to be included from c and c++ programs.,Thank you!,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2891,2020-05-19T09:00:24Z,2020-05-27T20:20:26Z,2020-05-27T20:20:28Z,MERGED,True,924,5,9,https://github.com/edenhill,Add simple generic and strictly typed hash maps,1,[],https://github.com/edenhill/librdkafka/pull/2891,https://github.com/edenhill,1,https://github.com/edenhill/librdkafka/pull/2891,"This will be used by the sticky assignor, but thought I'd easen up that future PR by filing this one separately.","This will be used by the sticky assignor, but thought I'd easen up that future PR by filing this one separately.",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2891,2020-05-19T09:00:24Z,2020-05-27T20:20:26Z,2020-05-27T20:20:28Z,MERGED,True,924,5,9,https://github.com/edenhill,Add simple generic and strictly typed hash maps,1,[],https://github.com/edenhill/librdkafka/pull/2891,https://github.com/edenhill,2,https://github.com/edenhill/librdkafka/pull/2891#issuecomment-633413808,"This will be used by the sticky assignor, but thought I'd easen up that future PR by filing this one separately.","A proper review would be good, it is written from scratch.
But we can postpone until the sticky PR if you want.",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2891,2020-05-19T09:00:24Z,2020-05-27T20:20:26Z,2020-05-27T20:20:28Z,MERGED,True,924,5,9,https://github.com/edenhill,Add simple generic and strictly typed hash maps,1,[],https://github.com/edenhill/librdkafka/pull/2891,https://github.com/mhowlett,3,https://github.com/edenhill/librdkafka/pull/2891#issuecomment-634833812,"This will be used by the sticky assignor, but thought I'd easen up that future PR by filing this one separately.",lgtm after comments addressed to your satisfaction.,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2892,2020-05-19T13:38:08Z,2020-05-21T08:45:07Z,2020-05-21T08:45:13Z,MERGED,True,26,18,3,https://github.com/neptoess,Upgrade pacman before the full msys2 upgrade,11,[],https://github.com/edenhill/librdkafka/pull/2892,https://github.com/neptoess,1,https://github.com/edenhill/librdkafka/pull/2892,"This should fix the MinGW travis errors, e.g. https://travis-ci.org/github/edenhill/librdkafka/jobs/688737080","This should fix the MinGW travis errors, e.g. https://travis-ci.org/github/edenhill/librdkafka/jobs/688737080",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2892,2020-05-19T13:38:08Z,2020-05-21T08:45:07Z,2020-05-21T08:45:13Z,MERGED,True,26,18,3,https://github.com/neptoess,Upgrade pacman before the full msys2 upgrade,11,[],https://github.com/edenhill/librdkafka/pull/2892,https://github.com/neptoess,2,https://github.com/edenhill/librdkafka/pull/2892#issuecomment-631032411,"This should fix the MinGW travis errors, e.g. https://travis-ci.org/github/edenhill/librdkafka/jobs/688737080","@edenhill ,
Integration tests are taking a while, but the MinGW build is all green. Does this PR need anything else?",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2892,2020-05-19T13:38:08Z,2020-05-21T08:45:07Z,2020-05-21T08:45:13Z,MERGED,True,26,18,3,https://github.com/neptoess,Upgrade pacman before the full msys2 upgrade,11,[],https://github.com/edenhill/librdkafka/pull/2892,https://github.com/neptoess,3,https://github.com/edenhill/librdkafka/pull/2892#issuecomment-631096328,"This should fix the MinGW travis errors, e.g. https://travis-ci.org/github/edenhill/librdkafka/jobs/688737080",Merge away,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2892,2020-05-19T13:38:08Z,2020-05-21T08:45:07Z,2020-05-21T08:45:13Z,MERGED,True,26,18,3,https://github.com/neptoess,Upgrade pacman before the full msys2 upgrade,11,[],https://github.com/edenhill/librdkafka/pull/2892,https://github.com/ed-alertedh,4,https://github.com/edenhill/librdkafka/pull/2892#issuecomment-631807434,"This should fix the MinGW travis errors, e.g. https://travis-ci.org/github/edenhill/librdkafka/jobs/688737080","TL;DR: LGTM, also happy to merge this :)",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2892,2020-05-19T13:38:08Z,2020-05-21T08:45:07Z,2020-05-21T08:45:13Z,MERGED,True,26,18,3,https://github.com/neptoess,Upgrade pacman before the full msys2 upgrade,11,[],https://github.com/edenhill/librdkafka/pull/2892,https://github.com/edenhill,5,https://github.com/edenhill/librdkafka/pull/2892#issuecomment-631965769,"This should fix the MinGW travis errors, e.g. https://travis-ci.org/github/edenhill/librdkafka/jobs/688737080",Thank you!,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2893,2020-05-19T14:33:34Z,2020-05-26T07:58:52Z,2020-07-06T13:31:20Z,MERGED,True,9,7,2,https://github.com/edenhill,Don't trigger broker connection for partitions that are not to be fetched (#2826),1,[],https://github.com/edenhill/librdkafka/pull/2893,https://github.com/edenhill,1,https://github.com/edenhill/librdkafka/pull/2893,,,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2894,2020-05-19T16:14:14Z,2020-05-20T08:08:05Z,2020-05-20T08:08:38Z,MERGED,True,1,0,1,https://github.com/Marwes,fix: Don't leak messages created by rd_kafka_message_new,1,[],https://github.com/edenhill/librdkafka/pull/2894,https://github.com/Marwes,1,https://github.com/edenhill/librdkafka/pull/2894,"Since RD_KAFKA_MSG_F_FREE_RKM is not set, the message would not be
freed when rd_kafka_msg_destroy.

  
    
      librdkafka/src/rdkafka_msg.c
    
    
        Lines 103 to 104
      in
      2e53c0f
    
  
  
    

        
          
           if (rkm->rkm_flags & RD_KAFKA_MSG_F_FREE_RKM) 
        

        
          
           	rd_free(rkm); 
        
    
  


I might very well be missing something about this, but it definitely looks like any messages created with rd_kafka_message_new will leak even if rd_kafka_message_destroy is called.","Since RD_KAFKA_MSG_F_FREE_RKM is not set, the message would not be
freed when rd_kafka_msg_destroy.

  
    
      librdkafka/src/rdkafka_msg.c
    
    
        Lines 103 to 104
      in
      2e53c0f
    
  
  
    

        
          
           if (rkm->rkm_flags & RD_KAFKA_MSG_F_FREE_RKM) 
        

        
          
           	rd_free(rkm); 
        
    
  


I might very well be missing something about this, but it definitely looks like any messages created with rd_kafka_message_new will leak even if rd_kafka_message_destroy is called.",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2894,2020-05-19T16:14:14Z,2020-05-20T08:08:05Z,2020-05-20T08:08:38Z,MERGED,True,1,0,1,https://github.com/Marwes,fix: Don't leak messages created by rd_kafka_message_new,1,[],https://github.com/edenhill/librdkafka/pull/2894,https://github.com/edenhill,2,https://github.com/edenhill/librdkafka/pull/2894#issuecomment-631312131,"Since RD_KAFKA_MSG_F_FREE_RKM is not set, the message would not be
freed when rd_kafka_msg_destroy.

  
    
      librdkafka/src/rdkafka_msg.c
    
    
        Lines 103 to 104
      in
      2e53c0f
    
  
  
    

        
          
           if (rkm->rkm_flags & RD_KAFKA_MSG_F_FREE_RKM) 
        

        
          
           	rd_free(rkm); 
        
    
  


I might very well be missing something about this, but it definitely looks like any messages created with rd_kafka_message_new will leak even if rd_kafka_message_destroy is called.",Thank you!,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2899,2020-05-22T14:32:04Z,2020-05-26T08:26:23Z,2020-05-26T08:26:27Z,MERGED,True,1,1,1,https://github.com/sky92zwq,"the configure script does not work correctly when configuration is  like ""--LDFLAGS='-Wl,--build-id=none'""",1,[],https://github.com/edenhill/librdkafka/pull/2899,https://github.com/sky92zwq,1,https://github.com/edenhill/librdkafka/pull/2899,"./configure --LDFLAGS='-Wl,--build-id=none'
there are two '=' in a configration, then  name=""${opt%=*}"" will Intercept LDFLAGS=-Wl,--build-id as name.
so  name=""${opt%%=*}""may be a good choice to Intercept LDFLAGS out.","./configure --LDFLAGS='-Wl,--build-id=none'
there are two '=' in a configration, then  name=""${opt%=*}"" will Intercept LDFLAGS=-Wl,--build-id as name.
so  name=""${opt%%=*}""may be a good choice to Intercept LDFLAGS out.",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2899,2020-05-22T14:32:04Z,2020-05-26T08:26:23Z,2020-05-26T08:26:27Z,MERGED,True,1,1,1,https://github.com/sky92zwq,"the configure script does not work correctly when configuration is  like ""--LDFLAGS='-Wl,--build-id=none'""",1,[],https://github.com/edenhill/librdkafka/pull/2899,https://github.com/edenhill,2,https://github.com/edenhill/librdkafka/pull/2899#issuecomment-633884095,"./configure --LDFLAGS='-Wl,--build-id=none'
there are two '=' in a configration, then  name=""${opt%=*}"" will Intercept LDFLAGS=-Wl,--build-id as name.
so  name=""${opt%%=*}""may be a good choice to Intercept LDFLAGS out.",Thank you!,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2902,2020-05-25T18:07:38Z,2020-05-27T08:42:21Z,2020-05-27T08:42:24Z,MERGED,True,308,14,5,https://github.com/edenhill,Add rd_kafka_produceva() using a vtype array as an alternative to va-args,2,[],https://github.com/edenhill/librdkafka/pull/2902,https://github.com/edenhill,1,https://github.com/edenhill/librdkafka/pull/2902,"It is problematic to construct va-arg lists for calling C in some high-level languages that wrap librdkafka, erlang in the case of #2895.
This new method allows constructing an array of structs instead.","It is problematic to construct va-arg lists for calling C in some high-level languages that wrap librdkafka, erlang in the case of #2895.
This new method allows constructing an array of structs instead.",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2902,2020-05-25T18:07:38Z,2020-05-27T08:42:21Z,2020-05-27T08:42:24Z,MERGED,True,308,14,5,https://github.com/edenhill,Add rd_kafka_produceva() using a vtype array as an alternative to va-args,2,[],https://github.com/edenhill/librdkafka/pull/2902,https://github.com/andrewthad,2,https://github.com/edenhill/librdkafka/pull/2902#issuecomment-633676591,"It is problematic to construct va-arg lists for calling C in some high-level languages that wrap librdkafka, erlang in the case of #2895.
This new method allows constructing an array of structs instead.","This looks like exactly what I was hoping for. Thanks. It's probably worth getting feedback from maintainers of librdkafka bindings in other languages. The bindings I'm working on are for Haskell, and I can confirm that this strategy works well for Haskell's foreign function interface and FFI-related tooling.",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2903,2020-05-26T08:36:13Z,2020-05-26T14:53:18Z,2020-05-26T14:53:22Z,MERGED,True,8,3,3,https://github.com/edenhill,configure args now take precedence over cached variables,1,[],https://github.com/edenhill/librdkafka/pull/2903,https://github.com/edenhill,1,https://github.com/edenhill/librdkafka/pull/2903,,,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2905,2020-05-27T09:04:22Z,2020-05-27T14:59:47Z,2020-05-27T14:59:52Z,MERGED,True,203,58,20,https://github.com/edenhill,Only trigger consumer group rejoin on complete Metadata responses,6,[],https://github.com/edenhill/librdkafka/pull/2905,https://github.com/edenhill,1,https://github.com/edenhill/librdkafka/pull/2905,..and some other fixes,..and some other fixes,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2907,2020-05-28T08:44:15Z,2020-05-28T20:37:04Z,2020-05-29T06:53:50Z,MERGED,True,98,18,1,https://github.com/edenhill,"Test 0097 was always skipped, and added new test case.",1,[],https://github.com/edenhill/librdkafka/pull/2907,https://github.com/edenhill,1,https://github.com/edenhill/librdkafka/pull/2907,,,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2908,2020-05-28T22:42:18Z,2020-07-08T15:07:58Z,2020-07-08T15:07:58Z,MERGED,True,80,8,8,https://github.com/mhowlett,lost partition notification for EAGER rebalance protocol,8,[],https://github.com/edenhill/librdkafka/pull/2908,https://github.com/mhowlett,1,https://github.com/edenhill/librdkafka/pull/2908,"ok, let's see what you think of this.
many less lines of code and much cleaner than adding RD_KAFKA_RESP_ERR__LOST_PARTITIONS, which is a good indicator it's better.","ok, let's see what you think of this.
many less lines of code and much cleaner than adding RD_KAFKA_RESP_ERR__LOST_PARTITIONS, which is a good indicator it's better.",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2908,2020-05-28T22:42:18Z,2020-07-08T15:07:58Z,2020-07-08T15:07:58Z,MERGED,True,80,8,8,https://github.com/mhowlett,lost partition notification for EAGER rebalance protocol,8,[],https://github.com/edenhill/librdkafka/pull/2908,https://github.com/mhowlett,2,https://github.com/edenhill/librdkafka/pull/2908#issuecomment-636951229,"ok, let's see what you think of this.
many less lines of code and much cleaner than adding RD_KAFKA_RESP_ERR__LOST_PARTITIONS, which is a good indicator it's better.","Since assignment_lost is cleared in the poll callback now (but not in the event_t handlers) we're half way there.

I think you mean via background_event_cb. I believe I follow the path of how all this works. I think this is a good example of the primary disadvantage of the queuing / async architecture of librdkafka - at the point the event is produced, there is no easy way to directly trace through the follow on effects. By contrast, with synchronous code, that is easy. To be able to work on librdkafka, I feel i need a pretty good mental model of how everything fits together to have reasonable confidence i'm not missing anything. Of course async message architecture has some advantages too.",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2908,2020-05-28T22:42:18Z,2020-07-08T15:07:58Z,2020-07-08T15:07:58Z,MERGED,True,80,8,8,https://github.com/mhowlett,lost partition notification for EAGER rebalance protocol,8,[],https://github.com/edenhill/librdkafka/pull/2908,https://github.com/mhowlett,3,https://github.com/edenhill/librdkafka/pull/2908#issuecomment-636987438,"ok, let's see what you think of this.
many less lines of code and much cleaner than adding RD_KAFKA_RESP_ERR__LOST_PARTITIONS, which is a good indicator it's better.","My gut feeling is that we want the assignment_lost to live on the OP_REBALANCE so/since it is associated with a specific rebalance, which would also void the need for locking the rkcg_assignment_lost.

I think that's definitely the best way to be modeling this requirement, but I don't see how you can expose that via a rd_kafka_assignment_lost (rd_kafka_t *rk) method. It's what I was effectively doing before when I had separate REVOKE and LOST event types.
rd_kafka_assignment_lost is conceptually checking the state of rk (cgrp), so the impl. matches that, so i think it's good.
I think this comes down to deciding for / against the additional rebalance err type. I'm still liking the current method, despite it not matching up to the problem perfectly (but nor does a separate LOST event type i think, since I think that should be probably more a property of a REVOKE event).",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2908,2020-05-28T22:42:18Z,2020-07-08T15:07:58Z,2020-07-08T15:07:58Z,MERGED,True,80,8,8,https://github.com/mhowlett,lost partition notification for EAGER rebalance protocol,8,[],https://github.com/edenhill/librdkafka/pull/2908,https://github.com/mhowlett,4,https://github.com/edenhill/librdkafka/pull/2908#issuecomment-644339047,"ok, let's see what you think of this.
many less lines of code and much cleaner than adding RD_KAFKA_RESP_ERR__LOST_PARTITIONS, which is a good indicator it's better.",shall we get this merged? the incr assign pr is pretty much ready to open now i think.,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2909,2020-05-29T12:44:16Z,2020-06-18T16:12:48Z,2020-06-18T16:12:55Z,MERGED,True,9,0,3,https://github.com/stosem,fix ssize_t definition in mingw32,1,[],https://github.com/edenhill/librdkafka/pull/2909,https://github.com/stosem,1,https://github.com/edenhill/librdkafka/pull/2909,"Building librdkafka with msys2 for win32 (i686) target failed with:
librdkafka\src\rdwin32.h:50:17: error: conflicting types for 'ssize_t'
   50 | typedef SSIZE_T ssize_t;
      |                 ^~~~~~~
In file included from C:/msys64/mingw32/i686-w64-mingw32/include/corecrt_stdio_config.h:10,
                 from C:/msys64/mingw32/i686-w64-mingw32/include/stdio.h:9,
                 from C:\workspace\librdkafka\src\rd.h:48,
                 from C:\workspace\librdkafka\src\crc32c.c:48:
C:/msys64/mingw32/i686-w64-mingw32/include/corecrt.h:52:13: note: previous declaration of 'ssize_t' was here
   52 | typedef int ssize_t;
      |             ^~~~~~~

this patch will fix this issue.","Building librdkafka with msys2 for win32 (i686) target failed with:
librdkafka\src\rdwin32.h:50:17: error: conflicting types for 'ssize_t'
   50 | typedef SSIZE_T ssize_t;
      |                 ^~~~~~~
In file included from C:/msys64/mingw32/i686-w64-mingw32/include/corecrt_stdio_config.h:10,
                 from C:/msys64/mingw32/i686-w64-mingw32/include/stdio.h:9,
                 from C:\workspace\librdkafka\src\rd.h:48,
                 from C:\workspace\librdkafka\src\crc32c.c:48:
C:/msys64/mingw32/i686-w64-mingw32/include/corecrt.h:52:13: note: previous declaration of 'ssize_t' was here
   52 | typedef int ssize_t;
      |             ^~~~~~~

this patch will fix this issue.",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2909,2020-05-29T12:44:16Z,2020-06-18T16:12:48Z,2020-06-18T16:12:55Z,MERGED,True,9,0,3,https://github.com/stosem,fix ssize_t definition in mingw32,1,[],https://github.com/edenhill/librdkafka/pull/2909,https://github.com/edenhill,2,https://github.com/edenhill/librdkafka/pull/2909#issuecomment-646132568,"Building librdkafka with msys2 for win32 (i686) target failed with:
librdkafka\src\rdwin32.h:50:17: error: conflicting types for 'ssize_t'
   50 | typedef SSIZE_T ssize_t;
      |                 ^~~~~~~
In file included from C:/msys64/mingw32/i686-w64-mingw32/include/corecrt_stdio_config.h:10,
                 from C:/msys64/mingw32/i686-w64-mingw32/include/stdio.h:9,
                 from C:\workspace\librdkafka\src\rd.h:48,
                 from C:\workspace\librdkafka\src\crc32c.c:48:
C:/msys64/mingw32/i686-w64-mingw32/include/corecrt.h:52:13: note: previous declaration of 'ssize_t' was here
   52 | typedef int ssize_t;
      |             ^~~~~~~

this patch will fix this issue.",Thank you!,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2911,2020-05-31T00:13:23Z,2020-09-30T10:20:38Z,2020-09-30T10:20:38Z,CLOSED,False,11,4,3,https://github.com/wbb1975,Add some error message when failed in decompressing kafka message in gzip,2,[],https://github.com/edenhill/librdkafka/pull/2911,https://github.com/wbb1975,1,https://github.com/edenhill/librdkafka/pull/2911,"In AWS Lambda enviroment, sometimes rd_kafka_msgset_reader_decompress will fail in decompressing kafka messages in gzip because of no enough memory. But from existing logs, you couldn't find any clue about it--instead the system will tell you an invalid compressed data.","In AWS Lambda enviroment, sometimes rd_kafka_msgset_reader_decompress will fail in decompressing kafka messages in gzip because of no enough memory. But from existing logs, you couldn't find any clue about it--instead the system will tell you an invalid compressed data.",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2913,2020-06-02T08:17:39Z,2020-06-18T16:11:05Z,2020-06-18T16:11:08Z,MERGED,True,395,95,17,https://github.com/edenhill,"Txn fixes: crash on error, and setting retriable flag",14,['bug'],https://github.com/edenhill/librdkafka/pull/2913,https://github.com/edenhill,1,https://github.com/edenhill/librdkafka/pull/2913,,,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2913,2020-06-02T08:17:39Z,2020-06-18T16:11:05Z,2020-06-18T16:11:08Z,MERGED,True,395,95,17,https://github.com/edenhill,"Txn fixes: crash on error, and setting retriable flag",14,['bug'],https://github.com/edenhill/librdkafka/pull/2913,https://github.com/edenhill,2,https://github.com/edenhill/librdkafka/pull/2913#issuecomment-643204263,,"Added tests, additional fixes.
Please re-review as soon as possible.",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2916,2020-06-03T02:44:56Z,2020-07-02T13:11:00Z,2020-07-02T17:01:48Z,MERGED,True,120,28,5,https://github.com/benesch,Ensure known toppars do not wind up in desired partitions list,1,[],https://github.com/edenhill/librdkafka/pull/2916,https://github.com/benesch,1,https://github.com/edenhill/librdkafka/pull/2916,"rd_kafka_cgrp_assign calls rd_kafka_toppar_desired_add0 rather than its
wrapper rd_kafka_toppar_desired_add. The ""add"" wrapper preserves the
invariant that a known topic should never get added to the desired
partitions queue, while the ""add0"" function does not.
Maintaining this invariant is important for
rd_kafka_topic_partition_cnt_update, which assumes that a toppar is in
either the list of known partitions or the list of desired partitions,
but not both. Violating this invariant results in the situation
described in #2915, where updating assignments can trigger incorrect
""unknown partition"" errors.
This patch rearranges rd_kafka_toppar_desired_add/add0 so that add0, in
addition to add, will avoid adding known partitions to the desired
partition list. The enclosed test correctly fails if run against the
current master (for the reasons described above).
Fix #2915.","rd_kafka_cgrp_assign calls rd_kafka_toppar_desired_add0 rather than its
wrapper rd_kafka_toppar_desired_add. The ""add"" wrapper preserves the
invariant that a known topic should never get added to the desired
partitions queue, while the ""add0"" function does not.
Maintaining this invariant is important for
rd_kafka_topic_partition_cnt_update, which assumes that a toppar is in
either the list of known partitions or the list of desired partitions,
but not both. Violating this invariant results in the situation
described in #2915, where updating assignments can trigger incorrect
""unknown partition"" errors.
This patch rearranges rd_kafka_toppar_desired_add/add0 so that add0, in
addition to add, will avoid adding known partitions to the desired
partition list. The enclosed test correctly fails if run against the
current master (for the reasons described above).
Fix #2915.",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2916,2020-06-03T02:44:56Z,2020-07-02T13:11:00Z,2020-07-02T17:01:48Z,MERGED,True,120,28,5,https://github.com/benesch,Ensure known toppars do not wind up in desired partitions list,1,[],https://github.com/edenhill/librdkafka/pull/2916,https://github.com/edenhill,2,https://github.com/edenhill/librdkafka/pull/2916#issuecomment-652996659,"rd_kafka_cgrp_assign calls rd_kafka_toppar_desired_add0 rather than its
wrapper rd_kafka_toppar_desired_add. The ""add"" wrapper preserves the
invariant that a known topic should never get added to the desired
partitions queue, while the ""add0"" function does not.
Maintaining this invariant is important for
rd_kafka_topic_partition_cnt_update, which assumes that a toppar is in
either the list of known partitions or the list of desired partitions,
but not both. Violating this invariant results in the situation
described in #2915, where updating assignments can trigger incorrect
""unknown partition"" errors.
This patch rearranges rd_kafka_toppar_desired_add/add0 so that add0, in
addition to add, will avoid adding known partitions to the desired
partition list. The enclosed test correctly fails if run against the
current master (for the reasons described above).
Fix #2915.","Fantastic, thank you!",True,{'THUMBS_UP': ['https://github.com/benesch']}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2924,2020-06-06T13:08:32Z,2020-06-16T19:15:39Z,2020-06-16T19:15:44Z,MERGED,True,1,1,1,https://github.com/omartijn,Fix incorrect cmake Module path for submodule use,1,[],https://github.com/edenhill/librdkafka/pull/2924,https://github.com/omartijn,1,https://github.com/edenhill/librdkafka/pull/2924,"When librdkafka is added to a project using add_subdirectory, the
include path for CMake modules that is added is incorrect, since it uses
CMAKE_SOURCE_DIR - which is the source directory of the root of the
project instead of CMAKE_CURRENT_SOURCE_DIR.
This matters not when librdkafka is built standalone (since the two
directories are the same), but when built as a submodule, it prevents
CMake from finding the bundled modules, like FindLibLZ4.cmake.","When librdkafka is added to a project using add_subdirectory, the
include path for CMake modules that is added is incorrect, since it uses
CMAKE_SOURCE_DIR - which is the source directory of the root of the
project instead of CMAKE_CURRENT_SOURCE_DIR.
This matters not when librdkafka is built standalone (since the two
directories are the same), but when built as a submodule, it prevents
CMake from finding the bundled modules, like FindLibLZ4.cmake.",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2924,2020-06-06T13:08:32Z,2020-06-16T19:15:39Z,2020-06-16T19:15:44Z,MERGED,True,1,1,1,https://github.com/omartijn,Fix incorrect cmake Module path for submodule use,1,[],https://github.com/edenhill/librdkafka/pull/2924,https://github.com/edenhill,2,https://github.com/edenhill/librdkafka/pull/2924#issuecomment-644961899,"When librdkafka is added to a project using add_subdirectory, the
include path for CMake modules that is added is incorrect, since it uses
CMAKE_SOURCE_DIR - which is the source directory of the root of the
project instead of CMAKE_CURRENT_SOURCE_DIR.
This matters not when librdkafka is built standalone (since the two
directories are the same), but when built as a submodule, it prevents
CMake from finding the bundled modules, like FindLibLZ4.cmake.",Thank you!,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2926,2020-06-07T19:22:28Z,2020-07-02T13:51:57Z,2020-07-02T13:52:01Z,MERGED,True,17,17,8,https://github.com/omartijn,Const correctness,3,[],https://github.com/edenhill/librdkafka/pull/2926,https://github.com/omartijn,1,https://github.com/edenhill/librdkafka/pull/2926,Improve const correctness by allowing Conf pointers to be const-qualified when constructing Handles. fatal_error member functions are now also const-qualified.,Improve const correctness by allowing Conf pointers to be const-qualified when constructing Handles. fatal_error member functions are now also const-qualified.,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2926,2020-06-07T19:22:28Z,2020-07-02T13:51:57Z,2020-07-02T13:52:01Z,MERGED,True,17,17,8,https://github.com/omartijn,Const correctness,3,[],https://github.com/edenhill/librdkafka/pull/2926,https://github.com/edenhill,2,https://github.com/edenhill/librdkafka/pull/2926#issuecomment-653018541,Improve const correctness by allowing Conf pointers to be const-qualified when constructing Handles. fatal_error member functions are now also const-qualified.,Thank you!!,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2929,2020-06-09T09:51:19Z,2020-07-02T18:22:09Z,2020-07-02T18:22:34Z,MERGED,True,4,1,1,https://github.com/pf-qiu,Don't rely on LastStableOffset for old brokers,2,[],https://github.com/edenhill/librdkafka/pull/2929,https://github.com/pf-qiu,1,https://github.com/edenhill/librdkafka/pull/2929,"Old kafka brokers(<2.1.0) always reports -1 as LastStableOffset.
This prevents librdkafka from emitting EOF messages. Use
HighwaterMarkOffset instead as before.","Old kafka brokers(<2.1.0) always reports -1 as LastStableOffset.
This prevents librdkafka from emitting EOF messages. Use
HighwaterMarkOffset instead as before.",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2929,2020-06-09T09:51:19Z,2020-07-02T18:22:09Z,2020-07-02T18:22:34Z,MERGED,True,4,1,1,https://github.com/pf-qiu,Don't rely on LastStableOffset for old brokers,2,[],https://github.com/edenhill/librdkafka/pull/2929,https://github.com/pf-qiu,2,https://github.com/edenhill/librdkafka/pull/2929#issuecomment-641175924,"Old kafka brokers(<2.1.0) always reports -1 as LastStableOffset.
This prevents librdkafka from emitting EOF messages. Use
HighwaterMarkOffset instead as before.",This should fix #2928,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2929,2020-06-09T09:51:19Z,2020-07-02T18:22:09Z,2020-07-02T18:22:34Z,MERGED,True,4,1,1,https://github.com/pf-qiu,Don't rely on LastStableOffset for old brokers,2,[],https://github.com/edenhill/librdkafka/pull/2929,https://github.com/edenhill,3,https://github.com/edenhill/librdkafka/pull/2929#issuecomment-653156191,"Old kafka brokers(<2.1.0) always reports -1 as LastStableOffset.
This prevents librdkafka from emitting EOF messages. Use
HighwaterMarkOffset instead as before.",Thank you @pf-qiu !,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2939,2020-06-17T12:10:17Z,2020-07-08T12:06:41Z,2020-07-08T12:06:45Z,MERGED,True,4061,322,24,https://github.com/edenhill,KIP-54: Sticky assignor (part 1),9,[],https://github.com/edenhill/librdkafka/pull/2939,https://github.com/edenhill,1,https://github.com/edenhill/librdkafka/pull/2939,"This is functionally complete and passes its unit tests, but needs another spin (in a follow-up PR) to:

update to latest AK changes
clean up FIXMEs
add integration tests","This is functionally complete and passes its unit tests, but needs another spin (in a follow-up PR) to:

update to latest AK changes
clean up FIXMEs
add integration tests",True,"{'HOORAY': ['https://github.com/agaurav', 'https://github.com/mhowlett']}"
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2949,2020-06-30T12:02:53Z,2020-07-02T05:23:09Z,2020-07-02T05:23:12Z,MERGED,True,292,279,5,https://github.com/edenhill,Fix regex crash on Windows,2,[],https://github.com/edenhill/librdkafka/pull/2949,https://github.com/edenhill,1,https://github.com/edenhill/librdkafka/pull/2949,"The regex code we use on Windows was using global variables and thus either bugged out or was crashed when multiple consumers with wildcard subscriptions were running in the same instance, as reported in numerous .NET issues.
This fix moves the global g variable (g for global!) to the Reprog object.","The regex code we use on Windows was using global variables and thus either bugged out or was crashed when multiple consumers with wildcard subscriptions were running in the same instance, as reported in numerous .NET issues.
This fix moves the global g variable (g for global!) to the Reprog object.",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2950,2020-06-30T12:03:57Z,2020-07-02T05:08:38Z,2020-07-02T05:08:42Z,MERGED,True,10,3,3,https://github.com/edenhill,Added --disable-option-checking (autoconf compat no-op),1,[],https://github.com/edenhill/librdkafka/pull/2950,https://github.com/edenhill,1,https://github.com/edenhill/librdkafka/pull/2950,"This is for compatibility with autoconf and is required to not break debian packaging builds.
Reported by @vincentbernat","This is for compatibility with autoconf and is required to not break debian packaging builds.
Reported by @vincentbernat",True,{'THUMBS_UP': ['https://github.com/vincentbernat']}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2953,2020-06-30T18:34:01Z,2020-07-02T12:49:25Z,2020-07-06T13:31:27Z,MERGED,True,359,197,23,https://github.com/edenhill,Added rd_kafka_message_broker_id() (#2952),5,[],https://github.com/edenhill/librdkafka/pull/2953,https://github.com/edenhill,1,https://github.com/edenhill/librdkafka/pull/2953,"This also refactors and cleans up OP_ERR and OP_CONSUMER_ERR calls.
The broker_id is also useful in our tests.","This also refactors and cleans up OP_ERR and OP_CONSUMER_ERR calls.
The broker_id is also useful in our tests.",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2953,2020-06-30T18:34:01Z,2020-07-02T12:49:25Z,2020-07-06T13:31:27Z,MERGED,True,359,197,23,https://github.com/edenhill,Added rd_kafka_message_broker_id() (#2952),5,[],https://github.com/edenhill/librdkafka/pull/2953,https://github.com/mhowlett,2,https://github.com/edenhill/librdkafka/pull/2953#issuecomment-652585182,"This also refactors and cleans up OP_ERR and OP_CONSUMER_ERR calls.
The broker_id is also useful in our tests.",seeking clarity on errors before hitting approve,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2954,2020-06-30T22:53:44Z,2020-10-21T10:57:53Z,2022-01-07T17:10:47Z,MERGED,True,306,21,12,https://github.com/abbycriswell,msg: sticky partitioning,1,[],https://github.com/edenhill/librdkafka/pull/2954,https://github.com/abbycriswell,1,https://github.com/edenhill/librdkafka/pull/2954,Improves batching of null-keyed messages.,Improves batching of null-keyed messages.,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2954,2020-06-30T22:53:44Z,2020-10-21T10:57:53Z,2022-01-07T17:10:47Z,MERGED,True,306,21,12,https://github.com/abbycriswell,msg: sticky partitioning,1,[],https://github.com/edenhill/librdkafka/pull/2954,https://github.com/abbycriswell,2,https://github.com/edenhill/librdkafka/pull/2954#issuecomment-685147141,Improves batching of null-keyed messages.,"Great stuff, Abby!
Left some comments.
Are you still in a position where you want to address these?

I am working on making these fixes, should be able to get them back to you soon.",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2954,2020-06-30T22:53:44Z,2020-10-21T10:57:53Z,2022-01-07T17:10:47Z,MERGED,True,306,21,12,https://github.com/abbycriswell,msg: sticky partitioning,1,[],https://github.com/edenhill/librdkafka/pull/2954,https://github.com/edenhill,3,https://github.com/edenhill/librdkafka/pull/2954#issuecomment-687116423,Improves batching of null-keyed messages.,"Abby, let me know when this is ready for re-review",True,{'THUMBS_UP': ['https://github.com/abbycriswell']}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2954,2020-06-30T22:53:44Z,2020-10-21T10:57:53Z,2022-01-07T17:10:47Z,MERGED,True,306,21,12,https://github.com/abbycriswell,msg: sticky partitioning,1,[],https://github.com/edenhill/librdkafka/pull/2954,https://github.com/edenhill,4,https://github.com/edenhill/librdkafka/pull/2954#issuecomment-689576763,Improves batching of null-keyed messages.,"Build failures:
https://travis-ci.org/github/edenhill/librdkafka/jobs/724386869#L716
https://travis-ci.org/github/edenhill/librdkafka/jobs/724386874#L477",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2954,2020-06-30T22:53:44Z,2020-10-21T10:57:53Z,2022-01-07T17:10:47Z,MERGED,True,306,21,12,https://github.com/abbycriswell,msg: sticky partitioning,1,[],https://github.com/edenhill/librdkafka/pull/2954,https://github.com/edenhill,5,https://github.com/edenhill/librdkafka/pull/2954#issuecomment-690238462,Improves batching of null-keyed messages.,"Test failure:
### Test ""0048_partitioner"" failed at 0048-partitioner.c:84:do_test_failed_partitioning() at Wed Sep  9 20:55:01 2020: ###
produce(): Expected UNKNOWN_PARTITION, got Success

https://travis-ci.org/github/edenhill/librdkafka/jobs/725716456#L9839",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2954,2020-06-30T22:53:44Z,2020-10-21T10:57:53Z,2022-01-07T17:10:47Z,MERGED,True,306,21,12,https://github.com/abbycriswell,msg: sticky partitioning,1,[],https://github.com/edenhill/librdkafka/pull/2954,https://github.com/abbycriswell,6,https://github.com/edenhill/librdkafka/pull/2954#issuecomment-698551575,Improves batching of null-keyed messages.,"Hey, finally figured out what the problem was. Wanted to give you a heads up.",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2954,2020-06-30T22:53:44Z,2020-10-21T10:57:53Z,2022-01-07T17:10:47Z,MERGED,True,306,21,12,https://github.com/abbycriswell,msg: sticky partitioning,1,[],https://github.com/edenhill/librdkafka/pull/2954,https://github.com/edenhill,7,https://github.com/edenhill/librdkafka/pull/2954#issuecomment-713487555,Improves batching of null-keyed messages.,"Thank you @abbycriswell for this very important feature that will not only improve latency and throughput of the producer, but also lessen the load on the Kafka cluster.
I'm very impressed by how the implementation turned out, very slick and minimalistic (in its greatest form)!
Good work, Abby!",True,{'HEART': ['https://github.com/abbycriswell']}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2954,2020-06-30T22:53:44Z,2020-10-21T10:57:53Z,2022-01-07T17:10:47Z,MERGED,True,306,21,12,https://github.com/abbycriswell,msg: sticky partitioning,1,[],https://github.com/edenhill/librdkafka/pull/2954,https://github.com/ijuma,8,https://github.com/edenhill/librdkafka/pull/2954#issuecomment-713567656,Improves batching of null-keyed messages.,Thanks @abbycriswell!,True,{'HEART': ['https://github.com/abbycriswell']}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2954,2020-06-30T22:53:44Z,2020-10-21T10:57:53Z,2022-01-07T17:10:47Z,MERGED,True,306,21,12,https://github.com/abbycriswell,msg: sticky partitioning,1,[],https://github.com/edenhill/librdkafka/pull/2954,https://github.com/abbycriswell,9,https://github.com/edenhill/librdkafka/pull/2954#issuecomment-716244930,Improves batching of null-keyed messages.,"I'm excited to see this feature completed and happy to know it will have a positive impact!
Thank you @edenhill, @ijuma, and @mhowlett for your help along the way!",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2957,2020-07-01T14:41:21Z,2020-07-02T12:53:50Z,2020-07-02T12:53:54Z,MERGED,True,230,43,13,https://github.com/edenhill,Fix consumer stall when partitions are reassigned (#2955),3,[],https://github.com/edenhill/librdkafka/pull/2957,https://github.com/edenhill,1,https://github.com/edenhill/librdkafka/pull/2957,.. and a locking fix for transactions.,.. and a locking fix for transactions.,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2958,2020-07-01T14:47:13Z,2020-07-02T05:22:10Z,2020-07-02T05:22:13Z,MERGED,True,6,9,2,https://github.com/edenhill,Consumer assignors could ignore topics if topic_cnt > member_cnt,1,[],https://github.com/edenhill/librdkafka/pull/2958,https://github.com/edenhill,1,https://github.com/edenhill/librdkafka/pull/2958,This is a backport from the kip-54 PR which I believe you've already reviewed.,This is a backport from the kip-54 PR which I believe you've already reviewed.,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2967,2020-07-06T13:29:46Z,2020-07-06T16:26:02Z,2020-07-06T16:26:05Z,MERGED,True,25,12,3,https://github.com/edenhill,Bandwidth friendlier consumer queueing defaults (#2853),1,[],https://github.com/edenhill/librdkafka/pull/2967,https://github.com/edenhill,1,https://github.com/edenhill/librdkafka/pull/2967,,,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2968,2020-07-06T14:52:39Z,2020-07-06T18:52:22Z,2020-07-06T18:52:25Z,MERGED,True,29,16,5,https://github.com/edenhill,Refresh metadata (at most every 10s) for desired but unavailable partitions (#2917),1,[],https://github.com/edenhill/librdkafka/pull/2968,https://github.com/edenhill,1,https://github.com/edenhill/librdkafka/pull/2968,,,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2970,2020-07-07T11:58:59Z,2020-07-07T18:58:47Z,2020-07-07T18:58:51Z,MERGED,True,3,3,2,https://github.com/edenhill,Test fixes,2,[],https://github.com/edenhill/librdkafka/pull/2970,https://github.com/edenhill,1,https://github.com/edenhill/librdkafka/pull/2970,,,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2974,2020-07-08T08:45:32Z,2020-07-08T16:03:12Z,2020-07-08T16:03:15Z,MERGED,True,31,10,5,https://github.com/edenhill,Change `linger.ms` default from 0.5 to 5ms to improve efficiency,2,[],https://github.com/edenhill/librdkafka/pull/2974,https://github.com/edenhill,1,https://github.com/edenhill/librdkafka/pull/2974,"Increased batching leads to less protocol overhead, less CPU usage
on client and brokers, increased throughput and in some cases also
improved latency.","Increased batching leads to less protocol overhead, less CPU usage
on client and brokers, increased throughput and in some cases also
improved latency.",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2976,2020-07-08T12:15:07Z,,2020-07-08T12:24:52Z,OPEN,False,8,6,1,https://github.com/csky2013,fix: produce thread block and never wakeup,1,[],https://github.com/edenhill/librdkafka/pull/2976,https://github.com/csky2013,1,https://github.com/edenhill/librdkafka/pull/2976,"When flag RdKafka::Producer::RK_MSG_BLOCK is set to produce, proudcer may blocked when queue full, and never waked up.
Here is two contition in code:


conditionA:
(rk->rk_curr_msgs.cnt + cnt >rk->rk_curr_msgs.max_cnt ||
(unsigned long long)(rk->rk_curr_msgs.size + size) >(unsigned long long)rk->rk_curr_msgs.max_size)


conditionB:
(rk->rk_curr_msgs.cnt - cnt == 0) ||
(rk->rk_curr_msgs.cnt >= rk->rk_curr_msgs.max_cnt &&
rk->rk_curr_msgs.cnt - cnt < rk->rk_curr_msgs.max_cnt) ||
(rk->rk_curr_msgs.size >= rk->rk_curr_msgs.max_size &&
rk->rk_curr_msgs.size - size < rk->rk_curr_msgs.max_size


Produce thread would block when conditionA==true, and wakeup when conditionB==true.
But we found, when the produce thread is blocked, the conditionB will not always satisfied.This is because, when the produce thread blocked, rk->rk_curr_msgs.cnt may less than rk->rk_curr_msgs.max_cnt,  when rk->rk_curr_msgs.size may less than rk->rk_curr_msgs.max_size at the same time, then the conditionB will never satisfied, so thread cannot waked up.
This fix solved this problem by change wakeup condition. When produce thread is blocked, and the space is enough, we should always awake them.","When flag RdKafka::Producer::RK_MSG_BLOCK is set to produce, proudcer may blocked when queue full, and never waked up.
Here is two contition in code:


conditionA:
(rk->rk_curr_msgs.cnt + cnt >rk->rk_curr_msgs.max_cnt ||
(unsigned long long)(rk->rk_curr_msgs.size + size) >(unsigned long long)rk->rk_curr_msgs.max_size)


conditionB:
(rk->rk_curr_msgs.cnt - cnt == 0) ||
(rk->rk_curr_msgs.cnt >= rk->rk_curr_msgs.max_cnt &&
rk->rk_curr_msgs.cnt - cnt < rk->rk_curr_msgs.max_cnt) ||
(rk->rk_curr_msgs.size >= rk->rk_curr_msgs.max_size &&
rk->rk_curr_msgs.size - size < rk->rk_curr_msgs.max_size


Produce thread would block when conditionA==true, and wakeup when conditionB==true.
But we found, when the produce thread is blocked, the conditionB will not always satisfied.This is because, when the produce thread blocked, rk->rk_curr_msgs.cnt may less than rk->rk_curr_msgs.max_cnt,  when rk->rk_curr_msgs.size may less than rk->rk_curr_msgs.max_size at the same time, then the conditionB will never satisfied, so thread cannot waked up.
This fix solved this problem by change wakeup condition. When produce thread is blocked, and the space is enough, we should always awake them.",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2980,2020-07-08T23:32:11Z,2020-07-22T06:17:52Z,2020-07-22T06:17:52Z,MERGED,True,1093,190,12,https://github.com/mhowlett,Incremental assign and unassign,3,[],https://github.com/edenhill/librdkafka/pull/2980,https://github.com/mhowlett,1,https://github.com/edenhill/librdkafka/pull/2980,"Ok, let's see what you think of this.
Previously opened and commented on here: mhowlett#1 . Unfortunately, the rebasing is a bit screwed up over there and i didn't bother to fix it properly. The contextual comments may still be useful to refer back to though.","Ok, let's see what you think of this.
Previously opened and commented on here: mhowlett#1 . Unfortunately, the rebasing is a bit screwed up over there and i didn't bother to fix it properly. The contextual comments may still be useful to refer back to though.",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2980,2020-07-08T23:32:11Z,2020-07-22T06:17:52Z,2020-07-22T06:17:52Z,MERGED,True,1093,190,12,https://github.com/mhowlett,Incremental assign and unassign,3,[],https://github.com/edenhill/librdkafka/pull/2980,https://github.com/edenhill,2,https://github.com/edenhill/librdkafka/pull/2980#issuecomment-656073403,"Ok, let's see what you think of this.
Previously opened and commented on here: mhowlett#1 . Unfortunately, the rebasing is a bit screwed up over there and i didn't bother to fix it properly. The contextual comments may still be useful to refer back to though.","0113 does not build on various platforms, use RdKafka::ERR__.. instead of RdKafka::ErrorCode::ERR__...
See CI failures.",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2982,2020-07-09T12:18:09Z,2020-07-17T07:53:23Z,2020-07-17T07:53:26Z,MERGED,True,21,5,4,https://github.com/edenhill,Add printf-like format checking to error_new(),1,[],https://github.com/edenhill/librdkafka/pull/2982,https://github.com/edenhill,1,https://github.com/edenhill/librdkafka/pull/2982,,,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2990,2020-07-15T11:03:45Z,2020-07-20T07:03:08Z,2020-07-20T07:05:06Z,MERGED,True,3,1,1,https://github.com/pf-qiu,Destroy control messages in batch mode,1,['bug'],https://github.com/edenhill/librdkafka/pull/2990,https://github.com/pf-qiu,1,https://github.com/edenhill/librdkafka/pull/2990,"If there are control messages in the topic, rd_kafka_destroy will
hang if only rd_kafka_consume_batch is used, rd_kafka_consume and
rd_kafka_consume_callback don't have this issue.
Release these messages before returning to the application.","If there are control messages in the topic, rd_kafka_destroy will
hang if only rd_kafka_consume_batch is used, rd_kafka_consume and
rd_kafka_consume_callback don't have this issue.
Release these messages before returning to the application.",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2990,2020-07-15T11:03:45Z,2020-07-20T07:03:08Z,2020-07-20T07:05:06Z,MERGED,True,3,1,1,https://github.com/pf-qiu,Destroy control messages in batch mode,1,['bug'],https://github.com/edenhill/librdkafka/pull/2990,https://github.com/edenhill,2,https://github.com/edenhill/librdkafka/pull/2990#issuecomment-660845065,"If there are control messages in the topic, rd_kafka_destroy will
hang if only rd_kafka_consume_batch is used, rd_kafka_consume and
rd_kafka_consume_callback don't have this issue.
Release these messages before returning to the application.",Thanks for this fix!,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2998,2020-07-17T19:43:16Z,,2020-07-17T21:19:25Z,OPEN,False,25,20,4,https://github.com/voutilad,Fixes and updates for building with VS 2019 on Win10,3,[],https://github.com/edenhill/librdkafka/pull/2998,https://github.com/voutilad,1,https://github.com/edenhill/librdkafka/pull/2998,"I was trying to build with Visual Studio 2019 on Win10 and had a lot of challenges as others have mentioned (see Issue #2816)
This PR makes the following changes:

Fixes the build.bat script to explicitly set the PlatformToolset
Call nuget restore in build.bat to make sure dependencies are fetched prior to building
Bump OpenSSL version to the latest LTS on win32 (1.1.1g) by updating the vcxproj to use the newer library names (and setting appropriate version based on win32 vs x64 target)
Updates the powershell script for installing openssl (install-openssl.ps1) to use the newer version as well as resolve issues running it in newer PowerShell environments by using Start-Process instead of cmd
Updates the win32 readme to mention latest OpenSSL version is 1.1.1 and clarify it expects you to install

I'm no Visual Studio expert and still cannot get the project to build using the VS2019 gui application...only via the msbuild tool on the command line. (I don't have access to VS2015 tooling to make and test the proper changes.)
I realize this PR combines both a dependency bump as well as some tooling changes, so if you'd like me to split it up into two different PRs, let me know.","I was trying to build with Visual Studio 2019 on Win10 and had a lot of challenges as others have mentioned (see Issue #2816)
This PR makes the following changes:

Fixes the build.bat script to explicitly set the PlatformToolset
Call nuget restore in build.bat to make sure dependencies are fetched prior to building
Bump OpenSSL version to the latest LTS on win32 (1.1.1g) by updating the vcxproj to use the newer library names (and setting appropriate version based on win32 vs x64 target)
Updates the powershell script for installing openssl (install-openssl.ps1) to use the newer version as well as resolve issues running it in newer PowerShell environments by using Start-Process instead of cmd
Updates the win32 readme to mention latest OpenSSL version is 1.1.1 and clarify it expects you to install

I'm no Visual Studio expert and still cannot get the project to build using the VS2019 gui application...only via the msbuild tool on the command line. (I don't have access to VS2015 tooling to make and test the proper changes.)
I realize this PR combines both a dependency bump as well as some tooling changes, so if you'd like me to split it up into two different PRs, let me know.",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2998,2020-07-17T19:43:16Z,,2020-07-17T21:19:25Z,OPEN,False,25,20,4,https://github.com/voutilad,Fixes and updates for building with VS 2019 on Win10,3,[],https://github.com/edenhill/librdkafka/pull/2998,https://github.com/voutilad,2,https://github.com/edenhill/librdkafka/pull/2998#issuecomment-660311259,"I was trying to build with Visual Studio 2019 on Win10 and had a lot of challenges as others have mentioned (see Issue #2816)
This PR makes the following changes:

Fixes the build.bat script to explicitly set the PlatformToolset
Call nuget restore in build.bat to make sure dependencies are fetched prior to building
Bump OpenSSL version to the latest LTS on win32 (1.1.1g) by updating the vcxproj to use the newer library names (and setting appropriate version based on win32 vs x64 target)
Updates the powershell script for installing openssl (install-openssl.ps1) to use the newer version as well as resolve issues running it in newer PowerShell environments by using Start-Process instead of cmd
Updates the win32 readme to mention latest OpenSSL version is 1.1.1 and clarify it expects you to install

I'm no Visual Studio expert and still cannot get the project to build using the VS2019 gui application...only via the msbuild tool on the command line. (I don't have access to VS2015 tooling to make and test the proper changes.)
I realize this PR combines both a dependency bump as well as some tooling changes, so if you'd like me to split it up into two different PRs, let me know.",AppVeyor failure looks like it needs the OpenSSL directory cache removed and repopulated.,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,2998,2020-07-17T19:43:16Z,,2020-07-17T21:19:25Z,OPEN,False,25,20,4,https://github.com/voutilad,Fixes and updates for building with VS 2019 on Win10,3,[],https://github.com/edenhill/librdkafka/pull/2998,https://github.com/voutilad,3,https://github.com/edenhill/librdkafka/pull/2998#issuecomment-660342573,"I was trying to build with Visual Studio 2019 on Win10 and had a lot of challenges as others have mentioned (see Issue #2816)
This PR makes the following changes:

Fixes the build.bat script to explicitly set the PlatformToolset
Call nuget restore in build.bat to make sure dependencies are fetched prior to building
Bump OpenSSL version to the latest LTS on win32 (1.1.1g) by updating the vcxproj to use the newer library names (and setting appropriate version based on win32 vs x64 target)
Updates the powershell script for installing openssl (install-openssl.ps1) to use the newer version as well as resolve issues running it in newer PowerShell environments by using Start-Process instead of cmd
Updates the win32 readme to mention latest OpenSSL version is 1.1.1 and clarify it expects you to install

I'm no Visual Studio expert and still cannot get the project to build using the VS2019 gui application...only via the msbuild tool on the command line. (I don't have access to VS2015 tooling to make and test the proper changes.)
I realize this PR combines both a dependency bump as well as some tooling changes, so if you'd like me to split it up into two different PRs, let me know.",TravisCI failures seem spurious. I had the mingw32 build pass in my version of Travis: https://travis-ci.org/github/voutilad/librdkafka/jobs/709324583,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,3006,2020-07-22T17:13:30Z,2020-07-24T04:45:05Z,2020-07-24T04:45:05Z,CLOSED,False,1807,290,16,https://github.com/mhowlett,Cooperative protocol implementation,3,[],https://github.com/edenhill/librdkafka/pull/3006,https://github.com/mhowlett,1,https://github.com/edenhill/librdkafka/pull/3006,"work in progress, but worth a bit of a pre-review i think. can ignore tests.","work in progress, but worth a bit of a pre-review i think. can ignore tests.",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,3006,2020-07-22T17:13:30Z,2020-07-24T04:45:05Z,2020-07-24T04:45:05Z,CLOSED,False,1807,290,16,https://github.com/mhowlett,Cooperative protocol implementation,3,[],https://github.com/edenhill/librdkafka/pull/3006,https://github.com/mhowlett,2,https://github.com/edenhill/librdkafka/pull/3006#issuecomment-663344353,"work in progress, but worth a bit of a pre-review i think. can ignore tests.","going to open a new version of this, hopefully tomorrow.",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,3012,2020-07-28T11:11:29Z,2020-07-28T11:12:01Z,2020-07-28T11:12:01Z,CLOSED,False,28,25,1,https://github.com/alexey-milovidov,Fix MSan error,1,[],https://github.com/edenhill/librdkafka/pull/3012,https://github.com/alexey-milovidov,1,https://github.com/edenhill/librdkafka/pull/3012,ClickHouse/ClickHouse#12990,ClickHouse/ClickHouse#12990,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,3012,2020-07-28T11:11:29Z,2020-07-28T11:12:01Z,2020-07-28T11:12:01Z,CLOSED,False,28,25,1,https://github.com/alexey-milovidov,Fix MSan error,1,[],https://github.com/edenhill/librdkafka/pull/3012,https://github.com/alexey-milovidov,2,https://github.com/edenhill/librdkafka/pull/3012#issuecomment-664979519,ClickHouse/ClickHouse#12990,"Sorry, wrong repo.",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,3029,2020-08-13T18:37:22Z,2020-08-18T12:32:28Z,2020-08-20T08:07:10Z,CLOSED,False,3,0,1,https://github.com/andrewthad,Document that rd_kafka_subscribe() copies its topic list argument,1,[],https://github.com/edenhill/librdkafka/pull/3029,https://github.com/andrewthad,1,https://github.com/edenhill/librdkafka/pull/3029,,,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,3030,2020-08-14T13:26:46Z,2020-08-17T08:28:36Z,2020-08-17T08:28:40Z,MERGED,True,1,1,2,https://github.com/hanickadot,Use consistent name for FindZSTD cmake file,1,[],https://github.com/edenhill/librdkafka/pull/3030,https://github.com/hanickadot,1,https://github.com/edenhill/librdkafka/pull/3030,"In version 1.5.0 cmake build system gives me this warning, as the find_package(Zstd) is not consistent with FindZSTD.cmake filename of the helper cmake script.
CMake Warning (dev) at /usr/local/Cellar/cmake/3.17.3/share/cmake/Modules/FindPackageHandleStandardArgs.cmake:272 (message):
  The package name passed to `find_package_handle_standard_args` (ZSTD) does
  not match the name of the calling package (Zstd).  This can lead to
  problems in calling code that expects `find_package` result variables
  (e.g., `_FOUND`) to follow a certain pattern.
Call Stack (most recent call first):
  librdkafka/packaging/cmake/Modules/FindZstd.cmake:18 (FIND_PACKAGE_HANDLE_STANDARD_ARGS)
  librdkafka/CMakeLists.txt:57 (find_package)
This warning is for project developers.  Use -Wno-dev to suppress it.","In version 1.5.0 cmake build system gives me this warning, as the find_package(Zstd) is not consistent with FindZSTD.cmake filename of the helper cmake script.
CMake Warning (dev) at /usr/local/Cellar/cmake/3.17.3/share/cmake/Modules/FindPackageHandleStandardArgs.cmake:272 (message):
  The package name passed to `find_package_handle_standard_args` (ZSTD) does
  not match the name of the calling package (Zstd).  This can lead to
  problems in calling code that expects `find_package` result variables
  (e.g., `_FOUND`) to follow a certain pattern.
Call Stack (most recent call first):
  librdkafka/packaging/cmake/Modules/FindZstd.cmake:18 (FIND_PACKAGE_HANDLE_STANDARD_ARGS)
  librdkafka/CMakeLists.txt:57 (find_package)
This warning is for project developers.  Use -Wno-dev to suppress it.",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,3030,2020-08-14T13:26:46Z,2020-08-17T08:28:36Z,2020-08-17T08:28:40Z,MERGED,True,1,1,2,https://github.com/hanickadot,Use consistent name for FindZSTD cmake file,1,[],https://github.com/edenhill/librdkafka/pull/3030,https://github.com/edenhill,2,https://github.com/edenhill/librdkafka/pull/3030#issuecomment-674739861,"In version 1.5.0 cmake build system gives me this warning, as the find_package(Zstd) is not consistent with FindZSTD.cmake filename of the helper cmake script.
CMake Warning (dev) at /usr/local/Cellar/cmake/3.17.3/share/cmake/Modules/FindPackageHandleStandardArgs.cmake:272 (message):
  The package name passed to `find_package_handle_standard_args` (ZSTD) does
  not match the name of the calling package (Zstd).  This can lead to
  problems in calling code that expects `find_package` result variables
  (e.g., `_FOUND`) to follow a certain pattern.
Call Stack (most recent call first):
  librdkafka/packaging/cmake/Modules/FindZstd.cmake:18 (FIND_PACKAGE_HANDLE_STANDARD_ARGS)
  librdkafka/CMakeLists.txt:57 (find_package)
This warning is for project developers.  Use -Wno-dev to suppress it.",Thank you!,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,3031,2020-08-14T14:55:19Z,2020-09-30T08:52:26Z,2020-09-30T08:52:31Z,MERGED,True,35,31,1,https://github.com/wolfchimneyrock,fix producer header option,4,[],https://github.com/edenhill/librdkafka/pull/3031,https://github.com/wolfchimneyrock,1,https://github.com/edenhill/librdkafka/pull/3031,"Currently, the ability to test producing messages with a header in rdkafka_performance is broken because of a subtle getopt error.  Although the intent is to give the option -H an optional argument, what is happening is optarg is always discarding an option even if present since the format string looks like ...HH: - it never parses the latter part.
There are a couple of considerations:

GNU optarg supports optional arguments with a double colon in the format string (would use H::) but not all platforms use GNU.
the POSIX standard actually forbids GNU-style optional arguments.

Thus IMHO the best approach is to just make them different options: -H <argument> for the producer, and -h for the consumer.","Currently, the ability to test producing messages with a header in rdkafka_performance is broken because of a subtle getopt error.  Although the intent is to give the option -H an optional argument, what is happening is optarg is always discarding an option even if present since the format string looks like ...HH: - it never parses the latter part.
There are a couple of considerations:

GNU optarg supports optional arguments with a double colon in the format string (would use H::) but not all platforms use GNU.
the POSIX standard actually forbids GNU-style optional arguments.

Thus IMHO the best approach is to just make them different options: -H <argument> for the producer, and -h for the consumer.",True,{'THUMBS_UP': ['https://github.com/gridaphobe']}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,3031,2020-08-14T14:55:19Z,2020-09-30T08:52:26Z,2020-09-30T08:52:31Z,MERGED,True,35,31,1,https://github.com/wolfchimneyrock,fix producer header option,4,[],https://github.com/edenhill/librdkafka/pull/3031,https://github.com/edenhill,2,https://github.com/edenhill/librdkafka/pull/3031#issuecomment-701256837,"Currently, the ability to test producing messages with a header in rdkafka_performance is broken because of a subtle getopt error.  Although the intent is to give the option -H an optional argument, what is happening is optarg is always discarding an option even if present since the format string looks like ...HH: - it never parses the latter part.
There are a couple of considerations:

GNU optarg supports optional arguments with a double colon in the format string (would use H::) but not all platforms use GNU.
the POSIX standard actually forbids GNU-style optional arguments.

Thus IMHO the best approach is to just make them different options: -H <argument> for the producer, and -h for the consumer.",Thank you!,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,3032,2020-08-16T23:50:21Z,2020-10-16T20:54:37Z,2020-10-16T20:54:37Z,CLOSED,False,6904,1593,49,https://github.com/mhowlett,Cooperative protocol implementation,30,[],https://github.com/edenhill/librdkafka/pull/3032,https://github.com/mhowlett,1,https://github.com/edenhill/librdkafka/pull/3032,"WIP but a pre-review would be mighty helpful. And any enlightenment on why the uncommented 0113 sub test has a ref-count problem on termination mighty mighty helpful (I don't properly get the pause/resume logic on rebalance, it's related to that). Look forward to syncing on all of this.","WIP but a pre-review would be mighty helpful. And any enlightenment on why the uncommented 0113 sub test has a ref-count problem on termination mighty mighty helpful (I don't properly get the pause/resume logic on rebalance, it's related to that). Look forward to syncing on all of this.",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,3032,2020-08-16T23:50:21Z,2020-10-16T20:54:37Z,2020-10-16T20:54:37Z,CLOSED,False,6904,1593,49,https://github.com/mhowlett,Cooperative protocol implementation,30,[],https://github.com/edenhill/librdkafka/pull/3032,https://github.com/eran-levy,2,https://github.com/edenhill/librdkafka/pull/3032#issuecomment-683377922,"WIP but a pre-review would be mighty helpful. And any enlightenment on why the uncommented 0113 sub test has a ref-count problem on termination mighty mighty helpful (I don't properly get the pause/resume logic on rebalance, it's related to that). Look forward to syncing on all of this.","@edenhill @mhowlett thanks alot, looking forward for the incremental cooperative rebalancing protocol implementation, when is it scheduled to be released?",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,3032,2020-08-16T23:50:21Z,2020-10-16T20:54:37Z,2020-10-16T20:54:37Z,CLOSED,False,6904,1593,49,https://github.com/mhowlett,Cooperative protocol implementation,30,[],https://github.com/edenhill/librdkafka/pull/3032,https://github.com/mhowlett,3,https://github.com/edenhill/librdkafka/pull/3032#issuecomment-683499952,"WIP but a pre-review would be mighty helpful. And any enlightenment on why the uncommented 0113 sub test has a ref-count problem on termination mighty mighty helpful (I don't properly get the pause/resume logic on rebalance, it's related to that). Look forward to syncing on all of this.","@eran-levy - when it's done. we're working on it, it's not stalled.",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,3032,2020-08-16T23:50:21Z,2020-10-16T20:54:37Z,2020-10-16T20:54:37Z,CLOSED,False,6904,1593,49,https://github.com/mhowlett,Cooperative protocol implementation,30,[],https://github.com/edenhill/librdkafka/pull/3032,https://github.com/edenhill,4,https://github.com/edenhill/librdkafka/pull/3032#issuecomment-683909521,"WIP but a pre-review would be mighty helpful. And any enlightenment on why the uncommented 0113 sub test has a ref-count problem on termination mighty mighty helpful (I don't properly get the pause/resume logic on rebalance, it's related to that). Look forward to syncing on all of this.",@eran-levy We'll hopefully have it ready later this year.,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,3032,2020-08-16T23:50:21Z,2020-10-16T20:54:37Z,2020-10-16T20:54:37Z,CLOSED,False,6904,1593,49,https://github.com/mhowlett,Cooperative protocol implementation,30,[],https://github.com/edenhill/librdkafka/pull/3032,https://github.com/edenhill,5,https://github.com/edenhill/librdkafka/pull/3032#issuecomment-710617298,"WIP but a pre-review would be mighty helpful. And any enlightenment on why the uncommented 0113 sub test has a ref-count problem on termination mighty mighty helpful (I don't properly get the pause/resume logic on rebalance, it's related to that). Look forward to syncing on all of this.",Closed. Continued in #3111,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,3035,2020-08-18T14:55:27Z,2020-08-20T08:05:55Z,2020-08-20T08:05:59Z,MERGED,True,0,9,1,https://github.com/Quuxplusone,Remove offset.store.method=broker from the examples/ directory.,1,[],https://github.com/edenhill/librdkafka/pull/3035,https://github.com/Quuxplusone,1,https://github.com/edenhill/librdkafka/pull/3035,"This config option is deprecated and should no longer be used in examples.
The non-deprecated behavior is implicitly ""offset.store.method=broker"".
Trying to set ""offset.store.method=broker"" explicitly gives a warning, and
trying to set ""offset.store.method"" to anything else is deprecated AND gives
a warning.
However, ""offset.store.method=file"" is still supported in src/,
and therefore tested. I didn't touch tests/.
Addresses #3016.","This config option is deprecated and should no longer be used in examples.
The non-deprecated behavior is implicitly ""offset.store.method=broker"".
Trying to set ""offset.store.method=broker"" explicitly gives a warning, and
trying to set ""offset.store.method"" to anything else is deprecated AND gives
a warning.
However, ""offset.store.method=file"" is still supported in src/,
and therefore tested. I didn't touch tests/.
Addresses #3016.",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,3035,2020-08-18T14:55:27Z,2020-08-20T08:05:55Z,2020-08-20T08:05:59Z,MERGED,True,0,9,1,https://github.com/Quuxplusone,Remove offset.store.method=broker from the examples/ directory.,1,[],https://github.com/edenhill/librdkafka/pull/3035,https://github.com/edenhill,2,https://github.com/edenhill/librdkafka/pull/3035#issuecomment-677444193,"This config option is deprecated and should no longer be used in examples.
The non-deprecated behavior is implicitly ""offset.store.method=broker"".
Trying to set ""offset.store.method=broker"" explicitly gives a warning, and
trying to set ""offset.store.method"" to anything else is deprecated AND gives
a warning.
However, ""offset.store.method=file"" is still supported in src/,
and therefore tested. I didn't touch tests/.
Addresses #3016.",Thank you!,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,3036,2020-08-18T17:13:44Z,2020-10-21T11:07:47Z,2020-10-21T11:07:47Z,CLOSED,False,1,1,1,https://github.com/dogac00,Fix Typo.,1,[],https://github.com/edenhill/librdkafka/pull/3036,https://github.com/dogac00,1,https://github.com/edenhill/librdkafka/pull/3036,,,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,3039,2020-08-19T22:46:43Z,2020-08-20T07:47:44Z,2020-08-20T07:47:44Z,MERGED,True,3,3,1,https://github.com/eb-emilio,Fix typos in STATISTICS.md,1,[],https://github.com/edenhill/librdkafka/pull/3039,https://github.com/eb-emilio,1,https://github.com/edenhill/librdkafka/pull/3039,,,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,3039,2020-08-19T22:46:43Z,2020-08-20T07:47:44Z,2020-08-20T07:47:44Z,MERGED,True,3,3,1,https://github.com/eb-emilio,Fix typos in STATISTICS.md,1,[],https://github.com/edenhill/librdkafka/pull/3039,https://github.com/edenhill,2,https://github.com/edenhill/librdkafka/pull/3039#issuecomment-677434673,,Thank you!,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,3042,2020-08-21T09:54:03Z,,2021-03-10T14:52:42Z,OPEN,False,273,139,11,https://github.com/masariello,Add dual shared and static cmake targets and enable cpack package generation,11,[],https://github.com/edenhill/librdkafka/pull/3042,https://github.com/masariello,1,https://github.com/edenhill/librdkafka/pull/3042,,,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,3042,2020-08-21T09:54:03Z,,2021-03-10T14:52:42Z,OPEN,False,273,139,11,https://github.com/masariello,Add dual shared and static cmake targets and enable cpack package generation,11,[],https://github.com/edenhill/librdkafka/pull/3042,https://github.com/edenhill,2,https://github.com/edenhill/librdkafka/pull/3042#issuecomment-680721292,,"@raulbocanegra @Oxymoron79 @benesch
Can you please review these CMake changes?",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,3042,2020-08-21T09:54:03Z,,2021-03-10T14:52:42Z,OPEN,False,273,139,11,https://github.com/masariello,Add dual shared and static cmake targets and enable cpack package generation,11,[],https://github.com/edenhill/librdkafka/pull/3042,https://github.com/masariello,3,https://github.com/edenhill/librdkafka/pull/3042#issuecomment-692306098,,"@edenhill @raulbocanegra @Oxymoron79 @benesch
I think I covered all the corners about a month ago.
Can this be merged now?
Thanks",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,3042,2020-08-21T09:54:03Z,,2021-03-10T14:52:42Z,OPEN,False,273,139,11,https://github.com/masariello,Add dual shared and static cmake targets and enable cpack package generation,11,[],https://github.com/edenhill/librdkafka/pull/3042,https://github.com/edenhill,4,https://github.com/edenhill/librdkafka/pull/3042#issuecomment-726014221,,"There's similar work being done in #3130, would be good if these two efforts could be combined.",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,3042,2020-08-21T09:54:03Z,,2021-03-10T14:52:42Z,OPEN,False,273,139,11,https://github.com/masariello,Add dual shared and static cmake targets and enable cpack package generation,11,[],https://github.com/edenhill/librdkafka/pull/3042,https://github.com/edenhill,5,https://github.com/edenhill/librdkafka/pull/3042#issuecomment-795543216,,Please resolve the conflicts,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,3045,2020-08-25T08:54:24Z,2020-08-25T16:02:58Z,2020-08-25T16:03:01Z,MERGED,True,17,4,2,https://github.com/edenhill,Fix topic destroy of light-weight topic objects,2,[],https://github.com/edenhill/librdkafka/pull/3045,https://github.com/edenhill,1,https://github.com/edenhill/librdkafka/pull/3045,,,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,3046,2020-08-25T13:24:58Z,2020-08-26T06:28:17Z,2020-08-26T06:28:22Z,MERGED,True,7,4,1,https://github.com/filimonov,Fix for use-of-uninitialized-value in rdaddr.c,1,[],https://github.com/edenhill/librdkafka/pull/3046,https://github.com/filimonov,1,https://github.com/edenhill/librdkafka/pull/3046,"Fix error reported by Memory sanitizer, see ClickHouse/ClickHouse#12990
Backporting fix by @alexey-milovidov from ClickHouse#1","Fix error reported by Memory sanitizer, see ClickHouse/ClickHouse#12990
Backporting fix by @alexey-milovidov from ClickHouse#1",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,3046,2020-08-25T13:24:58Z,2020-08-26T06:28:17Z,2020-08-26T06:28:22Z,MERGED,True,7,4,1,https://github.com/filimonov,Fix for use-of-uninitialized-value in rdaddr.c,1,[],https://github.com/edenhill/librdkafka/pull/3046,https://github.com/filimonov,2,https://github.com/edenhill/librdkafka/pull/3046#issuecomment-680045835,"Fix error reported by Memory sanitizer, see ClickHouse/ClickHouse#12990
Backporting fix by @alexey-milovidov from ClickHouse#1",Travis failure is unrelated (GPG signature check during rpm build).,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,3046,2020-08-25T13:24:58Z,2020-08-26T06:28:17Z,2020-08-26T06:28:22Z,MERGED,True,7,4,1,https://github.com/filimonov,Fix for use-of-uninitialized-value in rdaddr.c,1,[],https://github.com/edenhill/librdkafka/pull/3046,https://github.com/edenhill,3,https://github.com/edenhill/librdkafka/pull/3046#issuecomment-680685923,"Fix error reported by Memory sanitizer, see ClickHouse/ClickHouse#12990
Backporting fix by @alexey-milovidov from ClickHouse#1",Thank you!,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,3048,2020-08-26T23:14:42Z,2020-09-01T06:55:09Z,2020-09-01T06:55:15Z,MERGED,True,3,3,1,https://github.com/yagnasrinath,Not using timespec_get on android api level < 29,2,[],https://github.com/edenhill/librdkafka/pull/3048,https://github.com/yagnasrinath,1,https://github.com/edenhill/librdkafka/pull/3048,,,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,3048,2020-08-26T23:14:42Z,2020-09-01T06:55:09Z,2020-09-01T06:55:15Z,MERGED,True,3,3,1,https://github.com/yagnasrinath,Not using timespec_get on android api level < 29,2,[],https://github.com/edenhill/librdkafka/pull/3048,https://github.com/edenhill,2,https://github.com/edenhill/librdkafka/pull/3048#issuecomment-684487459,,Thank you!,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,3049,2020-08-27T08:33:18Z,2020-09-04T06:29:56Z,2020-09-04T08:42:33Z,MERGED,True,28,20,6,https://github.com/edenhill,Fix roundrobin crash (v.1.5.0 regression),4,[],https://github.com/edenhill/librdkafka/pull/3049,https://github.com/edenhill,1,https://github.com/edenhill/librdkafka/pull/3049,Issue #3024,Issue #3024,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,3050,2020-08-28T04:28:03Z,2020-08-28T06:28:58Z,2020-08-28T06:29:04Z,MERGED,True,1,1,1,https://github.com/arhoads,Fix misspelled join state field in STATISTICS.md,1,[],https://github.com/edenhill/librdkafka/pull/3050,https://github.com/arhoads,1,https://github.com/edenhill/librdkafka/pull/3050,join_state is the spelling used in the outputted JSON,join_state is the spelling used in the outputted JSON,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,3050,2020-08-28T04:28:03Z,2020-08-28T06:28:58Z,2020-08-28T06:29:04Z,MERGED,True,1,1,1,https://github.com/arhoads,Fix misspelled join state field in STATISTICS.md,1,[],https://github.com/edenhill/librdkafka/pull/3050,https://github.com/arhoads,2,https://github.com/edenhill/librdkafka/pull/3050#issuecomment-682318038,join_state is the spelling used in the outputted JSON,"As seen here: 
  
    
      librdkafka/src/rdkafka.c
    
    
         Line 1704
      in
      329c97f
    
  
  
    

        
          
           ""\""join_state\"": \""%s\"", """,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,3050,2020-08-28T04:28:03Z,2020-08-28T06:28:58Z,2020-08-28T06:29:04Z,MERGED,True,1,1,1,https://github.com/arhoads,Fix misspelled join state field in STATISTICS.md,1,[],https://github.com/edenhill/librdkafka/pull/3050,https://github.com/edenhill,3,https://github.com/edenhill/librdkafka/pull/3050#issuecomment-682352527,join_state is the spelling used in the outputted JSON,Thank you!,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,3053,2020-08-28T19:47:31Z,2020-09-04T08:28:27Z,2020-09-04T08:28:32Z,MERGED,True,49,13,3,https://github.com/gridaphobe,add recent additions to kafka broker error codes,3,[],https://github.com/edenhill/librdkafka/pull/3053,https://github.com/gridaphobe,1,https://github.com/edenhill/librdkafka/pull/3053,"See https://github.com/apache/kafka/blob/b937ec75677f8af13bf6fda686f07e9c62cdd20f/clients/src/main/java/org/apache/kafka/common/protocol/Errors.java#L317-L328 for the upstream error code names and descriptions. It looks like THROTTLING_QUOTA_EXCEEDED  and PRODUCER_FENCED  have not made it into a released broker yet, so we may want to hold off on them until they're released. But the others are documented at https://kafka.apache.org/protocol.html#protocol_error_codes already, and INVALID_RECORD in particular is being returned for messages with no keys that are produced to compacted topics.","See https://github.com/apache/kafka/blob/b937ec75677f8af13bf6fda686f07e9c62cdd20f/clients/src/main/java/org/apache/kafka/common/protocol/Errors.java#L317-L328 for the upstream error code names and descriptions. It looks like THROTTLING_QUOTA_EXCEEDED  and PRODUCER_FENCED  have not made it into a released broker yet, so we may want to hold off on them until they're released. But the others are documented at https://kafka.apache.org/protocol.html#protocol_error_codes already, and INVALID_RECORD in particular is being returned for messages with no keys that are produced to compacted topics.",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,3053,2020-08-28T19:47:31Z,2020-09-04T08:28:27Z,2020-09-04T08:28:32Z,MERGED,True,49,13,3,https://github.com/gridaphobe,add recent additions to kafka broker error codes,3,[],https://github.com/edenhill/librdkafka/pull/3053,https://github.com/edenhill,2,https://github.com/edenhill/librdkafka/pull/3053#issuecomment-687006265,"See https://github.com/apache/kafka/blob/b937ec75677f8af13bf6fda686f07e9c62cdd20f/clients/src/main/java/org/apache/kafka/common/protocol/Errors.java#L317-L328 for the upstream error code names and descriptions. It looks like THROTTLING_QUOTA_EXCEEDED  and PRODUCER_FENCED  have not made it into a released broker yet, so we may want to hold off on them until they're released. But the others are documented at https://kafka.apache.org/protocol.html#protocol_error_codes already, and INVALID_RECORD in particular is being returned for messages with no keys that are produced to compacted topics.",Thank you!,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,3055,2020-08-31T19:09:49Z,2020-09-04T06:38:40Z,2020-09-10T17:43:45Z,MERGED,True,307,130,6,https://github.com/edenhill,Configuration property enhancements,3,[],https://github.com/edenhill/librdkafka/pull/3055,https://github.com/edenhill,1,https://github.com/edenhill/librdkafka/pull/3055,,,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,3055,2020-08-31T19:09:49Z,2020-09-04T06:38:40Z,2020-09-10T17:43:45Z,MERGED,True,307,130,6,https://github.com/edenhill,Configuration property enhancements,3,[],https://github.com/edenhill/librdkafka/pull/3055,https://github.com/edenhill,2,https://github.com/edenhill/librdkafka/pull/3055#issuecomment-684442545,,"This provides more meaningful error messages when a user tries to configure disabled-at-build-time features, such as Kerberos.
And.. consumer properties set on a producer, and vice versa, are now logged as warning on client instantiation.
And.. CONFIGURATION.md will no longer be updated due to missing build features which will avoid dirty trees on clean builds.",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,3055,2020-08-31T19:09:49Z,2020-09-04T06:38:40Z,2020-09-10T17:43:45Z,MERGED,True,307,130,6,https://github.com/edenhill,Configuration property enhancements,3,[],https://github.com/edenhill/librdkafka/pull/3055,https://github.com/edenhill,3,https://github.com/edenhill/librdkafka/pull/3055#issuecomment-684444025,,The CI failures are unrelated,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,3055,2020-08-31T19:09:49Z,2020-09-04T06:38:40Z,2020-09-10T17:43:45Z,MERGED,True,307,130,6,https://github.com/edenhill,Configuration property enhancements,3,[],https://github.com/edenhill/librdkafka/pull/3055,https://github.com/edenhill,4,https://github.com/edenhill/librdkafka/pull/3055#issuecomment-690553941,,"except it seems unfortunate these are just warnings (i'm guessing you have your reasons),

That would be a breaking change, requiring a semver bump.

and i can't think why log.configuration.warnings has enough value to bother with.

Okay, then I'll remove it.",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,3058,2020-09-01T10:11:55Z,2020-09-01T10:13:45Z,2020-09-01T10:13:51Z,MERGED,True,1,1,1,https://github.com/zhangzhanhong,Update consumer.c,1,[],https://github.com/edenhill/librdkafka/pull/3058,https://github.com/zhangzhanhong,1,https://github.com/edenhill/librdkafka/pull/3058,Line 246 may be rkm->payload,Line 246 may be rkm->payload,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,3058,2020-09-01T10:11:55Z,2020-09-01T10:13:45Z,2020-09-01T10:13:51Z,MERGED,True,1,1,1,https://github.com/zhangzhanhong,Update consumer.c,1,[],https://github.com/edenhill/librdkafka/pull/3058,https://github.com/edenhill,2,https://github.com/edenhill/librdkafka/pull/3058#issuecomment-684724141,Line 246 may be rkm->payload,Thank you for the fix!,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,3059,2020-09-01T21:12:53Z,2020-09-03T07:26:13Z,2020-09-03T11:20:35Z,MERGED,True,27,6,6,https://github.com/gridaphobe,rdkafka_mock: fix message v2 overhead sanity check,2,[],https://github.com/edenhill/librdkafka/pull/3059,https://github.com/gridaphobe,1,https://github.com/edenhill/librdkafka/pull/3059,"The Message V2 API uses VARINTs, which means we have to use the
minimal overhead to sanity check the size of a ProduceRequest.","The Message V2 API uses VARINTs, which means we have to use the
minimal overhead to sanity check the size of a ProduceRequest.",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,3059,2020-09-01T21:12:53Z,2020-09-03T07:26:13Z,2020-09-03T11:20:35Z,MERGED,True,27,6,6,https://github.com/gridaphobe,rdkafka_mock: fix message v2 overhead sanity check,2,[],https://github.com/edenhill/librdkafka/pull/3059,https://github.com/edenhill,2,https://github.com/edenhill/librdkafka/pull/3059#issuecomment-686308458,"The Message V2 API uses VARINTs, which means we have to use the
minimal overhead to sanity check the size of a ProduceRequest.",How did this problem surface? In one of the existing tests or your own use of the mock broker?,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,3059,2020-09-01T21:12:53Z,2020-09-03T07:26:13Z,2020-09-03T11:20:35Z,MERGED,True,27,6,6,https://github.com/gridaphobe,rdkafka_mock: fix message v2 overhead sanity check,2,[],https://github.com/edenhill/librdkafka/pull/3059,https://github.com/edenhill,3,https://github.com/edenhill/librdkafka/pull/3059#issuecomment-686308697,"The Message V2 API uses VARINTs, which means we have to use the
minimal overhead to sanity check the size of a ProduceRequest.",Thank you!,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,3059,2020-09-01T21:12:53Z,2020-09-03T07:26:13Z,2020-09-03T11:20:35Z,MERGED,True,27,6,6,https://github.com/gridaphobe,rdkafka_mock: fix message v2 overhead sanity check,2,[],https://github.com/edenhill/librdkafka/pull/3059,https://github.com/gridaphobe,4,https://github.com/edenhill/librdkafka/pull/3059#issuecomment-686421100,"The Message V2 API uses VARINTs, which means we have to use the
minimal overhead to sanity check the size of a ProduceRequest.","We discovered it in our own use of the mock broker. We were sending tiny 5 byte messages in some unit tests. The mock broker worked fine as long as we didnt allow batching, but once we let messages batch, they were all rejected with invalid message size.

Sent from my iPhone
 On Sep 3, 2020, at 03:26, Magnus Edenhill ***@***.***> wrote:

 
 How did this problem surface? In one of the existing tests or your own use of the mock broker?

 
 You are receiving this because you authored the thread.
 Reply to this email directly, view it on GitHub, or unsubscribe.",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,3059,2020-09-01T21:12:53Z,2020-09-03T07:26:13Z,2020-09-03T11:20:35Z,MERGED,True,27,6,6,https://github.com/gridaphobe,rdkafka_mock: fix message v2 overhead sanity check,2,[],https://github.com/edenhill/librdkafka/pull/3059,https://github.com/edenhill,5,https://github.com/edenhill/librdkafka/pull/3059#issuecomment-686422232,"The Message V2 API uses VARINTs, which means we have to use the
minimal overhead to sanity check the size of a ProduceRequest.","Superb, thank you!",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,3060,2020-09-02T08:28:49Z,2020-09-09T06:01:01Z,2020-09-09T06:01:04Z,MERGED,True,117,16,8,https://github.com/edenhill,Fix destructor for KafkaConsumer,2,[],https://github.com/edenhill/librdkafka/pull/3060,https://github.com/edenhill,1,https://github.com/edenhill/librdkafka/pull/3060,,,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,3060,2020-09-02T08:28:49Z,2020-09-09T06:01:01Z,2020-09-09T06:01:04Z,MERGED,True,117,16,8,https://github.com/edenhill,Fix destructor for KafkaConsumer,2,[],https://github.com/edenhill/librdkafka/pull/3060,https://github.com/edenhill,2,https://github.com/edenhill/librdkafka/pull/3060#issuecomment-688090096,,@mhowlett Please re-review,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,3062,2020-09-03T13:37:59Z,2020-09-07T11:45:11Z,2020-09-07T11:45:13Z,MERGED,True,383,39,15,https://github.com/edenhill,Add proper producer support for topic authorization failures,3,[],https://github.com/edenhill/librdkafka/pull/3062,https://github.com/edenhill,1,https://github.com/edenhill/librdkafka/pull/3062,And fix a TxnMgr bug it surfaced  .,And fix a TxnMgr bug it surfaced  .,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,3063,2020-09-04T10:52:50Z,2020-09-07T07:12:26Z,2020-09-07T07:12:29Z,MERGED,True,61,0,4,https://github.com/edenhill,Treat cluster authentication failures as fatal in transactional producer (#2994),1,[],https://github.com/edenhill/librdkafka/pull/3063,https://github.com/edenhill,1,https://github.com/edenhill/librdkafka/pull/3063,,,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,3066,2020-09-10T07:15:14Z,2020-09-10T12:25:19Z,2020-09-10T12:25:19Z,CLOSED,False,3,1,1,https://github.com/leolin49,Update consumer.c,1,[],https://github.com/edenhill/librdkafka/pull/3066,https://github.com/leolin49,1,https://github.com/edenhill/librdkafka/pull/3066,"I used this example in a recent project, but I found some problems.
rd_kafka_conf_set(conf, ""auto.offset.reset"", ""earliest"", errstr, sizeof(errstr))
This code will get ""errstr""(No such configuration property: ""auto.offset.reset"").
so I think the configuration property is not in the ""rd_kafka_conf_t"", it maybe in the ""rd_kafka_topic_conf_t""
Of course, I'm not sure what I said is right or wrong","I used this example in a recent project, but I found some problems.
rd_kafka_conf_set(conf, ""auto.offset.reset"", ""earliest"", errstr, sizeof(errstr))
This code will get ""errstr""(No such configuration property: ""auto.offset.reset"").
so I think the configuration property is not in the ""rd_kafka_conf_t"", it maybe in the ""rd_kafka_topic_conf_t""
Of course, I'm not sure what I said is right or wrong",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,3066,2020-09-10T07:15:14Z,2020-09-10T12:25:19Z,2020-09-10T12:25:19Z,CLOSED,False,3,1,1,https://github.com/leolin49,Update consumer.c,1,[],https://github.com/edenhill/librdkafka/pull/3066,https://github.com/edenhill,2,https://github.com/edenhill/librdkafka/pull/3066#issuecomment-690245328,"I used this example in a recent project, but I found some problems.
rd_kafka_conf_set(conf, ""auto.offset.reset"", ""earliest"", errstr, sizeof(errstr))
This code will get ""errstr""(No such configuration property: ""auto.offset.reset"").
so I think the configuration property is not in the ""rd_kafka_conf_t"", it maybe in the ""rd_kafka_topic_conf_t""
Of course, I'm not sure what I said is right or wrong","No, it is fine set to topic configs on the global config object, in fact it is the proper way to do it unless you new topic-specific config.
My guess is that you're on a very old version of librdkafka that does not allow this.
The consumer.c example should be updated to only use the global config and not the topic config object.",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,3067,2020-09-10T10:49:54Z,2020-09-10T22:54:42Z,2020-09-10T22:54:45Z,MERGED,True,134,2,6,https://github.com/edenhill,Treat KafkaStorageError as a retriable ProduceRequest error (#3026),1,[],https://github.com/edenhill/librdkafka/pull/3067,https://github.com/edenhill,1,https://github.com/edenhill/librdkafka/pull/3067,,,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,3068,2020-09-10T12:22:22Z,2020-09-10T22:54:54Z,2020-09-10T22:54:57Z,MERGED,True,40,29,8,https://github.com/edenhill,Pass thru proper C message error string to C++ API,2,[],https://github.com/edenhill/librdkafka/pull/3068,https://github.com/edenhill,1,https://github.com/edenhill/librdkafka/pull/3068,,,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,3069,2020-09-10T17:47:04Z,2020-09-10T22:55:04Z,2020-09-10T22:55:07Z,MERGED,True,1,12,5,https://github.com/edenhill,Revert/remove log.configuration.warnings,1,[],https://github.com/edenhill/librdkafka/pull/3069,https://github.com/edenhill,1,https://github.com/edenhill/librdkafka/pull/3069,,,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,3073,2020-09-16T10:18:10Z,2020-09-22T14:18:29Z,2020-09-22T14:18:32Z,MERGED,True,1001,395,27,https://github.com/edenhill,Fix consumer stall when commit fails during rebalance (#2933),11,[],https://github.com/edenhill/librdkafka/pull/3073,https://github.com/edenhill,1,https://github.com/edenhill/librdkafka/pull/3073,Refactors OffsetCommit handling.,Refactors OffsetCommit handling.,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,3074,2020-09-16T13:57:26Z,2020-09-22T14:18:00Z,2020-09-22T14:18:03Z,MERGED,True,406,141,19,https://github.com/edenhill,Suppress repeated TOPIC_AUTHORIZATION_FAILED (on Fetch) errors (#3072),5,[],https://github.com/edenhill/librdkafka/pull/3074,https://github.com/edenhill,1,https://github.com/edenhill/librdkafka/pull/3074,,,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,3076,2020-09-17T21:05:16Z,2020-09-18T06:48:27Z,2020-09-18T06:48:31Z,MERGED,True,22,7,7,https://github.com/edenhill,Change default message retries to infinity,3,[],https://github.com/edenhill/librdkafka/pull/3076,https://github.com/edenhill,1,https://github.com/edenhill/librdkafka/pull/3076,"..since this is the recommendation.
The retries property was also reused for the default protocol request retry count, so changed that to a fixed value of 2 instead.","..since this is the recommendation.
The retries property was also reused for the default protocol request retry count, so changed that to a fixed value of 2 instead.",True,{'HEART': ['https://github.com/ybyzek']}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,3078,2020-09-18T08:50:27Z,2020-09-22T14:17:17Z,2020-09-22T14:17:21Z,MERGED,True,50,28,9,https://github.com/edenhill,Socket fixes,5,[],https://github.com/edenhill/librdkafka/pull/3078,https://github.com/edenhill,1,https://github.com/edenhill/librdkafka/pull/3078,,,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,3086,2020-09-22T15:29:32Z,2020-09-27T13:07:08Z,2020-09-27T13:07:10Z,MERGED,True,12,6,2,https://github.com/edenhill,Remove incorrect call to zlib's getInflateHeaders,1,['security'],https://github.com/edenhill/librdkafka/pull/3086,https://github.com/edenhill,1,https://github.com/edenhill/librdkafka/pull/3086,Reported by Ilja van Sprundel.,Reported by Ilja van Sprundel.,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,3087,2020-09-23T14:26:03Z,2020-09-29T16:17:02Z,2020-09-29T16:17:06Z,MERGED,True,278,37,10,https://github.com/edenhill,Transactional producer fixes,5,[],https://github.com/edenhill/librdkafka/pull/3087,https://github.com/edenhill,1,https://github.com/edenhill/librdkafka/pull/3087,See commit messages for context,See commit messages for context,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,3088,2020-09-25T11:03:00Z,2020-09-27T13:06:53Z,2020-09-27T13:06:55Z,MERGED,True,18,30,4,https://github.com/edenhill,Clean up rd_kafka_topic_partition_t helper functions a bit,1,[],https://github.com/edenhill/librdkafka/pull/3088,https://github.com/edenhill,1,https://github.com/edenhill/librdkafka/pull/3088,,,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,3092,2020-09-29T16:28:39Z,2020-09-29T17:19:29Z,2020-09-29T17:19:33Z,MERGED,True,6,2,3,https://github.com/edenhill,Change producer request.timeout.ms from 5 to 30s to match Java producer,1,[],https://github.com/edenhill/librdkafka/pull/3092,https://github.com/edenhill,1,https://github.com/edenhill/librdkafka/pull/3092,,,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,3095,2020-10-03T08:46:08Z,,2020-10-03T08:46:08Z,OPEN,False,14,14,2,https://github.com/lordgamez,Fix compilation of librdkafka in VS2017,1,[],https://github.com/edenhill/librdkafka/pull/3095,https://github.com/lordgamez,1,https://github.com/edenhill/librdkafka/pull/3095,"I had problems while compiling librdkafka in VS2017 both in GUI and CLI. Part of the problem was due to the zlib nuget packages depending on the PlatformToolSet v140 which was referenced in a number of issues like #2816  and #2731. This could be avoided by replacing the package with the zlib_native package which has a zlib version of v.1.2.11.  It may also address the issue #2934.
The linked OpenSSL library names also needed to be updated to reflect the library names in the latest version 1.1.1h.","I had problems while compiling librdkafka in VS2017 both in GUI and CLI. Part of the problem was due to the zlib nuget packages depending on the PlatformToolSet v140 which was referenced in a number of issues like #2816  and #2731. This could be avoided by replacing the package with the zlib_native package which has a zlib version of v.1.2.11.  It may also address the issue #2934.
The linked OpenSSL library names also needed to be updated to reflect the library names in the latest version 1.1.1h.",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,3096,2020-10-03T12:25:26Z,2020-10-03T12:45:19Z,2020-10-03T12:45:20Z,CLOSED,False,70,0,1,https://github.com/malyabansalper,Matrix Multiplication,1,[],https://github.com/edenhill/librdkafka/pull/3096,https://github.com/malyabansalper,1,https://github.com/edenhill/librdkafka/pull/3096,"Simple Matrix Multiplication program in CPP
Please accept the pull request","Simple Matrix Multiplication program in CPP
Please accept the pull request",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,3096,2020-10-03T12:25:26Z,2020-10-03T12:45:19Z,2020-10-03T12:45:20Z,CLOSED,False,70,0,1,https://github.com/malyabansalper,Matrix Multiplication,1,[],https://github.com/edenhill/librdkafka/pull/3096,https://github.com/edenhill,2,https://github.com/edenhill/librdkafka/pull/3096#issuecomment-703098069,"Simple Matrix Multiplication program in CPP
Please accept the pull request",Not relevant to project.,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,3100,2020-10-05T22:21:11Z,2020-10-22T10:52:09Z,2020-10-22T10:52:16Z,MERGED,True,48,11,9,https://github.com/azat,Use reentrant version of rand() to avoid locking,1,[],https://github.com/edenhill/librdkafka/pull/3100,https://github.com/azat,1,https://github.com/edenhill/librdkafka/pull/3100,"rand() has a global lock 1, which is very very suboptiomal, so let's
consider the following example, suppose you have 1000 threads and you
need to generate 10e6 random values (if you have 10e6 messages with
random distribution you will need them):

rand:   0m2.336s
rand_r: 0m0.041s

So rand() is ~60x times slower the rand_r(), for the reproducer see 2.
And also this means that with random partitioning you need to spend
extra 2.4 seconds to publish 10e6 message (while in real life, with
librdkafka performance will be even worse, since lots of threads will
contend for the lock, call futex and will slow down other threads too)
Plus here is top of syscalls (yes it is very inaccurate due to strace
overhead, but I think that for this particular usecase it is ok, and I
want to underline this anyway):
$ strace -qq -f -c /tmp/rand_r |& head -n5
% time     seconds  usecs/call     calls    errors syscall
------ ----------- ----------- --------- --------- ----------------
 46.13    0.064718          64      1000           clone
 13.93    0.019540           6      3001           rt_sigprocmask
 13.47    0.018891          18      1000           madvise
$ strace -qq -f -c /tmp/rand |& head -n5
% time     seconds  usecs/call     calls    errors syscall
------ ----------- ----------- --------- --------- ----------------
 86.76    4.615370          68     67369     26161 futex <-- **
  4.11    0.218796          72      3001           rt_sigprocmask
  3.17    0.168655         168      1000           clone

And since librdkafka uses threads massively this can become a bottleneck
(for example if you have consumer/producer per partition).
v2: introduce rd_rand_r() for win32 (that lacks of rand_r()).
v3: detect rand_r() at runtime use rand_s() on win32.
v4: add more information into the commit.","rand() has a global lock 1, which is very very suboptiomal, so let's
consider the following example, suppose you have 1000 threads and you
need to generate 10e6 random values (if you have 10e6 messages with
random distribution you will need them):

rand:   0m2.336s
rand_r: 0m0.041s

So rand() is ~60x times slower the rand_r(), for the reproducer see 2.
And also this means that with random partitioning you need to spend
extra 2.4 seconds to publish 10e6 message (while in real life, with
librdkafka performance will be even worse, since lots of threads will
contend for the lock, call futex and will slow down other threads too)
Plus here is top of syscalls (yes it is very inaccurate due to strace
overhead, but I think that for this particular usecase it is ok, and I
want to underline this anyway):
$ strace -qq -f -c /tmp/rand_r |& head -n5
% time     seconds  usecs/call     calls    errors syscall
------ ----------- ----------- --------- --------- ----------------
 46.13    0.064718          64      1000           clone
 13.93    0.019540           6      3001           rt_sigprocmask
 13.47    0.018891          18      1000           madvise
$ strace -qq -f -c /tmp/rand |& head -n5
% time     seconds  usecs/call     calls    errors syscall
------ ----------- ----------- --------- --------- ----------------
 86.76    4.615370          68     67369     26161 futex <-- **
  4.11    0.218796          72      3001           rt_sigprocmask
  3.17    0.168655         168      1000           clone

And since librdkafka uses threads massively this can become a bottleneck
(for example if you have consumer/producer per partition).
v2: introduce rd_rand_r() for win32 (that lacks of rand_r()).
v3: detect rand_r() at runtime use rand_s() on win32.
v4: add more information into the commit.",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,3100,2020-10-05T22:21:11Z,2020-10-22T10:52:09Z,2020-10-22T10:52:16Z,MERGED,True,48,11,9,https://github.com/azat,Use reentrant version of rand() to avoid locking,1,[],https://github.com/edenhill/librdkafka/pull/3100,https://github.com/edenhill,2,https://github.com/edenhill/librdkafka/pull/3100#issuecomment-704222835,"rand() has a global lock 1, which is very very suboptiomal, so let's
consider the following example, suppose you have 1000 threads and you
need to generate 10e6 random values (if you have 10e6 messages with
random distribution you will need them):

rand:   0m2.336s
rand_r: 0m0.041s

So rand() is ~60x times slower the rand_r(), for the reproducer see 2.
And also this means that with random partitioning you need to spend
extra 2.4 seconds to publish 10e6 message (while in real life, with
librdkafka performance will be even worse, since lots of threads will
contend for the lock, call futex and will slow down other threads too)
Plus here is top of syscalls (yes it is very inaccurate due to strace
overhead, but I think that for this particular usecase it is ok, and I
want to underline this anyway):
$ strace -qq -f -c /tmp/rand_r |& head -n5
% time     seconds  usecs/call     calls    errors syscall
------ ----------- ----------- --------- --------- ----------------
 46.13    0.064718          64      1000           clone
 13.93    0.019540           6      3001           rt_sigprocmask
 13.47    0.018891          18      1000           madvise
$ strace -qq -f -c /tmp/rand |& head -n5
% time     seconds  usecs/call     calls    errors syscall
------ ----------- ----------- --------- --------- ----------------
 86.76    4.615370          68     67369     26161 futex <-- **
  4.11    0.218796          72      3001           rt_sigprocmask
  3.17    0.168655         168      1000           clone

And since librdkafka uses threads massively this can become a bottleneck
(for example if you have consumer/producer per partition).
v2: introduce rd_rand_r() for win32 (that lacks of rand_r()).
v3: detect rand_r() at runtime use rand_s() on win32.
v4: add more information into the commit.","Windows build needs to be fixed.
Must not remove configuration properties without a semver bump, so if enable.random.seed is no longer used it should be marked with _RK_DEPRECATED, but I still think we need to use it on Windows (where there is no rand_r()) as rd_jitter() is called from the application thread.",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,3100,2020-10-05T22:21:11Z,2020-10-22T10:52:09Z,2020-10-22T10:52:16Z,MERGED,True,48,11,9,https://github.com/azat,Use reentrant version of rand() to avoid locking,1,[],https://github.com/edenhill/librdkafka/pull/3100,https://github.com/azat,3,https://github.com/edenhill/librdkafka/pull/3100#issuecomment-704584066,"rand() has a global lock 1, which is very very suboptiomal, so let's
consider the following example, suppose you have 1000 threads and you
need to generate 10e6 random values (if you have 10e6 messages with
random distribution you will need them):

rand:   0m2.336s
rand_r: 0m0.041s

So rand() is ~60x times slower the rand_r(), for the reproducer see 2.
And also this means that with random partitioning you need to spend
extra 2.4 seconds to publish 10e6 message (while in real life, with
librdkafka performance will be even worse, since lots of threads will
contend for the lock, call futex and will slow down other threads too)
Plus here is top of syscalls (yes it is very inaccurate due to strace
overhead, but I think that for this particular usecase it is ok, and I
want to underline this anyway):
$ strace -qq -f -c /tmp/rand_r |& head -n5
% time     seconds  usecs/call     calls    errors syscall
------ ----------- ----------- --------- --------- ----------------
 46.13    0.064718          64      1000           clone
 13.93    0.019540           6      3001           rt_sigprocmask
 13.47    0.018891          18      1000           madvise
$ strace -qq -f -c /tmp/rand |& head -n5
% time     seconds  usecs/call     calls    errors syscall
------ ----------- ----------- --------- --------- ----------------
 86.76    4.615370          68     67369     26161 futex <-- **
  4.11    0.218796          72      3001           rt_sigprocmask
  3.17    0.168655         168      1000           clone

And since librdkafka uses threads massively this can become a bottleneck
(for example if you have consumer/producer per partition).
v2: introduce rd_rand_r() for win32 (that lacks of rand_r()).
v3: detect rand_r() at runtime use rand_s() on win32.
v4: add more information into the commit.","doozer/target/xenial-amd64

|[31m 0104_fetch_from_follower_mock            |     FAILED |  59.356s [0m|[31m test_consumer_poll():3804: Consume: consumer_poll() timeout (0/1 eof, 0/1000 msgs)[0m

flacky?",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,3100,2020-10-05T22:21:11Z,2020-10-22T10:52:09Z,2020-10-22T10:52:16Z,MERGED,True,48,11,9,https://github.com/azat,Use reentrant version of rand() to avoid locking,1,[],https://github.com/edenhill/librdkafka/pull/3100,https://github.com/azat,4,https://github.com/edenhill/librdkafka/pull/3100#issuecomment-704584460,"rand() has a global lock 1, which is very very suboptiomal, so let's
consider the following example, suppose you have 1000 threads and you
need to generate 10e6 random values (if you have 10e6 messages with
random distribution you will need them):

rand:   0m2.336s
rand_r: 0m0.041s

So rand() is ~60x times slower the rand_r(), for the reproducer see 2.
And also this means that with random partitioning you need to spend
extra 2.4 seconds to publish 10e6 message (while in real life, with
librdkafka performance will be even worse, since lots of threads will
contend for the lock, call futex and will slow down other threads too)
Plus here is top of syscalls (yes it is very inaccurate due to strace
overhead, but I think that for this particular usecase it is ok, and I
want to underline this anyway):
$ strace -qq -f -c /tmp/rand_r |& head -n5
% time     seconds  usecs/call     calls    errors syscall
------ ----------- ----------- --------- --------- ----------------
 46.13    0.064718          64      1000           clone
 13.93    0.019540           6      3001           rt_sigprocmask
 13.47    0.018891          18      1000           madvise
$ strace -qq -f -c /tmp/rand |& head -n5
% time     seconds  usecs/call     calls    errors syscall
------ ----------- ----------- --------- --------- ----------------
 86.76    4.615370          68     67369     26161 futex <-- **
  4.11    0.218796          72      3001           rt_sigprocmask
  3.17    0.168655         168      1000           clone

And since librdkafka uses threads massively this can become a bottleneck
(for example if you have consumer/producer per partition).
v2: introduce rd_rand_r() for win32 (that lacks of rand_r()).
v3: detect rand_r() at runtime use rand_s() on win32.
v4: add more information into the commit.","Windows build needs to be fixed.

Added version of rand_r (rk_rand_r)

Must not remove configuration properties without a semver bump, so if enable.random.seed is no longer used it should be marked with _RK_DEPRECATED

Fixed

but I still think we need to use it on Windows (where there is no rand_r()) as rd_jitter() is called from the application thread.

After rk_rand_r(), looks like there is no point in this?",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,3100,2020-10-05T22:21:11Z,2020-10-22T10:52:09Z,2020-10-22T10:52:16Z,MERGED,True,48,11,9,https://github.com/azat,Use reentrant version of rand() to avoid locking,1,[],https://github.com/edenhill/librdkafka/pull/3100,https://github.com/edenhill,5,https://github.com/edenhill/librdkafka/pull/3100#issuecomment-704735911,"rand() has a global lock 1, which is very very suboptiomal, so let's
consider the following example, suppose you have 1000 threads and you
need to generate 10e6 random values (if you have 10e6 messages with
random distribution you will need them):

rand:   0m2.336s
rand_r: 0m0.041s

So rand() is ~60x times slower the rand_r(), for the reproducer see 2.
And also this means that with random partitioning you need to spend
extra 2.4 seconds to publish 10e6 message (while in real life, with
librdkafka performance will be even worse, since lots of threads will
contend for the lock, call futex and will slow down other threads too)
Plus here is top of syscalls (yes it is very inaccurate due to strace
overhead, but I think that for this particular usecase it is ok, and I
want to underline this anyway):
$ strace -qq -f -c /tmp/rand_r |& head -n5
% time     seconds  usecs/call     calls    errors syscall
------ ----------- ----------- --------- --------- ----------------
 46.13    0.064718          64      1000           clone
 13.93    0.019540           6      3001           rt_sigprocmask
 13.47    0.018891          18      1000           madvise
$ strace -qq -f -c /tmp/rand |& head -n5
% time     seconds  usecs/call     calls    errors syscall
------ ----------- ----------- --------- --------- ----------------
 86.76    4.615370          68     67369     26161 futex <-- **
  4.11    0.218796          72      3001           rt_sigprocmask
  3.17    0.168655         168      1000           clone

And since librdkafka uses threads massively this can become a bottleneck
(for example if you have consumer/producer per partition).
v2: introduce rd_rand_r() for win32 (that lacks of rand_r()).
v3: detect rand_r() at runtime use rand_s() on win32.
v4: add more information into the commit.","It seems rand_r() is deprecated and might be removed from future posix versions:
https://stackoverflow.com/a/29135420/1821055
This means there might be portability issues with relying on rand_r() always existing on Unix-like platforms.
What we could do is check for rand_r() in configure at build time, and if not available fall back on standard rand(). This means the srand config must not be deprecated.
But I'm curious, on what platform did you see performance issues with rand(), and do you have any numbers to show comparing librdkafka with rand() vs rand_r()?",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,3100,2020-10-05T22:21:11Z,2020-10-22T10:52:09Z,2020-10-22T10:52:16Z,MERGED,True,48,11,9,https://github.com/azat,Use reentrant version of rand() to avoid locking,1,[],https://github.com/edenhill/librdkafka/pull/3100,https://github.com/azat,6,https://github.com/edenhill/librdkafka/pull/3100#issuecomment-704793704,"rand() has a global lock 1, which is very very suboptiomal, so let's
consider the following example, suppose you have 1000 threads and you
need to generate 10e6 random values (if you have 10e6 messages with
random distribution you will need them):

rand:   0m2.336s
rand_r: 0m0.041s

So rand() is ~60x times slower the rand_r(), for the reproducer see 2.
And also this means that with random partitioning you need to spend
extra 2.4 seconds to publish 10e6 message (while in real life, with
librdkafka performance will be even worse, since lots of threads will
contend for the lock, call futex and will slow down other threads too)
Plus here is top of syscalls (yes it is very inaccurate due to strace
overhead, but I think that for this particular usecase it is ok, and I
want to underline this anyway):
$ strace -qq -f -c /tmp/rand_r |& head -n5
% time     seconds  usecs/call     calls    errors syscall
------ ----------- ----------- --------- --------- ----------------
 46.13    0.064718          64      1000           clone
 13.93    0.019540           6      3001           rt_sigprocmask
 13.47    0.018891          18      1000           madvise
$ strace -qq -f -c /tmp/rand |& head -n5
% time     seconds  usecs/call     calls    errors syscall
------ ----------- ----------- --------- --------- ----------------
 86.76    4.615370          68     67369     26161 futex <-- **
  4.11    0.218796          72      3001           rt_sigprocmask
  3.17    0.168655         168      1000           clone

And since librdkafka uses threads massively this can become a bottleneck
(for example if you have consumer/producer per partition).
v2: introduce rd_rand_r() for win32 (that lacks of rand_r()).
v3: detect rand_r() at runtime use rand_s() on win32.
v4: add more information into the commit.","This means there might be portability issues with relying on rand_r() always existing on Unix-like platforms.

This PR does not contain rand_r already (rand_r inlined).

But I'm curious, on what platform did you see performance issues with rand()

rand() in glibc contains global lock and it is very bad for performance

and do you have any numbers to show comparing librdkafka with rand() vs rand_r()?

So here is the gist that will show the problem in numbers - https://gist.github.com/azat/690c3d63ec485555059c2a0ea7bff1c4
It shows that rand() is slower then rand_r() ~60 times, or it takes 2.4 seconds to generate 10e6 random numbers from multiple threads.
And this means that with random partitioning you need to spend extra 2.4 seconds to publish 10e6 message (while in real life, with librdkafka performance will be even  worse, since lots of threads will contend for the lock, call futex and will slow down other threads too)",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,3100,2020-10-05T22:21:11Z,2020-10-22T10:52:09Z,2020-10-22T10:52:16Z,MERGED,True,48,11,9,https://github.com/azat,Use reentrant version of rand() to avoid locking,1,[],https://github.com/edenhill/librdkafka/pull/3100,https://github.com/azat,7,https://github.com/edenhill/librdkafka/pull/3100#issuecomment-704799429,"rand() has a global lock 1, which is very very suboptiomal, so let's
consider the following example, suppose you have 1000 threads and you
need to generate 10e6 random values (if you have 10e6 messages with
random distribution you will need them):

rand:   0m2.336s
rand_r: 0m0.041s

So rand() is ~60x times slower the rand_r(), for the reproducer see 2.
And also this means that with random partitioning you need to spend
extra 2.4 seconds to publish 10e6 message (while in real life, with
librdkafka performance will be even worse, since lots of threads will
contend for the lock, call futex and will slow down other threads too)
Plus here is top of syscalls (yes it is very inaccurate due to strace
overhead, but I think that for this particular usecase it is ok, and I
want to underline this anyway):
$ strace -qq -f -c /tmp/rand_r |& head -n5
% time     seconds  usecs/call     calls    errors syscall
------ ----------- ----------- --------- --------- ----------------
 46.13    0.064718          64      1000           clone
 13.93    0.019540           6      3001           rt_sigprocmask
 13.47    0.018891          18      1000           madvise
$ strace -qq -f -c /tmp/rand |& head -n5
% time     seconds  usecs/call     calls    errors syscall
------ ----------- ----------- --------- --------- ----------------
 86.76    4.615370          68     67369     26161 futex <-- **
  4.11    0.218796          72      3001           rt_sigprocmask
  3.17    0.168655         168      1000           clone

And since librdkafka uses threads massively this can become a bottleneck
(for example if you have consumer/producer per partition).
v2: introduce rd_rand_r() for win32 (that lacks of rand_r()).
v3: detect rand_r() at runtime use rand_s() on win32.
v4: add more information into the commit.","And here are some numbers for time spend in syscalls (yes it is very inaccurate due to strace overhead, but it shows the problem for this particular case)
$ strace -qq -f -c /tmp/rand_r |& head -n5
% time     seconds  usecs/call     calls    errors syscall
------ ----------- ----------- --------- --------- ----------------
 46.13    0.064718          64      1000           clone
 13.93    0.019540           6      3001           rt_sigprocmask
 13.47    0.018891          18      1000           madvise

$ strace -qq -f -c /tmp/rand |& head -n5
% time     seconds  usecs/call     calls    errors syscall
------ ----------- ----------- --------- --------- ----------------
 86.76    4.615370          68     67369     26161 futex <-- **
  4.11    0.218796          72      3001           rt_sigprocmask
  3.17    0.168655         168      1000           clone",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,3100,2020-10-05T22:21:11Z,2020-10-22T10:52:09Z,2020-10-22T10:52:16Z,MERGED,True,48,11,9,https://github.com/azat,Use reentrant version of rand() to avoid locking,1,[],https://github.com/edenhill/librdkafka/pull/3100,https://github.com/edenhill,8,https://github.com/edenhill/librdkafka/pull/3100#issuecomment-704890178,"rand() has a global lock 1, which is very very suboptiomal, so let's
consider the following example, suppose you have 1000 threads and you
need to generate 10e6 random values (if you have 10e6 messages with
random distribution you will need them):

rand:   0m2.336s
rand_r: 0m0.041s

So rand() is ~60x times slower the rand_r(), for the reproducer see 2.
And also this means that with random partitioning you need to spend
extra 2.4 seconds to publish 10e6 message (while in real life, with
librdkafka performance will be even worse, since lots of threads will
contend for the lock, call futex and will slow down other threads too)
Plus here is top of syscalls (yes it is very inaccurate due to strace
overhead, but I think that for this particular usecase it is ok, and I
want to underline this anyway):
$ strace -qq -f -c /tmp/rand_r |& head -n5
% time     seconds  usecs/call     calls    errors syscall
------ ----------- ----------- --------- --------- ----------------
 46.13    0.064718          64      1000           clone
 13.93    0.019540           6      3001           rt_sigprocmask
 13.47    0.018891          18      1000           madvise
$ strace -qq -f -c /tmp/rand |& head -n5
% time     seconds  usecs/call     calls    errors syscall
------ ----------- ----------- --------- --------- ----------------
 86.76    4.615370          68     67369     26161 futex <-- **
  4.11    0.218796          72      3001           rt_sigprocmask
  3.17    0.168655         168      1000           clone

And since librdkafka uses threads massively this can become a bottleneck
(for example if you have consumer/producer per partition).
v2: introduce rd_rand_r() for win32 (that lacks of rand_r()).
v3: detect rand_r() at runtime use rand_s() on win32.
v4: add more information into the commit.","Wow, that's quite a difference.",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,3100,2020-10-05T22:21:11Z,2020-10-22T10:52:09Z,2020-10-22T10:52:16Z,MERGED,True,48,11,9,https://github.com/azat,Use reentrant version of rand() to avoid locking,1,[],https://github.com/edenhill/librdkafka/pull/3100,https://github.com/azat,9,https://github.com/edenhill/librdkafka/pull/3100#issuecomment-713463143,"rand() has a global lock 1, which is very very suboptiomal, so let's
consider the following example, suppose you have 1000 threads and you
need to generate 10e6 random values (if you have 10e6 messages with
random distribution you will need them):

rand:   0m2.336s
rand_r: 0m0.041s

So rand() is ~60x times slower the rand_r(), for the reproducer see 2.
And also this means that with random partitioning you need to spend
extra 2.4 seconds to publish 10e6 message (while in real life, with
librdkafka performance will be even worse, since lots of threads will
contend for the lock, call futex and will slow down other threads too)
Plus here is top of syscalls (yes it is very inaccurate due to strace
overhead, but I think that for this particular usecase it is ok, and I
want to underline this anyway):
$ strace -qq -f -c /tmp/rand_r |& head -n5
% time     seconds  usecs/call     calls    errors syscall
------ ----------- ----------- --------- --------- ----------------
 46.13    0.064718          64      1000           clone
 13.93    0.019540           6      3001           rt_sigprocmask
 13.47    0.018891          18      1000           madvise
$ strace -qq -f -c /tmp/rand |& head -n5
% time     seconds  usecs/call     calls    errors syscall
------ ----------- ----------- --------- --------- ----------------
 86.76    4.615370          68     67369     26161 futex <-- **
  4.11    0.218796          72      3001           rt_sigprocmask
  3.17    0.168655         168      1000           clone

And since librdkafka uses threads massively this can become a bottleneck
(for example if you have consumer/producer per partition).
v2: introduce rd_rand_r() for win32 (that lacks of rand_r()).
v3: detect rand_r() at runtime use rand_s() on win32.
v4: add more information into the commit.",@edenhill friendly ping,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,3100,2020-10-05T22:21:11Z,2020-10-22T10:52:09Z,2020-10-22T10:52:16Z,MERGED,True,48,11,9,https://github.com/azat,Use reentrant version of rand() to avoid locking,1,[],https://github.com/edenhill/librdkafka/pull/3100,https://github.com/azat,10,https://github.com/edenhill/librdkafka/pull/3100#issuecomment-713829341,"rand() has a global lock 1, which is very very suboptiomal, so let's
consider the following example, suppose you have 1000 threads and you
need to generate 10e6 random values (if you have 10e6 messages with
random distribution you will need them):

rand:   0m2.336s
rand_r: 0m0.041s

So rand() is ~60x times slower the rand_r(), for the reproducer see 2.
And also this means that with random partitioning you need to spend
extra 2.4 seconds to publish 10e6 message (while in real life, with
librdkafka performance will be even worse, since lots of threads will
contend for the lock, call futex and will slow down other threads too)
Plus here is top of syscalls (yes it is very inaccurate due to strace
overhead, but I think that for this particular usecase it is ok, and I
want to underline this anyway):
$ strace -qq -f -c /tmp/rand_r |& head -n5
% time     seconds  usecs/call     calls    errors syscall
------ ----------- ----------- --------- --------- ----------------
 46.13    0.064718          64      1000           clone
 13.93    0.019540           6      3001           rt_sigprocmask
 13.47    0.018891          18      1000           madvise
$ strace -qq -f -c /tmp/rand |& head -n5
% time     seconds  usecs/call     calls    errors syscall
------ ----------- ----------- --------- --------- ----------------
 86.76    4.615370          68     67369     26161 futex <-- **
  4.11    0.218796          72      3001           rt_sigprocmask
  3.17    0.168655         168      1000           clone

And since librdkafka uses threads massively this can become a bottleneck
(for example if you have consumer/producer per partition).
v2: introduce rd_rand_r() for win32 (that lacks of rand_r()).
v3: detect rand_r() at runtime use rand_s() on win32.
v4: add more information into the commit.",Updated (test failures looks unrelated),True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,3100,2020-10-05T22:21:11Z,2020-10-22T10:52:09Z,2020-10-22T10:52:16Z,MERGED,True,48,11,9,https://github.com/azat,Use reentrant version of rand() to avoid locking,1,[],https://github.com/edenhill/librdkafka/pull/3100,https://github.com/edenhill,11,https://github.com/edenhill/librdkafka/pull/3100#issuecomment-714409966,"rand() has a global lock 1, which is very very suboptiomal, so let's
consider the following example, suppose you have 1000 threads and you
need to generate 10e6 random values (if you have 10e6 messages with
random distribution you will need them):

rand:   0m2.336s
rand_r: 0m0.041s

So rand() is ~60x times slower the rand_r(), for the reproducer see 2.
And also this means that with random partitioning you need to spend
extra 2.4 seconds to publish 10e6 message (while in real life, with
librdkafka performance will be even worse, since lots of threads will
contend for the lock, call futex and will slow down other threads too)
Plus here is top of syscalls (yes it is very inaccurate due to strace
overhead, but I think that for this particular usecase it is ok, and I
want to underline this anyway):
$ strace -qq -f -c /tmp/rand_r |& head -n5
% time     seconds  usecs/call     calls    errors syscall
------ ----------- ----------- --------- --------- ----------------
 46.13    0.064718          64      1000           clone
 13.93    0.019540           6      3001           rt_sigprocmask
 13.47    0.018891          18      1000           madvise
$ strace -qq -f -c /tmp/rand |& head -n5
% time     seconds  usecs/call     calls    errors syscall
------ ----------- ----------- --------- --------- ----------------
 86.76    4.615370          68     67369     26161 futex <-- **
  4.11    0.218796          72      3001           rt_sigprocmask
  3.17    0.168655         168      1000           clone

And since librdkafka uses threads massively this can become a bottleneck
(for example if you have consumer/producer per partition).
v2: introduce rd_rand_r() for win32 (that lacks of rand_r()).
v3: detect rand_r() at runtime use rand_s() on win32.
v4: add more information into the commit.","This is great, thank you!",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,3111,2020-10-16T20:52:19Z,2020-10-20T17:00:13Z,2020-10-20T17:00:13Z,MERGED,True,7573,1694,55,https://github.com/edenhill,"Incremental rebalancing: cgrp and assignment refactoring, tests, fixes",24,[],https://github.com/edenhill/librdkafka/pull/3111,https://github.com/edenhill,1,https://github.com/edenhill/librdkafka/pull/3111,This is a continuation of #3032 (which can now be closed).,This is a continuation of #3032 (which can now be closed).,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,3111,2020-10-16T20:52:19Z,2020-10-20T17:00:13Z,2020-10-20T17:00:13Z,MERGED,True,7573,1694,55,https://github.com/edenhill,"Incremental rebalancing: cgrp and assignment refactoring, tests, fixes",24,[],https://github.com/edenhill/librdkafka/pull/3111,https://github.com/mhowlett,2,https://github.com/edenhill/librdkafka/pull/3111#issuecomment-713000580,This is a continuation of #3032 (which can now be closed).,"LGTM still. My comments are mostly around high level structure, but I see addressing that as a much bigger task outside the scope of this PR.",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,3112,2020-10-21T09:39:28Z,2020-10-22T20:34:20Z,2020-10-22T20:34:24Z,MERGED,True,256,92,16,https://github.com/edenhill,Incremental fixes to incremental rebalancing,4,[],https://github.com/edenhill/librdkafka/pull/3112,https://github.com/edenhill,1,https://github.com/edenhill/librdkafka/pull/3112,,,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,3113,2020-10-21T15:55:47Z,,2020-10-27T14:09:01Z,OPEN,False,651,3,14,https://github.com/coignetp,Add `dogstatsd.endpoint` configuration option,7,[],https://github.com/edenhill/librdkafka/pull/3113,https://github.com/coignetp,1,https://github.com/edenhill/librdkafka/pull/3113,"Hi!
I'd like to add a configuration option named dogstatsd.endpoint. The goal here is to allow the lib to emit to a DogStatsD endpoint (some documentation about DogStatsD here https://docs.datadoghq.com/developers/dogstatsd/, https://docs.datadoghq.com/developers/dogstatsd/datagram_shell). When dogstatsd.endpoint is set, some of the statistics are emitted to this endpoint every statistics.interval.ms ms.
This option provides an alternative to parsing the json stats and help people understand what's happening in their consumers/producers.
I still have a few questions below, like what are the best librdkafka tools to parse the dogstatsd address.
Please tell me what you think about this change!","Hi!
I'd like to add a configuration option named dogstatsd.endpoint. The goal here is to allow the lib to emit to a DogStatsD endpoint (some documentation about DogStatsD here https://docs.datadoghq.com/developers/dogstatsd/, https://docs.datadoghq.com/developers/dogstatsd/datagram_shell). When dogstatsd.endpoint is set, some of the statistics are emitted to this endpoint every statistics.interval.ms ms.
This option provides an alternative to parsing the json stats and help people understand what's happening in their consumers/producers.
I still have a few questions below, like what are the best librdkafka tools to parse the dogstatsd address.
Please tell me what you think about this change!",True,{'HOORAY': ['https://github.com/ofek']}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,3113,2020-10-21T15:55:47Z,,2020-10-27T14:09:01Z,OPEN,False,651,3,14,https://github.com/coignetp,Add `dogstatsd.endpoint` configuration option,7,[],https://github.com/edenhill/librdkafka/pull/3113,https://github.com/edenhill,2,https://github.com/edenhill/librdkafka/pull/3113#issuecomment-717076495,"Hi!
I'd like to add a configuration option named dogstatsd.endpoint. The goal here is to allow the lib to emit to a DogStatsD endpoint (some documentation about DogStatsD here https://docs.datadoghq.com/developers/dogstatsd/, https://docs.datadoghq.com/developers/dogstatsd/datagram_shell). When dogstatsd.endpoint is set, some of the statistics are emitted to this endpoint every statistics.interval.ms ms.
This option provides an alternative to parsing the json stats and help people understand what's happening in their consumers/producers.
I still have a few questions below, like what are the best librdkafka tools to parse the dogstatsd address.
Please tell me what you think about this change!","Hi Paul,
thanks for your work on this, it does seem like a useful feature to customers of Datadog.
However, I'm reluctant to add builtin support for a proprietary system, I think
this would be better as an add-on library that a user uses in conjunction with librdkafka, or as
a librdkafka plugin.
There's also asks to provide prometheus integration in librdkafka, but that too should reside outside of
the librdkafka code base.
What I think we want to do is add interceptors to the internal stats so that a (datadog, prometheus, ..) plugin
can intercept the metrics and bake it in a form that suits them.
I haven't thought this through in detail since I don't know the exact requirements, but due to the way that metrics
are collected inside librdkafka I want to avoid having interceptors for each update to a metric, but instead in the
metrics collector (that fires every statistics.interval.ms).
Maybe an interceptor that get's called for each emitted metric, as so:
void on_metrics_emit (rd_kafka_t *rk, const char *metrics_name, const rd_kafka_metric_value_t *value, void *ic_opaque)
where metrics_name is ""brokers.3.rxbytes"" and metric_value is { .type = RD_KAFKA_METRIC_COUNTER_U64, .value.u64 = 12345 }.
Let me know your thoughts on this overall approach.
/Magnus
P.S. There's also ongoing work to formalize a standard metrics interface for Kafka clients, a KIP will be posted shortly.",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,3113,2020-10-21T15:55:47Z,,2020-10-27T14:09:01Z,OPEN,False,651,3,14,https://github.com/coignetp,Add `dogstatsd.endpoint` configuration option,7,[],https://github.com/edenhill/librdkafka/pull/3113,https://github.com/coignetp,3,https://github.com/edenhill/librdkafka/pull/3113#issuecomment-717263530,"Hi!
I'd like to add a configuration option named dogstatsd.endpoint. The goal here is to allow the lib to emit to a DogStatsD endpoint (some documentation about DogStatsD here https://docs.datadoghq.com/developers/dogstatsd/, https://docs.datadoghq.com/developers/dogstatsd/datagram_shell). When dogstatsd.endpoint is set, some of the statistics are emitted to this endpoint every statistics.interval.ms ms.
This option provides an alternative to parsing the json stats and help people understand what's happening in their consumers/producers.
I still have a few questions below, like what are the best librdkafka tools to parse the dogstatsd address.
Please tell me what you think about this change!","Hi,
Thank you for your detailed answer. I understand your concerns, and Im trying to understand how can we make an interesting plugin or add-on. Just a quick question, would adding the support for statsd instead of dogstatsd inside librdkafka be reasonable? (https://github.com/statsd/statsd)
Also, in order to create a plugin, should we go here https://www.confluent.io/hub/ so it could be in the confluent-community package? Is there a documentation to do that? And if we create a dogstatsd-interceptor plugin and someone wants to use it, they would need to set ""plugin.library.paths"" configuration to dogstatsd-interceptor, and the plugin would implement an on_metric_emit interceptor or something, right? I just want to be sure I understand the process here.
About the standard metrics interface,if I understand correctly, it is for all Kafka client to expose their metrics? (confluent-kafka-python and confluent-kafka-go for example). How would this affect librdkafka metrics interface? It may be interesting to wait for it to be posted before creating the plugin, thanks for the information!
Concerning the addition of a new interceptor, I agree that having an interceptor at each metric update should be avoided, and I believe it can be a good idea to have statistics.interval.ms as a common configuration for stuff related to metrics update.
Just to be sure, this interceptor on_metrics_emit would be called for each emitted metric, meaning it would get called ~100 times every statistics.interval.ms with a different metric name and value? What about sending them by batch (like a rd_avg_t value type), or even sending them all at once? What would be the cons of having an interceptor that gets called once every statistics.interval.ms with all the metrics? Something like:
void on_all_metrics_emit (rd_kafka_t *rk, const char **metrics_name, const rd_kafka_metric_value_t **values, const ssize_t size, void *ic_opaque)
where metrics_name is {""brokers.2.tx"", ""brokers.2.txbytes"", ""brokers.2.rx"", ""brokers.2.rxbytes""} and values as a table of what you described.
Thanks again for all the information you provided!",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,3114,2020-10-21T21:37:15Z,,2021-06-21T17:13:27Z,OPEN,False,24,2,4,https://github.com/Quuxplusone,Add rd_kafka_poll_wake() to wake up threads blocked in rd_kafka_poll().,2,[],https://github.com/edenhill/librdkafka/pull/3114,https://github.com/Quuxplusone,1,https://github.com/edenhill/librdkafka/pull/3114,"The idea is that if one thread is blocked in rd_kafka_poll(rk, -1),
another thread can do rd_kafka_poll_wake(rk) to wake it up so
that we can cleanly join the polling thread and cleanly exit.
Addresses #614.
Upstreaming on the hope that if you take it, we don't have to maintain it as a patch anymore.","The idea is that if one thread is blocked in rd_kafka_poll(rk, -1),
another thread can do rd_kafka_poll_wake(rk) to wake it up so
that we can cleanly join the polling thread and cleanly exit.
Addresses #614.
Upstreaming on the hope that if you take it, we don't have to maintain it as a patch anymore.",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,3114,2020-10-21T21:37:15Z,,2021-06-21T17:13:27Z,OPEN,False,24,2,4,https://github.com/Quuxplusone,Add rd_kafka_poll_wake() to wake up threads blocked in rd_kafka_poll().,2,[],https://github.com/edenhill/librdkafka/pull/3114,https://github.com/abbccdda,2,https://github.com/edenhill/librdkafka/pull/3114#issuecomment-865205744,"The idea is that if one thread is blocked in rd_kafka_poll(rk, -1),
another thread can do rd_kafka_poll_wake(rk) to wake it up so
that we can cleanly join the polling thread and cleanly exit.
Addresses #614.
Upstreaming on the hope that if you take it, we don't have to maintain it as a patch anymore.",Any update on this thread?,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,3116,2020-10-22T07:20:22Z,2020-10-22T07:44:54Z,2020-10-22T07:45:00Z,MERGED,True,1,1,1,https://github.com/amichelotti,stdarg.h included for va_start macro use,1,[],https://github.com/edenhill/librdkafka/pull/3116,https://github.com/amichelotti,1,https://github.com/edenhill/librdkafka/pull/3116,"stdarg.h must included explicitly for va_start and va_end use in old compilers.
Resolve issue:
https://github.com/edenhill/librdkafka/issues/3115","stdarg.h must included explicitly for va_start and va_end use in old compilers.
Resolve issue:
https://github.com/edenhill/librdkafka/issues/3115",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,3116,2020-10-22T07:20:22Z,2020-10-22T07:44:54Z,2020-10-22T07:45:00Z,MERGED,True,1,1,1,https://github.com/amichelotti,stdarg.h included for va_start macro use,1,[],https://github.com/edenhill/librdkafka/pull/3116,https://github.com/edenhill,2,https://github.com/edenhill/librdkafka/pull/3116#issuecomment-714300565,"stdarg.h must included explicitly for va_start and va_end use in old compilers.
Resolve issue:
https://github.com/edenhill/librdkafka/issues/3115","Oops, thank you!",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,3123,2020-10-27T04:12:46Z,2020-10-27T08:19:08Z,2020-10-27T08:19:08Z,MERGED,True,1,0,1,https://github.com/kenneth-jia,Add a new C++ client in README,1,[],https://github.com/edenhill/librdkafka/pull/3123,https://github.com/kenneth-jia,1,https://github.com/edenhill/librdkafka/pull/3123,"@edenhill Glad the c++ API is finally ready. And welcome to review and comment. 
https://github.com/Morgan-Stanley/modern-cpp-kafka","@edenhill Glad the c++ API is finally ready. And welcome to review and comment. 
https://github.com/Morgan-Stanley/modern-cpp-kafka",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,3124,2020-10-27T12:35:15Z,2020-11-02T15:31:23Z,2020-11-02T15:31:44Z,MERGED,True,10,1,1,https://github.com/shahidhs-ibm,Travis s390x support,1,[],https://github.com/edenhill/librdkafka/pull/3124,https://github.com/shahidhs-ibm,1,https://github.com/edenhill/librdkafka/pull/3124,"Travis CI officially supports s390x builds, adding support for same.","Travis CI officially supports s390x builds, adding support for same.",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,3124,2020-10-27T12:35:15Z,2020-11-02T15:31:23Z,2020-11-02T15:31:44Z,MERGED,True,10,1,1,https://github.com/shahidhs-ibm,Travis s390x support,1,[],https://github.com/edenhill/librdkafka/pull/3124,https://github.com/edenhill,2,https://github.com/edenhill/librdkafka/pull/3124#issuecomment-720544192,"Travis CI officially supports s390x builds, adding support for same.","Thank you!
I changed the commit message as not to imply that s390 is supported, which it is not.",True,{'HOORAY': ['https://github.com/shahidhs-ibm']}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,3126,2020-10-29T17:02:38Z,2020-11-05T09:09:33Z,2020-11-05T09:09:35Z,MERGED,True,96,43,12,https://github.com/edenhill,OpenSSL 1.1.1h and some other stuff,8,[],https://github.com/edenhill/librdkafka/pull/3126,https://github.com/edenhill,1,https://github.com/edenhill/librdkafka/pull/3126,,,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,3130,2020-11-02T13:45:46Z,2021-04-07T06:30:24Z,2021-11-05T12:46:21Z,MERGED,True,166,115,7,https://github.com/neptoess,Build static librdkafka_windows.a with travis,25,[],https://github.com/edenhill/librdkafka/pull/3130,https://github.com/neptoess,1,https://github.com/edenhill/librdkafka/pull/3130,"Add to the MinGW travis worker to have it build a librdkafka static bundle, per confluentinc/confluent-kafka-go#555 (comment)","Add to the MinGW travis worker to have it build a librdkafka static bundle, per confluentinc/confluent-kafka-go#555 (comment)",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,3130,2020-11-02T13:45:46Z,2021-04-07T06:30:24Z,2021-11-05T12:46:21Z,MERGED,True,166,115,7,https://github.com/neptoess,Build static librdkafka_windows.a with travis,25,[],https://github.com/edenhill/librdkafka/pull/3130,https://github.com/neptoess,2,https://github.com/edenhill/librdkafka/pull/3130#issuecomment-721217568,"Add to the MinGW travis worker to have it build a librdkafka static bundle, per confluentinc/confluent-kafka-go#555 (comment)","@edenhill
This should be ready to go. Let me know if it needs anything else.",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,3130,2020-11-02T13:45:46Z,2021-04-07T06:30:24Z,2021-11-05T12:46:21Z,MERGED,True,166,115,7,https://github.com/neptoess,Build static librdkafka_windows.a with travis,25,[],https://github.com/edenhill/librdkafka/pull/3130,https://github.com/neptoess,3,https://github.com/edenhill/librdkafka/pull/3130#issuecomment-721406399,"Add to the MinGW travis worker to have it build a librdkafka static bundle, per confluentinc/confluent-kafka-go#555 (comment)","Well, looks like one of the tests seg faulted in the latest Travis run, but the previous commit (1a589f6) ran that same test without issue, and all the latest commit changed was packaging.py",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,3130,2020-11-02T13:45:46Z,2021-04-07T06:30:24Z,2021-11-05T12:46:21Z,MERGED,True,166,115,7,https://github.com/neptoess,Build static librdkafka_windows.a with travis,25,[],https://github.com/edenhill/librdkafka/pull/3130,https://github.com/neptoess,4,https://github.com/edenhill/librdkafka/pull/3130#issuecomment-745537418,"Add to the MinGW travis worker to have it build a librdkafka static bundle, per confluentinc/confluent-kafka-go#555 (comment)",@edenhill Is there anything you need from me to help move this along?,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,3130,2020-11-02T13:45:46Z,2021-04-07T06:30:24Z,2021-11-05T12:46:21Z,MERGED,True,166,115,7,https://github.com/neptoess,Build static librdkafka_windows.a with travis,25,[],https://github.com/edenhill/librdkafka/pull/3130,https://github.com/neptoess,5,https://github.com/edenhill/librdkafka/pull/3130#issuecomment-769054614,"Add to the MinGW travis worker to have it build a librdkafka static bundle, per confluentinc/confluent-kafka-go#555 (comment)","@edenhill
Am I correct in thinking that the cmake changes needing review is the reason this hasn't been merged yet?
@masariello @omartijn @benesch @ed-alertedh @SpaceIm
I would greatly appreciate if you could look over this PR and provide any feedback",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,3130,2020-11-02T13:45:46Z,2021-04-07T06:30:24Z,2021-11-05T12:46:21Z,MERGED,True,166,115,7,https://github.com/neptoess,Build static librdkafka_windows.a with travis,25,[],https://github.com/edenhill/librdkafka/pull/3130,https://github.com/masariello,6,https://github.com/edenhill/librdkafka/pull/3130#issuecomment-769493064,"Add to the MinGW travis worker to have it build a librdkafka static bundle, per confluentinc/confluent-kafka-go#555 (comment)",Do mingw compilers have different ABIs for debug and optmized code builds? If so we should set CMAKE_DEBUG_POSTFIX and make sure that all the packaging logic flows with it.,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,3130,2020-11-02T13:45:46Z,2021-04-07T06:30:24Z,2021-11-05T12:46:21Z,MERGED,True,166,115,7,https://github.com/neptoess,Build static librdkafka_windows.a with travis,25,[],https://github.com/edenhill/librdkafka/pull/3130,https://github.com/neptoess,7,https://github.com/edenhill/librdkafka/pull/3130#issuecomment-769496652,"Add to the MinGW travis worker to have it build a librdkafka static bundle, per confluentinc/confluent-kafka-go#555 (comment)","@masariello
I will investigate the ABIs of debug vs optimized builds.
As for having the logic in packaging follow, are you saying we should distribute both release and debug binaries of librdkafka?",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,3130,2020-11-02T13:45:46Z,2021-04-07T06:30:24Z,2021-11-05T12:46:21Z,MERGED,True,166,115,7,https://github.com/neptoess,Build static librdkafka_windows.a with travis,25,[],https://github.com/edenhill/librdkafka/pull/3130,https://github.com/masariello,8,https://github.com/edenhill/librdkafka/pull/3130#issuecomment-769504400,"Add to the MinGW travis worker to have it build a librdkafka static bundle, per confluentinc/confluent-kafka-go#555 (comment)","Where ABIs are different users do need both debug and optimized builds. So in that case they need to be distributed together, yes
MSVC does have differing ABIs, for ex.
Because of this kind of issues I've seen projects providing the cpp APIs as hpp only. Basically the lib is shipped as core C libs that are not affected by ABI issues + a bunch of hpp files",True,{'THUMBS_UP': ['https://github.com/neptoess']}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,3130,2020-11-02T13:45:46Z,2021-04-07T06:30:24Z,2021-11-05T12:46:21Z,MERGED,True,166,115,7,https://github.com/neptoess,Build static librdkafka_windows.a with travis,25,[],https://github.com/edenhill/librdkafka/pull/3130,https://github.com/neptoess,9,https://github.com/edenhill/librdkafka/pull/3130#issuecomment-770267558,"Add to the MinGW travis worker to have it build a librdkafka static bundle, per confluentinc/confluent-kafka-go#555 (comment)","@masariello
I can find little to no good information regarding this (ABI compatibility issues between object files built with/without optimization) with regard to mingw-w64 gcc / g++
Is there some type of test we can do to verify this? I would imagine any issues could be detected during linking, right?",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,3130,2020-11-02T13:45:46Z,2021-04-07T06:30:24Z,2021-11-05T12:46:21Z,MERGED,True,166,115,7,https://github.com/neptoess,Build static librdkafka_windows.a with travis,25,[],https://github.com/edenhill/librdkafka/pull/3130,https://github.com/neptoess,10,https://github.com/edenhill/librdkafka/pull/3130#issuecomment-776788065,"Add to the MinGW travis worker to have it build a librdkafka static bundle, per confluentinc/confluent-kafka-go#555 (comment)","@masariello
Upon further digging through the GCC docs, it looks like this ABI concern is only limited to C++ binaries. Since this PR is really just focused on getting a MinGW built static C library (to distribute with confluent-kafka-go), and I don't think @edenhill has any near term plans to drop vcpkg or NuGet distribution of Windows binaries for librdkafka, I think it's safe to say it shouldn't block this PR from being merged.
Agree?",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,3130,2020-11-02T13:45:46Z,2021-04-07T06:30:24Z,2021-11-05T12:46:21Z,MERGED,True,166,115,7,https://github.com/neptoess,Build static librdkafka_windows.a with travis,25,[],https://github.com/edenhill/librdkafka/pull/3130,https://github.com/masariello,11,https://github.com/edenhill/librdkafka/pull/3130#issuecomment-776885388,"Add to the MinGW travis worker to have it build a librdkafka static bundle, per confluentinc/confluent-kafka-go#555 (comment)","@masariello
Upon further digging through the GCC docs, it looks like this ABI concern is only limited to C++ binaries. Since this PR is really just focused on getting a MinGW built static C library (to distribute with confluent-kafka-go), and I don't think @edenhill has any near term plans to drop vcpkg or NuGet distribution of Windows binaries for librdkafka, I think it's safe to say it shouldn't block this PR from being merged.
Agree?

Yep. Agreed",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,3130,2020-11-02T13:45:46Z,2021-04-07T06:30:24Z,2021-11-05T12:46:21Z,MERGED,True,166,115,7,https://github.com/neptoess,Build static librdkafka_windows.a with travis,25,[],https://github.com/edenhill/librdkafka/pull/3130,https://github.com/neptoess,12,https://github.com/edenhill/librdkafka/pull/3130#issuecomment-813390867,"Add to the MinGW travis worker to have it build a librdkafka static bundle, per confluentinc/confluent-kafka-go#555 (comment)","@edenhill ,
I do hate to keep bothering you with this PR, but is there anything I can do to help move this along? There was another comment in confluentinc/confluent-kafka-go#128 from someone struggling to use confluent-kafka-go on Windows, so I think there's still merit in getting this merged.",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,3130,2020-11-02T13:45:46Z,2021-04-07T06:30:24Z,2021-11-05T12:46:21Z,MERGED,True,166,115,7,https://github.com/neptoess,Build static librdkafka_windows.a with travis,25,[],https://github.com/edenhill/librdkafka/pull/3130,https://github.com/edenhill,13,https://github.com/edenhill/librdkafka/pull/3130#issuecomment-814137085,"Add to the MinGW travis worker to have it build a librdkafka static bundle, per confluentinc/confluent-kafka-go#555 (comment)","Oh, please also make sure the libraries are stripped (to reduce size).",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,3130,2020-11-02T13:45:46Z,2021-04-07T06:30:24Z,2021-11-05T12:46:21Z,MERGED,True,166,115,7,https://github.com/neptoess,Build static librdkafka_windows.a with travis,25,[],https://github.com/edenhill/librdkafka/pull/3130,https://github.com/neptoess,14,https://github.com/edenhill/librdkafka/pull/3130#issuecomment-814176116,"Add to the MinGW travis worker to have it build a librdkafka static bundle, per confluentinc/confluent-kafka-go#555 (comment)","@edenhill

Oh, please also make sure the libraries are stripped (to reduce size).

Not 100% positive on how to verify that, but I don't see any obvious debug symbols in the final static library.
$ objdump --syms librdkafka_windows.a | grep debug
[249](sec  1)(fl 0x00)(ty  20)(scl   2) (nx 0) 0x00000000000082d0 SSL_set_debug
[  9](sec  1)(fl 0x00)(ty  20)(scl   2) (nx 0) 0x0000000000000190 CRYPTO_set_mem_debug
[  0](sec  1)(fl 0x00)(ty  20)(scl   2) (nx 1) 0x0000000000000000 BIO_debug_callback
debug.c.obj:     file format pe-x86-64
[  8](sec  3)(fl 0x00)(ty   0)(scl   2) (nx 0) 0x0000000000000000 g_debuglevel
[ 16](sec  1)(fl 0x00)(ty  20)(scl   2) (nx 0) 0x0000000000000540 rd_kafka_event_debug_contexts
[155](sec  1)(fl 0x00)(ty  20)(scl   2) (nx 0) 0x0000000000015af0 rd_kafka_get_debug_contexts

EDIT:
File size also appears inline with the existing bundled libs
$ ls -lh *.a
-rw-r--r-- 1 neptoess None 6.7M Jan 28 08:33 librdkafka_darwin.a
-rw-r--r-- 1 neptoess None 8.7M Jan 28 08:33 librdkafka_glibc_linux.a
-rw-r--r-- 1 neptoess None 8.8M Jan 28 08:33 librdkafka_musl_linux.a
-rw-r--r-- 1 neptoess None 8.6M Feb 10 08:45 librdkafka_windows.a",True,{'THUMBS_UP': ['https://github.com/edenhill']}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,3130,2020-11-02T13:45:46Z,2021-04-07T06:30:24Z,2021-11-05T12:46:21Z,MERGED,True,166,115,7,https://github.com/neptoess,Build static librdkafka_windows.a with travis,25,[],https://github.com/edenhill/librdkafka/pull/3130,https://github.com/edenhill,15,https://github.com/edenhill/librdkafka/pull/3130#issuecomment-814221537,"Add to the MinGW travis worker to have it build a librdkafka static bundle, per confluentinc/confluent-kafka-go#555 (comment)","You could jab a:
ls -la *.a
strip -g *.a
ls -la *a

in the build script (after ranlib) and see if there's any difference",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,3130,2020-11-02T13:45:46Z,2021-04-07T06:30:24Z,2021-11-05T12:46:21Z,MERGED,True,166,115,7,https://github.com/neptoess,Build static librdkafka_windows.a with travis,25,[],https://github.com/edenhill/librdkafka/pull/3130,https://github.com/neptoess,16,https://github.com/edenhill/librdkafka/pull/3130#issuecomment-814239334,"Add to the MinGW travis worker to have it build a librdkafka static bundle, per confluentinc/confluent-kafka-go#555 (comment)","You could jab a:
ls -la *.a
strip -g *.a
ls -la *a

in the build script (after ranlib) and see if there's any difference

No difference. I guess that makes sense, since we're not compiling with debug symbols enabled. Should I throw the strip command in the build script anyway?",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,3130,2020-11-02T13:45:46Z,2021-04-07T06:30:24Z,2021-11-05T12:46:21Z,MERGED,True,166,115,7,https://github.com/neptoess,Build static librdkafka_windows.a with travis,25,[],https://github.com/edenhill/librdkafka/pull/3130,https://github.com/edenhill,17,https://github.com/edenhill/librdkafka/pull/3130#issuecomment-814246221,"Add to the MinGW travis worker to have it build a librdkafka static bundle, per confluentinc/confluent-kafka-go#555 (comment)","Yeah. Please do, can't hurt.",True,{'THUMBS_UP': ['https://github.com/neptoess']}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,3130,2020-11-02T13:45:46Z,2021-04-07T06:30:24Z,2021-11-05T12:46:21Z,MERGED,True,166,115,7,https://github.com/neptoess,Build static librdkafka_windows.a with travis,25,[],https://github.com/edenhill/librdkafka/pull/3130,https://github.com/neptoess,18,https://github.com/edenhill/librdkafka/pull/3130#issuecomment-814301725,"Add to the MinGW travis worker to have it build a librdkafka static bundle, per confluentinc/confluent-kafka-go#555 (comment)",strip -g calls added,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,3130,2020-11-02T13:45:46Z,2021-04-07T06:30:24Z,2021-11-05T12:46:21Z,MERGED,True,166,115,7,https://github.com/neptoess,Build static librdkafka_windows.a with travis,25,[],https://github.com/edenhill/librdkafka/pull/3130,https://github.com/edenhill,19,https://github.com/edenhill/librdkafka/pull/3130#issuecomment-814353551,"Add to the MinGW travis worker to have it build a librdkafka static bundle, per confluentinc/confluent-kafka-go#555 (comment)",Great! Is this ready to be merged?,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,3130,2020-11-02T13:45:46Z,2021-04-07T06:30:24Z,2021-11-05T12:46:21Z,MERGED,True,166,115,7,https://github.com/neptoess,Build static librdkafka_windows.a with travis,25,[],https://github.com/edenhill/librdkafka/pull/3130,https://github.com/neptoess,20,https://github.com/edenhill/librdkafka/pull/3130#issuecomment-814458570,"Add to the MinGW travis worker to have it build a librdkafka static bundle, per confluentinc/confluent-kafka-go#555 (comment)","@edenhill
Ready to merge",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,3130,2020-11-02T13:45:46Z,2021-04-07T06:30:24Z,2021-11-05T12:46:21Z,MERGED,True,166,115,7,https://github.com/neptoess,Build static librdkafka_windows.a with travis,25,[],https://github.com/edenhill/librdkafka/pull/3130,https://github.com/edenhill,21,https://github.com/edenhill/librdkafka/pull/3130#issuecomment-814641920,"Add to the MinGW travis worker to have it build a librdkafka static bundle, per confluentinc/confluent-kafka-go#555 (comment)","Superb! Great work (and perseverence) on this, @neptoess !",True,{'HOORAY': ['https://github.com/neptoess']}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,3130,2020-11-02T13:45:46Z,2021-04-07T06:30:24Z,2021-11-05T12:46:21Z,MERGED,True,166,115,7,https://github.com/neptoess,Build static librdkafka_windows.a with travis,25,[],https://github.com/edenhill/librdkafka/pull/3130,https://github.com/edenhill,22,https://github.com/edenhill/librdkafka/pull/3130#issuecomment-958093612,"Add to the MinGW travis worker to have it build a librdkafka static bundle, per confluentinc/confluent-kafka-go#555 (comment)","Hey @neptoess , can you help me figure out why the Travis-CI mingw jobs are silently failing?
I've added some debugging and it seems like the test-runner.exe fails to execute, but I don't see an error anywhere (stderr and stdout seems to be weirdly mixed up).
Can you try to reproduce this issue locally?
https://app.travis-ci.com/github/edenhill/librdkafka/jobs/546274490#L1046",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,3130,2020-11-02T13:45:46Z,2021-04-07T06:30:24Z,2021-11-05T12:46:21Z,MERGED,True,166,115,7,https://github.com/neptoess,Build static librdkafka_windows.a with travis,25,[],https://github.com/edenhill/librdkafka/pull/3130,https://github.com/edbordin,23,https://github.com/edenhill/librdkafka/pull/3130#issuecomment-958689615,"Add to the MinGW travis worker to have it build a librdkafka static bundle, per confluentinc/confluent-kafka-go#555 (comment)","I had issues with GCC LTO being broken on MinGW/MSYS at some point in the past year - the resulting executables would just segfault. Is there any chance this is being built with -flto or related flags?
edit: found the issue tracking it msys2/MINGW-packages#8074",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,3130,2020-11-02T13:45:46Z,2021-04-07T06:30:24Z,2021-11-05T12:46:21Z,MERGED,True,166,115,7,https://github.com/neptoess,Build static librdkafka_windows.a with travis,25,[],https://github.com/edenhill/librdkafka/pull/3130,https://github.com/edenhill,24,https://github.com/edenhill/librdkafka/pull/3130#issuecomment-958929212,"Add to the MinGW travis worker to have it build a librdkafka static bundle, per confluentinc/confluent-kafka-go#555 (comment)","Thanks @edbordin, there doesn't seem to be an -flto flag in the make output though:
https://app.travis-ci.com/github/edenhill/librdkafka/jobs/546447298",True,{'THUMBS_UP': ['https://github.com/edbordin']}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,3130,2020-11-02T13:45:46Z,2021-04-07T06:30:24Z,2021-11-05T12:46:21Z,MERGED,True,166,115,7,https://github.com/neptoess,Build static librdkafka_windows.a with travis,25,[],https://github.com/edenhill/librdkafka/pull/3130,https://github.com/neptoess,25,https://github.com/edenhill/librdkafka/pull/3130#issuecomment-958939208,"Add to the MinGW travis worker to have it build a librdkafka static bundle, per confluentinc/confluent-kafka-go#555 (comment)","Looking into this. I'm thinking this has to do with MSYS2 updating the MinGW compiler to GCC 11
https://packages.msys2.org/package/mingw-w64-x86_64-gcc
This happened 2021/10/21. When I did pacman -Syu on my existing MSYS2 install, it did not upgrade beyond GCC 10.3. Uninstalling and reinstalling MSYS2, and copying the pacman commands from the Travis script left me with 11.2. Will do some poking around, but I'm guessing there are some libraries being linked into our build that were built with GCC 10.",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,3130,2020-11-02T13:45:46Z,2021-04-07T06:30:24Z,2021-11-05T12:46:21Z,MERGED,True,166,115,7,https://github.com/neptoess,Build static librdkafka_windows.a with travis,25,[],https://github.com/edenhill/librdkafka/pull/3130,https://github.com/neptoess,26,https://github.com/edenhill/librdkafka/pull/3130#issuecomment-958952404,"Add to the MinGW travis worker to have it build a librdkafka static bundle, per confluentinc/confluent-kafka-go#555 (comment)","I did a fresh MSYS2 install, fresh git pull, and built the tstwinb branch locally. All tests passed. Going to focus on Travis now, since I think the code (both library and build scripts) is fine.
test-run.log",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,3130,2020-11-02T13:45:46Z,2021-04-07T06:30:24Z,2021-11-05T12:46:21Z,MERGED,True,166,115,7,https://github.com/neptoess,Build static librdkafka_windows.a with travis,25,[],https://github.com/edenhill/librdkafka/pull/3130,https://github.com/edenhill,27,https://github.com/edenhill/librdkafka/pull/3130#issuecomment-958957187,"Add to the MinGW travis worker to have it build a librdkafka static bundle, per confluentinc/confluent-kafka-go#555 (comment)","Thanks Bill!
Going forward, is it possible to version pin the toolchain and dependencies on the mingw builder to avoid this in the future?",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,3130,2020-11-02T13:45:46Z,2021-04-07T06:30:24Z,2021-11-05T12:46:21Z,MERGED,True,166,115,7,https://github.com/neptoess,Build static librdkafka_windows.a with travis,25,[],https://github.com/edenhill/librdkafka/pull/3130,https://github.com/neptoess,28,https://github.com/edenhill/librdkafka/pull/3130#issuecomment-958978784,"Add to the MinGW travis worker to have it build a librdkafka static bundle, per confluentinc/confluent-kafka-go#555 (comment)","Well, because my local just worked (with a completely fresh MSYS2 install), I'm not confident this is a package version issue. I was going to try https://github.com/neptoess/librdkafka/commit/3ddeb537bdb83fd53e84afc3acd37adff7e76583 (I should have added a --no-confirm so it doesn't hang) to see if pacman reports any packages needing updated after the choco upgrade command in the build script, but, due to the Travis pricing changes, I can't do any Travis builds at the moment (I have to email them for the usage based plan).
Would it be possible to manually trigger a build on a previous commit that worked?",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,3130,2020-11-02T13:45:46Z,2021-04-07T06:30:24Z,2021-11-05T12:46:21Z,MERGED,True,166,115,7,https://github.com/neptoess,Build static librdkafka_windows.a with travis,25,[],https://github.com/edenhill/librdkafka/pull/3130,https://github.com/edenhill,29,https://github.com/edenhill/librdkafka/pull/3130#issuecomment-958988874,"Add to the MinGW travis worker to have it build a librdkafka static bundle, per confluentinc/confluent-kafka-go#555 (comment)","Yeah, Travis costs are a sudden and steep barrier.
I think you can use my credits though if you open a PR and comment out all the other workers from .travis.yaml so you just have the single mingw one.",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,3130,2020-11-02T13:45:46Z,2021-04-07T06:30:24Z,2021-11-05T12:46:21Z,MERGED,True,166,115,7,https://github.com/neptoess,Build static librdkafka_windows.a with travis,25,[],https://github.com/edenhill/librdkafka/pull/3130,https://github.com/neptoess,30,https://github.com/edenhill/librdkafka/pull/3130#issuecomment-959000792,"Add to the MinGW travis worker to have it build a librdkafka static bundle, per confluentinc/confluent-kafka-go#555 (comment)",Good idea. Watching it now https://app.travis-ci.com/github/edenhill/librdkafka/builds/241104613,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,3130,2020-11-02T13:45:46Z,2021-04-07T06:30:24Z,2021-11-05T12:46:21Z,MERGED,True,166,115,7,https://github.com/neptoess,Build static librdkafka_windows.a with travis,25,[],https://github.com/edenhill/librdkafka/pull/3130,https://github.com/neptoess,31,https://github.com/edenhill/librdkafka/pull/3130#issuecomment-959794663,"Add to the MinGW travis worker to have it build a librdkafka static bundle, per confluentinc/confluent-kafka-go#555 (comment)","https://app.travis-ci.com/github/edenhill/librdkafka/builds/241106018
Even rebuilding v1.8.2 failed. Going to be tricky to debug, since a fresh install of MSYS2 on Win10 works fine.",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,3130,2020-11-02T13:45:46Z,2021-04-07T06:30:24Z,2021-11-05T12:46:21Z,MERGED,True,166,115,7,https://github.com/neptoess,Build static librdkafka_windows.a with travis,25,[],https://github.com/edenhill/librdkafka/pull/3130,https://github.com/edenhill,32,https://github.com/edenhill/librdkafka/pull/3130#issuecomment-959806976,"Add to the MinGW travis worker to have it build a librdkafka static bundle, per confluentinc/confluent-kafka-go#555 (comment)","These are the mingw toolchain version diffs between the proper 1.8.2  build and your build:
--- 543777511s  2021-11-03 19:25:32.823602012 +0100
+++ 241106018s  2021-11-03 19:24:25.243884678 +0100
@@ -3,15 +3,15 @@
  mingw-w64-x86_64-bzip2-1.0.8-2-any downloading...
  mingw-w64-x86_64-ca-certificates-20210119-1-any downloading...
  mingw-w64-x86_64-c-ares-1.17.2-1-any downloading...
- mingw-w64-x86_64-cmake-3.21.3-1-any downloading...
- mingw-w64-x86_64-crt-git-9.0.0.6316.acdc7adc9-1-any downloading...
+ mingw-w64-x86_64-cmake-3.21.4-1-any downloading...
+ mingw-w64-x86_64-crt-git-9.0.0.6327.f29c1101f-1-any downloading...
  mingw-w64-x86_64-curl-7.79.1-1-any downloading...
  mingw-w64-x86_64-expat-2.4.1-1-any downloading...
-mingw-w64-x86_64-gcc-10.3.0-8-any downloading...
- mingw-w64-x86_64-gcc-libs-10.3.0-8-any downloading...
+ mingw-w64-x86_64-gcc-11.2.0-1-any downloading...
+ mingw-w64-x86_64-gcc-libs-11.2.0-1-any downloading...
  mingw-w64-x86_64-gettext-0.19.8.1-10-any downloading...
  mingw-w64-x86_64-gmp-6.2.1-2-any downloading...
- mingw-w64-x86_64-headers-git-9.0.0.6316.acdc7adc9-1-any downloading...
+ mingw-w64-x86_64-headers-git-9.0.0.6327.f29c1101f-1-any downloading...
  mingw-w64-x86_64-isl-0.24-1-any downloading...
  mingw-w64-x86_64-jansson-2.14-1-any downloading...
  mingw-w64-x86_64-jemalloc-5.2.1-2-any downloading...
@@ -26,8 +26,8 @@
  mingw-w64-x86_64-libtasn1-4.17.0-1-any downloading...
  mingw-w64-x86_64-libtre-git-r128.6fb7206-2-any downloading...
  mingw-w64-x86_64-libunistring-0.9.10-4-any downloading...
- mingw-w64-x86_64-libuv-1.42.0-2-any downloading...
- mingw-w64-x86_64-libwinpthread-git-9.0.0.6316.acdc7adc9-1-any downloading...
+ mingw-w64-x86_64-libuv-1.42.0-3-any downloading...
+ mingw-w64-x86_64-libwinpthread-git-9.0.0.6327.f29c1101f-1-any downloading...
  mingw-w64-x86_64-libxml2-2.9.12-3-any downloading...
  mingw-w64-x86_64-lz4-1.9.3-1-any downloading...
  mingw-w64-x86_64-make-4.3-1-any downloading...
@@ -40,7 +40,7 @@
  mingw-w64-x86_64-pkgconf-1.8.0-2-any downloading...
  mingw-w64-x86_64-rhash-1.4.2-1-any downloading...
  mingw-w64-x86_64-windows-default-manifest-6.4-3-any downloading...
- mingw-w64-x86_64-winpthreads-git-9.0.0.6316.acdc7adc9-1-any downloading...
+ mingw-w64-x86_64-winpthreads-git-9.0.0.6327.f29c1101f-1-any downloading...
  mingw-w64-x86_64-xz-5.2.5-2-any downloading...
  mingw-w64-x86_64-zlib-1.2.11-9-any downloading...
  mingw-w64-x86_64-zstd-1.5.0-1-any downloading...",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,3130,2020-11-02T13:45:46Z,2021-04-07T06:30:24Z,2021-11-05T12:46:21Z,MERGED,True,166,115,7,https://github.com/neptoess,Build static librdkafka_windows.a with travis,25,[],https://github.com/edenhill/librdkafka/pull/3130,https://github.com/neptoess,33,https://github.com/edenhill/librdkafka/pull/3130#issuecomment-960963821,"Add to the MinGW travis worker to have it build a librdkafka static bundle, per confluentinc/confluent-kafka-go#555 (comment)","Hmm. These package versions all exactly match what I have locally. Is there any way to pull the test-runner.exe artifact out of Travis? I'd like to try running it locally, to rule out anything funny going on between Travis's bash shell and the new C runtime.",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,3130,2020-11-02T13:45:46Z,2021-04-07T06:30:24Z,2021-11-05T12:46:21Z,MERGED,True,166,115,7,https://github.com/neptoess,Build static librdkafka_windows.a with travis,25,[],https://github.com/edenhill/librdkafka/pull/3130,https://github.com/edenhill,34,https://github.com/edenhill/librdkafka/pull/3130#issuecomment-961364401,"Add to the MinGW travis worker to have it build a librdkafka static bundle, per confluentinc/confluent-kafka-go#555 (comment)","Not easily, no.
That shell invocation is a bit messed up output-wise, so maybe we're not seeing an error message for that reason.
Can we try to execute test-runner.exe in a later step and separate shell in travis?",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,3130,2020-11-02T13:45:46Z,2021-04-07T06:30:24Z,2021-11-05T12:46:21Z,MERGED,True,166,115,7,https://github.com/neptoess,Build static librdkafka_windows.a with travis,25,[],https://github.com/edenhill/librdkafka/pull/3130,https://github.com/neptoess,35,https://github.com/edenhill/librdkafka/pull/3130#issuecomment-961376292,"Add to the MinGW travis worker to have it build a librdkafka static bundle, per confluentinc/confluent-kafka-go#555 (comment)","Well, that's a little confusing. Breaking tests out into a separate step worked.
https://app.travis-ci.com/github/edenhill/librdkafka/jobs/546739494",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,3130,2020-11-02T13:45:46Z,2021-04-07T06:30:24Z,2021-11-05T12:46:21Z,MERGED,True,166,115,7,https://github.com/neptoess,Build static librdkafka_windows.a with travis,25,[],https://github.com/edenhill/librdkafka/pull/3130,https://github.com/edenhill,36,https://github.com/edenhill/librdkafka/pull/3130#issuecomment-961388052,"Add to the MinGW travis worker to have it build a librdkafka static bundle, per confluentinc/confluent-kafka-go#555 (comment)","That's good news!
Your new run-tests.sh script does not have the PATH update of the original script, maybe that's what messed up the original?
I have a vague memory that PATH also affects library loading on Windows, not sure if that is true for mingw though.",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,3130,2020-11-02T13:45:46Z,2021-04-07T06:30:24Z,2021-11-05T12:46:21Z,MERGED,True,166,115,7,https://github.com/neptoess,Build static librdkafka_windows.a with travis,25,[],https://github.com/edenhill/librdkafka/pull/3130,https://github.com/neptoess,37,https://github.com/edenhill/librdkafka/pull/3130#issuecomment-961404649,"Add to the MinGW travis worker to have it build a librdkafka static bundle, per confluentinc/confluent-kafka-go#555 (comment)","%PATH% does affect library loading on Windows, but I believe %PATH% is only checked if the DLL isn't found in the working directory first. This is why it's so common to see people modify, e.g. games, by making proxy DLLs and copying them to the bin folder.
Also, looks like https://app.travis-ci.com/github/edenhill/librdkafka/builds/241213359 worked too, so we don't even need to use a different stage in Travis. I'm guessing just tearing down and making a new shell is the fix.",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,3130,2020-11-02T13:45:46Z,2021-04-07T06:30:24Z,2021-11-05T12:46:21Z,MERGED,True,166,115,7,https://github.com/neptoess,Build static librdkafka_windows.a with travis,25,[],https://github.com/edenhill/librdkafka/pull/3130,https://github.com/neptoess,38,https://github.com/edenhill/librdkafka/pull/3130#issuecomment-961455854,"Add to the MinGW travis worker to have it build a librdkafka static bundle, per confluentinc/confluent-kafka-go#555 (comment)",Opened #3607 to get the fix merged into master. Let me know if you want me to take a different approach,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,3130,2020-11-02T13:45:46Z,2021-04-07T06:30:24Z,2021-11-05T12:46:21Z,MERGED,True,166,115,7,https://github.com/neptoess,Build static librdkafka_windows.a with travis,25,[],https://github.com/edenhill/librdkafka/pull/3130,https://github.com/edenhill,39,https://github.com/edenhill/librdkafka/pull/3130#issuecomment-961724482,"Add to the MinGW travis worker to have it build a librdkafka static bundle, per confluentinc/confluent-kafka-go#555 (comment)","Thanks for so resolutely taking on this issue, @neptoess, much appreciated!",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,3130,2020-11-02T13:45:46Z,2021-04-07T06:30:24Z,2021-11-05T12:46:21Z,MERGED,True,166,115,7,https://github.com/neptoess,Build static librdkafka_windows.a with travis,25,[],https://github.com/edenhill/librdkafka/pull/3130,https://github.com/neptoess,40,https://github.com/edenhill/librdkafka/pull/3130#issuecomment-961867521,"Add to the MinGW travis worker to have it build a librdkafka static bundle, per confluentinc/confluent-kafka-go#555 (comment)",No problem. Happy to help,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,3131,2020-11-02T19:23:21Z,2020-11-05T09:51:44Z,2020-11-09T11:12:10Z,MERGED,True,150,18,7,https://github.com/edenhill,Add `ssl.ca.certificate.stores`,1,[],https://github.com/edenhill/librdkafka/pull/3131,https://github.com/edenhill,1,https://github.com/edenhill/librdkafka/pull/3131,,,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,3135,2020-11-05T13:06:07Z,2020-11-06T09:11:53Z,2020-11-06T09:11:53Z,MERGED,True,55,12,2,https://github.com/gridaphobe,main thread should keep polling while cgrp is alive (#3127),1,[],https://github.com/edenhill/librdkafka/pull/3135,https://github.com/gridaphobe,1,https://github.com/edenhill/librdkafka/pull/3135,"During termination, the main thread's loop continues as long
as there are ops left to poll, but is also responsible for
terminating the cgrp via rd_kafka_cgrp_serve. It is possible
for the loop to exit before the cgrp is terminated if you
unsubscribe and then immediately destroy the rdkafka handle,
which leaves some stray refcnts and hangs.
The solution is to keep the polling loop going as long as
there are remaining ops OR until the cgrp is terminated.
I also discovered a data race while investigating this issue.
TSAN should occasionally catch the race while running the
updated test 0116, but unfortunately it's pretty non-deterministic..
fixes #3127","During termination, the main thread's loop continues as long
as there are ops left to poll, but is also responsible for
terminating the cgrp via rd_kafka_cgrp_serve. It is possible
for the loop to exit before the cgrp is terminated if you
unsubscribe and then immediately destroy the rdkafka handle,
which leaves some stray refcnts and hangs.
The solution is to keep the polling loop going as long as
there are remaining ops OR until the cgrp is terminated.
I also discovered a data race while investigating this issue.
TSAN should occasionally catch the race while running the
updated test 0116, but unfortunately it's pretty non-deterministic..
fixes #3127",True,"{'HEART': ['https://github.com/mhowlett', 'https://github.com/mlongob', 'https://github.com/edenhill', 'https://github.com/amotl']}"
edenhill/librdkafka,https://github.com/edenhill/librdkafka,3135,2020-11-05T13:06:07Z,2020-11-06T09:11:53Z,2020-11-06T09:11:53Z,MERGED,True,55,12,2,https://github.com/gridaphobe,main thread should keep polling while cgrp is alive (#3127),1,[],https://github.com/edenhill/librdkafka/pull/3135,https://github.com/edenhill,2,https://github.com/edenhill/librdkafka/pull/3135#issuecomment-722371402,"During termination, the main thread's loop continues as long
as there are ops left to poll, but is also responsible for
terminating the cgrp via rd_kafka_cgrp_serve. It is possible
for the loop to exit before the cgrp is terminated if you
unsubscribe and then immediately destroy the rdkafka handle,
which leaves some stray refcnts and hangs.
The solution is to keep the polling loop going as long as
there are remaining ops OR until the cgrp is terminated.
I also discovered a data race while investigating this issue.
TSAN should occasionally catch the race while running the
updated test 0116, but unfortunately it's pretty non-deterministic..
fixes #3127","Eric, you're the best!",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,3135,2020-11-05T13:06:07Z,2020-11-06T09:11:53Z,2020-11-06T09:11:53Z,MERGED,True,55,12,2,https://github.com/gridaphobe,main thread should keep polling while cgrp is alive (#3127),1,[],https://github.com/edenhill/librdkafka/pull/3135,https://github.com/edenhill,3,https://github.com/edenhill/librdkafka/pull/3135#issuecomment-722380332,"During termination, the main thread's loop continues as long
as there are ops left to poll, but is also responsible for
terminating the cgrp via rd_kafka_cgrp_serve. It is possible
for the loop to exit before the cgrp is terminated if you
unsubscribe and then immediately destroy the rdkafka handle,
which leaves some stray refcnts and hangs.
The solution is to keep the polling loop going as long as
there are remaining ops OR until the cgrp is terminated.
I also discovered a data race while investigating this issue.
TSAN should occasionally catch the race while running the
updated test 0116, but unfortunately it's pretty non-deterministic..
fixes #3127","I think you can back out the rktp_last_error changes, I fixed that on master (those locations dont need to reset the last error at all).",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,3135,2020-11-05T13:06:07Z,2020-11-06T09:11:53Z,2020-11-06T09:11:53Z,MERGED,True,55,12,2,https://github.com/gridaphobe,main thread should keep polling while cgrp is alive (#3127),1,[],https://github.com/edenhill/librdkafka/pull/3135,https://github.com/edenhill,4,https://github.com/edenhill/librdkafka/pull/3135#issuecomment-722380461,"During termination, the main thread's loop continues as long
as there are ops left to poll, but is also responsible for
terminating the cgrp via rd_kafka_cgrp_serve. It is possible
for the loop to exit before the cgrp is terminated if you
unsubscribe and then immediately destroy the rdkafka handle,
which leaves some stray refcnts and hangs.
The solution is to keep the polling loop going as long as
there are remaining ops OR until the cgrp is terminated.
I also discovered a data race while investigating this issue.
TSAN should occasionally catch the race while running the
updated test 0116, but unfortunately it's pretty non-deterministic..
fixes #3127","Also,  for test-case!",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,3135,2020-11-05T13:06:07Z,2020-11-06T09:11:53Z,2020-11-06T09:11:53Z,MERGED,True,55,12,2,https://github.com/gridaphobe,main thread should keep polling while cgrp is alive (#3127),1,[],https://github.com/edenhill/librdkafka/pull/3135,https://github.com/gridaphobe,5,https://github.com/edenhill/librdkafka/pull/3135#issuecomment-722415909,"During termination, the main thread's loop continues as long
as there are ops left to poll, but is also responsible for
terminating the cgrp via rd_kafka_cgrp_serve. It is possible
for the loop to exit before the cgrp is terminated if you
unsubscribe and then immediately destroy the rdkafka handle,
which leaves some stray refcnts and hangs.
The solution is to keep the polling loop going as long as
there are remaining ops OR until the cgrp is terminated.
I also discovered a data race while investigating this issue.
TSAN should occasionally catch the race while running the
updated test 0116, but unfortunately it's pretty non-deterministic..
fixes #3127","I think you can back out the rktp_last_error changes, I fixed that on master (those locations dont need to reset the last error at all).

Awesome, done!",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,3137,2020-11-06T14:42:26Z,2020-11-07T10:51:13Z,2020-11-07T10:51:25Z,MERGED,True,93,7,5,https://github.com/edenhill,AK 2.7.0(rc1) changes,4,[],https://github.com/edenhill/librdkafka/pull/3137,https://github.com/edenhill,1,https://github.com/edenhill/librdkafka/pull/3137,,,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,3138,2020-11-09T07:59:41Z,2020-11-09T13:22:27Z,2020-11-09T13:22:29Z,MERGED,True,27,25,2,https://github.com/edenhill,Minor style fixes for sticky partitioning,1,[],https://github.com/edenhill/librdkafka/pull/3138,https://github.com/edenhill,1,https://github.com/edenhill/librdkafka/pull/3138,Removing red stuff from emacs,Removing red stuff from emacs,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,3144,2020-11-13T11:05:25Z,2020-11-16T19:28:59Z,2020-11-16T19:29:02Z,MERGED,True,761,300,28,https://github.com/edenhill,"OpenSSL upgrade, CI fixes, test stabilization, and more!",33,[],https://github.com/edenhill/librdkafka/pull/3144,https://github.com/edenhill,1,https://github.com/edenhill/librdkafka/pull/3144,0113 n_wildcard is still flaky.,0113 n_wildcard is still flaky.,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,3148,2020-11-16T09:04:38Z,2020-12-08T18:08:37Z,2020-12-08T18:08:41Z,MERGED,True,732,524,7,https://github.com/edenhill,Import lz4 v1.9.3,3,[],https://github.com/edenhill/librdkafka/pull/3148,https://github.com/edenhill,1,https://github.com/edenhill/librdkafka/pull/3148,"This is a clean import, you dont need to review the code itself.","This is a clean import, you dont need to review the code itself.",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,3151,2020-11-17T22:57:00Z,2020-11-18T12:49:48Z,2020-11-19T14:52:34Z,MERGED,True,44,0,2,https://github.com/DavidKorczynski,Added a first fuzzer with purpose of integrating to OSS-Fuzz,4,[],https://github.com/edenhill/librdkafka/pull/3151,https://github.com/DavidKorczynski,1,https://github.com/edenhill/librdkafka/pull/3151,"Hi!
I have been doing some work on fuzzing Librdkafka and also integrating it into OSS-Fuzz (https://github.com/google/oss-fuzz). OSS-Fuzz is service from Google that performs continuous fuzzing of important open source projects. It's completely free and I would be happy to supply fuzzers to Librdkafka to help with the security of the project. By integrating into OSS-Fuzz you will receive bug reports, coverage reports and more from OSS-Fuzz, and all I need is an email(s) from you that will receive the reports.
I started with a simple fuzzer for a portion of the regex code, but would be happy to extend with more fuzzers if you would like to integrate.","Hi!
I have been doing some work on fuzzing Librdkafka and also integrating it into OSS-Fuzz (https://github.com/google/oss-fuzz). OSS-Fuzz is service from Google that performs continuous fuzzing of important open source projects. It's completely free and I would be happy to supply fuzzers to Librdkafka to help with the security of the project. By integrating into OSS-Fuzz you will receive bug reports, coverage reports and more from OSS-Fuzz, and all I need is an email(s) from you that will receive the reports.
I started with a simple fuzzer for a portion of the regex code, but would be happy to extend with more fuzzers if you would like to integrate.",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,3151,2020-11-17T22:57:00Z,2020-11-18T12:49:48Z,2020-11-19T14:52:34Z,MERGED,True,44,0,2,https://github.com/DavidKorczynski,Added a first fuzzer with purpose of integrating to OSS-Fuzz,4,[],https://github.com/edenhill/librdkafka/pull/3151,https://github.com/edenhill,2,https://github.com/edenhill/librdkafka/pull/3151#issuecomment-729279440,"Hi!
I have been doing some work on fuzzing Librdkafka and also integrating it into OSS-Fuzz (https://github.com/google/oss-fuzz). OSS-Fuzz is service from Google that performs continuous fuzzing of important open source projects. It's completely free and I would be happy to supply fuzzers to Librdkafka to help with the security of the project. By integrating into OSS-Fuzz you will receive bug reports, coverage reports and more from OSS-Fuzz, and all I need is an email(s) from you that will receive the reports.
I started with a simple fuzzer for a portion of the regex code, but would be happy to extend with more fuzzers if you would like to integrate.","This is great! Could you add a README.md to tests/fuzzer that instructs how to run/trigger the fuzzer? E.g., point to OSS-Fuzzer github repo instructions or whatever is needed, and where to check for results?
For notifications, please use rdkafka@edenhill.se",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,3151,2020-11-17T22:57:00Z,2020-11-18T12:49:48Z,2020-11-19T14:52:34Z,MERGED,True,44,0,2,https://github.com/DavidKorczynski,Added a first fuzzer with purpose of integrating to OSS-Fuzz,4,[],https://github.com/edenhill/librdkafka/pull/3151,https://github.com/DavidKorczynski,3,https://github.com/edenhill/librdkafka/pull/3151#issuecomment-729282591,"Hi!
I have been doing some work on fuzzing Librdkafka and also integrating it into OSS-Fuzz (https://github.com/google/oss-fuzz). OSS-Fuzz is service from Google that performs continuous fuzzing of important open source projects. It's completely free and I would be happy to supply fuzzers to Librdkafka to help with the security of the project. By integrating into OSS-Fuzz you will receive bug reports, coverage reports and more from OSS-Fuzz, and all I need is an email(s) from you that will receive the reports.
I started with a simple fuzzer for a portion of the regex code, but would be happy to extend with more fuzzers if you would like to integrate.","Great! I added a README - once we merge this the commands in the README will be runnable.
For reference, this is the PR in the OSS-Fuzz repository that contains the necessary logic for integrating librdkafka: google/oss-fuzz#4652",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,3151,2020-11-17T22:57:00Z,2020-11-18T12:49:48Z,2020-11-19T14:52:34Z,MERGED,True,44,0,2,https://github.com/DavidKorczynski,Added a first fuzzer with purpose of integrating to OSS-Fuzz,4,[],https://github.com/edenhill/librdkafka/pull/3151,https://github.com/edenhill,4,https://github.com/edenhill/librdkafka/pull/3151#issuecomment-729656051,"Hi!
I have been doing some work on fuzzing Librdkafka and also integrating it into OSS-Fuzz (https://github.com/google/oss-fuzz). OSS-Fuzz is service from Google that performs continuous fuzzing of important open source projects. It's completely free and I would be happy to supply fuzzers to Librdkafka to help with the security of the project. By integrating into OSS-Fuzz you will receive bug reports, coverage reports and more from OSS-Fuzz, and all I need is an email(s) from you that will receive the reports.
I started with a simple fuzzer for a portion of the regex code, but would be happy to extend with more fuzzers if you would like to integrate.",Thank you!,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,3151,2020-11-17T22:57:00Z,2020-11-18T12:49:48Z,2020-11-19T14:52:34Z,MERGED,True,44,0,2,https://github.com/DavidKorczynski,Added a first fuzzer with purpose of integrating to OSS-Fuzz,4,[],https://github.com/edenhill/librdkafka/pull/3151,https://github.com/edenhill,5,https://github.com/edenhill/librdkafka/pull/3151#issuecomment-730397581,"Hi!
I have been doing some work on fuzzing Librdkafka and also integrating it into OSS-Fuzz (https://github.com/google/oss-fuzz). OSS-Fuzz is service from Google that performs continuous fuzzing of important open source projects. It's completely free and I would be happy to supply fuzzers to Librdkafka to help with the security of the project. By integrating into OSS-Fuzz you will receive bug reports, coverage reports and more from OSS-Fuzz, and all I need is an email(s) from you that will receive the reports.
I started with a simple fuzzer for a portion of the regex code, but would be happy to extend with more fuzzers if you would like to integrate.","I got a couple of fuzzer failure emails, but I'm not authorized to view the test results, how do I get added?",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,3151,2020-11-17T22:57:00Z,2020-11-18T12:49:48Z,2020-11-19T14:52:34Z,MERGED,True,44,0,2,https://github.com/DavidKorczynski,Added a first fuzzer with purpose of integrating to OSS-Fuzz,4,[],https://github.com/edenhill/librdkafka/pull/3151,https://github.com/DavidKorczynski,6,https://github.com/edenhill/librdkafka/pull/3151#issuecomment-730415933,"Hi!
I have been doing some work on fuzzing Librdkafka and also integrating it into OSS-Fuzz (https://github.com/google/oss-fuzz). OSS-Fuzz is service from Google that performs continuous fuzzing of important open source projects. It's completely free and I would be happy to supply fuzzers to Librdkafka to help with the security of the project. By integrating into OSS-Fuzz you will receive bug reports, coverage reports and more from OSS-Fuzz, and all I need is an email(s) from you that will receive the reports.
I started with a simple fuzzer for a portion of the regex code, but would be happy to extend with more fuzzers if you would like to integrate.","You should be able to access the errors from here: https://oss-fuzz.com/
Here https://github.com/google/oss-fuzz/blob/master/projects/librdkafka/project.yaml is the configuration file for the project. I added the email rdkafka@edenhill.se so you if log in to oss-fuzz.com with that email you are good to go. You may have to attach the rdkafka@edenhill.se email to a google account, but if you prefer to switch a gmail account you can tell me and I will update the configuration at https://github.com/google/oss-fuzz/blob/master/projects/librdkafka/project.yaml",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,3151,2020-11-17T22:57:00Z,2020-11-18T12:49:48Z,2020-11-19T14:52:34Z,MERGED,True,44,0,2,https://github.com/DavidKorczynski,Added a first fuzzer with purpose of integrating to OSS-Fuzz,4,[],https://github.com/edenhill/librdkafka/pull/3151,https://github.com/edenhill,7,https://github.com/edenhill/librdkafka/pull/3151#issuecomment-730422701,"Hi!
I have been doing some work on fuzzing Librdkafka and also integrating it into OSS-Fuzz (https://github.com/google/oss-fuzz). OSS-Fuzz is service from Google that performs continuous fuzzing of important open source projects. It's completely free and I would be happy to supply fuzzers to Librdkafka to help with the security of the project. By integrating into OSS-Fuzz you will receive bug reports, coverage reports and more from OSS-Fuzz, and all I need is an email(s) from you that will receive the reports.
I started with a simple fuzzer for a portion of the regex code, but would be happy to extend with more fuzzers if you would like to integrate.","Ah, so that's what that was for, then please switch it to magnus@edenhill.se, thanks!",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,3151,2020-11-17T22:57:00Z,2020-11-18T12:49:48Z,2020-11-19T14:52:34Z,MERGED,True,44,0,2,https://github.com/DavidKorczynski,Added a first fuzzer with purpose of integrating to OSS-Fuzz,4,[],https://github.com/edenhill/librdkafka/pull/3151,https://github.com/DavidKorczynski,8,https://github.com/edenhill/librdkafka/pull/3151#issuecomment-730426787,"Hi!
I have been doing some work on fuzzing Librdkafka and also integrating it into OSS-Fuzz (https://github.com/google/oss-fuzz). OSS-Fuzz is service from Google that performs continuous fuzzing of important open source projects. It's completely free and I would be happy to supply fuzzers to Librdkafka to help with the security of the project. By integrating into OSS-Fuzz you will receive bug reports, coverage reports and more from OSS-Fuzz, and all I need is an email(s) from you that will receive the reports.
I started with a simple fuzzer for a portion of the regex code, but would be happy to extend with more fuzzers if you would like to integrate.","Apologies for confusion! Fixed it up now, it should be merged soon. If you can't watch the bug reports due to adding the email after the bugs were found (I don't think this is the case but cannot remember entirely) then let me know and I will forward all of the existing reports to you.",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,3153,2020-11-19T08:07:08Z,2020-11-20T14:06:02Z,2020-11-20T14:06:05Z,MERGED,True,80,3,7,https://github.com/edenhill,Test and CI robustness,3,[],https://github.com/edenhill/librdkafka/pull/3153,https://github.com/edenhill,1,https://github.com/edenhill/librdkafka/pull/3153,,,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,3154,2020-11-19T12:21:36Z,2020-11-19T12:40:44Z,2020-11-19T12:40:48Z,MERGED,True,1,1,1,https://github.com/vitalyzhakov,Fix typo in readme,1,[],https://github.com/edenhill/librdkafka/pull/3154,https://github.com/vitalyzhakov,1,https://github.com/edenhill/librdkafka/pull/3154,,,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,3154,2020-11-19T12:21:36Z,2020-11-19T12:40:44Z,2020-11-19T12:40:48Z,MERGED,True,1,1,1,https://github.com/vitalyzhakov,Fix typo in readme,1,[],https://github.com/edenhill/librdkafka/pull/3154,https://github.com/edenhill,2,https://github.com/edenhill/librdkafka/pull/3154#issuecomment-730350061,,Thank you!,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,3156,2020-11-20T16:41:42Z,2020-11-24T17:36:31Z,2020-11-24T17:36:36Z,MERGED,True,5,5,2,https://github.com/jjhoughton,"Fix docs, change RD_KAFKA_CONF_RES_OK to RD_KAFKA_CONF_OK",1,[],https://github.com/edenhill/librdkafka/pull/3156,https://github.com/jjhoughton,1,https://github.com/edenhill/librdkafka/pull/3156,Signed-off-by: Joshua Houghton joshua.houghton@ripjar.com,Signed-off-by: Joshua Houghton joshua.houghton@ripjar.com,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,3156,2020-11-20T16:41:42Z,2020-11-24T17:36:31Z,2020-11-24T17:36:36Z,MERGED,True,5,5,2,https://github.com/jjhoughton,"Fix docs, change RD_KAFKA_CONF_RES_OK to RD_KAFKA_CONF_OK",1,[],https://github.com/edenhill/librdkafka/pull/3156,https://github.com/edenhill,2,https://github.com/edenhill/librdkafka/pull/3156#issuecomment-733130433,Signed-off-by: Joshua Houghton joshua.houghton@ripjar.com,Thank you!,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,3160,2020-11-24T13:24:47Z,,2020-11-25T11:08:53Z,OPEN,False,1,1,1,https://github.com/mhowlett,RD_KAFKA_RESP_ERR__PARTITION_EOF is not a broker error,1,[],https://github.com/edenhill/librdkafka/pull/3160,https://github.com/mhowlett,1,https://github.com/edenhill/librdkafka/pull/3160,"i guess you've reasoned that what is here already is most correct, but i think there's a lot of value in differentiating whether an error is local or broker strictly on whether the error was generated on the broker or not. PR just a suggestion.","i guess you've reasoned that what is here already is most correct, but i think there's a lot of value in differentiating whether an error is local or broker strictly on whether the error was generated on the broker or not. PR just a suggestion.",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,3161,2020-11-25T09:02:15Z,2020-12-10T17:17:10Z,2020-12-10T17:17:14Z,MERGED,True,425,50,21,https://github.com/edenhill,Misc fixes,11,[],https://github.com/edenhill/librdkafka/pull/3161,https://github.com/edenhill,1,https://github.com/edenhill/librdkafka/pull/3161,,,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,3161,2020-11-25T09:02:15Z,2020-12-10T17:17:10Z,2020-12-10T17:17:14Z,MERGED,True,425,50,21,https://github.com/edenhill,Misc fixes,11,[],https://github.com/edenhill/librdkafka/pull/3161,https://github.com/edenhill,2,https://github.com/edenhill/librdkafka/pull/3161#issuecomment-740807150,,@mhowlett Can I please get a review on this?,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,3163,2020-11-30T20:49:28Z,2020-12-08T18:05:45Z,2020-12-08T18:05:45Z,MERGED,True,71,47,3,https://github.com/mhowlett,Incremental rebalancing fixes,3,[],https://github.com/edenhill/librdkafka/pull/3163,https://github.com/mhowlett,1,https://github.com/edenhill/librdkafka/pull/3163,"partitions revoked events should never be emitted in the incremental rebalancing case when the assignment is empty.
in the n_wildcard test it is valid for either one rebalance or two rebalances to be required for the consumers subscription to be updated.","partitions revoked events should never be emitted in the incremental rebalancing case when the assignment is empty.
in the n_wildcard test it is valid for either one rebalance or two rebalances to be required for the consumers subscription to be updated.",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,3165,2020-12-02T09:36:18Z,2020-12-02T09:46:24Z,2020-12-02T09:46:24Z,CLOSED,False,17,16,1,https://github.com/theidexisted,Add colorful echo steps,1,[],https://github.com/edenhill/librdkafka/pull/3165,https://github.com/theidexisted,1,https://github.com/edenhill/librdkafka/pull/3165,,,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,3169,2020-12-04T16:47:18Z,2020-12-08T18:09:34Z,2020-12-08T18:09:34Z,MERGED,True,48,34,2,https://github.com/mhowlett,Skip aborted messages in compressed MessageSets,6,[],https://github.com/edenhill/librdkafka/pull/3169,https://github.com/mhowlett,1,https://github.com/edenhill/librdkafka/pull/3169,resolves #3020,resolves #3020,True,{'HOORAY': ['https://github.com/gridaphobe']}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,3169,2020-12-04T16:47:18Z,2020-12-08T18:09:34Z,2020-12-08T18:09:34Z,MERGED,True,48,34,2,https://github.com/mhowlett,Skip aborted messages in compressed MessageSets,6,[],https://github.com/edenhill/librdkafka/pull/3169,https://github.com/mhowlett,2,https://github.com/edenhill/librdkafka/pull/3169#issuecomment-740760234,resolves #3020,"there's no 1.5.3 section in the changelog yet. maybe something like:
""The consumer would deliver messages in aborted transactions to the application even when isolation.level was set to read_committed in the case of compressed message sets.""",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,3169,2020-12-04T16:47:18Z,2020-12-08T18:09:34Z,2020-12-08T18:09:34Z,MERGED,True,48,34,2,https://github.com/mhowlett,Skip aborted messages in compressed MessageSets,6,[],https://github.com/edenhill/librdkafka/pull/3169,https://github.com/edenhill,3,https://github.com/edenhill/librdkafka/pull/3169#issuecomment-740769079,resolves #3020,"Re changelog, a right, this is on master, add it to the 1.6.0 changelog and I'll do the 1.5.3 backporting.",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,3169,2020-12-04T16:47:18Z,2020-12-08T18:09:34Z,2020-12-08T18:09:34Z,MERGED,True,48,34,2,https://github.com/mhowlett,Skip aborted messages in compressed MessageSets,6,[],https://github.com/edenhill/librdkafka/pull/3169,https://github.com/mhowlett,4,https://github.com/edenhill/librdkafka/pull/3169#issuecomment-740771664,resolves #3020,how about you just copy paste the text above? seems quicker.,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,3169,2020-12-04T16:47:18Z,2020-12-08T18:09:34Z,2020-12-08T18:09:34Z,MERGED,True,48,34,2,https://github.com/mhowlett,Skip aborted messages in compressed MessageSets,6,[],https://github.com/edenhill/librdkafka/pull/3169,https://github.com/edenhill,5,https://github.com/edenhill/librdkafka/pull/3169#issuecomment-740780403,resolves #3020,quicker for who? ;),True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,3173,2020-12-09T17:29:31Z,2020-12-10T10:21:18Z,2020-12-10T10:21:18Z,MERGED,True,8,8,1,https://github.com/gridaphobe,move note about transaction.timeout.ms to begin_transaction,1,[],https://github.com/edenhill/librdkafka/pull/3173,https://github.com/gridaphobe,1,https://github.com/edenhill/librdkafka/pull/3173,"As far as I can tell, there is no broker-side requirement to perform transactional operations after calling rd_kafka_init_transactions, and the client in examples/transactions.c works just fine if I insert arbitrary delays between init_transactions and begin_transaction. It would also be strange if this requirement did exist, because init_transactions is meant to be called once at startup and we can't necessarily control the rate of outgoing messages.","As far as I can tell, there is no broker-side requirement to perform transactional operations after calling rd_kafka_init_transactions, and the client in examples/transactions.c works just fine if I insert arbitrary delays between init_transactions and begin_transaction. It would also be strange if this requirement did exist, because init_transactions is meant to be called once at startup and we can't necessarily control the rate of outgoing messages.",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,3174,2020-12-10T17:20:31Z,2020-12-16T07:53:00Z,2020-12-16T07:53:06Z,MERGED,True,117,26,10,https://github.com/edenhill,Add debug=conf to show config on startup,2,[],https://github.com/edenhill/librdkafka/pull/3174,https://github.com/edenhill,1,https://github.com/edenhill/librdkafka/pull/3174,"Ryan, please go through the config properties in rdkafka_conf.c and check if any senstitive ones are missing the _RK_SENSITIVE flag, e.g.,
https://github.com/edenhill/librdkafka/blob/confdump/src/rdkafka_conf.c#L728
The rest is all yours, @mhowlett
Example output with debug=conf:
1607620582.284 RDKAFKA-7-INIT: rdkafka#producer-1: [thrd:app]: librdkafka v1.6.0-PRE4-25-g43eae6-devel-O0 (0x1060000) rdkafka#producer-1 initialized (builtin.features gzip,snappy,ssl,sasl,regex,lz4,sasl_gssapi,sasl_plain,sasl_scram,plugins,zstd,sasl_oauthbearer, GCC GXX PKGCONFIG INSTALL GNULD LDS C11THREADS LIBDL PLUGINS ZLIB SSL SASL_CYRUS ZSTD HDRHISTOGRAM LZ4_EXT SYSLOG RAPIDJSON SNAPPY SOCKEM SASL_SCRAM SASL_OAUTHBEARER CRC32C_HW, debug 0x40000)
1607620582.292 RDKAFKA-4-CONFWARN: rdkafka#producer-1: [thrd:app]: Configuration property `sasl.username` only applies when `sasl.mechanism` is set to PLAIN or SCRAM-SHA-..
1607620582.294 RDKAFKA-5-CONFWARN: rdkafka#producer-1: [thrd:app]: No `bootstrap.servers` configured: client will not be able to connect to Kafka cluster
1607620582.299 RDKAFKA-7-CONF: rdkafka#producer-1: [thrd:app]: Client configuration:
1607620582.299 RDKAFKA-7-CONF: rdkafka#producer-1: [thrd:app]:   client.software.version = 1.6.0-PRE4-25-g43eae6-devel-O0
1607620582.299 RDKAFKA-7-CONF: rdkafka#producer-1: [thrd:app]:   debug = conf
1607620582.299 RDKAFKA-7-CONF: rdkafka#producer-1: [thrd:app]:   log_cb = 0x12d362
1607620582.299 RDKAFKA-7-CONF: rdkafka#producer-1: [thrd:app]:   internal.termination.signal = 29
1607620582.299 RDKAFKA-7-CONF: rdkafka#producer-1: [thrd:app]:   sasl.username = [redacted]
1607620582.299 RDKAFKA-7-CONF: rdkafka#producer-1: [thrd:app]:   sasl.password = [redacted]","Ryan, please go through the config properties in rdkafka_conf.c and check if any senstitive ones are missing the _RK_SENSITIVE flag, e.g.,
https://github.com/edenhill/librdkafka/blob/confdump/src/rdkafka_conf.c#L728
The rest is all yours, @mhowlett
Example output with debug=conf:
1607620582.284 RDKAFKA-7-INIT: rdkafka#producer-1: [thrd:app]: librdkafka v1.6.0-PRE4-25-g43eae6-devel-O0 (0x1060000) rdkafka#producer-1 initialized (builtin.features gzip,snappy,ssl,sasl,regex,lz4,sasl_gssapi,sasl_plain,sasl_scram,plugins,zstd,sasl_oauthbearer, GCC GXX PKGCONFIG INSTALL GNULD LDS C11THREADS LIBDL PLUGINS ZLIB SSL SASL_CYRUS ZSTD HDRHISTOGRAM LZ4_EXT SYSLOG RAPIDJSON SNAPPY SOCKEM SASL_SCRAM SASL_OAUTHBEARER CRC32C_HW, debug 0x40000)
1607620582.292 RDKAFKA-4-CONFWARN: rdkafka#producer-1: [thrd:app]: Configuration property `sasl.username` only applies when `sasl.mechanism` is set to PLAIN or SCRAM-SHA-..
1607620582.294 RDKAFKA-5-CONFWARN: rdkafka#producer-1: [thrd:app]: No `bootstrap.servers` configured: client will not be able to connect to Kafka cluster
1607620582.299 RDKAFKA-7-CONF: rdkafka#producer-1: [thrd:app]: Client configuration:
1607620582.299 RDKAFKA-7-CONF: rdkafka#producer-1: [thrd:app]:   client.software.version = 1.6.0-PRE4-25-g43eae6-devel-O0
1607620582.299 RDKAFKA-7-CONF: rdkafka#producer-1: [thrd:app]:   debug = conf
1607620582.299 RDKAFKA-7-CONF: rdkafka#producer-1: [thrd:app]:   log_cb = 0x12d362
1607620582.299 RDKAFKA-7-CONF: rdkafka#producer-1: [thrd:app]:   internal.termination.signal = 29
1607620582.299 RDKAFKA-7-CONF: rdkafka#producer-1: [thrd:app]:   sasl.username = [redacted]
1607620582.299 RDKAFKA-7-CONF: rdkafka#producer-1: [thrd:app]:   sasl.password = [redacted]",True,{'HOORAY': ['https://github.com/gridaphobe']}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,3177,2020-12-11T11:54:07Z,2021-01-04T09:15:31Z,2021-01-04T09:15:32Z,MERGED,True,116,47,15,https://github.com/edenhill,Add arm64 travis job and linux-arm64 (ubuntu 18.04) build to NuGet package,11,[],https://github.com/edenhill/librdkafka/pull/3177,https://github.com/edenhill,1,https://github.com/edenhill/librdkafka/pull/3177,,,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,3177,2020-12-11T11:54:07Z,2021-01-04T09:15:31Z,2021-01-04T09:15:32Z,MERGED,True,116,47,15,https://github.com/edenhill,Add arm64 travis job and linux-arm64 (ubuntu 18.04) build to NuGet package,11,[],https://github.com/edenhill/librdkafka/pull/3177,https://github.com/edenhill,2,https://github.com/edenhill/librdkafka/pull/3177#issuecomment-744283240,,"14M, it needs to be stripped, will look into it.",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,3178,2020-12-11T21:20:07Z,2020-12-14T08:50:54Z,2020-12-14T08:51:00Z,MERGED,True,3,3,1,https://github.com/mamapanda,Return added broker in rd_kafka_broker_update,1,[],https://github.com/edenhill/librdkafka/pull/3178,https://github.com/mamapanda,1,https://github.com/edenhill/librdkafka/pull/3178,"Based on the doxygen comment, a newly added broker should probably be returned via the rkbp parameter.","Based on the doxygen comment, a newly added broker should probably be returned via the rkbp parameter.",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,3178,2020-12-11T21:20:07Z,2020-12-14T08:50:54Z,2020-12-14T08:51:00Z,MERGED,True,3,3,1,https://github.com/mamapanda,Return added broker in rd_kafka_broker_update,1,[],https://github.com/edenhill/librdkafka/pull/3178,https://github.com/edenhill,2,https://github.com/edenhill/librdkafka/pull/3178#issuecomment-744284141,"Based on the doxygen comment, a newly added broker should probably be returned via the rkbp parameter.",Thank you!,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,3179,2020-12-12T22:49:28Z,2021-03-15T12:41:46Z,2021-03-15T12:41:50Z,MERGED,True,33,68,2,https://github.com/azat,Fix possible corrupted decompression of snappy format,2,[],https://github.com/edenhill/librdkafka/pull/3179,https://github.com/azat,1,https://github.com/edenhill/librdkafka/pull/3179,"In incremental_copy_fast_path there is undefined behavior (and in some
other places too).
And under this circumstances gcc10 with -O1 -ftree-loop-vectorize (or
simply -O3), due to loop unroll, generates code that do copy by 16 bytes
at a time for the second loop (MOVDQU+MOVUPS), while this is not correct
since the memory may be overlapped and may be changed in the previous
iteration.
Fix this by eliminating those UB, by using memcpy over direct store/load
since these days direct store/loads looks redundant. Even on ARM 1.
Upstream patch: andikleen/snappy-c#22
NOTE: clang is fine, and other older versions of gcc too
This bug was found by ClickHouse test suite.","In incremental_copy_fast_path there is undefined behavior (and in some
other places too).
And under this circumstances gcc10 with -O1 -ftree-loop-vectorize (or
simply -O3), due to loop unroll, generates code that do copy by 16 bytes
at a time for the second loop (MOVDQU+MOVUPS), while this is not correct
since the memory may be overlapped and may be changed in the previous
iteration.
Fix this by eliminating those UB, by using memcpy over direct store/load
since these days direct store/loads looks redundant. Even on ARM 1.
Upstream patch: andikleen/snappy-c#22
NOTE: clang is fine, and other older versions of gcc too
This bug was found by ClickHouse test suite.",True,{'THUMBS_UP': ['https://github.com/alexey-milovidov']}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,3179,2020-12-12T22:49:28Z,2021-03-15T12:41:46Z,2021-03-15T12:41:50Z,MERGED,True,33,68,2,https://github.com/azat,Fix possible corrupted decompression of snappy format,2,[],https://github.com/edenhill/librdkafka/pull/3179,https://github.com/azat,2,https://github.com/edenhill/librdkafka/pull/3179#issuecomment-743917831,"In incremental_copy_fast_path there is undefined behavior (and in some
other places too).
And under this circumstances gcc10 with -O1 -ftree-loop-vectorize (or
simply -O3), due to loop unroll, generates code that do copy by 16 bytes
at a time for the second loop (MOVDQU+MOVUPS), while this is not correct
since the memory may be overlapped and may be changed in the previous
iteration.
Fix this by eliminating those UB, by using memcpy over direct store/load
since these days direct store/loads looks redundant. Even on ARM 1.
Upstream patch: andikleen/snappy-c#22
NOTE: clang is fine, and other older versions of gcc too
This bug was found by ClickHouse test suite.","continuous-integration/appveyor/pr  AppVeyor build failed

gcc'ism is not supported (get_unaligned_memcpy/put_unaligned_memcpy)",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,3179,2020-12-12T22:49:28Z,2021-03-15T12:41:46Z,2021-03-15T12:41:50Z,MERGED,True,33,68,2,https://github.com/azat,Fix possible corrupted decompression of snappy format,2,[],https://github.com/edenhill/librdkafka/pull/3179,https://github.com/azat,3,https://github.com/edenhill/librdkafka/pull/3179#issuecomment-744031611,"In incremental_copy_fast_path there is undefined behavior (and in some
other places too).
And under this circumstances gcc10 with -O1 -ftree-loop-vectorize (or
simply -O3), due to loop unroll, generates code that do copy by 16 bytes
at a time for the second loop (MOVDQU+MOVUPS), while this is not correct
since the memory may be overlapped and may be changed in the previous
iteration.
Fix this by eliminating those UB, by using memcpy over direct store/load
since these days direct store/loads looks redundant. Even on ARM 1.
Upstream patch: andikleen/snappy-c#22
NOTE: clang is fine, and other older versions of gcc too
This bug was found by ClickHouse test suite.","continuous-integration/travis-ci/pr  The Travis CI build failed

https://travis-ci.org/github/edenhill/librdkafka/jobs/749404551#L2915
| 0104_fetch_from_follower_mock            |     FAILED |  92.407s | test_consumer_poll():3864: Consume: consumer_poll() timeout (0/1 eof, 0/1000 msgs)

Is it flaky?",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,3179,2020-12-12T22:49:28Z,2021-03-15T12:41:46Z,2021-03-15T12:41:50Z,MERGED,True,33,68,2,https://github.com/azat,Fix possible corrupted decompression of snappy format,2,[],https://github.com/edenhill/librdkafka/pull/3179,https://github.com/edenhill,4,https://github.com/edenhill/librdkafka/pull/3179#issuecomment-744269629,"In incremental_copy_fast_path there is undefined behavior (and in some
other places too).
And under this circumstances gcc10 with -O1 -ftree-loop-vectorize (or
simply -O3), due to loop unroll, generates code that do copy by 16 bytes
at a time for the second loop (MOVDQU+MOVUPS), while this is not correct
since the memory may be overlapped and may be changed in the previous
iteration.
Fix this by eliminating those UB, by using memcpy over direct store/load
since these days direct store/loads looks redundant. Even on ARM 1.
Upstream patch: andikleen/snappy-c#22
NOTE: clang is fine, and other older versions of gcc too
This bug was found by ClickHouse test suite.","@azat Yes, that test is flaky on CI, but never locally, so a bit tricky to debug. You can ignore it.",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,3179,2020-12-12T22:49:28Z,2021-03-15T12:41:46Z,2021-03-15T12:41:50Z,MERGED,True,33,68,2,https://github.com/azat,Fix possible corrupted decompression of snappy format,2,[],https://github.com/edenhill/librdkafka/pull/3179,https://github.com/azat,5,https://github.com/edenhill/librdkafka/pull/3179#issuecomment-749693530,"In incremental_copy_fast_path there is undefined behavior (and in some
other places too).
And under this circumstances gcc10 with -O1 -ftree-loop-vectorize (or
simply -O3), due to loop unroll, generates code that do copy by 16 bytes
at a time for the second loop (MOVDQU+MOVUPS), while this is not correct
since the memory may be overlapped and may be changed in the previous
iteration.
Fix this by eliminating those UB, by using memcpy over direct store/load
since these days direct store/loads looks redundant. Even on ARM 1.
Upstream patch: andikleen/snappy-c#22
NOTE: clang is fine, and other older versions of gcc too
This bug was found by ClickHouse test suite.",@edenhill any feedback on this patch?,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,3179,2020-12-12T22:49:28Z,2021-03-15T12:41:46Z,2021-03-15T12:41:50Z,MERGED,True,33,68,2,https://github.com/azat,Fix possible corrupted decompression of snappy format,2,[],https://github.com/edenhill/librdkafka/pull/3179,https://github.com/edenhill,6,https://github.com/edenhill/librdkafka/pull/3179#issuecomment-799387935,"In incremental_copy_fast_path there is undefined behavior (and in some
other places too).
And under this circumstances gcc10 with -O1 -ftree-loop-vectorize (or
simply -O3), due to loop unroll, generates code that do copy by 16 bytes
at a time for the second loop (MOVDQU+MOVUPS), while this is not correct
since the memory may be overlapped and may be changed in the previous
iteration.
Fix this by eliminating those UB, by using memcpy over direct store/load
since these days direct store/loads looks redundant. Even on ARM 1.
Upstream patch: andikleen/snappy-c#22
NOTE: clang is fine, and other older versions of gcc too
This bug was found by ClickHouse test suite.",Thank you!,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,3180,2020-12-15T10:02:26Z,,2021-10-21T19:48:24Z,OPEN,False,666,8,15,https://github.com/KJTsanaktsidis,Sketch of what a callback interface for TLS might look like,1,[],https://github.com/edenhill/librdkafka/pull/3180,https://github.com/KJTsanaktsidis,1,https://github.com/edenhill/librdkafka/pull/3180,"In #2868, (and then again in #3157), there was a discussion about how to update the SSL client certificate of a librdkafka client in a rotation scenario. Librdkafka already supports rd_kafka_conf_set_ssl_cert_verify_cb for delegating TLS CA validation checks to a client callback; in this PR, I added an analogous rd_kafka_conf_set_ssl_cert_fetch_cb option for setting a callback that provides a TLS certificate to librdkafka when a connection is opened.
The callback is called each time a TLS connection is opened, and it's expected that the application will fill the provided buffer with the TLS client certificate, TLS key, and any intermediates (up to 16, because I couldn't figure out a good way to make that number dynamic, but we can probably make something dynamic work I guess).
This is a bit of a WIP:
* Obviously whatever feedback comes along :)
* The test implementation is very rough. In particular, I wonder if all of the TLS cert generation stuff should be moved into trivup? If so I can make a PR there.
* I haven't quite figured out how to use this interface from golang - the whole point of it was I wanted to be able to plumb a golang tls.Context object into a librdkafka connection, so that we can integrate confluent-kafka-go with the golang stdlib TLS stack.
* Looks like I have a lot of CI fun to deal with.","In #2868, (and then again in #3157), there was a discussion about how to update the SSL client certificate of a librdkafka client in a rotation scenario. Librdkafka already supports rd_kafka_conf_set_ssl_cert_verify_cb for delegating TLS CA validation checks to a client callback; in this PR, I added an analogous rd_kafka_conf_set_ssl_cert_fetch_cb option for setting a callback that provides a TLS certificate to librdkafka when a connection is opened.
The callback is called each time a TLS connection is opened, and it's expected that the application will fill the provided buffer with the TLS client certificate, TLS key, and any intermediates (up to 16, because I couldn't figure out a good way to make that number dynamic, but we can probably make something dynamic work I guess).
This is a bit of a WIP:
* Obviously whatever feedback comes along :)
* The test implementation is very rough. In particular, I wonder if all of the TLS cert generation stuff should be moved into trivup? If so I can make a PR there.
* I haven't quite figured out how to use this interface from golang - the whole point of it was I wanted to be able to plumb a golang tls.Context object into a librdkafka connection, so that we can integrate confluent-kafka-go with the golang stdlib TLS stack.
* Looks like I have a lot of CI fun to deal with.",True,"{'HEART': ['https://github.com/dstelljes', 'https://github.com/alex3165'], 'THUMBS_UP': ['https://github.com/zuiderkwast', 'https://github.com/gautam-jha', 'https://github.com/jgiannuzzi']}"
edenhill/librdkafka,https://github.com/edenhill/librdkafka,3180,2020-12-15T10:02:26Z,,2021-10-21T19:48:24Z,OPEN,False,666,8,15,https://github.com/KJTsanaktsidis,Sketch of what a callback interface for TLS might look like,1,[],https://github.com/edenhill/librdkafka/pull/3180,https://github.com/KJTsanaktsidis,2,https://github.com/edenhill/librdkafka/pull/3180#issuecomment-745727756,"In #2868, (and then again in #3157), there was a discussion about how to update the SSL client certificate of a librdkafka client in a rotation scenario. Librdkafka already supports rd_kafka_conf_set_ssl_cert_verify_cb for delegating TLS CA validation checks to a client callback; in this PR, I added an analogous rd_kafka_conf_set_ssl_cert_fetch_cb option for setting a callback that provides a TLS certificate to librdkafka when a connection is opened.
The callback is called each time a TLS connection is opened, and it's expected that the application will fill the provided buffer with the TLS client certificate, TLS key, and any intermediates (up to 16, because I couldn't figure out a good way to make that number dynamic, but we can probably make something dynamic work I guess).
This is a bit of a WIP:
* Obviously whatever feedback comes along :)
* The test implementation is very rough. In particular, I wonder if all of the TLS cert generation stuff should be moved into trivup? If so I can make a PR there.
* I haven't quite figured out how to use this interface from golang - the whole point of it was I wanted to be able to plumb a golang tls.Context object into a librdkafka connection, so that we can integrate confluent-kafka-go with the golang stdlib TLS stack.
* Looks like I have a lot of CI fun to deal with.","hum. There's a failed test that I assume is flaky, but also, the test case I added doesn't actually run:
[0m[36m[0121_ssl_cb                 /  0.000s] ================= Running test 0121_ssl_cb =================
[0m[36m[0121_ssl_cb                 /  0.000s] ==== Stats written to file stats_0121_ssl_cb_2700797783708785096.json ====
[0m[36m[0121_ssl_cb                 /  0.000s] Feature ""ssl"" is built-in
[0m[36m[0121_ssl_cb                 /  0.000s] Feature ""ssl_client_cert_callback"" is built-in
[0m[33m[0121_ssl_cb                 /  0.000s] WARN: SKIPPING TEST: Test requires RDK_SSL_ca_pem env var
[0m[33m[0121_ssl_cb                 /  0.000s] WARN: SKIPPING TEST: Test requires RDK_SSL_pub_pem env var
[0m[33m[0121_ssl_cb                 /  0.000s] WARN: SKIPPING TEST: Test requires RDK_SSL_priv_pem env var
[0m[33m[0121_ssl_cb                 /  0.000s] WARN: SKIPPING TEST: Test requires RDK_UNTRUSTEDSSL_pub_pem env var
[0m[33m[0121_ssl_cb                 /  0.000s] WARN: SKIPPING TEST: Test requires RDK_UNTRUSTEDSSL_priv_pem env var
[0m[33m[0121_ssl_cb                 /  0.000s] WARN: SKIPPING TEST: Test requires RDK_INTERMEDIATESSL_pub_pem env var
[0m[33m[0121_ssl_cb                 /  0.000s] WARN: SKIPPING TEST: Test requires RDK_INTERMEDIATESSL_priv_pem env var
[0m[33m[0121_ssl_cb                 /  0.000s] WARN: SKIPPING TEST: Test requires RDK_INTERMEDIATESSL_intermediate_pub_pem env var
[0m[33m[0121_ssl_cb                 /  0.000s] WARN: SKIPPING TEST: Test requires RDK_INTERMEDIATESSL_intermediate_priv_pem env var
[0m[36m[0121_ssl_cb                 /  0.000s] 0121_ssl_cb: duration 0.074ms
[0m[36m[0121_ssl_cb                 /  0.000s] ================= Test 0121_ssl_cb SKIPPED =================

looks like trivup isn't run with the SSL stuff on travis?",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,3180,2020-12-15T10:02:26Z,,2021-10-21T19:48:24Z,OPEN,False,666,8,15,https://github.com/KJTsanaktsidis,Sketch of what a callback interface for TLS might look like,1,[],https://github.com/edenhill/librdkafka/pull/3180,https://github.com/KJTsanaktsidis,3,https://github.com/edenhill/librdkafka/pull/3180#issuecomment-747169558,"In #2868, (and then again in #3157), there was a discussion about how to update the SSL client certificate of a librdkafka client in a rotation scenario. Librdkafka already supports rd_kafka_conf_set_ssl_cert_verify_cb for delegating TLS CA validation checks to a client callback; in this PR, I added an analogous rd_kafka_conf_set_ssl_cert_fetch_cb option for setting a callback that provides a TLS certificate to librdkafka when a connection is opened.
The callback is called each time a TLS connection is opened, and it's expected that the application will fill the provided buffer with the TLS client certificate, TLS key, and any intermediates (up to 16, because I couldn't figure out a good way to make that number dynamic, but we can probably make something dynamic work I guess).
This is a bit of a WIP:
* Obviously whatever feedback comes along :)
* The test implementation is very rough. In particular, I wonder if all of the TLS cert generation stuff should be moved into trivup? If so I can make a PR there.
* I haven't quite figured out how to use this interface from golang - the whole point of it was I wanted to be able to plumb a golang tls.Context object into a librdkafka connection, so that we can integrate confluent-kafka-go with the golang stdlib TLS stack.
* Looks like I have a lot of CI fun to deal with.","Looks like i'm going to get conflicts every time someone else merges a new testcase, so I'll probably leave them for now until this is closer to being merged.",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,3180,2020-12-15T10:02:26Z,,2021-10-21T19:48:24Z,OPEN,False,666,8,15,https://github.com/KJTsanaktsidis,Sketch of what a callback interface for TLS might look like,1,[],https://github.com/edenhill/librdkafka/pull/3180,https://github.com/KJTsanaktsidis,4,https://github.com/edenhill/librdkafka/pull/3180#issuecomment-755106625,"In #2868, (and then again in #3157), there was a discussion about how to update the SSL client certificate of a librdkafka client in a rotation scenario. Librdkafka already supports rd_kafka_conf_set_ssl_cert_verify_cb for delegating TLS CA validation checks to a client callback; in this PR, I added an analogous rd_kafka_conf_set_ssl_cert_fetch_cb option for setting a callback that provides a TLS certificate to librdkafka when a connection is opened.
The callback is called each time a TLS connection is opened, and it's expected that the application will fill the provided buffer with the TLS client certificate, TLS key, and any intermediates (up to 16, because I couldn't figure out a good way to make that number dynamic, but we can probably make something dynamic work I guess).
This is a bit of a WIP:
* Obviously whatever feedback comes along :)
* The test implementation is very rough. In particular, I wonder if all of the TLS cert generation stuff should be moved into trivup? If so I can make a PR there.
* I haven't quite figured out how to use this interface from golang - the whole point of it was I wanted to be able to plumb a golang tls.Context object into a librdkafka connection, so that we can integrate confluent-kafka-go with the golang stdlib TLS stack.
* Looks like I have a lot of CI fun to deal with.",I also implemented this interface in confluent-kafka-go to allow passing a stdlib tls.Config object to handle TLS: https://github.com/confluentinc/confluent-kafka-go/compare/zendesk:ktsanaktsidis/ssl_cb_spike,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,3180,2020-12-15T10:02:26Z,,2021-10-21T19:48:24Z,OPEN,False,666,8,15,https://github.com/KJTsanaktsidis,Sketch of what a callback interface for TLS might look like,1,[],https://github.com/edenhill/librdkafka/pull/3180,https://github.com/edenhill,5,https://github.com/edenhill/librdkafka/pull/3180#issuecomment-796736236,"In #2868, (and then again in #3157), there was a discussion about how to update the SSL client certificate of a librdkafka client in a rotation scenario. Librdkafka already supports rd_kafka_conf_set_ssl_cert_verify_cb for delegating TLS CA validation checks to a client callback; in this PR, I added an analogous rd_kafka_conf_set_ssl_cert_fetch_cb option for setting a callback that provides a TLS certificate to librdkafka when a connection is opened.
The callback is called each time a TLS connection is opened, and it's expected that the application will fill the provided buffer with the TLS client certificate, TLS key, and any intermediates (up to 16, because I couldn't figure out a good way to make that number dynamic, but we can probably make something dynamic work I guess).
This is a bit of a WIP:
* Obviously whatever feedback comes along :)
* The test implementation is very rough. In particular, I wonder if all of the TLS cert generation stuff should be moved into trivup? If so I can make a PR there.
* I haven't quite figured out how to use this interface from golang - the whole point of it was I wanted to be able to plumb a golang tls.Context object into a librdkafka connection, so that we can integrate confluent-kafka-go with the golang stdlib TLS stack.
* Looks like I have a lot of CI fun to deal with.","Have not had time to look at this in detail, but will soon.


The test implementation is very rough. In particular, I wonder if all of the TLS cert generation stuff should be moved into trivup? If so I can make a PR there.
Sounds like a good plan. Trivup already has the SslApp that generates certs and stuff.




I haven't quite figured out how to use this interface from golang - the whole point of it was I wanted to be able to plumb a golang tls.Context object into a librdkafka connection, so that we can integrate confluent-kafka-go with the golang stdlib TLS stack.


Yeah, we can't really spontaneously call out (callbacks) from C to Go from threads not managed by the Go runtime, which would be the case here since the callbacks would be trigger by the rdkafka broker threads. This is why the go client makes use of rd_kafka_event_t's rather than callbacks, and those events are polled from queues in Go-maintained threads.
Making the SSL handshake enqueue an event, etc, to gather the required certificates would indeed be possible, albeit a bit complex. Before going down that route I'd like to understand the underlying requirements;
There is a requirement to allow certificate/key rotations, but that would not need to be per-broker.
Is there a use-case for per-broker certs/keys?

Looks like I have a lot of CI fun to deal with.

Some of the tests are flaky in CI (despite vast efforts to reproduce these issues locally).",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,3180,2020-12-15T10:02:26Z,,2021-10-21T19:48:24Z,OPEN,False,666,8,15,https://github.com/KJTsanaktsidis,Sketch of what a callback interface for TLS might look like,1,[],https://github.com/edenhill/librdkafka/pull/3180,https://github.com/edenhill,6,https://github.com/edenhill/librdkafka/pull/3180#issuecomment-796773437,"In #2868, (and then again in #3157), there was a discussion about how to update the SSL client certificate of a librdkafka client in a rotation scenario. Librdkafka already supports rd_kafka_conf_set_ssl_cert_verify_cb for delegating TLS CA validation checks to a client callback; in this PR, I added an analogous rd_kafka_conf_set_ssl_cert_fetch_cb option for setting a callback that provides a TLS certificate to librdkafka when a connection is opened.
The callback is called each time a TLS connection is opened, and it's expected that the application will fill the provided buffer with the TLS client certificate, TLS key, and any intermediates (up to 16, because I couldn't figure out a good way to make that number dynamic, but we can probably make something dynamic work I guess).
This is a bit of a WIP:
* Obviously whatever feedback comes along :)
* The test implementation is very rough. In particular, I wonder if all of the TLS cert generation stuff should be moved into trivup? If so I can make a PR there.
* I haven't quite figured out how to use this interface from golang - the whole point of it was I wanted to be able to plumb a golang tls.Context object into a librdkafka connection, so that we can integrate confluent-kafka-go with the golang stdlib TLS stack.
* Looks like I have a lot of CI fun to deal with.","After some superficial pondering I'm leaning towards doing what we did with OAUTHBEARER token refreshes:

have a notification callback that simply says that something needs to be done, in the OAUTHBEARER case its just a refresh interval callback, in this case here it might be a refresh interval callback aswell (based on some ssl.certificate.refresh.interval.ms?) and/or a callback triggered (from the main loop, not broker thread) when a SSL connection fails due to cert verification failure (or whatever the error is).  E.g., ssl_certificate_refresh_cb
add a rd_kafka_ssl_certificate_set(rk, broker_id(or -1 for all), type(private,public,ca), encoding(PEM), buffer, buffer_size).  This call creates a new SSL_CTX for use by any new connections made after this point.

This solves multiple problems:

since the callback (or event) is triggered on the main thread it does not block the broker thread, nor does not have any of the Go problems.
the callback is not required since the callback itself does not do anything, it is ssl_certificate_set() that performs the change, and this function can be called spontaneously by the application at any time if it has some other means of knowing when certs need to be rotated. Very flexible.",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,3180,2020-12-15T10:02:26Z,,2021-10-21T19:48:24Z,OPEN,False,666,8,15,https://github.com/KJTsanaktsidis,Sketch of what a callback interface for TLS might look like,1,[],https://github.com/edenhill/librdkafka/pull/3180,https://github.com/KJTsanaktsidis,7,https://github.com/edenhill/librdkafka/pull/3180#issuecomment-799139098,"In #2868, (and then again in #3157), there was a discussion about how to update the SSL client certificate of a librdkafka client in a rotation scenario. Librdkafka already supports rd_kafka_conf_set_ssl_cert_verify_cb for delegating TLS CA validation checks to a client callback; in this PR, I added an analogous rd_kafka_conf_set_ssl_cert_fetch_cb option for setting a callback that provides a TLS certificate to librdkafka when a connection is opened.
The callback is called each time a TLS connection is opened, and it's expected that the application will fill the provided buffer with the TLS client certificate, TLS key, and any intermediates (up to 16, because I couldn't figure out a good way to make that number dynamic, but we can probably make something dynamic work I guess).
This is a bit of a WIP:
* Obviously whatever feedback comes along :)
* The test implementation is very rough. In particular, I wonder if all of the TLS cert generation stuff should be moved into trivup? If so I can make a PR there.
* I haven't quite figured out how to use this interface from golang - the whole point of it was I wanted to be able to plumb a golang tls.Context object into a librdkafka connection, so that we can integrate confluent-kafka-go with the golang stdlib TLS stack.
* Looks like I have a lot of CI fun to deal with.","Thanks for having a look.

Yeah, we can't really spontaneously call out (callbacks) from C to Go from threads not managed by the Go runtime

Actually you can - I got this working by using //export, see https://github.com/zendesk/confluent-kafka-go/blob/zendesk_release/kafka/tlscb_thunk.c and https://github.com/zendesk/confluent-kafka-go/blob/zendesk_release/kafka/tlscb.go. Maybe this is a bad idea for some reason I'm not aware of, but it certainly seems to work.

Is there a use-case for per-broker certs/keys?

Not that I can think of, no (maybe someone else has this use case, but we don't).
However, the interface for Golang's tls.Config object looks like this: GetClientCertificate func(*CertificateRequestInfo) (*Certificate, error). Basically, a standard Golang tls.Config object expects to be called once per connection to return a certificate for that connection. So even though I doubt anybody using librdkafka really wants to be able to use a different client certificate per broker, the callback-based interface of the Golang TLS infrastructure kind of makes the assumption that it's possible. Hopefully that makes sense?

rd_kafka_ssl_certificate_set

Would be backwards compared to how Go wants to handle TLS - that connections trigger a callback on a tls.Config object to fetch the right certificate.

Making the SSL handshake enqueue an event, etc, to gather the required certificates would indeed be possible, albeit a bit complex

Seems to be the best solution for Golang (assuming the callbacks are bad for some reason), because it would allow confluent-kafka-go to call out to GetClientCertificate in response to the event and load the right cert.

Sidebar:

This is why the go client makes use of rd_kafka_event_t's rather than callbacks, and those events are polled from queues in Go-maintained threads.

I wonder if it might be better to actually make the Golang client use a rebalance callback based on the method I described  , rather than polling for those events - that might make some of the incremental rebalance stuff a bit more straightfoward, perhaps (eg. https://github.com/confluentinc/confluent-kafka-go/blob/master/kafka/consumer.go#L413 )",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,3180,2020-12-15T10:02:26Z,,2021-10-21T19:48:24Z,OPEN,False,666,8,15,https://github.com/KJTsanaktsidis,Sketch of what a callback interface for TLS might look like,1,[],https://github.com/edenhill/librdkafka/pull/3180,https://github.com/edenhill,8,https://github.com/edenhill/librdkafka/pull/3180#issuecomment-799431453,"In #2868, (and then again in #3157), there was a discussion about how to update the SSL client certificate of a librdkafka client in a rotation scenario. Librdkafka already supports rd_kafka_conf_set_ssl_cert_verify_cb for delegating TLS CA validation checks to a client callback; in this PR, I added an analogous rd_kafka_conf_set_ssl_cert_fetch_cb option for setting a callback that provides a TLS certificate to librdkafka when a connection is opened.
The callback is called each time a TLS connection is opened, and it's expected that the application will fill the provided buffer with the TLS client certificate, TLS key, and any intermediates (up to 16, because I couldn't figure out a good way to make that number dynamic, but we can probably make something dynamic work I guess).
This is a bit of a WIP:
* Obviously whatever feedback comes along :)
* The test implementation is very rough. In particular, I wonder if all of the TLS cert generation stuff should be moved into trivup? If so I can make a PR there.
* I haven't quite figured out how to use this interface from golang - the whole point of it was I wanted to be able to plumb a golang tls.Context object into a librdkafka connection, so that we can integrate confluent-kafka-go with the golang stdlib TLS stack.
* Looks like I have a lot of CI fun to deal with.","Actually you can [call Go from unmanaged threads]..

This did not use to be the case. So, is this officially supported now?
..

Seems to be the best solution for Golang

Right, but quite costly implementation and complexity-wise, and that's need to be balanced to the benefit of such an interface, which seems mostly to be Go tls-package compliant rather than functionally.",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,3180,2020-12-15T10:02:26Z,,2021-10-21T19:48:24Z,OPEN,False,666,8,15,https://github.com/KJTsanaktsidis,Sketch of what a callback interface for TLS might look like,1,[],https://github.com/edenhill/librdkafka/pull/3180,https://github.com/KJTsanaktsidis,9,https://github.com/edenhill/librdkafka/pull/3180#issuecomment-800179014,"In #2868, (and then again in #3157), there was a discussion about how to update the SSL client certificate of a librdkafka client in a rotation scenario. Librdkafka already supports rd_kafka_conf_set_ssl_cert_verify_cb for delegating TLS CA validation checks to a client callback; in this PR, I added an analogous rd_kafka_conf_set_ssl_cert_fetch_cb option for setting a callback that provides a TLS certificate to librdkafka when a connection is opened.
The callback is called each time a TLS connection is opened, and it's expected that the application will fill the provided buffer with the TLS client certificate, TLS key, and any intermediates (up to 16, because I couldn't figure out a good way to make that number dynamic, but we can probably make something dynamic work I guess).
This is a bit of a WIP:
* Obviously whatever feedback comes along :)
* The test implementation is very rough. In particular, I wonder if all of the TLS cert generation stuff should be moved into trivup? If so I can make a PR there.
* I haven't quite figured out how to use this interface from golang - the whole point of it was I wanted to be able to plumb a golang tls.Context object into a librdkafka connection, so that we can integrate confluent-kafka-go with the golang stdlib TLS stack.
* Looks like I have a lot of CI fun to deal with.","Right, but quite costly implementation and complexity-wise, and that's need to be balanced to the benefit of such an interface, which seems mostly to be Go tls-package compliant rather than functionally.

You're absolutely right of course - I'm looking at this from a very golang-centric viewpoint but librdkafka powers a wide range of bindings and the needs of each need to be balanced with the complexity it would introduce to librdkafka.

This did not use to be the case. So, is this officially supported now?

Took a bit of digging, but yes, afaict calling Go from C-created threads is officially supported:

Original issue tracking the implementation of this: golang/go#4435
CL for the implementation: https://codereview.appspot.com/7304104
There's a testcase in the tree for a regression that affected this functionality: https://golang.org/misc/cgo/test/issue8517_windows.c

So the question is, if the calling-Go-from-C is not an issue, what's the best interface for doing dynamic TLS in librdkafka? Should it be callback based with something like rd_kafka_conf_set_ssl_cert_fetch_cb, or application driven using a rd_kafka_ssl_certificate_set function?
In our use-case, there are three reasons why I think we would prefer the rd_kafka_conf_set_ssl_cert_fetch_cb approach:

As mentioned, it integrates with Golang's tls.Config
It seems like a natural analogue to rd_kafka_conf_set_ssl_cert_verify_cb
In production at Zendesk, we run most apps, including Kafka consumers, in Kubernetes. We use consul-template (https://github.com/hashicorp/consul-template) running in a sidecar container to fetch short-lived certificates from Hashicorp Vault, and store them in a tmpfs volume accessible to our application. It's not super-trivial to get notified about this happening from the application side, because the sidecar is in a separate PID namespace and so can't do something obvious like sending SIGHUP to our app (there are of course other ways for this communication to happen e.g. over a named pipe or socket stored on the volume, or with inotify, but they all involve a bit of mucking around). With a callback approach, however, the application wouldn't need to know that new TLS certificates are available; instead, it would simply just use the latest on-disk versions of that certificate each time a connection is initiated.

(Hopefully that made sense - me know if you want me to go into this use case in a bit more detail by the way).
However - just because this works for Go, I realise there might be other languages where using a C -> lang callback interface might be a problem. What are the important targets for you? Perhaps I could do some research?",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,3180,2020-12-15T10:02:26Z,,2021-10-21T19:48:24Z,OPEN,False,666,8,15,https://github.com/KJTsanaktsidis,Sketch of what a callback interface for TLS might look like,1,[],https://github.com/edenhill/librdkafka/pull/3180,https://github.com/edenhill,10,https://github.com/edenhill/librdkafka/pull/3180#issuecomment-800284980,"In #2868, (and then again in #3157), there was a discussion about how to update the SSL client certificate of a librdkafka client in a rotation scenario. Librdkafka already supports rd_kafka_conf_set_ssl_cert_verify_cb for delegating TLS CA validation checks to a client callback; in this PR, I added an analogous rd_kafka_conf_set_ssl_cert_fetch_cb option for setting a callback that provides a TLS certificate to librdkafka when a connection is opened.
The callback is called each time a TLS connection is opened, and it's expected that the application will fill the provided buffer with the TLS client certificate, TLS key, and any intermediates (up to 16, because I couldn't figure out a good way to make that number dynamic, but we can probably make something dynamic work I guess).
This is a bit of a WIP:
* Obviously whatever feedback comes along :)
* The test implementation is very rough. In particular, I wonder if all of the TLS cert generation stuff should be moved into trivup? If so I can make a PR there.
* I haven't quite figured out how to use this interface from golang - the whole point of it was I wanted to be able to plumb a golang tls.Context object into a librdkafka connection, so that we can integrate confluent-kafka-go with the golang stdlib TLS stack.
* Looks like I have a lot of CI fun to deal with.","Took a bit of digging, but yes, afaict calling Go from C-created threads is officially supported:

That's great news!

However - just because this works for Go, I realise there might be other languages where using a C -> lang callback

Spontaneous C callbacks works fairly well in both Python and .NET, so I think your proposed ..ssl_cert_fetch_cb will work fine.
Thanks for taking the time to dig into this problem.",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,3180,2020-12-15T10:02:26Z,,2021-10-21T19:48:24Z,OPEN,False,666,8,15,https://github.com/KJTsanaktsidis,Sketch of what a callback interface for TLS might look like,1,[],https://github.com/edenhill/librdkafka/pull/3180,https://github.com/KJTsanaktsidis,11,https://github.com/edenhill/librdkafka/pull/3180#issuecomment-802364497,"In #2868, (and then again in #3157), there was a discussion about how to update the SSL client certificate of a librdkafka client in a rotation scenario. Librdkafka already supports rd_kafka_conf_set_ssl_cert_verify_cb for delegating TLS CA validation checks to a client callback; in this PR, I added an analogous rd_kafka_conf_set_ssl_cert_fetch_cb option for setting a callback that provides a TLS certificate to librdkafka when a connection is opened.
The callback is called each time a TLS connection is opened, and it's expected that the application will fill the provided buffer with the TLS client certificate, TLS key, and any intermediates (up to 16, because I couldn't figure out a good way to make that number dynamic, but we can probably make something dynamic work I guess).
This is a bit of a WIP:
* Obviously whatever feedback comes along :)
* The test implementation is very rough. In particular, I wonder if all of the TLS cert generation stuff should be moved into trivup? If so I can make a PR there.
* I haven't quite figured out how to use this interface from golang - the whole point of it was I wanted to be able to plumb a golang tls.Context object into a librdkafka connection, so that we can integrate confluent-kafka-go with the golang stdlib TLS stack.
* Looks like I have a lot of CI fun to deal with.","Cool! If we're happy to go along with the callback approach, is there anything else in this PR I should redesign now? Or should I move on to tidying up the test/trivup stuff next?",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,3180,2020-12-15T10:02:26Z,,2021-10-21T19:48:24Z,OPEN,False,666,8,15,https://github.com/KJTsanaktsidis,Sketch of what a callback interface for TLS might look like,1,[],https://github.com/edenhill/librdkafka/pull/3180,https://github.com/KJTsanaktsidis,12,https://github.com/edenhill/librdkafka/pull/3180#issuecomment-815667587,"In #2868, (and then again in #3157), there was a discussion about how to update the SSL client certificate of a librdkafka client in a rotation scenario. Librdkafka already supports rd_kafka_conf_set_ssl_cert_verify_cb for delegating TLS CA validation checks to a client callback; in this PR, I added an analogous rd_kafka_conf_set_ssl_cert_fetch_cb option for setting a callback that provides a TLS certificate to librdkafka when a connection is opened.
The callback is called each time a TLS connection is opened, and it's expected that the application will fill the provided buffer with the TLS client certificate, TLS key, and any intermediates (up to 16, because I couldn't figure out a good way to make that number dynamic, but we can probably make something dynamic work I guess).
This is a bit of a WIP:
* Obviously whatever feedback comes along :)
* The test implementation is very rough. In particular, I wonder if all of the TLS cert generation stuff should be moved into trivup? If so I can make a PR there.
* I haven't quite figured out how to use this interface from golang - the whole point of it was I wanted to be able to plumb a golang tls.Context object into a librdkafka connection, so that we can integrate confluent-kafka-go with the golang stdlib TLS stack.
* Looks like I have a lot of CI fun to deal with.","Thanks for the detailed review - all of your comments make good sense so I'm happy to switch that stuff around.
Sounds like the first step is to extract the SSL testing stuff into trivup. I'm currently working through some feedback on one of my confluent-kafka-go PR's, when I'm done with that I'll tackle this.
After that, I'll give the tests a bit of love and attention as you suggested, change the memory-management aspects ofthe interface to be based on passing around rd_kafka_malloc'd blocks of memory, and go over the code style with a fine tooth comb.
Thanks again, really appreciate it.",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,3180,2020-12-15T10:02:26Z,,2021-10-21T19:48:24Z,OPEN,False,666,8,15,https://github.com/KJTsanaktsidis,Sketch of what a callback interface for TLS might look like,1,[],https://github.com/edenhill/librdkafka/pull/3180,https://github.com/edenhill,13,https://github.com/edenhill/librdkafka/pull/3180#issuecomment-815669388,"In #2868, (and then again in #3157), there was a discussion about how to update the SSL client certificate of a librdkafka client in a rotation scenario. Librdkafka already supports rd_kafka_conf_set_ssl_cert_verify_cb for delegating TLS CA validation checks to a client callback; in this PR, I added an analogous rd_kafka_conf_set_ssl_cert_fetch_cb option for setting a callback that provides a TLS certificate to librdkafka when a connection is opened.
The callback is called each time a TLS connection is opened, and it's expected that the application will fill the provided buffer with the TLS client certificate, TLS key, and any intermediates (up to 16, because I couldn't figure out a good way to make that number dynamic, but we can probably make something dynamic work I guess).
This is a bit of a WIP:
* Obviously whatever feedback comes along :)
* The test implementation is very rough. In particular, I wonder if all of the TLS cert generation stuff should be moved into trivup? If so I can make a PR there.
* I haven't quite figured out how to use this interface from golang - the whole point of it was I wanted to be able to plumb a golang tls.Context object into a librdkafka connection, so that we can integrate confluent-kafka-go with the golang stdlib TLS stack.
* Looks like I have a lot of CI fun to deal with.","No, Thank you! 
I'm not sure you're aware but there's also the OpenSSL engine support landing in the same release, which is somewhat related to your work:
#3315",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,3180,2020-12-15T10:02:26Z,,2021-10-21T19:48:24Z,OPEN,False,666,8,15,https://github.com/KJTsanaktsidis,Sketch of what a callback interface for TLS might look like,1,[],https://github.com/edenhill/librdkafka/pull/3180,https://github.com/KJTsanaktsidis,14,https://github.com/edenhill/librdkafka/pull/3180#issuecomment-817105183,"In #2868, (and then again in #3157), there was a discussion about how to update the SSL client certificate of a librdkafka client in a rotation scenario. Librdkafka already supports rd_kafka_conf_set_ssl_cert_verify_cb for delegating TLS CA validation checks to a client callback; in this PR, I added an analogous rd_kafka_conf_set_ssl_cert_fetch_cb option for setting a callback that provides a TLS certificate to librdkafka when a connection is opened.
The callback is called each time a TLS connection is opened, and it's expected that the application will fill the provided buffer with the TLS client certificate, TLS key, and any intermediates (up to 16, because I couldn't figure out a good way to make that number dynamic, but we can probably make something dynamic work I guess).
This is a bit of a WIP:
* Obviously whatever feedback comes along :)
* The test implementation is very rough. In particular, I wonder if all of the TLS cert generation stuff should be moved into trivup? If so I can make a PR there.
* I haven't quite figured out how to use this interface from golang - the whole point of it was I wanted to be able to plumb a golang tls.Context object into a librdkafka connection, so that we can integrate confluent-kafka-go with the golang stdlib TLS stack.
* Looks like I have a lot of CI fun to deal with.","Actually the OpenSSL Engine stuff is interesting. I had a bit of a dig through the code, I think it would be possible in confluent-kafka-go to register an OpenSSL engine which provided client keys to librdkafka, and pass that as the engine_id parameter.
I couldn't see an obvious way to also have the engine overwrite the TLS server certificate verification routine, which is a bummer, but of course the existing callback can still be used for this.
Reckon that's an approach worth pursuing, or is the specific callback for loading a client cert still worth having?",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,3180,2020-12-15T10:02:26Z,,2021-10-21T19:48:24Z,OPEN,False,666,8,15,https://github.com/KJTsanaktsidis,Sketch of what a callback interface for TLS might look like,1,[],https://github.com/edenhill/librdkafka/pull/3180,https://github.com/edenhill,15,https://github.com/edenhill/librdkafka/pull/3180#issuecomment-821037370,"In #2868, (and then again in #3157), there was a discussion about how to update the SSL client certificate of a librdkafka client in a rotation scenario. Librdkafka already supports rd_kafka_conf_set_ssl_cert_verify_cb for delegating TLS CA validation checks to a client callback; in this PR, I added an analogous rd_kafka_conf_set_ssl_cert_fetch_cb option for setting a callback that provides a TLS certificate to librdkafka when a connection is opened.
The callback is called each time a TLS connection is opened, and it's expected that the application will fill the provided buffer with the TLS client certificate, TLS key, and any intermediates (up to 16, because I couldn't figure out a good way to make that number dynamic, but we can probably make something dynamic work I guess).
This is a bit of a WIP:
* Obviously whatever feedback comes along :)
* The test implementation is very rough. In particular, I wonder if all of the TLS cert generation stuff should be moved into trivup? If so I can make a PR there.
* I haven't quite figured out how to use this interface from golang - the whole point of it was I wanted to be able to plumb a golang tls.Context object into a librdkafka connection, so that we can integrate confluent-kafka-go with the golang stdlib TLS stack.
* Looks like I have a lot of CI fun to deal with.","I think it'll be cumbersome to write an engine and include that in the Go client (seeing how much work and fiddling it was to get librdkafka and all its dependencies bundled), so I think a standalone TLS interface is still warranted.",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,3180,2020-12-15T10:02:26Z,,2021-10-21T19:48:24Z,OPEN,False,666,8,15,https://github.com/KJTsanaktsidis,Sketch of what a callback interface for TLS might look like,1,[],https://github.com/edenhill/librdkafka/pull/3180,https://github.com/KJTsanaktsidis,16,https://github.com/edenhill/librdkafka/pull/3180#issuecomment-835114511,"In #2868, (and then again in #3157), there was a discussion about how to update the SSL client certificate of a librdkafka client in a rotation scenario. Librdkafka already supports rd_kafka_conf_set_ssl_cert_verify_cb for delegating TLS CA validation checks to a client callback; in this PR, I added an analogous rd_kafka_conf_set_ssl_cert_fetch_cb option for setting a callback that provides a TLS certificate to librdkafka when a connection is opened.
The callback is called each time a TLS connection is opened, and it's expected that the application will fill the provided buffer with the TLS client certificate, TLS key, and any intermediates (up to 16, because I couldn't figure out a good way to make that number dynamic, but we can probably make something dynamic work I guess).
This is a bit of a WIP:
* Obviously whatever feedback comes along :)
* The test implementation is very rough. In particular, I wonder if all of the TLS cert generation stuff should be moved into trivup? If so I can make a PR there.
* I haven't quite figured out how to use this interface from golang - the whole point of it was I wanted to be able to plumb a golang tls.Context object into a librdkafka connection, so that we can integrate confluent-kafka-go with the golang stdlib TLS stack.
* Looks like I have a lot of CI fun to deal with.","@edenhill Think this is ready for another look. I've adjusted the direction of memory ownership so that the callbacks allocate buffers with rd_kafka_mem_malloc and pass ownership of them to librdkafka, which seems to simplify things a fair bit.
I've done my best to keep the formatting consistent, but if there's a tool or configuration of some kind in the tree that fixes up the formatting a-la-rubocop or something let me know and I'll feed it through that as well.",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,3180,2020-12-15T10:02:26Z,,2021-10-21T19:48:24Z,OPEN,False,666,8,15,https://github.com/KJTsanaktsidis,Sketch of what a callback interface for TLS might look like,1,[],https://github.com/edenhill/librdkafka/pull/3180,https://github.com/KJTsanaktsidis,17,https://github.com/edenhill/librdkafka/pull/3180#issuecomment-859985755,"In #2868, (and then again in #3157), there was a discussion about how to update the SSL client certificate of a librdkafka client in a rotation scenario. Librdkafka already supports rd_kafka_conf_set_ssl_cert_verify_cb for delegating TLS CA validation checks to a client callback; in this PR, I added an analogous rd_kafka_conf_set_ssl_cert_fetch_cb option for setting a callback that provides a TLS certificate to librdkafka when a connection is opened.
The callback is called each time a TLS connection is opened, and it's expected that the application will fill the provided buffer with the TLS client certificate, TLS key, and any intermediates (up to 16, because I couldn't figure out a good way to make that number dynamic, but we can probably make something dynamic work I guess).
This is a bit of a WIP:
* Obviously whatever feedback comes along :)
* The test implementation is very rough. In particular, I wonder if all of the TLS cert generation stuff should be moved into trivup? If so I can make a PR there.
* I haven't quite figured out how to use this interface from golang - the whole point of it was I wanted to be able to plumb a golang tls.Context object into a librdkafka connection, so that we can integrate confluent-kafka-go with the golang stdlib TLS stack.
* Looks like I have a lot of CI fun to deal with.","Hey, just a polite bump @edenhill to see if you've had a chance to look at this yet? :) ",True,"{'ROCKET': ['https://github.com/zuiderkwast', 'https://github.com/jgiannuzzi']}"
edenhill/librdkafka,https://github.com/edenhill/librdkafka,3180,2020-12-15T10:02:26Z,,2021-10-21T19:48:24Z,OPEN,False,666,8,15,https://github.com/KJTsanaktsidis,Sketch of what a callback interface for TLS might look like,1,[],https://github.com/edenhill/librdkafka/pull/3180,https://github.com/KJTsanaktsidis,18,https://github.com/edenhill/librdkafka/pull/3180#issuecomment-897357367,"In #2868, (and then again in #3157), there was a discussion about how to update the SSL client certificate of a librdkafka client in a rotation scenario. Librdkafka already supports rd_kafka_conf_set_ssl_cert_verify_cb for delegating TLS CA validation checks to a client callback; in this PR, I added an analogous rd_kafka_conf_set_ssl_cert_fetch_cb option for setting a callback that provides a TLS certificate to librdkafka when a connection is opened.
The callback is called each time a TLS connection is opened, and it's expected that the application will fill the provided buffer with the TLS client certificate, TLS key, and any intermediates (up to 16, because I couldn't figure out a good way to make that number dynamic, but we can probably make something dynamic work I guess).
This is a bit of a WIP:
* Obviously whatever feedback comes along :)
* The test implementation is very rough. In particular, I wonder if all of the TLS cert generation stuff should be moved into trivup? If so I can make a PR there.
* I haven't quite figured out how to use this interface from golang - the whole point of it was I wanted to be able to plumb a golang tls.Context object into a librdkafka connection, so that we can integrate confluent-kafka-go with the golang stdlib TLS stack.
* Looks like I have a lot of CI fun to deal with.",@edenhill just wondering if you had any further thoughts on this?,True,{'THUMBS_UP': ['https://github.com/zuiderkwast']}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,3180,2020-12-15T10:02:26Z,,2021-10-21T19:48:24Z,OPEN,False,666,8,15,https://github.com/KJTsanaktsidis,Sketch of what a callback interface for TLS might look like,1,[],https://github.com/edenhill/librdkafka/pull/3180,https://github.com/KJTsanaktsidis,19,https://github.com/edenhill/librdkafka/pull/3180#issuecomment-932846033,"In #2868, (and then again in #3157), there was a discussion about how to update the SSL client certificate of a librdkafka client in a rotation scenario. Librdkafka already supports rd_kafka_conf_set_ssl_cert_verify_cb for delegating TLS CA validation checks to a client callback; in this PR, I added an analogous rd_kafka_conf_set_ssl_cert_fetch_cb option for setting a callback that provides a TLS certificate to librdkafka when a connection is opened.
The callback is called each time a TLS connection is opened, and it's expected that the application will fill the provided buffer with the TLS client certificate, TLS key, and any intermediates (up to 16, because I couldn't figure out a good way to make that number dynamic, but we can probably make something dynamic work I guess).
This is a bit of a WIP:
* Obviously whatever feedback comes along :)
* The test implementation is very rough. In particular, I wonder if all of the TLS cert generation stuff should be moved into trivup? If so I can make a PR there.
* I haven't quite figured out how to use this interface from golang - the whole point of it was I wanted to be able to plumb a golang tls.Context object into a librdkafka connection, so that we can integrate confluent-kafka-go with the golang stdlib TLS stack.
* Looks like I have a lot of CI fun to deal with.","Weve been using this patch in production for a couple of months now and it works great.
What language are you using librdkafka for? If Golang I can recommend the above patch for confluent-kafka-go too.",True,{'THUMBS_UP': ['https://github.com/bjosv']}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,3187,2020-12-21T23:18:27Z,2020-12-23T11:22:30Z,2021-01-01T11:45:03Z,MERGED,True,2247,809,34,https://github.com/edenhill,KIP-447,10,[],https://github.com/edenhill/librdkafka/pull/3187,https://github.com/edenhill,1,https://github.com/edenhill/librdkafka/pull/3187,,,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,3187,2020-12-21T23:18:27Z,2020-12-23T11:22:30Z,2021-01-01T11:45:03Z,MERGED,True,2247,809,34,https://github.com/edenhill,KIP-447,10,[],https://github.com/edenhill/librdkafka/pull/3187,https://github.com/edenhill,2,https://github.com/edenhill/librdkafka/pull/3187#issuecomment-750168787,,Thank you for the review!,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,3197,2021-01-04T11:20:16Z,2021-01-04T18:28:45Z,2021-01-04T18:28:49Z,MERGED,True,7,12,5,https://github.com/edenhill,Revert transaction.timeout.ms change,4,[],https://github.com/edenhill/librdkafka/pull/3197,https://github.com/edenhill,1,https://github.com/edenhill/librdkafka/pull/3197,Turns out the KIP-447 recommendation to reduce transaction.timeout.ms is no longer true.,Turns out the KIP-447 recommendation to reduce transaction.timeout.ms is no longer true.,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,3199,2021-01-05T11:21:50Z,2021-03-25T08:22:17Z,2021-03-25T08:22:20Z,MERGED,True,29,0,6,https://github.com/snar,add FreeBSD variant of pthread_setname_np,2,[],https://github.com/edenhill/librdkafka/pull/3199,https://github.com/snar,1,https://github.com/edenhill/librdkafka/pull/3199,"FreeBSD uses a bit different variant of pthread_set_name_np, this patch
allows use of this function.","FreeBSD uses a bit different variant of pthread_set_name_np, this patch
allows use of this function.",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,3199,2021-01-05T11:21:50Z,2021-03-25T08:22:17Z,2021-03-25T08:22:20Z,MERGED,True,29,0,6,https://github.com/snar,add FreeBSD variant of pthread_setname_np,2,[],https://github.com/edenhill/librdkafka/pull/3199,https://github.com/edenhill,2,https://github.com/edenhill/librdkafka/pull/3199#issuecomment-806458806,"FreeBSD uses a bit different variant of pthread_set_name_np, this patch
allows use of this function.",Thank you!,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,3206,2021-01-08T19:05:07Z,2021-02-24T13:44:24Z,2021-02-24T13:44:24Z,MERGED,True,1,1,1,https://github.com/rettakjak,Include sys/wait.h for OpenBSD,1,[],https://github.com/edenhill/librdkafka/pull/3206,https://github.com/rettakjak,1,https://github.com/edenhill/librdkafka/pull/3206,"Like FreeBSD, OpenBSD needs to include sys/wait.h for macros like WIFSIGNALED.","Like FreeBSD, OpenBSD needs to include sys/wait.h for macros like WIFSIGNALED.",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,3206,2021-01-08T19:05:07Z,2021-02-24T13:44:24Z,2021-02-24T13:44:24Z,MERGED,True,1,1,1,https://github.com/rettakjak,Include sys/wait.h for OpenBSD,1,[],https://github.com/edenhill/librdkafka/pull/3206,https://github.com/rettakjak,2,https://github.com/edenhill/librdkafka/pull/3206#issuecomment-757957827,"Like FreeBSD, OpenBSD needs to include sys/wait.h for macros like WIFSIGNALED.","sys/wait.h needs to be included when on OpenBSD in order to build librdkafka.  I see that a CI build failed though.  Looking at the failure and at the failures in other PRs, I think this is a typical sort of failure.  Is that correct?",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,3206,2021-01-08T19:05:07Z,2021-02-24T13:44:24Z,2021-02-24T13:44:24Z,MERGED,True,1,1,1,https://github.com/rettakjak,Include sys/wait.h for OpenBSD,1,[],https://github.com/edenhill/librdkafka/pull/3206,https://github.com/rettakjak,3,https://github.com/edenhill/librdkafka/pull/3206#issuecomment-785065265,"Like FreeBSD, OpenBSD needs to include sys/wait.h for macros like WIFSIGNALED.",@edenhill friendly ping,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,3206,2021-01-08T19:05:07Z,2021-02-24T13:44:24Z,2021-02-24T13:44:24Z,MERGED,True,1,1,1,https://github.com/rettakjak,Include sys/wait.h for OpenBSD,1,[],https://github.com/edenhill/librdkafka/pull/3206,https://github.com/edenhill,4,https://github.com/edenhill/librdkafka/pull/3206#issuecomment-785084814,"Like FreeBSD, OpenBSD needs to include sys/wait.h for macros like WIFSIGNALED.",Thank you!,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,3209,2021-01-11T07:54:20Z,2021-01-11T15:16:24Z,2021-01-11T15:16:28Z,MERGED,True,2,2,2,https://github.com/edenhill,Update OpenSSL to 1.1.1i,1,[],https://github.com/edenhill/librdkafka/pull/3209,https://github.com/edenhill,1,https://github.com/edenhill/librdkafka/pull/3209,,,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,3211,2021-01-11T11:25:31Z,2021-01-14T08:37:17Z,2021-01-14T08:37:20Z,MERGED,True,44,57,6,https://github.com/edenhill,Misc 1.6.0,2,[],https://github.com/edenhill/librdkafka/pull/3211,https://github.com/edenhill,1,https://github.com/edenhill/librdkafka/pull/3211,,,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,3218,2021-01-18T11:06:26Z,2021-01-18T13:42:59Z,2021-01-18T13:43:02Z,MERGED,True,99,9,5,https://github.com/edenhill,Fix cgrp commit cnt and session timeout enforcement,2,[],https://github.com/edenhill/librdkafka/pull/3218,https://github.com/edenhill,1,https://github.com/edenhill/librdkafka/pull/3218,,,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,3219,2021-01-18T11:16:19Z,2021-01-18T13:43:35Z,2021-01-18T13:43:39Z,MERGED,True,72,5,4,https://github.com/edenhill,Build pypa/manylinux2010 to replace debian9 build in nuget librdkafka.redist,1,[],https://github.com/edenhill/librdkafka/pull/3219,https://github.com/edenhill,1,https://github.com/edenhill/librdkafka/pull/3219,,,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,3220,2021-01-19T02:11:38Z,2021-02-09T06:16:36Z,2021-02-09T06:16:36Z,CLOSED,False,10,1,1,https://github.com/mamapanda,Check for rd_kafka_snappy_init_env_sg failure,1,[],https://github.com/edenhill/librdkafka/pull/3220,https://github.com/mamapanda,1,https://github.com/edenhill/librdkafka/pull/3220,I saw this and thought it might be more robust to check rd_kafka_snappy_init_env_sg for errors here before proceeding.,I saw this and thought it might be more robust to check rd_kafka_snappy_init_env_sg for errors here before proceeding.,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,3220,2021-01-19T02:11:38Z,2021-02-09T06:16:36Z,2021-02-09T06:16:36Z,CLOSED,False,10,1,1,https://github.com/mamapanda,Check for rd_kafka_snappy_init_env_sg failure,1,[],https://github.com/edenhill/librdkafka/pull/3220,https://github.com/edenhill,2,https://github.com/edenhill/librdkafka/pull/3220#issuecomment-762642764,I saw this and thought it might be more robust to check rd_kafka_snappy_init_env_sg for errors here before proceeding.,"Did you see this fail?
The only reason they would fail is the process has run out of virtual memory space, in which case it is unlikely that the log will be able to allocate its necessary memory and everything will crash.",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,3220,2021-01-19T02:11:38Z,2021-02-09T06:16:36Z,2021-02-09T06:16:36Z,CLOSED,False,10,1,1,https://github.com/mamapanda,Check for rd_kafka_snappy_init_env_sg failure,1,[],https://github.com/edenhill/librdkafka/pull/3220,https://github.com/mamapanda,3,https://github.com/edenhill/librdkafka/pull/3220#issuecomment-762644899,I saw this and thought it might be more robust to check rd_kafka_snappy_init_env_sg for errors here before proceeding.,I did not. I happened to come across this for a different reason and thought it should've been checked. My bad if I was mistaken!,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,3227,2021-01-22T19:50:53Z,2021-01-25T22:25:20Z,2021-01-25T22:25:24Z,MERGED,True,979,159,18,https://github.com/edenhill,Transactional producer fixes,18,[],https://github.com/edenhill/librdkafka/pull/3227,https://github.com/edenhill,1,https://github.com/edenhill/librdkafka/pull/3227,,,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,3227,2021-01-22T19:50:53Z,2021-01-25T22:25:20Z,2021-01-25T22:25:24Z,MERGED,True,979,159,18,https://github.com/edenhill,Transactional producer fixes,18,[],https://github.com/edenhill/librdkafka/pull/3227,https://github.com/edenhill,2,https://github.com/edenhill/librdkafka/pull/3227#issuecomment-765647938,,The commit log is probably useful to understand the changes.,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,3233,2021-01-28T11:04:09Z,2021-03-09T13:24:40Z,2021-03-09T23:36:22Z,MERGED,True,3,0,1,https://github.com/SpaceIm,CMake: append sasl lib folder for linker if found through pkg-config,1,[],https://github.com/edenhill/librdkafka/pull/3233,https://github.com/SpaceIm,1,https://github.com/edenhill/librdkafka/pull/3233,closes #3232,closes #3232,True,{'HEART': ['https://github.com/prince-chrismc']}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,3233,2021-01-28T11:04:09Z,2021-03-09T13:24:40Z,2021-03-09T23:36:22Z,MERGED,True,3,0,1,https://github.com/SpaceIm,CMake: append sasl lib folder for linker if found through pkg-config,1,[],https://github.com/edenhill/librdkafka/pull/3233,https://github.com/edenhill,2,https://github.com/edenhill/librdkafka/pull/3233#issuecomment-768980574,closes #3232,"Thank you, since Cmake is not officially supported and I don't know enough about it to provide a valuable code review, please ping (@ - mention) some of the previous contributors of the CMake stuff.",True,{'THUMBS_UP': ['https://github.com/SpaceIm']}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,3233,2021-01-28T11:04:09Z,2021-03-09T13:24:40Z,2021-03-09T23:36:22Z,MERGED,True,3,0,1,https://github.com/SpaceIm,CMake: append sasl lib folder for linker if found through pkg-config,1,[],https://github.com/edenhill/librdkafka/pull/3233,https://github.com/SpaceIm,3,https://github.com/edenhill/librdkafka/pull/3233#issuecomment-768981308,closes #3232,ping @neptoess @masariello @omartijn @benesch @ed-alertedh,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,3233,2021-01-28T11:04:09Z,2021-03-09T13:24:40Z,2021-03-09T23:36:22Z,MERGED,True,3,0,1,https://github.com/SpaceIm,CMake: append sasl lib folder for linker if found through pkg-config,1,[],https://github.com/edenhill/librdkafka/pull/3233,https://github.com/neptoess,4,https://github.com/edenhill/librdkafka/pull/3233#issuecomment-769048177,closes #3232,"@edenhill
This looks good to me (https://cmake.org/cmake/help/v3.2/module/FindPkgConfig.html will be useful for anyone else reviewing).
@SpaceIm
I'm curious what platform you're on, since I only use CMake for Windows builds.",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,3233,2021-01-28T11:04:09Z,2021-03-09T13:24:40Z,2021-03-09T23:36:22Z,MERGED,True,3,0,1,https://github.com/SpaceIm,CMake: append sasl lib folder for linker if found through pkg-config,1,[],https://github.com/edenhill/librdkafka/pull/3233,https://github.com/SpaceIm,5,https://github.com/edenhill/librdkafka/pull/3233#issuecomment-769114858,closes #3232,"@neptoess This issue was seen on Ubuntu conan-io/conan-center-index#4325 by a user of conan (C/C++ package manager). conan recipe of librdkafka uses CMake build files (easier to integrate for all platforms).
This link is more relevant, since min CMake version in build files is 3.2: https://cmake.org/cmake/help/v3.2/module/FindPkgConfig.html (it means we can't use imported targets of pkg_check_modules introduced in 3.6, which is far more robust).",True,{'THUMBS_UP': ['https://github.com/neptoess']}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,3233,2021-01-28T11:04:09Z,2021-03-09T13:24:40Z,2021-03-09T23:36:22Z,MERGED,True,3,0,1,https://github.com/SpaceIm,CMake: append sasl lib folder for linker if found through pkg-config,1,[],https://github.com/edenhill/librdkafka/pull/3233,https://github.com/edenhill,6,https://github.com/edenhill/librdkafka/pull/3233#issuecomment-793894320,closes #3232,Thank you!,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,3233,2021-01-28T11:04:09Z,2021-03-09T13:24:40Z,2021-03-09T23:36:22Z,MERGED,True,3,0,1,https://github.com/SpaceIm,CMake: append sasl lib folder for linker if found through pkg-config,1,[],https://github.com/edenhill/librdkafka/pull/3233,https://github.com/edbordin,7,https://github.com/edenhill/librdkafka/pull/3233#issuecomment-794607969,closes #3232,I've decided to try to retire my work account and just use my personal one for everything (which seems to be pretty standard). I'll still get notifications for ed-alertedh but in future please try to ping me on this one instead if you can remember!,True,{'THUMBS_UP': ['https://github.com/edenhill']}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,3235,2021-01-28T23:31:48Z,2021-03-09T13:27:14Z,2021-03-09T13:27:14Z,MERGED,True,1,1,1,https://github.com/hallfox,sticky_assignor: Fix incorrect access of map entry,1,[],https://github.com/edenhill/librdkafka/pull/3235,https://github.com/hallfox,1,https://github.com/edenhill/librdkafka/pull/3235,"The balance function accesses a key (typed as a const char*) instead of
the likely intended value, based on the type actually being casted. This
seems to mean that initializing is always set to false, and it is
causing a unaligned memory access on Solaris 64-bit builds.","The balance function accesses a key (typed as a const char*) instead of
the likely intended value, based on the type actually being casted. This
seems to mean that initializing is always set to false, and it is
causing a unaligned memory access on Solaris 64-bit builds.",True,"{'THUMBS_UP': ['https://github.com/gridaphobe', 'https://github.com/chrisbeard', 'https://github.com/shanson7']}"
edenhill/librdkafka,https://github.com/edenhill/librdkafka,3236,2021-01-29T16:27:19Z,2021-03-15T16:15:15Z,2021-03-15T16:15:15Z,CLOSED,False,98,28,3,https://github.com/ajbarb,Add loopback sock resiliency,1,[],https://github.com/edenhill/librdkafka/pull/3236,https://github.com/ajbarb,1,https://github.com/edenhill/librdkafka/pull/3236,This change adds recovery action for rebuilding the loopback sockets if they get into bad state. Resolve issue #3139.,This change adds recovery action for rebuilding the loopback sockets if they get into bad state. Resolve issue #3139.,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,3236,2021-01-29T16:27:19Z,2021-03-15T16:15:15Z,2021-03-15T16:15:15Z,CLOSED,False,98,28,3,https://github.com/ajbarb,Add loopback sock resiliency,1,[],https://github.com/edenhill/librdkafka/pull/3236,https://github.com/ajbarb,2,https://github.com/edenhill/librdkafka/pull/3236#issuecomment-795938137,This change adds recovery action for rebuilding the loopback sockets if they get into bad state. Resolve issue #3139.,"@edenhill, any chance this can be added to current milestone? I will be happy to address any changes needed asap.",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,3236,2021-01-29T16:27:19Z,2021-03-15T16:15:15Z,2021-03-15T16:15:15Z,CLOSED,False,98,28,3,https://github.com/ajbarb,Add loopback sock resiliency,1,[],https://github.com/edenhill/librdkafka/pull/3236,https://github.com/edenhill,3,https://github.com/edenhill/librdkafka/pull/3236#issuecomment-796641073,This change adds recovery action for rebuilding the loopback sockets if they get into bad state. Resolve issue #3139.,Do we know why these localhost connections are being closed in the first place?,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,3236,2021-01-29T16:27:19Z,2021-03-15T16:15:15Z,2021-03-15T16:15:15Z,CLOSED,False,98,28,3,https://github.com/ajbarb,Add loopback sock resiliency,1,[],https://github.com/edenhill/librdkafka/pull/3236,https://github.com/ajbarb,4,https://github.com/edenhill/librdkafka/pull/3236#issuecomment-798718269,This change adds recovery action for rebuilding the loopback sockets if they get into bad state. Resolve issue #3139.,"Do we know why these localhost connections are being closed in the first place?

No I tried looking at the event logs and couldn't find anything. One other way to debug this was to enable the TCP/IP ETW traces on a port level since they could be huge. Since the ports are dynamic here I couldn't pursue it. At this point the adding resiliency made more sense.",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,3236,2021-01-29T16:27:19Z,2021-03-15T16:15:15Z,2021-03-15T16:15:15Z,CLOSED,False,98,28,3,https://github.com/ajbarb,Add loopback sock resiliency,1,[],https://github.com/edenhill/librdkafka/pull/3236,https://github.com/edenhill,5,https://github.com/edenhill/librdkafka/pull/3236#issuecomment-799394421,This change adds recovery action for rebuilding the loopback sockets if they get into bad state. Resolve issue #3139.,"I don't agree, without finding the root cause we don't know if this is the best, or even proper solution to the problem.
As the fix involves state in the fast-path we'll want to be careful and not jump to a seemingly ok fix without knowing if that's really the case.",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,3236,2021-01-29T16:27:19Z,2021-03-15T16:15:15Z,2021-03-15T16:15:15Z,CLOSED,False,98,28,3,https://github.com/ajbarb,Add loopback sock resiliency,1,[],https://github.com/edenhill/librdkafka/pull/3236,https://github.com/ajbarb,6,https://github.com/edenhill/librdkafka/pull/3236#issuecomment-799540482,This change adds recovery action for rebuilding the loopback sockets if they get into bad state. Resolve issue #3139.,"I don't agree, without finding the root cause we don't know if this is the best, or even proper solution to the problem.
As the fix involves state in the fast-path we'll want to be careful and not jump to a seemingly ok fix without knowing if that's really the case.

I see. Do you have any recommendation on if something could be done about the revents that are not being handled today:
rktrans->rktrans_pfd[1].revents  // POLLERR and POLLHUP
I feel it doesn't seem right to let the poll spin the way it is doing irrespective of root cause since this code is vulnerable to this missing events. What is an okay action here ignoring the recovery part that I tried.",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,3236,2021-01-29T16:27:19Z,2021-03-15T16:15:15Z,2021-03-15T16:15:15Z,CLOSED,False,98,28,3,https://github.com/ajbarb,Add loopback sock resiliency,1,[],https://github.com/edenhill/librdkafka/pull/3236,https://github.com/edenhill,7,https://github.com/edenhill/librdkafka/pull/3236#issuecomment-799542621,This change adds recovery action for rebuilding the loopback sockets if they get into bad state. Resolve issue #3139.,"I think we should close the socket on POLLHUP for sure, and log an error to help people troubleshoot it.
But I'd rather not see us add the re-setup code until we understand the issue in more detail.",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,3236,2021-01-29T16:27:19Z,2021-03-15T16:15:15Z,2021-03-15T16:15:15Z,CLOSED,False,98,28,3,https://github.com/ajbarb,Add loopback sock resiliency,1,[],https://github.com/edenhill/librdkafka/pull/3236,https://github.com/ajbarb,8,https://github.com/edenhill/librdkafka/pull/3236#issuecomment-799547708,This change adds recovery action for rebuilding the loopback sockets if they get into bad state. Resolve issue #3139.,"I think we should close the socket on POLLHUP for sure, and log an error to help people troubleshoot it.
But I'd rather not see us add the re-setup code until we understand the issue in more detail.

Sounds good. Let me explore closing the socket and logging an error. Will do that in a separate PR. Thank you!",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,3237,2021-01-29T17:22:27Z,2021-03-15T16:20:21Z,2021-03-15T16:29:23Z,MERGED,True,13,12,3,https://github.com/ajbarb,Increment the backoff exponentially,4,[],https://github.com/edenhill/librdkafka/pull/3237,https://github.com/ajbarb,1,https://github.com/edenhill/librdkafka/pull/3237,Address issue #3166,Address issue #3166,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,3237,2021-01-29T17:22:27Z,2021-03-15T16:20:21Z,2021-03-15T16:29:23Z,MERGED,True,13,12,3,https://github.com/ajbarb,Increment the backoff exponentially,4,[],https://github.com/edenhill/librdkafka/pull/3237,https://github.com/edenhill,2,https://github.com/edenhill/librdkafka/pull/3237#issuecomment-799551758,Address issue #3166,Thank you!,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,3238,2021-01-29T17:26:40Z,2021-03-11T10:16:31Z,2021-03-11T10:16:31Z,MERGED,True,11,4,1,https://github.com/ajbarb,Force address resolution if the broker epoch changes,4,[],https://github.com/edenhill/librdkafka/pull/3238,https://github.com/ajbarb,1,https://github.com/edenhill/librdkafka/pull/3238,Resolves #2944,Resolves #2944,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,3238,2021-01-29T17:26:40Z,2021-03-11T10:16:31Z,2021-03-11T10:16:31Z,MERGED,True,11,4,1,https://github.com/ajbarb,Force address resolution if the broker epoch changes,4,[],https://github.com/edenhill/librdkafka/pull/3238,https://github.com/edenhill,2,https://github.com/edenhill/librdkafka/pull/3238#issuecomment-793917537,Resolves #2944,"Is this fix really verified? forceResetAddress does not seem to be used in the resolve function.
I think a simpler approach is simply to reset rkb_ts_rsal_last to 0.",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,3238,2021-01-29T17:26:40Z,2021-03-11T10:16:31Z,2021-03-11T10:16:31Z,MERGED,True,11,4,1,https://github.com/ajbarb,Force address resolution if the broker epoch changes,4,[],https://github.com/edenhill/librdkafka/pull/3238,https://github.com/ajbarb,3,https://github.com/edenhill/librdkafka/pull/3238#issuecomment-794586934,Resolves #2944,"Is this fix really verified? forceResetAddress does not seem to be used in the resolve function.
I think a simpler approach is simply to reset rkb_ts_rsal_last to 0.

I missed to move a change. I updated the PR and yeah the change is verified. My worry with setting rkb_ts_rsal_last to zero is that it depends on an user config broker_addr_ttl and also how rd_clock is implemented.",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,3240,2021-01-31T10:54:06Z,2021-03-11T13:20:18Z,2021-03-11T13:20:18Z,CLOSED,False,17,15,6,https://github.com/pavel-pimenov,Fix PVS Studio (part-1),1,[],https://github.com/edenhill/librdkafka/pull/3240,https://github.com/pavel-pimenov,1,https://github.com/edenhill/librdkafka/pull/3240,#3190,#3190,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,3240,2021-01-31T10:54:06Z,2021-03-11T13:20:18Z,2021-03-11T13:20:18Z,CLOSED,False,17,15,6,https://github.com/pavel-pimenov,Fix PVS Studio (part-1),1,[],https://github.com/edenhill/librdkafka/pull/3240,https://github.com/edenhill,2,https://github.com/edenhill/librdkafka/pull/3240#issuecomment-796730418,#3190,"Most of these are false positives in the current code's calling conventions, and others were fixed in a recent PR.",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,3245,2021-02-03T18:48:49Z,2021-02-05T16:48:06Z,2021-02-05T16:48:09Z,MERGED,True,175,13,6,https://github.com/edenhill,Txn crash fix (E4424),3,[],https://github.com/edenhill/librdkafka/pull/3245,https://github.com/edenhill,1,https://github.com/edenhill/librdkafka/pull/3245,,,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,3247,2021-02-05T17:35:43Z,2021-02-10T09:09:35Z,2021-03-09T06:59:47Z,MERGED,True,468,23,11,https://github.com/edenhill,Fix multiple firings of coord_req leading to crash (ESC4444),3,[],https://github.com/edenhill/librdkafka/pull/3247,https://github.com/edenhill,1,https://github.com/edenhill/librdkafka/pull/3247,"It was hairy to get the testcase to trigger the chain of events needed to reproduce the issue, so bear with me on the 0105 test additions.","It was hairy to get the testcase to trigger the chain of events needed to reproduce the issue, so bear with me on the 0105 test additions.",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,3249,2021-02-07T23:42:10Z,2021-02-08T06:52:23Z,2021-02-08T06:52:28Z,MERGED,True,4,0,1,https://github.com/benesch,Set WITH_STATIC_LIB_libcrypto appropriately in CMake build system,1,[],https://github.com/edenhill/librdkafka/pull/3249,https://github.com/benesch,1,https://github.com/edenhill/librdkafka/pull/3249,"This define is set by the mklove build system when linking against a
static copy of OpenSSL and controls some important SSL initialization
behavior. This commit teaches the CMake build system to do the same.","This define is set by the mklove build system when linking against a
static copy of OpenSSL and controls some important SSL initialization
behavior. This commit teaches the CMake build system to do the same.",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,3249,2021-02-07T23:42:10Z,2021-02-08T06:52:23Z,2021-02-08T06:52:28Z,MERGED,True,4,0,1,https://github.com/benesch,Set WITH_STATIC_LIB_libcrypto appropriately in CMake build system,1,[],https://github.com/edenhill/librdkafka/pull/3249,https://github.com/edenhill,2,https://github.com/edenhill/librdkafka/pull/3249#issuecomment-774916733,"This define is set by the mklove build system when linking against a
static copy of OpenSSL and controls some important SSL initialization
behavior. This commit teaches the CMake build system to do the same.",Thank you!,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,3251,2021-02-08T19:26:40Z,2021-02-16T09:07:24Z,2021-02-16T09:07:24Z,MERGED,True,2,0,1,https://github.com/tfricks,Update va_list reference to match the other functions and fix an embedded compile issue.,1,[],https://github.com/edenhill/librdkafka/pull/3251,https://github.com/tfricks,1,https://github.com/edenhill/librdkafka/pull/3251,"Some embedded toolchains (uClibc MIPS in this case) will not define va_list before the rdkafka_error.h is loaded. A va_list undefined error stops the compile. Mimicking the other functions in the header, using the ""..."" syntax, resolves the compile issue with these toolchains, while not breaking other platforms.","Some embedded toolchains (uClibc MIPS in this case) will not define va_list before the rdkafka_error.h is loaded. A va_list undefined error stops the compile. Mimicking the other functions in the header, using the ""..."" syntax, resolves the compile issue with these toolchains, while not breaking other platforms.",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,3251,2021-02-08T19:26:40Z,2021-02-16T09:07:24Z,2021-02-16T09:07:24Z,MERGED,True,2,0,1,https://github.com/tfricks,Update va_list reference to match the other functions and fix an embedded compile issue.,1,[],https://github.com/edenhill/librdkafka/pull/3251,https://github.com/edenhill,2,https://github.com/edenhill/librdkafka/pull/3251#issuecomment-775803875,"Some embedded toolchains (uClibc MIPS in this case) will not define va_list before the rdkafka_error.h is loaded. A va_list undefined error stops the compile. Mimicking the other functions in the header, using the ""..."" syntax, resolves the compile issue with these toolchains, while not breaking other platforms.","I don't believe ... and va_list are the same thing and can't be used interchangably, they might be on some platforms, but probably not portable.
It sounds like the original problem is an include order issue, so can you try to fix the include of stdarg.h in the correct place instead?",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,3251,2021-02-08T19:26:40Z,2021-02-16T09:07:24Z,2021-02-16T09:07:24Z,MERGED,True,2,0,1,https://github.com/tfricks,Update va_list reference to match the other functions and fix an embedded compile issue.,1,[],https://github.com/edenhill/librdkafka/pull/3251,https://github.com/tfricks,3,https://github.com/edenhill/librdkafka/pull/3251#issuecomment-776082226,"Some embedded toolchains (uClibc MIPS in this case) will not define va_list before the rdkafka_error.h is loaded. A va_list undefined error stops the compile. Mimicking the other functions in the header, using the ""..."" syntax, resolves the compile issue with these toolchains, while not breaking other platforms.","Seems like stdargs.h must be included before rdkafka_error.h or va_list isn't defined. I updated to the include method as suggested. I am not sure why 32 bit machines were failing. I had just copied the other functions in the header and C file. Thanks for taking a look.
I suppose just importing stdargs.h inside of rdkafka_error.h may work too. What do you think?",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,3251,2021-02-08T19:26:40Z,2021-02-16T09:07:24Z,2021-02-16T09:07:24Z,MERGED,True,2,0,1,https://github.com/tfricks,Update va_list reference to match the other functions and fix an embedded compile issue.,1,[],https://github.com/edenhill/librdkafka/pull/3251,https://github.com/edenhill,4,https://github.com/edenhill/librdkafka/pull/3251#issuecomment-776556789,"Some embedded toolchains (uClibc MIPS in this case) will not define va_list before the rdkafka_error.h is loaded. A va_list undefined error stops the compile. Mimicking the other functions in the header, using the ""..."" syntax, resolves the compile issue with these toolchains, while not breaking other platforms.",Can you show the original build error (in its entirety) without any modifications made?,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,3251,2021-02-08T19:26:40Z,2021-02-16T09:07:24Z,2021-02-16T09:07:24Z,MERGED,True,2,0,1,https://github.com/tfricks,Update va_list reference to match the other functions and fix an embedded compile issue.,1,[],https://github.com/edenhill/librdkafka/pull/3251,https://github.com/tfricks,5,https://github.com/edenhill/librdkafka/pull/3251#issuecomment-776823020,"Some embedded toolchains (uClibc MIPS in this case) will not define va_list before the rdkafka_error.h is loaded. A va_list undefined error stops the compile. Mimicking the other functions in the header, using the ""..."" syntax, resolves the compile issue with these toolchains, while not breaking other platforms.","I get the same error for all files that I changed before importing <stdarg.h>
mipsel-linux-uclibc-gcc -MD -MP -g -O2 -fPIC -Wall -Wsign-compare -Wfloat-equal -Wpointer-arith -Wcast-align -DLIBRDKAFKA_GIT_VERSION=""\""v1.6.0\""""  -c rdkafka.c -o rdkafka.o
mipsel-linux-uclibc-gcc -MD -MP -g -O2 -fPIC -Wall -Wsign-compare -Wfloat-equal -Wpointer-arith -Wcast-align -DLIBRDKAFKA_GIT_VERSION=""\""v1.6.0\""""  -c rdkafka_broker.c -o rdkafka_broker.o
mipsel-linux-uclibc-gcc -MD -MP -g -O2 -fPIC -Wall -Wsign-compare -Wfloat-equal -Wpointer-arith -Wcast-align -DLIBRDKAFKA_GIT_VERSION=""\""v1.6.0\""""  -c rdkafka_msg.c -o rdkafka_msg.o
In file included from rdkafka_msg.c:38:0:
rdkafka_error.h:51:58: error: expected declaration specifiers or '...' before 'va_list'
../mklove/Makefile.base:97: recipe for target 'rdkafka_msg.o' failed
make[2]: *** [rdkafka_msg.o] Error 1
make[2]: Leaving directory '/home/USER/src/hello-kafka/lib/librdkafka/src'
Makefile:27: recipe for target 'libs' failed
make[1]: *** [libs] Error 2
make[1]: Leaving directory '/home/USER/src/hello-kafka/lib/librdkafka'",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,3251,2021-02-08T19:26:40Z,2021-02-16T09:07:24Z,2021-02-16T09:07:24Z,MERGED,True,2,0,1,https://github.com/tfricks,Update va_list reference to match the other functions and fix an embedded compile issue.,1,[],https://github.com/edenhill/librdkafka/pull/3251,https://github.com/edenhill,6,https://github.com/edenhill/librdkafka/pull/3251#issuecomment-776825666,"Some embedded toolchains (uClibc MIPS in this case) will not define va_list before the rdkafka_error.h is loaded. A va_list undefined error stops the compile. Mimicking the other functions in the header, using the ""..."" syntax, resolves the compile issue with these toolchains, while not breaking other platforms.",Try adding stdarg.h include to rdkafka_error.h,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,3251,2021-02-08T19:26:40Z,2021-02-16T09:07:24Z,2021-02-16T09:07:24Z,MERGED,True,2,0,1,https://github.com/tfricks,Update va_list reference to match the other functions and fix an embedded compile issue.,1,[],https://github.com/edenhill/librdkafka/pull/3251,https://github.com/tfricks,7,https://github.com/edenhill/librdkafka/pull/3251#issuecomment-776842116,"Some embedded toolchains (uClibc MIPS in this case) will not define va_list before the rdkafka_error.h is loaded. A va_list undefined error stops the compile. Mimicking the other functions in the header, using the ""..."" syntax, resolves the compile issue with these toolchains, while not breaking other platforms.","That compiles as well, thanks.",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,3251,2021-02-08T19:26:40Z,2021-02-16T09:07:24Z,2021-02-16T09:07:24Z,MERGED,True,2,0,1,https://github.com/tfricks,Update va_list reference to match the other functions and fix an embedded compile issue.,1,[],https://github.com/edenhill/librdkafka/pull/3251,https://github.com/tfricks,8,https://github.com/edenhill/librdkafka/pull/3251#issuecomment-779337308,"Some embedded toolchains (uClibc MIPS in this case) will not define va_list before the rdkafka_error.h is loaded. A va_list undefined error stops the compile. Mimicking the other functions in the header, using the ""..."" syntax, resolves the compile issue with these toolchains, while not breaking other platforms.",Is travis failing a normal thing? Do I need to do anything else?,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,3251,2021-02-08T19:26:40Z,2021-02-16T09:07:24Z,2021-02-16T09:07:24Z,MERGED,True,2,0,1,https://github.com/tfricks,Update va_list reference to match the other functions and fix an embedded compile issue.,1,[],https://github.com/edenhill/librdkafka/pull/3251,https://github.com/edenhill,9,https://github.com/edenhill/librdkafka/pull/3251#issuecomment-779692380,"Some embedded toolchains (uClibc MIPS in this case) will not define va_list before the rdkafka_error.h is loaded. A va_list undefined error stops the compile. Mimicking the other functions in the header, using the ""..."" syntax, resolves the compile issue with these toolchains, while not breaking other platforms.","The CI failures are unrelated.
Thank you for this fix!",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,3256,2021-02-10T14:16:02Z,2021-02-11T15:49:10Z,2021-03-09T06:59:46Z,MERGED,True,268,49,6,https://github.com/edenhill,ESC-4410: Partitions stuck in PEND_TXN state if multiple AddPartitionsToTxn reqs are sent,2,[],https://github.com/edenhill/librdkafka/pull/3256,https://github.com/edenhill,1,https://github.com/edenhill/librdkafka/pull/3256,"This also fixes an issue with partition list not remaining sorted in these cases, which could affect subsequent AddPartitionsToTxn reqs.","This also fixes an issue with partition list not remaining sorted in these cases, which could affect subsequent AddPartitionsToTxn reqs.",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,3258,2021-02-11T11:43:00Z,2021-02-11T12:05:17Z,2021-02-11T12:07:04Z,MERGED,True,3,3,1,https://github.com/sanjay24,transactions example: minor bug fixes (#3257),1,[],https://github.com/edenhill/librdkafka/pull/3258,https://github.com/sanjay24,1,https://github.com/edenhill/librdkafka/pull/3258,"Abort transcation if partitions are lost and commit if they are revoked.
Correctly iterate over all the offsets in the list","Abort transcation if partitions are lost and commit if they are revoked.
Correctly iterate over all the offsets in the list",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,3258,2021-02-11T11:43:00Z,2021-02-11T12:05:17Z,2021-02-11T12:07:04Z,MERGED,True,3,3,1,https://github.com/sanjay24,transactions example: minor bug fixes (#3257),1,[],https://github.com/edenhill/librdkafka/pull/3258,https://github.com/edenhill,2,https://github.com/edenhill/librdkafka/pull/3258#issuecomment-777404819,"Abort transcation if partitions are lost and commit if they are revoked.
Correctly iterate over all the offsets in the list",Thank you!,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,3259,2021-02-11T15:44:36Z,,2021-03-31T08:03:04Z,OPEN,False,2750,53,188,https://github.com/AlexeiBaranov,Port to IBMi OS400,33,[],https://github.com/edenhill/librdkafka/pull/3259,https://github.com/AlexeiBaranov,1,https://github.com/edenhill/librdkafka/pull/3259,"Port to IBM i OS400
The proposed changes are required to be able to build, test and use librdkafka on the native IBM i operating system - OS400.
All necessary changes are isolated using #ifndef/ifdef with predefined IBM i macro OS400","Port to IBM i OS400
The proposed changes are required to be able to build, test and use librdkafka on the native IBM i operating system - OS400.
All necessary changes are isolated using #ifndef/ifdef with predefined IBM i macro OS400",True,"{'THUMBS_UP': ['https://github.com/m1h43l', 'https://github.com/tullyelly', 'https://github.com/martintosney', 'https://github.com/jkyeung', 'https://github.com/worksofliam', 'https://github.com/holterandreasb', 'https://github.com/GajenderTyagi']}"
edenhill/librdkafka,https://github.com/edenhill/librdkafka,3259,2021-02-11T15:44:36Z,,2021-03-31T08:03:04Z,OPEN,False,2750,53,188,https://github.com/AlexeiBaranov,Port to IBMi OS400,33,[],https://github.com/edenhill/librdkafka/pull/3259,https://github.com/edenhill,2,https://github.com/edenhill/librdkafka/pull/3259#issuecomment-780446485,"Port to IBM i OS400
The proposed changes are required to be able to build, test and use librdkafka on the native IBM i operating system - OS400.
All necessary changes are isolated using #ifndef/ifdef with predefined IBM i macro OS400","Interesting PR.
I will not have time to review this in detail yet, but from a short skim..:

Move the os400 directory to packaging/os400
There's got to be a better way to add that char-pragma than to modify every single source file. Is there a compiler flag to icc? Or is it sufficient to add it to rdposix.h?
Maintain the librdkafka code style, see https://github.com/edenhill/librdkafka/blob/master/CONTRIBUTING.md

This is still a niche platform so if the changes become too intrusive we'll probably hold off on merging in which case a fork might be a better option. But we'll see when we get closer to the finish line.
Also, this port will not be officially supported since we don't have access to, or knowledge of, IBMi OS400.",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,3259,2021-02-11T15:44:36Z,,2021-03-31T08:03:04Z,OPEN,False,2750,53,188,https://github.com/AlexeiBaranov,Port to IBMi OS400,33,[],https://github.com/edenhill/librdkafka/pull/3259,https://github.com/AlexeiBaranov,3,https://github.com/edenhill/librdkafka/pull/3259#issuecomment-780521956,"Port to IBM i OS400
The proposed changes are required to be able to build, test and use librdkafka on the native IBM i operating system - OS400.
All necessary changes are isolated using #ifndef/ifdef with predefined IBM i macro OS400","Move the os400 directory to packaging/os400
done
 There's got to be a better way to add that char-pragma than to modify every single source file. Is there a compiler flag to icc? Or is it sufficient to add it to rdposix.h?
Unfortunately this ugly way is the only one. icc does not process correctly the encoding parameter from the command line, and can use the encoding that set at the very beginning of the *.c file. Even if you change the encoding in the included *.h file, this setting will be valid only until the end of this *.h file
 Maintain the librdkafka code style, see https://github.com/edenhill/librdkafka/blob/master/CONTRIBUTING.md
More or less done, updated sources are committed

I can reduce ""ifdef hell"" a bit. In some places, I had to move to #ifdef some code fragments, where it was enough to make slightly change, for example:
TEST_ASSERT(rkev); - icc can't compile this line - it think that second parameter is required
TEST_ASSERT(rkev, """"); - compiled by icc without error
Obviously, such a change will not affect the existing code. But for now, I put it in a separate #ifdef for clarity.
The same thing with static char buffers - they could be extended in the master code to fit strings produced by OS400 functions like snprintf(""...%p..."")

This is still a niche platform so if the changes become too intrusive we'll probably hold off on merging in which case a fork might be a better option. But we'll see when we get closer to the finish line.
Also, this port will not be officially supported since we don't have access to, or knowledge of, IBMi OS400.

I understand and agree.",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,3260,2021-02-12T09:30:07Z,2021-02-12T10:44:15Z,2021-02-12T10:44:15Z,MERGED,True,3,3,1,https://github.com/sanjay24,tansactions example: do not use consumer handle for txn api (#3257),4,[],https://github.com/edenhill/librdkafka/pull/3260,https://github.com/sanjay24,1,https://github.com/edenhill/librdkafka/pull/3260,"rd_kafka_begin_transaction called with consumer handle, should be producer instead","rd_kafka_begin_transaction called with consumer handle, should be producer instead",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,3260,2021-02-12T09:30:07Z,2021-02-12T10:44:15Z,2021-02-12T10:44:15Z,MERGED,True,3,3,1,https://github.com/sanjay24,tansactions example: do not use consumer handle for txn api (#3257),4,[],https://github.com/edenhill/librdkafka/pull/3260,https://github.com/edenhill,2,https://github.com/edenhill/librdkafka/pull/3260#issuecomment-778119749,"rd_kafka_begin_transaction called with consumer handle, should be producer instead","Ouch, thanks!",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,3264,2021-02-15T09:30:13Z,2021-02-17T07:01:00Z,2021-02-17T07:01:04Z,MERGED,True,397,35,10,https://github.com/edenhill,Treat idempotent producer fatal errors as transactional producer fatal errors,2,[],https://github.com/edenhill/librdkafka/pull/3264,https://github.com/edenhill,1,https://github.com/edenhill/librdkafka/pull/3264,,,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,3267,2021-02-16T18:09:59Z,2021-02-17T07:02:51Z,2021-02-17T07:02:53Z,MERGED,True,60,21,10,https://github.com/edenhill,Make auto.offset.reset=error distinguishable from other consumer errors,1,[],https://github.com/edenhill/librdkafka/pull/3267,https://github.com/edenhill,1,https://github.com/edenhill/librdkafka/pull/3267,"Prior to this change the original error was raised, which did not give the
application a chance to differentiate between other consumer errors and
auto.offset.reset=error.
This one is well overdue.","Prior to this change the original error was raised, which did not give the
application a chance to differentiate between other consumer errors and
auto.offset.reset=error.
This one is well overdue.",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,3269,2021-02-18T02:12:16Z,2021-04-13T15:01:20Z,2021-04-13T15:01:21Z,MERGED,True,275,6,9,https://github.com/jliunyu,Clean buffer after rebalancing for batch queue,18,[],https://github.com/edenhill/librdkafka/pull/3269,https://github.com/jliunyu,1,https://github.com/edenhill/librdkafka/pull/3269,"Summary: This is bug fix for confluentinc/confluent-kafka-python#1013
Issue: Buffer is not cleaned after rebalance if messages are polled using the batch queue method, so the consumer will still get old messages.
Solution: when assign happens, a new op event with type RD_KAFKA_OP_BARRIER will be created, a new version is been created at mean time. If the consumer met this event, will clean the buffer by comparing the version of msgs and the new version just created.
Test passed in local test:
Testing:
[0122_buffer_cleaning_after_rebalance/ 62.283s] ================= Test 0122_buffer_cleaning_after_rebalance PASSED =================
[                      / 63.161s] ALL-TESTS: duration 63160.668ms
TEST 20210224223610 (bare, scenario default) SUMMARY
#==================================================================#
|                                    |     PASSED |  63.161s |
| 0122_buffer_cleaning_after_rebalance     |     PASSED |  62.283s |
#==================================================================#
[                      / 63.163s] 0 thread(s) in use by librdkafka
[                      / 63.163s]
============== ALL TESTS PASSED ==============

./test-runner in bare mode PASSED!","Summary: This is bug fix for confluentinc/confluent-kafka-python#1013
Issue: Buffer is not cleaned after rebalance if messages are polled using the batch queue method, so the consumer will still get old messages.
Solution: when assign happens, a new op event with type RD_KAFKA_OP_BARRIER will be created, a new version is been created at mean time. If the consumer met this event, will clean the buffer by comparing the version of msgs and the new version just created.
Test passed in local test:
Testing:
[0122_buffer_cleaning_after_rebalance/ 62.283s] ================= Test 0122_buffer_cleaning_after_rebalance PASSED =================
[                      / 63.161s] ALL-TESTS: duration 63160.668ms
TEST 20210224223610 (bare, scenario default) SUMMARY
#==================================================================#
|                                    |     PASSED |  63.161s |
| 0122_buffer_cleaning_after_rebalance     |     PASSED |  62.283s |
#==================================================================#
[                      / 63.163s] 0 thread(s) in use by librdkafka
[                      / 63.163s]
============== ALL TESTS PASSED ==============

./test-runner in bare mode PASSED!",True,{'HOORAY': ['https://github.com/olejorgenb']}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,3271,2021-02-18T10:56:35Z,2021-02-19T21:19:15Z,2021-02-19T21:19:15Z,MERGED,True,152,14,8,https://github.com/edenhill,Allow empty transactions to be committed,1,[],https://github.com/edenhill/librdkafka/pull/3271,https://github.com/edenhill,1,https://github.com/edenhill/librdkafka/pull/3271,,,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,3272,2021-02-18T14:55:46Z,2021-02-22T07:37:11Z,2021-02-22T07:37:15Z,MERGED,True,285,22,12,https://github.com/edenhill,Retry fetching committed offsets indefinitely (#3265),2,[],https://github.com/edenhill/librdkafka/pull/3272,https://github.com/edenhill,1,https://github.com/edenhill/librdkafka/pull/3272,,,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,3274,2021-02-24T07:33:21Z,2021-02-24T08:35:46Z,2021-02-24T08:35:57Z,MERGED,True,9,2,3,https://github.com/edenhill,Default ssl.ca.location to probe on OSX,1,[],https://github.com/edenhill/librdkafka/pull/3274,https://github.com/edenhill,1,https://github.com/edenhill/librdkafka/pull/3274,Let's get this into 1.6.1,Let's get this into 1.6.1,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,3274,2021-02-24T07:33:21Z,2021-02-24T08:35:46Z,2021-02-24T08:35:57Z,MERGED,True,9,2,3,https://github.com/edenhill,Default ssl.ca.location to probe on OSX,1,[],https://github.com/edenhill/librdkafka/pull/3274,https://github.com/edenhill,2,https://github.com/edenhill/librdkafka/pull/3274#issuecomment-784905385,Let's get this into 1.6.1,Thanks The Ryan!,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,3275,2021-02-24T13:57:54Z,2021-03-25T12:06:04Z,2021-03-25T12:06:07Z,MERGED,True,1071,336,34,https://github.com/edenhill,KIP-360: Make idemp fatal errors recoverable for txn producer,9,[],https://github.com/edenhill/librdkafka/pull/3275,https://github.com/edenhill,1,https://github.com/edenhill/librdkafka/pull/3275,,,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,3285,2021-03-08T18:41:36Z,2021-03-08T20:06:10Z,2021-03-09T06:59:44Z,MERGED,True,32,8,4,https://github.com/edenhill,Don't destroy metadata cache until brokers are destroyed (#3279),1,[],https://github.com/edenhill/librdkafka/pull/3285,https://github.com/edenhill,1,https://github.com/edenhill/librdkafka/pull/3285,As ops on broker queues may have references to the metadata cache's mutex,As ops on broker queues may have references to the metadata cache's mutex,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,3286,2021-03-08T18:44:22Z,2021-03-09T06:59:30Z,2021-03-09T06:59:33Z,MERGED,True,395,11,19,https://github.com/edenhill,Add connections.max.idle.ms,1,[],https://github.com/edenhill/librdkafka/pull/3286,https://github.com/edenhill,1,https://github.com/edenhill/librdkafka/pull/3286,,,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,3287,2021-03-09T10:04:19Z,2021-03-10T13:49:41Z,2021-03-10T13:49:41Z,MERGED,True,12,3,2,https://github.com/edenhill,Make broker_update() blocking to assure state is synchronized,1,[],https://github.com/edenhill/librdkafka/pull/3287,https://github.com/edenhill,1,https://github.com/edenhill/librdkafka/pull/3287,As reported in confluentinc/confluent-kafka-python#1043,As reported in confluentinc/confluent-kafka-python#1043,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,3288,2021-03-09T10:52:34Z,2021-03-10T14:04:50Z,2021-03-10T15:53:25Z,MERGED,True,95,43,16,https://github.com/edenhill,PVS studio fixes,6,[],https://github.com/edenhill/librdkafka/pull/3288,https://github.com/edenhill,1,https://github.com/edenhill/librdkafka/pull/3288,,,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,3289,2021-03-09T11:19:12Z,2021-03-10T14:06:43Z,2021-03-10T15:55:44Z,MERGED,True,3,3,2,https://github.com/edenhill,Change connections.max.idle.ms default on Azure to <4 minutes,1,[],https://github.com/edenhill/librdkafka/pull/3289,https://github.com/edenhill,1,https://github.com/edenhill/librdkafka/pull/3289,..since the load balancer default timeout is 4 minutes.,..since the load balancer default timeout is 4 minutes.,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,3289,2021-03-09T11:19:12Z,2021-03-10T14:06:43Z,2021-03-10T15:55:44Z,MERGED,True,3,3,2,https://github.com/edenhill,Change connections.max.idle.ms default on Azure to <4 minutes,1,[],https://github.com/edenhill/librdkafka/pull/3289,https://github.com/edenhill,2,https://github.com/edenhill/librdkafka/pull/3289#issuecomment-793739614,..since the load balancer default timeout is 4 minutes.,"Turns out the default timeout is 4 minutes on azure:
https://docs.microsoft.com/en-us/azure/load-balancer/load-balancer-tcp-reset#configurable-tcp-idle-timeout",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,3289,2021-03-09T11:19:12Z,2021-03-10T14:06:43Z,2021-03-10T15:55:44Z,MERGED,True,3,3,2,https://github.com/edenhill,Change connections.max.idle.ms default on Azure to <4 minutes,1,[],https://github.com/edenhill/librdkafka/pull/3289,https://github.com/edenhill,3,https://github.com/edenhill/librdkafka/pull/3289#issuecomment-795458549,..since the load balancer default timeout is 4 minutes.,"WONDER NO MORE
   LIKE YOU DID BEFORE
 NOW IT IS FOUR",True,"{'LAUGH': ['https://github.com/will118', 'https://github.com/mhowlett']}"
edenhill/librdkafka,https://github.com/edenhill/librdkafka,3298,2021-03-15T15:59:33Z,2021-03-18T16:02:42Z,2021-03-18T16:02:42Z,MERGED,True,38,12,7,https://github.com/edenhill,Cgrp metadata cache fix and some test fixes,4,[],https://github.com/edenhill/librdkafka/pull/3298,https://github.com/edenhill,1,https://github.com/edenhill/librdkafka/pull/3298,,,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,3307,2021-03-21T06:14:54Z,2021-03-23T08:06:35Z,2021-04-17T07:36:36Z,MERGED,True,1,0,1,https://github.com/pavel-pimenov,* include wincrypt.h,1,[],https://github.com/edenhill/librdkafka/pull/3307,https://github.com/pavel-pimenov,1,https://github.com/edenhill/librdkafka/pull/3307,"Severity	Code	Description	Project	File	Line	Suppression State
Error (active)	E0020	identifier ""CERT_STORE_OPEN_EXISTING_FLAG"" is undefined
Error (active)	E0020	identifier ""CERT_STORE_PROV_SYSTEM"" is undefined
Error (active)	E0020	identifier ""CERT_STORE_READONLY_FLAG"" is undefined
Error (active)	E0020	identifier ""CERT_SYSTEM_STORE_CURRENT_USER""
Error (active)	E0020	identifier ""HCERTSTORE"" is undefined
Error (active)	E0020	identifier ""PCCERT_CONTEXT"" is undefined","Severity	Code	Description	Project	File	Line	Suppression State
Error (active)	E0020	identifier ""CERT_STORE_OPEN_EXISTING_FLAG"" is undefined
Error (active)	E0020	identifier ""CERT_STORE_PROV_SYSTEM"" is undefined
Error (active)	E0020	identifier ""CERT_STORE_READONLY_FLAG"" is undefined
Error (active)	E0020	identifier ""CERT_SYSTEM_STORE_CURRENT_USER""
Error (active)	E0020	identifier ""HCERTSTORE"" is undefined
Error (active)	E0020	identifier ""PCCERT_CONTEXT"" is undefined",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,3307,2021-03-21T06:14:54Z,2021-03-23T08:06:35Z,2021-04-17T07:36:36Z,MERGED,True,1,0,1,https://github.com/pavel-pimenov,* include wincrypt.h,1,[],https://github.com/edenhill/librdkafka/pull/3307,https://github.com/edenhill,2,https://github.com/edenhill/librdkafka/pull/3307#issuecomment-804115504,"Severity	Code	Description	Project	File	Line	Suppression State
Error (active)	E0020	identifier ""CERT_STORE_OPEN_EXISTING_FLAG"" is undefined
Error (active)	E0020	identifier ""CERT_STORE_PROV_SYSTEM"" is undefined
Error (active)	E0020	identifier ""CERT_STORE_READONLY_FLAG"" is undefined
Error (active)	E0020	identifier ""CERT_SYSTEM_STORE_CURRENT_USER""
Error (active)	E0020	identifier ""HCERTSTORE"" is undefined
Error (active)	E0020	identifier ""PCCERT_CONTEXT"" is undefined","What is reporting these errors, seeing how VS successfully builds it as-is?",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,3307,2021-03-21T06:14:54Z,2021-03-23T08:06:35Z,2021-04-17T07:36:36Z,MERGED,True,1,0,1,https://github.com/pavel-pimenov,* include wincrypt.h,1,[],https://github.com/edenhill/librdkafka/pull/3307,https://github.com/pavel-pimenov,3,https://github.com/edenhill/librdkafka/pull/3307#issuecomment-804598143,"Severity	Code	Description	Project	File	Line	Suppression State
Error (active)	E0020	identifier ""CERT_STORE_OPEN_EXISTING_FLAG"" is undefined
Error (active)	E0020	identifier ""CERT_STORE_PROV_SYSTEM"" is undefined
Error (active)	E0020	identifier ""CERT_STORE_READONLY_FLAG"" is undefined
Error (active)	E0020	identifier ""CERT_SYSTEM_STORE_CURRENT_USER""
Error (active)	E0020	identifier ""HCERTSTORE"" is undefined
Error (active)	E0020	identifier ""PCCERT_CONTEXT"" is undefined",,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,3307,2021-03-21T06:14:54Z,2021-03-23T08:06:35Z,2021-04-17T07:36:36Z,MERGED,True,1,0,1,https://github.com/pavel-pimenov,* include wincrypt.h,1,[],https://github.com/edenhill/librdkafka/pull/3307,https://github.com/edenhill,4,https://github.com/edenhill/librdkafka/pull/3307#issuecomment-804691353,"Severity	Code	Description	Project	File	Line	Suppression State
Error (active)	E0020	identifier ""CERT_STORE_OPEN_EXISTING_FLAG"" is undefined
Error (active)	E0020	identifier ""CERT_STORE_PROV_SYSTEM"" is undefined
Error (active)	E0020	identifier ""CERT_STORE_READONLY_FLAG"" is undefined
Error (active)	E0020	identifier ""CERT_SYSTEM_STORE_CURRENT_USER""
Error (active)	E0020	identifier ""HCERTSTORE"" is undefined
Error (active)	E0020	identifier ""PCCERT_CONTEXT"" is undefined","Interesting, what VS version?",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,3307,2021-03-21T06:14:54Z,2021-03-23T08:06:35Z,2021-04-17T07:36:36Z,MERGED,True,1,0,1,https://github.com/pavel-pimenov,* include wincrypt.h,1,[],https://github.com/edenhill/librdkafka/pull/3307,https://github.com/pavel-pimenov,5,https://github.com/edenhill/librdkafka/pull/3307#issuecomment-804701316,"Severity	Code	Description	Project	File	Line	Suppression State
Error (active)	E0020	identifier ""CERT_STORE_OPEN_EXISTING_FLAG"" is undefined
Error (active)	E0020	identifier ""CERT_STORE_PROV_SYSTEM"" is undefined
Error (active)	E0020	identifier ""CERT_STORE_READONLY_FLAG"" is undefined
Error (active)	E0020	identifier ""CERT_SYSTEM_STORE_CURRENT_USER""
Error (active)	E0020	identifier ""HCERTSTORE"" is undefined
Error (active)	E0020	identifier ""PCCERT_CONTEXT"" is undefined",,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,3307,2021-03-21T06:14:54Z,2021-03-23T08:06:35Z,2021-04-17T07:36:36Z,MERGED,True,1,0,1,https://github.com/pavel-pimenov,* include wincrypt.h,1,[],https://github.com/edenhill/librdkafka/pull/3307,https://github.com/edenhill,6,https://github.com/edenhill/librdkafka/pull/3307#issuecomment-804702622,"Severity	Code	Description	Project	File	Line	Suppression State
Error (active)	E0020	identifier ""CERT_STORE_OPEN_EXISTING_FLAG"" is undefined
Error (active)	E0020	identifier ""CERT_STORE_PROV_SYSTEM"" is undefined
Error (active)	E0020	identifier ""CERT_STORE_READONLY_FLAG"" is undefined
Error (active)	E0020	identifier ""CERT_SYSTEM_STORE_CURRENT_USER""
Error (active)	E0020	identifier ""HCERTSTORE"" is undefined
Error (active)	E0020	identifier ""PCCERT_CONTEXT"" is undefined","Okay, good to know. Thanks ",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,3308,2021-03-21T06:21:54Z,2021-03-22T14:45:36Z,2021-04-17T07:36:32Z,CLOSED,False,3,2,1,https://github.com/pavel-pimenov,fix 'apis' pointer was used after the memory was releaase,1,[],https://github.com/edenhill/librdkafka/pull/3308,https://github.com/pavel-pimenov,1,https://github.com/edenhill/librdkafka/pull/3308,V774 [CWE-416] The 'apis' pointer was used after the memory was released. rdkafka_broker.c 2450,V774 [CWE-416] The 'apis' pointer was used after the memory was released. rdkafka_broker.c 2450,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,3308,2021-03-21T06:21:54Z,2021-03-22T14:45:36Z,2021-04-17T07:36:32Z,CLOSED,False,3,2,1,https://github.com/pavel-pimenov,fix 'apis' pointer was used after the memory was releaase,1,[],https://github.com/edenhill/librdkafka/pull/3308,https://github.com/edenhill,2,https://github.com/edenhill/librdkafka/pull/3308#issuecomment-804119029,V774 [CWE-416] The 'apis' pointer was used after the memory was released. rdkafka_broker.c 2450,"False positive, since err is set the function will return and apis will not be used.",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,3309,2021-03-21T06:24:55Z,2021-04-06T10:37:30Z,2021-04-17T07:36:32Z,CLOSED,False,1,0,1,https://github.com/pavel-pimenov,V796 [CWE-484] It is possible that 'break' statement,1,[],https://github.com/edenhill/librdkafka/pull/3309,https://github.com/pavel-pimenov,1,https://github.com/edenhill/librdkafka/pull/3309,...is missing in switch statement. rdkafka_conf.c 3250,...is missing in switch statement. rdkafka_conf.c 3250,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,3310,2021-03-21T06:29:14Z,2021-03-25T08:19:58Z,2021-04-17T07:36:31Z,CLOSED,False,3,3,2,https://github.com/pavel-pimenov,Fix uninitialized variable,1,[],https://github.com/edenhill/librdkafka/pull/3310,https://github.com/pavel-pimenov,1,https://github.com/edenhill/librdkafka/pull/3310,"V614 [CWE-457] Potentially uninitialized variable 'CoordId' used. rdkafka_cgrp.c 589
V614 [CWE-457] Potentially uninitialized variable 'CoordPort' used. rdkafka_cgrp.c 591
V614 [CWE-457] Potentially uninitialized variable 'is_modified' used. rdkafka_conf.c 4207","V614 [CWE-457] Potentially uninitialized variable 'CoordId' used. rdkafka_cgrp.c 589
V614 [CWE-457] Potentially uninitialized variable 'CoordPort' used. rdkafka_cgrp.c 591
V614 [CWE-457] Potentially uninitialized variable 'is_modified' used. rdkafka_conf.c 4207",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,3310,2021-03-21T06:29:14Z,2021-03-25T08:19:58Z,2021-04-17T07:36:31Z,CLOSED,False,3,3,2,https://github.com/pavel-pimenov,Fix uninitialized variable,1,[],https://github.com/edenhill/librdkafka/pull/3310,https://github.com/edenhill,2,https://github.com/edenhill/librdkafka/pull/3310#issuecomment-804117348,"V614 [CWE-457] Potentially uninitialized variable 'CoordId' used. rdkafka_cgrp.c 589
V614 [CWE-457] Potentially uninitialized variable 'CoordPort' used. rdkafka_cgrp.c 591
V614 [CWE-457] Potentially uninitialized variable 'is_modified' used. rdkafka_conf.c 4207","Both of these are false positives, and while it might help silence some warnings it is a disservice to the code reader since the initialization suggests that these variables are used without being set first, which is false.",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,3313,2021-03-22T20:58:47Z,2021-04-06T00:39:48Z,2021-04-06T00:39:48Z,CLOSED,False,10,3,3,https://github.com/mhowlett,Prevent auto-commit during rebalance,1,[],https://github.com/edenhill/librdkafka/pull/3313,https://github.com/mhowlett,1,https://github.com/edenhill/librdkafka/pull/3313,"resolves the commit fail issue reported in #3306 by disallowing auto commits whilst a rebalance is in progress.
does not resolve the partition shuffling issue.","resolves the commit fail issue reported in #3306 by disallowing auto commits whilst a rebalance is in progress.
does not resolve the partition shuffling issue.",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,3313,2021-03-22T20:58:47Z,2021-04-06T00:39:48Z,2021-04-06T00:39:48Z,CLOSED,False,10,3,3,https://github.com/mhowlett,Prevent auto-commit during rebalance,1,[],https://github.com/edenhill/librdkafka/pull/3313,https://github.com/mhowlett,2,https://github.com/edenhill/librdkafka/pull/3313#issuecomment-804398863,"resolves the commit fail issue reported in #3306 by disallowing auto commits whilst a rebalance is in progress.
does not resolve the partition shuffling issue.","on second thoughts, manual committing will need consideration as well. WIP.",True,"{'HEART': ['https://github.com/sgjurano', 'https://github.com/oronsh'], 'THUMBS_UP': ['https://github.com/JB-doogls', 'https://github.com/sgjurano', 'https://github.com/oronsh']}"
edenhill/librdkafka,https://github.com/edenhill/librdkafka,3313,2021-03-22T20:58:47Z,2021-04-06T00:39:48Z,2021-04-06T00:39:48Z,CLOSED,False,10,3,3,https://github.com/mhowlett,Prevent auto-commit during rebalance,1,[],https://github.com/edenhill/librdkafka/pull/3313,https://github.com/mhowlett,3,https://github.com/edenhill/librdkafka/pull/3313#issuecomment-813742745,"resolves the commit fail issue reported in #3306 by disallowing auto commits whilst a rebalance is in progress.
does not resolve the partition shuffling issue.",see #3226,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,3315,2021-03-23T06:15:27Z,2021-04-08T17:56:48Z,2021-04-08T17:57:24Z,MERGED,True,720,1,17,https://github.com/adinigam,Adding OpenSSL engine support in LibrdKafka for SSL connections,43,[],https://github.com/edenhill/librdkafka/pull/3315,https://github.com/adinigam,1,https://github.com/edenhill/librdkafka/pull/3315,"Currently LibrdKafka uses Openssl behind the scenes to establish SSL channel between broker and client.
To create the SSL channel, OpenSSL needs public and private keys. Today these keys are provided in byte* format to openssl. Byte* can either be generated by librdkafka or application itself. But ultimately to generate the byte* format, keys needs to be exported from where-ever it is residing.
In many scenarios exporting might not be an option since it might not be permitted by hardware/software, eg: if application is using non-exportable certificates in windows certificate store.
Good news is OpenSSL provides plugin model which is called engine. This engine can be used to interact with the storage and use the public and private keys for the certificate without exporting in byte* format. Such clients will have to write custom engines.
I am proposing to enhance LibrdKafka to support OpenSSL engine. Below are the changes I am taking up in this PR.


Add new parameter ""ssl.engine.location"" which will take the full path to the binary containing the openssl engine


This engine will be initialized if the parameter ""ssl.engine.location"" value is not null. Below is the initialization code in the method rd_kafka_ssl_ctx_init


Call engine method ENGINE_load_ssl_client_cert with appropriate params to fetch the pointers to the public and private keys. Key parameter is building and disposing the stack of ca-certs in rd_kafka_ssl_set_certs.


Note: this is a fresh Pr on #2602","Currently LibrdKafka uses Openssl behind the scenes to establish SSL channel between broker and client.
To create the SSL channel, OpenSSL needs public and private keys. Today these keys are provided in byte* format to openssl. Byte* can either be generated by librdkafka or application itself. But ultimately to generate the byte* format, keys needs to be exported from where-ever it is residing.
In many scenarios exporting might not be an option since it might not be permitted by hardware/software, eg: if application is using non-exportable certificates in windows certificate store.
Good news is OpenSSL provides plugin model which is called engine. This engine can be used to interact with the storage and use the public and private keys for the certificate without exporting in byte* format. Such clients will have to write custom engines.
I am proposing to enhance LibrdKafka to support OpenSSL engine. Below are the changes I am taking up in this PR.


Add new parameter ""ssl.engine.location"" which will take the full path to the binary containing the openssl engine


This engine will be initialized if the parameter ""ssl.engine.location"" value is not null. Below is the initialization code in the method rd_kafka_ssl_ctx_init


Call engine method ENGINE_load_ssl_client_cert with appropriate params to fetch the pointers to the public and private keys. Key parameter is building and disposing the stack of ca-certs in rd_kafka_ssl_set_certs.


Note: this is a fresh Pr on #2602",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,3315,2021-03-23T06:15:27Z,2021-04-08T17:56:48Z,2021-04-08T17:57:24Z,MERGED,True,720,1,17,https://github.com/adinigam,Adding OpenSSL engine support in LibrdKafka for SSL connections,43,[],https://github.com/edenhill/librdkafka/pull/3315,https://github.com/adinigam,2,https://github.com/edenhill/librdkafka/pull/3315#issuecomment-804654158,"Currently LibrdKafka uses Openssl behind the scenes to establish SSL channel between broker and client.
To create the SSL channel, OpenSSL needs public and private keys. Today these keys are provided in byte* format to openssl. Byte* can either be generated by librdkafka or application itself. But ultimately to generate the byte* format, keys needs to be exported from where-ever it is residing.
In many scenarios exporting might not be an option since it might not be permitted by hardware/software, eg: if application is using non-exportable certificates in windows certificate store.
Good news is OpenSSL provides plugin model which is called engine. This engine can be used to interact with the storage and use the public and private keys for the certificate without exporting in byte* format. Such clients will have to write custom engines.
I am proposing to enhance LibrdKafka to support OpenSSL engine. Below are the changes I am taking up in this PR.


Add new parameter ""ssl.engine.location"" which will take the full path to the binary containing the openssl engine


This engine will be initialized if the parameter ""ssl.engine.location"" value is not null. Below is the initialization code in the method rd_kafka_ssl_ctx_init


Call engine method ENGINE_load_ssl_client_cert with appropriate params to fetch the pointers to the public and private keys. Key parameter is building and disposing the stack of ca-certs in rd_kafka_ssl_set_certs.


Note: this is a fresh Pr on #2602","@edenhill For some reason I was unable to rebase to latest master, hence sending fresh PR on latest master.
But just curious to know even when all comments were resolved and all gates were green why was the merge being pushed further and further (1.5.2 -> 1.6 -> 1.7) almost for an year ?
Just asking this question if i could have done anything to speed up my checkins.",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,3315,2021-03-23T06:15:27Z,2021-04-08T17:56:48Z,2021-04-08T17:57:24Z,MERGED,True,720,1,17,https://github.com/adinigam,Adding OpenSSL engine support in LibrdKafka for SSL connections,43,[],https://github.com/edenhill/librdkafka/pull/3315,https://github.com/edenhill,3,https://github.com/edenhill/librdkafka/pull/3315#issuecomment-813875536,"Currently LibrdKafka uses Openssl behind the scenes to establish SSL channel between broker and client.
To create the SSL channel, OpenSSL needs public and private keys. Today these keys are provided in byte* format to openssl. Byte* can either be generated by librdkafka or application itself. But ultimately to generate the byte* format, keys needs to be exported from where-ever it is residing.
In many scenarios exporting might not be an option since it might not be permitted by hardware/software, eg: if application is using non-exportable certificates in windows certificate store.
Good news is OpenSSL provides plugin model which is called engine. This engine can be used to interact with the storage and use the public and private keys for the certificate without exporting in byte* format. Such clients will have to write custom engines.
I am proposing to enhance LibrdKafka to support OpenSSL engine. Below are the changes I am taking up in this PR.


Add new parameter ""ssl.engine.location"" which will take the full path to the binary containing the openssl engine


This engine will be initialized if the parameter ""ssl.engine.location"" value is not null. Below is the initialization code in the method rd_kafka_ssl_ctx_init


Call engine method ENGINE_load_ssl_client_cert with appropriate params to fetch the pointers to the public and private keys. Key parameter is building and disposing the stack of ca-certs in rd_kafka_ssl_set_certs.


Note: this is a fresh Pr on #2602",The PR comments (here and in the closed PR) need to be addressed this week for this feature to be eligible for the upcoming v1.7.0 release.,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,3315,2021-03-23T06:15:27Z,2021-04-08T17:56:48Z,2021-04-08T17:57:24Z,MERGED,True,720,1,17,https://github.com/adinigam,Adding OpenSSL engine support in LibrdKafka for SSL connections,43,[],https://github.com/edenhill/librdkafka/pull/3315,https://github.com/adinigam,4,https://github.com/edenhill/librdkafka/pull/3315#issuecomment-814541950,"Currently LibrdKafka uses Openssl behind the scenes to establish SSL channel between broker and client.
To create the SSL channel, OpenSSL needs public and private keys. Today these keys are provided in byte* format to openssl. Byte* can either be generated by librdkafka or application itself. But ultimately to generate the byte* format, keys needs to be exported from where-ever it is residing.
In many scenarios exporting might not be an option since it might not be permitted by hardware/software, eg: if application is using non-exportable certificates in windows certificate store.
Good news is OpenSSL provides plugin model which is called engine. This engine can be used to interact with the storage and use the public and private keys for the certificate without exporting in byte* format. Such clients will have to write custom engines.
I am proposing to enhance LibrdKafka to support OpenSSL engine. Below are the changes I am taking up in this PR.


Add new parameter ""ssl.engine.location"" which will take the full path to the binary containing the openssl engine


This engine will be initialized if the parameter ""ssl.engine.location"" value is not null. Below is the initialization code in the method rd_kafka_ssl_ctx_init


Call engine method ENGINE_load_ssl_client_cert with appropriate params to fetch the pointers to the public and private keys. Key parameter is building and disposing the stack of ca-certs in rd_kafka_ssl_set_certs.


Note: this is a fresh Pr on #2602","The PR comments (here and in the closed PR) need to be addressed this week for this feature to be eligible for the upcoming v1.7.0 release.

@edenhill  Please check now. I missed out on some comments in previous pr",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,3315,2021-03-23T06:15:27Z,2021-04-08T17:56:48Z,2021-04-08T17:57:24Z,MERGED,True,720,1,17,https://github.com/adinigam,Adding OpenSSL engine support in LibrdKafka for SSL connections,43,[],https://github.com/edenhill/librdkafka/pull/3315,https://github.com/ajbarb,5,https://github.com/edenhill/librdkafka/pull/3315#issuecomment-814609847,"Currently LibrdKafka uses Openssl behind the scenes to establish SSL channel between broker and client.
To create the SSL channel, OpenSSL needs public and private keys. Today these keys are provided in byte* format to openssl. Byte* can either be generated by librdkafka or application itself. But ultimately to generate the byte* format, keys needs to be exported from where-ever it is residing.
In many scenarios exporting might not be an option since it might not be permitted by hardware/software, eg: if application is using non-exportable certificates in windows certificate store.
Good news is OpenSSL provides plugin model which is called engine. This engine can be used to interact with the storage and use the public and private keys for the certificate without exporting in byte* format. Such clients will have to write custom engines.
I am proposing to enhance LibrdKafka to support OpenSSL engine. Below are the changes I am taking up in this PR.


Add new parameter ""ssl.engine.location"" which will take the full path to the binary containing the openssl engine


This engine will be initialized if the parameter ""ssl.engine.location"" value is not null. Below is the initialization code in the method rd_kafka_ssl_ctx_init


Call engine method ENGINE_load_ssl_client_cert with appropriate params to fetch the pointers to the public and private keys. Key parameter is building and disposing the stack of ca-certs in rd_kafka_ssl_set_certs.


Note: this is a fresh Pr on #2602","The PR comments (here and in the closed PR) need to be addressed this week for this feature to be eligible for the upcoming v1.7.0 release.

@edenhill Please check now. I missed out on some comments in previous pr

@adinigam thanks for addressing the comments.
@edenhill thank you so much for the comments and please let us know if something is missing.",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,3315,2021-03-23T06:15:27Z,2021-04-08T17:56:48Z,2021-04-08T17:57:24Z,MERGED,True,720,1,17,https://github.com/adinigam,Adding OpenSSL engine support in LibrdKafka for SSL connections,43,[],https://github.com/edenhill/librdkafka/pull/3315,https://github.com/edenhill,6,https://github.com/edenhill/librdkafka/pull/3315#issuecomment-816023829,"Currently LibrdKafka uses Openssl behind the scenes to establish SSL channel between broker and client.
To create the SSL channel, OpenSSL needs public and private keys. Today these keys are provided in byte* format to openssl. Byte* can either be generated by librdkafka or application itself. But ultimately to generate the byte* format, keys needs to be exported from where-ever it is residing.
In many scenarios exporting might not be an option since it might not be permitted by hardware/software, eg: if application is using non-exportable certificates in windows certificate store.
Good news is OpenSSL provides plugin model which is called engine. This engine can be used to interact with the storage and use the public and private keys for the certificate without exporting in byte* format. Such clients will have to write custom engines.
I am proposing to enhance LibrdKafka to support OpenSSL engine. Below are the changes I am taking up in this PR.


Add new parameter ""ssl.engine.location"" which will take the full path to the binary containing the openssl engine


This engine will be initialized if the parameter ""ssl.engine.location"" value is not null. Below is the initialization code in the method rd_kafka_ssl_ctx_init


Call engine method ENGINE_load_ssl_client_cert with appropriate params to fetch the pointers to the public and private keys. Key parameter is building and disposing the stack of ca-certs in rd_kafka_ssl_set_certs.


Note: this is a fresh Pr on #2602",Thank's alot for contributing this functionality! ,True,"{'THUMBS_UP': ['https://github.com/adinigam'], 'ROCKET': ['https://github.com/ajbarb']}"
edenhill/librdkafka,https://github.com/edenhill/librdkafka,3320,2021-03-25T12:23:35Z,2021-03-25T12:59:32Z,2021-03-25T12:59:35Z,MERGED,True,27,15,5,https://github.com/edenhill,Stats: Fix consumer_lag ,2,[],https://github.com/edenhill/librdkafka/pull/3320,https://github.com/edenhill,1,https://github.com/edenhill/librdkafka/pull/3320,"This makes consumer_lag show the committed consumer lag, i.e., the same thing as what existing consumer_lag monitoring applications will see (but without querying the coordinator), and adds a consumer_lag_stored which is the consumer lag based on the stored offset (the to-be-committed offset) which is the lag from the application's point of view.","This makes consumer_lag show the committed consumer lag, i.e., the same thing as what existing consumer_lag monitoring applications will see (but without querying the coordinator), and adds a consumer_lag_stored which is the consumer lag based on the stored offset (the to-be-committed offset) which is the lag from the application's point of view.",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,3325,2021-03-31T09:35:34Z,2021-03-31T18:45:06Z,2021-03-31T18:45:08Z,MERGED,True,122,23,3,https://github.com/edenhill,commit_transaction() could succeed for timed out messages,1,[],https://github.com/edenhill/librdkafka/pull/3325,https://github.com/edenhill,1,https://github.com/edenhill/librdkafka/pull/3325,confluentinc/confluent-kafka-dotnet#1568,confluentinc/confluent-kafka-dotnet#1568,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,3326,2021-04-01T13:50:33Z,2021-04-06T06:33:49Z,2021-10-26T13:44:25Z,MERGED,True,168,10,4,https://github.com/edenhill,Don't auto-commit during rebalancing,2,[],https://github.com/edenhill/librdkafka/pull/3326,https://github.com/edenhill,1,https://github.com/edenhill/librdkafka/pull/3326,,,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,3326,2021-04-01T13:50:33Z,2021-04-06T06:33:49Z,2021-10-26T13:44:25Z,MERGED,True,168,10,4,https://github.com/edenhill,Don't auto-commit during rebalancing,2,[],https://github.com/edenhill/librdkafka/pull/3326,https://github.com/poolik,2,https://github.com/edenhill/librdkafka/pull/3326#issuecomment-951955198,,"Hi @mhowlett and @edenhill!
@mhowlett your last comment is very apt. We're hitting exactly this issue after upgrading to librdkafka 1.7.0 and trying out cooperative incremental rebalancing.
We are committing offsets ourselves in the background and would now need the same capability - skip commits while rebalancing is going on.
The issue is that I went through the public C++ as well as C API and I don't see how this could be done at the moment. Do you have any reccomendations how we could best access the rkcg_join_state through some public APIs?",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,3327,2021-04-04T15:53:06Z,2021-04-04T21:26:31Z,2021-04-04T21:26:31Z,CLOSED,False,1,1,1,https://github.com/hmenn,cmake: Fix using absolute path in include directories,1,[],https://github.com/edenhill/librdkafka/pull/3327,https://github.com/hmenn,1,https://github.com/edenhill/librdkafka/pull/3327,"LIST_DIR was problematic with cmake target files. It produces
absolute paths and that crashs build on yocto. SOURCE_DIR has the
same abilities without path mixes.","LIST_DIR was problematic with cmake target files. It produces
absolute paths and that crashs build on yocto. SOURCE_DIR has the
same abilities without path mixes.",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,3329,2021-04-05T10:55:10Z,2021-04-06T13:58:45Z,2021-04-06T13:58:49Z,MERGED,True,8,8,1,https://github.com/hmenn,cmake: Fix using absolute path in include directories,1,[],https://github.com/edenhill/librdkafka/pull/3329,https://github.com/hmenn,1,https://github.com/edenhill/librdkafka/pull/3329,"External library include directories were binded as public dependency.
That causes absolute path rependency and It produces
absolute paths and that crashs build on yocto.

You can check this pr for additional information about crash: openembedded/meta-openembedded#324 and this https://errors.yoctoproject.org/Errors/Details/575207/","External library include directories were binded as public dependency.
That causes absolute path rependency and It produces
absolute paths and that crashs build on yocto.

You can check this pr for additional information about crash: openembedded/meta-openembedded#324 and this https://errors.yoctoproject.org/Errors/Details/575207/",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,3329,2021-04-05T10:55:10Z,2021-04-06T13:58:45Z,2021-04-06T13:58:49Z,MERGED,True,8,8,1,https://github.com/hmenn,cmake: Fix using absolute path in include directories,1,[],https://github.com/edenhill/librdkafka/pull/3329,https://github.com/edenhill,2,https://github.com/edenhill/librdkafka/pull/3329#issuecomment-814010326,"External library include directories were binded as public dependency.
That causes absolute path rependency and It produces
absolute paths and that crashs build on yocto.

You can check this pr for additional information about crash: openembedded/meta-openembedded#324 and this https://errors.yoctoproject.org/Errors/Details/575207/",Ping @benesch @ed-alertedh @neptoess for review,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,3329,2021-04-05T10:55:10Z,2021-04-06T13:58:45Z,2021-04-06T13:58:49Z,MERGED,True,8,8,1,https://github.com/hmenn,cmake: Fix using absolute path in include directories,1,[],https://github.com/edenhill/librdkafka/pull/3329,https://github.com/edenhill,3,https://github.com/edenhill/librdkafka/pull/3329#issuecomment-814143433,"External library include directories were binded as public dependency.
That causes absolute path rependency and It produces
absolute paths and that crashs build on yocto.

You can check this pr for additional information about crash: openembedded/meta-openembedded#324 and this https://errors.yoctoproject.org/Errors/Details/575207/",Thanks!,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,3330,2021-04-06T08:10:42Z,2021-04-06T15:32:33Z,2021-04-06T15:37:37Z,MERGED,True,8,1,2,https://github.com/edenhill,Use TLS_client_method() instead of deprecated SSLv23_client_method(),1,[],https://github.com/edenhill/librdkafka/pull/3330,https://github.com/edenhill,1,https://github.com/edenhill/librdkafka/pull/3330,,,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,3331,2021-04-06T10:35:34Z,2021-04-06T15:40:04Z,2021-04-06T15:40:06Z,MERGED,True,30,17,4,https://github.com/edenhill,"Fix ""Invalid txn state transition: .."" crash",2,[],https://github.com/edenhill/librdkafka/pull/3331,https://github.com/edenhill,1,https://github.com/edenhill/librdkafka/pull/3331,,,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,3332,2021-04-06T16:20:36Z,2021-04-14T12:59:51Z,2021-04-14T12:59:53Z,MERGED,True,20,9,3,https://github.com/edenhill,Issue3305,3,[],https://github.com/edenhill/librdkafka/pull/3332,https://github.com/edenhill,1,https://github.com/edenhill/librdkafka/pull/3332,,,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,3336,2021-04-07T12:53:17Z,2021-04-07T14:54:13Z,2021-04-07T14:57:08Z,MERGED,True,0,1,1,https://github.com/neptoess,Remove CMAKE_FIND_LIBRARY_SUFFIXES From MinGW Build Script,1,[],https://github.com/edenhill/librdkafka/pull/3336,https://github.com/neptoess,1,https://github.com/edenhill/librdkafka/pull/3336,"Remove CMAKE_FIND_LIBRARY_SUFFIXES from packaging/mingw-w64/configure-build-msys2-mingw-static.sh
It turns out that this will not cause any issues with the static bundle
Thanks to @SpaceIm for initially pointing it out.","Remove CMAKE_FIND_LIBRARY_SUFFIXES from packaging/mingw-w64/configure-build-msys2-mingw-static.sh
It turns out that this will not cause any issues with the static bundle
Thanks to @SpaceIm for initially pointing it out.",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,3336,2021-04-07T12:53:17Z,2021-04-07T14:54:13Z,2021-04-07T14:57:08Z,MERGED,True,0,1,1,https://github.com/neptoess,Remove CMAKE_FIND_LIBRARY_SUFFIXES From MinGW Build Script,1,[],https://github.com/edenhill/librdkafka/pull/3336,https://github.com/edenhill,2,https://github.com/edenhill/librdkafka/pull/3336#issuecomment-814981502,"Remove CMAKE_FIND_LIBRARY_SUFFIXES from packaging/mingw-w64/configure-build-msys2-mingw-static.sh
It turns out that this will not cause any issues with the static bundle
Thanks to @SpaceIm for initially pointing it out.",Thanks ,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,3340,2021-04-08T10:27:17Z,2021-04-14T14:25:44Z,2021-04-14T14:25:56Z,MERGED,True,193,43,10,https://github.com/edenhill,Don't call kinit cmd from rd_kafka_new(),7,[],https://github.com/edenhill/librdkafka/pull/3340,https://github.com/edenhill,1,https://github.com/edenhill/librdkafka/pull/3340,I think the broker wakeups might also fix #2912,I think the broker wakeups might also fix #2912,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,3345,2021-04-13T13:32:34Z,2021-04-13T14:23:38Z,2021-04-13T14:23:42Z,MERGED,True,1,1,1,https://github.com/nick-zh,update php bindings,1,[],https://github.com/edenhill/librdkafka/pull/3345,https://github.com/nick-zh,1,https://github.com/edenhill/librdkafka/pull/3345,"I took the liberty of also removing phpkafka, it seems to me, that it is not maintained anymore","I took the liberty of also removing phpkafka, it seems to me, that it is not maintained anymore",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,3345,2021-04-13T13:32:34Z,2021-04-13T14:23:38Z,2021-04-13T14:23:42Z,MERGED,True,1,1,1,https://github.com/nick-zh,update php bindings,1,[],https://github.com/edenhill/librdkafka/pull/3345,https://github.com/edenhill,2,https://github.com/edenhill/librdkafka/pull/3345#issuecomment-818777542,"I took the liberty of also removing phpkafka, it seems to me, that it is not maintained anymore",Thanks!,True,{'HOORAY': ['https://github.com/nick-zh']}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,3347,2021-04-14T08:31:52Z,2021-04-14T12:59:23Z,2021-04-14T12:59:29Z,MERGED,True,227,124,12,https://github.com/edenhill,Fix queue wakeups (#2912),7,[],https://github.com/edenhill/librdkafka/pull/3347,https://github.com/edenhill,1,https://github.com/edenhill/librdkafka/pull/3347,..and some minor cleanups,..and some minor cleanups,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,3352,2021-04-16T08:24:07Z,2021-04-16T10:46:15Z,2021-06-22T08:53:38Z,MERGED,True,1,1,1,https://github.com/shanson7,Fix qlen value in debug statement,2,[],https://github.com/edenhill/librdkafka/pull/3352,https://github.com/shanson7,1,https://github.com/edenhill/librdkafka/pull/3352,"Per gitter convo this debug statement prints the same value (rd_kafka_q_len(&msetr->msetr_rkq)) twice, when it seems one of these values should be the length of the parent queue.","Per gitter convo this debug statement prints the same value (rd_kafka_q_len(&msetr->msetr_rkq)) twice, when it seems one of these values should be the length of the parent queue.",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,3352,2021-04-16T08:24:07Z,2021-04-16T10:46:15Z,2021-06-22T08:53:38Z,MERGED,True,1,1,1,https://github.com/shanson7,Fix qlen value in debug statement,2,[],https://github.com/edenhill/librdkafka/pull/3352,https://github.com/edenhill,2,https://github.com/edenhill/librdkafka/pull/3352#issuecomment-821089528,"Per gitter convo this debug statement prints the same value (rd_kafka_q_len(&msetr->msetr_rkq)) twice, when it seems one of these values should be the length of the parent queue.",Thanks!,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,3353,2021-04-16T11:59:16Z,2021-04-16T14:32:56Z,2021-04-16T14:32:58Z,MERGED,True,158,39,11,https://github.com/edenhill,KIP-360 fixes,4,[],https://github.com/edenhill/librdkafka/pull/3353,https://github.com/edenhill,1,https://github.com/edenhill/librdkafka/pull/3353,,,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,3354,2021-04-16T13:21:41Z,2021-04-16T14:49:29Z,2021-04-16T14:54:47Z,MERGED,True,3,1,2,https://github.com/neptoess,CMake: Remove Libs.private entries from rdkafka-static.pc and rdkafka++-static.pc,1,[],https://github.com/edenhill/librdkafka/pull/3354,https://github.com/neptoess,1,https://github.com/edenhill/librdkafka/pull/3354,"Per @edenhill comment here, when building static pkg-config files, add any additional linker flags for dynamic dependencies, e.g. -ldl on *nix, -lws2_32 on Windows, to Libs, not Libs.private. As this only affects CMake static builds, the risk is minimal (nothing uses these builds yet, but the hope is that confluent-kafka-go will be able to use static MinGW-built artifacts to provide Windows support)","Per @edenhill comment here, when building static pkg-config files, add any additional linker flags for dynamic dependencies, e.g. -ldl on *nix, -lws2_32 on Windows, to Libs, not Libs.private. As this only affects CMake static builds, the risk is minimal (nothing uses these builds yet, but the hope is that confluent-kafka-go will be able to use static MinGW-built artifacts to provide Windows support)",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,3354,2021-04-16T13:21:41Z,2021-04-16T14:49:29Z,2021-04-16T14:54:47Z,MERGED,True,3,1,2,https://github.com/neptoess,CMake: Remove Libs.private entries from rdkafka-static.pc and rdkafka++-static.pc,1,[],https://github.com/edenhill/librdkafka/pull/3354,https://github.com/edenhill,2,https://github.com/edenhill/librdkafka/pull/3354#issuecomment-821231175,"Per @edenhill comment here, when building static pkg-config files, add any additional linker flags for dynamic dependencies, e.g. -ldl on *nix, -lws2_32 on Windows, to Libs, not Libs.private. As this only affects CMake static builds, the risk is minimal (nothing uses these builds yet, but the hope is that confluent-kafka-go will be able to use static MinGW-built artifacts to provide Windows support)",Thanks!,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,3359,2021-04-17T07:40:05Z,,2021-04-19T17:32:08Z,OPEN,False,9,9,5,https://github.com/pavel-pimenov,[Windows] VC++ fix potentially uninitialized local pointer variable,1,[],https://github.com/edenhill/librdkafka/pull/3359,https://github.com/pavel-pimenov,1,https://github.com/edenhill/librdkafka/pull/3359,"Error	C4703	potentially uninitialized local pointer variable 'mcgrp' used	librdkafka	\librdkafka\src\rdkafka_mock_handlers.c	1250
Error	C4703	potentially uninitialized local pointer variable 'mcgrp' used	librdkafka	\librdkafka\src\rdkafka_mock_handlers.c	1321
Error	C4703	potentially uninitialized local pointer variable 'ignore' used	librdkafka	\librdkafka\src\rdkafka_sticky_assignor.c	1045
Error	C4703	potentially uninitialized local pointer variable 'ignore' used	librdkafka	\librdkafka\src\rdkafka_sticky_assignor.c	1401
Error	C4703	potentially uninitialized local pointer variable 'consumer' used	librdkafka	\librdkafka\src\rdkafka_sticky_assignor.c	948
Error	C4703	potentially uninitialized local pointer variable 'rktpar' used	librdkafka	\librdkafka\src\rdkafka_partition.c	1428
Error	C4703	potentially uninitialized local pointer variable 'rktpar' used	librdkafka	\librdkafka\src\rdkafka_partition.c	105
Error	C4703	potentially uninitialized local pointer variable 'inst' used	librdkafka	\librdkafka\src\regexp.c	715","Error	C4703	potentially uninitialized local pointer variable 'mcgrp' used	librdkafka	\librdkafka\src\rdkafka_mock_handlers.c	1250
Error	C4703	potentially uninitialized local pointer variable 'mcgrp' used	librdkafka	\librdkafka\src\rdkafka_mock_handlers.c	1321
Error	C4703	potentially uninitialized local pointer variable 'ignore' used	librdkafka	\librdkafka\src\rdkafka_sticky_assignor.c	1045
Error	C4703	potentially uninitialized local pointer variable 'ignore' used	librdkafka	\librdkafka\src\rdkafka_sticky_assignor.c	1401
Error	C4703	potentially uninitialized local pointer variable 'consumer' used	librdkafka	\librdkafka\src\rdkafka_sticky_assignor.c	948
Error	C4703	potentially uninitialized local pointer variable 'rktpar' used	librdkafka	\librdkafka\src\rdkafka_partition.c	1428
Error	C4703	potentially uninitialized local pointer variable 'rktpar' used	librdkafka	\librdkafka\src\rdkafka_partition.c	105
Error	C4703	potentially uninitialized local pointer variable 'inst' used	librdkafka	\librdkafka\src\regexp.c	715",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,3359,2021-04-17T07:40:05Z,,2021-04-19T17:32:08Z,OPEN,False,9,9,5,https://github.com/pavel-pimenov,[Windows] VC++ fix potentially uninitialized local pointer variable,1,[],https://github.com/edenhill/librdkafka/pull/3359,https://github.com/edenhill,2,https://github.com/edenhill/librdkafka/pull/3359#issuecomment-822321767,"Error	C4703	potentially uninitialized local pointer variable 'mcgrp' used	librdkafka	\librdkafka\src\rdkafka_mock_handlers.c	1250
Error	C4703	potentially uninitialized local pointer variable 'mcgrp' used	librdkafka	\librdkafka\src\rdkafka_mock_handlers.c	1321
Error	C4703	potentially uninitialized local pointer variable 'ignore' used	librdkafka	\librdkafka\src\rdkafka_sticky_assignor.c	1045
Error	C4703	potentially uninitialized local pointer variable 'ignore' used	librdkafka	\librdkafka\src\rdkafka_sticky_assignor.c	1401
Error	C4703	potentially uninitialized local pointer variable 'consumer' used	librdkafka	\librdkafka\src\rdkafka_sticky_assignor.c	948
Error	C4703	potentially uninitialized local pointer variable 'rktpar' used	librdkafka	\librdkafka\src\rdkafka_partition.c	1428
Error	C4703	potentially uninitialized local pointer variable 'rktpar' used	librdkafka	\librdkafka\src\rdkafka_partition.c	105
Error	C4703	potentially uninitialized local pointer variable 'inst' used	librdkafka	\librdkafka\src\regexp.c	715","All these are false positives. What is reporting these warnings? Is it Visual studio? If so, which version?",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,3359,2021-04-17T07:40:05Z,,2021-04-19T17:32:08Z,OPEN,False,9,9,5,https://github.com/pavel-pimenov,[Windows] VC++ fix potentially uninitialized local pointer variable,1,[],https://github.com/edenhill/librdkafka/pull/3359,https://github.com/pavel-pimenov,3,https://github.com/edenhill/librdkafka/pull/3359#issuecomment-822644168,"Error	C4703	potentially uninitialized local pointer variable 'mcgrp' used	librdkafka	\librdkafka\src\rdkafka_mock_handlers.c	1250
Error	C4703	potentially uninitialized local pointer variable 'mcgrp' used	librdkafka	\librdkafka\src\rdkafka_mock_handlers.c	1321
Error	C4703	potentially uninitialized local pointer variable 'ignore' used	librdkafka	\librdkafka\src\rdkafka_sticky_assignor.c	1045
Error	C4703	potentially uninitialized local pointer variable 'ignore' used	librdkafka	\librdkafka\src\rdkafka_sticky_assignor.c	1401
Error	C4703	potentially uninitialized local pointer variable 'consumer' used	librdkafka	\librdkafka\src\rdkafka_sticky_assignor.c	948
Error	C4703	potentially uninitialized local pointer variable 'rktpar' used	librdkafka	\librdkafka\src\rdkafka_partition.c	1428
Error	C4703	potentially uninitialized local pointer variable 'rktpar' used	librdkafka	\librdkafka\src\rdkafka_partition.c	105
Error	C4703	potentially uninitialized local pointer variable 'inst' used	librdkafka	\librdkafka\src\regexp.c	715",,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,3360,2021-04-19T14:43:34Z,2021-04-19T15:49:39Z,2021-04-19T15:49:42Z,MERGED,True,131,6,4,https://github.com/edenhill,Fixes for the sticky assignor balancing part,2,[],https://github.com/edenhill/librdkafka/pull/3360,https://github.com/edenhill,1,https://github.com/edenhill/librdkafka/pull/3360,,,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,3362,2021-04-20T15:42:02Z,2021-04-20T19:07:19Z,2021-04-20T19:07:23Z,MERGED,True,232,16,6,https://github.com/edenhill,Additional sticky fixes,2,[],https://github.com/edenhill/librdkafka/pull/3362,https://github.com/edenhill,1,https://github.com/edenhill/librdkafka/pull/3362,,,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,3363,2021-04-21T08:29:59Z,2021-04-21T16:16:25Z,2021-04-21T16:21:09Z,MERGED,True,32,10,4,https://github.com/edenhill,Fix SSL handshake busy-loop due to POLLOUT being set in CONNECT state,1,[],https://github.com/edenhill/librdkafka/pull/3363,https://github.com/edenhill,1,https://github.com/edenhill/librdkafka/pull/3363,".. by adding a new SSL_HANDSHAKE broker state specific to this state of
the connection establishment.",".. by adding a new SSL_HANDSHAKE broker state specific to this state of
the connection establishment.",True,{'THUMBS_UP': ['https://github.com/huanggx-sea']}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,3364,2021-04-22T08:02:16Z,2021-04-22T14:11:00Z,2021-04-22T14:11:03Z,MERGED,True,15,8,4,https://github.com/edenhill,"OpenSSL bump, changelog",3,[],https://github.com/edenhill/librdkafka/pull/3364,https://github.com/edenhill,1,https://github.com/edenhill/librdkafka/pull/3364,,,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,3366,2021-04-25T09:30:49Z,,2021-06-16T09:21:58Z,OPEN,False,1,1,1,https://github.com/pavel-pimenov,wchar_t -> CHAR,1,[],https://github.com/edenhill/librdkafka/pull/3366,https://github.com/pavel-pimenov,1,https://github.com/edenhill/librdkafka/pull/3366,"fix librdkafka\src\rdkafka_sasl_win32.c(182,58):
warning C4133: 'function': incompatible types - from 'wchar_t [512]' to 'SEC_CHAR *'","fix librdkafka\src\rdkafka_sasl_win32.c(182,58):
warning C4133: 'function': incompatible types - from 'wchar_t [512]' to 'SEC_CHAR *'",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,3366,2021-04-25T09:30:49Z,,2021-06-16T09:21:58Z,OPEN,False,1,1,1,https://github.com/pavel-pimenov,wchar_t -> CHAR,1,[],https://github.com/edenhill/librdkafka/pull/3366,https://github.com/edenhill,2,https://github.com/edenhill/librdkafka/pull/3366#issuecomment-862199759,"fix librdkafka\src\rdkafka_sasl_win32.c(182,58):
warning C4133: 'function': incompatible types - from 'wchar_t [512]' to 'SEC_CHAR *'","This yields another warning now:
C:/tools/msys64/mingw64/x86_64-w64-mingw32/include/sspi.h:860:16: note: expected 'SEC_WCHAR *' {aka 'short unsigned int *'} but argument is of type 'CHAR *' {aka 'char *'}
  860 |     SEC_WCHAR *pszTargetName,
      |     ~~~~~~~~~~~^~~~~~~~~~~~~

https://travis-ci.org/github/edenhill/librdkafka/jobs/768291649#L539",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,3368,2021-04-27T09:36:25Z,2021-04-27T09:43:42Z,2021-04-27T09:43:58Z,CLOSED,False,15,13,1,https://github.com/theidexisted,fix #3367; Resolve conflict with prefix and libdir,1,[],https://github.com/edenhill/librdkafka/pull/3368,https://github.com/theidexisted,1,https://github.com/edenhill/librdkafka/pull/3368,,,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,3368,2021-04-27T09:36:25Z,2021-04-27T09:43:42Z,2021-04-27T09:43:58Z,CLOSED,False,15,13,1,https://github.com/theidexisted,fix #3367; Resolve conflict with prefix and libdir,1,[],https://github.com/edenhill/librdkafka/pull/3368,https://github.com/edenhill,2,https://github.com/edenhill/librdkafka/pull/3368#issuecomment-827471282,,"libdir is the install directory, not the search path.",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,3369,2021-04-27T10:13:19Z,2021-04-27T18:25:03Z,2021-04-27T18:25:05Z,MERGED,True,77,44,2,https://github.com/edenhill,Correctly handle empty aborted transactions,1,[],https://github.com/edenhill/librdkafka/pull/3369,https://github.com/edenhill,1,https://github.com/edenhill/librdkafka/pull/3369,"Seems like there is a case where aborted transactions will have an
ABORT ctrl message marker, but not be included in the aborted transactions
list.
This would cause the next aborted-transaction-offset to be popped from the
aborted transaction list even though it did not match the aborted transaction
marker. This in turn could lead to aborted messages for subsequent aborted
transactions in the same MessageSet to not be skipped and instead delivered
to the application.
The fix is to silently (unless debug) ignore these unsolicited abort markers
if they're not in the aborted transaction list and not pop off the next
non-matching offset from the aborted transaction list.","Seems like there is a case where aborted transactions will have an
ABORT ctrl message marker, but not be included in the aborted transactions
list.
This would cause the next aborted-transaction-offset to be popped from the
aborted transaction list even though it did not match the aborted transaction
marker. This in turn could lead to aborted messages for subsequent aborted
transactions in the same MessageSet to not be skipped and instead delivered
to the application.
The fix is to silently (unless debug) ignore these unsolicited abort markers
if they're not in the aborted transaction list and not pop off the next
non-matching offset from the aborted transaction list.",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,3370,2021-04-28T12:20:55Z,2021-04-28T14:01:26Z,2021-04-28T14:01:30Z,MERGED,True,16,7,3,https://github.com/edenhill,"Fix static windows build artifacts, and a CI test failure",2,[],https://github.com/edenhill/librdkafka/pull/3370,https://github.com/edenhill,1,https://github.com/edenhill/librdkafka/pull/3370,"That CI test failure has been bugging me for ages, but not being able to reproduce locally until today with a hefty dose of valgrind and cpulimit. Time-based tests are crap.","That CI test failure has been bugging me for ages, but not being able to reproduce locally until today with a hefty dose of valgrind and cpulimit. Time-based tests are crap.",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,3372,2021-04-30T10:40:38Z,2021-04-30T16:38:32Z,2021-06-21T15:42:56Z,MERGED,True,7,2,5,https://github.com/edenhill,KIP-735: increase default session.timeout.ms,2,[],https://github.com/edenhill/librdkafka/pull/3372,https://github.com/edenhill,1,https://github.com/edenhill/librdkafka/pull/3372,,,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,3374,2021-05-02T00:20:38Z,,2021-05-03T17:31:11Z,OPEN,False,15,5,2,https://github.com/GregBowyer,Clang cl compilation fixes,2,[],https://github.com/edenhill/librdkafka/pull/3374,https://github.com/GregBowyer,1,https://github.com/edenhill/librdkafka/pull/3374,"Clang when run as clang-cl on windows has subtle issues with SIMD intrinsics. Essentially it tries to parse its own headers but gets confused about the handling of none standard __attribute__ markers. This leads it to confuse the meaning of certain SIMD types.
Fortunately librdkafka (and snappy as embedded) only need some basic operations that clang (even as clang-cl` support). As such we can use the builtins","Clang when run as clang-cl on windows has subtle issues with SIMD intrinsics. Essentially it tries to parse its own headers but gets confused about the handling of none standard __attribute__ markers. This leads it to confuse the meaning of certain SIMD types.
Fortunately librdkafka (and snappy as embedded) only need some basic operations that clang (even as clang-cl` support). As such we can use the builtins",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,3375,2021-05-03T07:40:31Z,2021-05-03T19:44:12Z,2021-05-03T19:44:14Z,MERGED,True,44,21,3,https://github.com/edenhill,cooperative-sticky consumer unsubscribe() now leaves the group,3,[],https://github.com/edenhill/librdkafka/pull/3375,https://github.com/edenhill,1,https://github.com/edenhill/librdkafka/pull/3375,The increase in session.timeout.ms surfaced the fact that we don't leave the group on unsubscribe in the incremental case.,The increase in session.timeout.ms surfaced the fact that we don't leave the group on unsubscribe in the incremental case.,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,3376,2021-05-03T17:03:22Z,2021-08-24T08:44:27Z,2021-08-24T08:44:30Z,MERGED,True,13,0,1,https://github.com/filimonov,Add support for buildling on illumos,1,[],https://github.com/edenhill/librdkafka/pull/3376,https://github.com/filimonov,1,https://github.com/edenhill/librdkafka/pull/3376,"Pushing the patch sent by @bnaecker in ClickHouse#4 to the upstream.

This is part of a larger set of changes for building ClickHouse on illumos.
illumos does not use the DT_REG and friends macros for determining the type of a directory entry. An extra stat(2) system call is required, followed by a similar query on the struct stat returned from that call.

More details: ClickHouse/ClickHouse#23746","Pushing the patch sent by @bnaecker in ClickHouse#4 to the upstream.

This is part of a larger set of changes for building ClickHouse on illumos.
illumos does not use the DT_REG and friends macros for determining the type of a directory entry. An extra stat(2) system call is required, followed by a similar query on the struct stat returned from that call.

More details: ClickHouse/ClickHouse#23746",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,3376,2021-05-03T17:03:22Z,2021-08-24T08:44:27Z,2021-08-24T08:44:30Z,MERGED,True,13,0,1,https://github.com/filimonov,Add support for buildling on illumos,1,[],https://github.com/edenhill/librdkafka/pull/3376,https://github.com/edenhill,2,https://github.com/edenhill/librdkafka/pull/3376#issuecomment-904447402,"Pushing the patch sent by @bnaecker in ClickHouse#4 to the upstream.

This is part of a larger set of changes for building ClickHouse on illumos.
illumos does not use the DT_REG and friends macros for determining the type of a directory entry. An extra stat(2) system call is required, followed by a similar query on the struct stat returned from that call.

More details: ClickHouse/ClickHouse#23746",Thank you!,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,3378,2021-05-04T08:41:08Z,2021-05-04T09:26:44Z,2021-05-04T09:26:47Z,MERGED,True,11,2,2,https://github.com/edenhill,Disconnects during SSL Handshake are now ERR__TRANSPORT rather than ERR__SSL,1,[],https://github.com/edenhill/librdkafka/pull/3378,https://github.com/edenhill,1,https://github.com/edenhill/librdkafka/pull/3378,,,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,3379,2021-05-04T11:19:27Z,2021-08-24T07:05:46Z,2021-08-24T07:05:48Z,MERGED,True,152,0,2,https://github.com/edenhill,Add cleanup-s3.py script,1,[],https://github.com/edenhill/librdkafka/pull/3379,https://github.com/edenhill,1,https://github.com/edenhill/librdkafka/pull/3379,,,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,3382,2021-05-06T09:15:20Z,2021-05-06T15:50:05Z,2021-05-06T15:50:07Z,MERGED,True,469,870,21,https://github.com/edenhill,Stats updates,7,[],https://github.com/edenhill/librdkafka/pull/3382,https://github.com/edenhill,1,https://github.com/edenhill/librdkafka/pull/3382,"I'd really like to get the .age field into statistics for the upcoming release.
Don't pay too much attention to the stats python tools, they're WIP and will be modified for each troubleshooting occassion.
Also added a warning for people that use set_default_topic_conf in a bad way.","I'd really like to get the .age field into statistics for the upcoming release.
Don't pay too much attention to the stats python tools, they're WIP and will be modified for each troubleshooting occassion.
Also added a warning for people that use set_default_topic_conf in a bad way.",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,3383,2021-05-07T10:50:35Z,,2021-05-10T08:15:13Z,OPEN,False,2,2,2,https://github.com/ellimist,Improve compression codec error message,1,[],https://github.com/edenhill/librdkafka/pull/3383,https://github.com/ellimist,1,https://github.com/edenhill/librdkafka/pull/3383,"Currently the error message: Not implemented is pretty cryptic since it lacks any context.
This PR addresses that, by specifying this is about a missing compression codec.","Currently the error message: Not implemented is pretty cryptic since it lacks any context.
This PR addresses that, by specifying this is about a missing compression codec.",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,3383,2021-05-07T10:50:35Z,,2021-05-10T08:15:13Z,OPEN,False,2,2,2,https://github.com/ellimist,Improve compression codec error message,1,[],https://github.com/edenhill/librdkafka/pull/3383,https://github.com/dasch,2,https://github.com/edenhill/librdkafka/pull/3383#issuecomment-834340284,"Currently the error message: Not implemented is pretty cryptic since it lacks any context.
This PR addresses that, by specifying this is about a missing compression codec.",We've had several cases where users spent a long time debugging because this error message was so unclear.  to improving it!,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,3390,2021-05-13T14:07:57Z,2021-05-13T15:56:00Z,2021-05-13T15:56:04Z,MERGED,True,1,1,1,https://github.com/PrateekJoshi,Fix for issue #3389,1,[],https://github.com/edenhill/librdkafka/pull/3390,https://github.com/PrateekJoshi,1,https://github.com/edenhill/librdkafka/pull/3390,,,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,3390,2021-05-13T14:07:57Z,2021-05-13T15:56:00Z,2021-05-13T15:56:04Z,MERGED,True,1,1,1,https://github.com/PrateekJoshi,Fix for issue #3389,1,[],https://github.com/edenhill/librdkafka/pull/3390,https://github.com/edenhill,2,https://github.com/edenhill/librdkafka/pull/3390#issuecomment-840656159,,Thank you!,True,{'THUMBS_UP': ['https://github.com/PrateekJoshi']}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,3404,2021-05-25T18:22:45Z,,2021-08-12T08:13:59Z,OPEN,False,1,0,1,https://github.com/m8mble,Support finding LZ4 from installed package,1,[],https://github.com/edenhill/librdkafka/pull/3404,https://github.com/m8mble,1,https://github.com/edenhill/librdkafka/pull/3404,"When built with ENABLE_LZ4_EXT=ON, consuming RdKafka via
find_package also requires lz4. Since lz4 doesn't bring an
LZ4Config.cmake, this package is found using RdKafka's custom
FindLZ4.cmake. But this ""find module"" isn't searched in
RdKafka's install target by default 1.
Attempting to do so results in
By not providing ""FindLZ4.cmake"" in CMAKE_MODULE_PATH this project has
asked CMake to find a package configuration file provided by ""LZ4"", but
CMake did not find one.
[...]
with current releases.
This is why extending the CMAKE_MODULE_PATH to the RdKafka
install target is necessary. This is actually common practice
e.g. 2.","When built with ENABLE_LZ4_EXT=ON, consuming RdKafka via
find_package also requires lz4. Since lz4 doesn't bring an
LZ4Config.cmake, this package is found using RdKafka's custom
FindLZ4.cmake. But this ""find module"" isn't searched in
RdKafka's install target by default 1.
Attempting to do so results in
By not providing ""FindLZ4.cmake"" in CMAKE_MODULE_PATH this project has
asked CMake to find a package configuration file provided by ""LZ4"", but
CMake did not find one.
[...]
with current releases.
This is why extending the CMAKE_MODULE_PATH to the RdKafka
install target is necessary. This is actually common practice
e.g. 2.",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,3404,2021-05-25T18:22:45Z,,2021-08-12T08:13:59Z,OPEN,False,1,0,1,https://github.com/m8mble,Support finding LZ4 from installed package,1,[],https://github.com/edenhill/librdkafka/pull/3404,https://github.com/m8mble,2,https://github.com/edenhill/librdkafka/pull/3404#issuecomment-848496002,"When built with ENABLE_LZ4_EXT=ON, consuming RdKafka via
find_package also requires lz4. Since lz4 doesn't bring an
LZ4Config.cmake, this package is found using RdKafka's custom
FindLZ4.cmake. But this ""find module"" isn't searched in
RdKafka's install target by default 1.
Attempting to do so results in
By not providing ""FindLZ4.cmake"" in CMAKE_MODULE_PATH this project has
asked CMake to find a package configuration file provided by ""LZ4"", but
CMake did not find one.
[...]
with current releases.
This is why extending the CMAKE_MODULE_PATH to the RdKafka
install target is necessary. This is actually common practice
e.g. 2.","I might need some help with Doozer CI errors:

Status: Temporary failure: GIT: Unable to create GIT repo -- failed to make directory './(null)': Permission denied because Unable to create heap repo.BxCgKo6DQq.0 -- Read-only file system, No more retries",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,3404,2021-05-25T18:22:45Z,,2021-08-12T08:13:59Z,OPEN,False,1,0,1,https://github.com/m8mble,Support finding LZ4 from installed package,1,[],https://github.com/edenhill/librdkafka/pull/3404,https://github.com/edenhill,3,https://github.com/edenhill/librdkafka/pull/3404#issuecomment-848498398,"When built with ENABLE_LZ4_EXT=ON, consuming RdKafka via
find_package also requires lz4. Since lz4 doesn't bring an
LZ4Config.cmake, this package is found using RdKafka's custom
FindLZ4.cmake. But this ""find module"" isn't searched in
RdKafka's install target by default 1.
Attempting to do so results in
By not providing ""FindLZ4.cmake"" in CMAKE_MODULE_PATH this project has
asked CMake to find a package configuration file provided by ""LZ4"", but
CMake did not find one.
[...]
with current releases.
This is why extending the CMAKE_MODULE_PATH to the RdKafka
install target is necessary. This is actually common practice
e.g. 2.","Doozer had some issues unrelated to your PR, but the cmake job passed so that's good ",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,3404,2021-05-25T18:22:45Z,,2021-08-12T08:13:59Z,OPEN,False,1,0,1,https://github.com/m8mble,Support finding LZ4 from installed package,1,[],https://github.com/edenhill/librdkafka/pull/3404,https://github.com/edenhill,4,https://github.com/edenhill/librdkafka/pull/3404#issuecomment-848499629,"When built with ENABLE_LZ4_EXT=ON, consuming RdKafka via
find_package also requires lz4. Since lz4 doesn't bring an
LZ4Config.cmake, this package is found using RdKafka's custom
FindLZ4.cmake. But this ""find module"" isn't searched in
RdKafka's install target by default 1.
Attempting to do so results in
By not providing ""FindLZ4.cmake"" in CMAKE_MODULE_PATH this project has
asked CMake to find a package configuration file provided by ""LZ4"", but
CMake did not find one.
[...]
with current releases.
This is why extending the CMAKE_MODULE_PATH to the RdKafka
install target is necessary. This is actually common practice
e.g. 2.",Pinging @Oxymoron79  @neptoess  @benesch @snar for review,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,3404,2021-05-25T18:22:45Z,,2021-08-12T08:13:59Z,OPEN,False,1,0,1,https://github.com/m8mble,Support finding LZ4 from installed package,1,[],https://github.com/edenhill/librdkafka/pull/3404,https://github.com/neptoess,5,https://github.com/edenhill/librdkafka/pull/3404#issuecomment-849232302,"When built with ENABLE_LZ4_EXT=ON, consuming RdKafka via
find_package also requires lz4. Since lz4 doesn't bring an
LZ4Config.cmake, this package is found using RdKafka's custom
FindLZ4.cmake. But this ""find module"" isn't searched in
RdKafka's install target by default 1.
Attempting to do so results in
By not providing ""FindLZ4.cmake"" in CMAKE_MODULE_PATH this project has
asked CMake to find a package configuration file provided by ""LZ4"", but
CMake did not find one.
[...]
with current releases.
This is why extending the CMAKE_MODULE_PATH to the RdKafka
install target is necessary. This is actually common practice
e.g. 2.","Pinging @Oxymoron79 @neptoess @benesch @snar for review

Looks good to me. A nice part about the way this is done is that, if LZ4 ever starts shipping their own Find.cmake, this will automatically use that instead of the rdkafka one.",True,{'THUMBS_UP': ['https://github.com/m8mble']}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,3404,2021-05-25T18:22:45Z,,2021-08-12T08:13:59Z,OPEN,False,1,0,1,https://github.com/m8mble,Support finding LZ4 from installed package,1,[],https://github.com/edenhill/librdkafka/pull/3404,https://github.com/Oxymoron79,6,https://github.com/edenhill/librdkafka/pull/3404#issuecomment-852845514,"When built with ENABLE_LZ4_EXT=ON, consuming RdKafka via
find_package also requires lz4. Since lz4 doesn't bring an
LZ4Config.cmake, this package is found using RdKafka's custom
FindLZ4.cmake. But this ""find module"" isn't searched in
RdKafka's install target by default 1.
Attempting to do so results in
By not providing ""FindLZ4.cmake"" in CMAKE_MODULE_PATH this project has
asked CMake to find a package configuration file provided by ""LZ4"", but
CMake did not find one.
[...]
with current releases.
This is why extending the CMAKE_MODULE_PATH to the RdKafka
install target is necessary. This is actually common practice
e.g. 2.","Pinging @Oxymoron79 @neptoess @benesch @snar for review

Looks good to me as well.
I just wonder if the chosen installation directory for the FindLZ4.cmake file (see https://github.com/edenhill/librdkafka/blob/master/CMakeLists.txt#L250) should be adjusted to install it to a directory that CMake searches by default?",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,3404,2021-05-25T18:22:45Z,,2021-08-12T08:13:59Z,OPEN,False,1,0,1,https://github.com/m8mble,Support finding LZ4 from installed package,1,[],https://github.com/edenhill/librdkafka/pull/3404,https://github.com/neptoess,7,https://github.com/edenhill/librdkafka/pull/3404#issuecomment-852964036,"When built with ENABLE_LZ4_EXT=ON, consuming RdKafka via
find_package also requires lz4. Since lz4 doesn't bring an
LZ4Config.cmake, this package is found using RdKafka's custom
FindLZ4.cmake. But this ""find module"" isn't searched in
RdKafka's install target by default 1.
Attempting to do so results in
By not providing ""FindLZ4.cmake"" in CMAKE_MODULE_PATH this project has
asked CMake to find a package configuration file provided by ""LZ4"", but
CMake did not find one.
[...]
with current releases.
This is why extending the CMAKE_MODULE_PATH to the RdKafka
install target is necessary. This is actually common practice
e.g. 2.","Pinging @Oxymoron79 @neptoess @benesch @snar for review

Looks good to me as well.
I just wonder if the chosen installation directory for the FindLZ4.cmake file (see https://github.com/edenhill/librdkafka/blob/master/CMakeLists.txt#L250) should be adjusted to install it to a directory that CMake searches by default?

Definitely a little bit of a tricky decision. If this were the CMake install command for the LZ4 library, I would 100% agree that FindLZ4 should end up in a directory CMake searches by default. But this is librdkafka. Not sure it makes sense for this library to distribute CMake Find* scripts for other libraries.",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,3404,2021-05-25T18:22:45Z,,2021-08-12T08:13:59Z,OPEN,False,1,0,1,https://github.com/m8mble,Support finding LZ4 from installed package,1,[],https://github.com/edenhill/librdkafka/pull/3404,https://github.com/m8mble,8,https://github.com/edenhill/librdkafka/pull/3404#issuecomment-853300682,"When built with ENABLE_LZ4_EXT=ON, consuming RdKafka via
find_package also requires lz4. Since lz4 doesn't bring an
LZ4Config.cmake, this package is found using RdKafka's custom
FindLZ4.cmake. But this ""find module"" isn't searched in
RdKafka's install target by default 1.
Attempting to do so results in
By not providing ""FindLZ4.cmake"" in CMAKE_MODULE_PATH this project has
asked CMake to find a package configuration file provided by ""LZ4"", but
CMake did not find one.
[...]
with current releases.
This is why extending the CMAKE_MODULE_PATH to the RdKafka
install target is necessary. This is actually common practice
e.g. 2.","I just wonder if the chosen installation directory for the FindLZ4.cmake file (see https://github.com/edenhill/librdkafka/blob/master/CMakeLists.txt#L250) should be adjusted to install it to a directory that CMake searches by default?

Probably complicated: According to the cmake find procedure documentation, this requires writing files into some folder named lz4. This likely raises concerns of LZ4, e.g. might cause conflicts if they ever were to distribute their own cmake-config.",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,3404,2021-05-25T18:22:45Z,,2021-08-12T08:13:59Z,OPEN,False,1,0,1,https://github.com/m8mble,Support finding LZ4 from installed package,1,[],https://github.com/edenhill/librdkafka/pull/3404,https://github.com/Oxymoron79,9,https://github.com/edenhill/librdkafka/pull/3404#issuecomment-853635714,"When built with ENABLE_LZ4_EXT=ON, consuming RdKafka via
find_package also requires lz4. Since lz4 doesn't bring an
LZ4Config.cmake, this package is found using RdKafka's custom
FindLZ4.cmake. But this ""find module"" isn't searched in
RdKafka's install target by default 1.
Attempting to do so results in
By not providing ""FindLZ4.cmake"" in CMAKE_MODULE_PATH this project has
asked CMake to find a package configuration file provided by ""LZ4"", but
CMake did not find one.
[...]
with current releases.
This is why extending the CMAKE_MODULE_PATH to the RdKafka
install target is necessary. This is actually common practice
e.g. 2.","@neptoess, @m8mble Good points.Thanks for your explanations.",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,3404,2021-05-25T18:22:45Z,,2021-08-12T08:13:59Z,OPEN,False,1,0,1,https://github.com/m8mble,Support finding LZ4 from installed package,1,[],https://github.com/edenhill/librdkafka/pull/3404,https://github.com/m8mble,10,https://github.com/edenhill/librdkafka/pull/3404#issuecomment-863793694,"When built with ENABLE_LZ4_EXT=ON, consuming RdKafka via
find_package also requires lz4. Since lz4 doesn't bring an
LZ4Config.cmake, this package is found using RdKafka's custom
FindLZ4.cmake. But this ""find module"" isn't searched in
RdKafka's install target by default 1.
Attempting to do so results in
By not providing ""FindLZ4.cmake"" in CMAKE_MODULE_PATH this project has
asked CMake to find a package configuration file provided by ""LZ4"", but
CMake did not find one.
[...]
with current releases.
This is why extending the CMAKE_MODULE_PATH to the RdKafka
install target is necessary. This is actually common practice
e.g. 2.","Is there anything I can do to push this PR over the line?
For my future self: the issue can be worked around by adding -DCMAKE_MODULE_PATH=/usr/lib64/cmake/RdKafka at CMake configure time.",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,3404,2021-05-25T18:22:45Z,,2021-08-12T08:13:59Z,OPEN,False,1,0,1,https://github.com/m8mble,Support finding LZ4 from installed package,1,[],https://github.com/edenhill/librdkafka/pull/3404,https://github.com/edenhill,11,https://github.com/edenhill/librdkafka/pull/3404#issuecomment-897438571,"When built with ENABLE_LZ4_EXT=ON, consuming RdKafka via
find_package also requires lz4. Since lz4 doesn't bring an
LZ4Config.cmake, this package is found using RdKafka's custom
FindLZ4.cmake. But this ""find module"" isn't searched in
RdKafka's install target by default 1.
Attempting to do so results in
By not providing ""FindLZ4.cmake"" in CMAKE_MODULE_PATH this project has
asked CMake to find a package configuration file provided by ""LZ4"", but
CMake did not find one.
[...]
with current releases.
This is why extending the CMAKE_MODULE_PATH to the RdKafka
install target is necessary. This is actually common practice
e.g. 2.","Since this might affect the vcpkg, can you review this @myd7349 ?",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,3410,2021-06-02T17:03:24Z,,2021-06-03T01:51:32Z,OPEN,False,1748,4,13,https://github.com/lesterfan,KIP-396: Implement AlterConsumerGroupOffsets and ListConsumerGroupOffsets admin APIs,1,[],https://github.com/edenhill/librdkafka/pull/3410,https://github.com/lesterfan,1,https://github.com/edenhill/librdkafka/pull/3410,"This is a very rough implementation of KIP-396. I tried to follow the Java implementation and have the alterConsumerGroupOffsets() API call just send an OffsetCommitRequest and the listConsumerGroupOffsets() API call just send an OffsetFetchRequest. I did run into some issues, and I thought it would be good to just share this rough (but working) patch here with these caveats and get some feedback:

There already is code in librdkafka to send an OffsetCommitRequst, parse an OffsetCommitResponse, send an OffsetFetchRequest, and parse an OffsetFetchResponse. It would be ideal to re-use them for these admin API calls. However, these function's interfaces expect an rd_kafka_broker_t * and other parameters which we don't have access to in the admin API. I couldn't figure out how to refactor the code to re-use these functions and ended up copy-pasting a large part of the implementation of these functions in these admin API calls, leading to significant code duplication.
In the OffsetFetchRequest, there is an option to set topics to NULL if we want to fetch offsets for all topics. This is a useful feature to expose in the admin API, and I thought it would make sense to have a NULL rd_kafka_topic_partition_list* correspond to having a NULL list of topics for this request. However, many helper functions like rd_kafka_topic_partition_list_copy don't account for cases where the toppar list is NULL, so I added a check there. This widens the contract of these helper functions so I wasn't sure if it was the correct way of going about this.","This is a very rough implementation of KIP-396. I tried to follow the Java implementation and have the alterConsumerGroupOffsets() API call just send an OffsetCommitRequest and the listConsumerGroupOffsets() API call just send an OffsetFetchRequest. I did run into some issues, and I thought it would be good to just share this rough (but working) patch here with these caveats and get some feedback:

There already is code in librdkafka to send an OffsetCommitRequst, parse an OffsetCommitResponse, send an OffsetFetchRequest, and parse an OffsetFetchResponse. It would be ideal to re-use them for these admin API calls. However, these function's interfaces expect an rd_kafka_broker_t * and other parameters which we don't have access to in the admin API. I couldn't figure out how to refactor the code to re-use these functions and ended up copy-pasting a large part of the implementation of these functions in these admin API calls, leading to significant code duplication.
In the OffsetFetchRequest, there is an option to set topics to NULL if we want to fetch offsets for all topics. This is a useful feature to expose in the admin API, and I thought it would make sense to have a NULL rd_kafka_topic_partition_list* correspond to having a NULL list of topics for this request. However, many helper functions like rd_kafka_topic_partition_list_copy don't account for cases where the toppar list is NULL, so I added a check there. This widens the contract of these helper functions so I wasn't sure if it was the correct way of going about this.",True,"{'THUMBS_UP': ['https://github.com/hallfox', 'https://github.com/mlongob', 'https://github.com/icatat']}"
edenhill/librdkafka,https://github.com/edenhill/librdkafka,3416,2021-06-10T10:09:35Z,2021-06-14T15:15:54Z,2021-06-14T15:15:57Z,MERGED,True,155,39,6,https://github.com/edenhill,OffsetCommit retry fixes,4,[],https://github.com/edenhill/librdkafka/pull/3416,https://github.com/edenhill,1,https://github.com/edenhill/librdkafka/pull/3416,,,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,3428,2021-06-15T14:12:10Z,2021-07-12T08:37:16Z,2021-07-12T08:37:25Z,MERGED,True,371,105,16,https://github.com/edenhill,query_watermark_offsets() did not respect isolation.level,4,[],https://github.com/edenhill/librdkafka/pull/3428,https://github.com/edenhill,1,https://github.com/edenhill/librdkafka/pull/3428,,,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,3428,2021-06-15T14:12:10Z,2021-07-12T08:37:16Z,2021-07-12T08:37:25Z,MERGED,True,371,105,16,https://github.com/edenhill,query_watermark_offsets() did not respect isolation.level,4,[],https://github.com/edenhill/librdkafka/pull/3428,https://github.com/edenhill,2,https://github.com/edenhill/librdkafka/pull/3428#issuecomment-878086386,,Since this PR blocks other work I'll merge it. We can revert the isolation.level changes per API later (but before next release) as we see fit.,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,3431,2021-06-16T21:42:39Z,2021-07-12T08:43:21Z,2021-07-12T08:43:26Z,MERGED,True,2,1,1,https://github.com/mpekalski,Mark sasl ready if relogin disabled,1,[],https://github.com/edenhill/librdkafka/pull/3431,https://github.com/mpekalski,1,https://github.com/edenhill/librdkafka/pull/3431,"It fixes the issue #3430
Basically if one has a cached kerberos ticket and disables relogin then sasl should be marked as ready and there should be no first attempt to refresh the ticket.","It fixes the issue #3430
Basically if one has a cached kerberos ticket and disables relogin then sasl should be marked as ready and there should be no first attempt to refresh the ticket.",True,"{'THUMBS_UP': ['https://github.com/andrew-szymanski', 'https://github.com/dyumin']}"
edenhill/librdkafka,https://github.com/edenhill/librdkafka,3431,2021-06-16T21:42:39Z,2021-07-12T08:43:21Z,2021-07-12T08:43:26Z,MERGED,True,2,1,1,https://github.com/mpekalski,Mark sasl ready if relogin disabled,1,[],https://github.com/edenhill/librdkafka/pull/3431,https://github.com/edenhill,2,https://github.com/edenhill/librdkafka/pull/3431#issuecomment-878090242,"It fixes the issue #3430
Basically if one has a cached kerberos ticket and disables relogin then sasl should be marked as ready and there should be no first attempt to refresh the ticket.",Thanks for this!,True,"{'THUMBS_UP': ['https://github.com/mpekalski', 'https://github.com/dyumin']}"
edenhill/librdkafka,https://github.com/edenhill/librdkafka,3435,2021-06-17T20:00:21Z,2021-08-23T06:52:47Z,2021-08-31T17:32:00Z,MERGED,True,5,0,2,https://github.com/hpk,Keep session alive during long consumer group rebalances,2,[],https://github.com/edenhill/librdkafka/pull/3435,https://github.com/hpk,1,https://github.com/edenhill/librdkafka/pull/3435,"If a consumer group rebalance takes longer than session.timeout.ms (for example, one of the consumers is taking a long time to complete a batch of messages), then other consumers will fail their session timeout check after some time. This is because heartbeat responses from the broker with a REBALANCE_IN_PROGRESS error do not update the session timeout.
This PR simply updates the timer when REBALANCE_IN_PROGRESS is received in a heartbeat response.
There was a similar improvement in the Java client here: apache/kafka#8834
That change modified the broker to send no error in response to heartbeats during a rebalance, but this PR should fix the issue for clients of older brokers.","If a consumer group rebalance takes longer than session.timeout.ms (for example, one of the consumers is taking a long time to complete a batch of messages), then other consumers will fail their session timeout check after some time. This is because heartbeat responses from the broker with a REBALANCE_IN_PROGRESS error do not update the session timeout.
This PR simply updates the timer when REBALANCE_IN_PROGRESS is received in a heartbeat response.
There was a similar improvement in the Java client here: apache/kafka#8834
That change modified the broker to send no error in response to heartbeats during a rebalance, but this PR should fix the issue for clients of older brokers.",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,3435,2021-06-17T20:00:21Z,2021-08-23T06:52:47Z,2021-08-31T17:32:00Z,MERGED,True,5,0,2,https://github.com/hpk,Keep session alive during long consumer group rebalances,2,[],https://github.com/edenhill/librdkafka/pull/3435,https://github.com/hpk,2,https://github.com/edenhill/librdkafka/pull/3435#issuecomment-885801265,"If a consumer group rebalance takes longer than session.timeout.ms (for example, one of the consumers is taking a long time to complete a batch of messages), then other consumers will fail their session timeout check after some time. This is because heartbeat responses from the broker with a REBALANCE_IN_PROGRESS error do not update the session timeout.
This PR simply updates the timer when REBALANCE_IN_PROGRESS is received in a heartbeat response.
There was a similar improvement in the Java client here: apache/kafka#8834
That change modified the broker to send no error in response to heartbeats during a rebalance, but this PR should fix the issue for clients of older brokers.",@edenhill Any thoughts on this? This is a pretty low-risk change that potentially fixes #3450,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,3435,2021-06-17T20:00:21Z,2021-08-23T06:52:47Z,2021-08-31T17:32:00Z,MERGED,True,5,0,2,https://github.com/hpk,Keep session alive during long consumer group rebalances,2,[],https://github.com/edenhill/librdkafka/pull/3435,https://github.com/edenhill,3,https://github.com/edenhill/librdkafka/pull/3435#issuecomment-903491925,"If a consumer group rebalance takes longer than session.timeout.ms (for example, one of the consumers is taking a long time to complete a batch of messages), then other consumers will fail their session timeout check after some time. This is because heartbeat responses from the broker with a REBALANCE_IN_PROGRESS error do not update the session timeout.
This PR simply updates the timer when REBALANCE_IN_PROGRESS is received in a heartbeat response.
There was a similar improvement in the Java client here: apache/kafka#8834
That change modified the broker to send no error in response to heartbeats during a rebalance, but this PR should fix the issue for clients of older brokers.","Good fix, thanks!",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,3436,2021-06-17T23:32:19Z,2021-07-12T08:39:55Z,2021-07-12T14:45:02Z,CLOSED,False,1,1,1,https://github.com/hallfox,Fix auto commit not running for non-subscription consumers,1,[],https://github.com/edenhill/librdkafka/pull/3436,https://github.com/hallfox,1,https://github.com/edenhill/librdkafka/pull/3436,Taking a shot at addressing the issue in #3420. Something's amiss with the rkcg_join_state at this point so we attempt to avoid the check unless there is a current subscription.,Taking a shot at addressing the issue in #3420. Something's amiss with the rkcg_join_state at this point so we attempt to avoid the check unless there is a current subscription.,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,3436,2021-06-17T23:32:19Z,2021-07-12T08:39:55Z,2021-07-12T14:45:02Z,CLOSED,False,1,1,1,https://github.com/hallfox,Fix auto commit not running for non-subscription consumers,1,[],https://github.com/edenhill/librdkafka/pull/3436,https://github.com/edenhill,2,https://github.com/edenhill/librdkafka/pull/3436#issuecomment-878088145,Taking a shot at addressing the issue in #3420. Something's amiss with the rkcg_join_state at this point so we attempt to avoid the check unless there is a current subscription.,"Thanks for your contribution, but closing this in favour of this #3454 which adds tests.",True,{'THUMBS_UP': ['https://github.com/hallfox']}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,3451,2021-07-09T04:08:40Z,2021-08-23T06:49:36Z,2021-08-23T06:49:36Z,CLOSED,False,0,1,1,https://github.com/hrchu,Remove allow.auto.create.topics,1,[],https://github.com/edenhill/librdkafka/pull/3451,https://github.com/hrchu,1,https://github.com/edenhill/librdkafka/pull/3451,Remove allow.auto.create.topics from CONFIGURATION.md since it doesn't work anymore.,Remove allow.auto.create.topics from CONFIGURATION.md since it doesn't work anymore.,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,3454,2021-07-12T08:39:02Z,2021-07-12T13:31:30Z,2021-07-12T13:31:37Z,MERGED,True,42,14,3,https://github.com/edenhill,Timed auto commits did not work when using only assign() and not subscribe(),2,[],https://github.com/edenhill/librdkafka/pull/3454,https://github.com/edenhill,1,https://github.com/edenhill/librdkafka/pull/3454,This regression was introduced in v1.7.0,This regression was introduced in v1.7.0,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,3455,2021-07-12T11:26:07Z,2021-07-12T15:07:52Z,2021-07-12T15:07:54Z,MERGED,True,884,301,17,https://github.com/edenhill,"Allow only one simultaneous Join or Sync GroupRequest, and make auto.offset.reset more reliable",7,[],https://github.com/edenhill/librdkafka/pull/3455,https://github.com/edenhill,1,https://github.com/edenhill/librdkafka/pull/3455,,,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,3455,2021-07-12T11:26:07Z,2021-07-12T15:07:52Z,2021-07-12T15:07:54Z,MERGED,True,884,301,17,https://github.com/edenhill,"Allow only one simultaneous Join or Sync GroupRequest, and make auto.offset.reset more reliable",7,[],https://github.com/edenhill/librdkafka/pull/3455,https://github.com/edenhill,2,https://github.com/edenhill/librdkafka/pull/3455#issuecomment-878201870,,"There should really be an initial.offset=beginning in addition to auto.offset.reset, that is:
if there is no committed offset (which is not an error), use initial.offset.  On error use auto.offset.reset.",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,3477,2021-07-28T20:52:04Z,2021-08-18T15:26:28Z,2021-08-18T15:26:29Z,CLOSED,False,6,0,1,https://github.com/chrisbeard,Fix DeleteRecords crash on empty partial response,1,[],https://github.com/edenhill/librdkafka/pull/3477,https://github.com/chrisbeard,1,https://github.com/edenhill/librdkafka/pull/3477,Addresses the crash outlined in #3476,Addresses the crash outlined in #3476,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,3477,2021-07-28T20:52:04Z,2021-08-18T15:26:28Z,2021-08-18T15:26:29Z,CLOSED,False,6,0,1,https://github.com/chrisbeard,Fix DeleteRecords crash on empty partial response,1,[],https://github.com/edenhill/librdkafka/pull/3477,https://github.com/edenhill,2,https://github.com/edenhill/librdkafka/pull/3477#issuecomment-901208979,Addresses the crash outlined in #3476,"This turned out to be a bit more complex as we want to propagate the error code from the failed request to the admin result.
Will file a PR for that, so closing this, but thanks anyway!",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,3490,2021-08-10T13:00:04Z,2021-08-12T18:59:06Z,2021-08-12T18:59:06Z,MERGED,True,146,106,11,https://github.com/edenhill,Use vcpkgs for dependencies on Windows,2,[],https://github.com/edenhill/librdkafka/pull/3490,https://github.com/edenhill,1,https://github.com/edenhill/librdkafka/pull/3490,"This addresses the zlib upgrade in #2934 and makes sure we'll have up-to-date dependencies going forward.
It also makes adding additional dependencies easier.","This addresses the zlib upgrade in #2934 and makes sure we'll have up-to-date dependencies going forward.
It also makes adding additional dependencies easier.",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,3493,2021-08-11T07:06:08Z,2021-08-12T18:57:37Z,2021-08-12T18:57:39Z,MERGED,True,12,10,6,https://github.com/edenhill,Rename crc32c() to rd_crc32c() to avoid conflict with other (static) libraries (#3461),1,[],https://github.com/edenhill/librdkafka/pull/3493,https://github.com/edenhill,1,https://github.com/edenhill/librdkafka/pull/3493,,,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,3494,2021-08-11T08:29:33Z,2021-08-12T18:58:15Z,2021-08-12T18:58:18Z,MERGED,True,126,11,11,https://github.com/edenhill,Override linger.ms when flush()ing (#3489),2,[],https://github.com/edenhill/librdkafka/pull/3494,https://github.com/edenhill,1,https://github.com/edenhill/librdkafka/pull/3494,,,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,3496,2021-08-12T01:48:04Z,2022-05-12T08:26:45Z,2022-05-12T08:26:46Z,CLOSED,False,9890,640,120,https://github.com/garrett528,sasl: Enable AWS_MSK_IAM SASL mechanism (#3402),14,[],https://github.com/edenhill/librdkafka/pull/3496,https://github.com/garrett528,1,https://github.com/edenhill/librdkafka/pull/3496,"AWS_MSK_IAM is a new SASL mechanism for
authenticating clients to AWS MSK Kafka
clusters and use IAM-based controls to
set Kafka ACLs and permissions. This change
provides support to allow clients to pass
AWS credentials at runtime which is used
to build the SASL payload and authenticate
clients to IAM enabled MSK clusters. It adds
a new SASL mechanism, AWS_MSK_IAM, as well
as configuration options to set the following:

AWS access key id
AWS secret access key
AWS region
AWS security token
The SASL handshake requires a specific payload
that is described here:
https://github.com/aws/aws-msk-iam-auth

NOTE: As of now, the changes in this PR will allow librdkafka to connect for the valid duration of the credentials provided. For permanent credentials, the connection will stay open indefinitely. For STS temporary credentials, the connection will stay open until those credentials expire. We have some work slated over the next couple of weeks to enable a refresh callback where clients provide updated credentials as needed. We will be following the example from the sasl_oauthbearer to provide a similar mechanism where an event can be handled by the client to provide refreshed credentials. Please expect that to be in a follow-up PR.","AWS_MSK_IAM is a new SASL mechanism for
authenticating clients to AWS MSK Kafka
clusters and use IAM-based controls to
set Kafka ACLs and permissions. This change
provides support to allow clients to pass
AWS credentials at runtime which is used
to build the SASL payload and authenticate
clients to IAM enabled MSK clusters. It adds
a new SASL mechanism, AWS_MSK_IAM, as well
as configuration options to set the following:

AWS access key id
AWS secret access key
AWS region
AWS security token
The SASL handshake requires a specific payload
that is described here:
https://github.com/aws/aws-msk-iam-auth

NOTE: As of now, the changes in this PR will allow librdkafka to connect for the valid duration of the credentials provided. For permanent credentials, the connection will stay open indefinitely. For STS temporary credentials, the connection will stay open until those credentials expire. We have some work slated over the next couple of weeks to enable a refresh callback where clients provide updated credentials as needed. We will be following the example from the sasl_oauthbearer to provide a similar mechanism where an event can be handled by the client to provide refreshed credentials. Please expect that to be in a follow-up PR.",True,"{'THUMBS_UP': ['https://github.com/ipetrushevskiy', 'https://github.com/maxkochubey', 'https://github.com/brokenbot', 'https://github.com/kppullin']}"
edenhill/librdkafka,https://github.com/edenhill/librdkafka,3496,2021-08-12T01:48:04Z,2022-05-12T08:26:45Z,2022-05-12T08:26:46Z,CLOSED,False,9890,640,120,https://github.com/garrett528,sasl: Enable AWS_MSK_IAM SASL mechanism (#3402),14,[],https://github.com/edenhill/librdkafka/pull/3496,https://github.com/edenhill,2,https://github.com/edenhill/librdkafka/pull/3496#issuecomment-901183346,"AWS_MSK_IAM is a new SASL mechanism for
authenticating clients to AWS MSK Kafka
clusters and use IAM-based controls to
set Kafka ACLs and permissions. This change
provides support to allow clients to pass
AWS credentials at runtime which is used
to build the SASL payload and authenticate
clients to IAM enabled MSK clusters. It adds
a new SASL mechanism, AWS_MSK_IAM, as well
as configuration options to set the following:

AWS access key id
AWS secret access key
AWS region
AWS security token
The SASL handshake requires a specific payload
that is described here:
https://github.com/aws/aws-msk-iam-auth

NOTE: As of now, the changes in this PR will allow librdkafka to connect for the valid duration of the credentials provided. For permanent credentials, the connection will stay open indefinitely. For STS temporary credentials, the connection will stay open until those credentials expire. We have some work slated over the next couple of weeks to enable a refresh callback where clients provide updated credentials as needed. We will be following the example from the sasl_oauthbearer to provide a similar mechanism where an event can be handled by the client to provide refreshed credentials. Please expect that to be in a follow-up PR.","I'm sorry Garrett but before you get to far with this PR I need to remind you, as mentioned on the discussion thread already; I really don't like to see inclusion of vendor-specific functionality in librdkafka, so the likelyhood of this PR getting merged is very low.
Highly recommend to take the formal open source route and submit a KIP for your new proposed SASL mechanism, or even better, use an existing standardized non-vendor-specific one (e.g., OAUTHBEARER).",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,3496,2021-08-12T01:48:04Z,2022-05-12T08:26:45Z,2022-05-12T08:26:46Z,CLOSED,False,9890,640,120,https://github.com/garrett528,sasl: Enable AWS_MSK_IAM SASL mechanism (#3402),14,[],https://github.com/edenhill/librdkafka/pull/3496,https://github.com/garrett528,3,https://github.com/edenhill/librdkafka/pull/3496#issuecomment-901949573,"AWS_MSK_IAM is a new SASL mechanism for
authenticating clients to AWS MSK Kafka
clusters and use IAM-based controls to
set Kafka ACLs and permissions. This change
provides support to allow clients to pass
AWS credentials at runtime which is used
to build the SASL payload and authenticate
clients to IAM enabled MSK clusters. It adds
a new SASL mechanism, AWS_MSK_IAM, as well
as configuration options to set the following:

AWS access key id
AWS secret access key
AWS region
AWS security token
The SASL handshake requires a specific payload
that is described here:
https://github.com/aws/aws-msk-iam-auth

NOTE: As of now, the changes in this PR will allow librdkafka to connect for the valid duration of the credentials provided. For permanent credentials, the connection will stay open indefinitely. For STS temporary credentials, the connection will stay open until those credentials expire. We have some work slated over the next couple of weeks to enable a refresh callback where clients provide updated credentials as needed. We will be following the example from the sasl_oauthbearer to provide a similar mechanism where an event can be handled by the client to provide refreshed credentials. Please expect that to be in a follow-up PR.","I'm sorry Garrett but before you get to far with this PR I need to remind you, as mentioned on the discussion thread already; I really don't like to see inclusion of vendor-specific functionality in librdkafka, so the likelyhood of this PR getting merged is very low.
Highly recommend to take the formal open source route and submit a KIP for your new proposed SASL mechanism, or even better, use an existing standardized non-vendor-specific one (e.g., OAUTHBEARER).

thanks for the response. we're going to have to continue on our fork in the meantime since we need this functionality now and can't wait for the KIP process. hopefully there's a path forward to support this functionality in librdkafka since it's a crucial library for the Kafka ecosystem and there are a lot of AWS customers out there (like ourselves) that are being pushed to use IAM auth by their respective security teams.
would you be open to having some sort of SASL plugin mechanism that would allow for custom implementations? it seems like we could build around the rd_kafka_sasl_provider interface to allow callers to provide implementation details that would live outside of the librdkafka repo. if you were open to that, it would be great to collaborate on that design so that you're happy with whatever gets built.",True,"{'THUMBS_UP': ['https://github.com/ipetrushevskiy', 'https://github.com/rafael-herscovici-cko', 'https://github.com/erichulburd']}"
edenhill/librdkafka,https://github.com/edenhill/librdkafka,3496,2021-08-12T01:48:04Z,2022-05-12T08:26:45Z,2022-05-12T08:26:46Z,CLOSED,False,9890,640,120,https://github.com/garrett528,sasl: Enable AWS_MSK_IAM SASL mechanism (#3402),14,[],https://github.com/edenhill/librdkafka/pull/3496,https://github.com/tooptoop4,4,https://github.com/edenhill/librdkafka/pull/3496#issuecomment-956697234,"AWS_MSK_IAM is a new SASL mechanism for
authenticating clients to AWS MSK Kafka
clusters and use IAM-based controls to
set Kafka ACLs and permissions. This change
provides support to allow clients to pass
AWS credentials at runtime which is used
to build the SASL payload and authenticate
clients to IAM enabled MSK clusters. It adds
a new SASL mechanism, AWS_MSK_IAM, as well
as configuration options to set the following:

AWS access key id
AWS secret access key
AWS region
AWS security token
The SASL handshake requires a specific payload
that is described here:
https://github.com/aws/aws-msk-iam-auth

NOTE: As of now, the changes in this PR will allow librdkafka to connect for the valid duration of the credentials provided. For permanent credentials, the connection will stay open indefinitely. For STS temporary credentials, the connection will stay open until those credentials expire. We have some work slated over the next couple of weeks to enable a refresh callback where clients provide updated credentials as needed. We will be following the example from the sasl_oauthbearer to provide a similar mechanism where an event can be handled by the client to provide refreshed credentials. Please expect that to be in a follow-up PR.",@garrett528 have u built kafkacat that works with IAM MSK too?,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,3496,2021-08-12T01:48:04Z,2022-05-12T08:26:45Z,2022-05-12T08:26:46Z,CLOSED,False,9890,640,120,https://github.com/garrett528,sasl: Enable AWS_MSK_IAM SASL mechanism (#3402),14,[],https://github.com/edenhill/librdkafka/pull/3496,https://github.com/garrett528,5,https://github.com/edenhill/librdkafka/pull/3496#issuecomment-956947512,"AWS_MSK_IAM is a new SASL mechanism for
authenticating clients to AWS MSK Kafka
clusters and use IAM-based controls to
set Kafka ACLs and permissions. This change
provides support to allow clients to pass
AWS credentials at runtime which is used
to build the SASL payload and authenticate
clients to IAM enabled MSK clusters. It adds
a new SASL mechanism, AWS_MSK_IAM, as well
as configuration options to set the following:

AWS access key id
AWS secret access key
AWS region
AWS security token
The SASL handshake requires a specific payload
that is described here:
https://github.com/aws/aws-msk-iam-auth

NOTE: As of now, the changes in this PR will allow librdkafka to connect for the valid duration of the credentials provided. For permanent credentials, the connection will stay open indefinitely. For STS temporary credentials, the connection will stay open until those credentials expire. We have some work slated over the next couple of weeks to enable a refresh callback where clients provide updated credentials as needed. We will be following the example from the sasl_oauthbearer to provide a similar mechanism where an event can be handled by the client to provide refreshed credentials. Please expect that to be in a follow-up PR.","@garrett528 have u built kafkacat that works with IAM MSK too?

yes, got that working. i'll add to the README in the fork if you're interested. unfortunately, it will involve building librdkafka and kcat from source. it doesn't look like this is going to be accepted any time soon but better the fork than nothing.",True,"{'THUMBS_UP': ['https://github.com/tooptoop4', 'https://github.com/digicyc']}"
edenhill/librdkafka,https://github.com/edenhill/librdkafka,3496,2021-08-12T01:48:04Z,2022-05-12T08:26:45Z,2022-05-12T08:26:46Z,CLOSED,False,9890,640,120,https://github.com/garrett528,sasl: Enable AWS_MSK_IAM SASL mechanism (#3402),14,[],https://github.com/edenhill/librdkafka/pull/3496,https://github.com/tooptoop4,6,https://github.com/edenhill/librdkafka/pull/3496#issuecomment-957534244,"AWS_MSK_IAM is a new SASL mechanism for
authenticating clients to AWS MSK Kafka
clusters and use IAM-based controls to
set Kafka ACLs and permissions. This change
provides support to allow clients to pass
AWS credentials at runtime which is used
to build the SASL payload and authenticate
clients to IAM enabled MSK clusters. It adds
a new SASL mechanism, AWS_MSK_IAM, as well
as configuration options to set the following:

AWS access key id
AWS secret access key
AWS region
AWS security token
The SASL handshake requires a specific payload
that is described here:
https://github.com/aws/aws-msk-iam-auth

NOTE: As of now, the changes in this PR will allow librdkafka to connect for the valid duration of the credentials provided. For permanent credentials, the connection will stay open indefinitely. For STS temporary credentials, the connection will stay open until those credentials expire. We have some work slated over the next couple of weeks to enable a refresh callback where clients provide updated credentials as needed. We will be following the example from the sasl_oauthbearer to provide a similar mechanism where an event can be handled by the client to provide refreshed credentials. Please expect that to be in a follow-up PR.",@garrett528 that would be great. does it work with IAM role (like for java clients) instead of supplying aws keys/tokens  that need to be rotated?,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,3496,2021-08-12T01:48:04Z,2022-05-12T08:26:45Z,2022-05-12T08:26:46Z,CLOSED,False,9890,640,120,https://github.com/garrett528,sasl: Enable AWS_MSK_IAM SASL mechanism (#3402),14,[],https://github.com/edenhill/librdkafka/pull/3496,https://github.com/shrinjay-mio,7,https://github.com/edenhill/librdkafka/pull/3496#issuecomment-961302357,"AWS_MSK_IAM is a new SASL mechanism for
authenticating clients to AWS MSK Kafka
clusters and use IAM-based controls to
set Kafka ACLs and permissions. This change
provides support to allow clients to pass
AWS credentials at runtime which is used
to build the SASL payload and authenticate
clients to IAM enabled MSK clusters. It adds
a new SASL mechanism, AWS_MSK_IAM, as well
as configuration options to set the following:

AWS access key id
AWS secret access key
AWS region
AWS security token
The SASL handshake requires a specific payload
that is described here:
https://github.com/aws/aws-msk-iam-auth

NOTE: As of now, the changes in this PR will allow librdkafka to connect for the valid duration of the credentials provided. For permanent credentials, the connection will stay open indefinitely. For STS temporary credentials, the connection will stay open until those credentials expire. We have some work slated over the next couple of weeks to enable a refresh callback where clients provide updated credentials as needed. We will be following the example from the sasl_oauthbearer to provide a similar mechanism where an event can be handled by the client to provide refreshed credentials. Please expect that to be in a follow-up PR.","@garrett528 Hi Garrett, we're looking at using your fork for our .NET application, have you already tried integrating your changes into confluent-kafka-dotnet? If so, how did you achieve that? Just swapping the DLLs that confluence-kafka-dotnet was pointing to?
Further, do you know what consequences this has for maintainability and receiving updates, specifically security updates?",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,3496,2021-08-12T01:48:04Z,2022-05-12T08:26:45Z,2022-05-12T08:26:46Z,CLOSED,False,9890,640,120,https://github.com/garrett528,sasl: Enable AWS_MSK_IAM SASL mechanism (#3402),14,[],https://github.com/edenhill/librdkafka/pull/3496,https://github.com/garrett528,8,https://github.com/edenhill/librdkafka/pull/3496#issuecomment-961434473,"AWS_MSK_IAM is a new SASL mechanism for
authenticating clients to AWS MSK Kafka
clusters and use IAM-based controls to
set Kafka ACLs and permissions. This change
provides support to allow clients to pass
AWS credentials at runtime which is used
to build the SASL payload and authenticate
clients to IAM enabled MSK clusters. It adds
a new SASL mechanism, AWS_MSK_IAM, as well
as configuration options to set the following:

AWS access key id
AWS secret access key
AWS region
AWS security token
The SASL handshake requires a specific payload
that is described here:
https://github.com/aws/aws-msk-iam-auth

NOTE: As of now, the changes in this PR will allow librdkafka to connect for the valid duration of the credentials provided. For permanent credentials, the connection will stay open indefinitely. For STS temporary credentials, the connection will stay open until those credentials expire. We have some work slated over the next couple of weeks to enable a refresh callback where clients provide updated credentials as needed. We will be following the example from the sasl_oauthbearer to provide a similar mechanism where an event can be handled by the client to provide refreshed credentials. Please expect that to be in a follow-up PR.","@tooptoop4 still need to update the directions on our fork for kcat but yes the underlying library can optionally use STS for generating temp creds based on roles (that's what we're using internally) or take permanent credentials. The duration_seconds will determine how long the temp creds last for and the library has a mechanism to automatically refresh those credentials 80% of the way through their lifetime so we always have valid credentials.
@shrinjay-mio we're going to try our best to keep the fork up-to-date with the mainline changes but i can't guarantee timelines. if there are concerns, please open up PRs against the fork that we can review (definitely open to collaborating on this stuff as needed).
i haven't used confluent-kafka-dotnet but i have used confluent-kafka-go and there is a way to compile the package using dynamic linking. when you build the forked librdkafka from source, you generate an .so file that can be used instead of the static .a file. i'll add directions on how to do that for Go and maybe that will inform you on how to handle it in Dotnet (i'm not a Windows/dotnet guy so not sure what the exact terminology and build procedure looks like there)
ideally, we would be able to come to an agreement with the maintainer and merge this into the main branch and remove our fork. i'd rather help maintain this library than have to periodically fetch and merge upstream. however, that seems unlikely based on the responses in this thread.",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,3496,2021-08-12T01:48:04Z,2022-05-12T08:26:45Z,2022-05-12T08:26:46Z,CLOSED,False,9890,640,120,https://github.com/garrett528,sasl: Enable AWS_MSK_IAM SASL mechanism (#3402),14,[],https://github.com/edenhill/librdkafka/pull/3496,https://github.com/shrinjay-mio,9,https://github.com/edenhill/librdkafka/pull/3496#issuecomment-961445923,"AWS_MSK_IAM is a new SASL mechanism for
authenticating clients to AWS MSK Kafka
clusters and use IAM-based controls to
set Kafka ACLs and permissions. This change
provides support to allow clients to pass
AWS credentials at runtime which is used
to build the SASL payload and authenticate
clients to IAM enabled MSK clusters. It adds
a new SASL mechanism, AWS_MSK_IAM, as well
as configuration options to set the following:

AWS access key id
AWS secret access key
AWS region
AWS security token
The SASL handshake requires a specific payload
that is described here:
https://github.com/aws/aws-msk-iam-auth

NOTE: As of now, the changes in this PR will allow librdkafka to connect for the valid duration of the credentials provided. For permanent credentials, the connection will stay open indefinitely. For STS temporary credentials, the connection will stay open until those credentials expire. We have some work slated over the next couple of weeks to enable a refresh callback where clients provide updated credentials as needed. We will be following the example from the sasl_oauthbearer to provide a similar mechanism where an event can be handled by the client to provide refreshed credentials. Please expect that to be in a follow-up PR.","Sounds good! It would be really helpful to see the instructions for Go, just tried building it but unfortunately I'm getting an error that the ""sasl.aws.access.key.id"" doesn't exist when I try to build a producer.",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,3496,2021-08-12T01:48:04Z,2022-05-12T08:26:45Z,2022-05-12T08:26:46Z,CLOSED,False,9890,640,120,https://github.com/garrett528,sasl: Enable AWS_MSK_IAM SASL mechanism (#3402),14,[],https://github.com/edenhill/librdkafka/pull/3496,https://github.com/garrett528,10,https://github.com/edenhill/librdkafka/pull/3496#issuecomment-964276673,"AWS_MSK_IAM is a new SASL mechanism for
authenticating clients to AWS MSK Kafka
clusters and use IAM-based controls to
set Kafka ACLs and permissions. This change
provides support to allow clients to pass
AWS credentials at runtime which is used
to build the SASL payload and authenticate
clients to IAM enabled MSK clusters. It adds
a new SASL mechanism, AWS_MSK_IAM, as well
as configuration options to set the following:

AWS access key id
AWS secret access key
AWS region
AWS security token
The SASL handshake requires a specific payload
that is described here:
https://github.com/aws/aws-msk-iam-auth

NOTE: As of now, the changes in this PR will allow librdkafka to connect for the valid duration of the credentials provided. For permanent credentials, the connection will stay open indefinitely. For STS temporary credentials, the connection will stay open until those credentials expire. We have some work slated over the next couple of weeks to enable a refresh callback where clients provide updated credentials as needed. We will be following the example from the sasl_oauthbearer to provide a similar mechanism where an event can be handled by the client to provide refreshed credentials. Please expect that to be in a follow-up PR.","@shrinjay-mio @tooptoop4 please see the documentation here https://github.com/UrbanCompass/librdkafka/blob/master/README_FORK.md.
if you build a container image with this, i'd recommend using an Ubuntu 20 image or something more recent so the required packages are able to be installed.",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,3496,2021-08-12T01:48:04Z,2022-05-12T08:26:45Z,2022-05-12T08:26:46Z,CLOSED,False,9890,640,120,https://github.com/garrett528,sasl: Enable AWS_MSK_IAM SASL mechanism (#3402),14,[],https://github.com/edenhill/librdkafka/pull/3496,https://github.com/shrinjay-mio,11,https://github.com/edenhill/librdkafka/pull/3496#issuecomment-964312526,"AWS_MSK_IAM is a new SASL mechanism for
authenticating clients to AWS MSK Kafka
clusters and use IAM-based controls to
set Kafka ACLs and permissions. This change
provides support to allow clients to pass
AWS credentials at runtime which is used
to build the SASL payload and authenticate
clients to IAM enabled MSK clusters. It adds
a new SASL mechanism, AWS_MSK_IAM, as well
as configuration options to set the following:

AWS access key id
AWS secret access key
AWS region
AWS security token
The SASL handshake requires a specific payload
that is described here:
https://github.com/aws/aws-msk-iam-auth

NOTE: As of now, the changes in this PR will allow librdkafka to connect for the valid duration of the credentials provided. For permanent credentials, the connection will stay open indefinitely. For STS temporary credentials, the connection will stay open until those credentials expire. We have some work slated over the next couple of weeks to enable a refresh callback where clients provide updated credentials as needed. We will be following the example from the sasl_oauthbearer to provide a similar mechanism where an event can be handled by the client to provide refreshed credentials. Please expect that to be in a follow-up PR.","@garrett528 The .NET build system is quite a bit different from how Go handles dependencies, using this fork essentially requires the user to uninstall the librdkafka redistributable that comes with confluent-kafka-dotnet, rebuild confluent-kafka-dotnet, and install that new version. I can verify that once that's been done, this fork works with confluent-kafka-dotnet.",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,3496,2021-08-12T01:48:04Z,2022-05-12T08:26:45Z,2022-05-12T08:26:46Z,CLOSED,False,9890,640,120,https://github.com/garrett528,sasl: Enable AWS_MSK_IAM SASL mechanism (#3402),14,[],https://github.com/edenhill/librdkafka/pull/3496,https://github.com/tooptoop4,12,https://github.com/edenhill/librdkafka/pull/3496#issuecomment-964849512,"AWS_MSK_IAM is a new SASL mechanism for
authenticating clients to AWS MSK Kafka
clusters and use IAM-based controls to
set Kafka ACLs and permissions. This change
provides support to allow clients to pass
AWS credentials at runtime which is used
to build the SASL payload and authenticate
clients to IAM enabled MSK clusters. It adds
a new SASL mechanism, AWS_MSK_IAM, as well
as configuration options to set the following:

AWS access key id
AWS secret access key
AWS region
AWS security token
The SASL handshake requires a specific payload
that is described here:
https://github.com/aws/aws-msk-iam-auth

NOTE: As of now, the changes in this PR will allow librdkafka to connect for the valid duration of the credentials provided. For permanent credentials, the connection will stay open indefinitely. For STS temporary credentials, the connection will stay open until those credentials expire. We have some work slated over the next couple of weeks to enable a refresh callback where clients provide updated credentials as needed. We will be following the example from the sasl_oauthbearer to provide a similar mechanism where an event can be handled by the client to provide refreshed credentials. Please expect that to be in a follow-up PR.","@garrett528 getting this error when running make (also https://app.travis-ci.com/github/edenhill/librdkafka/jobs/547340793 has similar error)
checking for OS or distribution... ok (amazon)
checking for C compiler from CC env... failed
checking for gcc (by command)... ok
checking for C++ compiler from CXX env... failed
checking for C++ compiler (g++)... ok
checking executable ld... ok
checking executable nm... ok
checking executable objdump... ok
checking executable strip... ok
checking executable libtool... ok
checking executable ranlib... ok
checking for pkgconfig (by command)... ok
checking for install (by command)... ok
checking for GNU ar... ok
checking for PIC (by compile)... ok
checking for GNU-compatible linker options... ok
checking for GNU linker-script ld flag... ok
checking for __atomic_32 (by compile)... ok
checking for __atomic_64 (by compile)... ok
checking for socket (by compile)... ok
parsing version '0x010802ff'... ok (1.8.2)
checking for librt (by pkg-config)... failed
checking for librt (by compile)... ok
checking for libpthread (by pkg-config)... failed
checking for libpthread (by compile)... ok
checking for c11threads (by pkg-config)... failed
checking for c11threads (by compile)... failed (disable)
checking for libdl (by pkg-config)... failed
checking for libdl (by compile)... ok
checking for zlib (by pkg-config)... ok
checking for libcrypto (by pkg-config)... ok
checking for libssl (by pkg-config)... ok
checking for libsasl2 (by pkg-config)... ok
checking for libzstd (by pkg-config)... ok
checking for libcurl (by pkg-config)... ok
checking for libm (by pkg-config)... failed
checking for libm (by compile)... ok
checking for liblz4 (by pkg-config)... failed
checking for liblz4 (by compile)... failed (disable)
checking for syslog (by compile)... ok
checking for rapidjson (by compile)... failed (disable)
checking for curl (by pkg-config)... failed
checking for curl (by compile)... ok
checking for libxml2 (by pkg-config)... failed
checking for libxml2 (by compile)... failed (disable)
checking for crc32chw (by compile)... ok
checking for regex (by compile)... ok
checking for rand_r (by compile)... ok
checking for strndup (by compile)... ok
checking for strlcpy (by compile)... failed (disable)
checking for strerror_r (by compile)... ok
checking for strcasestr (by compile)... ok
checking for pthread_setname_gnu (by compile)... ok
checking for nm (by env NM)... ok (cached)
checking for python3 (by command)... ok
checking for getrusage (by compile)... ok
Generated Makefile.config
Generated config.h

Configuration summary:
  prefix                   /usr/local
  MKL_DISTRO               amazon
  SOLIB_EXT                .so
  ARCH                     x86_64
  CPU                      generic
  GEN_PKG_CONFIG           y
  MKL_APP_NAME             librdkafka
  MKL_APP_DESC_ONELINE     The Apache Kafka C/C++ library
  CC                       gcc
  CXX                      g++
  LD                       ld
  NM                       nm
  OBJDUMP                  objdump
  STRIP                    strip
  LIBTOOL                  libtool
  RANLIB                   ranlib
  CPPFLAGS                 -g -O2 -fPIC -Wall -Wsign-compare -Wfloat-equal -Wpointer-arith -Wcast-align
  PKG_CONFIG               pkg-config
  INSTALL                  /usr/bin/install
  HAS_GNU_AR               y
  LIB_LDFLAGS              -shared -Wl,-soname,$(LIBFILENAME)
  LDFLAG_LINKERSCRIPT      -Wl,--version-script=
  RDKAFKA_VERSION_STR      1.8.2
  MKL_APP_VERSION          1.8.2
  LIBS                     -lcurl -lm -lcurl   -lzstd   -lsasl2   -lssl -lcrypto   -lcrypto   -lz   -ldl -lpthread -lrt
  MKL_PKGCONFIG_LIBS_PRIVATE -lcurl -lm -ldl -lpthread -lrt
  MKL_PKGCONFIG_REQUIRES_PRIVATE zlib libcrypto libssl libsasl2 libzstd libcurl
  CFLAGS
  MKL_PKGCONFIG_REQUIRES   zlib libcrypto libssl libsasl2 libzstd libcurl
  CXXFLAGS                 -Wno-non-virtual-dtor
  SYMDUMPER                $(NM) -D
  MKL_DYNAMIC_LIBS         -lcurl -lm -lcurl -lzstd -lsasl2 -lssl -lcrypto -lcrypto -lz -ldl -lpthread -lrt
  exec_prefix              /usr/local
  bindir                   /usr/local/bin
  sbindir                  /usr/local/sbin
  libexecdir               /usr/local/libexec
  datadir                  /usr/local/share
  sysconfdir               /usr/local/etc
  sharedstatedir           /usr/local/com
  localstatedir            /usr/local/var
  runstatedir              /usr/local/var/run
  libdir                   /usr/local/lib
  includedir               /usr/local/include
  infodir                  /usr/local/info
  mandir                   /usr/local/man
  BUILT_WITH               GCC GXX PKGCONFIG INSTALL GNULD LDS LIBDL PLUGINS ZLIB SSL SASL_CYRUS ZSTD CURL HDRHISTOGRAM SYSLOG SNAPPY SOCKEM SASL_SCRAM SASL_OAUTHBEARER CRC32C_HW
Generated config.cache

Now type 'make' to build



$ make
make[1]: Entering directory `/home/user/r/librdkafka-1.9.0-AWS_MSK_IAM/src'
gcc -MD -MP -g -O2 -fPIC -Wall -Wsign-compare -Wfloat-equal -Wpointer-arith -Wcast-align  -c rdkafka.c -o rdkafka.o
gcc -MD -MP -g -O2 -fPIC -Wall -Wsign-compare -Wfloat-equal -Wpointer-arith -Wcast-align  -c rdkafka_broker.c -o rdkafka_broker.o
gcc -MD -MP -g -O2 -fPIC -Wall -Wsign-compare -Wfloat-equal -Wpointer-arith -Wcast-align  -c rdkafka_msg.c -o rdkafka_msg.o
gcc -MD -MP -g -O2 -fPIC -Wall -Wsign-compare -Wfloat-equal -Wpointer-arith -Wcast-align  -c rdkafka_topic.c -o rdkafka_topic.o
gcc -MD -MP -g -O2 -fPIC -Wall -Wsign-compare -Wfloat-equal -Wpointer-arith -Wcast-align  -c rdkafka_conf.c -o rdkafka_conf.o
In file included from /usr/include/openssl/buffer.h:68:0,
                 from /usr/include/openssl/x509.h:70,
                 from /usr/include/openssl/ssl.h:156,
                 from rdkafka_int.h:55,
                 from rdkafka_conf.c:29:
rdkafka_conf.c:127:21: error: struct <anonymous> has no member named aws_access_key_id
 #define _RK(field)  offsetof(rd_kafka_conf_t, field)
                     ^
rdkafka_conf.c:907:6: note: in expansion of macro _RK
      _RK(sasl.aws_access_key_id),
      ^~~
rdkafka_conf.c:127:21: error: struct <anonymous> has no member named aws_secret_access_key
 #define _RK(field)  offsetof(rd_kafka_conf_t, field)
                     ^
rdkafka_conf.c:910:6: note: in expansion of macro _RK
      _RK(sasl.aws_secret_access_key),
      ^~~
rdkafka_conf.c:127:21: error: struct <anonymous> has no member named aws_region
 #define _RK(field)  offsetof(rd_kafka_conf_t, field)
                     ^
rdkafka_conf.c:913:6: note: in expansion of macro _RK
      _RK(sasl.aws_region),
      ^~~
rdkafka_conf.c:127:21: error: struct <anonymous> has no member named enable_use_sts
 #define _RK(field)  offsetof(rd_kafka_conf_t, field)
                     ^
rdkafka_conf.c:916:6: note: in expansion of macro _RK
      _RK(sasl.enable_use_sts),
      ^~~
rdkafka_conf.c:127:21: error: struct <anonymous> has no member named aws_security_token
 #define _RK(field)  offsetof(rd_kafka_conf_t, field)
                     ^
rdkafka_conf.c:922:6: note: in expansion of macro _RK
      _RK(sasl.aws_security_token),
      ^~~
rdkafka_conf.c:127:21: error: struct <anonymous> has no member named role_arn
 #define _RK(field)  offsetof(rd_kafka_conf_t, field)
                     ^
rdkafka_conf.c:926:6: note: in expansion of macro _RK
      _RK(sasl.role_arn), ""AWS RoleARN to use for calling STS.""},
      ^~~
rdkafka_conf.c:127:21: error: struct <anonymous> has no member named role_session_name
 #define _RK(field)  offsetof(rd_kafka_conf_t, field)
                     ^
rdkafka_conf.c:928:6: note: in expansion of macro _RK
      _RK(sasl.role_session_name), ""Session name to use for STS AssumeRole.""},
      ^~~
rdkafka_conf.c:127:21: error: struct <anonymous> has no member named duration_sec
 #define _RK(field)  offsetof(rd_kafka_conf_t, field)
                     ^
rdkafka_conf.c:930:7: note: in expansion of macro _RK
       _RK(sasl.duration_sec), ""The duration, in seconds, of the role session. ""
       ^~~
make[1]: *** [rdkafka_conf.o] Error 1
make[1]: Leaving directory `/home/user/r/librdkafka-1.9.0-AWS_MSK_IAM/src'
make: *** [libs] Error 2",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,3496,2021-08-12T01:48:04Z,2022-05-12T08:26:45Z,2022-05-12T08:26:46Z,CLOSED,False,9890,640,120,https://github.com/garrett528,sasl: Enable AWS_MSK_IAM SASL mechanism (#3402),14,[],https://github.com/edenhill/librdkafka/pull/3496,https://github.com/garrett528,13,https://github.com/edenhill/librdkafka/pull/3496#issuecomment-965144024,"AWS_MSK_IAM is a new SASL mechanism for
authenticating clients to AWS MSK Kafka
clusters and use IAM-based controls to
set Kafka ACLs and permissions. This change
provides support to allow clients to pass
AWS credentials at runtime which is used
to build the SASL payload and authenticate
clients to IAM enabled MSK clusters. It adds
a new SASL mechanism, AWS_MSK_IAM, as well
as configuration options to set the following:

AWS access key id
AWS secret access key
AWS region
AWS security token
The SASL handshake requires a specific payload
that is described here:
https://github.com/aws/aws-msk-iam-auth

NOTE: As of now, the changes in this PR will allow librdkafka to connect for the valid duration of the credentials provided. For permanent credentials, the connection will stay open indefinitely. For STS temporary credentials, the connection will stay open until those credentials expire. We have some work slated over the next couple of weeks to enable a refresh callback where clients provide updated credentials as needed. We will be following the example from the sasl_oauthbearer to provide a similar mechanism where an event can be handled by the client to provide refreshed credentials. Please expect that to be in a follow-up PR.","@tooptoop4 doesn't look like the ./configure call is finding libxml2. you need to make sure pkgconfig can find the corresponding .pc file for it (for Ubuntu 20, it's usually located in /usr/lib/x86_64-linux-gnu/pkgconfig/libxml-2.0.pc and you may need to symlink it to a path where pkgconfig can find it. for example in Ubuntu, ln -sf /usr/lib/x86_64-linux-gnu/pkgconfig/libxml-2.0.pc /usr/lib/x86_64-linux-gnu/pkgconfig/libxml2.pc.
if you don't know where it is, try find / -name libxml-2.0.pc. once that's correctly linked, you should see one of these switch to ""ok"":
checking for libxml2 (by pkg-config)... failed
checking for libxml2 (by compile)... failed (disable)

and BUILT_WITH               GCC GXX PKGCONFIG INSTALL GNULD LDS LIBDL PLUGINS ZLIB SSL SASL_CYRUS ZSTD CURL HDRHISTOGRAM SYSLOG SNAPPY SOCKEM SASL_SCRAM SASL_OAUTHBEARER CRC32C_HW will have ""SASL_AWS_MSK_IAM""",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,3496,2021-08-12T01:48:04Z,2022-05-12T08:26:45Z,2022-05-12T08:26:46Z,CLOSED,False,9890,640,120,https://github.com/garrett528,sasl: Enable AWS_MSK_IAM SASL mechanism (#3402),14,[],https://github.com/edenhill/librdkafka/pull/3496,https://github.com/tooptoop4,14,https://github.com/edenhill/librdkafka/pull/3496#issuecomment-965792011,"AWS_MSK_IAM is a new SASL mechanism for
authenticating clients to AWS MSK Kafka
clusters and use IAM-based controls to
set Kafka ACLs and permissions. This change
provides support to allow clients to pass
AWS credentials at runtime which is used
to build the SASL payload and authenticate
clients to IAM enabled MSK clusters. It adds
a new SASL mechanism, AWS_MSK_IAM, as well
as configuration options to set the following:

AWS access key id
AWS secret access key
AWS region
AWS security token
The SASL handshake requires a specific payload
that is described here:
https://github.com/aws/aws-msk-iam-auth

NOTE: As of now, the changes in this PR will allow librdkafka to connect for the valid duration of the credentials provided. For permanent credentials, the connection will stay open indefinitely. For STS temporary credentials, the connection will stay open until those credentials expire. We have some work slated over the next couple of weeks to enable a refresh callback where clients provide updated credentials as needed. We will be following the example from the sasl_oauthbearer to provide a similar mechanism where an event can be handled by the client to provide refreshed credentials. Please expect that to be in a follow-up PR.","after getting libxml2, got further but next error below:
make[1]: Entering directory `/home/user/librdkafka-1.9.0-AWS_MSK_IAM/src'
WARNING: librdkafka-static.a: Not creating self-contained static library librdkafka-static.a: no static libraries available/enabled
Generating pkg-config file rdkafka-static.pc
Checking librdkafka integrity
librdkafka.so.1                OK
librdkafka.a                   OK
Symbol visibility              OK
make[1]: Leaving directory `/home/user/librdkafka-1.9.0-AWS_MSK_IAM/src'
make[1]: Entering directory `/home/user/librdkafka-1.9.0-AWS_MSK_IAM/src-cpp'
Generating pkg-config file rdkafka++-static.pc
Checking librdkafka++ integrity
librdkafka++.so.1              OK
librdkafka++.a                 OK
make[1]: Leaving directory `/home/user/librdkafka-1.9.0-AWS_MSK_IAM/src-cpp'
make -C examples
make[1]: Entering directory `/home/user/librdkafka-1.9.0-AWS_MSK_IAM/examples'
gcc -I/usr/include/libxml2 -g -O2 -fPIC -Wall -Wsign-compare -Wfloat-equal -Wpointer-arith -Wcast-align  -I../src rdkafka_example.c -o rdkafka_example  \
        ../src/librdkafka.a -lxml2 -lcurl -lm -lcurl   -lzstd   -lsasl2   -lssl -lcrypto   -lcrypto   -lz   -ldl -lpthread -lrt
../src/librdkafka.a(rdkafka_aws.o): In function `rd_kafka_aws_build_string_to_sign':
/home/user/librdkafka-1.9.0-AWS_MSK_IAM/src/rdkafka_aws.c:310: undefined reference to `EVP_MD_CTX_new'
/home/user/librdkafka-1.9.0-AWS_MSK_IAM/src/rdkafka_aws.c:314: undefined reference to `EVP_MD_CTX_free'
../src/librdkafka.a(rdkafka_aws.o): In function `rd_kafka_aws_build_canonical_request':
/home/user/librdkafka-1.9.0-AWS_MSK_IAM/src/rdkafka_aws.c:265: undefined reference to `EVP_MD_CTX_new'
/home/user/librdkafka-1.9.0-AWS_MSK_IAM/src/rdkafka_aws.c:269: undefined reference to `EVP_MD_CTX_free'
collect2: error: ld returned 1 exit status
make[1]: *** [rdkafka_example] Error 1
make[1]: Leaving directory `/home/user/librdkafka-1.9.0-AWS_MSK_IAM/examples'
make: *** [examples] Error 2",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,3496,2021-08-12T01:48:04Z,2022-05-12T08:26:45Z,2022-05-12T08:26:46Z,CLOSED,False,9890,640,120,https://github.com/garrett528,sasl: Enable AWS_MSK_IAM SASL mechanism (#3402),14,[],https://github.com/edenhill/librdkafka/pull/3496,https://github.com/garrett528,15,https://github.com/edenhill/librdkafka/pull/3496#issuecomment-965903374,"AWS_MSK_IAM is a new SASL mechanism for
authenticating clients to AWS MSK Kafka
clusters and use IAM-based controls to
set Kafka ACLs and permissions. This change
provides support to allow clients to pass
AWS credentials at runtime which is used
to build the SASL payload and authenticate
clients to IAM enabled MSK clusters. It adds
a new SASL mechanism, AWS_MSK_IAM, as well
as configuration options to set the following:

AWS access key id
AWS secret access key
AWS region
AWS security token
The SASL handshake requires a specific payload
that is described here:
https://github.com/aws/aws-msk-iam-auth

NOTE: As of now, the changes in this PR will allow librdkafka to connect for the valid duration of the credentials provided. For permanent credentials, the connection will stay open indefinitely. For STS temporary credentials, the connection will stay open until those credentials expire. We have some work slated over the next couple of weeks to enable a refresh callback where clients provide updated credentials as needed. We will be following the example from the sasl_oauthbearer to provide a similar mechanism where an event can be handled by the client to provide refreshed credentials. Please expect that to be in a follow-up PR.",That is coming from openssl. Did you make sure all of the packages laid out in the Readme are installed on your machine?,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,3496,2021-08-12T01:48:04Z,2022-05-12T08:26:45Z,2022-05-12T08:26:46Z,CLOSED,False,9890,640,120,https://github.com/garrett528,sasl: Enable AWS_MSK_IAM SASL mechanism (#3402),14,[],https://github.com/edenhill/librdkafka/pull/3496,https://github.com/tooptoop4,16,https://github.com/edenhill/librdkafka/pull/3496#issuecomment-965909125,"AWS_MSK_IAM is a new SASL mechanism for
authenticating clients to AWS MSK Kafka
clusters and use IAM-based controls to
set Kafka ACLs and permissions. This change
provides support to allow clients to pass
AWS credentials at runtime which is used
to build the SASL payload and authenticate
clients to IAM enabled MSK clusters. It adds
a new SASL mechanism, AWS_MSK_IAM, as well
as configuration options to set the following:

AWS access key id
AWS secret access key
AWS region
AWS security token
The SASL handshake requires a specific payload
that is described here:
https://github.com/aws/aws-msk-iam-auth

NOTE: As of now, the changes in this PR will allow librdkafka to connect for the valid duration of the credentials provided. For permanent credentials, the connection will stay open indefinitely. For STS temporary credentials, the connection will stay open until those credentials expire. We have some work slated over the next couple of weeks to enable a refresh callback where clients provide updated credentials as needed. We will be following the example from the sasl_oauthbearer to provide a similar mechanism where an event can be handled by the client to provide refreshed credentials. Please expect that to be in a follow-up PR.","@garrett528 is below openssl version too old?
openssl version
OpenSSL 1.0.2k-fips  26 Jan 2017
UPDATE: was able to build with this code change https://stackoverflow.com/a/46769674
Tested kcat with it too, works well with access/secret/session passed in but wasn't able to get it to assume role like java clients can",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,3496,2021-08-12T01:48:04Z,2022-05-12T08:26:45Z,2022-05-12T08:26:46Z,CLOSED,False,9890,640,120,https://github.com/garrett528,sasl: Enable AWS_MSK_IAM SASL mechanism (#3402),14,[],https://github.com/edenhill/librdkafka/pull/3496,https://github.com/garrett528,17,https://github.com/edenhill/librdkafka/pull/3496#issuecomment-966348339,"AWS_MSK_IAM is a new SASL mechanism for
authenticating clients to AWS MSK Kafka
clusters and use IAM-based controls to
set Kafka ACLs and permissions. This change
provides support to allow clients to pass
AWS credentials at runtime which is used
to build the SASL payload and authenticate
clients to IAM enabled MSK clusters. It adds
a new SASL mechanism, AWS_MSK_IAM, as well
as configuration options to set the following:

AWS access key id
AWS secret access key
AWS region
AWS security token
The SASL handshake requires a specific payload
that is described here:
https://github.com/aws/aws-msk-iam-auth

NOTE: As of now, the changes in this PR will allow librdkafka to connect for the valid duration of the credentials provided. For permanent credentials, the connection will stay open indefinitely. For STS temporary credentials, the connection will stay open until those credentials expire. We have some work slated over the next couple of weeks to enable a refresh callback where clients provide updated credentials as needed. We will be following the example from the sasl_oauthbearer to provide a similar mechanism where an event can be handled by the client to provide refreshed credentials. Please expect that to be in a follow-up PR.","@tooptoop4 good to hear it built. the assumeRole should be working. is the role you're assuming set up properly? usually, i have to get temp credentials from the role i want to assume to pass in. also, make sure the role can assume itself",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,3496,2021-08-12T01:48:04Z,2022-05-12T08:26:45Z,2022-05-12T08:26:46Z,CLOSED,False,9890,640,120,https://github.com/garrett528,sasl: Enable AWS_MSK_IAM SASL mechanism (#3402),14,[],https://github.com/edenhill/librdkafka/pull/3496,https://github.com/shrinjay-mio,18,https://github.com/edenhill/librdkafka/pull/3496#issuecomment-966440479,"AWS_MSK_IAM is a new SASL mechanism for
authenticating clients to AWS MSK Kafka
clusters and use IAM-based controls to
set Kafka ACLs and permissions. This change
provides support to allow clients to pass
AWS credentials at runtime which is used
to build the SASL payload and authenticate
clients to IAM enabled MSK clusters. It adds
a new SASL mechanism, AWS_MSK_IAM, as well
as configuration options to set the following:

AWS access key id
AWS secret access key
AWS region
AWS security token
The SASL handshake requires a specific payload
that is described here:
https://github.com/aws/aws-msk-iam-auth

NOTE: As of now, the changes in this PR will allow librdkafka to connect for the valid duration of the credentials provided. For permanent credentials, the connection will stay open indefinitely. For STS temporary credentials, the connection will stay open until those credentials expire. We have some work slated over the next couple of weeks to enable a refresh callback where clients provide updated credentials as needed. We will be following the example from the sasl_oauthbearer to provide a similar mechanism where an event can be handled by the client to provide refreshed credentials. Please expect that to be in a follow-up PR.","@tooptoop4 doesn't look like the ./configure call is finding libxml2. you need to make sure pkgconfig can find the corresponding .pc file for it (for Ubuntu 20, it's usually located in /usr/lib/x86_64-linux-gnu/pkgconfig/libxml-2.0.pc and you may need to symlink it to a path where pkgconfig can find it. for example in Ubuntu, ln -sf /usr/lib/x86_64-linux-gnu/pkgconfig/libxml-2.0.pc /usr/lib/x86_64-linux-gnu/pkgconfig/libxml2.pc.
if you don't know where it is, try find / -name libxml-2.0.pc. once that's correctly linked, you should see one of these switch to ""ok"":
checking for libxml2 (by pkg-config)... failed
checking for libxml2 (by compile)... failed (disable)

and BUILT_WITH GCC GXX PKGCONFIG INSTALL GNULD LDS LIBDL PLUGINS ZLIB SSL SASL_CYRUS ZSTD CURL HDRHISTOGRAM SYSLOG SNAPPY SOCKEM SASL_SCRAM SASL_OAUTHBEARER CRC32C_HW will have ""SASL_AWS_MSK_IAM""

I'm having the same issue trying to install on a docker container, but the issue is it doesn't even seem like libxml 2 is being installed as attempting to use the find command for the libxml2 package returns nothing. Any idea?",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,3496,2021-08-12T01:48:04Z,2022-05-12T08:26:45Z,2022-05-12T08:26:46Z,CLOSED,False,9890,640,120,https://github.com/garrett528,sasl: Enable AWS_MSK_IAM SASL mechanism (#3402),14,[],https://github.com/edenhill/librdkafka/pull/3496,https://github.com/garrett528,19,https://github.com/edenhill/librdkafka/pull/3496#issuecomment-966441937,"AWS_MSK_IAM is a new SASL mechanism for
authenticating clients to AWS MSK Kafka
clusters and use IAM-based controls to
set Kafka ACLs and permissions. This change
provides support to allow clients to pass
AWS credentials at runtime which is used
to build the SASL payload and authenticate
clients to IAM enabled MSK clusters. It adds
a new SASL mechanism, AWS_MSK_IAM, as well
as configuration options to set the following:

AWS access key id
AWS secret access key
AWS region
AWS security token
The SASL handshake requires a specific payload
that is described here:
https://github.com/aws/aws-msk-iam-auth

NOTE: As of now, the changes in this PR will allow librdkafka to connect for the valid duration of the credentials provided. For permanent credentials, the connection will stay open indefinitely. For STS temporary credentials, the connection will stay open until those credentials expire. We have some work slated over the next couple of weeks to enable a refresh callback where clients provide updated credentials as needed. We will be following the example from the sasl_oauthbearer to provide a similar mechanism where an event can be handled by the client to provide refreshed credentials. Please expect that to be in a follow-up PR.",@shrinjay-mio what base image are you using? we use ubuntu 20 and it can find the packages. make sure to run sudo apt-get update before trying to install on the image,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,3496,2021-08-12T01:48:04Z,2022-05-12T08:26:45Z,2022-05-12T08:26:46Z,CLOSED,False,9890,640,120,https://github.com/garrett528,sasl: Enable AWS_MSK_IAM SASL mechanism (#3402),14,[],https://github.com/edenhill/librdkafka/pull/3496,https://github.com/shrinjay-mio,20,https://github.com/edenhill/librdkafka/pull/3496#issuecomment-966454094,"AWS_MSK_IAM is a new SASL mechanism for
authenticating clients to AWS MSK Kafka
clusters and use IAM-based controls to
set Kafka ACLs and permissions. This change
provides support to allow clients to pass
AWS credentials at runtime which is used
to build the SASL payload and authenticate
clients to IAM enabled MSK clusters. It adds
a new SASL mechanism, AWS_MSK_IAM, as well
as configuration options to set the following:

AWS access key id
AWS secret access key
AWS region
AWS security token
The SASL handshake requires a specific payload
that is described here:
https://github.com/aws/aws-msk-iam-auth

NOTE: As of now, the changes in this PR will allow librdkafka to connect for the valid duration of the credentials provided. For permanent credentials, the connection will stay open indefinitely. For STS temporary credentials, the connection will stay open until those credentials expire. We have some work slated over the next couple of weeks to enable a refresh callback where clients provide updated credentials as needed. We will be following the example from the sasl_oauthbearer to provide a similar mechanism where an event can be handled by the client to provide refreshed credentials. Please expect that to be in a follow-up PR.","We're using the aspnet:5.0 base image... which I just realized uses Debian 11 and not Ubuntu 20. After digging up an image that uses Ubuntu 20 (mcr.microsoft.com/dotnet/aspnet:5.0.12-focal-amd64), it now seems to work fine. Really sorry about that, I'm not super familiar with docker yet.",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,3496,2021-08-12T01:48:04Z,2022-05-12T08:26:45Z,2022-05-12T08:26:46Z,CLOSED,False,9890,640,120,https://github.com/garrett528,sasl: Enable AWS_MSK_IAM SASL mechanism (#3402),14,[],https://github.com/edenhill/librdkafka/pull/3496,https://github.com/garrett528,21,https://github.com/edenhill/librdkafka/pull/3496#issuecomment-966455962,"AWS_MSK_IAM is a new SASL mechanism for
authenticating clients to AWS MSK Kafka
clusters and use IAM-based controls to
set Kafka ACLs and permissions. This change
provides support to allow clients to pass
AWS credentials at runtime which is used
to build the SASL payload and authenticate
clients to IAM enabled MSK clusters. It adds
a new SASL mechanism, AWS_MSK_IAM, as well
as configuration options to set the following:

AWS access key id
AWS secret access key
AWS region
AWS security token
The SASL handshake requires a specific payload
that is described here:
https://github.com/aws/aws-msk-iam-auth

NOTE: As of now, the changes in this PR will allow librdkafka to connect for the valid duration of the credentials provided. For permanent credentials, the connection will stay open indefinitely. For STS temporary credentials, the connection will stay open until those credentials expire. We have some work slated over the next couple of weeks to enable a refresh callback where clients provide updated credentials as needed. We will be following the example from the sasl_oauthbearer to provide a similar mechanism where an event can be handled by the client to provide refreshed credentials. Please expect that to be in a follow-up PR.","@shrinjay-mio i'm sure you can figure it out for debian 11 as well. i just haven't had to yet so have no great guidance there.
this is just standard cross OS dependency management and pain points so if you figure it out, please add the information to the README so others can follow.",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,3496,2021-08-12T01:48:04Z,2022-05-12T08:26:45Z,2022-05-12T08:26:46Z,CLOSED,False,9890,640,120,https://github.com/garrett528,sasl: Enable AWS_MSK_IAM SASL mechanism (#3402),14,[],https://github.com/edenhill/librdkafka/pull/3496,https://github.com/shrinjay-mio,22,https://github.com/edenhill/librdkafka/pull/3496#issuecomment-966688334,"AWS_MSK_IAM is a new SASL mechanism for
authenticating clients to AWS MSK Kafka
clusters and use IAM-based controls to
set Kafka ACLs and permissions. This change
provides support to allow clients to pass
AWS credentials at runtime which is used
to build the SASL payload and authenticate
clients to IAM enabled MSK clusters. It adds
a new SASL mechanism, AWS_MSK_IAM, as well
as configuration options to set the following:

AWS access key id
AWS secret access key
AWS region
AWS security token
The SASL handshake requires a specific payload
that is described here:
https://github.com/aws/aws-msk-iam-auth

NOTE: As of now, the changes in this PR will allow librdkafka to connect for the valid duration of the credentials provided. For permanent credentials, the connection will stay open indefinitely. For STS temporary credentials, the connection will stay open until those credentials expire. We have some work slated over the next couple of weeks to enable a refresh callback where clients provide updated credentials as needed. We will be following the example from the sasl_oauthbearer to provide a similar mechanism where an event can be handled by the client to provide refreshed credentials. Please expect that to be in a follow-up PR.","Anyone else getting:
Failed to load the librdkafka native library.
   at Confluent.Kafka.Impl.Librdkafka.TrySetDelegates(List`1 nativeMethodCandidateTypes) 
   ...

When trying to load the librdkafka .so file in  a docker container from Confluent.Kafka.Dotnet?",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,3496,2021-08-12T01:48:04Z,2022-05-12T08:26:45Z,2022-05-12T08:26:46Z,CLOSED,False,9890,640,120,https://github.com/garrett528,sasl: Enable AWS_MSK_IAM SASL mechanism (#3402),14,[],https://github.com/edenhill/librdkafka/pull/3496,https://github.com/garrett528,23,https://github.com/edenhill/librdkafka/pull/3496#issuecomment-967151941,"AWS_MSK_IAM is a new SASL mechanism for
authenticating clients to AWS MSK Kafka
clusters and use IAM-based controls to
set Kafka ACLs and permissions. This change
provides support to allow clients to pass
AWS credentials at runtime which is used
to build the SASL payload and authenticate
clients to IAM enabled MSK clusters. It adds
a new SASL mechanism, AWS_MSK_IAM, as well
as configuration options to set the following:

AWS access key id
AWS secret access key
AWS region
AWS security token
The SASL handshake requires a specific payload
that is described here:
https://github.com/aws/aws-msk-iam-auth

NOTE: As of now, the changes in this PR will allow librdkafka to connect for the valid duration of the credentials provided. For permanent credentials, the connection will stay open indefinitely. For STS temporary credentials, the connection will stay open until those credentials expire. We have some work slated over the next couple of weeks to enable a refresh callback where clients provide updated credentials as needed. We will be following the example from the sasl_oauthbearer to provide a similar mechanism where an event can be handled by the client to provide refreshed credentials. Please expect that to be in a follow-up PR.","@shrinjay-mio not sure about Dotnet but i would make sure that the LD_LIBRARY_PATH includes the path to wherever you copied your .so file.
for example in our Dockerfile, we do COPY librdkafka.so.1 /app and then add ENV LD_LIBRARY_PATH $LD_LIBRARY_PATH:/app so the linker can find it at runtime",True,{'THUMBS_UP': ['https://github.com/shrinjay-mio']}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,3496,2021-08-12T01:48:04Z,2022-05-12T08:26:45Z,2022-05-12T08:26:46Z,CLOSED,False,9890,640,120,https://github.com/garrett528,sasl: Enable AWS_MSK_IAM SASL mechanism (#3402),14,[],https://github.com/edenhill/librdkafka/pull/3496,https://github.com/shrinjay-mio,24,https://github.com/edenhill/librdkafka/pull/3496#issuecomment-974521865,"AWS_MSK_IAM is a new SASL mechanism for
authenticating clients to AWS MSK Kafka
clusters and use IAM-based controls to
set Kafka ACLs and permissions. This change
provides support to allow clients to pass
AWS credentials at runtime which is used
to build the SASL payload and authenticate
clients to IAM enabled MSK clusters. It adds
a new SASL mechanism, AWS_MSK_IAM, as well
as configuration options to set the following:

AWS access key id
AWS secret access key
AWS region
AWS security token
The SASL handshake requires a specific payload
that is described here:
https://github.com/aws/aws-msk-iam-auth

NOTE: As of now, the changes in this PR will allow librdkafka to connect for the valid duration of the credentials provided. For permanent credentials, the connection will stay open indefinitely. For STS temporary credentials, the connection will stay open until those credentials expire. We have some work slated over the next couple of weeks to enable a refresh callback where clients provide updated credentials as needed. We will be following the example from the sasl_oauthbearer to provide a similar mechanism where an event can be handled by the client to provide refreshed credentials. Please expect that to be in a follow-up PR.",Were the files in win32 ever updated to allow this fork to build on windows?,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,3496,2021-08-12T01:48:04Z,2022-05-12T08:26:45Z,2022-05-12T08:26:46Z,CLOSED,False,9890,640,120,https://github.com/garrett528,sasl: Enable AWS_MSK_IAM SASL mechanism (#3402),14,[],https://github.com/edenhill/librdkafka/pull/3496,https://github.com/garrett528,25,https://github.com/edenhill/librdkafka/pull/3496#issuecomment-974523997,"AWS_MSK_IAM is a new SASL mechanism for
authenticating clients to AWS MSK Kafka
clusters and use IAM-based controls to
set Kafka ACLs and permissions. This change
provides support to allow clients to pass
AWS credentials at runtime which is used
to build the SASL payload and authenticate
clients to IAM enabled MSK clusters. It adds
a new SASL mechanism, AWS_MSK_IAM, as well
as configuration options to set the following:

AWS access key id
AWS secret access key
AWS region
AWS security token
The SASL handshake requires a specific payload
that is described here:
https://github.com/aws/aws-msk-iam-auth

NOTE: As of now, the changes in this PR will allow librdkafka to connect for the valid duration of the credentials provided. For permanent credentials, the connection will stay open indefinitely. For STS temporary credentials, the connection will stay open until those credentials expire. We have some work slated over the next couple of weeks to enable a refresh callback where clients provide updated credentials as needed. We will be following the example from the sasl_oauthbearer to provide a similar mechanism where an event can be handled by the client to provide refreshed credentials. Please expect that to be in a follow-up PR.",No I only work with Linux and osx. Don't even have a windows to test with. Feel free to open a pr with changes,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,3496,2021-08-12T01:48:04Z,2022-05-12T08:26:45Z,2022-05-12T08:26:46Z,CLOSED,False,9890,640,120,https://github.com/garrett528,sasl: Enable AWS_MSK_IAM SASL mechanism (#3402),14,[],https://github.com/edenhill/librdkafka/pull/3496,https://github.com/shrinjay-mio,26,https://github.com/edenhill/librdkafka/pull/3496#issuecomment-974524937,"AWS_MSK_IAM is a new SASL mechanism for
authenticating clients to AWS MSK Kafka
clusters and use IAM-based controls to
set Kafka ACLs and permissions. This change
provides support to allow clients to pass
AWS credentials at runtime which is used
to build the SASL payload and authenticate
clients to IAM enabled MSK clusters. It adds
a new SASL mechanism, AWS_MSK_IAM, as well
as configuration options to set the following:

AWS access key id
AWS secret access key
AWS region
AWS security token
The SASL handshake requires a specific payload
that is described here:
https://github.com/aws/aws-msk-iam-auth

NOTE: As of now, the changes in this PR will allow librdkafka to connect for the valid duration of the credentials provided. For permanent credentials, the connection will stay open indefinitely. For STS temporary credentials, the connection will stay open until those credentials expire. We have some work slated over the next couple of weeks to enable a refresh callback where clients provide updated credentials as needed. We will be following the example from the sasl_oauthbearer to provide a similar mechanism where an event can be handled by the client to provide refreshed credentials. Please expect that to be in a follow-up PR.","Got it, just working through the changes but wasn't sure if I even needed to make them. I had to start by manually adding the WITH_SASL_AWS_MSK_IAM preprocessor directive to the build. While this did work, there is now another error, ""unresolved external symbol _rd_kafka_sasl_aws_msk_iam"". I take it this is a build issue, not a platform one, are there any other preprocessor directives I need to add or any further guidance?",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,3496,2021-08-12T01:48:04Z,2022-05-12T08:26:45Z,2022-05-12T08:26:46Z,CLOSED,False,9890,640,120,https://github.com/garrett528,sasl: Enable AWS_MSK_IAM SASL mechanism (#3402),14,[],https://github.com/edenhill/librdkafka/pull/3496,https://github.com/garrett528,27,https://github.com/edenhill/librdkafka/pull/3496#issuecomment-974527662,"AWS_MSK_IAM is a new SASL mechanism for
authenticating clients to AWS MSK Kafka
clusters and use IAM-based controls to
set Kafka ACLs and permissions. This change
provides support to allow clients to pass
AWS credentials at runtime which is used
to build the SASL payload and authenticate
clients to IAM enabled MSK clusters. It adds
a new SASL mechanism, AWS_MSK_IAM, as well
as configuration options to set the following:

AWS access key id
AWS secret access key
AWS region
AWS security token
The SASL handshake requires a specific payload
that is described here:
https://github.com/aws/aws-msk-iam-auth

NOTE: As of now, the changes in this PR will allow librdkafka to connect for the valid duration of the credentials provided. For permanent credentials, the connection will stay open indefinitely. For STS temporary credentials, the connection will stay open until those credentials expire. We have some work slated over the next couple of weeks to enable a refresh callback where clients provide updated credentials as needed. We will be following the example from the sasl_oauthbearer to provide a similar mechanism where an event can be handled by the client to provide refreshed credentials. Please expect that to be in a follow-up PR.",Are you sure you have all of the required packages installed? Only thing I can think of is that the preprocessor is skipping that definition since a package is missing (like SSL support or something).o,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,3496,2021-08-12T01:48:04Z,2022-05-12T08:26:45Z,2022-05-12T08:26:46Z,CLOSED,False,9890,640,120,https://github.com/garrett528,sasl: Enable AWS_MSK_IAM SASL mechanism (#3402),14,[],https://github.com/edenhill/librdkafka/pull/3496,https://github.com/shrinjay-mio,28,https://github.com/edenhill/librdkafka/pull/3496#issuecomment-974529535,"AWS_MSK_IAM is a new SASL mechanism for
authenticating clients to AWS MSK Kafka
clusters and use IAM-based controls to
set Kafka ACLs and permissions. This change
provides support to allow clients to pass
AWS credentials at runtime which is used
to build the SASL payload and authenticate
clients to IAM enabled MSK clusters. It adds
a new SASL mechanism, AWS_MSK_IAM, as well
as configuration options to set the following:

AWS access key id
AWS secret access key
AWS region
AWS security token
The SASL handshake requires a specific payload
that is described here:
https://github.com/aws/aws-msk-iam-auth

NOTE: As of now, the changes in this PR will allow librdkafka to connect for the valid duration of the credentials provided. For permanent credentials, the connection will stay open indefinitely. For STS temporary credentials, the connection will stay open until those credentials expire. We have some work slated over the next couple of weeks to enable a refresh callback where clients provide updated credentials as needed. We will be following the example from the sasl_oauthbearer to provide a similar mechanism where an event can be handled by the client to provide refreshed credentials. Please expect that to be in a follow-up PR.","I have openssl, zstd, zlib and libxml installed, is that all?",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,3496,2021-08-12T01:48:04Z,2022-05-12T08:26:45Z,2022-05-12T08:26:46Z,CLOSED,False,9890,640,120,https://github.com/garrett528,sasl: Enable AWS_MSK_IAM SASL mechanism (#3402),14,[],https://github.com/edenhill/librdkafka/pull/3496,https://github.com/garrett528,29,https://github.com/edenhill/librdkafka/pull/3496#issuecomment-974530456,"AWS_MSK_IAM is a new SASL mechanism for
authenticating clients to AWS MSK Kafka
clusters and use IAM-based controls to
set Kafka ACLs and permissions. This change
provides support to allow clients to pass
AWS credentials at runtime which is used
to build the SASL payload and authenticate
clients to IAM enabled MSK clusters. It adds
a new SASL mechanism, AWS_MSK_IAM, as well
as configuration options to set the following:

AWS access key id
AWS secret access key
AWS region
AWS security token
The SASL handshake requires a specific payload
that is described here:
https://github.com/aws/aws-msk-iam-auth

NOTE: As of now, the changes in this PR will allow librdkafka to connect for the valid duration of the credentials provided. For permanent credentials, the connection will stay open indefinitely. For STS temporary credentials, the connection will stay open until those credentials expire. We have some work slated over the next couple of weeks to enable a refresh callback where clients provide updated credentials as needed. We will be following the example from the sasl_oauthbearer to provide a similar mechanism where an event can be handled by the client to provide refreshed credentials. Please expect that to be in a follow-up PR.",See the comments above regarding the '. /configure' . You also definitely need curl.,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,3496,2021-08-12T01:48:04Z,2022-05-12T08:26:45Z,2022-05-12T08:26:46Z,CLOSED,False,9890,640,120,https://github.com/garrett528,sasl: Enable AWS_MSK_IAM SASL mechanism (#3402),14,[],https://github.com/edenhill/librdkafka/pull/3496,https://github.com/shrinjay-mio,30,https://github.com/edenhill/librdkafka/pull/3496#issuecomment-974530835,"AWS_MSK_IAM is a new SASL mechanism for
authenticating clients to AWS MSK Kafka
clusters and use IAM-based controls to
set Kafka ACLs and permissions. This change
provides support to allow clients to pass
AWS credentials at runtime which is used
to build the SASL payload and authenticate
clients to IAM enabled MSK clusters. It adds
a new SASL mechanism, AWS_MSK_IAM, as well
as configuration options to set the following:

AWS access key id
AWS secret access key
AWS region
AWS security token
The SASL handshake requires a specific payload
that is described here:
https://github.com/aws/aws-msk-iam-auth

NOTE: As of now, the changes in this PR will allow librdkafka to connect for the valid duration of the credentials provided. For permanent credentials, the connection will stay open indefinitely. For STS temporary credentials, the connection will stay open until those credentials expire. We have some work slated over the next couple of weeks to enable a refresh callback where clients provide updated credentials as needed. We will be following the example from the sasl_oauthbearer to provide a similar mechanism where an event can be handled by the client to provide refreshed credentials. Please expect that to be in a follow-up PR.","Forgot to mention, that's already been installed. I tried to reverse engineer the configure script but it seems all else that was needed was libxml and curl, both of which are installed.",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,3496,2021-08-12T01:48:04Z,2022-05-12T08:26:45Z,2022-05-12T08:26:46Z,CLOSED,False,9890,640,120,https://github.com/garrett528,sasl: Enable AWS_MSK_IAM SASL mechanism (#3402),14,[],https://github.com/edenhill/librdkafka/pull/3496,https://github.com/gburke-ppb,31,https://github.com/edenhill/librdkafka/pull/3496#issuecomment-1122052521,"AWS_MSK_IAM is a new SASL mechanism for
authenticating clients to AWS MSK Kafka
clusters and use IAM-based controls to
set Kafka ACLs and permissions. This change
provides support to allow clients to pass
AWS credentials at runtime which is used
to build the SASL payload and authenticate
clients to IAM enabled MSK clusters. It adds
a new SASL mechanism, AWS_MSK_IAM, as well
as configuration options to set the following:

AWS access key id
AWS secret access key
AWS region
AWS security token
The SASL handshake requires a specific payload
that is described here:
https://github.com/aws/aws-msk-iam-auth

NOTE: As of now, the changes in this PR will allow librdkafka to connect for the valid duration of the credentials provided. For permanent credentials, the connection will stay open indefinitely. For STS temporary credentials, the connection will stay open until those credentials expire. We have some work slated over the next couple of weeks to enable a refresh callback where clients provide updated credentials as needed. We will be following the example from the sasl_oauthbearer to provide a similar mechanism where an event can be handled by the client to provide refreshed credentials. Please expect that to be in a follow-up PR.","Hey guys.  Any movement with this in the last while?  IAM is something that we are looking to be able to use in many apps, and many languages' libraries are reliant on this one.
Can we get this merged in soon, please?",True,{'THUMBS_UP': ['https://github.com/tooptoop4']}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,3496,2021-08-12T01:48:04Z,2022-05-12T08:26:45Z,2022-05-12T08:26:46Z,CLOSED,False,9890,640,120,https://github.com/garrett528,sasl: Enable AWS_MSK_IAM SASL mechanism (#3402),14,[],https://github.com/edenhill/librdkafka/pull/3496,https://github.com/edenhill,32,https://github.com/edenhill/librdkafka/pull/3496#issuecomment-1124683543,"AWS_MSK_IAM is a new SASL mechanism for
authenticating clients to AWS MSK Kafka
clusters and use IAM-based controls to
set Kafka ACLs and permissions. This change
provides support to allow clients to pass
AWS credentials at runtime which is used
to build the SASL payload and authenticate
clients to IAM enabled MSK clusters. It adds
a new SASL mechanism, AWS_MSK_IAM, as well
as configuration options to set the following:

AWS access key id
AWS secret access key
AWS region
AWS security token
The SASL handshake requires a specific payload
that is described here:
https://github.com/aws/aws-msk-iam-auth

NOTE: As of now, the changes in this PR will allow librdkafka to connect for the valid duration of the credentials provided. For permanent credentials, the connection will stay open indefinitely. For STS temporary credentials, the connection will stay open until those credentials expire. We have some work slated over the next couple of weeks to enable a refresh callback where clients provide updated credentials as needed. We will be following the example from the sasl_oauthbearer to provide a similar mechanism where an event can be handled by the client to provide refreshed credentials. Please expect that to be in a follow-up PR.","Closing this PR to avoid further confusion, see:
#3496 (comment)",True,"{'THUMBS_DOWN': ['https://github.com/tooptoop4', 'https://github.com/Kilowhisky']}"
edenhill/librdkafka,https://github.com/edenhill/librdkafka,3505,2021-08-18T16:27:43Z,2021-08-24T08:13:54Z,2021-08-24T08:13:56Z,MERGED,True,88,13,6,https://github.com/edenhill,Fix crash in DeleteRecords on request error,2,[],https://github.com/edenhill/librdkafka/pull/3505,https://github.com/edenhill,1,https://github.com/edenhill/librdkafka/pull/3505,#3476,#3476,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,3507,2021-08-20T14:54:01Z,2021-08-20T16:43:37Z,2021-11-22T14:15:41Z,MERGED,True,22,16,3,https://github.com/edenhill,Decrease the amount of work being done on Travis-CI,3,[],https://github.com/edenhill/librdkafka/pull/3507,https://github.com/edenhill,1,https://github.com/edenhill/librdkafka/pull/3507,.. to reduce the build minutes since they're now limited.,.. to reduce the build minutes since they're now limited.,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,3507,2021-08-20T14:54:01Z,2021-08-20T16:43:37Z,2021-11-22T14:15:41Z,MERGED,True,22,16,3,https://github.com/edenhill,Decrease the amount of work being done on Travis-CI,3,[],https://github.com/edenhill/librdkafka/pull/3507,https://github.com/shahidhs-ibm,2,https://github.com/edenhill/librdkafka/pull/3507#issuecomment-975572354,.. to reduce the build minutes since they're now limited.,"@edenhill It seems that s390x Travis builds has reduced very much after this PR. Is this done because of Travis credits shortage? If yes, we can try raising request on Travis for increasing credits. Please let us know if you need any help.",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,3508,2021-08-20T17:18:45Z,2021-08-23T06:39:32Z,2021-08-23T06:39:37Z,MERGED,True,4077,26,26,https://github.com/edenhill,cURL and cJSON support,12,[],https://github.com/edenhill/librdkafka/pull/3508,https://github.com/edenhill,1,https://github.com/edenhill/librdkafka/pull/3508,This provides the required dependencies for the upcoming KIP-768 work.,This provides the required dependencies for the upcoming KIP-768 work.,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,3510,2021-08-24T04:09:38Z,2021-08-31T07:22:04Z,2021-08-31T07:22:04Z,MERGED,True,205,17,9,https://github.com/jliunyu,OAuth/OIDC: Add fields to client configuration,1,[],https://github.com/edenhill/librdkafka/pull/3510,https://github.com/jliunyu,1,https://github.com/edenhill/librdkafka/pull/3510,"Local test:
./run-test.sh
############## ./test-runner ################
### Running test ./test-runner in bare mode ###
[<MAIN>                      /  0.000s] Setting test timeout to 10s * 1.0
[<MAIN>                      /  0.000s] Git version: e143b4a1@kip768-coding
[<MAIN>                      /  0.000s] Broker version: 2.3.0 (2.3.0.0)
[<MAIN>                      /  0.000s] Tests to run : 0126
[<MAIN>                      /  0.000s] Test mode    : bare
[<MAIN>                      /  0.000s] Test scenario: default
[<MAIN>                      /  0.000s] Test filter  : no filter
[<MAIN>                      /  0.000s] Test timeout multiplier: 2.7
[<MAIN>                      /  0.000s] Action on test failure: continue other tests
[<MAIN>                      /  0.000s] Current directory: /Users/jliu/Documents/Code/jliunyu/librdkafka/tests
[<MAIN>                      /  0.000s] Setting test timeout to 30s * 2.7
[<MAIN>                      /  0.001s] 1 test(s) running: 0126_oauthbearer_oidc
[0126_oauthbearer_oidc       /  0.000s] ================= Running test 0126_oauthbearer_oidc =================
[0126_oauthbearer_oidc       /  0.000s] ==== Stats written to file stats_0126_oauthbearer_oidc_7294066187078906598.json ====
[0126_oauthbearer_oidc       /  0.000s] [ do_test_create_producer:48: Test producer with oidc configuration ]
[0126_oauthbearer_oidc       /  0.002s] Setting test timeout to 60s * 2.7
[0126_oauthbearer_oidc       /  0.054s] Created    kafka instance 0126_oauthbearer_oidc#producer-1
[0126_oauthbearer_oidc       /  0.055s] Using topic ""rdkafkatest_rnd1aed7f67de2ce6_0126-oauthbearer_oidc""
[0126_oauthbearer_oidc       /  0.055s] Creating topic ""rdkafkatest_rnd1aed7f67de2ce6_0126-oauthbearer_oidc"" (partitions=1, replication_factor=1, timeout=26666)
[0126_oauthbearer_oidc       /  0.128s] CreateTopics: duration 73.897ms
[0126_oauthbearer_oidc       /  0.131s] Produce to rdkafkatest_rnd1aed7f67de2ce6_0126-oauthbearer_oidc [1]: messages #0..0
[0126_oauthbearer_oidc       /  0.131s] SUM(POLL): duration 0.002ms
[0126_oauthbearer_oidc       /  0.131s] PRODUCE: duration 0.039ms
[0126_oauthbearer_oidc       /  0.131s] PRODUCE.DELIVERY.WAIT: duration 0.000ms
[0126_oauthbearer_oidc       /  0.137s] Created    kafka instance 0126_oauthbearer_oidc#consumer-2
[0126_oauthbearer_oidc       /  0.137s] Subscribing to topic rdkafkatest_rnd1aed7f67de2ce6_0126-oauthbearer_oidc in group rdkafkatest_rnd1aed7f67de2ce6_0126-oauthbearer_oidc (expecting 1 msgs with testid 0)
[0126_oauthbearer_oidc       /  0.137s] consume.easy: consume 1 messages
[<MAIN>                      /  1.001s] 1 test(s) running: 0126_oauthbearer_oidc
[<MAIN>                      /  2.001s] 1 test(s) running: 0126_oauthbearer_oidc
[<MAIN>                      /  3.005s] 1 test(s) running: 0126_oauthbearer_oidc
[<MAIN>                      /  4.009s] 1 test(s) running: 0126_oauthbearer_oidc
[0126_oauthbearer_oidc       /  4.862s] rdkafkatest_rnd1aed7f67de2ce6_0126-oauthbearer_oidc [0] reached EOF at offset 0
[0126_oauthbearer_oidc       /  4.862s] CONSUME: duration 4724.758ms
[0126_oauthbearer_oidc       /  4.862s] consume.easy: consumed 0/1 messages (1/1 EOFs)
[0126_oauthbearer_oidc       /  4.862s] Closing consumer 0126_oauthbearer_oidc#consumer-2
[0126_oauthbearer_oidc       /  4.873s] CONSUMER.CLOSE: duration 10.551ms
[0126_oauthbearer_oidc       /  4.876s] [ do_test_create_producer:48: Test producer with oidc configuration: PASS (4.88s) ]
[0126_oauthbearer_oidc       /  4.877s] 0126_oauthbearer_oidc: duration 4876.512ms
[0126_oauthbearer_oidc       /  4.877s] ================= Test 0126_oauthbearer_oidc PASSED =================
[<MAIN>                      /  5.010s] ALL-TESTS: duration 5009.423ms
TEST 20210830213554 (bare, scenario default) SUMMARY
#==================================================================#
| <MAIN>                                   |     PASSED |   5.010s |
| 0126_oauthbearer_oidc                    |     PASSED |   4.877s |
#==================================================================#
[<MAIN>                      /  5.027s] 0 thread(s) in use by librdkafka
[<MAIN>                      /  5.027s] 
============== ALL TESTS PASSED ==============
###
###  ./test-runner in bare mode PASSED! ###
###","Local test:
./run-test.sh
############## ./test-runner ################
### Running test ./test-runner in bare mode ###
[<MAIN>                      /  0.000s] Setting test timeout to 10s * 1.0
[<MAIN>                      /  0.000s] Git version: e143b4a1@kip768-coding
[<MAIN>                      /  0.000s] Broker version: 2.3.0 (2.3.0.0)
[<MAIN>                      /  0.000s] Tests to run : 0126
[<MAIN>                      /  0.000s] Test mode    : bare
[<MAIN>                      /  0.000s] Test scenario: default
[<MAIN>                      /  0.000s] Test filter  : no filter
[<MAIN>                      /  0.000s] Test timeout multiplier: 2.7
[<MAIN>                      /  0.000s] Action on test failure: continue other tests
[<MAIN>                      /  0.000s] Current directory: /Users/jliu/Documents/Code/jliunyu/librdkafka/tests
[<MAIN>                      /  0.000s] Setting test timeout to 30s * 2.7
[<MAIN>                      /  0.001s] 1 test(s) running: 0126_oauthbearer_oidc
[0126_oauthbearer_oidc       /  0.000s] ================= Running test 0126_oauthbearer_oidc =================
[0126_oauthbearer_oidc       /  0.000s] ==== Stats written to file stats_0126_oauthbearer_oidc_7294066187078906598.json ====
[0126_oauthbearer_oidc       /  0.000s] [ do_test_create_producer:48: Test producer with oidc configuration ]
[0126_oauthbearer_oidc       /  0.002s] Setting test timeout to 60s * 2.7
[0126_oauthbearer_oidc       /  0.054s] Created    kafka instance 0126_oauthbearer_oidc#producer-1
[0126_oauthbearer_oidc       /  0.055s] Using topic ""rdkafkatest_rnd1aed7f67de2ce6_0126-oauthbearer_oidc""
[0126_oauthbearer_oidc       /  0.055s] Creating topic ""rdkafkatest_rnd1aed7f67de2ce6_0126-oauthbearer_oidc"" (partitions=1, replication_factor=1, timeout=26666)
[0126_oauthbearer_oidc       /  0.128s] CreateTopics: duration 73.897ms
[0126_oauthbearer_oidc       /  0.131s] Produce to rdkafkatest_rnd1aed7f67de2ce6_0126-oauthbearer_oidc [1]: messages #0..0
[0126_oauthbearer_oidc       /  0.131s] SUM(POLL): duration 0.002ms
[0126_oauthbearer_oidc       /  0.131s] PRODUCE: duration 0.039ms
[0126_oauthbearer_oidc       /  0.131s] PRODUCE.DELIVERY.WAIT: duration 0.000ms
[0126_oauthbearer_oidc       /  0.137s] Created    kafka instance 0126_oauthbearer_oidc#consumer-2
[0126_oauthbearer_oidc       /  0.137s] Subscribing to topic rdkafkatest_rnd1aed7f67de2ce6_0126-oauthbearer_oidc in group rdkafkatest_rnd1aed7f67de2ce6_0126-oauthbearer_oidc (expecting 1 msgs with testid 0)
[0126_oauthbearer_oidc       /  0.137s] consume.easy: consume 1 messages
[<MAIN>                      /  1.001s] 1 test(s) running: 0126_oauthbearer_oidc
[<MAIN>                      /  2.001s] 1 test(s) running: 0126_oauthbearer_oidc
[<MAIN>                      /  3.005s] 1 test(s) running: 0126_oauthbearer_oidc
[<MAIN>                      /  4.009s] 1 test(s) running: 0126_oauthbearer_oidc
[0126_oauthbearer_oidc       /  4.862s] rdkafkatest_rnd1aed7f67de2ce6_0126-oauthbearer_oidc [0] reached EOF at offset 0
[0126_oauthbearer_oidc       /  4.862s] CONSUME: duration 4724.758ms
[0126_oauthbearer_oidc       /  4.862s] consume.easy: consumed 0/1 messages (1/1 EOFs)
[0126_oauthbearer_oidc       /  4.862s] Closing consumer 0126_oauthbearer_oidc#consumer-2
[0126_oauthbearer_oidc       /  4.873s] CONSUMER.CLOSE: duration 10.551ms
[0126_oauthbearer_oidc       /  4.876s] [ do_test_create_producer:48: Test producer with oidc configuration: PASS (4.88s) ]
[0126_oauthbearer_oidc       /  4.877s] 0126_oauthbearer_oidc: duration 4876.512ms
[0126_oauthbearer_oidc       /  4.877s] ================= Test 0126_oauthbearer_oidc PASSED =================
[<MAIN>                      /  5.010s] ALL-TESTS: duration 5009.423ms
TEST 20210830213554 (bare, scenario default) SUMMARY
#==================================================================#
| <MAIN>                                   |     PASSED |   5.010s |
| 0126_oauthbearer_oidc                    |     PASSED |   4.877s |
#==================================================================#
[<MAIN>                      /  5.027s] 0 thread(s) in use by librdkafka
[<MAIN>                      /  5.027s] 
============== ALL TESTS PASSED ==============
###
###  ./test-runner in bare mode PASSED! ###
###",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,3510,2021-08-24T04:09:38Z,2021-08-31T07:22:04Z,2021-08-31T07:22:04Z,MERGED,True,205,17,9,https://github.com/jliunyu,OAuth/OIDC: Add fields to client configuration,1,[],https://github.com/edenhill/librdkafka/pull/3510,https://github.com/jliunyu,2,https://github.com/edenhill/librdkafka/pull/3510#issuecomment-907025283,"Local test:
./run-test.sh
############## ./test-runner ################
### Running test ./test-runner in bare mode ###
[<MAIN>                      /  0.000s] Setting test timeout to 10s * 1.0
[<MAIN>                      /  0.000s] Git version: e143b4a1@kip768-coding
[<MAIN>                      /  0.000s] Broker version: 2.3.0 (2.3.0.0)
[<MAIN>                      /  0.000s] Tests to run : 0126
[<MAIN>                      /  0.000s] Test mode    : bare
[<MAIN>                      /  0.000s] Test scenario: default
[<MAIN>                      /  0.000s] Test filter  : no filter
[<MAIN>                      /  0.000s] Test timeout multiplier: 2.7
[<MAIN>                      /  0.000s] Action on test failure: continue other tests
[<MAIN>                      /  0.000s] Current directory: /Users/jliu/Documents/Code/jliunyu/librdkafka/tests
[<MAIN>                      /  0.000s] Setting test timeout to 30s * 2.7
[<MAIN>                      /  0.001s] 1 test(s) running: 0126_oauthbearer_oidc
[0126_oauthbearer_oidc       /  0.000s] ================= Running test 0126_oauthbearer_oidc =================
[0126_oauthbearer_oidc       /  0.000s] ==== Stats written to file stats_0126_oauthbearer_oidc_7294066187078906598.json ====
[0126_oauthbearer_oidc       /  0.000s] [ do_test_create_producer:48: Test producer with oidc configuration ]
[0126_oauthbearer_oidc       /  0.002s] Setting test timeout to 60s * 2.7
[0126_oauthbearer_oidc       /  0.054s] Created    kafka instance 0126_oauthbearer_oidc#producer-1
[0126_oauthbearer_oidc       /  0.055s] Using topic ""rdkafkatest_rnd1aed7f67de2ce6_0126-oauthbearer_oidc""
[0126_oauthbearer_oidc       /  0.055s] Creating topic ""rdkafkatest_rnd1aed7f67de2ce6_0126-oauthbearer_oidc"" (partitions=1, replication_factor=1, timeout=26666)
[0126_oauthbearer_oidc       /  0.128s] CreateTopics: duration 73.897ms
[0126_oauthbearer_oidc       /  0.131s] Produce to rdkafkatest_rnd1aed7f67de2ce6_0126-oauthbearer_oidc [1]: messages #0..0
[0126_oauthbearer_oidc       /  0.131s] SUM(POLL): duration 0.002ms
[0126_oauthbearer_oidc       /  0.131s] PRODUCE: duration 0.039ms
[0126_oauthbearer_oidc       /  0.131s] PRODUCE.DELIVERY.WAIT: duration 0.000ms
[0126_oauthbearer_oidc       /  0.137s] Created    kafka instance 0126_oauthbearer_oidc#consumer-2
[0126_oauthbearer_oidc       /  0.137s] Subscribing to topic rdkafkatest_rnd1aed7f67de2ce6_0126-oauthbearer_oidc in group rdkafkatest_rnd1aed7f67de2ce6_0126-oauthbearer_oidc (expecting 1 msgs with testid 0)
[0126_oauthbearer_oidc       /  0.137s] consume.easy: consume 1 messages
[<MAIN>                      /  1.001s] 1 test(s) running: 0126_oauthbearer_oidc
[<MAIN>                      /  2.001s] 1 test(s) running: 0126_oauthbearer_oidc
[<MAIN>                      /  3.005s] 1 test(s) running: 0126_oauthbearer_oidc
[<MAIN>                      /  4.009s] 1 test(s) running: 0126_oauthbearer_oidc
[0126_oauthbearer_oidc       /  4.862s] rdkafkatest_rnd1aed7f67de2ce6_0126-oauthbearer_oidc [0] reached EOF at offset 0
[0126_oauthbearer_oidc       /  4.862s] CONSUME: duration 4724.758ms
[0126_oauthbearer_oidc       /  4.862s] consume.easy: consumed 0/1 messages (1/1 EOFs)
[0126_oauthbearer_oidc       /  4.862s] Closing consumer 0126_oauthbearer_oidc#consumer-2
[0126_oauthbearer_oidc       /  4.873s] CONSUMER.CLOSE: duration 10.551ms
[0126_oauthbearer_oidc       /  4.876s] [ do_test_create_producer:48: Test producer with oidc configuration: PASS (4.88s) ]
[0126_oauthbearer_oidc       /  4.877s] 0126_oauthbearer_oidc: duration 4876.512ms
[0126_oauthbearer_oidc       /  4.877s] ================= Test 0126_oauthbearer_oidc PASSED =================
[<MAIN>                      /  5.010s] ALL-TESTS: duration 5009.423ms
TEST 20210830213554 (bare, scenario default) SUMMARY
#==================================================================#
| <MAIN>                                   |     PASSED |   5.010s |
| 0126_oauthbearer_oidc                    |     PASSED |   4.877s |
#==================================================================#
[<MAIN>                      /  5.027s] 0 thread(s) in use by librdkafka
[<MAIN>                      /  5.027s] 
============== ALL TESTS PASSED ==============
###
###  ./test-runner in bare mode PASSED! ###
###",I will update the test part for the new fields later.,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,3512,2021-08-25T11:15:50Z,2021-08-27T12:50:24Z,2021-08-27T12:50:26Z,MERGED,True,275,1,3,https://github.com/edenhill,Added string splitter and kv splitter,1,[],https://github.com/edenhill/librdkafka/pull/3512,https://github.com/edenhill,1,https://github.com/edenhill/librdkafka/pull/3512,These can be used to split the sasl.oauthbearer.extensions=.. into a format suitable for set_token().,These can be used to split the sasl.oauthbearer.extensions=.. into a format suitable for set_token().,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,3516,2021-08-26T13:40:36Z,2021-08-26T21:07:11Z,2021-08-26T21:07:14Z,MERGED,True,77,12,6,https://github.com/edenhill,Verify checksums of source dependencies,2,[],https://github.com/edenhill/librdkafka/pull/3516,https://github.com/edenhill,1,https://github.com/edenhill/librdkafka/pull/3516,,,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,3520,2021-08-26T17:49:47Z,2021-08-26T21:07:31Z,2021-08-26T21:07:34Z,MERGED,True,13,7,6,https://github.com/edenhill,Travis: log in to docker to avoid rate limiting,3,[],https://github.com/edenhill/librdkafka/pull/3520,https://github.com/edenhill,1,https://github.com/edenhill/librdkafka/pull/3520,,,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,3521,2021-08-27T06:19:01Z,2021-08-27T07:45:28Z,2021-08-27T07:45:31Z,MERGED,True,1,0,1,https://github.com/jenny-cheung,Fix a small error due to the unreleased lock before program exit,1,[],https://github.com/edenhill/librdkafka/pull/3521,https://github.com/jenny-cheung,1,https://github.com/edenhill/librdkafka/pull/3521,Fix a small error due to the unreleased lock skm->lock before program exit.,Fix a small error due to the unreleased lock skm->lock before program exit.,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,3521,2021-08-27T06:19:01Z,2021-08-27T07:45:28Z,2021-08-27T07:45:31Z,MERGED,True,1,0,1,https://github.com/jenny-cheung,Fix a small error due to the unreleased lock before program exit,1,[],https://github.com/edenhill/librdkafka/pull/3521,https://github.com/edenhill,2,https://github.com/edenhill/librdkafka/pull/3521#issuecomment-906999790,Fix a small error due to the unreleased lock skm->lock before program exit.,Thank you!,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,3533,2021-09-08T11:01:40Z,2021-09-13T05:46:14Z,2021-09-13T05:46:19Z,MERGED,True,2,0,2,https://github.com/chrisnovakovic,mklove: make zlib test program compilable,1,[],https://github.com/edenhill/librdkafka/pull/3533,https://github.com/chrisnovakovic,1,https://github.com/edenhill/librdkafka/pull/3533,"The test program that is used at compile-time to detect whether zlib is available fails to compile due to NULL being undefined:
_mkltmpyos55w.c:5:20: error: use of undeclared identifier 'NULL'
     z_stream *p = NULL;
                   ^
1 error generated.

This means that zlib availability is only automatically detected when using pkg-config.
Import stddef.h (which defines NULL) in the test program, allowing zlib to be automatically detected via a compilation check.","The test program that is used at compile-time to detect whether zlib is available fails to compile due to NULL being undefined:
_mkltmpyos55w.c:5:20: error: use of undeclared identifier 'NULL'
     z_stream *p = NULL;
                   ^
1 error generated.

This means that zlib availability is only automatically detected when using pkg-config.
Import stddef.h (which defines NULL) in the test program, allowing zlib to be automatically detected via a compilation check.",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,3533,2021-09-08T11:01:40Z,2021-09-13T05:46:14Z,2021-09-13T05:46:19Z,MERGED,True,2,0,2,https://github.com/chrisnovakovic,mklove: make zlib test program compilable,1,[],https://github.com/edenhill/librdkafka/pull/3533,https://github.com/chrisnovakovic,2,https://github.com/edenhill/librdkafka/pull/3533#issuecomment-915238172,"The test program that is used at compile-time to detect whether zlib is available fails to compile due to NULL being undefined:
_mkltmpyos55w.c:5:20: error: use of undeclared identifier 'NULL'
     z_stream *p = NULL;
                   ^
1 error generated.

This means that zlib availability is only automatically detected when using pkg-config.
Import stddef.h (which defines NULL) in the test program, allowing zlib to be automatically detected via a compilation check.",The sole failing test doesn't appear to be caused by this change.,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,3533,2021-09-08T11:01:40Z,2021-09-13T05:46:14Z,2021-09-13T05:46:19Z,MERGED,True,2,0,2,https://github.com/chrisnovakovic,mklove: make zlib test program compilable,1,[],https://github.com/edenhill/librdkafka/pull/3533,https://github.com/edenhill,3,https://github.com/edenhill/librdkafka/pull/3533#issuecomment-917860421,"The test program that is used at compile-time to detect whether zlib is available fails to compile due to NULL being undefined:
_mkltmpyos55w.c:5:20: error: use of undeclared identifier 'NULL'
     z_stream *p = NULL;
                   ^
1 error generated.

This means that zlib availability is only automatically detected when using pkg-config.
Import stddef.h (which defines NULL) in the test program, allowing zlib to be automatically detected via a compilation check.",Thank you!,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,3534,2021-09-08T15:17:05Z,,2021-09-08T15:17:05Z,OPEN,False,22,20,5,https://github.com/chrisnovakovic,Avoid unused variable warnings for return values used only in assertions,1,[],https://github.com/edenhill/librdkafka/pull/3534,https://github.com/chrisnovakovic,1,https://github.com/edenhill/librdkafka/pull/3534,"The return values of many functions are assigned to variables for the sole purpose of using them in assertion conditions, which causes unused variable warnings to be emitted at compile-time when NDEBUG is defined. Use some compiler trickery in the definition of rd_assert() to avoid unused variable warnings when variables in the assertion condition are only used for that purpose, and ensure that assertions whose conditions contain single-use variables use rd_assert() instead of assert().","The return values of many functions are assigned to variables for the sole purpose of using them in assertion conditions, which causes unused variable warnings to be emitted at compile-time when NDEBUG is defined. Use some compiler trickery in the definition of rd_assert() to avoid unused variable warnings when variables in the assertion condition are only used for that purpose, and ensure that assertions whose conditions contain single-use variables use rd_assert() instead of assert().",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,3535,2021-09-08T16:00:49Z,,2022-05-03T07:16:52Z,OPEN,False,19,12,3,https://github.com/chrisnovakovic,ssl: support libssls with no ENGINE implementation,1,[],https://github.com/edenhill/librdkafka/pull/3535,https://github.com/chrisnovakovic,1,https://github.com/edenhill/librdkafka/pull/3535,"OpenSSL can be built without ENGINE support, and some libssl-compatible forks (e.g. BoringSSL) don't contain any ENGINE implementation at all - guard all references to the ENGINE API using OPENSSL_NO_ENGINE so these libssls can be used with librdkafka.","OpenSSL can be built without ENGINE support, and some libssl-compatible forks (e.g. BoringSSL) don't contain any ENGINE implementation at all - guard all references to the ENGINE API using OPENSSL_NO_ENGINE so these libssls can be used with librdkafka.",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,3535,2021-09-08T16:00:49Z,,2022-05-03T07:16:52Z,OPEN,False,19,12,3,https://github.com/chrisnovakovic,ssl: support libssls with no ENGINE implementation,1,[],https://github.com/edenhill/librdkafka/pull/3535,https://github.com/edenhill,2,https://github.com/edenhill/librdkafka/pull/3535#issuecomment-1109486138,"OpenSSL can be built without ENGINE support, and some libssl-compatible forks (e.g. BoringSSL) don't contain any ENGINE implementation at all - guard all references to the ENGINE API using OPENSSL_NO_ENGINE so these libssls can be used with librdkafka.","Please rebase this on latest master, and verify that it builds correctly with LibreSSL.
Thanks!",True,{'THUMBS_UP': ['https://github.com/chrisnovakovic']}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,3535,2021-09-08T16:00:49Z,,2022-05-03T07:16:52Z,OPEN,False,19,12,3,https://github.com/chrisnovakovic,ssl: support libssls with no ENGINE implementation,1,[],https://github.com/edenhill/librdkafka/pull/3535,https://github.com/chrisnovakovic,3,https://github.com/edenhill/librdkafka/pull/3535#issuecomment-1111577912,"OpenSSL can be built without ENGINE support, and some libssl-compatible forks (e.g. BoringSSL) don't contain any ENGINE implementation at all - guard all references to the ENGINE API using OPENSSL_NO_ENGINE so these libssls can be used with librdkafka.","While rebasing on master, I also refactored the changes to src/rdkafka_ssl.c - now there's an OPENSSL_HAS_ENGINE macro that eliminates the need to repeat the conditional every time.
The following table lists the result of the 0124_openssl_invalid_engine test with various libssls before and after applying this PR:



libssl
master
#3535




OpenSSL 1.0.2u
SKIPPED
SKIPPED


OpenSSL 1.1.1m with ENGINE
PASSED
PASSED


OpenSSL 1.1.1m without ENGINE
Build failure
SKIPPED


LibreSSL 3.3.4
SKIPPED
PASSED



SKIPPED indicates that the test was skipped with the reason Configuration property ""ssl.engine.location"" not supported in this build: OpenSSL with ENGINE support not available at build time.
Note that the test isn't skipped on LibreSSL any more - LibreSSL 3.3 does contain an ENGINE component, and it should probably be used if enabled (e.g. so it can make use of AES-NI).",True,{'THUMBS_UP': ['https://github.com/edenhill']}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,3535,2021-09-08T16:00:49Z,,2022-05-03T07:16:52Z,OPEN,False,19,12,3,https://github.com/chrisnovakovic,ssl: support libssls with no ENGINE implementation,1,[],https://github.com/edenhill/librdkafka/pull/3535,https://github.com/edenhill,4,https://github.com/edenhill/librdkafka/pull/3535#issuecomment-1112158342,"OpenSSL can be built without ENGINE support, and some libssl-compatible forks (e.g. BoringSSL) don't contain any ENGINE implementation at all - guard all references to the ENGINE API using OPENSSL_NO_ENGINE so these libssls can be used with librdkafka.",Some style-fixes still needed (run make style-fix),True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,3535,2021-09-08T16:00:49Z,,2022-05-03T07:16:52Z,OPEN,False,19,12,3,https://github.com/chrisnovakovic,ssl: support libssls with no ENGINE implementation,1,[],https://github.com/edenhill/librdkafka/pull/3535,https://github.com/chrisnovakovic,5,https://github.com/edenhill/librdkafka/pull/3535#issuecomment-1112706060,"OpenSSL can be built without ENGINE support, and some libssl-compatible forks (e.g. BoringSSL) don't contain any ENGINE implementation at all - guard all references to the ENGINE API using OPENSSL_NO_ENGINE so these libssls can be used with librdkafka.","Unfortunately it seems that clang-format bounces between
        .unsupported = ""OpenSSL with ENGINE support not available at build ""   \
                       ""time""

and
        .unsupported =                                                         \
            ""OpenSSL with ENGINE support not available at build ""              \
            ""time""


as recommendations - using one always causes it to suggest the other. There's no way that's not a bug in clang-format, but I'll just reword that string so it fits on a single line.",True,{'THUMBS_UP': ['https://github.com/edenhill']}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,3535,2021-09-08T16:00:49Z,,2022-05-03T07:16:52Z,OPEN,False,19,12,3,https://github.com/chrisnovakovic,ssl: support libssls with no ENGINE implementation,1,[],https://github.com/edenhill/librdkafka/pull/3535,https://github.com/chrisnovakovic,6,https://github.com/edenhill/librdkafka/pull/3535#issuecomment-1112718160,"OpenSSL can be built without ENGINE support, and some libssl-compatible forks (e.g. BoringSSL) don't contain any ENGINE implementation at all - guard all references to the ENGINE API using OPENSSL_NO_ENGINE so these libssls can be used with librdkafka.","Tried to get clever by using a string that was unambiguously over the 80-column limit, but clang-format wasn't having any of that either. Reverted to a shorter, single-line message and it looks like we're there now. Sorry for all the noise!",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,3535,2021-09-08T16:00:49Z,,2022-05-03T07:16:52Z,OPEN,False,19,12,3,https://github.com/chrisnovakovic,ssl: support libssls with no ENGINE implementation,1,[],https://github.com/edenhill/librdkafka/pull/3535,https://github.com/edenhill,7,https://github.com/edenhill/librdkafka/pull/3535#issuecomment-1115804601,"OpenSSL can be built without ENGINE support, and some libssl-compatible forks (e.g. BoringSSL) don't contain any ENGINE implementation at all - guard all references to the ENGINE API using OPENSSL_NO_ENGINE so these libssls can be used with librdkafka.","Thanks for all your work on this @chrisnovakovic!
We're a bit too close to the 1.9.0 release to merge this, but we'll make sure to include it in a 1.9.1 release (scheduled for June).",True,{'THUMBS_UP': ['https://github.com/chrisnovakovic']}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,3539,2021-09-10T14:58:50Z,2021-09-20T12:38:41Z,2021-09-20T12:38:45Z,MERGED,True,369,69,10,https://github.com/edenhill,Use native scheduler on Windows,4,[],https://github.com/edenhill/librdkafka/pull/3539,https://github.com/edenhill,1,https://github.com/edenhill/librdkafka/pull/3539,"We've seen an increase in concerns about the internal localhost TCP connections that we use to wake up internal queue listeners.
This here PR changes the IO/queue scheduler to use WSAWaitForMultiplEvents() on Windows, allowing a single call
to wait for both socket IO and queue cond-var signals. Thanks to this the localhost TCP connections are no more.
Manual testing showed a 5-10% throughput increase for the producer.","We've seen an increase in concerns about the internal localhost TCP connections that we use to wake up internal queue listeners.
This here PR changes the IO/queue scheduler to use WSAWaitForMultiplEvents() on Windows, allowing a single call
to wait for both socket IO and queue cond-var signals. Thanks to this the localhost TCP connections are no more.
Manual testing showed a 5-10% throughput increase for the producer.",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,3544,2021-09-14T21:32:24Z,2021-09-16T14:28:51Z,2021-09-16T14:28:51Z,CLOSED,False,2,2,1,https://github.com/confluentmark,update .travis.yml,2,[],https://github.com/edenhill/librdkafka/pull/3544,https://github.com/confluentmark,1,https://github.com/edenhill/librdkafka/pull/3544,testing travisci bug report,testing travisci bug report,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,3547,2021-09-15T18:49:20Z,2021-09-15T19:46:12Z,2021-09-15T19:46:18Z,MERGED,True,7,6,4,https://github.com/edenhill,Rotated keys,4,[],https://github.com/edenhill/librdkafka/pull/3547,https://github.com/edenhill,1,https://github.com/edenhill/librdkafka/pull/3547,,,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,3550,2021-09-17T09:44:09Z,2021-09-20T12:37:18Z,2021-11-24T14:18:25Z,MERGED,True,146,36,9,https://github.com/edenhill,Add 'ssl.ca.pem' property (#2380),1,[],https://github.com/edenhill/librdkafka/pull/3550,https://github.com/edenhill,1,https://github.com/edenhill/librdkafka/pull/3550,"Fixes daughter clients as well, such as confluentinc/confluent-kafka-dotnet#1453","Fixes daughter clients as well, such as confluentinc/confluent-kafka-dotnet#1453",True,"{'HEART': ['https://github.com/dstelljes', 'https://github.com/harshpatel94']}"
edenhill/librdkafka,https://github.com/edenhill/librdkafka,3550,2021-09-17T09:44:09Z,2021-09-20T12:37:18Z,2021-11-24T14:18:25Z,MERGED,True,146,36,9,https://github.com/edenhill,Add 'ssl.ca.pem' property (#2380),1,[],https://github.com/edenhill/librdkafka/pull/3550,https://github.com/stefk,2,https://github.com/edenhill/librdkafka/pull/3550#issuecomment-977920863,"Fixes daughter clients as well, such as confluentinc/confluent-kafka-dotnet#1453",@edenhill any chance to get a corresponding update of the bundled librdkafka in https://github.com/confluentinc/confluent-kafka-go?,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,3553,2021-09-20T12:58:51Z,,2022-04-07T07:23:41Z,OPEN,False,225,32,11,https://github.com/edenhill,Add 'fetch.queue.backoff.ms' to the consumer (#2879),1,[],https://github.com/edenhill/librdkafka/pull/3553,https://github.com/edenhill,1,https://github.com/edenhill/librdkafka/pull/3553,,,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,3555,2021-09-21T08:20:57Z,2021-09-24T07:04:09Z,2021-09-24T07:04:09Z,CLOSED,False,404,3,8,https://github.com/jliunyu,Retrieve token with unittest,5,[],https://github.com/edenhill/librdkafka/pull/3555,https://github.com/jliunyu,1,https://github.com/edenhill/librdkafka/pull/3555,"Extract the jwt from response token:
""eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCIsImtpZCI6ImFiY2VkZmcifQ.eyJpYXQiOjE2MzIzNjkzMTQsInN1YiI6InN1YiIsImV4cCI6MTYzMjM2OTYxNH0.Rnw42hwB6GwWx0xkScPegArfVSLhCNooMtnT4mX6GTM""","Extract the jwt from response token:
""eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCIsImtpZCI6ImFiY2VkZmcifQ.eyJpYXQiOjE2MzIzNjkzMTQsInN1YiI6InN1YiIsImV4cCI6MTYzMjM2OTYxNH0.Rnw42hwB6GwWx0xkScPegArfVSLhCNooMtnT4mX6GTM""",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,3555,2021-09-21T08:20:57Z,2021-09-24T07:04:09Z,2021-09-24T07:04:09Z,CLOSED,False,404,3,8,https://github.com/jliunyu,Retrieve token with unittest,5,[],https://github.com/edenhill/librdkafka/pull/3555,https://github.com/edenhill,2,https://github.com/edenhill/librdkafka/pull/3555#issuecomment-923754407,"Extract the jwt from response token:
""eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCIsImtpZCI6ImFiY2VkZmcifQ.eyJpYXQiOjE2MzIzNjkzMTQsInN1YiI6InN1YiIsImV4cCI6MTYzMjM2OTYxNH0.Rnw42hwB6GwWx0xkScPegArfVSLhCNooMtnT4mX6GTM""","I rebased kip768 on master, so please update your local kip768 and then rebase your PR branch on kip768 and force push the PR branch again.
E.g.:
$ git checkout kip768
$ git pull --rebase origin kip768
$ git checkout your_pr_branch
$ git rebase kip768
$ make  # verify things are ok
# if all looks good
$ git push --dry-run --force yourupstream your_pr_branch
# (remove --dry-run when it looks ok)
or, a simpler approach if this is just one commit:
update kip768 according to above instructions
create a new PR branch
add your changes to this new PR branch, commit
push the new PR branch.",True,{'THUMBS_UP': ['https://github.com/jliunyu']}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,3558,2021-09-22T12:18:44Z,2021-09-23T14:09:32Z,2021-09-23T14:09:36Z,MERGED,True,73,11,5,https://github.com/edenhill,Improve nuget release script,2,[],https://github.com/edenhill/librdkafka/pull/3558,https://github.com/edenhill,1,https://github.com/edenhill/librdkafka/pull/3558,"Verify artifact file contents and architectures.
Verify that artifact attributes match.
Get README, CONFIG,.. etc, from artifacts instead of local source tree
(which may not match the released version).","Verify artifact file contents and architectures.
Verify that artifact attributes match.
Get README, CONFIG,.. etc, from artifacts instead of local source tree
(which may not match the released version).",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,3560,2021-09-27T03:44:26Z,2021-11-23T14:50:10Z,2021-11-23T14:50:10Z,MERGED,True,1038,59,16,https://github.com/jliunyu,Retrieve jwt token from token provider,11,[],https://github.com/edenhill/librdkafka/pull/3560,https://github.com/jliunyu,1,https://github.com/edenhill/librdkafka/pull/3560,"Retrieve jwt from token provider and forward it to the broker:  the example received from provider is listed as the json below, extract the token key with key work ""access_token"".
{
    access_token: eyJ0eXAiOiJKV1QiLCJhbGciOiJIUzI1NiIsImtpZCI6ImFiY2VkZmcifQ.eyJleHAiOjE2MzI2ODYwNjAsImlhdCI6MTYzMjY4NTc2MCwic3ViIjoic3ViIn0.yY0F9NUD-MJydlF0NzfNXQtl7gYKO6A-yKM-6_6RTcg
}

Test:

The extract jwt from json is tested with unit test.
The HTTP(S) request to http server part is tested with the following function which is not added to the unit test part:

int unittest_sasl_oauthbearer_oidc (void) {
        rd_kafka_conf_res_t res;

        rd_kafka_conf_t *conf = NULL;
        rd_kafka_t *rk;
        char errstr[512];

        RD_UT_BEGIN();

        conf = rd_kafka_conf_new();
        res = rd_kafka_conf_set(conf, ""sasl.oauthbearer.token.endpoint.url"",
                                ""http://localhost:8080/retrieve"", NULL, 0);
        res = rd_kafka_conf_set(conf, ""sasl.oauthbearer.client.id"",
                                ""123"", NULL, 0);
        res = rd_kafka_conf_set(conf, ""sasl.oauthbearer.client.secret"",
                               ""abc"", NULL, 0);
        res = rd_kafka_conf_set(conf, ""sasl.oauthbearer.scope"",
                                ""test-scope"", NULL, 0);
        res = rd_kafka_conf_set(conf, ""sasl.oauthbearer.extensions"",
                                ""grant_type=client_credentials,""
                                ""scope=test-scope"",
                                NULL, 0);
        rk = rd_kafka_new(RD_KAFKA_CONSUMER, conf, errstr, sizeof(errstr));
        rd_kafka_conf_set_oauthbearer_oidc_token_refresh_cb(rk, NULL, NULL);

        rd_kafka_destroy(rk);
        RD_UT_PASS();
}


Tested with OKTA as token provider and mock up broker.

[0126_oauthbearer_oidc       /  4.993s] [ do_test_produce_consumer_with_OIDC:48: Test producer and consumer with oidc configuration: PASS (4.99s) ]
[0126_oauthbearer_oidc       /  4.994s] [ do_test_produce_consumer_with_OIDC_should_fail:110: Test OAUTHBEARER/OIDC failing with invalid JWT ]
[0126_oauthbearer_oidc       /  4.995s] Created    kafka instance 0126_oauthbearer_oidc#consumer-3
[<MAIN>                      /  5.026s] 1 test(s) running: 0126_oauthbearer_oidc
[<MAIN>                      /  6.031s] 1 test(s) running: 0126_oauthbearer_oidc
%3|1635444868.772|FAIL|0126_oauthbearer_oidc#consumer-3| [thrd:sasl_plaintext://0.0.0.0:57804/bootstrap]: sasl_plaintext://0.0.0.0:57804/bootstrap: SASL authentication error: {""status"":""invalid_token""} (after 313ms in state AUTH_REQ)
[0126_oauthbearer_oidc       /  6.314s] Expected error: Local: Authentication failure: sasl_plaintext://0.0.0.0:57804/bootstrap: SASL authentication error: {""status"":""invalid_token""} (after 313ms in state AUTH_REQ)
[0126_oauthbearer_oidc       /  6.314s] Closing consumer 0126_oauthbearer_oidc#consumer-3
[0126_oauthbearer_oidc       /  6.314s] CONSUMER.CLOSE: duration 0.050ms
[0126_oauthbearer_oidc       /  6.315s] [ do_test_produce_consumer_with_OIDC_should_fail:110: Test OAUTHBEARER/OIDC failing with invalid JWT: PASS (1.32s) ]
[0126_oauthbearer_oidc       /  6.315s] 0126_oauthbearer_oidc: duration 6314.545ms
[0126_oauthbearer_oidc       /  6.315s] ================= Test 0126_oauthbearer_oidc PASSED =================
[<MAIN>                      /  7.036s] ALL-TESTS: duration 7035.850ms
TEST 20211028111429 (bare, scenario default) SUMMARY
#==================================================================#
| <MAIN>                                   |     PASSED |   7.037s |
| 0126_oauthbearer_oidc                    |     PASSED |   6.315s |
#==================================================================#
[<MAIN>                      /  7.041s] 0 thread(s) in use by librdkafka
[<MAIN>                      /  7.041s] 
============== ALL TESTS PASSED ==============
###
###  ./test-runner in bare mode PASSED! ###
###
bash-3.2$ 


The build failure from Windows is not related to this PR:

[100%] Built target test-runner
Install the project...
-- Install configuration: """"
-- Installing: C:/Users/travis/build/edenhill/librdkafka/dest/lib/cmake/RdKafka/RdKafkaConfig.cmake
-- Installing: C:/Users/travis/build/edenhill/librdkafka/dest/lib/cmake/RdKafka/RdKafkaConfigVersion.cmake
-- Installing: C:/Users/travis/build/edenhill/librdkafka/dest/lib/cmake/RdKafka/FindLZ4.cmake
-- Installing: C:/Users/travis/build/edenhill/librdkafka/dest/lib/cmake/RdKafka/RdKafkaTargets.cmake
-- Installing: C:/Users/travis/build/edenhill/librdkafka/dest/lib/cmake/RdKafka/RdKafkaTargets-noconfig.cmake
-- Installing: C:/Users/travis/build/edenhill/librdkafka/dest/share/licenses/librdkafka/LICENSES.txt
-- Installing: C:/Users/travis/build/edenhill/librdkafka/dest/lib/pkgconfig/rdkafka-static.pc
-- Installing: C:/Users/travis/build/edenhill/librdkafka/dest/lib/librdkafka.a
-- Installing: C:/Users/travis/build/edenhill/librdkafka/dest/include/librdkafka/rdkafka.h
-- Installing: C:/Users/travis/build/edenhill/librdkafka/dest/include/librdkafka/rdkafka_mock.h
-- Installing: C:/Users/travis/build/edenhill/librdkafka/dest/lib/pkgconfig/rdkafka++-static.pc
-- Installing: C:/Users/travis/build/edenhill/librdkafka/dest/lib/librdkafka++.a
-- Installing: C:/Users/travis/build/edenhill/librdkafka/dest/include/librdkafka/rdkafkacpp.h
~/build/edenhill/librdkafka/mergescratch ~/build/edenhill/librdkafka
~/build/edenhill/librdkafka


Tested with mock up token provider.

[0126_oauthbearer_oidc       /  7.192s] 0126_oauthbearer_oidc: duration 7191.931ms
[0126_oauthbearer_oidc       /  7.192s] ================= Test 0126_oauthbearer_oidc PASSED =================
[<MAIN>                      /  8.018s] ALL-TESTS: duration 8018.276ms
TEST 20211115233334 (bare, scenario default) SUMMARY
#==================================================================#
| <MAIN>                                   |     PASSED |   8.019s |
| 0126_oauthbearer_oidc                    |     PASSED |   7.192s |
#==================================================================#
[<MAIN>                      /  8.021s] 0 thread(s) in use by librdkafka
[<MAIN>                      /  8.021s] 
============== ALL TESTS PASSED ==============
###
###  ./test-runner in bare mode PASSED! ###
###


valgrind: TESTS=0126 ./run-test.sh valgrind --exit-on-first-error=no
Saw two leaks, but I don't think they are related to my change.
(1) related to rd_http_req_destroy(&hreq)

==116526==     in use at exit: 80 bytes in 1 blocks
==116526==   total heap usage: 6,618 allocs, 6,617 frees, 433,555 bytes allocated
==116526== 
==116526== 80 bytes in 1 blocks are definitely lost in loss record 1 of 1
==116526==    at 0x483B7F3: malloc (in /usr/lib/x86_64-linux-gnu/valgrind/vgpreload_memcheck-amd64-linux.so)
==116526==    by 0x4A051CC: rd_malloc (rd.h:131)
==116526==    by 0x4A05F26: rd_buf_new (rdbuf.c:380)
==116526==    by 0x4A622C9: rd_http_req_init (rdhttp.c:139)
==116526==    by 0x4A65468: ut_sasl_oauthbearer_oidc_should_succeed (rdkafka_sasl_oauthbearer_oidc.c:410)
==116526==    by 0x4A6598B: unittest_sasl_oauthbearer_oidc (rdkafka_sasl_oauthbearer_oidc.c:504)
==116526==    by 0x4A116FF: rd_unittest (rdunittest.c:497)
==116526==    by 0x48DEFCC: rd_kafka_unittest (rdkafka.c:4860)
==116526==    by 0x1A5A3B: main_0000_unittests (0000-unittests.c:68)
==116526==    by 0x2884B7: run_test0 (test.c:1103)
==116526==    by 0x288ADD: run_test_from_thread (test.c:1167)
==116526==    by 0x4B686D9: start_thread (pthread_create.c:474)
==116526== 
==116526== 

Saw the same error for rdhttp.c.
==110197== 80 bytes in 1 blocks are definitely lost in loss record 1 of 2
==110197==    at 0x483B7F3: malloc (in /usr/lib/x86_64-linux-gnu/valgrind/vgpreload_memcheck-amd64-linux.so)
==110197==    by 0x4A051CC: rd_malloc (rd.h:131)
==110197==    by 0x4A05F26: rd_buf_new (rdbuf.c:380)
==110197==    by 0x4A622EF: rd_http_req_init (rdhttp.c:139)
==110197==    by 0x4A62B9E: rd_http_get_json (rdhttp.c:388)
==110197==    by 0x4A63009: unittest_http (rdhttp.c:475)
==110197==    by 0x4A11725: rd_unittest (rdunittest.c:497)
==110197==    by 0x48DEFCC: rd_kafka_unittest (rdkafka.c:4860)
==110197==    by 0x1A5A3B: main_0000_unittests (0000-unittests.c:68)
==110197==    by 0x2884B7: run_test0 (test.c:1103)
==110197==    by 0x288ADD: run_test_from_thread (test.c:1167)
==110197==    by 0x4B686D9: start_thread (pthread_create.c:474)

(2) Related to test_create_topic()
HEAP SUMMARY:
==121240==     in use at exit: 5,032 bytes in 70 blocks
==121240==   total heap usage: 6,830 allocs, 6,760 frees, 1,155,127 bytes allocated
==121240== 
==121240== 8 bytes in 1 blocks are indirectly lost in loss record 1 of 13
==121240==    at 0x483DD99: calloc (in /usr/lib/x86_64-linux-gnu/valgrind/vgpreload_memcheck-amd64-linux.so)
==121240==    by 0x4999A2B: rd_calloc (rd.h:125)
==121240==    by 0x4999E88: rd_list_prealloc_elems (rdlist.c:108)
==121240==    by 0x4999D88: rd_list_init_copy (rdlist.c:69)
==121240==    by 0x49D3DB6: rd_kafka_NewTopic_copy (rdkafka_admin.c:1479)
==121240==    by 0x49D68D1: rd_kafka_CreateTopics (rdkafka_admin.c:1739)
==121240==    by 0x299822: test_admin_create_topic (test.c:4585)
==121240==    by 0x299C99: test_create_topic (test.c:4653)
==121240==    by 0x1B0B60: do_test_produce_consumer_with_OIDC (0126-oauthbearer_oidc.c:66)
==121240==    by 0x1B1876: main_0126_oauthbearer_oidc (0126-oauthbearer_oidc.c:206)
==121240==    by 0x2884C3: run_test0 (test.c:1103)
==121240==    by 0x288AE9: run_test_from_thread (test.c:1167)
==121240== 
==121240== 
==121240== Exit program on first error (--exit-on-first-error=yes)


Saw the same error for 0090-idempotence.c
HEAP SUMMARY:
==120515==     in use at exit: 340 bytes in 8 blocks
==120515==   total heap usage: 6,359 allocs, 6,351 frees, 2,160,748 bytes allocated
==120515== 
==120515== 8 bytes in 1 blocks are indirectly lost in loss record 1 of 7
==120515==    at 0x483DD99: calloc (in /usr/lib/x86_64-linux-gnu/valgrind/vgpreload_memcheck-amd64-linux.so)
==120515==    by 0x4999A2B: rd_calloc (rd.h:125)
==120515==    by 0x4999E88: rd_list_prealloc_elems (rdlist.c:108)
==120515==    by 0x4999D88: rd_list_init_copy (rdlist.c:69)
==120515==    by 0x49D3DB6: rd_kafka_NewTopic_copy (rdkafka_admin.c:1479)
==120515==    by 0x49D68D1: rd_kafka_CreateTopics (rdkafka_admin.c:1739)
==120515==    by 0x299822: test_admin_create_topic (test.c:4585)
==120515==    by 0x299C99: test_create_topic (test.c:4653)
==120515==    by 0x176AB0: do_test_implicit_ack (0090-idempotence.c:133)
==120515==    by 0x177078: main_0090_idempotence (0090-idempotence.c:165)
==120515==    by 0x2884C3: run_test0 (test.c:1103)
==120515==    by 0x288AE9: run_test_from_thread (test.c:1167)
==120515== 
==120515== 


Since the above errors are not related to this PR, I will dig more and send a new PR to fix.

Tested with manually set the token urls:

VALID_OIDC_URL=http://localhost:8080/retrieve INVALID_OIDC_URL=http://localhost:8080/retrieve/badformat EXPIRED_TOKEN_OIDC_URL=http://localhost:8080/retrieve/expire TESTS=0126 make

[0126_oauthbearer_oidc       / 29.667s] ================= Test 0126_oauthbearer_oidc PASSED =================
[<MAIN>                      / 30.100s] ALL-TESTS: duration 30099.377ms
TEST 20211121131641 (bare, scenario default) SUMMARY
#==================================================================#
| <MAIN>                                   |     PASSED |  30.100s |
| 0126_oauthbearer_oidc                    |     PASSED |  29.667s |
#==================================================================#
[<MAIN>                      / 30.101s] 0 thread(s) in use by librdkafka
[<MAIN>                      / 30.101s] 
============== ALL TESTS PASSED ==============
###
###  ./test-runner in bare mode PASSED! ###
###","Retrieve jwt from token provider and forward it to the broker:  the example received from provider is listed as the json below, extract the token key with key work ""access_token"".
{
    access_token: eyJ0eXAiOiJKV1QiLCJhbGciOiJIUzI1NiIsImtpZCI6ImFiY2VkZmcifQ.eyJleHAiOjE2MzI2ODYwNjAsImlhdCI6MTYzMjY4NTc2MCwic3ViIjoic3ViIn0.yY0F9NUD-MJydlF0NzfNXQtl7gYKO6A-yKM-6_6RTcg
}

Test:

The extract jwt from json is tested with unit test.
The HTTP(S) request to http server part is tested with the following function which is not added to the unit test part:

int unittest_sasl_oauthbearer_oidc (void) {
        rd_kafka_conf_res_t res;

        rd_kafka_conf_t *conf = NULL;
        rd_kafka_t *rk;
        char errstr[512];

        RD_UT_BEGIN();

        conf = rd_kafka_conf_new();
        res = rd_kafka_conf_set(conf, ""sasl.oauthbearer.token.endpoint.url"",
                                ""http://localhost:8080/retrieve"", NULL, 0);
        res = rd_kafka_conf_set(conf, ""sasl.oauthbearer.client.id"",
                                ""123"", NULL, 0);
        res = rd_kafka_conf_set(conf, ""sasl.oauthbearer.client.secret"",
                               ""abc"", NULL, 0);
        res = rd_kafka_conf_set(conf, ""sasl.oauthbearer.scope"",
                                ""test-scope"", NULL, 0);
        res = rd_kafka_conf_set(conf, ""sasl.oauthbearer.extensions"",
                                ""grant_type=client_credentials,""
                                ""scope=test-scope"",
                                NULL, 0);
        rk = rd_kafka_new(RD_KAFKA_CONSUMER, conf, errstr, sizeof(errstr));
        rd_kafka_conf_set_oauthbearer_oidc_token_refresh_cb(rk, NULL, NULL);

        rd_kafka_destroy(rk);
        RD_UT_PASS();
}


Tested with OKTA as token provider and mock up broker.

[0126_oauthbearer_oidc       /  4.993s] [ do_test_produce_consumer_with_OIDC:48: Test producer and consumer with oidc configuration: PASS (4.99s) ]
[0126_oauthbearer_oidc       /  4.994s] [ do_test_produce_consumer_with_OIDC_should_fail:110: Test OAUTHBEARER/OIDC failing with invalid JWT ]
[0126_oauthbearer_oidc       /  4.995s] Created    kafka instance 0126_oauthbearer_oidc#consumer-3
[<MAIN>                      /  5.026s] 1 test(s) running: 0126_oauthbearer_oidc
[<MAIN>                      /  6.031s] 1 test(s) running: 0126_oauthbearer_oidc
%3|1635444868.772|FAIL|0126_oauthbearer_oidc#consumer-3| [thrd:sasl_plaintext://0.0.0.0:57804/bootstrap]: sasl_plaintext://0.0.0.0:57804/bootstrap: SASL authentication error: {""status"":""invalid_token""} (after 313ms in state AUTH_REQ)
[0126_oauthbearer_oidc       /  6.314s] Expected error: Local: Authentication failure: sasl_plaintext://0.0.0.0:57804/bootstrap: SASL authentication error: {""status"":""invalid_token""} (after 313ms in state AUTH_REQ)
[0126_oauthbearer_oidc       /  6.314s] Closing consumer 0126_oauthbearer_oidc#consumer-3
[0126_oauthbearer_oidc       /  6.314s] CONSUMER.CLOSE: duration 0.050ms
[0126_oauthbearer_oidc       /  6.315s] [ do_test_produce_consumer_with_OIDC_should_fail:110: Test OAUTHBEARER/OIDC failing with invalid JWT: PASS (1.32s) ]
[0126_oauthbearer_oidc       /  6.315s] 0126_oauthbearer_oidc: duration 6314.545ms
[0126_oauthbearer_oidc       /  6.315s] ================= Test 0126_oauthbearer_oidc PASSED =================
[<MAIN>                      /  7.036s] ALL-TESTS: duration 7035.850ms
TEST 20211028111429 (bare, scenario default) SUMMARY
#==================================================================#
| <MAIN>                                   |     PASSED |   7.037s |
| 0126_oauthbearer_oidc                    |     PASSED |   6.315s |
#==================================================================#
[<MAIN>                      /  7.041s] 0 thread(s) in use by librdkafka
[<MAIN>                      /  7.041s] 
============== ALL TESTS PASSED ==============
###
###  ./test-runner in bare mode PASSED! ###
###
bash-3.2$ 


The build failure from Windows is not related to this PR:

[100%] Built target test-runner
Install the project...
-- Install configuration: """"
-- Installing: C:/Users/travis/build/edenhill/librdkafka/dest/lib/cmake/RdKafka/RdKafkaConfig.cmake
-- Installing: C:/Users/travis/build/edenhill/librdkafka/dest/lib/cmake/RdKafka/RdKafkaConfigVersion.cmake
-- Installing: C:/Users/travis/build/edenhill/librdkafka/dest/lib/cmake/RdKafka/FindLZ4.cmake
-- Installing: C:/Users/travis/build/edenhill/librdkafka/dest/lib/cmake/RdKafka/RdKafkaTargets.cmake
-- Installing: C:/Users/travis/build/edenhill/librdkafka/dest/lib/cmake/RdKafka/RdKafkaTargets-noconfig.cmake
-- Installing: C:/Users/travis/build/edenhill/librdkafka/dest/share/licenses/librdkafka/LICENSES.txt
-- Installing: C:/Users/travis/build/edenhill/librdkafka/dest/lib/pkgconfig/rdkafka-static.pc
-- Installing: C:/Users/travis/build/edenhill/librdkafka/dest/lib/librdkafka.a
-- Installing: C:/Users/travis/build/edenhill/librdkafka/dest/include/librdkafka/rdkafka.h
-- Installing: C:/Users/travis/build/edenhill/librdkafka/dest/include/librdkafka/rdkafka_mock.h
-- Installing: C:/Users/travis/build/edenhill/librdkafka/dest/lib/pkgconfig/rdkafka++-static.pc
-- Installing: C:/Users/travis/build/edenhill/librdkafka/dest/lib/librdkafka++.a
-- Installing: C:/Users/travis/build/edenhill/librdkafka/dest/include/librdkafka/rdkafkacpp.h
~/build/edenhill/librdkafka/mergescratch ~/build/edenhill/librdkafka
~/build/edenhill/librdkafka


Tested with mock up token provider.

[0126_oauthbearer_oidc       /  7.192s] 0126_oauthbearer_oidc: duration 7191.931ms
[0126_oauthbearer_oidc       /  7.192s] ================= Test 0126_oauthbearer_oidc PASSED =================
[<MAIN>                      /  8.018s] ALL-TESTS: duration 8018.276ms
TEST 20211115233334 (bare, scenario default) SUMMARY
#==================================================================#
| <MAIN>                                   |     PASSED |   8.019s |
| 0126_oauthbearer_oidc                    |     PASSED |   7.192s |
#==================================================================#
[<MAIN>                      /  8.021s] 0 thread(s) in use by librdkafka
[<MAIN>                      /  8.021s] 
============== ALL TESTS PASSED ==============
###
###  ./test-runner in bare mode PASSED! ###
###


valgrind: TESTS=0126 ./run-test.sh valgrind --exit-on-first-error=no
Saw two leaks, but I don't think they are related to my change.
(1) related to rd_http_req_destroy(&hreq)

==116526==     in use at exit: 80 bytes in 1 blocks
==116526==   total heap usage: 6,618 allocs, 6,617 frees, 433,555 bytes allocated
==116526== 
==116526== 80 bytes in 1 blocks are definitely lost in loss record 1 of 1
==116526==    at 0x483B7F3: malloc (in /usr/lib/x86_64-linux-gnu/valgrind/vgpreload_memcheck-amd64-linux.so)
==116526==    by 0x4A051CC: rd_malloc (rd.h:131)
==116526==    by 0x4A05F26: rd_buf_new (rdbuf.c:380)
==116526==    by 0x4A622C9: rd_http_req_init (rdhttp.c:139)
==116526==    by 0x4A65468: ut_sasl_oauthbearer_oidc_should_succeed (rdkafka_sasl_oauthbearer_oidc.c:410)
==116526==    by 0x4A6598B: unittest_sasl_oauthbearer_oidc (rdkafka_sasl_oauthbearer_oidc.c:504)
==116526==    by 0x4A116FF: rd_unittest (rdunittest.c:497)
==116526==    by 0x48DEFCC: rd_kafka_unittest (rdkafka.c:4860)
==116526==    by 0x1A5A3B: main_0000_unittests (0000-unittests.c:68)
==116526==    by 0x2884B7: run_test0 (test.c:1103)
==116526==    by 0x288ADD: run_test_from_thread (test.c:1167)
==116526==    by 0x4B686D9: start_thread (pthread_create.c:474)
==116526== 
==116526== 

Saw the same error for rdhttp.c.
==110197== 80 bytes in 1 blocks are definitely lost in loss record 1 of 2
==110197==    at 0x483B7F3: malloc (in /usr/lib/x86_64-linux-gnu/valgrind/vgpreload_memcheck-amd64-linux.so)
==110197==    by 0x4A051CC: rd_malloc (rd.h:131)
==110197==    by 0x4A05F26: rd_buf_new (rdbuf.c:380)
==110197==    by 0x4A622EF: rd_http_req_init (rdhttp.c:139)
==110197==    by 0x4A62B9E: rd_http_get_json (rdhttp.c:388)
==110197==    by 0x4A63009: unittest_http (rdhttp.c:475)
==110197==    by 0x4A11725: rd_unittest (rdunittest.c:497)
==110197==    by 0x48DEFCC: rd_kafka_unittest (rdkafka.c:4860)
==110197==    by 0x1A5A3B: main_0000_unittests (0000-unittests.c:68)
==110197==    by 0x2884B7: run_test0 (test.c:1103)
==110197==    by 0x288ADD: run_test_from_thread (test.c:1167)
==110197==    by 0x4B686D9: start_thread (pthread_create.c:474)

(2) Related to test_create_topic()
HEAP SUMMARY:
==121240==     in use at exit: 5,032 bytes in 70 blocks
==121240==   total heap usage: 6,830 allocs, 6,760 frees, 1,155,127 bytes allocated
==121240== 
==121240== 8 bytes in 1 blocks are indirectly lost in loss record 1 of 13
==121240==    at 0x483DD99: calloc (in /usr/lib/x86_64-linux-gnu/valgrind/vgpreload_memcheck-amd64-linux.so)
==121240==    by 0x4999A2B: rd_calloc (rd.h:125)
==121240==    by 0x4999E88: rd_list_prealloc_elems (rdlist.c:108)
==121240==    by 0x4999D88: rd_list_init_copy (rdlist.c:69)
==121240==    by 0x49D3DB6: rd_kafka_NewTopic_copy (rdkafka_admin.c:1479)
==121240==    by 0x49D68D1: rd_kafka_CreateTopics (rdkafka_admin.c:1739)
==121240==    by 0x299822: test_admin_create_topic (test.c:4585)
==121240==    by 0x299C99: test_create_topic (test.c:4653)
==121240==    by 0x1B0B60: do_test_produce_consumer_with_OIDC (0126-oauthbearer_oidc.c:66)
==121240==    by 0x1B1876: main_0126_oauthbearer_oidc (0126-oauthbearer_oidc.c:206)
==121240==    by 0x2884C3: run_test0 (test.c:1103)
==121240==    by 0x288AE9: run_test_from_thread (test.c:1167)
==121240== 
==121240== 
==121240== Exit program on first error (--exit-on-first-error=yes)


Saw the same error for 0090-idempotence.c
HEAP SUMMARY:
==120515==     in use at exit: 340 bytes in 8 blocks
==120515==   total heap usage: 6,359 allocs, 6,351 frees, 2,160,748 bytes allocated
==120515== 
==120515== 8 bytes in 1 blocks are indirectly lost in loss record 1 of 7
==120515==    at 0x483DD99: calloc (in /usr/lib/x86_64-linux-gnu/valgrind/vgpreload_memcheck-amd64-linux.so)
==120515==    by 0x4999A2B: rd_calloc (rd.h:125)
==120515==    by 0x4999E88: rd_list_prealloc_elems (rdlist.c:108)
==120515==    by 0x4999D88: rd_list_init_copy (rdlist.c:69)
==120515==    by 0x49D3DB6: rd_kafka_NewTopic_copy (rdkafka_admin.c:1479)
==120515==    by 0x49D68D1: rd_kafka_CreateTopics (rdkafka_admin.c:1739)
==120515==    by 0x299822: test_admin_create_topic (test.c:4585)
==120515==    by 0x299C99: test_create_topic (test.c:4653)
==120515==    by 0x176AB0: do_test_implicit_ack (0090-idempotence.c:133)
==120515==    by 0x177078: main_0090_idempotence (0090-idempotence.c:165)
==120515==    by 0x2884C3: run_test0 (test.c:1103)
==120515==    by 0x288AE9: run_test_from_thread (test.c:1167)
==120515== 
==120515== 


Since the above errors are not related to this PR, I will dig more and send a new PR to fix.

Tested with manually set the token urls:

VALID_OIDC_URL=http://localhost:8080/retrieve INVALID_OIDC_URL=http://localhost:8080/retrieve/badformat EXPIRED_TOKEN_OIDC_URL=http://localhost:8080/retrieve/expire TESTS=0126 make

[0126_oauthbearer_oidc       / 29.667s] ================= Test 0126_oauthbearer_oidc PASSED =================
[<MAIN>                      / 30.100s] ALL-TESTS: duration 30099.377ms
TEST 20211121131641 (bare, scenario default) SUMMARY
#==================================================================#
| <MAIN>                                   |     PASSED |  30.100s |
| 0126_oauthbearer_oidc                    |     PASSED |  29.667s |
#==================================================================#
[<MAIN>                      / 30.101s] 0 thread(s) in use by librdkafka
[<MAIN>                      / 30.101s] 
============== ALL TESTS PASSED ==============
###
###  ./test-runner in bare mode PASSED! ###
###",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,3565,2021-09-30T08:40:47Z,2021-09-30T12:40:56Z,2021-09-30T12:40:56Z,MERGED,True,181,47,10,https://github.com/edenhill,Release fixes for 1.8.2: build OpenSSL 1.1.1 on OSX,11,[],https://github.com/edenhill/librdkafka/pull/3565,https://github.com/edenhill,1,https://github.com/edenhill/librdkafka/pull/3565,,,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,3567,2021-10-01T13:34:48Z,2021-10-01T20:26:31Z,2021-10-01T20:26:34Z,MERGED,True,32,5,4,https://github.com/edenhill,Fix ssl.ca.location on OSX and make openssl source downloads more robust,2,[],https://github.com/edenhill/librdkafka/pull/3567,https://github.com/edenhill,1,https://github.com/edenhill/librdkafka/pull/3567,,,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,3567,2021-10-01T13:34:48Z,2021-10-01T20:26:31Z,2021-10-01T20:26:34Z,MERGED,True,32,5,4,https://github.com/edenhill,Fix ssl.ca.location on OSX and make openssl source downloads more robust,2,[],https://github.com/edenhill/librdkafka/pull/3567,https://github.com/edenhill,2,https://github.com/edenhill/librdkafka/pull/3567#issuecomment-932240913,,This is for the 1.8.2 release,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,3570,2021-10-05T10:08:21Z,2021-10-05T17:25:18Z,2021-10-05T17:25:18Z,MERGED,True,5,2,2,https://github.com/edenhill,Travis: bump Linux base builder from trusty to xenial to circumvent ISRG cert expiry,1,[],https://github.com/edenhill/librdkafka/pull/3570,https://github.com/edenhill,1,https://github.com/edenhill/librdkafka/pull/3570,.. which causes older versions of OpenSSL+curl to fail to download OpenSSL..,.. which causes older versions of OpenSSL+curl to fail to download OpenSSL..,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,3572,2021-10-05T20:13:23Z,2021-10-06T19:12:58Z,2021-10-06T19:13:01Z,MERGED,True,603,119,27,https://github.com/edenhill,Support for triggering SASL refresh callbacks on background thread,4,[],https://github.com/edenhill/librdkafka/pull/3572,https://github.com/edenhill,1,https://github.com/edenhill/librdkafka/pull/3572,"This PR solves two problems:

Jing will need it for the internal OIDC refresh callback so it is triggered automatically on the background thread.
Bindings, and advanced applications that provide their own refresh callbacks, no longer need to call poll() (et.al) prior to performing any other broker-needed APIs. Instead they need to set enable.sasl.callback.queue=true and call rd_kafka_sasl_background_callbacks_enable(). This may seem a bit complex and it would be much nicer if we could sort this out from rd_kafka_new(), but we need to guarantee that the background callback is triggered after rd_kafka_new() returns and the high-level client bindings have set up whatever high-level client instance objects they need. Failure to do so would trigger a refresh callback possibly referencing an object, or Null, that has not yet been returned to the user/app. So.. this extra call is needed to make sure that doesn't happen. And yes, that is really needed.

Any high-level binding that exposes a refresh callback API to the user needs to set enable.sasl.callback.queue=true and call rd_kafka_sasl_background_callbacks_enable() after rd_kafka_new() has returned.","This PR solves two problems:

Jing will need it for the internal OIDC refresh callback so it is triggered automatically on the background thread.
Bindings, and advanced applications that provide their own refresh callbacks, no longer need to call poll() (et.al) prior to performing any other broker-needed APIs. Instead they need to set enable.sasl.callback.queue=true and call rd_kafka_sasl_background_callbacks_enable(). This may seem a bit complex and it would be much nicer if we could sort this out from rd_kafka_new(), but we need to guarantee that the background callback is triggered after rd_kafka_new() returns and the high-level client bindings have set up whatever high-level client instance objects they need. Failure to do so would trigger a refresh callback possibly referencing an object, or Null, that has not yet been returned to the user/app. So.. this extra call is needed to make sure that doesn't happen. And yes, that is really needed.

Any high-level binding that exposes a refresh callback API to the user needs to set enable.sasl.callback.queue=true and call rd_kafka_sasl_background_callbacks_enable() after rd_kafka_new() has returned.",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,3573,2021-10-06T10:14:29Z,2021-10-06T18:37:55Z,2021-10-06T18:37:58Z,MERGED,True,119,34,4,https://github.com/edenhill,AddOffsetsToTxn Refresh errors did not trigger coord refresh (#3571),2,[],https://github.com/edenhill/librdkafka/pull/3573,https://github.com/edenhill,1,https://github.com/edenhill/librdkafka/pull/3573,,,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,3573,2021-10-06T10:14:29Z,2021-10-06T18:37:55Z,2021-10-06T18:37:58Z,MERGED,True,119,34,4,https://github.com/edenhill,AddOffsetsToTxn Refresh errors did not trigger coord refresh (#3571),2,[],https://github.com/edenhill/librdkafka/pull/3573,https://github.com/jliunyu,2,https://github.com/edenhill/librdkafka/pull/3573#issuecomment-936727812,,"Looks the build error is not related to this PR, but we might need to restart the CI job before merging:)
0.01s$ ccache -s || echo ""CCache is not available.""
install.2
0.00s$ rm -rf artifacts dest
install.3
0.00s$ mkdir dest artifacts
install.4
15.32s$ if [[ $TRAVIS_OS_NAME == ""linux"" ]]; then sudo apt update || true; fi
install.5
0.00s$ if [[ $TRAVIS_DIST == ""trusty"" || $TRAVIS_DIST == ""xenial"" ]]; then sudo apt-get install -y libssl1.0.0 libssl-dev ; fi
install.6
3.61s$ if [[ $TRAVIS_DIST == ""bionic"" || $TRAVIS_DIST == ""focal"" ]]; then sudo apt-get install -y libssl1.1 libssl-dev ; fi
10.25s$ if [[ -n $DOCKER_PASSWORD && $TRAVIS_OS_NAME == ""linux"" ]]; then echo ""$DOCKER_PASSWORD"" | docker login -u ""$DOCKER_USERNAME"" --password-stdin ; fi
Error response from daemon: Get https://registry-1.docker.io/v2/: net/http: TLS handshake timeout
The command ""if [[ -n $DOCKER_PASSWORD && $TRAVIS_OS_NAME == ""linux"" ]]; then echo ""$DOCKER_PASSWORD"" | docker login -u ""$DOCKER_USERNAME"" --password-stdin ; fi"" failed and exited with 1 during .
Your build has been stopped.",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,3577,2021-10-08T13:01:34Z,2021-10-11T20:15:44Z,2021-10-11T20:15:47Z,MERGED,True,174,13,7,https://github.com/edenhill,Abort transaction on OUT_OF_ORDER_SEQ (and other epoch bumps),2,[],https://github.com/edenhill/librdkafka/pull/3577,https://github.com/edenhill,1,https://github.com/edenhill/librdkafka/pull/3577,,,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,3582,2021-10-16T17:39:19Z,2021-11-03T11:13:12Z,2021-11-04T02:30:27Z,MERGED,True,1,0,1,https://github.com/benesch,Fix memory leak in admin requests,1,[],https://github.com/edenhill/librdkafka/pull/3582,https://github.com/benesch,1,https://github.com/edenhill/librdkafka/pull/3582,"Fix a memory leak introduces in ca1b30e in which the arguments to an
admin request were not being freed. Discovered by the test suite for
rust-rdkafka 0.","Fix a memory leak introduces in ca1b30e in which the arguments to an
admin request were not being freed. Discovered by the test suite for
rust-rdkafka 0.",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,3582,2021-10-16T17:39:19Z,2021-11-03T11:13:12Z,2021-11-04T02:30:27Z,MERGED,True,1,0,1,https://github.com/benesch,Fix memory leak in admin requests,1,[],https://github.com/edenhill/librdkafka/pull/3582,https://github.com/edenhill,2,https://github.com/edenhill/librdkafka/pull/3582#issuecomment-958932407,"Fix a memory leak introduces in ca1b30e in which the arguments to an
admin request were not being freed. Discovered by the test suite for
rust-rdkafka 0.",Thanks @benesch !,True,{'THUMBS_UP': ['https://github.com/benesch']}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,3586,2021-10-19T13:16:47Z,2021-10-19T13:17:10Z,2021-10-19T13:17:10Z,CLOSED,False,2602,51,27,https://github.com/garrett528,Garrett528/debug ubuntu20,10,[],https://github.com/edenhill/librdkafka/pull/3586,https://github.com/garrett528,1,https://github.com/edenhill/librdkafka/pull/3586,,,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,3587,2021-10-20T07:51:06Z,2021-10-25T16:14:54Z,2021-10-25T16:14:56Z,MERGED,True,6,5,1,https://github.com/edenhill,Update list of supported KIPs,1,[],https://github.com/edenhill/librdkafka/pull/3587,https://github.com/edenhill,1,https://github.com/edenhill/librdkafka/pull/3587,,,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,3591,2021-10-25T08:51:10Z,,2022-01-20T16:44:35Z,OPEN,False,1017,120,16,https://github.com/edenhill,[DO NOT MERGE] Transactional fix backports to v1.6.x,10,[],https://github.com/edenhill/librdkafka/pull/3591,https://github.com/edenhill,1,https://github.com/edenhill/librdkafka/pull/3591,"Do not merge this, since it is already on the correct branch (1.6.x).","Do not merge this, since it is already on the correct branch (1.6.x).",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,3592,2021-10-27T20:19:08Z,2021-10-27T20:19:19Z,2021-10-27T20:20:57Z,CLOSED,False,29,1,5,https://github.com/emelyanovtv,cgrp: expose rebalancing true/false from API.,1,[],https://github.com/edenhill/librdkafka/pull/3592,https://github.com/emelyanovtv,1,https://github.com/edenhill/librdkafka/pull/3592,Both C as well as C++ API now offer this ability.,Both C as well as C++ API now offer this ability.,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,3593,2021-10-28T06:47:44Z,2022-04-07T08:01:33Z,2022-04-07T08:01:33Z,CLOSED,False,29,1,5,https://github.com/poolik,cgrp: expose rebalancing true/false from API.,1,[],https://github.com/edenhill/librdkafka/pull/3593,https://github.com/poolik,1,https://github.com/edenhill/librdkafka/pull/3593,"Attempt to expose rebalancing information through public APIs.
Goal is to be able to suspend user level background committing as it's needed during incremental cooperative rebalancing. @mhowlett made the same comment at the end of this PR:
#3326
I'd update changelog and test cases etc once someone has confirmed that this general approach is fine.","Attempt to expose rebalancing information through public APIs.
Goal is to be able to suspend user level background committing as it's needed during incremental cooperative rebalancing. @mhowlett made the same comment at the end of this PR:
#3326
I'd update changelog and test cases etc once someone has confirmed that this general approach is fine.",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,3593,2021-10-28T06:47:44Z,2022-04-07T08:01:33Z,2022-04-07T08:01:33Z,CLOSED,False,29,1,5,https://github.com/poolik,cgrp: expose rebalancing true/false from API.,1,[],https://github.com/edenhill/librdkafka/pull/3593,https://github.com/poolik,2,https://github.com/edenhill/librdkafka/pull/3593#issuecomment-953558472,"Attempt to expose rebalancing information through public APIs.
Goal is to be able to suspend user level background committing as it's needed during incremental cooperative rebalancing. @mhowlett made the same comment at the end of this PR:
#3326
I'd update changelog and test cases etc once someone has confirmed that this general approach is fine.","AppVeyor build failed with (for x64, passed for x86):

fatal: unable to access 'https://github.com/edenhill/librdkafka.git/': Could not resolve host: github.com",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,3595,2021-10-28T10:21:43Z,2021-10-28T17:40:30Z,2021-10-28T17:40:32Z,MERGED,True,4,3,2,https://github.com/edenhill,Direct questions to the github discussions forum to keep issue load down,1,[],https://github.com/edenhill/librdkafka/pull/3595,https://github.com/edenhill,1,https://github.com/edenhill/librdkafka/pull/3595,,,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,3596,2021-10-28T11:38:30Z,2021-10-28T17:40:59Z,2021-10-28T17:40:59Z,MERGED,True,319,4,9,https://github.com/edenhill,Code style checker and formatter (clang-format),3,[],https://github.com/edenhill/librdkafka/pull/3596,https://github.com/edenhill,1,https://github.com/edenhill/librdkafka/pull/3596,Replace one nitpicker (me) with another nitpicker (clang-format).,Replace one nitpicker (me) with another nitpicker (clang-format).,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,3596,2021-10-28T11:38:30Z,2021-10-28T17:40:59Z,2021-10-28T17:40:59Z,MERGED,True,319,4,9,https://github.com/edenhill,Code style checker and formatter (clang-format),3,[],https://github.com/edenhill/librdkafka/pull/3596,https://github.com/edenhill,2,https://github.com/edenhill/librdkafka/pull/3596#issuecomment-954060956,Replace one nitpicker (me) with another nitpicker (clang-format).,Thanks for the quick reviews,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,3597,2021-10-28T15:08:09Z,,2022-04-26T07:59:43Z,OPEN,False,9,4,1,https://github.com/alpire,Fix out-of-bounds memory accesses in regex parsing,2,[],https://github.com/edenhill/librdkafka/pull/3597,https://github.com/alpire,1,https://github.com/edenhill/librdkafka/pull/3597,"This PR fixes out-of-bounds memory accesses in regex parsing found by OSS-Fuzz.
Out-of-bounds write
The implicit casting from integer to unsigned char of min & max in parserep allowed a maliciously crafted input to bypass the max < min check in parserep. This would later lead to a mismatch between the result of count and the number of emit calls, resulting in an heap out-of-bounds write in emit.
Example input: 3{3,65}3{55555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555553,65}35{61,}
Fixes https://bugs.chromium.org/p/oss-fuzz/issues/detail?id=27631
Fixes https://bugs.chromium.org/p/oss-fuzz/issues/detail?id=27735
Out-of-bounds read in parseatom
Example input: ((((((((((((((((\17+
Fixes https://bugs.chromium.org/p/oss-fuzz/issues/detail?id=28561","This PR fixes out-of-bounds memory accesses in regex parsing found by OSS-Fuzz.
Out-of-bounds write
The implicit casting from integer to unsigned char of min & max in parserep allowed a maliciously crafted input to bypass the max < min check in parserep. This would later lead to a mismatch between the result of count and the number of emit calls, resulting in an heap out-of-bounds write in emit.
Example input: 3{3,65}3{55555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555553,65}35{61,}
Fixes https://bugs.chromium.org/p/oss-fuzz/issues/detail?id=27631
Fixes https://bugs.chromium.org/p/oss-fuzz/issues/detail?id=27735
Out-of-bounds read in parseatom
Example input: ((((((((((((((((\17+
Fixes https://bugs.chromium.org/p/oss-fuzz/issues/detail?id=28561",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,3598,2021-10-28T17:55:35Z,2021-10-28T21:06:39Z,2021-10-28T21:06:42Z,MERGED,True,57959,58596,354,https://github.com/edenhill,Style fix all the codes,2,[],https://github.com/edenhill/librdkafka/pull/3598,https://github.com/edenhill,1,https://github.com/edenhill/librdkafka/pull/3598,"These are all style changes performed by clang-format, autopep8, and a smaller set of manual style fixes on the Python support scripts.","These are all style changes performed by clang-format, autopep8, and a smaller set of manual style fixes on the Python support scripts.",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,3601,2021-11-02T08:48:16Z,2021-11-02T09:27:56Z,2021-11-02T09:27:59Z,CLOSED,False,4,0,2,https://github.com/edenhill,Visual Studio project: use VCPKG manifests,1,[],https://github.com/edenhill/librdkafka/pull/3601,https://github.com/edenhill,1,https://github.com/edenhill/librdkafka/pull/3601,,,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,3602,2021-11-02T10:24:51Z,2021-11-02T13:25:51Z,2021-11-02T13:25:56Z,MERGED,True,70,10,3,https://github.com/edenhill,Fix OpenSSL_Applink crash on Windows (#3554),1,[],https://github.com/edenhill/librdkafka/pull/3602,https://github.com/edenhill,1,https://github.com/edenhill/librdkafka/pull/3602,,,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,3604,2021-11-02T15:48:17Z,2021-11-02T15:48:21Z,2021-11-02T15:49:44Z,CLOSED,False,2571,36,26,https://github.com/garrett528,merge edenhill,11,[],https://github.com/edenhill/librdkafka/pull/3604,https://github.com/garrett528,1,https://github.com/edenhill/librdkafka/pull/3604,,,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,3605,2021-11-03T12:49:00Z,2021-11-04T21:46:42Z,2021-11-04T21:46:42Z,CLOSED,False,7,93,3,https://github.com/neptoess,[TEST] See If We Can Make Travis Work,2,[],https://github.com/edenhill/librdkafka/pull/3605,https://github.com/neptoess,1,https://github.com/edenhill/librdkafka/pull/3605,,,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,3606,2021-11-03T13:08:37Z,2021-11-04T21:46:34Z,2021-11-04T21:46:34Z,CLOSED,False,7,94,3,https://github.com/neptoess,[TEST] Rebuild v1.8.2,3,[],https://github.com/edenhill/librdkafka/pull/3606,https://github.com/neptoess,1,https://github.com/edenhill/librdkafka/pull/3606,,,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,3607,2021-11-04T21:46:23Z,2021-11-05T08:17:13Z,2022-03-28T17:05:20Z,MERGED,True,7,3,3,https://github.com/neptoess,Fix MinGW Travis build issues,1,[],https://github.com/edenhill/librdkafka/pull/3607,https://github.com/neptoess,1,https://github.com/edenhill/librdkafka/pull/3607,Relative discussion in #3130,Relative discussion in #3130,True,{'HEART': ['https://github.com/edenhill']}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,3607,2021-11-04T21:46:23Z,2021-11-05T08:17:13Z,2022-03-28T17:05:20Z,MERGED,True,7,3,3,https://github.com/neptoess,Fix MinGW Travis build issues,1,[],https://github.com/edenhill/librdkafka/pull/3607,https://github.com/edenhill,2,https://github.com/edenhill/librdkafka/pull/3607#issuecomment-961703132,Relative discussion in #3130,"Perfect, thank you!",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,3607,2021-11-04T21:46:23Z,2021-11-05T08:17:13Z,2022-03-28T17:05:20Z,MERGED,True,7,3,3,https://github.com/neptoess,Fix MinGW Travis build issues,1,[],https://github.com/edenhill/librdkafka/pull/3607,https://github.com/edenhill,3,https://github.com/edenhill/librdkafka/pull/3607#issuecomment-1080564142,Relative discussion in #3130,"Hey @neptoess , pinging you here in lack of better communication alternatives:
The dynamic mingw build is failing (silently) because msys2 now uses gcc 11 which introduces a new _ZSt28__throw_bad_array_new_lengthv symbol,  but the libstdc++ on the system is compiled with an older gcc that  does not have this symbol, so trying to execute the test runner fails silently.
It is basically this issue:
stan-dev/pystan#294
But I'm not sure that solution (symlink/replace) is viable for our msys2 builder.
Do you have any ideas?
https://app.travis-ci.com/github/edenhill/librdkafka/jobs/564513967",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,3607,2021-11-04T21:46:23Z,2021-11-05T08:17:13Z,2022-03-28T17:05:20Z,MERGED,True,7,3,3,https://github.com/neptoess,Fix MinGW Travis build issues,1,[],https://github.com/edenhill/librdkafka/pull/3607,https://github.com/neptoess,4,https://github.com/edenhill/librdkafka/pull/3607#issuecomment-1080620796,Relative discussion in #3130,"Added you on LinkedIn for a communication alternative (can also provide email if you'd like).
Let me play around with the build a bit on my local machine and see if I can come up with anything.",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,3607,2021-11-04T21:46:23Z,2021-11-05T08:17:13Z,2022-03-28T17:05:20Z,MERGED,True,7,3,3,https://github.com/neptoess,Fix MinGW Travis build issues,1,[],https://github.com/edenhill/librdkafka/pull/3607,https://github.com/neptoess,5,https://github.com/edenhill/librdkafka/pull/3607#issuecomment-1080686973,Relative discussion in #3130,"v1.9.0-PRE11.build.log
Upgraded my msys2 packages and did the build. As the old joke goes, ""works on my machine"".
I also just realized that the build you linked was the dynamic build (pinging @ed-alertedh @edbordin since he was involved there), but the static build worked (https://app.travis-ci.com/github/edenhill/librdkafka/jobs/564513968). The build log you linked seems to cut off without even attempting to run the tests. Bizarre.
Adding a $msys2 pacman -Q to the end of travis-before-install.sh might be useful, so we can easily see exactly which packages, and which versions, are installed during the build.",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,3607,2021-11-04T21:46:23Z,2021-11-05T08:17:13Z,2022-03-28T17:05:20Z,MERGED,True,7,3,3,https://github.com/neptoess,Fix MinGW Travis build issues,1,[],https://github.com/edenhill/librdkafka/pull/3607,https://github.com/edenhill,6,https://github.com/edenhill/librdkafka/pull/3607#issuecomment-1080851608,Relative discussion in #3130,"The build log you linked seems to cut off without even attempting to run the tests. Bizarre.

Yeah, turns out linkage errors are not emitted to the shell, the process just exits with a 127 return code.
Running it with strace, locally, did however show the symbol issue.
Do you think it would be possible just to version pin the gcc toolchain to a 10 release?",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,3607,2021-11-04T21:46:23Z,2021-11-05T08:17:13Z,2022-03-28T17:05:20Z,MERGED,True,7,3,3,https://github.com/neptoess,Fix MinGW Travis build issues,1,[],https://github.com/edenhill/librdkafka/pull/3607,https://github.com/neptoess,7,https://github.com/edenhill/librdkafka/pull/3607#issuecomment-1080919604,Relative discussion in #3130,"Do you think it would be possible just to version pin the gcc toolchain to a 10 release?

That's kind of nasty (https://www.msys2.org/docs/package-management/#installing-a-specific-version-of-a-package-or-a-stand-alone-packages), but I'll play around with the dynamically linked build locally and see what I can figure out.",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,3622,2021-11-18T16:57:37Z,2022-01-20T16:29:41Z,2022-01-20T16:29:44Z,MERGED,True,9,3,2,https://github.com/edenhill,clusterid() would fail if there were no topics in metadata (#3620),1,[],https://github.com/edenhill/librdkafka/pull/3622,https://github.com/edenhill,1,https://github.com/edenhill/librdkafka/pull/3622,,,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,3625,2021-11-21T03:46:33Z,2022-04-26T15:39:21Z,2022-04-26T15:39:26Z,MERGED,True,10,0,2,https://github.com/aiquestion,Fix: fix callback missing when metadata request failed,3,[],https://github.com/edenhill/librdkafka/pull/3625,https://github.com/aiquestion,1,https://github.com/edenhill/librdkafka/pull/3625,"We experience this issue when main thread cgrp start to do a rebalance
and failed when parsing metadata, the cgrp callback will be lost
and result in consumer unassigned.
If rd_kafka_handle_Metadata failed and will not retry, need to enque
the callback with err.","We experience this issue when main thread cgrp start to do a rebalance
and failed when parsing metadata, the cgrp callback will be lost
and result in consumer unassigned.
If rd_kafka_handle_Metadata failed and will not retry, need to enque
the callback with err.",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,3625,2021-11-21T03:46:33Z,2022-04-26T15:39:21Z,2022-04-26T15:39:26Z,MERGED,True,10,0,2,https://github.com/aiquestion,Fix: fix callback missing when metadata request failed,3,[],https://github.com/edenhill/librdkafka/pull/3625,https://github.com/edenhill,2,https://github.com/edenhill/librdkafka/pull/3625#issuecomment-1109940851,"We experience this issue when main thread cgrp start to do a rebalance
and failed when parsing metadata, the cgrp callback will be lost
and result in consumer unassigned.
If rd_kafka_handle_Metadata failed and will not retry, need to enque
the callback with err.",Good find!,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,3625,2021-11-21T03:46:33Z,2022-04-26T15:39:21Z,2022-04-26T15:39:26Z,MERGED,True,10,0,2,https://github.com/aiquestion,Fix: fix callback missing when metadata request failed,3,[],https://github.com/edenhill/librdkafka/pull/3625,https://github.com/edenhill,3,https://github.com/edenhill/librdkafka/pull/3625#issuecomment-1109951568,"We experience this issue when main thread cgrp start to do a rebalance
and failed when parsing metadata, the cgrp callback will be lost
and result in consumer unassigned.
If rd_kafka_handle_Metadata failed and will not retry, need to enque
the callback with err.",Thank you!,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,3639,2021-11-30T16:37:18Z,2021-12-06T14:48:30Z,2021-12-06T14:48:33Z,MERGED,True,158,24,7,https://github.com/edenhill,"MsgSets with just aborted msgs raised a MSG_SIZE error, and fix backoffs",1,[],https://github.com/edenhill/librdkafka/pull/3639,https://github.com/edenhill,1,https://github.com/edenhill/librdkafka/pull/3639,"Fixes #2993.
This also removes fetch backoffs on underflows (truncated responses).","Fixes #2993.
This also removes fetch backoffs on underflows (truncated responses).",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,3642,2021-12-06T11:42:55Z,2021-12-06T12:02:11Z,2021-12-06T12:02:18Z,MERGED,True,1,1,1,https://github.com/mattclarke,rdkafkacpp.h: fixed typo in docs,1,[],https://github.com/edenhill/librdkafka/pull/3642,https://github.com/mattclarke,1,https://github.com/edenhill/librdkafka/pull/3642,Missing letter in Comsum(p)tion in rdkafkacpp.h docs,Missing letter in Comsum(p)tion in rdkafkacpp.h docs,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,3642,2021-12-06T11:42:55Z,2021-12-06T12:02:11Z,2021-12-06T12:02:18Z,MERGED,True,1,1,1,https://github.com/mattclarke,rdkafkacpp.h: fixed typo in docs,1,[],https://github.com/edenhill/librdkafka/pull/3642,https://github.com/edenhill,2,https://github.com/edenhill/librdkafka/pull/3642#issuecomment-986710577,Missing letter in Comsum(p)tion in rdkafkacpp.h docs,Thanks Matt!,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,3646,2021-12-14T05:31:17Z,2022-02-07T09:36:15Z,2022-02-07T09:36:25Z,MERGED,True,69,14,5,https://github.com/jliunyu,Integration test for OIDC,6,[],https://github.com/edenhill/librdkafka/pull/3646,https://github.com/jliunyu,1,https://github.com/edenhill/librdkafka/pull/3646,"Integration test for OAuth/OIDC with AK 3.1.0.
This one has dependency on edenhill/trivup#13, since the OauthbearerOIDCApp is not merged yet, so the build failure is expected.
Local test:
1)
./sasl_test.py 3.1.0
#### Version 3.1.0, suite OAuth/OIDC: STARTING
[2022-02-01 17:31:01.344881] KafkaBrokerApp-3: Failed to set RLIMIT_NOFILE(9223372036854775807,9223372036854775807): current limit exceeds maximum limit
[2022-02-01 17:31:01.351036] KafkaBrokerApp-4: Failed to set RLIMIT_NOFILE(9223372036854775807,9223372036854775807): current limit exceeds maximum limit
[2022-02-01 17:31:01.358083] KafkaBrokerApp-5: Failed to set RLIMIT_NOFILE(9223372036854775807,9223372036854775807): current limit exceeds maximum limit
# Connect to cluster with bootstrap.servers PLAINTEXT://localhost:52993,PLAINTEXT://localhost:52997,PLAINTEXT://localhost:53001
# librdkafka regression tests started, logs in /Users/jliu/Documents/Code/jliunyu/librdkafka/tests/tmp/LibrdkafkaTestCluster/43765461/LibrdkafkaTestApp/8ecae6be
#### Version 3.1.0, suite OAuth/OIDC: PASSED: All 1/1 tests passed as expected
#### Test output: /Users/jliu/Documents/Code/jliunyu/librdkafka/tests/tmp/LibrdkafkaTestCluster/43765461/LibrdkafkaTestApp/8ecae6be/stderr.log
#### Full test suite report (1 suite(s))
PASSED  OAuth/OIDC @ 3.1.0                                : All 1/1 tests passed as expected
#### 1 suites PASSED, 0 suites FAILED




./sasl_test.py
Version 3.1.0, suite SASL_SSL PLAIN: FAILED: 72/75 tests passed: expected all to pass
The 3 failed tests are not related, same failure without my change:
FAILED   --> 0052_msg_timestamps
FAILED   --> 0077_compaction
FAILED   --> 0113_cooperative_rebalance
AppVeyor build failure is download error which is not related.","Integration test for OAuth/OIDC with AK 3.1.0.
This one has dependency on edenhill/trivup#13, since the OauthbearerOIDCApp is not merged yet, so the build failure is expected.
Local test:
1)
./sasl_test.py 3.1.0
#### Version 3.1.0, suite OAuth/OIDC: STARTING
[2022-02-01 17:31:01.344881] KafkaBrokerApp-3: Failed to set RLIMIT_NOFILE(9223372036854775807,9223372036854775807): current limit exceeds maximum limit
[2022-02-01 17:31:01.351036] KafkaBrokerApp-4: Failed to set RLIMIT_NOFILE(9223372036854775807,9223372036854775807): current limit exceeds maximum limit
[2022-02-01 17:31:01.358083] KafkaBrokerApp-5: Failed to set RLIMIT_NOFILE(9223372036854775807,9223372036854775807): current limit exceeds maximum limit
# Connect to cluster with bootstrap.servers PLAINTEXT://localhost:52993,PLAINTEXT://localhost:52997,PLAINTEXT://localhost:53001
# librdkafka regression tests started, logs in /Users/jliu/Documents/Code/jliunyu/librdkafka/tests/tmp/LibrdkafkaTestCluster/43765461/LibrdkafkaTestApp/8ecae6be
#### Version 3.1.0, suite OAuth/OIDC: PASSED: All 1/1 tests passed as expected
#### Test output: /Users/jliu/Documents/Code/jliunyu/librdkafka/tests/tmp/LibrdkafkaTestCluster/43765461/LibrdkafkaTestApp/8ecae6be/stderr.log
#### Full test suite report (1 suite(s))
PASSED  OAuth/OIDC @ 3.1.0                                : All 1/1 tests passed as expected
#### 1 suites PASSED, 0 suites FAILED




./sasl_test.py
Version 3.1.0, suite SASL_SSL PLAIN: FAILED: 72/75 tests passed: expected all to pass
The 3 failed tests are not related, same failure without my change:
FAILED   --> 0052_msg_timestamps
FAILED   --> 0077_compaction
FAILED   --> 0113_cooperative_rebalance
AppVeyor build failure is download error which is not related.",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,3646,2021-12-14T05:31:17Z,2022-02-07T09:36:15Z,2022-02-07T09:36:25Z,MERGED,True,69,14,5,https://github.com/jliunyu,Integration test for OIDC,6,[],https://github.com/edenhill/librdkafka/pull/3646,https://github.com/edenhill,2,https://github.com/edenhill/librdkafka/pull/3646#issuecomment-1031257661,"Integration test for OAuth/OIDC with AK 3.1.0.
This one has dependency on edenhill/trivup#13, since the OauthbearerOIDCApp is not merged yet, so the build failure is expected.
Local test:
1)
./sasl_test.py 3.1.0
#### Version 3.1.0, suite OAuth/OIDC: STARTING
[2022-02-01 17:31:01.344881] KafkaBrokerApp-3: Failed to set RLIMIT_NOFILE(9223372036854775807,9223372036854775807): current limit exceeds maximum limit
[2022-02-01 17:31:01.351036] KafkaBrokerApp-4: Failed to set RLIMIT_NOFILE(9223372036854775807,9223372036854775807): current limit exceeds maximum limit
[2022-02-01 17:31:01.358083] KafkaBrokerApp-5: Failed to set RLIMIT_NOFILE(9223372036854775807,9223372036854775807): current limit exceeds maximum limit
# Connect to cluster with bootstrap.servers PLAINTEXT://localhost:52993,PLAINTEXT://localhost:52997,PLAINTEXT://localhost:53001
# librdkafka regression tests started, logs in /Users/jliu/Documents/Code/jliunyu/librdkafka/tests/tmp/LibrdkafkaTestCluster/43765461/LibrdkafkaTestApp/8ecae6be
#### Version 3.1.0, suite OAuth/OIDC: PASSED: All 1/1 tests passed as expected
#### Test output: /Users/jliu/Documents/Code/jliunyu/librdkafka/tests/tmp/LibrdkafkaTestCluster/43765461/LibrdkafkaTestApp/8ecae6be/stderr.log
#### Full test suite report (1 suite(s))
PASSED  OAuth/OIDC @ 3.1.0                                : All 1/1 tests passed as expected
#### 1 suites PASSED, 0 suites FAILED




./sasl_test.py
Version 3.1.0, suite SASL_SSL PLAIN: FAILED: 72/75 tests passed: expected all to pass
The 3 failed tests are not related, same failure without my change:
FAILED   --> 0052_msg_timestamps
FAILED   --> 0077_compaction
FAILED   --> 0113_cooperative_rebalance
AppVeyor build failure is download error which is not related.",Great stuff @jliunyu !,True,{'THUMBS_UP': ['https://github.com/jliunyu']}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,3665,2022-01-03T12:19:09Z,2022-01-05T08:57:19Z,2022-01-05T08:57:19Z,CLOSED,False,6,7,1,https://github.com/amitkumar50,Update INTRODUCTION.md,1,[],https://github.com/edenhill/librdkafka/pull/3665,https://github.com/amitkumar50,1,https://github.com/edenhill/librdkafka/pull/3665,,,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,3665,2022-01-03T12:19:09Z,2022-01-05T08:57:19Z,2022-01-05T08:57:19Z,CLOSED,False,6,7,1,https://github.com/amitkumar50,Update INTRODUCTION.md,1,[],https://github.com/edenhill/librdkafka/pull/3665,https://github.com/edenhill,2,https://github.com/edenhill/librdkafka/pull/3665#issuecomment-1005496127,,Thank you but I think the short intro blurb should be before the ToC so people understand what the ToC relates to.,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,3669,2022-01-07T14:14:27Z,2022-04-11T08:16:45Z,2022-04-11T08:16:48Z,MERGED,True,7,2,3,https://github.com/larry-cdn77,Handle absent log.queue gracefully in rd_kafka_set_log_queue,1,[],https://github.com/edenhill/librdkafka/pull/3669,https://github.com/larry-cdn77,1,https://github.com/edenhill/librdkafka/pull/3669,To address issue #3664,To address issue #3664,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,3669,2022-01-07T14:14:27Z,2022-04-11T08:16:45Z,2022-04-11T08:16:48Z,MERGED,True,7,2,3,https://github.com/larry-cdn77,Handle absent log.queue gracefully in rd_kafka_set_log_queue,1,[],https://github.com/edenhill/librdkafka/pull/3669,https://github.com/edenhill,2,https://github.com/edenhill/librdkafka/pull/3669#issuecomment-1094691548,To address issue #3664,Thank you!,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,3672,2022-01-12T16:24:09Z,2022-01-19T10:22:06Z,2022-01-19T10:22:09Z,MERGED,True,567,68,12,https://github.com/edenhill,Save idempotent producer partition state on partition removal,4,[],https://github.com/edenhill/librdkafka/pull/3672,https://github.com/edenhill,1,https://github.com/edenhill/librdkafka/pull/3672,..in case it comes back.,..in case it comes back.,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,3676,2022-01-18T08:48:47Z,2022-01-20T16:44:31Z,2022-01-20T16:44:37Z,MERGED,True,718,85,11,https://github.com/edenhill,keepmsgid backport to 1.6.x,4,[],https://github.com/edenhill/librdkafka/pull/3676,https://github.com/edenhill,1,https://github.com/edenhill/librdkafka/pull/3676,,,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,3676,2022-01-18T08:48:47Z,2022-01-20T16:44:31Z,2022-01-20T16:44:37Z,MERGED,True,718,85,11,https://github.com/edenhill,keepmsgid backport to 1.6.x,4,[],https://github.com/edenhill/librdkafka/pull/3676,https://github.com/edenhill,2,https://github.com/edenhill/librdkafka/pull/3676#issuecomment-1016291305,,"This is the same fix as #3672 , but adjusted for the 1.6.x branch.",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,3677,2022-01-18T15:08:21Z,2022-04-07T13:41:21Z,2022-04-07T13:41:21Z,CLOSED,False,202,391,14,https://github.com/edenhill,Circleci project setup,6,[],https://github.com/edenhill/librdkafka/pull/3677,https://github.com/edenhill,1,https://github.com/edenhill/librdkafka/pull/3677,,,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,3678,2022-01-19T10:05:16Z,2022-01-20T16:09:43Z,2022-01-20T16:09:46Z,MERGED,True,20,11,5,https://github.com/edenhill,Fix rkbuf_rkb assert on malformed JoinGroupResponse.metadata,2,[],https://github.com/edenhill/librdkafka/pull/3678,https://github.com/edenhill,1,https://github.com/edenhill/librdkafka/pull/3678,confluentinc/confluent-kafka-go#650,confluentinc/confluent-kafka-go#650,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,3682,2022-01-20T04:17:12Z,2022-01-24T07:43:23Z,2022-01-24T13:13:58Z,MERGED,True,11,14,3,https://github.com/lpsinger,sasl.oauthbearer.extensions should be optional,1,[],https://github.com/edenhill/librdkafka/pull/3682,https://github.com/lpsinger,1,https://github.com/edenhill/librdkafka/pull/3682,Fixes confluentinc/confluent-kafka-python#1269.,Fixes confluentinc/confluent-kafka-python#1269.,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,3682,2022-01-20T04:17:12Z,2022-01-24T07:43:23Z,2022-01-24T13:13:58Z,MERGED,True,11,14,3,https://github.com/lpsinger,sasl.oauthbearer.extensions should be optional,1,[],https://github.com/edenhill/librdkafka/pull/3682,https://github.com/edenhill,2,https://github.com/edenhill/librdkafka/pull/3682#issuecomment-1019804199,Fixes confluentinc/confluent-kafka-python#1269.,Thank you @lpsinger !,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,3682,2022-01-20T04:17:12Z,2022-01-24T07:43:23Z,2022-01-24T13:13:58Z,MERGED,True,11,14,3,https://github.com/lpsinger,sasl.oauthbearer.extensions should be optional,1,[],https://github.com/edenhill/librdkafka/pull/3682,https://github.com/lpsinger,3,https://github.com/edenhill/librdkafka/pull/3682#issuecomment-1020087574,Fixes confluentinc/confluent-kafka-python#1269.,Thank you!,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,3697,2022-02-01T11:04:36Z,2022-02-01T21:25:41Z,2022-02-01T21:25:43Z,MERGED,True,6,3,2,https://github.com/edenhill,Added AK 3.1.0 to test versions,1,[],https://github.com/edenhill/librdkafka/pull/3697,https://github.com/edenhill,1,https://github.com/edenhill/librdkafka/pull/3697,,,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,3698,2022-02-01T11:05:05Z,2022-02-01T21:26:22Z,2022-02-01T21:26:22Z,MERGED,True,7,3,3,https://github.com/edenhill,1.9.0 updates,2,[],https://github.com/edenhill/librdkafka/pull/3698,https://github.com/edenhill,1,https://github.com/edenhill/librdkafka/pull/3698,,,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,3700,2022-02-01T12:59:54Z,2022-02-24T10:49:25Z,2022-02-24T10:49:29Z,MERGED,True,31,14,1,https://github.com/ladislavmacoun,Replace deprecated zookeeper flag with bootstrap,4,[],https://github.com/edenhill/librdkafka/pull/3700,https://github.com/ladislavmacoun,1,https://github.com/edenhill/librdkafka/pull/3700,"Fixes: #3699
Signed-off-by: Ladislav Macoun ladislavmacoun@gmail.com","Fixes: #3699
Signed-off-by: Ladislav Macoun ladislavmacoun@gmail.com",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,3700,2022-02-01T12:59:54Z,2022-02-24T10:49:25Z,2022-02-24T10:49:29Z,MERGED,True,31,14,1,https://github.com/ladislavmacoun,Replace deprecated zookeeper flag with bootstrap,4,[],https://github.com/edenhill/librdkafka/pull/3700,https://github.com/edenhill,2,https://github.com/edenhill/librdkafka/pull/3700#issuecomment-1049729517,"Fixes: #3699
Signed-off-by: Ladislav Macoun ladislavmacoun@gmail.com",Thank you!,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,3704,2022-02-02T19:29:40Z,2022-02-03T07:38:10Z,2022-02-03T14:31:53Z,MERGED,True,16,14,3,https://github.com/lpsinger,sasl.oauthbearer.scope should be optional + fix buffer overflow when generating request body for auth token request,1,[],https://github.com/edenhill/librdkafka/pull/3704,https://github.com/lpsinger,1,https://github.com/edenhill/librdkafka/pull/3704,"According to the section 4.4.2 of RFC 6749, the scope is optional in the access token request in client credentials flow.
And indeed, for OIDC providers that I find in the wild such as Amazon Cognito, the scope is optional. If the scope is omitted from the request, then the returned access token will contain any and all scope(s) that are configured for the client.
See https://datatracker.ietf.org/doc/html/rfc6749#section-4.4.2","According to the section 4.4.2 of RFC 6749, the scope is optional in the access token request in client credentials flow.
And indeed, for OIDC providers that I find in the wild such as Amazon Cognito, the scope is optional. If the scope is omitted from the request, then the returned access token will contain any and all scope(s) that are configured for the client.
See https://datatracker.ietf.org/doc/html/rfc6749#section-4.4.2",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,3704,2022-02-02T19:29:40Z,2022-02-03T07:38:10Z,2022-02-03T14:31:53Z,MERGED,True,16,14,3,https://github.com/lpsinger,sasl.oauthbearer.scope should be optional + fix buffer overflow when generating request body for auth token request,1,[],https://github.com/edenhill/librdkafka/pull/3704,https://github.com/lpsinger,2,https://github.com/edenhill/librdkafka/pull/3704#issuecomment-1028382767,"According to the section 4.4.2 of RFC 6749, the scope is optional in the access token request in client credentials flow.
And indeed, for OIDC providers that I find in the wild such as Amazon Cognito, the scope is optional. If the scope is omitted from the request, then the returned access token will contain any and all scope(s) that are configured for the client.
See https://datatracker.ietf.org/doc/html/rfc6749#section-4.4.2","Note that in this PR, I also fixed a subtle buffer overflow / off-by-1 error: post_fields_size should equal strlen(post_fields), not strlen(post_fields) + 1. See example code at https://curl.se/libcurl/c/CURLOPT_POSTFIELDSIZE.html.",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,3704,2022-02-02T19:29:40Z,2022-02-03T07:38:10Z,2022-02-03T14:31:53Z,MERGED,True,16,14,3,https://github.com/lpsinger,sasl.oauthbearer.scope should be optional + fix buffer overflow when generating request body for auth token request,1,[],https://github.com/edenhill/librdkafka/pull/3704,https://github.com/edenhill,3,https://github.com/edenhill/librdkafka/pull/3704#issuecomment-1028686940,"According to the section 4.4.2 of RFC 6749, the scope is optional in the access token request in client credentials flow.
And indeed, for OIDC providers that I find in the wild such as Amazon Cognito, the scope is optional. If the scope is omitted from the request, then the returned access token will contain any and all scope(s) that are configured for the client.
See https://datatracker.ietf.org/doc/html/rfc6749#section-4.4.2",Thank you ,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,3708,2022-02-03T12:08:31Z,2022-02-04T07:05:42Z,2022-02-04T07:05:44Z,MERGED,True,368,17,8,https://github.com/edenhill,Fix hang in list_groups() when cluster is unavailable (#3705),2,[],https://github.com/edenhill/librdkafka/pull/3708,https://github.com/edenhill,1,https://github.com/edenhill/librdkafka/pull/3708,,,True,"{'THUMBS_UP': ['https://github.com/olegrok', 'https://github.com/hackallcode']}"
edenhill/librdkafka,https://github.com/edenhill/librdkafka,3713,2022-02-07T16:54:59Z,2022-02-07T19:40:24Z,2022-02-07T19:40:27Z,MERGED,True,75,50,7,https://github.com/edenhill,Additional OIDC stuff,6,[],https://github.com/edenhill/librdkafka/pull/3713,https://github.com/edenhill,1,https://github.com/edenhill/librdkafka/pull/3713,,,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,3717,2022-02-10T14:18:38Z,2022-02-10T19:00:14Z,2022-02-10T19:00:16Z,MERGED,True,94,9,4,https://github.com/edenhill,Fix linger.ms/message.timeout.ms config checking,2,[],https://github.com/edenhill/librdkafka/pull/3717,https://github.com/edenhill,1,https://github.com/edenhill/librdkafka/pull/3717,,,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,3721,2022-02-10T19:11:08Z,,2022-05-13T16:06:16Z,OPEN,False,189,3,6,https://github.com/ladislavmacoun,Fix exhausted producer epochs,3,[],https://github.com/edenhill/librdkafka/pull/3721,https://github.com/ladislavmacoun,1,https://github.com/edenhill/librdkafka/pull/3721,Fixes: #3720,Fixes: #3720,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,3721,2022-02-10T19:11:08Z,,2022-05-13T16:06:16Z,OPEN,False,189,3,6,https://github.com/ladislavmacoun,Fix exhausted producer epochs,3,[],https://github.com/edenhill/librdkafka/pull/3721,https://github.com/ladislavmacoun,2,https://github.com/edenhill/librdkafka/pull/3721#issuecomment-1100871527,Fixes: #3720,"@edenhill Thank you for the review!
I've addressed the issues you have pointed out.",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,3733,2022-02-16T12:32:04Z,2022-02-21T10:57:47Z,2022-02-21T10:57:47Z,CLOSED,False,3,3,1,https://github.com/larry-cdn77,Save one superfluous message timeout toppar scan,1,[],https://github.com/edenhill/librdkafka/pull/3733,https://github.com/larry-cdn77,1,https://github.com/edenhill/librdkafka/pull/3733,"C short-circuit evaluation seems to cobble the message timeout scanning condition in rd_kafka_broker_producer_serve such that instead of first iteration only (as intended according to adjacent code comment) you get a timeout in the first and second, so typically two scans instead of one
The side effect appears to have been introduced in 8af1b39 and getting rid of it is a minor optimisation, but consideration worthy for high-partition count topics","C short-circuit evaluation seems to cobble the message timeout scanning condition in rd_kafka_broker_producer_serve such that instead of first iteration only (as intended according to adjacent code comment) you get a timeout in the first and second, so typically two scans instead of one
The side effect appears to have been introduced in 8af1b39 and getting rid of it is a minor optimisation, but consideration worthy for high-partition count topics",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,3733,2022-02-16T12:32:04Z,2022-02-21T10:57:47Z,2022-02-21T10:57:47Z,CLOSED,False,3,3,1,https://github.com/larry-cdn77,Save one superfluous message timeout toppar scan,1,[],https://github.com/edenhill/librdkafka/pull/3733,https://github.com/edenhill,2,https://github.com/edenhill/librdkafka/pull/3733#issuecomment-1041910404,"C short-circuit evaluation seems to cobble the message timeout scanning condition in rd_kafka_broker_producer_serve such that instead of first iteration only (as intended according to adjacent code comment) you get a timeout in the first and second, so typically two scans instead of one
The side effect appears to have been introduced in 8af1b39 and getting rid of it is a minor optimisation, but consideration worthy for high-partition count topics","Good catch!
How did you find this?",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,3733,2022-02-16T12:32:04Z,2022-02-21T10:57:47Z,2022-02-21T10:57:47Z,CLOSED,False,3,3,1,https://github.com/larry-cdn77,Save one superfluous message timeout toppar scan,1,[],https://github.com/edenhill/librdkafka/pull/3733,https://github.com/larry-cdn77,3,https://github.com/edenhill/librdkafka/pull/3733#issuecomment-1041955714,"C short-circuit evaluation seems to cobble the message timeout scanning condition in rd_kafka_broker_producer_serve such that instead of first iteration only (as intended according to adjacent code comment) you get a timeout in the first and second, so typically two scans instead of one
The side effect appears to have been introduced in 8af1b39 and getting rid of it is a minor optimisation, but consideration worthy for high-partition count topics",Thanks for the quick response. How did I spot this? Lots of digging  I have corrected the change as per your comments.,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,3733,2022-02-16T12:32:04Z,2022-02-21T10:57:47Z,2022-02-21T10:57:47Z,CLOSED,False,3,3,1,https://github.com/larry-cdn77,Save one superfluous message timeout toppar scan,1,[],https://github.com/edenhill/librdkafka/pull/3733,https://github.com/larry-cdn77,4,https://github.com/edenhill/librdkafka/pull/3733#issuecomment-1046748067,"C short-circuit evaluation seems to cobble the message timeout scanning condition in rd_kafka_broker_producer_serve such that instead of first iteration only (as intended according to adjacent code comment) you get a timeout in the first and second, so typically two scans instead of one
The side effect appears to have been introduced in 8af1b39 and getting rid of it is a minor optimisation, but consideration worthy for high-partition count topics",I accidentally created this PR from my fork's master  move to #3739,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,3736,2022-02-17T14:36:43Z,2022-03-01T12:03:52Z,2022-03-01T12:13:34Z,MERGED,True,2,2,1,https://github.com/lanceshelton,Update to fedora:35 to fix the CentOS 8 build,1,[],https://github.com/edenhill/librdkafka/pull/3736,https://github.com/lanceshelton,1,https://github.com/edenhill/librdkafka/pull/3736,"mock epel-8-x86_64 is now broken in fedora:33:
https://bugzilla.redhat.com/show_bug.cgi?id=2049024
Update to fedora:35 with mock configs:
centos+epel-7-x86_64
centos-stream+epel-8-x86_64","mock epel-8-x86_64 is now broken in fedora:33:
https://bugzilla.redhat.com/show_bug.cgi?id=2049024
Update to fedora:35 with mock configs:
centos+epel-7-x86_64
centos-stream+epel-8-x86_64",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,3736,2022-02-17T14:36:43Z,2022-03-01T12:03:52Z,2022-03-01T12:13:34Z,MERGED,True,2,2,1,https://github.com/lanceshelton,Update to fedora:35 to fix the CentOS 8 build,1,[],https://github.com/edenhill/librdkafka/pull/3736,https://github.com/edenhill,2,https://github.com/edenhill/librdkafka/pull/3736#issuecomment-1055381511,"mock epel-8-x86_64 is now broken in fedora:33:
https://bugzilla.redhat.com/show_bug.cgi?id=2049024
Update to fedora:35 with mock configs:
centos+epel-7-x86_64
centos-stream+epel-8-x86_64",Thank you!,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,3739,2022-02-21T10:54:36Z,2022-02-24T11:14:04Z,2022-02-24T11:14:08Z,MERGED,True,3,3,1,https://github.com/larry-cdn77,Save one superfluous message timeout toppar scan,1,[],https://github.com/edenhill/librdkafka/pull/3739,https://github.com/larry-cdn77,1,https://github.com/edenhill/librdkafka/pull/3739,"C short-circuit evaluation seems to cobble the message timeout scanning condition in rd_kafka_broker_producer_serve such that instead of first iteration only (as intended according to adjacent code comment) you get a timeout in the first and second, so typically two scans instead of one
The side effect appears to have been introduced in 8af1b39 and getting rid of it is a minor optimisation, but consideration worthy for high-partition count topics","C short-circuit evaluation seems to cobble the message timeout scanning condition in rd_kafka_broker_producer_serve such that instead of first iteration only (as intended according to adjacent code comment) you get a timeout in the first and second, so typically two scans instead of one
The side effect appears to have been introduced in 8af1b39 and getting rid of it is a minor optimisation, but consideration worthy for high-partition count topics",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,3739,2022-02-21T10:54:36Z,2022-02-24T11:14:04Z,2022-02-24T11:14:08Z,MERGED,True,3,3,1,https://github.com/larry-cdn77,Save one superfluous message timeout toppar scan,1,[],https://github.com/edenhill/librdkafka/pull/3739,https://github.com/larry-cdn77,2,https://github.com/edenhill/librdkafka/pull/3739#issuecomment-1046747068,"C short-circuit evaluation seems to cobble the message timeout scanning condition in rd_kafka_broker_producer_serve such that instead of first iteration only (as intended according to adjacent code comment) you get a timeout in the first and second, so typically two scans instead of one
The side effect appears to have been introduced in 8af1b39 and getting rid of it is a minor optimisation, but consideration worthy for high-partition count topics",Supersedes #3733 accidentally created from my fork's master,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,3739,2022-02-21T10:54:36Z,2022-02-24T11:14:04Z,2022-02-24T11:14:08Z,MERGED,True,3,3,1,https://github.com/larry-cdn77,Save one superfluous message timeout toppar scan,1,[],https://github.com/edenhill/librdkafka/pull/3739,https://github.com/edenhill,3,https://github.com/edenhill/librdkafka/pull/3739#issuecomment-1049751416,"C short-circuit evaluation seems to cobble the message timeout scanning condition in rd_kafka_broker_producer_serve such that instead of first iteration only (as intended according to adjacent code comment) you get a timeout in the first and second, so typically two scans instead of one
The side effect appears to have been introduced in 8af1b39 and getting rid of it is a minor optimisation, but consideration worthy for high-partition count topics",Thank you!,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,3741,2022-02-22T10:54:30Z,2022-03-23T15:07:08Z,2022-03-23T15:07:08Z,MERGED,True,815,23,4,https://github.com/emasab,fix: acl binding enum checks,6,[],https://github.com/edenhill/librdkafka/pull/3741,https://github.com/emasab,1,https://github.com/edenhill/librdkafka/pull/3741,"If some kinds of enums like ANY or MATCH are passed when creating an AclBinding, an error should be given before calling the CreateAcls operation on the Kafka broker, in order to return a better explaination, because in these cases it returns an ""unknown error"". Also when a different value is returned from a future version of Kafka that value is mapped to UNKNOWN.
The ACL binding destroy array function is added too","If some kinds of enums like ANY or MATCH are passed when creating an AclBinding, an error should be given before calling the CreateAcls operation on the Kafka broker, in order to return a better explaination, because in these cases it returns an ""unknown error"". Also when a different value is returned from a future version of Kafka that value is mapped to UNKNOWN.
The ACL binding destroy array function is added too",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,3741,2022-02-22T10:54:30Z,2022-03-23T15:07:08Z,2022-03-23T15:07:08Z,MERGED,True,815,23,4,https://github.com/emasab,fix: acl binding enum checks,6,[],https://github.com/edenhill/librdkafka/pull/3741,https://github.com/emasab,2,https://github.com/edenhill/librdkafka/pull/3741#issuecomment-1050769792,"If some kinds of enums like ANY or MATCH are passed when creating an AclBinding, an error should be given before calling the CreateAcls operation on the Kafka broker, in order to return a better explaination, because in these cases it returns an ""unknown error"". Also when a different value is returned from a future version of Kafka that value is mapped to UNKNOWN.
The ACL binding destroy array function is added too",Thanks Magnus! I've addressed those issues,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,3750,2022-02-28T15:20:54Z,,2022-05-28T16:53:35Z,OPEN,False,33,19,2,https://github.com/larry-cdn77,Combine message timeout drain bumps,2,[],https://github.com/edenhill/librdkafka/pull/3750,https://github.com/larry-cdn77,1,https://github.com/edenhill/librdkafka/pull/3750,"Amend idempotent producer message timeout handling from #2163 in relation to CPU usage issue #3692
Issue an idempotent producer drain-bump only once per entire scanning cycle rather than for each toppar
Passes test number 94 (idempotence msg timeout) extended to spot absence of detected timeouts","Amend idempotent producer message timeout handling from #2163 in relation to CPU usage issue #3692
Issue an idempotent producer drain-bump only once per entire scanning cycle rather than for each toppar
Passes test number 94 (idempotence msg timeout) extended to spot absence of detected timeouts",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,3750,2022-02-28T15:20:54Z,,2022-05-28T16:53:35Z,OPEN,False,33,19,2,https://github.com/larry-cdn77,Combine message timeout drain bumps,2,[],https://github.com/edenhill/librdkafka/pull/3750,https://github.com/larry-cdn77,2,https://github.com/edenhill/librdkafka/pull/3750#issuecomment-1140296485,"Amend idempotent producer message timeout handling from #2163 in relation to CPU usage issue #3692
Issue an idempotent producer drain-bump only once per entire scanning cycle rather than for each toppar
Passes test number 94 (idempotence msg timeout) extended to spot absence of detected timeouts","I think I understand the problem in principle, although at a glance I am unsure how any sending could occur during the scanning process that takes place in a toppar lock. If at all possible, making broker wake-ups lighter weight sounds like a better way of reducing the overhead of repeated drain-bump.
I am happy to close this PR",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,3754,2022-03-03T00:11:03Z,,2022-05-27T19:40:11Z,OPEN,False,296,9,9,https://github.com/vctoriawu,KIP 368: SASL Reauthentication,1,[],https://github.com/edenhill/librdkafka/pull/3754,https://github.com/vctoriawu,1,https://github.com/edenhill/librdkafka/pull/3754,Added SASL reauthentication for the following feature: KIP-368,Added SASL reauthentication for the following feature: KIP-368,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,3754,2022-03-03T00:11:03Z,,2022-05-27T19:40:11Z,OPEN,False,296,9,9,https://github.com/vctoriawu,KIP 368: SASL Reauthentication,1,[],https://github.com/edenhill/librdkafka/pull/3754,https://github.com/showuon,2,https://github.com/edenhill/librdkafka/pull/3754#issuecomment-1065833105,Added SASL reauthentication for the following feature: KIP-368,"@edenhill , could you please take a look? Thank you.",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,3754,2022-03-03T00:11:03Z,,2022-05-27T19:40:11Z,OPEN,False,296,9,9,https://github.com/vctoriawu,KIP 368: SASL Reauthentication,1,[],https://github.com/edenhill/librdkafka/pull/3754,https://github.com/showuon,3,https://github.com/edenhill/librdkafka/pull/3754#issuecomment-1065833386,Added SASL reauthentication for the following feature: KIP-368,"@julesbovet , please also take a look. Thank you.",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,3754,2022-03-03T00:11:03Z,,2022-05-27T19:40:11Z,OPEN,False,296,9,9,https://github.com/vctoriawu,KIP 368: SASL Reauthentication,1,[],https://github.com/edenhill/librdkafka/pull/3754,https://github.com/vctoriawu,4,https://github.com/edenhill/librdkafka/pull/3754#issuecomment-1139974032,Added SASL reauthentication for the following feature: KIP-368,@edenhill any updates on the additional requirements?,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,3755,2022-03-03T12:00:32Z,2022-03-03T16:27:36Z,2022-03-03T16:27:36Z,MERGED,True,6,3,1,https://github.com/rmoff,Add link to tutorial on Confluent Developer,1,[],https://github.com/edenhill/librdkafka/pull/3755,https://github.com/rmoff,1,https://github.com/edenhill/librdkafka/pull/3755,Also fix indenting of bullet list,Also fix indenting of bullet list,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,3767,2022-03-18T16:32:59Z,2022-03-21T10:56:05Z,2022-03-21T12:59:57Z,MERGED,True,7,4,4,https://github.com/sarroutbi,Grooming (compilation warning and potential issue),1,[],https://github.com/edenhill/librdkafka/pull/3767,https://github.com/sarroutbi,1,https://github.com/edenhill/librdkafka/pull/3767,Signed-off-by: Sergio Arroutbi sarroutb@redhat.com,Signed-off-by: Sergio Arroutbi sarroutb@redhat.com,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,3767,2022-03-18T16:32:59Z,2022-03-21T10:56:05Z,2022-03-21T12:59:57Z,MERGED,True,7,4,4,https://github.com/sarroutbi,Grooming (compilation warning and potential issue),1,[],https://github.com/edenhill/librdkafka/pull/3767,https://github.com/edenhill,2,https://github.com/edenhill/librdkafka/pull/3767#issuecomment-1073754581,Signed-off-by: Sergio Arroutbi sarroutb@redhat.com,"Thank you!
I notice that your branch is called covscan.. were these changes made as a result of a Coverity scan? If so; are there more to come?
I did a coverity scan a couple of years ago, which perhaps found one or two issues, but had hordes of false positives. Maybe the situation has improved?",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,3767,2022-03-18T16:32:59Z,2022-03-21T10:56:05Z,2022-03-21T12:59:57Z,MERGED,True,7,4,4,https://github.com/sarroutbi,Grooming (compilation warning and potential issue),1,[],https://github.com/edenhill/librdkafka/pull/3767,https://github.com/sarroutbi,3,https://github.com/edenhill/librdkafka/pull/3767#issuecomment-1073778662,Signed-off-by: Sergio Arroutbi sarroutb@redhat.com,"Hello @edenhill. You are welcome.
Yes, this change was result of a covscan execution in RHEL version of librdkafka. As you say, there were many issues, and the majority of them were false positives, but some of them were also ""true positives"". I analyzed all them and ended up with this PR, so from my side I expect no more changes related to covscan in the short term.",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,3767,2022-03-18T16:32:59Z,2022-03-21T10:56:05Z,2022-03-21T12:59:57Z,MERGED,True,7,4,4,https://github.com/sarroutbi,Grooming (compilation warning and potential issue),1,[],https://github.com/edenhill/librdkafka/pull/3767,https://github.com/edenhill,4,https://github.com/edenhill/librdkafka/pull/3767#issuecomment-1073864735,Signed-off-by: Sergio Arroutbi sarroutb@redhat.com,"Good to know, thanks for putting the effort in!",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,3770,2022-03-20T04:39:03Z,2022-03-24T17:03:38Z,2022-03-24T17:03:42Z,MERGED,True,1,1,1,https://github.com/kraj,cmake: Use CMAKE_INSTALL_LIBDIR,1,[],https://github.com/edenhill/librdkafka/pull/3770,https://github.com/kraj,1,https://github.com/edenhill/librdkafka/pull/3770,"this ensures that it is portable across platforms e.g. ppc64/linux
uses lib64 not lib
Signed-off-by: Khem Raj raj.khem@gmail.com","this ensures that it is portable across platforms e.g. ppc64/linux
uses lib64 not lib
Signed-off-by: Khem Raj raj.khem@gmail.com",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,3770,2022-03-20T04:39:03Z,2022-03-24T17:03:38Z,2022-03-24T17:03:42Z,MERGED,True,1,1,1,https://github.com/kraj,cmake: Use CMAKE_INSTALL_LIBDIR,1,[],https://github.com/edenhill/librdkafka/pull/3770,https://github.com/edenhill,2,https://github.com/edenhill/librdkafka/pull/3770#issuecomment-1077842986,"this ensures that it is portable across platforms e.g. ppc64/linux
uses lib64 not lib
Signed-off-by: Khem Raj raj.khem@gmail.com",Thank you!,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,3774,2022-03-22T13:33:47Z,2022-04-05T17:26:11Z,2022-05-13T13:22:26Z,MERGED,True,280,25,11,https://github.com/edenhill,Reset stored offset on assign() and prevent offsets_store() for unassigned partitions,1,[],https://github.com/edenhill/librdkafka/pull/3774,https://github.com/edenhill,1,https://github.com/edenhill/librdkafka/pull/3774,,,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,3774,2022-03-22T13:33:47Z,2022-04-05T17:26:11Z,2022-05-13T13:22:26Z,MERGED,True,280,25,11,https://github.com/edenhill,Reset stored offset on assign() and prevent offsets_store() for unassigned partitions,1,[],https://github.com/edenhill/librdkafka/pull/3774,https://github.com/varunkamra,2,https://github.com/edenhill/librdkafka/pull/3774#issuecomment-1126051254,,"@edenhill I had the exact consumer failure with following error:
offset reset (at offset 178229) to BEGINNING: fetch failed due to requested offset not available on the broker: Broker: Offset out of range

When will the new version of librdkafka be released which fixes it?",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,3782,2022-03-25T08:01:42Z,2022-04-05T13:23:08Z,2022-04-05T13:23:08Z,CLOSED,False,7,0,1,https://github.com/intfish123,fix: Incorrect stored offset will be use after rebalanced,2,[],https://github.com/edenhill/librdkafka/pull/3782,https://github.com/intfish123,1,https://github.com/edenhill/librdkafka/pull/3782,"Reset the stored offset in rebalanced, so that it will not commit previous stored offset to broker, (issue #3745, #3710).
And the codes in rdkafka_assignment.c -> rd_kafka_assignment_serve_removals() -> rd_kafka_offset_store0(rktp, RD_KAFKA_OFFSET_INVALID,RD_DONT_LOCK); can not fix the incorrect offset completely.","Reset the stored offset in rebalanced, so that it will not commit previous stored offset to broker, (issue #3745, #3710).
And the codes in rdkafka_assignment.c -> rd_kafka_assignment_serve_removals() -> rd_kafka_offset_store0(rktp, RD_KAFKA_OFFSET_INVALID,RD_DONT_LOCK); can not fix the incorrect offset completely.",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,3782,2022-03-25T08:01:42Z,2022-04-05T13:23:08Z,2022-04-05T13:23:08Z,CLOSED,False,7,0,1,https://github.com/intfish123,fix: Incorrect stored offset will be use after rebalanced,2,[],https://github.com/edenhill/librdkafka/pull/3782,https://github.com/intfish123,2,https://github.com/edenhill/librdkafka/pull/3782#issuecomment-1078786541,"Reset the stored offset in rebalanced, so that it will not commit previous stored offset to broker, (issue #3745, #3710).
And the codes in rdkafka_assignment.c -> rd_kafka_assignment_serve_removals() -> rd_kafka_offset_store0(rktp, RD_KAFKA_OFFSET_INVALID,RD_DONT_LOCK); can not fix the incorrect offset completely.","The base tag is v1.8.2, and I hope master can fix this problem quickly, because it happend daily on our production environment.",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,3782,2022-03-25T08:01:42Z,2022-04-05T13:23:08Z,2022-04-05T13:23:08Z,CLOSED,False,7,0,1,https://github.com/intfish123,fix: Incorrect stored offset will be use after rebalanced,2,[],https://github.com/edenhill/librdkafka/pull/3782,https://github.com/edenhill,3,https://github.com/edenhill/librdkafka/pull/3782#issuecomment-1079261215,"Reset the stored offset in rebalanced, so that it will not commit previous stored offset to broker, (issue #3745, #3710).
And the codes in rdkafka_assignment.c -> rd_kafka_assignment_serve_removals() -> rd_kafka_offset_store0(rktp, RD_KAFKA_OFFSET_INVALID,RD_DONT_LOCK); can not fix the incorrect offset completely.","Thanks for your contribution!
We already have a fix for this issue in this PR: #3774",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,3782,2022-03-25T08:01:42Z,2022-04-05T13:23:08Z,2022-04-05T13:23:08Z,CLOSED,False,7,0,1,https://github.com/intfish123,fix: Incorrect stored offset will be use after rebalanced,2,[],https://github.com/edenhill/librdkafka/pull/3782,https://github.com/edenhill,4,https://github.com/edenhill/librdkafka/pull/3782#issuecomment-1079261661,"Reset the stored offset in rebalanced, so that it will not commit previous stored offset to broker, (issue #3745, #3710).
And the codes in rdkafka_assignment.c -> rd_kafka_assignment_serve_removals() -> rd_kafka_offset_store0(rktp, RD_KAFKA_OFFSET_INVALID,RD_DONT_LOCK); can not fix the incorrect offset completely.","Can you try out the fix in PR #3774 and verify that it fixes the problem for your use-case?
Thanks",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,3782,2022-03-25T08:01:42Z,2022-04-05T13:23:08Z,2022-04-05T13:23:08Z,CLOSED,False,7,0,1,https://github.com/intfish123,fix: Incorrect stored offset will be use after rebalanced,2,[],https://github.com/edenhill/librdkafka/pull/3782,https://github.com/intfish123,5,https://github.com/edenhill/librdkafka/pull/3782#issuecomment-1080150481,"Reset the stored offset in rebalanced, so that it will not commit previous stored offset to broker, (issue #3745, #3710).
And the codes in rdkafka_assignment.c -> rd_kafka_assignment_serve_removals() -> rd_kafka_offset_store0(rktp, RD_KAFKA_OFFSET_INVALID,RD_DONT_LOCK); can not fix the incorrect offset completely.","Can you try out the fix in PR #3774 and verify that it fixes the problem for your use-case? Thanks

ok, I will try it and verify on my test environment.
And branch resetrestored can be used with confidence on production environment?",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,3782,2022-03-25T08:01:42Z,2022-04-05T13:23:08Z,2022-04-05T13:23:08Z,CLOSED,False,7,0,1,https://github.com/intfish123,fix: Incorrect stored offset will be use after rebalanced,2,[],https://github.com/edenhill/librdkafka/pull/3782,https://github.com/edenhill,6,https://github.com/edenhill/librdkafka/pull/3782#issuecomment-1080283543,"Reset the stored offset in rebalanced, so that it will not commit previous stored offset to broker, (issue #3745, #3710).
And the codes in rdkafka_assignment.c -> rd_kafka_assignment_serve_removals() -> rd_kafka_offset_store0(rktp, RD_KAFKA_OFFSET_INVALID,RD_DONT_LOCK); can not fix the incorrect offset completely.","The branch has been reviewed and tested, but it hasn't been released so there are no guarantees.
It would however be very valuable for everyone if it could be verified before merging,",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,3786,2022-03-29T11:52:38Z,2022-03-29T17:27:36Z,2022-03-29T17:27:38Z,MERGED,True,79,8,4,https://github.com/edenhill,Trigger op callbacks regardless for unhandled types in consume_batch_queue() et.al. (#3263),1,[],https://github.com/edenhill/librdkafka/pull/3786,https://github.com/edenhill,1,https://github.com/edenhill/librdkafka/pull/3786,,,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,3787,2022-03-29T12:55:22Z,2022-03-31T12:53:51Z,2022-03-31T12:54:15Z,MERGED,True,13,16,2,https://github.com/neptoess,Make dynamic MinGW build copy DLLs instead of trying to manipulate PATH,3,[],https://github.com/edenhill/librdkafka/pull/3787,https://github.com/neptoess,1,https://github.com/edenhill/librdkafka/pull/3787,"My stab at fixing the issues highlighted in the #3607 chat recently. When running locally, the two librdkafka DLLs were the only issue. Copying the DLLs to the same folder as the EXE is a much more common practice on Windows than adding the folder the DLLs live in to PATH, so I'm giving this a shot.","My stab at fixing the issues highlighted in the #3607 chat recently. When running locally, the two librdkafka DLLs were the only issue. Copying the DLLs to the same folder as the EXE is a much more common practice on Windows than adding the folder the DLLs live in to PATH, so I'm giving this a shot.",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,3787,2022-03-29T12:55:22Z,2022-03-31T12:53:51Z,2022-03-31T12:54:15Z,MERGED,True,13,16,2,https://github.com/neptoess,Make dynamic MinGW build copy DLLs instead of trying to manipulate PATH,3,[],https://github.com/edenhill/librdkafka/pull/3787,https://github.com/neptoess,2,https://github.com/edenhill/librdkafka/pull/3787#issuecomment-1081839642,"My stab at fixing the issues highlighted in the #3607 chat recently. When running locally, the two librdkafka DLLs were the only issue. Copying the DLLs to the same folder as the EXE is a much more common practice on Windows than adding the folder the DLLs live in to PATH, so I'm giving this a shot.","Hmm, Travis didn't fire off the dynamic MinGW build (https://app.travis-ci.com/github/edenhill/librdkafka/builds/248597005). I don't think I'm able to trigger it either. Can you help me out there @edenhill ? Thanks",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,3787,2022-03-29T12:55:22Z,2022-03-31T12:53:51Z,2022-03-31T12:54:15Z,MERGED,True,13,16,2,https://github.com/neptoess,Make dynamic MinGW build copy DLLs instead of trying to manipulate PATH,3,[],https://github.com/edenhill/librdkafka/pull/3787,https://github.com/edenhill,3,https://github.com/edenhill/librdkafka/pull/3787#issuecomment-1083144966,"My stab at fixing the issues highlighted in the #3607 chat recently. When running locally, the two librdkafka DLLs were the only issue. Copying the DLLs to the same folder as the EXE is a much more common practice on Windows than adding the folder the DLLs live in to PATH, so I'm giving this a shot.","@neptoess The dynamic build requires a tag, you can just remove that ""if: .."" statement in .travis.yml",True,{'THUMBS_UP': ['https://github.com/neptoess']}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,3787,2022-03-29T12:55:22Z,2022-03-31T12:53:51Z,2022-03-31T12:54:15Z,MERGED,True,13,16,2,https://github.com/neptoess,Make dynamic MinGW build copy DLLs instead of trying to manipulate PATH,3,[],https://github.com/edenhill/librdkafka/pull/3787,https://github.com/neptoess,4,https://github.com/edenhill/librdkafka/pull/3787#issuecomment-1083259338,"My stab at fixing the issues highlighted in the #3607 chat recently. When running locally, the two librdkafka DLLs were the only issue. Copying the DLLs to the same folder as the EXE is a much more common practice on Windows than adding the folder the DLLs live in to PATH, so I'm giving this a shot.","@neptoess The dynamic build requires a tag, you can just remove that ""if: .."" statement in .travis.yml

Thank you. Removed the if. Looks like my fix worked. My guess is that something changed on the Travis end where the old approach of updating PATH instead of copying the DLLs stopped working. Relying on environment variable updates is a major dice roll on Windows, so the ""copy the DLLs"" approach is, in my experience, a much more common practice.",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,3787,2022-03-29T12:55:22Z,2022-03-31T12:53:51Z,2022-03-31T12:54:15Z,MERGED,True,13,16,2,https://github.com/neptoess,Make dynamic MinGW build copy DLLs instead of trying to manipulate PATH,3,[],https://github.com/edenhill/librdkafka/pull/3787,https://github.com/edenhill,5,https://github.com/edenhill/librdkafka/pull/3787#issuecomment-1084540789,"My stab at fixing the issues highlighted in the #3607 chat recently. When running locally, the two librdkafka DLLs were the only issue. Copying the DLLs to the same folder as the EXE is a much more common practice on Windows than adding the folder the DLLs live in to PATH, so I'm giving this a shot.","Thank you for so quickly helping out with this, much appreciated!",True,{'THUMBS_UP': ['https://github.com/neptoess']}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,3788,2022-03-30T11:20:16Z,2022-03-30T18:13:39Z,2022-03-30T18:13:43Z,MERGED,True,134,84,13,https://github.com/edenhill,Release build fixes,17,[],https://github.com/edenhill/librdkafka/pull/3788,https://github.com/edenhill,1,https://github.com/edenhill/librdkafka/pull/3788,Fix all dependency fallouts since the last release. ,Fix all dependency fallouts since the last release. ,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,3789,2022-03-31T11:04:09Z,2022-03-31T13:24:45Z,2022-03-31T13:24:48Z,MERGED,True,1,1,1,https://github.com/edenhill,Fix regression from last PR: curl_ldflags,1,[],https://github.com/edenhill/librdkafka/pull/3789,https://github.com/edenhill,1,https://github.com/edenhill/librdkafka/pull/3789,,,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,3797,2022-04-05T08:31:26Z,2022-04-06T15:22:41Z,2022-04-06T15:22:44Z,MERGED,True,44,26,5,https://github.com/edenhill,Include broker_id in offset reset logs and corresponding consumer errors,1,[],https://github.com/edenhill/librdkafka/pull/3797,https://github.com/edenhill,1,https://github.com/edenhill/librdkafka/pull/3797,To ease diagnostics,To ease diagnostics,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,3798,2022-04-05T13:35:34Z,2022-04-07T07:59:12Z,2022-05-23T09:08:57Z,MERGED,True,629,175,25,https://github.com/edenhill,Improve (and fix) producer queue wakeups,16,[],https://github.com/edenhill/librdkafka/pull/3798,https://github.com/edenhill,1,https://github.com/edenhill/librdkafka/pull/3798,And some other goodies,And some other goodies,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,3798,2022-04-05T13:35:34Z,2022-04-07T07:59:12Z,2022-05-23T09:08:57Z,MERGED,True,629,175,25,https://github.com/edenhill,Improve (and fix) producer queue wakeups,16,[],https://github.com/edenhill/librdkafka/pull/3798,https://github.com/wmorgan6796,2,https://github.com/edenhill/librdkafka/pull/3798#issuecomment-1133339692,And some other goodies,Any chance we know when the official 1.9.0 release will be cut (which includes this fix). We can't upgrade until the official release is done unfortunately ,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,3798,2022-04-05T13:35:34Z,2022-04-07T07:59:12Z,2022-05-23T09:08:57Z,MERGED,True,629,175,25,https://github.com/edenhill,Improve (and fix) producer queue wakeups,16,[],https://github.com/edenhill/librdkafka/pull/3798,https://github.com/edenhill,3,https://github.com/edenhill/librdkafka/pull/3798#issuecomment-1134397999,And some other goodies,This week ,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,3799,2022-04-06T08:24:32Z,2022-04-07T07:21:16Z,2022-04-07T07:21:16Z,CLOSED,False,15,0,1,https://github.com/edenhill,Add PR stats,1,[],https://github.com/edenhill/librdkafka/pull/3799,https://github.com/edenhill,1,https://github.com/edenhill/librdkafka/pull/3799,"Pull reviewers stats
Stats for the last 180 days:




User
Total reviews
Time to review
Total comments





mhowlett
16
4h 32m
4



jliunyu
15
6h 58m
1



jonchiu
1
14h 8m
0



edenhill
9
2d 16h 47m
44","Pull reviewers stats
Stats for the last 180 days:




User
Total reviews
Time to review
Total comments





mhowlett
16
4h 32m
4



jliunyu
15
6h 58m
1



jonchiu
1
14h 8m
0



edenhill
9
2d 16h 47m
44",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,3799,2022-04-06T08:24:32Z,2022-04-07T07:21:16Z,2022-04-07T07:21:16Z,CLOSED,False,15,0,1,https://github.com/edenhill,Add PR stats,1,[],https://github.com/edenhill/librdkafka/pull/3799,https://github.com/edenhill,2,https://github.com/edenhill/librdkafka/pull/3799#issuecomment-1091168295,"Pull reviewers stats
Stats for the last 180 days:




User
Total reviews
Time to review
Total comments





mhowlett
16
4h 32m
4



jliunyu
15
6h 58m
1



jonchiu
1
14h 8m
0



edenhill
9
2d 16h 47m
44",meh,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,3800,2022-04-06T11:44:22Z,2022-04-07T06:54:03Z,2022-04-07T06:54:06Z,MERGED,True,26,6,3,https://github.com/edenhill,Txn: properly handle PRODUCER_FENCED in InitPid reply,1,[],https://github.com/edenhill/librdkafka/pull/3800,https://github.com/edenhill,1,https://github.com/edenhill/librdkafka/pull/3800,,,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,3802,2022-04-06T15:03:11Z,2022-04-07T06:54:17Z,2022-04-07T06:54:20Z,MERGED,True,34,16,8,https://github.com/edenhill,Provide reason to broker thread wakeups in debug logs,1,[],https://github.com/edenhill/librdkafka/pull/3802,https://github.com/edenhill,1,https://github.com/edenhill/librdkafka/pull/3802,This will make troubleshooting easier,This will make troubleshooting easier,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,3804,2022-04-07T07:52:10Z,2022-04-07T13:40:54Z,2022-04-07T13:40:56Z,MERGED,True,28,11,2,https://github.com/edenhill,Doc updates,2,[],https://github.com/edenhill/librdkafka/pull/3804,https://github.com/edenhill,1,https://github.com/edenhill/librdkafka/pull/3804,,,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,3805,2022-04-07T08:22:43Z,2022-04-07T13:41:06Z,2022-04-07T13:41:08Z,MERGED,True,40,32,9,https://github.com/edenhill,Make style checking mandatory (on CI) and fix some styling,2,[],https://github.com/edenhill/librdkafka/pull/3805,https://github.com/edenhill,1,https://github.com/edenhill/librdkafka/pull/3805,,,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,3806,2022-04-08T08:28:34Z,2022-04-11T14:19:20Z,2022-04-11T14:19:21Z,MERGED,True,11,9,3,https://github.com/edenhill,Don't wrap API timeout_ms after 1.5 days (#3034),2,[],https://github.com/edenhill/librdkafka/pull/3806,https://github.com/edenhill,1,https://github.com/edenhill/librdkafka/pull/3806,,,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,3807,2022-04-08T08:49:08Z,2022-04-08T14:36:16Z,2022-04-08T14:36:18Z,MERGED,True,40,44,3,https://github.com/edenhill,Some OIDC documentation fixes,1,[],https://github.com/edenhill/librdkafka/pull/3807,https://github.com/edenhill,1,https://github.com/edenhill/librdkafka/pull/3807,,,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,3811,2022-04-11T09:21:59Z,2022-04-12T08:11:44Z,2022-04-12T08:11:44Z,MERGED,True,134,32,11,https://github.com/edenhill,seek() now resets the app_offset so that resume() uses the seeked offset (#3471),1,[],https://github.com/edenhill/librdkafka/pull/3811,https://github.com/edenhill,1,https://github.com/edenhill/librdkafka/pull/3811,,,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,3812,2022-04-12T11:13:35Z,,2022-05-09T12:29:51Z,OPEN,False,1138,527,17,https://github.com/niamster,assignor: API for a custom consumer group assignment strategy,1,[],https://github.com/edenhill/librdkafka/pull/3812,https://github.com/niamster,1,https://github.com/edenhill/librdkafka/pull/3812,"This addresses #2284
Exported API is sufficient enough to implement existing assignment strategies and allow users define their own.
This PR does not change how rebalance works, it reworks existing API so it can be accessible via <librdkafka/rdkafka.h>.
Some structures are split into ""internal"" and ""public"" to minimise the amount of changes. The conversion from one to another is done just before the callbacks are called.
Open questions:

Should this new API be somehow mangled as ""experimental"" by adding _v0 or _exp suffix to functions and structures?
Should callbacks receive a single structure as an argument to be able to extend it in the future w/o breaking compatibility?
I'm not fond of passing ""opaque"" object to the rd_kafka_assignor_register function. I would prefer to bass it via rd_kafka_t but I don't have a strong opinion.

NOTE: there are some changes related to formatting. I have setup my editor to automatically call clang-format on save with a given (default repository) configuration.","This addresses #2284
Exported API is sufficient enough to implement existing assignment strategies and allow users define their own.
This PR does not change how rebalance works, it reworks existing API so it can be accessible via <librdkafka/rdkafka.h>.
Some structures are split into ""internal"" and ""public"" to minimise the amount of changes. The conversion from one to another is done just before the callbacks are called.
Open questions:

Should this new API be somehow mangled as ""experimental"" by adding _v0 or _exp suffix to functions and structures?
Should callbacks receive a single structure as an argument to be able to extend it in the future w/o breaking compatibility?
I'm not fond of passing ""opaque"" object to the rd_kafka_assignor_register function. I would prefer to bass it via rd_kafka_t but I don't have a strong opinion.

NOTE: there are some changes related to formatting. I have setup my editor to automatically call clang-format on save with a given (default repository) configuration.",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,3812,2022-04-12T11:13:35Z,,2022-05-09T12:29:51Z,OPEN,False,1138,527,17,https://github.com/niamster,assignor: API for a custom consumer group assignment strategy,1,[],https://github.com/edenhill/librdkafka/pull/3812,https://github.com/niamster,2,https://github.com/edenhill/librdkafka/pull/3812#issuecomment-1096699493,"This addresses #2284
Exported API is sufficient enough to implement existing assignment strategies and allow users define their own.
This PR does not change how rebalance works, it reworks existing API so it can be accessible via <librdkafka/rdkafka.h>.
Some structures are split into ""internal"" and ""public"" to minimise the amount of changes. The conversion from one to another is done just before the callbacks are called.
Open questions:

Should this new API be somehow mangled as ""experimental"" by adding _v0 or _exp suffix to functions and structures?
Should callbacks receive a single structure as an argument to be able to extend it in the future w/o breaking compatibility?
I'm not fond of passing ""opaque"" object to the rd_kafka_assignor_register function. I would prefer to bass it via rd_kafka_t but I don't have a strong opinion.

NOTE: there are some changes related to formatting. I have setup my editor to automatically call clang-format on save with a given (default repository) configuration.","Interesting, 0120_asymmetric_subscription fails on windows in Appveyor, while it does not fail on linux (Travis CI).
Passes fine on my local linux setup
TEST 20220412145306 (bare, scenario default) SUMMARY
#==================================================================#
| <MAIN>                                   |     PASSED |  13.003s |
| 0120_asymmetric_subscription             |     PASSED |  12.020s |
#==================================================================#
[<MAIN>                      / 13.003s] 0 thread(s) in use by librdkafka
[<MAIN>                      / 13.003s]
============== ALL TESTS PASSED ==============
###
###  ./test-runner in bare mode PASSED! ###
###

uname -a
Linux 5.13.0-23-generic #23-Ubuntu SMP Fri Nov 26 11:41:15 UTC 2021 x86_64 x86_64 x86_64 GNU/Linux

and in the OSX env:
TEST 20220412145911 (bare, scenario default) SUMMARY
#==================================================================#
| <MAIN>                                   |     PASSED |  11.032s |
| 0120_asymmetric_subscription             |     PASSED |  10.090s |
#==================================================================#
[<MAIN>                      / 11.034s] 0 thread(s) in use by librdkafka
[<MAIN>                      / 11.034s]
============== ALL TESTS PASSED ==============
###
###  ./test-runner in bare mode PASSED! ###
###

Darwin 20.6.0 Darwin Kernel Version 20.6.0: Tue Feb 22 21:10:41 PST 2022; root:xnu-7195.141.26~1/RELEASE_X86_64 x86_64 i386 MacBookPro15,2 Darwin

EDIT: it does not pass in Appveyor only, it passes windows build in Travis CI",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,3812,2022-04-12T11:13:35Z,,2022-05-09T12:29:51Z,OPEN,False,1138,527,17,https://github.com/niamster,assignor: API for a custom consumer group assignment strategy,1,[],https://github.com/edenhill/librdkafka/pull/3812,https://github.com/niamster,3,https://github.com/edenhill/librdkafka/pull/3812#issuecomment-1096835819,"This addresses #2284
Exported API is sufficient enough to implement existing assignment strategies and allow users define their own.
This PR does not change how rebalance works, it reworks existing API so it can be accessible via <librdkafka/rdkafka.h>.
Some structures are split into ""internal"" and ""public"" to minimise the amount of changes. The conversion from one to another is done just before the callbacks are called.
Open questions:

Should this new API be somehow mangled as ""experimental"" by adding _v0 or _exp suffix to functions and structures?
Should callbacks receive a single structure as an argument to be able to extend it in the future w/o breaking compatibility?
I'm not fond of passing ""opaque"" object to the rd_kafka_assignor_register function. I would prefer to bass it via rd_kafka_t but I don't have a strong opinion.

NOTE: there are some changes related to formatting. I have setup my editor to automatically call clang-format on save with a given (default repository) configuration.","This is fixed, commit is squashed, the issues was in this loop where I relied on the order, but the array might be manipulated by the caller (sort, etc.).
Why it was failing only in Appveyor   (I will try to dig deeper)? I'm wondering how we can add fuzziness to such tests to make such issues easily reproducible.
EDIT: I've found the root cause. In Appveyor (which uses MSVC) strndup("""", 0) return NULL .... while in other env it returns """".
The fix (committed, squashed) is to check rkgm_group_instance_id for NULL using RD_KAFKAP_STR_IS_NULL(), so rd_kafka_assignor_topic_cmp properly sorts by member id (and ignores group_instance_id).
This raises a question: should rd_kafka_assignor_topic_cmp be fixed to enforce comparison of member_id if group_instance_id are equal  ?",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,3812,2022-04-12T11:13:35Z,,2022-05-09T12:29:51Z,OPEN,False,1138,527,17,https://github.com/niamster,assignor: API for a custom consumer group assignment strategy,1,[],https://github.com/edenhill/librdkafka/pull/3812,https://github.com/niamster,4,https://github.com/edenhill/librdkafka/pull/3812#issuecomment-1121037239,"This addresses #2284
Exported API is sufficient enough to implement existing assignment strategies and allow users define their own.
This PR does not change how rebalance works, it reworks existing API so it can be accessible via <librdkafka/rdkafka.h>.
Some structures are split into ""internal"" and ""public"" to minimise the amount of changes. The conversion from one to another is done just before the callbacks are called.
Open questions:

Should this new API be somehow mangled as ""experimental"" by adding _v0 or _exp suffix to functions and structures?
Should callbacks receive a single structure as an argument to be able to extend it in the future w/o breaking compatibility?
I'm not fond of passing ""opaque"" object to the rd_kafka_assignor_register function. I would prefer to bass it via rd_kafka_t but I don't have a strong opinion.

NOTE: there are some changes related to formatting. I have setup my editor to automatically call clang-format on save with a given (default repository) configuration.",@edenhill are you interested in this PR?,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,3813,2022-04-12T12:22:46Z,2022-04-26T09:56:53Z,2022-04-26T09:58:07Z,MERGED,True,1,1,1,https://github.com/niamster,fix compilation error (minor),1,[],https://github.com/edenhill/librdkafka/pull/3813,https://github.com/niamster,1,https://github.com/edenhill/librdkafka/pull/3813,rdkafka.c:4381:63: error: adding 'int' to a string does not append to,rdkafka.c:4381:63: error: adding 'int' to a string does not append to,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,3813,2022-04-12T12:22:46Z,2022-04-26T09:56:53Z,2022-04-26T09:58:07Z,MERGED,True,1,1,1,https://github.com/niamster,fix compilation error (minor),1,[],https://github.com/edenhill/librdkafka/pull/3813,https://github.com/edenhill,2,https://github.com/edenhill/librdkafka/pull/3813#issuecomment-1109464087,rdkafka.c:4381:63: error: adding 'int' to a string does not append to,"Now that's a silly error, pointer arithmetics is a thing, what compiler and with what -W.. flags reports this?",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,3813,2022-04-12T12:22:46Z,2022-04-26T09:56:53Z,2022-04-26T09:58:07Z,MERGED,True,1,1,1,https://github.com/niamster,fix compilation error (minor),1,[],https://github.com/edenhill/librdkafka/pull/3813,https://github.com/niamster,3,https://github.com/edenhill/librdkafka/pull/3813#issuecomment-1109534918,rdkafka.c:4381:63: error: adding 'int' to a string does not append to,"Now that's a silly error, pointer arithmetics is a thing, what compiler and with what -W.. flags reports this?

That's generated by -Wstring-plus-int which is enabled by default in clang v13.0.0",True,{'THUMBS_UP': ['https://github.com/edenhill']}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,3813,2022-04-12T12:22:46Z,2022-04-26T09:56:53Z,2022-04-26T09:58:07Z,MERGED,True,1,1,1,https://github.com/niamster,fix compilation error (minor),1,[],https://github.com/edenhill/librdkafka/pull/3813,https://github.com/edenhill,4,https://github.com/edenhill/librdkafka/pull/3813#issuecomment-1109596161,rdkafka.c:4381:63: error: adding 'int' to a string does not append to,Thank you!,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,3813,2022-04-12T12:22:46Z,2022-04-26T09:56:53Z,2022-04-26T09:58:07Z,MERGED,True,1,1,1,https://github.com/niamster,fix compilation error (minor),1,[],https://github.com/edenhill/librdkafka/pull/3813,https://github.com/niamster,5,https://github.com/edenhill/librdkafka/pull/3813#issuecomment-1109597327,rdkafka.c:4381:63: error: adding 'int' to a string does not append to,Thank you Magnus ,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,3814,2022-04-12T12:48:39Z,2022-04-14T08:43:04Z,2022-04-14T09:02:56Z,MERGED,True,1,1,1,https://github.com/niamster,fix undefined behaviour in config parsing,1,[],https://github.com/edenhill/librdkafka/pull/3814,https://github.com/niamster,1,https://github.com/edenhill/librdkafka/pull/3814,"Undefined behaviour error:
rdkafka_conf.c:2866:21: runtime error: index 20 out of bounds for type 'struct (anonymous struct at rdkafka_conf.c:87:9) const[20]'","Undefined behaviour error:
rdkafka_conf.c:2866:21: runtime error: index 20 out of bounds for type 'struct (anonymous struct at rdkafka_conf.c:87:9) const[20]'",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,3814,2022-04-12T12:48:39Z,2022-04-14T08:43:04Z,2022-04-14T09:02:56Z,MERGED,True,1,1,1,https://github.com/niamster,fix undefined behaviour in config parsing,1,[],https://github.com/edenhill/librdkafka/pull/3814,https://github.com/edenhill,2,https://github.com/edenhill/librdkafka/pull/3814#issuecomment-1098864693,"Undefined behaviour error:
rdkafka_conf.c:2866:21: runtime error: index 20 out of bounds for type 'struct (anonymous struct at rdkafka_conf.c:87:9) const[20]'","Very good, thank you!",True,{'HEART': ['https://github.com/niamster']}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,3814,2022-04-12T12:48:39Z,2022-04-14T08:43:04Z,2022-04-14T09:02:56Z,MERGED,True,1,1,1,https://github.com/niamster,fix undefined behaviour in config parsing,1,[],https://github.com/edenhill/librdkafka/pull/3814,https://github.com/niamster,3,https://github.com/edenhill/librdkafka/pull/3814#issuecomment-1098891520,"Undefined behaviour error:
rdkafka_conf.c:2866:21: runtime error: index 20 out of bounds for type 'struct (anonymous struct at rdkafka_conf.c:87:9) const[20]'","Very good, thank you!

Thanks!",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,3815,2022-04-13T09:07:00Z,2022-04-14T09:18:28Z,2022-04-14T09:18:32Z,MERGED,True,1,1,1,https://github.com/WhiteWind,Fix MAXPOLL error on Win32 (#3537),1,[],https://github.com/edenhill/librdkafka/pull/3815,https://github.com/WhiteWind,1,https://github.com/edenhill/librdkafka/pull/3815,"Using InterlockedCompareExchange64() in rd_atomic64_get() on Windows, because reading 64bit value on 32bit platform is not atomic. This avoids errors when calculating max poll timeout","Using InterlockedCompareExchange64() in rd_atomic64_get() on Windows, because reading 64bit value on 32bit platform is not atomic. This avoids errors when calculating max poll timeout",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,3815,2022-04-13T09:07:00Z,2022-04-14T09:18:28Z,2022-04-14T09:18:32Z,MERGED,True,1,1,1,https://github.com/WhiteWind,Fix MAXPOLL error on Win32 (#3537),1,[],https://github.com/edenhill/librdkafka/pull/3815,https://github.com/edenhill,2,https://github.com/edenhill/librdkafka/pull/3815#issuecomment-1098912279,"Using InterlockedCompareExchange64() in rd_atomic64_get() on Windows, because reading 64bit value on 32bit platform is not atomic. This avoids errors when calculating max poll timeout",Thank you for this!,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,3816,2022-04-14T08:39:59Z,2022-04-14T12:22:40Z,2022-04-14T12:22:46Z,MERGED,True,135,2,11,https://github.com/edenhill,KIP-601: Added `socket.connection.setup.timeout.ms`,2,[],https://github.com/edenhill/librdkafka/pull/3816,https://github.com/edenhill,1,https://github.com/edenhill/librdkafka/pull/3816,"The Java client has a more aggressive 10s default for this property followed
by an exponential backoff up to a socket.connection.setup.timeout.max.ms.
They need this approach since they try brokers one by one, but that's not
really the case in librdkafka. And since we want to avoid emitting an
ERR__ALL_BROKERS_DOWN too early/aggressively we can afford a higher
default value for the connect timeout and not use the exponential backoff
at all.","The Java client has a more aggressive 10s default for this property followed
by an exponential backoff up to a socket.connection.setup.timeout.max.ms.
They need this approach since they try brokers one by one, but that's not
really the case in librdkafka. And since we want to avoid emitting an
ERR__ALL_BROKERS_DOWN too early/aggressively we can afford a higher
default value for the connect timeout and not use the exponential backoff
at all.",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,3816,2022-04-14T08:39:59Z,2022-04-14T12:22:40Z,2022-04-14T12:22:46Z,MERGED,True,135,2,11,https://github.com/edenhill,KIP-601: Added `socket.connection.setup.timeout.ms`,2,[],https://github.com/edenhill/librdkafka/pull/3816,https://github.com/edenhill,2,https://github.com/edenhill/librdkafka/pull/3816#issuecomment-1099129141,"The Java client has a more aggressive 10s default for this property followed
by an exponential backoff up to a socket.connection.setup.timeout.max.ms.
They need this approach since they try brokers one by one, but that's not
really the case in librdkafka. And since we want to avoid emitting an
ERR__ALL_BROKERS_DOWN too early/aggressively we can afford a higher
default value for the connect timeout and not use the exponential backoff
at all.",Nice turnaround!,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,3818,2022-04-19T07:20:48Z,2022-04-26T07:47:23Z,2022-04-26T16:54:42Z,MERGED,True,203,0,7,https://github.com/jliunyu,Strategy ordering bug fix,5,[],https://github.com/edenhill/librdkafka/pull/3818,https://github.com/jliunyu,1,https://github.com/edenhill/librdkafka/pull/3818,"If multiple strategies are supported, the first strategy would be used. But range one is always used at this moment. This PR will fixed this issue.
TESTS: TESTS=0132 ./run-test.sh valgrind
[0132_strategy_ordering      / 12.606s] ================= Test 0132_strategy_ordering PASSED =================
[<MAIN>                      / 13.410s] ALL-TESTS: duration 13393.378ms
TEST 20220419071425 (valgrind, scenario default) SUMMARY
#==================================================================#
| <MAIN>                                   |     PASSED |  13.411s |
| 0132_strategy_ordering                   |     PASSED |  12.604s |
#==================================================================#
[<MAIN>                      / 14.009s] 0 thread(s) in use by librdkafka
[<MAIN>                      / 14.010s] 
============== ALL TESTS PASSED ==============
==1963337== 
==1963337== FILE DESCRIPTORS: 4 open at exit.
==1963337== Open file descriptor 63:
==1963337==    <inherited from parent>
==1963337== 
==1963337== Open file descriptor 2: /dev/pts/0
==1963337==    <inherited from parent>
==1963337== 
==1963337== Open file descriptor 1: /dev/pts/0
==1963337==    <inherited from parent>
==1963337== 
==1963337== Open file descriptor 0: /dev/pts/0
==1963337==    <inherited from parent>
==1963337== 
==1963337== 
==1963337== HEAP SUMMARY:
==1963337==     in use at exit: 0 bytes in 0 blocks
==1963337==   total heap usage: 3,529 allocs, 3,529 frees, 1,369,295 bytes allocated
==1963337== 
==1963337== All heap blocks were freed -- no leaks are possible
==1963337== 
==1963337== For lists of detected and suppressed errors, rerun with: -s
==1963337== ERROR SUMMARY: 0 errors from 0 contexts (suppressed: 0 from 0)
###
###  ./test-runner in valgrind mode PASSED! ###","If multiple strategies are supported, the first strategy would be used. But range one is always used at this moment. This PR will fixed this issue.
TESTS: TESTS=0132 ./run-test.sh valgrind
[0132_strategy_ordering      / 12.606s] ================= Test 0132_strategy_ordering PASSED =================
[<MAIN>                      / 13.410s] ALL-TESTS: duration 13393.378ms
TEST 20220419071425 (valgrind, scenario default) SUMMARY
#==================================================================#
| <MAIN>                                   |     PASSED |  13.411s |
| 0132_strategy_ordering                   |     PASSED |  12.604s |
#==================================================================#
[<MAIN>                      / 14.009s] 0 thread(s) in use by librdkafka
[<MAIN>                      / 14.010s] 
============== ALL TESTS PASSED ==============
==1963337== 
==1963337== FILE DESCRIPTORS: 4 open at exit.
==1963337== Open file descriptor 63:
==1963337==    <inherited from parent>
==1963337== 
==1963337== Open file descriptor 2: /dev/pts/0
==1963337==    <inherited from parent>
==1963337== 
==1963337== Open file descriptor 1: /dev/pts/0
==1963337==    <inherited from parent>
==1963337== 
==1963337== Open file descriptor 0: /dev/pts/0
==1963337==    <inherited from parent>
==1963337== 
==1963337== 
==1963337== HEAP SUMMARY:
==1963337==     in use at exit: 0 bytes in 0 blocks
==1963337==   total heap usage: 3,529 allocs, 3,529 frees, 1,369,295 bytes allocated
==1963337== 
==1963337== All heap blocks were freed -- no leaks are possible
==1963337== 
==1963337== For lists of detected and suppressed errors, rerun with: -s
==1963337== ERROR SUMMARY: 0 errors from 0 contexts (suppressed: 0 from 0)
###
###  ./test-runner in valgrind mode PASSED! ###",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,3820,2022-04-19T15:45:46Z,,2022-05-29T06:28:27Z,OPEN,False,67,6,2,https://github.com/larry-cdn77,Fix fast leader query back-off,1,[],https://github.com/edenhill/librdkafka/pull/3820,https://github.com/larry-cdn77,1,https://github.com/edenhill/librdkafka/pull/3820,Fix #3690: Metadata leader query timer backoff never applied,Fix #3690: Metadata leader query timer backoff never applied,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,3820,2022-04-19T15:45:46Z,,2022-05-29T06:28:27Z,OPEN,False,67,6,2,https://github.com/larry-cdn77,Fix fast leader query back-off,1,[],https://github.com/edenhill/librdkafka/pull/3820,https://github.com/larry-cdn77,2,https://github.com/edenhill/librdkafka/pull/3820#issuecomment-1102814828,Fix #3690: Metadata leader query timer backoff never applied,"Test checks to see that after 1 second 6 triggers occur of an exponentially backed off 10ms timer (you expect triggers at 10 ms, 30, 70, 150, 310 and 630)",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,3820,2022-04-19T15:45:46Z,,2022-05-29T06:28:27Z,OPEN,False,67,6,2,https://github.com/larry-cdn77,Fix fast leader query back-off,1,[],https://github.com/edenhill/librdkafka/pull/3820,https://github.com/larry-cdn77,3,https://github.com/edenhill/librdkafka/pull/3820#issuecomment-1102862025,Fix #3690: Metadata leader query timer backoff never applied,I don't immediately see why the rdkafka_timer symbols failed to link in the CI build,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,3820,2022-04-19T15:45:46Z,,2022-05-29T06:28:27Z,OPEN,False,67,6,2,https://github.com/larry-cdn77,Fix fast leader query back-off,1,[],https://github.com/edenhill/librdkafka/pull/3820,https://github.com/edenhill,4,https://github.com/edenhill/librdkafka/pull/3820#issuecomment-1109470764,Fix #3690: Metadata leader query timer backoff never applied,"Use in code (src/rdtimer.c) unittests to test the private implementation, see for example unittest_broker.
Also make sure the test runs quickly, preferably in under a handful of seconds.
You can run a single unittest by:
TESTS=0000 RD_UT_TEST=backoff make in the tests/ directory.",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,3820,2022-04-19T15:45:46Z,,2022-05-29T06:28:27Z,OPEN,False,67,6,2,https://github.com/larry-cdn77,Fix fast leader query back-off,1,[],https://github.com/edenhill/librdkafka/pull/3820,https://github.com/larry-cdn77,5,https://github.com/edenhill/librdkafka/pull/3820#issuecomment-1140292934,Fix #3690: Metadata leader query timer backoff never applied,Thank you. I had neglected the unit test framework. It is a unit test now reduced in duration to one second.,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,3820,2022-04-19T15:45:46Z,,2022-05-29T06:28:27Z,OPEN,False,67,6,2,https://github.com/larry-cdn77,Fix fast leader query back-off,1,[],https://github.com/edenhill/librdkafka/pull/3820,https://github.com/larry-cdn77,6,https://github.com/edenhill/librdkafka/pull/3820#issuecomment-1140386918,Fix #3690: Metadata leader query timer backoff never applied,"In order for my test to run reliably I had to round up the microsecond-millisecond conversion of sleep time in rd_kafka_timers_run. Otherwise the value is rounded down and I get a 'dribbling' effect of waiting a bit more, a bit more and a tiny bit more still...
                        sleeptime = rd_kafka_timers_next(rkts, timeout_us,
                                                         0 /*no-lock*/);

                        if (sleeptime > 0) {
                                cnd_timedwait_ms(&rkts->rkts_cond,
                                                 &rkts->rkts_lock,
                                                 (int)(sleeptime / 1000) + 1);
                        }",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,3822,2022-04-21T02:47:05Z,2022-04-26T07:26:31Z,2022-04-26T07:26:36Z,MERGED,True,1,1,1,https://github.com/uncrownedyd,fix config property introduction for custom partitioner callback,1,[],https://github.com/edenhill/librdkafka/pull/3822,https://github.com/uncrownedyd,1,https://github.com/edenhill/librdkafka/pull/3822,"I've noticed some confusion about introduction for the configuration property of 'custom partitioner callback', partitioner is a bit of different from partitioner_cb, according to the description below, it might be partitioner_cb here ?","I've noticed some confusion about introduction for the configuration property of 'custom partitioner callback', partitioner is a bit of different from partitioner_cb, according to the description below, it might be partitioner_cb here ?",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,3822,2022-04-21T02:47:05Z,2022-04-26T07:26:31Z,2022-04-26T07:26:36Z,MERGED,True,1,1,1,https://github.com/uncrownedyd,fix config property introduction for custom partitioner callback,1,[],https://github.com/edenhill/librdkafka/pull/3822,https://github.com/edenhill,2,https://github.com/edenhill/librdkafka/pull/3822#issuecomment-1109443593,"I've noticed some confusion about introduction for the configuration property of 'custom partitioner callback', partitioner is a bit of different from partitioner_cb, according to the description below, it might be partitioner_cb here ?","Good find, thank you!",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,3828,2022-04-25T10:11:45Z,2022-04-25T18:49:23Z,2022-04-25T18:49:24Z,MERGED,True,22,10,2,https://github.com/edenhill,Fix libcurl dependencies on OSX,2,[],https://github.com/edenhill/librdkafka/pull/3828,https://github.com/edenhill,1,https://github.com/edenhill/librdkafka/pull/3828,"Needed for static linking, such as in the Go client.","Needed for static linking, such as in the Go client.",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,3834,2022-04-27T14:09:43Z,2022-04-27T14:18:25Z,2022-04-27T14:18:26Z,MERGED,True,10,0,2,https://github.com/edenhill,Packaging: circumvent new git safe.directory behaviour,1,[],https://github.com/edenhill/librdkafka/pull/3834,https://github.com/edenhill,1,https://github.com/edenhill/librdkafka/pull/3834,,,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,3835,2022-04-28T12:51:59Z,2022-05-02T16:18:25Z,2022-05-02T16:18:26Z,MERGED,True,38,26,1,https://github.com/edenhill,NuGet packaging: librdkafka.redist is now built with VS 2019 and msvcr v142,1,[],https://github.com/edenhill/librdkafka/pull/3835,https://github.com/edenhill,1,https://github.com/edenhill/librdkafka/pull/3835,,,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,3837,2022-05-02T11:00:46Z,2022-05-02T15:32:28Z,2022-05-03T12:54:49Z,MERGED,True,84,50,4,https://github.com/edenhill,Allow any form of unassign*() during consumer close,2,[],https://github.com/edenhill/librdkafka/pull/3837,https://github.com/edenhill,1,https://github.com/edenhill/librdkafka/pull/3837,confluentinc/confluent-kafka-go#767 (comment),confluentinc/confluent-kafka-go#767 (comment),True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,3837,2022-05-02T11:00:46Z,2022-05-02T15:32:28Z,2022-05-03T12:54:49Z,MERGED,True,84,50,4,https://github.com/edenhill,Allow any form of unassign*() during consumer close,2,[],https://github.com/edenhill/librdkafka/pull/3837,https://github.com/kevinconaway,2,https://github.com/edenhill/librdkafka/pull/3837#issuecomment-1116065354,confluentinc/confluent-kafka-go#767 (comment),Thank you!,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,3838,2022-05-03T17:09:32Z,2022-05-04T06:07:39Z,2022-05-04T06:07:39Z,MERGED,True,39,19,9,https://github.com/edenhill,Fix librdkafka-static.a generation,3,[],https://github.com/edenhill/librdkafka/pull/3838,https://github.com/edenhill,1,https://github.com/edenhill/librdkafka/pull/3838,"Building RPM packages on CP centos8 builders caused librdkafka-static.a not to be generated because all dependencies were provided by RPMs. But librdkafka-static.a is included in the librdkafka.spec RPM file, so RPM packaging failed.
This PR makes sure librdkafka-static.a is always generated; if there are no dependencies to link statically it will just be identical to librdkafka.a.
.. and some other minor build fixes.","Building RPM packages on CP centos8 builders caused librdkafka-static.a not to be generated because all dependencies were provided by RPMs. But librdkafka-static.a is included in the librdkafka.spec RPM file, so RPM packaging failed.
This PR makes sure librdkafka-static.a is always generated; if there are no dependencies to link statically it will just be identical to librdkafka.a.
.. and some other minor build fixes.",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,3844,2022-05-12T13:36:38Z,2022-05-12T15:54:25Z,2022-05-12T15:54:25Z,MERGED,True,288,39,8,https://github.com/edenhill,Retry coord_req connections,3,['bug'],https://github.com/edenhill/librdkafka/pull/3844,https://github.com/edenhill,1,https://github.com/edenhill/librdkafka/pull/3844,,,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,3845,2022-05-12T20:52:39Z,2022-05-16T10:56:38Z,2022-05-16T10:56:39Z,MERGED,True,3,0,1,https://github.com/edenhill,Add git safe.directory to RPM mock builder too,1,[],https://github.com/edenhill/librdkafka/pull/3845,https://github.com/edenhill,1,https://github.com/edenhill/librdkafka/pull/3845,Just build stuff,Just build stuff,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,3846,2022-05-15T16:06:49Z,2022-05-16T10:45:18Z,2022-05-16T10:45:18Z,MERGED,True,1,1,1,https://github.com/ihsinme,easy fix,1,[],https://github.com/edenhill/librdkafka/pull/3846,https://github.com/ihsinme,1,https://github.com/edenhill/librdkafka/pull/3846,,,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,3846,2022-05-15T16:06:49Z,2022-05-16T10:45:18Z,2022-05-16T10:45:18Z,MERGED,True,1,1,1,https://github.com/ihsinme,easy fix,1,[],https://github.com/edenhill/librdkafka/pull/3846,https://github.com/edenhill,2,https://github.com/edenhill/librdkafka/pull/3846#issuecomment-1127512321,,Thank you!,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,3847,2022-05-16T10:38:15Z,2022-05-16T17:01:27Z,2022-05-16T17:01:28Z,MERGED,True,91,24,5,https://github.com/edenhill,coord_req: re-query coordinator to avoid getting stuck,2,['enhancement'],https://github.com/edenhill/librdkafka/pull/3847,https://github.com/edenhill,1,https://github.com/edenhill/librdkafka/pull/3847,,,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,3849,2022-05-17T18:48:29Z,2022-05-23T08:30:15Z,2022-05-23T08:30:15Z,MERGED,True,1,1,1,https://github.com/Schm1tz1,Default of linger.ms mentioned in intro was out of sync with latest d,1,[],https://github.com/edenhill/librdkafka/pull/3849,https://github.com/Schm1tz1,1,https://github.com/edenhill/librdkafka/pull/3849,efault value - fixed that.,efault value - fixed that.,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,3850,2022-05-18T07:07:15Z,2022-05-23T08:28:53Z,2022-05-23T08:28:54Z,MERGED,True,4,0,1,https://github.com/emasab,Fix test 103 fails on Kafkas < 2.5.0,1,[],https://github.com/edenhill/librdkafka/pull/3850,https://github.com/emasab,1,https://github.com/edenhill/librdkafka/pull/3850,in versions < 2.5.0 a sleep is necessary before checking the committed offsets,in versions < 2.5.0 a sleep is necessary before checking the committed offsets,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,3853,2022-05-19T12:09:17Z,2022-05-30T13:06:48Z,2022-05-30T13:10:22Z,MERGED,True,23,0,3,https://github.com/espakm,Enable OauthBearer support when librdkafka is built with Curl and SSL using CMake,1,[],https://github.com/edenhill/librdkafka/pull/3853,https://github.com/espakm,1,https://github.com/edenhill/librdkafka/pull/3853,Resolves #3852,Resolves #3852,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,3853,2022-05-19T12:09:17Z,2022-05-30T13:06:48Z,2022-05-30T13:10:22Z,MERGED,True,23,0,3,https://github.com/espakm,Enable OauthBearer support when librdkafka is built with Curl and SSL using CMake,1,[],https://github.com/edenhill/librdkafka/pull/3853,https://github.com/espakm,2,https://github.com/edenhill/librdkafka/pull/3853#issuecomment-1141033892,Resolves #3852,"Have the builds been cancelled because of causing troubles or maybe the linux nodes were not running?
Can I help moving this forward?",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,3853,2022-05-19T12:09:17Z,2022-05-30T13:06:48Z,2022-05-30T13:10:22Z,MERGED,True,23,0,3,https://github.com/espakm,Enable OauthBearer support when librdkafka is built with Curl and SSL using CMake,1,[],https://github.com/edenhill/librdkafka/pull/3853,https://github.com/edenhill,3,https://github.com/edenhill/librdkafka/pull/3853#issuecomment-1141137039,Resolves #3852,Thank you!,True,{'THUMBS_UP': ['https://github.com/espakm']}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,3860,2022-05-26T14:14:37Z,2022-05-26T16:49:23Z,2022-05-26T16:49:24Z,MERGED,True,1,1,1,https://github.com/emasab,fix: correct free,1,[],https://github.com/edenhill/librdkafka/pull/3860,https://github.com/emasab,1,https://github.com/edenhill/librdkafka/pull/3860,in rd_kafka_CreateAclsResponse_parse rd_list,in rd_kafka_CreateAclsResponse_parse rd_list,True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,3862,2022-05-27T19:39:24Z,,2022-05-27T19:39:24Z,OPEN,False,271,3,7,https://github.com/vctoriawu,OpenSSL Engine Certificate Rotation,1,[],https://github.com/edenhill/librdkafka/pull/3862,https://github.com/vctoriawu,1,https://github.com/edenhill/librdkafka/pull/3862,"Currently, certificate are only read on the creation of a client. That means that if the cert changes during the lifetime of the client connection, the client will now have incorrect certificate information. This change reads the cert for every broker reconnection using openssl engine. This means that when certs get rotated, the librdkafka client will always have the most recent one","Currently, certificate are only read on the creation of a client. That means that if the cert changes during the lifetime of the client connection, the client will now have incorrect certificate information. This change reads the cert for every broker reconnection using openssl engine. This means that when certs get rotated, the librdkafka client will always have the most recent one",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,3863,2022-05-31T09:21:45Z,,2022-06-01T00:39:41Z,OPEN,False,370,55,12,https://github.com/edenhill,Added async:able consumer_close_queue() and consumer_closed(),1,[],https://github.com/edenhill/librdkafka/pull/3863,https://github.com/edenhill,1,https://github.com/edenhill/librdkafka/pull/3863,"This is mainly for the Go client, but can be used by applications as well.
This is blocking the v1.9.0 Go release.","This is mainly for the Go client, but can be used by applications as well.
This is blocking the v1.9.0 Go release.",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,3864,2022-05-31T17:40:18Z,2022-05-31T18:39:32Z,2022-05-31T18:39:32Z,MERGED,True,6,0,1,https://github.com/edenhill,Must call ResetEvent() for WaitForMultiplEvents()-polled cndvars,1,[],https://github.com/edenhill/librdkafka/pull/3864,https://github.com/edenhill,1,https://github.com/edenhill/librdkafka/pull/3864,"Not resetting the signalled event caused the next call(s) to
WSAWaitForMultipleEvents() to wake-up immediately, causing increased CPU usage.
Reported in confluentinc/confluent-kafka-dotnet#1809
This is a v1.9.0 regression, so no changelog entry.","Not resetting the signalled event caused the next call(s) to
WSAWaitForMultipleEvents() to wake-up immediately, causing increased CPU usage.
Reported in confluentinc/confluent-kafka-dotnet#1809
This is a v1.9.0 regression, so no changelog entry.",True,{}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,3864,2022-05-31T17:40:18Z,2022-05-31T18:39:32Z,2022-05-31T18:39:32Z,MERGED,True,6,0,1,https://github.com/edenhill,Must call ResetEvent() for WaitForMultiplEvents()-polled cndvars,1,[],https://github.com/edenhill/librdkafka/pull/3864,https://github.com/mhowlett,2,https://github.com/edenhill/librdkafka/pull/3864#issuecomment-1142459705,"Not resetting the signalled event caused the next call(s) to
WSAWaitForMultipleEvents() to wake-up immediately, causing increased CPU usage.
Reported in confluentinc/confluent-kafka-dotnet#1809
This is a v1.9.0 regression, so no changelog entry.",I have verified that this does resolve: confluentinc/confluent-kafka-dotnet#1809,True,{'HOORAY': ['https://github.com/edenhill']}
edenhill/librdkafka,https://github.com/edenhill/librdkafka,3868,2022-06-04T11:10:02Z,,2022-06-04T11:10:02Z,OPEN,False,2,0,1,https://github.com/alicerum,OpenSSL libraries path for M1 MacOS,1,[],https://github.com/edenhill/librdkafka/pull/3868,https://github.com/alicerum,1,https://github.com/edenhill/librdkafka/pull/3868,"As mentioned in the issue #3867
On m1 MacOS homebrew is located in a different place, so configure script is not able to locate libssl and libcrypto.
This change makes it possible to use those libraries during build on M1.","As mentioned in the issue #3867
On m1 MacOS homebrew is located in a different place, so configure script is not able to locate libssl and libcrypto.
This change makes it possible to use those libraries during build on M1.",True,{}
